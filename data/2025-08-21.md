<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 92]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students](https://arxiv.org/abs/2508.14057)
*Pablo G. Almeida,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Pedro Silva,Eduardo Luz*

Main category: cs.LG

TL;DR: 本研究探讨了将表格学生数据转换为图结构是否能提升辍学预测准确率，发现特定GNN配置（GraphSAGE+PCA-KMeans）比传统表格模型表现更优，但图生成策略对结果影响显著。


<details>
  <summary>Details</summary>
Motivation: 学生辍学是全球教育系统的重大挑战，传统机器学习模型在表格数据上已有不错表现，但图神经网络(GNNs)可能通过捕捉学生数据中的复杂关系来提供优势。

Method: 将表格学生数据转换为图结构（主要使用聚类技术），比较GNNs（自定义GCN和GraphSAGE）与表格模型（随机森林、XGBoost、TabNet）的性能，探索基于不同聚类算法（K-Means、HDBSCAN）和降维技术（PCA、UMAP）的图构建策略。

Result: 特定GNN配置（GraphSAGE+PCA-KMeans聚类）表现最佳，比最强表格基线（XGBoost）在宏观F1分数上提升约7个百分点，准确率提升近2个百分点。但其他GNN配置和图形构建方法并未一致超越表格模型。

Conclusion: GNNs在辍学预测中具有潜力，但图生成策略和GNN架构选择至关重要，突显了在该领域将表格数据优化转换为图学习数据所面临的挑战。

Abstract: Student dropout is a significant challenge in educational systems worldwide,
leading to substantial social and economic costs. Predicting students at risk
of dropout allows for timely interventions. While traditional Machine Learning
(ML) models operating on tabular data have shown promise, Graph Neural Networks
(GNNs) offer a potential advantage by capturing complex relationships inherent
in student data if structured as graphs. This paper investigates whether
transforming tabular student data into graph structures, primarily using
clustering techniques, enhances dropout prediction accuracy. We compare the
performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE)
on these generated graphs against established tabular models (Random Forest
(RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments
explore various graph construction strategies based on different clustering
algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques
(Principal Component Analysis (PCA), Uniform Manifold Approximation and
Projection (UMAP)). Our findings demonstrate that a specific GNN configuration,
GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior
performance, notably improving the macro F1-score by approximately 7 percentage
points and accuracy by nearly 2 percentage points over the strongest tabular
baseline (XGBoost). However, other GNN configurations and graph construction
methods did not consistently surpass tabular models, emphasizing the critical
role of the graph generation strategy and GNN architecture selection. This
highlights both the potential of GNNs and the challenges in optimally
transforming tabular data for graph-based learning in this domain.

</details>


### [2] [Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation](https://arxiv.org/abs/2508.14069)
*Chinmoy Biswas,Nafis Faisal,Vivek Chowdhury,Abrar Al-Shadid Abir,Sabir Mahmud,Mithon Rahman,Shaikh Anowarul Fattah,Hafiz Imtiaz*

Main category: cs.LG

TL;DR: 该研究展示了高斯插值方法在处理电力负荷预测中约62%稀疏数据集的有效性，特别是在宽平稳(WSS)数据假设下，LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中的稀疏性（缺失值或零值）给数据操作带来重大挑战，特别是在电力负荷预测等应用中，需要有效处理稀疏数据的方法。

Method: 对小时级电力负荷数据进行统计分析，使用高斯插值处理稀疏数据，训练多种机器学习和深度学习模型（包括LSTM），并比较它们的性能。

Result: 实证研究表明高斯插值适用于负荷预测问题，在约62%稀疏数据集上表现良好，且LSTM神经网络模型在各类模型中性能最优。

Conclusion: 高斯插值是处理稀疏电力负荷数据的有效方法，结合LSTM模型可以为宽平稳数据的负荷预测提供最佳解决方案。

Abstract: Sparsity, defined as the presence of missing or zero values in a dataset,
often poses a major challenge while operating on real-life datasets. Sparsity
in features or target data of the training dataset can be handled using various
interpolation methods, such as linear or polynomial interpolation, spline,
moving average, or can be simply imputed. Interpolation methods usually perform
well with Strict Sense Stationary (SSS) data. In this study, we show that an
approximately 62\% sparse dataset with hourly load data of a power plant can be
utilized for load forecasting assuming the data is Wide Sense Stationary (WSS),
if augmented with Gaussian interpolation. More specifically, we perform
statistical analysis on the data, and train multiple machine learning and deep
learning models on the dataset. By comparing the performance of these models,
we empirically demonstrate that Gaussian interpolation is a suitable option for
dealing with load forecasting problems. Additionally, we demonstrate that Long
Short-term Memory (LSTM)-based neural network model offers the best performance
among a diverse set of classical and neural network-based models.

</details>


### [3] [Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems](https://arxiv.org/abs/2508.14071)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.LG

TL;DR: 一种结合机器学习和元渗道的混合机制，通过边结构选择器模型分类解决边来指导局部搜索，用于解决车辆路线问题


<details>
  <summary>Details</summary>
Motivation: 为了提高元渗道算法在车辆路线问题中的性能，通过机器学习来指导局部搜索过程，避免无效移动

Method: 使用两种学习机制：表格化二元分类器（根据提升树和前向神经网络）和图神经网络（GNN）来预测被禁止的移动，并应用于先进元渗道基准算法

Result: 方法在不同基准元渗道、各种问题规模（包括30,000个客户节点）和问题变体（CVRP和CVRPTW）上都展现出性能提升，具有可扩展性和通用性

Conclusion: 这种混合机器学习和元渗道的方法能够有效提高车辆路线问题的解决效果，通过预测被禁止移动来指导搜索过程，在大规模问题上也保持良好性能

Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism
that is designed to solve Vehicle Routing Problems (VRPs). The main of our
method is an edge solution selector model, which classifies solution edges to
identify prohibited moves during the local search, hence guiding the search
process within metaheuristic baselines. Two learning-based mechanisms are used
to develop the edge selector: a simple tabular binary classifier and a Graph
Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees
and Feedforward Neural Network as the baseline algorithms. Adjustments to the
decision threshold are also applied to handle the class imbalance in the
problem instance. An alternative mechanism employs the GNN to utilize graph
structure for direct solution edge prediction, with the objective of guiding
local search by predicting prohibited moves. These hybrid mechanisms are then
applied in state-fo-the-art metaheuristic baselines. Our method demonstrates
both scalability and generalizability, achieving performance improvements
across different baseline metaheuristics, various problem sizes and variants,
including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time
Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000
customer nodes, supported by pair-wise statistical analysis, verify the
observed improvements.

</details>


### [4] [Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration](https://arxiv.org/abs/2508.14072)
*Anabel Yong*

Main category: cs.LG

TL;DR: GP-MOBO是一种新颖的多目标贝叶斯优化算法，通过整合高效的精确高斯过程处理完整分子指纹维度，在分子优化任务中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化方法在处理高维稀疏分子指纹时面临计算资源需求大和效率低的问题，需要开发更高效的算法来充分利用指纹维度信息。

Method: 开发了GP-MOBO算法，整合了快速精确高斯过程包，能够高效处理完整维度的稀疏分子指纹，无需大量计算资源，充分挖掘指纹维度信息。

Result: 在DockSTRING数据集上的实验表明，GP-MOBO在20次贝叶斯优化迭代中获得更高的几何平均值，在所有测试场景中更接近帕累托前沿，识别出更高质量和有效的SMILES分子。

Conclusion: GP-MOBO在计算开销最小的情况下，有效解决了复杂多目标优化挑战，在分子优化领域实现了最先进的性能，具有优异的探索能力和优化效果。

Abstract: We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm
that advances the state-of-the-art in molecular optimization. Our approach
integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of
efficiently handling the full dimensionality of sparse molecular fingerprints
without the need for extensive computational resources. GP-MOBO consistently
outperforms traditional methods like GP-BO by fully leveraging fingerprint
dimensionality, leading to the identification of higher-quality and valid
SMILES. Moreover, our model achieves a broader exploration of the chemical
search space, as demonstrated by its superior proximity to the Pareto front in
all tested scenarios. Empirical results from the DockSTRING dataset reveal that
GP-MOBO yields higher geometric mean values across 20 Bayesian optimization
iterations, underscoring its effectiveness and efficiency in addressing complex
multi-objective optimization challenges with minimal computational overhead.

</details>


### [5] [MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets](https://arxiv.org/abs/2508.14073)
*Qian Zhanga,Ruilin Zhang,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 提出MCLPD半监督学习框架，结合多视图对比预训练和轻量级监督微调，显著提升帕金森病跨数据集检测性能，仅需1-5%标注数据即可达到优异效果


<details>
  <summary>Details</summary>
Motivation: 解决EEG数据标注成本高导致的标注数据稀缺、数据集间差异大等问题，提升帕金森病检测模型在跨数据集场景下的鲁棒性和泛化能力

Method: MCLPD框架：1）在未标注UNM数据集上进行自监督预训练，使用时域和频域双重增强构建对比对；2）使用少量标注数据（UI和UC数据集）进行监督微调

Result: 仅使用1%标注数据时，在UI和UC数据集上分别达到0.91和0.81的F1分数；使用5%标注数据时分别提升至0.97和0.87，显著优于现有方法

Conclusion: MCLPD有效降低了标注数据依赖，显著提升了跨数据集泛化性能，证明了该框架在帕金森病EEG检测中的有效性

Abstract: Electroencephalography has been validated as an effective technique for
detecting Parkinson's disease,particularly in its early stages.However,the high
cost of EEG data annotation often results in limited dataset size and
considerable discrepancies across datasets,including differences in acquisition
protocols and subject demographics,significantly hinder the robustness and
generalizability of models in cross-dataset detection scenarios.To address such
challenges,this paper proposes a semi-supervised learning framework named
MCLPD,which integrates multi-view contrastive pre-training with lightweight
supervised fine-tuning to enhance cross-dataset PD detection performance.During
pre-training,MCLPD uses self-supervised learning on the unlabeled UNM
dataset.To build contrastive pairs,it applies dual augmentations in both time
and frequency domains,which enrich the data and naturally fuse time-frequency
information.In the fine-tuning phase,only a small proportion of labeled data
from another two datasets (UI and UC)is used for supervised
optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on
UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97
and 0.87,respectively,when 5%of labeled data is used.Compared to existing
methods,MCLPD substantially improves cross-dataset generalization while
reducing the dependency on labeled data,demonstrating the effectiveness of the
proposed framework.

</details>


### [6] [GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease](https://arxiv.org/abs/2508.14074)
*Qian Zhang,Ruilin Zhang,Biaokai Zhu,Xun Han,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 提出GEPD模型，使用GAN生成融合EEG数据增强跨数据集帕金森病检测，在跨数据集场景下达到84.3%准确率和84.0% F1分数


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病EEG检测方法在单个数据集表现良好，但不同数据集间检测方法差异大且数据集小，难以训练通用化的跨数据集模型

Method: 设计生成网络控制生成数据与真实数据分布相似性来创建融合EEG数据，并设计EEG信号质量评估模型保证生成质量；设计分类网络结合多个CNN捕捉EEG时频特征，保持通用化结构并确保易收敛

Result: 在跨数据集设置下性能与最先进模型相当，准确率84.3%，F1分数84.0%

Conclusion: 所提模型展示了良好的通用性，有助于神经疾病的诊断和监测

Abstract: Electroencephalography has been established as an effective method for
detecting Parkinson's disease, typically diagnosed early.Current Parkinson's
disease detection methods have shown significant success within individual
datasets, however, the variability in detection methods across different EEG
datasets and the small size of each dataset pose challenges for training a
generalizable model for cross-dataset scenarios. To address these issues, this
paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for
EEG-based cross-dataset classification of Parkinson's disease.First, we design
a generative network that creates fusion EEG data by controlling the
distribution similarity between generated data and real data.In addition, an
EEG signal quality assessment model is designed to ensure the quality of
generated data great.Second, we design a classification network that utilizes a
combination of multiple convolutional neural networks to effectively capture
the time-frequency characteristics of EEG signals, while maintaining a
generalizable structure and ensuring easy convergence.This work is dedicated to
utilizing intelligent methods to study pathological manifestations, aiming to
facilitate the diagnosis and monitoring of neurological diseases.The evaluation
results demonstrate that our model performs comparably to state-of-the-art
models in cross-dataset settings, achieving an accuracy of 84.3% and an
F1-score of 84.0%, showcasing the generalizability of the proposed model.

</details>


### [7] [Explainable Graph Spectral Clustering For Text Embeddings](https://arxiv.org/abs/2508.14075)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Piotr Borkowski,Dariusz Czerski,Eryk Laskowski*

Main category: cs.LG

TL;DR: 本文扩展了图谱聚类在文档分析中的可解释性方法，从基于词向量空间余弦相似度的文档嵌入推广到其他嵌入方法，特别是GloVe嵌入


<details>
  <summary>Details</summary>
Motivation: 在先前研究基础上，希望将文档图谱聚类结果的可解释性方法从特定的词向量空间扩展到更广泛的文档嵌入表示，以提高方法的通用性和适用性

Method: 采用GloVe等嵌入方法来构建文档表示，然后应用图谱聚类技术，并开发相应的可解释性分析方法

Result: 成功将原有的可解释性框架扩展到基于GloVe嵌入的文档表示，证明了方法的通用性和扩展性

Conclusion: 本文提出的方法能够有效处理不同文档嵌入方式下的图谱聚类结果解释问题，为文档分析提供了更灵活和通用的可解释性解决方案

Abstract: In a previous paper, we proposed an introduction to the explainability of
Graph Spectral Clustering results for textual documents, given that document
similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of
documents, in particular, based on the GloVe embedding idea.

</details>


### [8] [PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning](https://arxiv.org/abs/2508.14076)
*Mengdi Li,Guanqiao Chen,Xufeng Zhao,Haochen Wen,Shu Yang,Di Wang*

Main category: cs.LG

TL;DR: PersRM-R1是一个基于推理的奖励建模框架，通过少量个人示例就能识别和表示个人偏好因素，在准确性和泛化性方面优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型难以捕捉细微的、用户特定的偏好，特别是在数据有限和跨领域的情况下，需要开发能够更好理解个人偏好的奖励模型。

Method: 结合合成数据生成和两阶段训练流程（监督微调后接强化微调），专门设计用于从少量个人示例中识别个人因素的推理奖励建模框架。

Result: 实验结果表明PersRM-R1在准确性和泛化性方面优于同类规模模型，并能与更大模型的性能相匹配。

Conclusion: 该框架为开发更有效的个性化大语言模型铺平了道路，能够更好地理解和适应个人用户的特定偏好。

Abstract: Reward models (RMs), which are central to existing post-training methods, aim
to align LLM outputs with human values by providing feedback signals during
fine-tuning. However, existing RMs struggle to capture nuanced, user-specific
preferences, especially under limited data and across diverse domains. Thus, we
introduce PersRM-R1, the first reasoning-based reward modeling framework
specifically designed to identify and represent personal factors from only one
or a few personal exemplars. To address challenges including limited data
availability and the requirement for robust generalization, our approach
combines synthetic data generation with a two-stage training pipeline
consisting of supervised fine-tuning followed by reinforcement fine-tuning.
Experimental results demonstrate that PersRM-R1 outperforms existing models of
similar size and matches the performance of much larger models in both accuracy
and generalizability, paving the way for more effective personalized LLMs.

</details>


### [9] [Label Smoothing is a Pragmatic Information Bottleneck](https://arxiv.org/abs/2508.14077)
*Sota Kudo*

Main category: cs.LG

TL;DR: 标签平滑通过信息瓶颈理论重新解释，在模型足够灵活且无标签冲突的假设下，理论证明标签平滑输出探索信息瓶颈最优解，可作为信息瓶颈的实用实现方法。


<details>
  <summary>Details</summary>
Motivation: 重新审视标签平滑方法，从信息瓶颈理论角度为其提供理论解释，探索其作为信息瓶颈实用实现方法的潜力。

Method: 基于信息瓶颈理论框架，在模型足够灵活和无标签冲突的假设下，理论推导标签平滑输出与信息瓶颈最优解的关系，并通过实验验证。

Result: 理论证明标签平滑输出探索信息瓶颈最优解，实验显示标签平滑具有对目标无关因素不敏感的特性，符合信息瓶颈方法的性质。

Conclusion: 标签平滑可解释为信息瓶颈的实用实现方法，提供简单有效的正则化技术，具有对无关信息不敏感的优势。

Abstract: This study revisits label smoothing via a form of information bottleneck.
Under the assumption of sufficient model flexibility and no conflicting labels
for the same input, we theoretically and experimentally demonstrate that the
model output obtained through label smoothing explores the optimal solution of
the information bottleneck. Based on this, label smoothing can be interpreted
as a practical approach to the information bottleneck, enabling simple
implementation. As an information bottleneck method, we experimentally show
that label smoothing also exhibits the property of being insensitive to factors
that do not contain information about the target, or to factors that provide no
additional information about it when conditioned on another variable.

</details>


### [10] [Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction](https://arxiv.org/abs/2508.14078)
*Mohamed Hassan Abdalla Idris,Jakub Marek Cebula,Jebraeel Gholinezhad,Shamsul Masum,Hongjie Ma*

Main category: cs.LG

TL;DR: 本研究提出了一种结合产量指数特征选择和归纳包纶预测的机器学习框架，用于提高油气生产预测的稳健性和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 解决多元时间序列分析中油气生产预测的外样本稳健性问题，提高预测的可靠性和不确定性量化水平。

Method: 集成产量指数(PI)驱动的特征选择和归纳包纶预测(ICP)方法，测试了LSTM、BiLSTM、GRU和XGBoost等多种预测算法，使用MAE、预测偏差和预测方向准确率进行评估。

Result: LSTM模型表现最优，在PF14井的测试集和真实外样本预测中分别获得最低MAE(19.468和29.638)，PI特征选择有效降低了输入维度，ICP提供了有效的95%预测区间。

Conclusion: 结合领域特定知识和先进机器学习技术可显著提高油气生产预测的可靠性，为油田开发提供了更稳健的预测方案。

Abstract: This research introduces a new ML framework designed to enhance the
robustness of out-of-sample hydrocarbon production forecasting, specifically
addressing multivariate time series analysis. The proposed methodology
integrates Productivity Index (PI)-driven feature selection, a concept derived
from reservoir engineering, with Inductive Conformal Prediction (ICP) for
rigorous uncertainty quantification. Utilizing historical data from the Volve
(wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the
efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM),
Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient
Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H).
All the models achieved "out-of-sample" production forecasts for an upcoming
future timeframe. Model performance was comprehensively evaluated using
traditional error metrics (e.g., MAE) supplemented by Forecast Bias and
Prediction Direction Accuracy (PDA) to assess bias and trend-capturing
capabilities. The PI-based feature selection effectively reduced input
dimensionality compared to conventional numerical simulation workflows. The
uncertainty quantification was addressed using the ICP framework, a
distribution-free approach that guarantees valid prediction intervals (e.g.,
95% coverage) without reliance on distributional assumptions, offering a
distinct advantage over traditional confidence intervals, particularly for
complex, non-normal data. Results demonstrated the superior performance of the
LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample
forecast data (29.638) for well PF14, with subsequent validation on Norne well
E1H. These findings highlight the significant potential of combining
domain-specific knowledge with advanced ML techniques to improve the
reliability of hydrocarbon production forecasts.

</details>


### [11] [A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy](https://arxiv.org/abs/2508.14079)
*Maxime Heuillet,Rishika Bhagwatkar,Jonas Ngnawé,Yann Pequignot,Alexandre Larouche,Christian Gagné,Irina Rish,Ola Ahmad,Audrey Durand*

Main category: cs.LG

TL;DR: 本文通过大规模实证研究发现，在大数据集上监督预训练的卷积神经网络在鲁棒微调中表现最佳，挑战了当前流行的注意力架构和鲁棒预训练表示的优势假设。


<details>
  <summary>Details</summary>
Motivation: 深度学习中鲁棒微调的设计选择（如模型架构、预训练表示、优化策略等）如何影响泛化能力仍是一个开放问题，需要系统性的实证研究来指导实践。

Method: 在6个数据集、40种预训练架构、2种专用损失函数和3种适应协议下进行了1,440种训练配置和7,200次鲁棒性测量，涵盖了五种扰动类型，构建了迄今为止最全面的鲁棒微调基准。

Result: 发现监督预训练的卷积神经网络往往表现最佳，这与当前流行的注意力架构和鲁棒预训练表示的趋势形成对比，同时确认和挑战了先前的设计假设。

Conclusion: 研究为鲁棒微调提供了实用的指导，指出了有前景的研究方向，并强调了传统卷积架构在大规模监督预训练下的持续有效性。

Abstract: Deep learning models operating in the image domain are vulnerable to small
input perturbations. For years, robustness to such perturbations was pursued by
training models from scratch (i.e., with random initializations) using
specialized loss objectives. Recently, robust fine-tuning has emerged as a more
efficient alternative: instead of training from scratch, pretrained models are
adapted to maximize predictive performance and robustness. To conduct robust
fine-tuning, practitioners design an optimization strategy that includes the
model update protocol (e.g., full or partial) and the specialized loss
objective. Additional design choices include the architecture type and size,
and the pretrained representation. These design choices affect robust
generalization, which is the model's ability to maintain performance when
exposed to new and unseen perturbations at test time. Understanding how these
design choices influence generalization remains an open question with
significant practical implications. In response, we present an empirical study
spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3
adaptation protocols, yielding 1,440 training configurations and 7,200
robustness measurements across five perturbation types. To our knowledge, this
is the most diverse and comprehensive benchmark of robust fine-tuning to date.
While attention-based architectures and robust pretrained representations are
increasingly popular, we find that convolutional neural networks pretrained in
a supervised manner on large datasets often perform best. Our analysis both
confirms and challenges prior design assumptions, highlighting promising
research directions and offering practical guidance.

</details>


### [12] [KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge](https://arxiv.org/abs/2508.14080)
*Guanghao Jin,Jingpei Wu,Tianpei Guo,Yiyi Niu,Weidong Zhou,Guoyang Liu*

Main category: cs.LG

TL;DR: 提出了KnowDR-REC新基准，用于评估多模态大语言模型在知识驱动的视觉定位任务中的表现，发现现有模型仍存在困难并存在文本理解与视觉定位的解耦问题。


<details>
  <summary>Details</summary>
Motivation: 传统REC基准仅依赖图像内线索或缺乏细粒度实例标注，无法充分评估多模态大语言模型的推理能力，需要构建基于真实世界知识的基准来测试细粒度多模态推理。

Method: 构建KnowDR-REC基准，包含基于真实世界知识的样本、通过细粒度表达编辑构建的负样本，并引入三种新评估指标来系统探索模型内部推理过程。

Result: 评估16个最先进多模态模型，结果显示现有MLLMs在知识驱动的视觉定位任务中仍存在困难，观察到文本理解与视觉定位的解耦现象，模型受记忆的捷径相关性影响严重。

Conclusion: 该基准将推动开发更鲁棒、可解释和知识密集的视觉定位框架，促进更可靠和鲁棒的多模态系统在复杂现实场景中的发展。

Abstract: Referring Expression Comprehension (REC) is a popular multimodal task that
aims to accurately detect target objects within a single image based on a given
textual expression. However, due to the limitations of earlier models,
traditional REC benchmarks either rely solely on intra-image cues or lack
sufficiently fine-grained instance annotations, making them inadequate for
evaluating the reasoning capabilities of Multi-modal Large Language Models
(MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC,
characterized by three key features: Firstly, it is built upon real-world
knowledge, requiring fine-grained multimodal reasoning across text and image.
Secondly, the dataset includes elaborately constructed negative samples via
fine-grained expression editing, designed to evaluate a model's robustness and
anti-hallucination ability. Lastly, we introduce three novel evaluation metrics
to systematically explore the model's internal reasoning process. We evaluate
16 state-of-the-art multimodal models on KnowDR-REC, with experimental results
showing that existing MLLMs still struggle with knowledge-driven visual
grounding tasks. Furthermore, we observe a decoupling between textual
understanding and visual grounding in MLLMs, where many models are
significantly influenced by memorized shortcut correlations, which severely
affect their behavior on our benchmark and hinder genuine multimodal reasoning.
We anticipate that the proposed benchmark will inspire future research towards
developing more robust, interpretable, and knowledge-intensive visual grounding
frameworks, driving the development of more reliable and robust multimodal
systems for complex real-world scenarios.

</details>


### [13] [FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning](https://arxiv.org/abs/2508.14539)
*Tao Shen,Zexi Li,Didi Zhu,Ziyu Zhao,Chao Wu,Fei Wu*

Main category: cs.LG

TL;DR: 论文研究了联邦学习中的周期偏移问题，提出FedEve方法通过调节客户偏移来补偿周期偏移，在非IID数据下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异质性导致收敛慢和性能下降。除了已知的客户偏移外，跨设备FL中部分客户参与导致的周期偏移更为有害，但尚未被充分研究。

Method: 提出预测-观察框架，并实例化为FedEve方法。该方法让两种偏移相互补偿，通过调整客户偏移来对抑周期偏移的影响。

Result: 理论证明方法能降低模型更新的方差。大量实验表明在跨设备FL设置中，方法在非IID数据上表现超过其他方法。

Conclusion: 周期偏移是跨设备FL中的重要问题，与客户偏移相互作用。FedEve通过让两种偏移相互补偿，有效缓解了数据异质性带来的挑战。

Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model without exposing their private
data. Data heterogeneity is a fundamental challenge in FL, which can result in
poor convergence and performance degradation. Client drift has been recognized
as one of the factors contributing to this issue resulting from the multiple
local updates in FedAvg. However, in cross-device FL, a different form of drift
arises due to the partial client participation, but it has not been studied
well. This drift, we referred as period drift, occurs as participating clients
at each communication round may exhibit distinct data distribution that
deviates from that of all clients. It could be more harmful than client drift
since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client
drift, finding that period drift can have a particularly detrimental effect on
cross-device FL as the degree of data heterogeneity increases. To tackle these
issues, we propose a predict-observe framework and present an instantiated
method, FedEve, where these two types of drift can compensate each other to
mitigate their overall impact. We provide theoretical evidence that our
approach can reduce the variance of model updates. Extensive experiments
demonstrate that our method outperforms alternatives on non-iid data in
cross-device settings.

</details>


### [14] [Toward Lifelong Learning in Equilibrium Propagation: Sleep-like and Awake Rehearsal for Enhanced Stability](https://arxiv.org/abs/2508.14081)
*Yoshimasa Kubo,Jean Erik Delanois,Maxim Bazhenov*

Main category: cs.LG

TL;DR: 该论文提出了一种睡眠式回放巩固(SRC)算法，用于解决基于平衡传播(EP)训练的循环神经网络在持续学习中的灾难性遗忘问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 基于平衡传播训练的循环神经网络在持续学习中面临灾难性遗忘问题，而人脑通过睡眠中的记忆回放巩固机制能够有效保留新旧知识，这启发了研究者开发类似的算法。

Method: 提出了睡眠式回放巩固(SRC)算法，在每次新任务训练后实施，通过回放已学习信息来巩固记忆，并与清醒回放(rehearsal)技术结合使用。

Result: SRC显著提高了EP训练的多层RNN在持续学习中的抗遗忘能力，在多个数据集上表现优于包含正则化技术的前馈网络，甚至在某些数据集上超越了基于BPTT训练的模型。

Conclusion: 研究表明睡眠式回放技术适用于循环神经网络，为将类人学习行为整合到人工神经网络中展现了潜力。

Abstract: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP),
a biologically plausible training algorithm, have demonstrated strong
performance in various tasks such as image classification and reinforcement
learning. However, these networks face a critical challenge in continuous
learning: catastrophic forgetting, where previously acquired knowledge is
overwritten when new tasks are learned. This limitation contrasts with the
human brain's ability to retain and integrate both old and new knowledge, aided
by processes like memory consolidation during sleep through the replay of
learned information. To address this challenge in RNNs, here we propose a
sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs. We found
that SRC significantly improves RNN's resilience to catastrophic forgetting in
continuous learning scenarios. In class-incremental learning with SRC
implemented after each new task training, the EP-trained multilayer RNN model
(MRNN-EP) performed significantly better compared to feedforward networks
incorporating several well-established regularization techniques. The MRNN-EP
performed on par with MRNN trained using Backpropagation Through Time (BPTT)
when both were equipped with SRC on MNIST data and surpassed BPTT-based models
on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.
Combining SRC with rehearsal, also known as "awake replay", further boosted the
network's ability to retain long-term knowledge while continuing to learn new
tasks. Our study reveals the applicability of sleep-like replay techniques to
RNNs and highlights the potential for integrating human-like learning behaviors
into artificial neural networks (ANNs).

</details>


### [15] [Cooperative SGD with Dynamic Mixing Matrices](https://arxiv.org/abs/2508.14565)
*Soumya Sarkar,Shweta Jain*

Main category: cs.LG

TL;DR: 这篇论文提出了一个统一框架，用于分析具有动态拓扑结构和客户选择的分布式SGD算法，并提供了收敛性理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SGD算法假设固定拓扑结构和均匀节点贡献，但实验证明这些假设是次优的。非均匀聚合策略与动态拓扑结构结合可显著提升模型性能。

Method: 设计了一个统一框架，包含多种基于本地更新SGD的分布式算法，支持动态拓扑结构变化和客户选择机制。

Result: 该框架能够提供更优或至少相等的收敛性理论保证，超越了现有工作的理论分析。

Conclusion: 动态拓扑结构和非均匀聚合策略在分布式SGD中具有重要价值，该统一框架为这些算法提供了严格的理论基础。

Abstract: One of the most common methods to train machine learning algorithms today is
the stochastic gradient descent (SGD). In a distributed setting, SGD-based
algorithms have been shown to converge theoretically under specific
circumstances. A substantial number of works in the distributed SGD setting
assume a fixed topology for the edge devices. These papers also assume that the
contribution of nodes to the global model is uniform. However, experiments have
shown that such assumptions are suboptimal and a non uniform aggregation
strategy coupled with a dynamically shifting topology and client selection can
significantly improve the performance of such models. This paper details a
unified framework that covers several Local-Update SGD-based distributed
algorithms with dynamic topologies and provides improved or matching
theoretical guarantees on convergence compared to existing work.

</details>


### [16] [Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation](https://arxiv.org/abs/2508.14082)
*Ye Su,Hezhe Qiao,Wei Huang,Lin Chen*

Main category: cs.LG

TL;DR: 提出DRILL框架，将半监督回归任务转换为离散分布估计任务，通过解耦分布对齐提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有半监督回归方法过度依赖伪标签质量，直接回归容易过拟合且难以学习标签分布

Method: 将回归任务转换为离散分布估计，使用解耦分布对齐(DDA)对齐师生模型的目标桶和非目标桶分布

Result: 在多个领域数据集上的实验表明DRILL具有强泛化能力并优于竞争方法

Conclusion: DRILL框架通过离散化处理和分布对齐有效解决了半监督回归中的过拟合和伪标签质量问题

Abstract: Semi-supervised regression (SSR), which aims to predict continuous scores of
samples while reducing reliance on a large amount of labeled data, has recently
received considerable attention across various applications, including computer
vision, natural language processing, and audio and medical analysis. Existing
semi-supervised methods typically apply consistency regularization on the
general regression task by generating pseudo-labels. However, these methods
heavily rely on the quality of pseudo-labels, and direct regression fails to
learn the label distribution and can easily lead to overfitting. To address
these challenges, we introduce an end-to-end Decoupled Representation
distillation framework (DRILL) which is specially designed for the
semi-supervised regression task where we transform the general regression task
into a Discrete Distribution Estimation (DDE) task over multiple buckets to
better capture the underlying label distribution and mitigate the risk of
overfitting associated with direct regression. Then we employ the Decoupled
Distribution Alignment (DDA) to align the target bucket and non-target bucket
between teacher and student on the distribution of buckets, encouraging the
student to learn more robust and generalized knowledge from the teacher.
Extensive experiments conducted on datasets from diverse domains demonstrate
that the proposed DRILL has strong generalization and outperforms the competing
methods.

</details>


### [17] [Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data](https://arxiv.org/abs/2508.14769)
*Ahmed Mujtaba,Gleb Radchenko,Radu Prodan,Marc Masana*

Main category: cs.LG

TL;DR: 提出EdgeFD方法，通过KMeans密度比估计器简化客户端数据处理，无需服务器端过滤，在资源受限设备上实现高效联邦蒸馏


<details>
  <summary>Details</summary>
Motivation: 现有联邦蒸馏方法需要复杂的知识共享策略和昂贵的统计密度比估计，服务器端过滤引入延迟，需要更高效、轻量级的解决方案

Method: EdgeFD方法使用基于KMeans的密度比估计器有效过滤客户端的内分布和外分布代理数据，无需预训练教师模型和服务器端过滤

Result: 在多种数据分布场景下（强非IID、弱非IID、IID）均优于现有方法，准确率接近IID场景，计算开销显著降低

Conclusion: EdgeFD提高了联邦蒸馏的可扩展性和实际应用性，特别适合资源受限的边缘设备部署

Abstract: Federated distillation has emerged as a promising collaborative machine
learning approach, offering enhanced privacy protection and reduced
communication compared to traditional federated learning by exchanging model
outputs (soft logits) rather than full model parameters. However, existing
methods employ complex selective knowledge-sharing strategies that require
clients to identify in-distribution proxy data through computationally
expensive statistical density ratio estimators. Additionally, server-side
filtering of ambiguous knowledge introduces latency to the process. To address
these challenges, we propose a robust, resource-efficient EdgeFD method that
reduces the complexity of the client-side density ratio estimation and removes
the need for server-side filtering. EdgeFD introduces an efficient KMeans-based
density ratio estimator for effectively filtering both in-distribution and
out-of-distribution proxy data on clients, significantly improving the quality
of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,
including strong non-IID, weak non-IID, and IID data distributions on clients,
without requiring a pre-trained teacher model on the server for knowledge
distillation. Experimental results demonstrate that EdgeFD outperforms
state-of-the-art methods, consistently achieving accuracy levels close to IID
scenarios even under heterogeneous and challenging conditions. The
significantly reduced computational overhead of the KMeans-based estimator is
suitable for deployment on resource-constrained edge devices, thereby enhancing
the scalability and real-world applicability of federated distillation. The
code is available online for reproducibility.

</details>


### [18] [GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values](https://arxiv.org/abs/2508.14083)
*Songyu Ke,Chenyu Wu,Yuxuan Liang,Xiuwen Yi,Yanping Sun,Junbo Zhang,Yu Zheng*

Main category: cs.LG

TL;DR: 提出了一种基于对比自学习的时空数据框架CST，用于从低质量数据中准确推断POI人群流量，解决了标注数据稀缺、复杂时空依赖和GPS报告关联等挑战。


<details>
  <summary>Details</summary>
Motivation: 由于城市传感技术的限制，大多数数据源的质量不足以监测每个POI的人群流量，需要从低质量数据中准确推断人群流量，但面临标注数据稀缺、复杂时空依赖和多重相关性三大挑战。

Method: 将人群流量推断问题重新定义为自监督属性图表示学习任务，构建基于POI和距离的空间邻接图，采用对比学习技术利用大量未标注时空数据，使用交换预测方法从相似实例预测目标子图表示，最后用准确人群流量数据进行微调。

Result: 在两个真实世界数据集上的实验表明，基于大量噪声数据预训练的模型始终优于从头训练的模型。

Conclusion: 提出的对比自学习框架CST能够有效利用未标注时空数据，通过预训练和微调相结合的方式，显著提升了POI人群流量推断的准确性。

Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.

</details>


### [19] [Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure](https://arxiv.org/abs/2508.14085)
*Hanseul Kang,Shervin Karimkashi,Ville Vuorinen*

Main category: cs.LG

TL;DR: 提出一个可扩展的参数感知稀疏回归框架，用于从多参数模拟数据中发现可解释的偏微分方程和亚网格尺度闭合模型。通过四项创新技术改进SINDy方法，成功从数据中自主发现Smagorinsky型闭合结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统SINDy方法在处理多参数数据和物理单位一致性方面的局限性，为湍流建模提供数据驱动的闭合模型发现方法。

Method: 采用符号参数化、维度相似性过滤、内存高效的Gram矩阵累积和集成共识系数稳定性分析四项创新技术，构建参数感知的稀疏回归框架。

Result: 在一维基准测试中可靠恢复控制方程，在过滤Burgers数据集中发现SGS闭合模型τ_SGS=0.1603·Δ²(∂ū/∂x)²，对应Smagorinsky常数约0.4004，R²达到0.886。

Conclusion: 该框架能够从数据中识别物理意义明确的亚网格尺度形式并校准系数，为现有湍流建模方法提供了补充性的数据驱动方法。

Abstract: We present a scalable, parameter-aware sparse regression framework for
discovering interpretable partial differential equations and subgrid-scale
closures from multi-parameter simulation data. Building on SINDy (Sparse
Identification of Nonlinear Dynamics), our approach addresses key limitations
through four innovations: symbolic parameterisation enabling physical
parameters to vary within unified regression; Dimensional Similarity Filter
enforcing unit-consistency whilst reducing candidate libraries;
memory-efficient Gram-matrix accumulation enabling batch processing; and
ensemble consensus with coefficient stability analysis for robust model
identification.
  Validation on canonical one-dimensional benchmarks demonstrates reliable
recovery of governing equations across parameter ranges. Applied to filtered
Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} =
0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$,
corresponding to a Smagorinsky constant of approximately 0.4004. This
represents autonomous discovery of Smagorinsky-type closure structure from data
without prior theoretical assumptions.
  The discovered model achieves $R^2 = 0.886$ across filter scales and
demonstrates improved prediction accuracy compared to classical closures. The
framework's ability to identify physically meaningful SGS forms and calibrate
coefficients offers a complementary approach to existing turbulence modelling
methods, contributing to the growing field of data-driven closure discovery.

</details>


### [20] [EEGDM: EEG Representation Learning via Generative Diffusion Model](https://arxiv.org/abs/2508.14086)
*Jia Hong Puah,Sim Kuan Goh,Ziwei Zhang,Zixuan Ye,Chow Khuen Chan,Kheng Seang Lim,Si Lei Fong,Kok Sin Woon*

Main category: cs.LG

TL;DR: EEGDM框架使用生成扩散模型和结构化状态空间模型进行EEG表示学习，在保持轻量化的同时超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决EEG基础模型计算成本高、性能提升有限的问题，寻求更高效的EEG表示学习方法

Method: 开发结构化状态空间扩散预训练模型(SSMDP)捕捉EEG时序动态，使用去噪扩散概率模型训练，通过潜在融合变换器(LFT)进行下游分类

Result: 在Temple大学EEG事件语料库上表现优于现有方法，模型轻量化约19倍

Conclusion: EEGDM为当前基础模型提供了有前景的替代方案，在性能和效率方面都有显著优势

Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the
brain and diagnosing neurological disorders (e.g., epilepsy), learning
meaningful representations from raw EEG signals remains challenging due to
limited annotations and high signal variability. Recently, EEG foundation
models (FMs) have shown promising potential by adopting transformer
architectures and self-supervised pre-training methods from large language
models (e.g., masked prediction) to learn representations from diverse EEG
data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large
models often incurred high computational costs during both training and
inference, with only marginal performance improvements as model size increases.
In this work, we proposed EEG representation learning framework building upon
Generative Diffusion Model (EEGDM). Specifically, we developed structured
state-space model for diffusion pretraining (SSMDP) to better capture the
temporal dynamics of EEG signals and trained the architecture using a Denoising
Diffusion Probabilistic Model. The resulting latent EEG representations were
then used for downstream classification tasks via our proposed latent fusion
transformer (LFT). To evaluate our method, we used the multi-event Temple
University EEG Event Corpus and compared EEGDM with current state-of-the-art
approaches, including EEG FMs. Empirical results showed that our method
outperformed existing methods while being approximately 19x more lightweight.
These findings suggested that EEGDM offered a promising alternative to current
FMs. Our code is available at: https://github.com/jhpuah/EEGDM.

</details>


### [21] [FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics](https://arxiv.org/abs/2508.14087)
*David Park,Shuhang Li,Yi Huang,Xihaier Luo,Haiwang Yu,Yeonju Go,Christopher Pinkenburg,Yuewei Lin,Shinjae Yoo,Joseph Osborn,Jin Huang,Yihui Ren*

Main category: cs.LG

TL;DR: 该研究开发了一个用于粒子物理实验的科学基础模型，通过自监督学习处理稀疏的探测器数据，在多个下游任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自监督学习范式启发了科学基础模型的发展，但粒子物理实验数据具有稀疏、空间分布的特性，与自然语言差异很大，需要专门的方法来处理。

Method: 提出了新的自监督训练方法处理探测器数据，构建了包含1100万粒子碰撞事件的数据集，使用冻结权重和任务特定适配器的架构，模型参数达到1.88亿。

Result: 该基础模型在所有下游任务中均优于基线模型，表现出强大的数据效率适应性，提取的表征具有任务无关性但可通过线性映射专门化。

Conclusion: 证明了粒子物理基础模型的可扩展性和泛化能力，为科学领域的基础模型开发提供了重要参考。

Abstract: Large language models have revolutionized artificial intelligence by enabling
large, generalizable models trained through self-supervision. This paradigm has
inspired the development of scientific foundation models (FMs). However,
applying this capability to experimental particle physics is challenging due to
the sparse, spatially distributed nature of detector data, which differs
dramatically from natural language. This work addresses if an FM for particle
physics can scale and generalize across diverse tasks. We introduce a new
dataset with more than 11 million particle collision events and a suite of
downstream tasks and labeled data for evaluation. We propose a novel
self-supervised training method for detector data and demonstrate its neural
scalability with models that feature up to 188 million parameters. With frozen
weights and task-specific adapters, this FM consistently outperforms baseline
models across all downstream tasks. The performance also exhibits robust
data-efficient adaptation. Further analysis reveals that the representations
extracted by the FM are task-agnostic but can be specialized via a single
linear mapping for different downstream tasks.

</details>


### [22] [CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection](https://arxiv.org/abs/2508.14088)
*Haomin Wen,Shurui Cao,Leman Akoglu*

Main category: cs.LG

TL;DR: CoBAD是一个用于检测人类移动集体异常的新型模型，通过两阶段注意力机制和预训练方法，能够有效识别意外共现和缺席异常，在多个指标上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法主要关注个体移动模式，而集体异常检测需要建模个体间的时空依赖关系，这是一个未被充分探索的挑战领域。

Method: 提出CoBAD模型，使用集体事件序列和共现事件图表示问题，采用两阶段注意力机制建模个体移动模式和跨个体交互，通过掩码事件和链接重建任务进行预训练。

Result: 在大规模移动数据集上的实验表明，CoBAD在AUCROC指标上提升13%-18%，在AUCPR指标上提升19%-70%，显著优于现有基线方法。

Conclusion: CoBAD成功解决了集体异常检测问题，特别是能够检测被先前工作忽视的缺席异常，为公共安全和城市规划应用提供了有效工具。

Abstract: Detecting anomalies in human mobility is essential for applications such as
public safety and urban planning. While traditional anomaly detection methods
primarily focus on individual movement patterns (e.g., a child should stay at
home at night), collective anomaly detection aims to identify irregularities in
collective mobility behaviors across individuals (e.g., a child is at home
alone while the parents are elsewhere) and remains an underexplored challenge.
Unlike individual anomalies, collective anomalies require modeling
spatiotemporal dependencies between individuals, introducing additional
complexity. To address this gap, we propose CoBAD, a novel model designed to
capture Collective Behaviors for human mobility Anomaly Detection. We first
formulate the problem as unsupervised learning over Collective Event Sequences
(CES) with a co-occurrence event graph, where CES represents the event
sequences of related individuals. CoBAD then employs a two-stage attention
mechanism to model both the individual mobility patterns and the interactions
across multiple individuals. Pre-trained on large-scale collective behavior
data through masked event and link reconstruction tasks, CoBAD is able to
detect two types of collective anomalies: unexpected co-occurrence anomalies
and absence anomalies, the latter of which has been largely overlooked in prior
work. Extensive experiments on large-scale mobility datasets demonstrate that
CoBAD significantly outperforms existing anomaly detection baselines, achieving
an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is
available at https://github.com/wenhaomin/CoBAD.

</details>


### [23] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: 该论文提出了一种从使用评分函数的图神经网络(GNN)中提取可解释Datalog规则的方法，通过使GNN和评分函数具有单调性来保证规则提取的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN可解释性方法主要针对低表达能力的图编码/解码方法，无法处理使用评分函数进行链接预测的通用方法，需要开发新的可解释性技术。

Method: 通过使GNN和评分函数具有单调性，利用单调性提取可靠的解释规则，并为特定类别的单调GNN与评分函数定义等效Datalog程序的生成过程。

Result: 实验表明，在链接预测基准测试中，单调GNN和评分函数在实践中表现良好，并能产生大量可靠的规则。

Conclusion: 该方法成功地将可解释性扩展到使用评分函数的通用链接预测方法，为GNN提供了更好的可解释性保证。

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [24] [Physics-Informed Reward Machines](https://arxiv.org/abs/2508.14093)
*Daniel Ajeleye,Ashutosh Trivedi,Majid Zamani*

Main category: cs.LG

TL;DR: 该论文提出了物理信息奖励机(pRMs)，这是一种符号机器，用于表达强化学习中的复杂学习目标和奖励结构，通过反事实经验生成和奖励塑造技术显著提高了学习效率。


<details>
  <summary>Details</summary>
Motivation: 奖励机(RMs)虽然能够指定非马尔可夫奖励并提高强化学习的表达性和可编程性，但需要进一步改进以更好地处理物理环境中的复杂学习目标。

Method: 引入物理信息奖励机(pRMs)，开发能够利用pRMs的强化学习算法，通过反事实经验生成和奖励塑造技术来加速学习过程。

Result: 实验结果表明，在有限和连续物理环境中，pRMs技术显著加速了训练阶段的奖励获取，提高了多个控制任务的学习效率。

Conclusion: 物理信息奖励机(pRMs)为强化学习提供了更可编程、表达性更强和更高效的学习框架，特别是在物理环境任务中表现出显著优势。

Abstract: Reward machines (RMs) provide a structured way to specify non-Markovian
rewards in reinforcement learning (RL), thereby improving both expressiveness
and programmability. Viewed more broadly, they separate what is known about the
environment, captured by the reward mechanism, from what remains unknown and
must be discovered through sampling. This separation supports techniques such
as counterfactual experience generation and reward shaping, which reduce sample
complexity and speed up learning. We introduce physics-informed reward machines
(pRMs), a symbolic machine designed to express complex learning objectives and
reward structures for RL agents, thereby enabling more programmable,
expressive, and efficient learning. We present RL algorithms capable of
exploiting pRMs via counterfactual experiences and reward shaping. Our
experimental results show that these techniques accelerate reward acquisition
during the training phases of RL. We demonstrate the expressiveness and
effectiveness of pRMs through experiments in both finite and continuous
physical environments, illustrating that incorporating pRMs significantly
improves learning efficiency across several control tasks.

</details>


### [25] [Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets](https://arxiv.org/abs/2508.14094)
*Benjamin Pikus,Pratyush Ranjan Tiwari,Burton Ye*

Main category: cs.LG

TL;DR: 在资源受限的语言模型对齐中，优先选择难度较高的训练示例可以带来47%的性能提升，而简单示例的改善效果最小。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据收集成本昂贵，实践中的预算限制了可获取数据的量。需要研究在固定预算下，应优先选择哪类难度级别的示例来优化模型对齐效果。

Method: 采用Group Relative Policy Optimization (GRPO)精调方法，在不同模型规模和家族上比较四种子集选择策略（简单、中等、困难、随机难度），使用基础模型通过多样本评估获得难度估计。

Result: 训练最困难的示例导致最大的性能提升（最高达47%），而训练简单示例的改善效果最小。分析显示困难示例在GRPO训练中提供了更多的可学习机会。

Conclusion: 这些发现为预算受限的后训练提供了实用指南：在使用GRPO时，优先选择困难示例可在理论任务上带来实质性的性能提升。

Abstract: Collecting high-quality training examples for language model fine-tuning is
expensive, with practical budgets limiting the amount of data that can be
procured. We investigate a critical question for resource-constrained
alignment: under a fixed acquisition budget, should practitioners prioritize
examples that are easy, medium, hard, or of random difficulty? We study Group
Relative Policy Optimization (GRPO) fine-tuning across different model sizes
and families, comparing four subset selection policies chosen from the same
unlabeled pool using base-model difficulty estimates obtained via multi-sample
evaluation. Our experiments reveal that training on the hardest examples yields
the largest performance gains, up to 47%, while training on easy examples yield
the smallest gains. Analysis reveals that this effect arises from harder
examples providing more learnable opportunities during GRPO training. These
findings provide practical guidance for budget-constrained post-training:
prioritizing hard examples yields substantial performance gains on reasoning
tasks when using GRPO.

</details>


### [26] [Implicit Hypergraph Neural Network](https://arxiv.org/abs/2508.14101)
*Akash Choudhuri,Yongjian Zhong,Bijaya Adhikari*

Main category: cs.LG

TL;DR: 提出了隐式超图神经网络(IHNN)来解决现有超图神经网络在捕获长距离依赖时性能下降的问题，通过联合学习节点和超边的固定点表示来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络通常只进行少量消息传递轮次，只能捕获局部信息而忽略了长距离高阶依赖关系，但盲目增加消息传递轮次反而会降低性能。

Method: 提出IHNN框架，通过隐式微分联合学习节点和超边的固定点表示，使用可处理的投影梯度下降方法进行高效训练。

Result: 在真实世界超图的节点分类任务上，IHNN在大多数设置下优于现有最佳方法，建立了超图学习的新state-of-the-art。

Conclusion: IHNN通过隐式学习方法有效解决了超图神经网络的长距离依赖捕获问题，为超图学习提供了新的有效框架。

Abstract: Hypergraphs offer a generalized framework for capturing high-order
relationships between entities and have been widely applied in various domains,
including healthcare, social networks, and bioinformatics. Hypergraph neural
networks, which rely on message-passing between nodes over hyperedges to learn
latent representations, have emerged as the method of choice for predictive
tasks in many of these domains. These approaches typically perform only a small
number of message-passing rounds to learn the representations, which they then
utilize for predictions. The small number of message-passing rounds comes at a
cost, as the representations only capture local information and forego
long-range high-order dependencies. However, as we demonstrate, blindly
increasing the message-passing rounds to capture long-range dependency also
degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture
long-range dependencies in standard graphs while maintaining performance.
Despite their popularity, prior work has not studied long-range dependency
issues on hypergraph neural networks. Here, we first demonstrate that existing
hypergraph neural networks lose predictive power when aggregating more
information to capture long-range dependency. We then propose Implicit
Hypergraph Neural Network (IHNN), a novel framework that jointly learns
fixed-point representations for both nodes and hyperedges in an end-to-end
manner to alleviate this issue. Leveraging implicit differentiation, we
introduce a tractable projected gradient descent approach to train the model
efficiently. Extensive experiments on real-world hypergraphs for node
classification demonstrate that IHNN outperforms the closest prior works in
most settings, establishing a new state-of-the-art in hypergraph learning.

</details>


### [27] [Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces](https://arxiv.org/abs/2508.14102)
*Thomas Gallien*

Main category: cs.LG

TL;DR: 该论文分析了基于信任域的强化学习算法（TRPO和PPO）在不同动作空间维度下的优化行为，特别关注形态变化对优化景观的影响，并通过Swimmer环境进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和可重用控制策略需求的增长，形态泛化能力变得重要。虽然基于图的策略架构能够处理不同运动学结构，但信任域方法在变化动作空间维度下的行为尚未得到充分理解。

Method: 对TRPO和PPO进行理论分析，研究KL散度和策略裁剪惩罚约束下动作空间维度变化对优化景观的影响，并在Gymnasium Swimmer环境中进行实证评估。

Result: 论文提供了关于信任域方法在形态变化下优化行为的理论见解，并通过受控实验环境验证了这些发现。

Conclusion: 该研究为理解信任域优化方法在形态泛化场景中的行为提供了重要理论基础和实证支持，有助于开发更具通用性的控制策略。

Abstract: Trust region-based optimization methods have become foundational
reinforcement learning algorithms that offer stability and strong empirical
performance in continuous control tasks. Growing interest in scalable and
reusable control policies translate also in a demand for morphological
generalization, the ability of control policies to cope with different
kinematic structures. Graph-based policy architectures provide a natural and
effective mechanism to encode such structural differences. However, while these
architectures accommodate variable morphologies, the behavior of trust region
methods under varying action space dimensionality remains poorly understood. To
this end, we conduct a theoretical analysis of trust region-based policy
optimization methods, focusing on both Trust Region Policy Optimization (TRPO)
and its widely used first-order approximation, Proximal Policy Optimization
(PPO). The goal is to demonstrate how varying action space dimensionality
influence the optimization landscape, particularly under the constraints
imposed by KL-divergence or policy clipping penalties. Complementing the
theoretical insights, an empirical evaluation under morphological variation is
carried out using the Gymnasium Swimmer environment. This benchmark offers a
systematically controlled setting for varying the kinematic structure without
altering the underlying task, making it particularly well-suited to study
morphological generalization.

</details>


### [28] [From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery](https://arxiv.org/abs/2508.14111)
*Jiaqi Wei,Yuejin Yang,Xiang Zhang,Yuhan Chen,Xiang Zhuang,Zhangyang Gao,Dongzhan Zhou,Guangshuai Wang,Zhiqiang Gao,Juntai Cao,Zijie Qiu,Xuming He,Qiang Zhang,Chenyu You,Shuangjia Zheng,Ning Ding,Wanli Ouyang,Nanqing Dong,Yu Cheng,Siqi Sun,Lei Bai,Bowen Zhou*

Main category: cs.LG

TL;DR: 本文提出了"代理科学"作为AI for Science的新范式，AI系统从辅助工具发展为具有完整科学代理能力的自主研究伙伴，能够进行假设生成、实验设计、执行和分析等全流程科学研究。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在重塑科学发现，从专门的计算工具演变为自主研究伙伴。作者旨在将代理科学定位为AI for Science范式的关键阶段，统一之前碎片化的视角。

Method: 通过综合框架整合三种视角（过程导向、自主导向和机制导向），追溯AI for Science的演进，识别科学代理的五大核心能力，建立四阶段动态工作流模型，并跨领域综述应用。

Result: 建立了面向领域的自主科学发现综合框架，将代理科学定位为结构化范式，为AI驱动研究提供了系统化的理论基础和实践指导。

Conclusion: 代理科学代表了AI for Science的重要发展阶段，通过统一的框架和系统化的能力模型，为未来AI驱动的自主科学研究奠定了坚实基础，并指出了关键挑战和未来机遇。

Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from
specialized computational tools into autonomous research partners. We position
Agentic Science as a pivotal stage within the broader AI for Science paradigm,
where AI systems progress from partial assistance to full scientific agency.
Enabled by large language models (LLMs), multimodal systems, and integrated
research platforms, agentic AI shows capabilities in hypothesis generation,
experimental design, execution, analysis, and iterative refinement -- behaviors
once regarded as uniquely human. This survey provides a domain-oriented review
of autonomous scientific discovery across life sciences, chemistry, materials
science, and physics. We unify three previously fragmented perspectives --
process-oriented, autonomy-oriented, and mechanism-oriented -- through a
comprehensive framework that connects foundational capabilities, core
processes, and domain-specific realizations. Building on this framework, we (i)
trace the evolution of AI for Science, (ii) identify five core capabilities
underpinning scientific agency, (iii) model discovery as a dynamic four-stage
workflow, (iv) review applications across the above domains, and (v) synthesize
key challenges and future opportunities. This work establishes a
domain-oriented synthesis of autonomous scientific discovery and positions
Agentic Science as a structured paradigm for advancing AI-driven research.

</details>


### [29] [A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning](https://arxiv.org/abs/2508.14125)
*Madyan Bagosher,Tala Mustafa,Mohammad Alsmirat,Amal Al-Ali,Isam Mashhour Al Jawarneh*

Main category: cs.LG

TL;DR: 基于多源数据的智能停车预测框架，通过机器学习模型预测停车占位可用性，无需传感器装置，Random Forest模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 城市和大学校园停车难问题日益严重，学生需要快速找到空余停车位，现有停车空间有限，需要高效的停车位分配系统。

Method: 集成街道地图、移动性和气象数据，通过空间关联操作分析停车行为和车辆移动模式，使用线性回归、SVR、Random Forest回归和LSTM模型进行预测，采用网格搜索进行超参数调整。

Result: Random Forest回归模型表现最佳，RMSE为0.142，R2为0.582。LSTM模型在时间序列任务中可能更优，但需要更多数据和更长时间步长。

Conclusion: 提出的智能框架能够有效预测停车占位可用性，无需物理传感器，Random Forest模型在当前数据条件下表现最佳，LSTM模型在更长时间序列中有潜力。

Abstract: As urban populations continue to grow, cities face numerous challenges in
managing parking and determining occupancy. This issue is particularly
pronounced in university campuses, where students need to find vacant parking
spots quickly and conveniently during class timings. The limited availability
of parking spaces on campuses underscores the necessity of implementing
efficient systems to allocate vacant parking spots effectively. We propose a
smart framework that integrates multiple data sources, including street maps,
mobility, and meteorological data, through a spatial join operation to capture
parking behavior and vehicle movement patterns over the span of 3 consecutive
days with an hourly duration between 7AM till 3PM. The system will not require
any sensing tools to be installed in the street or in the parking area to
provide its services since all the data needed will be collected using location
services. The framework will use the expected parking entrance and time to
specify a suitable parking area. Several forecasting models, namely, Linear
Regression, Support Vector Regression (SVR), Random Forest Regression (RFR),
and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was
employed using grid search, and model performance is assessed using Root Mean
Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of
Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142
and highest R2 of 0.582. However, given the time-series nature of the task, an
LSTM model may perform better with additional data and longer timesteps.

</details>


### [30] [Comparison of derivative-free and gradient-based minimization for multi-objective compositional design of shape memory alloys](https://arxiv.org/abs/2508.14127)
*S. Josyula,Y. Noiman,E. J. Payton,T. Giovannelli*

Main category: cs.LG

TL;DR: 使用机器学习模型和优化算法来设计形状记忆合金，通过实验数据和物理特征训练树模型和神经网络，结合不同优化器寻找满足马氏体起始温度目标且成本最低的合金成分。


<details>
  <summary>Details</summary>
Motivation: 设计既满足性能目标又保持经济性和可持续性的形状记忆合金是一个复杂挑战，需要优化合金成分以达到特定马氏体起始温度同时最小化成本。

Method: 使用实验表征的合金数据集和物理特征训练两种机器学习模型（树集成和神经网络），分别与无导数优化器（COBYLA）和基于梯度的优化器（TRUST-CONSTR）配对进行数值优化。

Result: 两种模型预测精度相似，但基于神经网络的梯度优化器更稳定且能更一致地找到更好的解决方案，而无导数优化器在初始猜测远离目标时容易收敛到次优结果。

Conclusion: 研究展示了结合物理信息数据、机器学习模型和优化算法探索新形状记忆合金成分的实用方法，该方法可扩展到其他需要有限数据下进行设计权衡的材料领域。

Abstract: Designing shape memory alloys (SMAs) that meet performance targets while
remaining affordable and sustainable is a complex challenge. In this work, we
focus on optimizing SMA compositions to achieve a desired martensitic start
temperature (Ms) while minimizing cost. To do this, we use machine learning
models as surrogate predictors and apply numerical optimization methods to
search for suitable alloy combinations. We trained two types of machine
learning models, a tree-based ensemble and a neural network, using a dataset of
experimentally characterized alloys and physics-informed features. The
tree-based model was used with a derivative-free optimizer (COBYLA), while the
neural network, which provides gradient information, was paired with a
gradient-based optimizer (TRUST-CONSTR). Our results show that while both
models predict Ms with similar accuracy, the optimizer paired with the neural
network finds better solutions more consistently. COBYLA often converged to
suboptimal results, especially when the starting guess was far from the target.
The TRUST-CONSTR method showed more stable behavior and was better at reaching
alloy compositions that met both objectives. This study demonstrates a
practical approach to exploring new SMA compositions by combining
physics-informed data, machine learning models, and optimization algorithms.
Although the scale of our dataset is smaller than simulation-based efforts, the
use of experimental data improves the reliability of the predictions. The
approach can be extended to other materials where design trade-offs must be
made with limited data.

</details>


### [31] [ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification](https://arxiv.org/abs/2508.14134)
*Xin Wu,Fei Teng,Ji Zhang,Xingwang Li,Yuxuan Liang*

Main category: cs.LG

TL;DR: ERIS框架通过能量引导校准、权重正交性和对抗训练机制，实现有指导的特征解耦，在时间序列分类的OOD数据上平均提升4.04%准确率


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类方法在分布外数据上表现不佳，主要原因是模型将领域特定特征和标签相关特征纠缠在一起，形成伪相关性，而现有特征解耦方法缺乏语义指导

Method: 提出ERIS端到端框架：1）能量引导校准机制提供语义指导；2）权重级正交性策略强制结构独立性；3）辅助对抗训练机制注入结构化扰动增强鲁棒性

Result: 在四个基准测试上，ERIS相比最先进基线方法平均提升4.04%的准确率

Conclusion: 有效的特征解耦不仅需要数学约束，还需要语义指导来锚定分离过程，ERIS框架通过三个关键机制实现了有指导的可靠特征解耦

Abstract: An ideal time series classification (TSC) should be able to capture invariant
representations, but achieving reliable performance on out-of-distribution
(OOD) data remains a core obstacle. This obstacle arises from the way models
inherently entangle domain-specific and label-relevant features, resulting in
spurious correlations. While feature disentanglement aims to solve this,
current methods are largely unguided, lacking the semantic direction required
to isolate truly universal features. To address this, we propose an end-to-end
Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework
to enable guided and reliable feature disentanglement. The core idea is that
effective disentanglement requires not only mathematical constraints but also
semantic guidance to anchor the separation process. ERIS incorporates three key
mechanisms to achieve this goal. Specifically, we first introduce an
energy-guided calibration mechanism, which provides crucial semantic guidance
for the separation, enabling the model to self-calibrate. Additionally, a
weight-level orthogonality strategy enforces structural independence between
domain-specific and label-relevant features, thereby mitigating their
interference. Moreover, an auxiliary adversarial training mechanism enhances
robustness by injecting structured perturbations. Experiments demonstrate that
ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy
across four benchmarks.

</details>


### [32] [Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach](https://arxiv.org/abs/2508.14135)
*Collins O. Ogbodo,Timothy J. Rogers,Mattia Dal Borgo,David J. Wagg*

Main category: cs.LG

TL;DR: 本文提出了一个基于代理的自适应传感器布局框架，用于动态变化的模态测试环境，通过强化学习优化传感器位置配置。


<details>
  <summary>Details</summary>
Motivation: 传统模态测试设计方法过于静态和刚性，无法适应测试环境的变化，导致测试精度和适应性受限。需要开发能够动态调整传感器配置的智能决策支持系统。

Method: 采用基于代理的决策支持框架，将问题建模为部分可观察马尔可夫决策过程，通过双课程学习策略训练通用强化学习代理。

Result: 在钢悬臂结构上的案例研究表明，该方法能够有效优化不同频率段的传感器位置，验证了其在实验环境中的鲁棒性和实际应用性。

Conclusion: 所提出的自适应传感器布局框架能够显著提高模态测试的准确性和适应性，为工程实践提供了有效的决策支持工具。

Abstract: Modal testing plays a critical role in structural analysis by providing
essential insights into dynamic behaviour across a wide range of engineering
industries. In practice, designing an effective modal test campaign involves
complex experimental planning, comprising a series of interdependent decisions
that significantly influence the final test outcome. Traditional approaches to
test design are typically static-focusing only on global tests without
accounting for evolving test campaign parameters or the impact of such changes
on previously established decisions, such as sensor configurations, which have
been found to significantly influence test outcomes. These rigid methodologies
often compromise test accuracy and adaptability. To address these limitations,
this study introduces an agent-based decision support framework for adaptive
sensor placement across dynamically changing modal test environments. The
framework formulates the problem using an underspecified partially observable
Markov decision process, enabling the training of a generalist reinforcement
learning agent through a dual-curriculum learning strategy. A detailed case
study on a steel cantilever structure demonstrates the efficacy of the proposed
method in optimising sensor locations across frequency segments, validating its
robustness and real-world applicability in experimental settings.

</details>


### [33] [Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data](https://arxiv.org/abs/2508.14136)
*Leonardo Aldo Alejandro Barberi,Linda Maria De Cave*

Main category: cs.LG

TL;DR: 本文介绍使用拓扑数据分析(TDA)技术进行银行数据异常检测和客户分群的先进方法，通过Mapper算法和持久同调实现无监督模式发现


<details>
  <summary>Details</summary>
Motivation: 将抽象的拓扑数学理论与实际工业应用相结合，为银行业提供可操作的数据分析洞察，解决无监督异常检测和客户分群的实际问题

Method: 采用拓扑数据分析(TDA)技术，特别是Mapper算法和持久同调方法，开发无监督程序来分析客户银行数据中的拓扑信息

Result: 开发出的框架能够发现客户银行数据中有意义的模式，将拓扑数学理论与实际工业用例有效结合

Conclusion: 拓扑数据分析方法在银行数据挖掘中具有实用价值，能够为行业提供有价值的无监督学习解决方案

Abstract: This paper introduces advanced techniques of Topological Data Analysis (TDA)
for unsupervised anomaly detection and customer segmentation in banking data.
Using the Mapper algorithm and persistent homology, we develop unsupervised
procedures that uncover meaningful patterns in customers' banking data by
exploiting topological information. The framework we present in this paper
yields actionable insights that combine the abstract mathematical subject of
topology with real-life use cases that are useful in industry.

</details>


### [34] [Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques](https://arxiv.org/abs/2508.14137)
*Amalie Roark,Serio Agriesti,Francisco Camara Pereira,Guido Cantelmo*

Main category: cs.LG

TL;DR: 提出基于元学习的框架，利用多城市数据训练模型来估计交通网络宏观基本图(MFD)，解决检测器数据不足的问题，显著提升流量预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统MFD估计需要大量环形检测器数据，但实际应用中往往数据稀缺。需要一种能够跨城市泛化、在有限检测器条件下仍能准确估计MFD的方法。

Method: 采用元学习框架，结合多任务物理信息神经网络(MT-PINN)，利用多个城市的数据进行训练，使模型能够适应不同检测器配置和拓扑结构的新城市。

Result: 在流量预测方面实现了平均MSE提升约17500-36000（取决于检测器子集），成功实现了跨不同城市环境的泛化，在数据有限的城市表现优异。

Conclusion: 元学习框架有效解决了MFD估计中的数据稀缺问题，相比传统迁移学习方法表现更好，证明了在有限检测器条件下使用元学习的潜力。

Abstract: The Macroscopic Fundamental Diagram is a popular tool used to describe
traffic dynamics in an aggregated way, with applications ranging from traffic
control to incident analysis. However, estimating the MFD for a given network
requires large numbers of loop detectors, which is not always available in
practice. This article proposes a framework harnessing meta-learning, a
subcategory of machine learning that trains models to understand and adapt to
new tasks on their own, to alleviate the data scarcity challenge. The developed
model is trained and tested by leveraging data from multiple cities and
exploiting it to model the MFD of other cities with different shares of
detectors and topological structures. The proposed meta-learning framework is
applied to an ad-hoc Multi-Task Physics-Informed Neural Network, specifically
designed to estimate the MFD. Results show an average MSE improvement in flow
prediction ranging between ~ 17500 and 36000 (depending on the subset of loop
detectors tested). The meta-learning framework thus successfully generalizes
across diverse urban settings and improves performance on cities with limited
data, demonstrating the potential of using meta-learning when a limited number
of detectors is available. Finally, the proposed framework is validated against
traditional transfer learning approaches and tested with FitFun, a
non-parametric model from the literature, to prove its transferability.

</details>


### [35] [STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers](https://arxiv.org/abs/2508.14138)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Brent ByungHoon Kang,Hyeongboo Baek*

Main category: cs.LG

TL;DR: STAS框架通过时空自适应计算时间，解决了SNN视觉Transformer的高延迟和计算开销问题，在降低能耗的同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNN)虽然比人工神经网络(ANN)更节能，但由于多时间步操作特性导致高延迟和计算开销。现有的动态计算方法分散且不统一，需要一种综合解决方案

Method: 提出STAS框架，包含集成脉冲补丁分割(I-SPS)模块建立时间稳定性，以及自适应脉冲自注意力(A-SSA)模块进行时空二维token剪枝

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上分别减少能耗45.9%、43.8%和30.1%，同时准确率超过最先进模型

Conclusion: STAS通过协同设计静态架构和动态计算策略，成功解决了SNN视觉Transformer的时空冗余问题，实现了能耗和准确率的双重优化

Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural
networks (ANNs) but suffer from high latency and computational overhead due to
their multi-timestep operational nature. While various dynamic computation
methods have been developed to mitigate this by targeting spatial, temporal, or
architecture-specific redundancies, they remain fragmented. While the
principles of adaptive computation time (ACT) offer a robust foundation for a
unified approach, its application to SNN-based vision Transformers (ViTs) is
hindered by two core issues: the violation of its temporal similarity
prerequisite and a static architecture fundamentally unsuited for its
principles. To address these challenges, we propose STAS (Spatio-Temporal
Adaptive computation time for Spiking transformers), a framework that
co-designs the static architecture and dynamic computation policy. STAS
introduces an integrated spike patch splitting (I-SPS) module to establish
temporal stability by creating a unified input representation, thereby solving
the architectural problem of temporal dissimilarity. This stability, in turn,
allows our adaptive spiking self-attention (A-SSA) module to perform
two-dimensional token pruning across both spatial and temporal axes.
Implemented on spiking Transformer architectures and validated on CIFAR-10,
CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%,
and 30.1%, respectively, while simultaneously improving accuracy over SOTA
models.

</details>


### [36] [Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs](https://arxiv.org/abs/2508.14140)
*Orestis Konstantaropoulos,Stelios Manolis Smirnakis,Maria Papadopouli*

Main category: cs.LG

TL;DR: G2GNet是一种受生物神经网络启发的稀疏模块化架构，通过模仿小鼠视觉皮层的功能连接模式，在保持高精度的同时实现高达75%的稀疏度，显著减少计算量和参数数量。


<details>
  <summary>Details</summary>
Motivation: 受生物神经网络模块化、层次化和稀疏连接结构的启发，探索如何将小鼠视觉皮层的功能连接模式应用于人工神经网络设计，以在计算效率和性能之间取得更好平衡。

Method: 提出G2GNet架构，在前馈层间施加稀疏模块化连接；结合动态稀疏训练机制，在训练过程中修剪和重新生长连接；采用基于激活相关性的Hebbian启发式重连规则。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100等标准视觉基准测试中，G2GNet以显著更少的参数实现了比全连接模型更高的准确率（提升达4.3%），同时达到75%的稀疏度。

Conclusion: 这是首个将生物观测到的功能连接模式作为结构偏置融入ANN设计的架构，证明了生物启发的稀疏连接模式在提高神经网络效率和性能方面的有效性。

Abstract: The structure of biological neural circuits-modular, hierarchical, and
sparsely interconnected-reflects an efficient trade-off between wiring cost,
functional specialization, and robustness. These principles offer valuable
insights for artificial neural network (ANN) design, especially as networks
grow in depth and scale. Sparsity, in particular, has been widely explored for
reducing memory and computation, improving speed, and enhancing generalization.
Motivated by systems neuroscience findings, we explore how patterns of
functional connectivity in the mouse visual cortex-specifically,
ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet,
a novel architecture that imposes sparse, modular connectivity across
feedforward layers. Despite having significantly fewer parameters than fully
connected models, G2GNet achieves superior accuracy on standard vision
benchmarks. To our knowledge, this is the first architecture to incorporate
biologically observed functional connectivity patterns as a structural bias in
ANN design. We complement this static bias with a dynamic sparse training (DST)
mechanism that prunes and regrows edges during training. We also propose a
Hebbian-inspired rewiring rule based on activation correlations, drawing on
principles of biological plasticity. G2GNet achieves up to 75% sparsity while
improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST,
CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer
computations.

</details>


### [37] [Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation](https://arxiv.org/abs/2508.14143)
*Xin Li*

Main category: cs.LG

TL;DR: MAI框架将智能建模为基于记忆循环的推理而非梯度下降，通过结构重用实现能量高效推理，为通用智能提供统一理论基础


<details>
  <summary>Details</summary>
Motivation: 智能本质是非遍历的，需要结构化重用先验推理轨迹而非从头优化，解决现代AI的计算瓶颈问题

Method: 基于delta同调理论，将每个皮层柱建模为循环一致记忆状态上的局部推理算子，建立记忆摊销推理框架

Result: 建立了MAI与强化学习的时间反转对偶关系：RL从奖励向前传播价值，MAI从记忆向后重建潜在原因

Conclusion: MAI为基于结构、重用和记忆的智能提供了统一的生物学基础理论，对实现AGI具有深远意义

Abstract: Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).

</details>


### [38] [Noise Robust One-Class Intrusion Detection on Dynamic Graphs](https://arxiv.org/abs/2508.14192)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出概率化TGN-SVDD模型，通过高斯分布预测处理网络入侵检测中的噪声数据，显著提升检测鲁棒性


<details>
  <summary>Details</summary>
Motivation: 网络入侵检测领域面临污染和噪声数据输入的鲁棒性挑战，需要改进现有模型在噪声环境下的检测性能

Method: 开发概率化TGN-SVDD模型，为每个网络事件预测高斯分布参数，自然处理噪声对抗样本

Result: 在添加合成噪声的CIC-IDS2017数据集上实验显示，相比基线TGN-SVDD模型检测性能显著提升，特别是在高噪声水平下

Conclusion: 概率化方法能有效增强网络入侵检测模型对噪声输入的鲁棒性，为实际部署提供更可靠的解决方案

Abstract: In the domain of network intrusion detection, robustness against contaminated
and noisy data inputs remains a critical challenge. This study introduces a
probabilistic version of the Temporal Graph Network Support Vector Data
Description (TGN-SVDD) model, designed to enhance detection accuracy in the
presence of input noise. By predicting parameters of a Gaussian distribution
for each network event, our model is able to naturally address noisy
adversarials and improve robustness compared to a baseline model. Our
experiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate
significant improvements in detection performance compared to the baseline
TGN-SVDD model, especially as noise levels increase.

</details>


### [39] [Reliability comparison of vessel trajectory prediction models via Probability of Detection](https://arxiv.org/abs/2508.14198)
*Zahra Rastin,Kathrin Donandt,Dirk Söffker*

Main category: cs.LG

TL;DR: 本文研究船舶轨迹预测模型，重点评估不同深度学习方法在各种交通复杂情况下的性能和可靠性，通过检测概率分析提供了更全面的可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 以往的船舶轨迹预测模型忽视了具体交通情况的复杂性，缺乏对模型可靠性的系统评估。本研究旨在填补这一空白，通过分析不同交通复杂场景下的模型表现，提供更全面的可靠性认知。

Method: 采用检测概率分析来定量模型在不同交通场景下的可靠性，超越了传统的错误分布分析。将测试样本根据预测过程中的交通情况进行分类，获得每个类别的性能指标和可靠性估计。

Result: 综合评估结果揭示了不同预测方法的优缺点，以及在不同预测过程长度下可以保证安全预测的可靠性范围。

Conclusion: 这些发现为开发更可靠的船舶轨迹预测方法提供了有价值的指导，有助于提高未来内河航道导航的安全性和效率。

Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on
the evaluation of different deep learning-based approaches. The objective is to
assess model performance in diverse traffic complexities and compare the
reliability of the approaches. While previous VTP models overlook the specific
traffic situation complexity and lack reliability assessments, this research
uses a probability of detection analysis to quantify model reliability in
varying traffic scenarios, thus going beyond common error distribution
analyses. All models are evaluated on test samples categorized according to
their traffic situation during the prediction horizon, with performance metrics
and reliability estimates obtained for each category. The results of this
comprehensive evaluation provide a deeper understanding of the strengths and
weaknesses of the different prediction approaches, along with their reliability
in terms of the prediction horizon lengths for which safe forecasts can be
guaranteed. These findings can inform the development of more reliable vessel
trajectory prediction approaches, enhancing safety and efficiency in future
inland waterways navigation.

</details>


### [40] [Graph Concept Bottleneck Models](https://arxiv.org/abs/2508.14255)
*Haotian Xu,Tsui-Wei Weng,Lam M. Nguyen,Tengfei Ma*

Main category: cs.LG

TL;DR: GraphCBMs是一种新的概念瓶颈模型变体，通过构建潜在概念图来捕捉概念间的相关性，解决了现有CBMs忽略概念间隐藏关系的问题，在保持可解释性的同时提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型(CBMs)假设概念在给定标签时条件独立且相互隔离，忽略了概念之间存在的隐藏关系。然而实际中概念往往是相关的，改变一个概念会自然影响其相关概念。

Method: 提出GraphCBMs，通过构建潜在概念图来促进概念关系，可以与现有CBMs结合使用，在保持可解释性的同时增强模型性能。

Result: 在真实世界图像分类任务上的实验表明：1）在图像分类任务中表现更优，同时提供更多概念结构信息用于可解释性；2）能够利用潜在概念图进行更有效的干预；3）在不同训练和架构设置下性能稳健。

Conclusion: GraphCBMs通过显式建模概念间的关系，有效解决了传统CBMs忽略概念相关性的局限性，在保持模型可解释性的同时显著提升了性能表现。

Abstract: Concept Bottleneck Models (CBMs) provide explicit interpretations for deep
neural networks through concepts and allow intervention with concepts to adjust
final predictions. Existing CBMs assume concepts are conditionally independent
given labels and isolated from each other, ignoring the hidden relationships
among concepts. However, the set of concepts in CBMs often has an intrinsic
structure where concepts are generally correlated: changing one concept will
inherently impact its related concepts. To mitigate this limitation, we propose
GraphCBMs: a new variant of CBM that facilitates concept relationships by
constructing latent concept graphs, which can be combined with CBMs to enhance
model performance while retaining their interpretability. Our experiment
results on real-world image classification tasks demonstrate Graph CBMs offer
the following benefits: (1) superior in image classification tasks while
providing more concept structure information for interpretability; (2) able to
utilize latent concept graphs for more effective interventions; and (3) robust
in performance across different training and architecture settings.

</details>


### [41] [Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2508.14285)
*Liyi Zhang,Jake Snell,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: 提出了ABMLL方法，一种计算高效的贝叶斯元学习框架，用于改进LoRA微调后大语言模型的泛化能力和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 现有方法如上下文提示优化和元学习虽然能提升泛化能力，但内存和计算成本高昂，需要长上下文提示或二阶梯度更新

Method: 基于摊销贝叶斯元学习，重新定义LoRA中的任务特定参数和全局参数，使用新超参数平衡重构精度和参数保真度

Result: 在Unified-QA和CrossFit数据集上测试，ABMLL在准确率和预期校准误差方面均优于现有方法，可扩展到Llama3-8B等大模型

Conclusion: ABMLL提供有效的泛化能力，计算效率高，并能通过贝叶斯框架改进不确定性量化

Abstract: Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a
cost-effective way to incorporate information from a specific dataset. However,
it is often unclear how well the fine-tuned LLM will generalize, i.e., how well
it will perform on unseen datasets. Methods have been proposed to improve
generalization by optimizing with in-context prompts, or by using meta-learning
to fine-tune LLMs. However, these methods are expensive in memory and
computation, requiring either long-context prompts or saving copies of
parameters and using second-order gradient updates. To address these
challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This
method builds on amortized Bayesian meta-learning for smaller models, adapting
this approach to LLMs while maintaining its computational efficiency. We
reframe task-specific and global parameters in the context of LoRA and use a
set of new hyperparameters to balance reconstruction accuracy and the fidelity
of task-specific parameters to the global ones. ABMLL provides effective
generalization and scales to large models such as Llama3-8B. Furthermore, as a
result of using a Bayesian framework, ABMLL provides improved uncertainty
quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that
it outperforms existing methods on these benchmarks in terms of both accuracy
and expected calibration error.

</details>


### [42] [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
*Amirmohsen Sattarifard,Sepehr Lavasani,Ehsan Imani,Kunlin Zhang,Hanlin Xu,Fengyu Sun,Negar Hassanpour,Chao Gao*

Main category: cs.LG

TL;DR: GLASS是一种无需训练的动态剪枝方法，通过结合局部激活和全局重要性统计，在边缘设备上高效部署大语言模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在边缘硬件上部署大语言模型需要动态剪枝来减少计算量，但现有方法要么固定稀疏模式，要么增加运行时开销，零样本方法在短提示/长生成场景中效果不佳。

Method: 提出A/I-GLASS方法，使用基于激活和影响的全局-局部神经元重要性聚合，通过排名聚合选择前馈网络单元，无需训练或辅助预测器。

Result: 在多个LLM和基准测试中，GLASS显著优于先前的无训练方法，特别是在具有挑战性的长文本生成场景中，且不增加推理开销。

Conclusion: GLASS提供了一种有效的训练无关动态剪枝方案，能够在保持质量的同时显著减少计算需求，特别适合边缘设备部署。

Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive,
prompt-aware dynamic pruning to reduce computation without degrading quality.
Static or predictor-based schemes either lock in a single sparsity pattern or
incur extra runtime overhead, and recent zero-shot methods that rely on
statistics from a single prompt fail on short prompt and/or long generation
scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local
neural importance Aggregation for feed-forward network SparSification, two
training-free methods that dynamically select FFN units using a
rank-aggregation of prompt local and model-intrinsic global neuron statistics.
Empirical results across multiple LLMs and benchmarks demonstrate that GLASS
significantly outperforms prior training-free methods, particularly in
challenging long-form generation scenarios, without relying on auxiliary
predictors or adding any inference overhead.

</details>


### [43] [Learning Time-Varying Convexifications of Multiple Fairness Measures](https://arxiv.org/abs/2508.14311)
*Quan Zhou,Jakub Marecek,Robert Shorten*

Main category: cs.LG

TL;DR: 论文提出了一种在有限图结构反馈下学习时变多重公平性度量凸化的方法，用于处理多个群体和个体公平性概念的权重学习问题。


<details>
  <summary>Details</summary>
Motivation: 随着对多重公平性度量需求的增加，需要同时考虑多个群体和个体公平性概念，但这些公平性正则化器的相对权重是未知的、时变的，需要实时学习。

Method: 采用图结构反馈的有限信息，学习时变多重公平性度量的凸化方法，动态调整不同公平性概念的权重。

Result: 该方法能够有效处理时变权重问题，在有限反馈条件下实现多重公平性度量的优化平衡。

Conclusion: 提出的方法为解决多重公平性度量权重学习问题提供了有效解决方案，特别适用于权重时变和反馈信息有限的应用场景。

Abstract: There is an increasing appreciation that one may need to consider multiple
measures of fairness, e.g., considering multiple group and individual fairness
notions. The relative weights of the fairness regularisers are a priori
unknown, may be time varying, and need to be learned on the fly. We consider
the learning of time-varying convexifications of multiple fairness measures
with limited graph-structured feedback.

</details>


### [44] [Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS](https://arxiv.org/abs/2508.14313)
*Can Jin,Yang Zhou,Qixin Zhang,Hongwu Peng,Di Zhang,Marco Pavone,Ligong Han,Zhang-Wei Hong,Tong Che,Dimitris N. Metaxas*

Main category: cs.LG

TL;DR: AIRL-S统一了强化学习和搜索方法，通过对抗逆强化学习直接从正确推理轨迹学习动态过程奖励模型，无需人工标注，在推理时同时指导RL和搜索，在多个基准测试中平均提升9%性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法存在两个分离范式：RL方法不稳定且样本效率低，搜索方法需要昂贵的人工标注且易受分布偏移影响。需要一种统一的方法来解决这些问题。

Method: 使用对抗逆强化学习（AIRL）结合群体相对策略优化（GRPO），直接从正确推理轨迹学习密集的动态过程奖励模型，无需标注中间过程数据。

Result: 在8个基准测试（数学、科学推理、代码生成）中平均性能提升9%，达到GPT-4o水平，集成到多种搜索算法中都优于基于标注数据训练的基线PRM。

Conclusion: RL的奖励函数确实是最佳的过程奖励模型，为LLM复杂推理任务提供了鲁棒且成本效益高的解决方案。

Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen
into two largely separate paradigms: (1) reinforcement learning (RL) methods
that optimize sparse outcome-based rewards, yet suffer from instability and low
sample efficiency; and (2) search-based techniques guided by independently
trained, static process reward models (PRMs), which require expensive human- or
LLM-generated labels and often degrade under distribution shifts. In this
paper, we introduce AIRL-S, the first natural unification of RL-based and
search-based TTS. Central to AIRL-S is the insight that the reward function
learned during RL training inherently represents the ideal PRM for guiding
downstream search. Specifically, we leverage adversarial inverse reinforcement
learning (AIRL) combined with group relative policy optimization (GRPO) to
learn a dense, dynamic PRM directly from correct reasoning traces, entirely
eliminating the need for labeled intermediate process data. At inference, the
resulting PRM simultaneously serves as the critic for RL rollouts and as a
heuristic to effectively guide search procedures, facilitating robust reasoning
chain extension, mitigating reward hacking, and enhancing cross-task
generalization. Experimental results across eight benchmarks, including
mathematics, scientific reasoning, and code generation, demonstrate that our
unified approach improves performance by 9 % on average over the base model,
matching GPT-4o. Furthermore, when integrated into multiple search algorithms,
our PRM consistently outperforms all baseline PRMs trained with labeled data.
These results underscore that, indeed, your reward function for RL is your best
PRM for search, providing a robust and cost-effective solution to complex
reasoning tasks in LLMs.

</details>


### [45] [FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models](https://arxiv.org/abs/2508.14315)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark Webb*

Main category: cs.LG

TL;DR: FedRAIN-Lite是一个联邦强化学习框架，通过将智能体分配到纬度带来模拟GCM的空间分解，实现局部参数学习与全局聚合，DDPG算法在简化气候模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统气候模型的子网格参数化是静态且离线调优的，无法适应不断变化的气候状态，需要开发能够在线学习和适应的参数化方案。

Method: 使用联邦强化学习框架，将智能体分配到不同纬度带，在简化能量平衡气候模型（ebm-v1到ebm-v3）上测试三种RL算法，比较不同FedRL配置的性能。

Result: DDPG算法在ebm-v2和ebm-v3设置中 consistently优于静态和单智能体基线，在热带和中纬度地区具有更快的收敛速度和更低的区域加权RMSE。

Conclusion: DDPG的跨超参数迁移能力和低计算成本使其非常适合地理自适应参数学习，为高复杂度GCM提供了可扩展路径，并为物理对齐的在线学习气候模型提供了原型。

Abstract: Sub-grid parameterisations in climate models are traditionally static and
tuned offline, limiting adaptability to evolving states. This work introduces
FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors
the spatial decomposition used in general circulation models (GCMs) by
assigning agents to latitude bands, enabling local parameter learning with
periodic global aggregation. Using a hierarchy of simplified energy-balance
climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble
(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under
different FedRL configurations. Results show that Deep Deterministic Policy
Gradient (DDPG) consistently outperforms both static and single-agent
baselines, with faster convergence and lower area-weighted RMSE in tropical and
mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to
transfer across hyperparameters and low computational cost make it well-suited
for geographically adaptive parameter learning. This capability offers a
scalable pathway towards high-complexity GCMs and provides a prototype for
physically aligned, online-learning climate models that can evolve with a
changing climate. Code accessible at
https://github.com/p3jitnath/climate-rl-fedrl.

</details>


### [46] [Multi-view Graph Condensation via Tensor Decomposition](https://arxiv.org/abs/2508.14330)
*Nícolas Roque dos Santos,Dawon Ahn,Diego Minatel,Alneu de Andrade Lopes,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于张量分解的多视图图压缩方法GCTD，通过张量分解技术替代计算密集的双层优化，在保持GNN性能的同时显著减少图规模，并在多个数据集上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法依赖计算密集的双层优化，且缺乏原始节点与合成节点之间的映射关系，限制了模型的可解释性。张量分解技术虽然在线性/多线性函数学习方面提供了更透明和资源友好的替代方案，但在图压缩领域的应用尚未探索。

Method: 提出GCTD方法，采用张量分解技术进行多视图图压缩，学习原始图的紧凑合成表示，避免了传统的双层优化过程，同时保持了节点映射关系。

Result: 在六个真实数据集上的实验表明，GCTD能有效减小图规模并保持GNN性能，在三个数据集上准确率提升达4.0%，在大图上与现有方法相比具有竞争力。

Conclusion: 张量分解技术能够有效应用于图压缩任务，GCTD方法在减少计算资源需求的同时保持了模型性能，为图神经网络的高效训练提供了新的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable results in various
real-world applications, including drug discovery, object detection, social
media analysis, recommender systems, and text classification. In contrast to
their vast potential, training them on large-scale graphs presents significant
computational challenges due to the resources required for their storage and
processing. Graph Condensation has emerged as a promising solution to reduce
these demands by learning a synthetic compact graph that preserves the
essential information of the original one while maintaining the GNN's
predictive performance. Despite their efficacy, current graph condensation
approaches frequently rely on a computationally intensive bi-level
optimization. Moreover, they fail to maintain a mapping between synthetic and
original nodes, limiting the interpretability of the model's decisions. In this
sense, a wide range of decomposition techniques have been applied to learn
linear or multi-linear functions from graph data, offering a more transparent
and less resource-intensive alternative. However, their applicability to graph
condensation remains unexplored. This paper addresses this gap and proposes a
novel method called Multi-view Graph Condensation via Tensor Decomposition
(GCTD) to investigate to what extent such techniques can synthesize an
informative smaller graph and achieve comparable downstream task performance.
Extensive experiments on six real-world datasets demonstrate that GCTD
effectively reduces graph size while preserving GNN performance, achieving up
to a 4.0\ improvement in accuracy on three out of six datasets and competitive
performance on large graphs compared to existing approaches. Our code is
available at https://anonymous.4open.science/r/gctd-345A.

</details>


### [47] [NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation](https://arxiv.org/abs/2508.14336)
*Xu Weng,K. V. Ling,Haochen Liu,Bingheng Wang,Kun Cao*

Main category: cs.LG

TL;DR: 提出NeRC框架，通过端到端学习使用易获取的位置标签而非不切实际的测距误差标签来校正GNSS测距误差，提升城市环境中移动设备的定位精度


<details>
  <summary>Details</summary>
Motivation: 城市环境中移动设备的GNSS定位面临复杂信号传播和低质量硬件导致的测距误差问题，传统数据驱动方法需要繁琐的测距误差标注

Method: 端到端神经测距校正框架，使用可微分移动水平位置估计处理测量序列进行定位并反向传播梯度，引入欧几里得距离场成本图减少对标注位置的需求

Result: 在公开基准和自收集数据集上验证了定位精度的显著提升，并在边缘设备上验证了实时性能

Conclusion: NeRC框架通过端到端学习有效解决了城市GNSS定位问题，无需测距误差标注，实现了精度提升和实时部署

Abstract: GNSS localization using everyday mobile devices is challenging in urban
environments, as ranging errors caused by the complex propagation of satellite
signals and low-quality onboard GNSS hardware are blamed for undermining
positioning accuracy. Researchers have pinned their hopes on data-driven
methods to regress such ranging errors from raw measurements. However, the
grueling annotation of ranging errors impedes their pace. This paper presents a
robust end-to-end Neural Ranging Correction (NeRC) framework, where
localization-related metrics serve as the task objective for training the
neural modules. Instead of seeking impractical ranging error labels, we train
the neural network using ground-truth locations that are relatively easy to
obtain. This functionality is supported by differentiable moving horizon
location estimation (MHE) that handles a horizon of measurements for
positioning and backpropagates the gradients for training. Even better, as a
blessing of end-to-end learning, we propose a new training paradigm using
Euclidean Distance Field (EDF) cost maps, which alleviates the demands on
labeled locations. We evaluate the proposed NeRC on public benchmarks and our
collected datasets, demonstrating its distinguished improvement in positioning
accuracy. We also deploy NeRC on the edge to verify its real-time performance
for mobile devices.

</details>


### [48] [On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks](https://arxiv.org/abs/2508.14338)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文研究了图神经网络中学习算法与图结构的相互作用，通过谱图理论分析SGD和Ridge回归的过度风险，揭示了图结构对学习算法性能的影响机制。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究主要关注插值机制下的收敛速度，且与图结构的联系较为粗糙。本文旨在填补这一空白，研究带噪声情况下图结构如何影响GNN学习算法的泛化性能。

Method: 将传统学习理论扩展到GNN场景，通过谱图理论分析SGD和Ridge回归的过度风险，比较不同图结构（规则图vs幂律图）的影响，并扩展到多层线性GNN分析。

Result: 建立了图结构与学习算法性能之间的耦合关系，发现图结构通过谱特性影响算法性能，多层GNN中过度风险呈现非各向同性增长，为过平滑问题提供了新视角。

Conclusion: 研究揭示了图结构、GNN和学习算法之间的紧密耦合关系，为实际应用中GNN算法设计和选择提供了理论指导和实践洞见。

Abstract: This paper studies the interplay between learning algorithms and graph
structure for graph neural networks (GNNs). Existing theoretical studies on the
learning dynamics of GNNs primarily focus on the convergence rates of learning
algorithms under the interpolation regime (noise-free) and offer only a crude
connection between these dynamics and the actual graph structure (e.g., maximum
degree). This paper aims to bridge this gap by investigating the excessive risk
(generalization performance) of learning algorithms in GNNs within the
generalization regime (with noise). Specifically, we extend the conventional
settings from the learning theory literature to the context of GNNs and examine
how graph structure influences the performance of learning algorithms such as
stochastic gradient descent (SGD) and Ridge regression. Our study makes several
key contributions toward understanding the interplay between graph structure
and learning in GNNs. First, we derive the excess risk profiles of SGD and
Ridge regression in GNNs and connect these profiles to the graph structure
through spectral graph theory. With this established framework, we further
explore how different graph structures (regular vs. power-law) impact the
performance of these algorithms through comparative analysis. Additionally, we
extend our analysis to multi-layer linear GNNs, revealing an increasing
non-isotropic effect on the excess risk profile, thereby offering new insights
into the over-smoothing issue in GNNs from the perspective of learning
algorithms. Our empirical results align with our theoretical predictions,
\emph{collectively showcasing a coupling relation among graph structure, GNNs
and learning algorithms, and providing insights on GNN algorithm design and
selection in practice.}

</details>


### [49] [A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations](https://arxiv.org/abs/2508.14340)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.LG

TL;DR: 本研究在自主网络操作(ACO)中引入教师指导技术，通过在CybORG环境中实现四种不同的教师指导方法，显著提高了训练效率和早期策略性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ACO应用需要智能体从零开始学习，导致收敛速度慢且早期性能差。虽然教师指导技术在其他领域已显示出潜力，但尚未应用于ACO领域。

Method: 在模拟的CybORG环境中实现了四种不同的教师指导技术，并进行比较评估。

Result: 教师集成可以显著提高训练效率，包括早期策略性能和收敛速度。

Conclusion: 教师指导技术在自主网络安全领域具有潜在的重要价值，能够有效解决ACO中学习效率低下的问题。

Abstract: Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to
train agents to make effective decisions in the cybersecurity domain. However,
existing ACO applications require agents to learn from scratch, leading to slow
convergence and poor early-stage performance. While teacher-guided techniques
have demonstrated promise in other domains, they have not yet been applied to
ACO. In this study, we implement four distinct teacher-guided techniques in the
simulated CybORG environment and conduct a comparative evaluation. Our results
demonstrate that teacher integration can significantly improve training
efficiency in terms of early policy performance and convergence speed,
highlighting its potential benefits for autonomous cybersecurity.

</details>


### [50] [Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation](https://arxiv.org/abs/2508.14342)
*Lingkai Kong,Haichuan Wang,Charles A. Emogor,Vincent Börsch-Supan,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 提出了一种结合流匹配和占用检测模型的创新方法，用于预测偷猎行为，解决了现有方法无法捕捉复杂时空模式的问题，并在乌干达国家公园数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 偷猎对野生动物和生物多样性构成严重威胁。现有基于线性模型或决策树的偷猎预测方法缺乏表达能力，无法捕捉复杂的非线性时空模式。

Method: 将流匹配与基于占用的检测模型相结合，在潜在空间中训练流以推断底层占用状态；采用从线性模型预测初始化的复合流，而不是扩散模型中标准的随机噪声，注入先验知识并提高泛化能力。

Result: 在乌干达两个国家公园的数据集评估中显示出预测准确性的持续提升。

Conclusion: 该方法有效解决了偷猎事件检测不完善和数据有限的问题，为保护干预措施提供了更准确的预测工具。

Abstract: Poaching poses significant threats to wildlife and biodiversity. A valuable
step in reducing poaching is to forecast poacher behavior, which can inform
patrol planning and other conservation interventions. Existing poaching
prediction methods based on linear models or decision trees lack the
expressivity to capture complex, nonlinear spatiotemporal patterns. Recent
advances in generative modeling, particularly flow matching, offer a more
flexible alternative. However, training such models on real-world poaching data
faces two central obstacles: imperfect detection of poaching events and limited
data. To address imperfect detection, we integrate flow matching with an
occupancy-based detection model and train the flow in latent space to infer the
underlying occupancy state. To mitigate data scarcity, we adopt a composite
flow initialized from a linear-model prediction rather than random noise which
is the standard in diffusion models, injecting prior knowledge and improving
generalization. Evaluations on datasets from two national parks in Uganda show
consistent gains in predictive accuracy.

</details>


### [51] [A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations](https://arxiv.org/abs/2508.14351)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文首次对基于分数的图生成模型（SGGMs）进行了非渐近收敛性分析，揭示了图结构拓扑特性等独特因素对收敛界的影响，并提供了超参数选择和归一化技术的理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分数生成模型（SGMs）的收敛性分析不适用于SGGMs，因为SGGMs涉及耦合的随机微分方程系统，图结构和节点特征由相互依赖但分离的SDE控制，理论行为特别是收敛性方面研究不足。

Method: 对SGGMs进行非渐近收敛性分析，重点关注三种图生成范式的收敛界（生成误差风险）：固定图结构的特征生成、固定节点特征的图结构生成、以及图结构和节点特征的联合生成。

Result: 分析揭示了SGGMs特有的多个因素（如图结构的拓扑特性）会影响收敛界，提供了超参数选择的理论见解，并通过合成图模型的受控实证研究验证了理论预测。

Conclusion: 这项工作深化了对SGGMs的理论理解，证明了其在关键领域的适用性，并为设计有效模型提供了实用指导，特别是在超参数选择和归一化技术方面。

Abstract: Score-based graph generative models (SGGMs) have proven effective in critical
applications such as drug discovery and protein synthesis. However, their
theoretical behavior, particularly regarding convergence, remains
underexplored. Unlike common score-based generative models (SGMs), which are
governed by a single stochastic differential equation (SDE), SGGMs involve a
system of coupled SDEs. In SGGMs, the graph structure and node features are
governed by separate but interdependent SDEs. This distinction makes existing
convergence analyses from SGMs inapplicable for SGGMs. In this work, we present
the first non-asymptotic convergence analysis for SGGMs, focusing on the
convergence bound (the risk of generative error) across three key graph
generation paradigms: (1) feature generation with a fixed graph structure, (2)
graph structure generation with fixed node features, and (3) joint generation
of both graph structure and node features. Our analysis reveals several unique
factors specific to SGGMs (e.g., the topological properties of the graph
structure) which affect the convergence bound. Additionally, we offer
theoretical insights into the selection of hyperparameters (e.g., sampling
steps and diffusion length) and advocate for techniques like normalization to
improve convergence. To validate our theoretical findings, we conduct a
controlled empirical study using synthetic graph models, and the results align
with our theoretical predictions. This work deepens the theoretical
understanding of SGGMs, demonstrates their applicability in critical domains,
and provides practical guidance for designing effective models.

</details>


### [52] [SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion](https://arxiv.org/abs/2508.14352)
*Junwei Su,Shan Wu*

Main category: cs.LG

TL;DR: SBGD模型通过将图表示细化到块图空间，解决了图扩散生成模型的内存可扩展性和尺寸泛化问题，显著降低了内存复杂度并提升了生成性能


<details>
  <summary>Details</summary>
Motivation: 图扩散生成模型(GDGMs)面临内存可扩展性差和尺寸泛化能力有限的问题，无法处理大规模图数据，限制了在实际应用中的可行性

Method: 提出随机块图扩散(SBGD)模型，将图表示细化到块图空间，融入基于真实图模式的结构先验，显著降低内存复杂度

Result: SBGD实现了显著的内存改进(高达6倍)，同时保持可比甚至更优的图生成性能，并能更好地泛化到未见过的图尺寸

Conclusion: SBGD不仅是一个可扩展且有效的GDGM，还展示了生成建模中的模块化原则，为通过分解复杂任务探索生成模型提供了新途径

Abstract: Graph diffusion generative models (GDGMs) have emerged as powerful tools for
generating high-quality graphs. However, their broader adoption faces
challenges in \emph{scalability and size generalization}. GDGMs struggle to
scale to large graphs due to their high memory requirements, as they typically
operate in the full graph space, requiring the entire graph to be stored in
memory during training and inference. This constraint limits their feasibility
for large-scale real-world graphs. GDGMs also exhibit poor size generalization,
with limited ability to generate graphs of sizes different from those in the
training data, restricting their adaptability across diverse applications. To
address these challenges, we propose the stochastic block graph diffusion
(SBGD) model, which refines graph representations into a block graph space.
This space incorporates structural priors based on real-world graph patterns,
significantly reducing memory complexity and enabling scalability to large
graphs. The block representation also improves size generalization by capturing
fundamental graph structures. Empirical results show that SBGD achieves
significant memory improvements (up to 6$\times$) while maintaining comparable
or even superior graph generation performance relative to state-of-the-art
methods. Furthermore, experiments demonstrate that SBGD better generalizes to
unseen graph sizes. The significance of SBGD extends beyond being a scalable
and effective GDGM; it also exemplifies the principle of modularization in
generative modeling, offering a new avenue for exploring generative models by
decomposing complex tasks into more manageable components.

</details>


### [53] [Organ-Agents: Virtual Human Physiology Simulator via LLMs](https://arxiv.org/abs/2508.14357)
*Rihao Chang,He Jiao,Weizhi Nie,Honglin Guo,Keliang Xie,Zhenhua Wu,Lina Zhao,Yunpeng Bai,Yongtao Ma,Lanjun Wang,Yuting Su,Xi Gao,Weijie Wang,Nicu Sebe,Bruno Lepri,Bingwei Sun*

Main category: cs.LG

TL;DR: Organ-Agents是一个基于大语言模型的多智能体框架，用于模拟人体生理系统，在脓毒症患者数据上表现出高精度仿真能力，并得到临床医生的验证认可。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的新进展来模拟复杂生理系统，为重症监护提供精确诊断、治疗模拟和假设测试的数字孪生工具。

Method: 采用多智能体框架，每个模拟器针对特定生理系统进行监督微调，然后使用强化学习指导的协调机制，包括动态参考选择和错误校正。

Result: 在4,509名保留患者上实现高仿真精度（各系统MSE<0.16），外部验证显示在分布偏移下性能稳定，临床医生评价真实性和生理合理性平均评分分别为3.9和3.7。

Conclusion: Organ-Agents是一个可信、可解释且可推广的数字孪生系统，可用于重症监护的精确诊断、治疗策略模拟和下游预警任务。

Abstract: Recent advances in large language models (LLMs) have enabled new
possibilities in simulating complex physiological systems. We introduce
Organ-Agents, a multi-agent framework that simulates human physiology via
LLM-driven agents. Each Simulator models a specific system (e.g.,
cardiovascular, renal, immune). Training consists of supervised fine-tuning on
system-specific time-series data, followed by reinforcement-guided coordination
using dynamic reference selection and error correction. We curated data from
7,134 sepsis patients and 7,895 controls, generating high-resolution
trajectories across 9 systems and 125 variables. Organ-Agents achieved high
simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and
robustness across SOFA-based severity strata. External validation on 22,689 ICU
patients from two hospitals showed moderate degradation under distribution
shifts with stable simulation. Organ-Agents faithfully reproduces critical
multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with
coherent timing and phase progression. Evaluation by 15 critical care
physicians confirmed realism and physiological plausibility (mean Likert
ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations
under alternative sepsis treatment strategies, generating trajectories and
APACHE II scores aligned with matched real-world patients. In downstream early
warning tasks, classifiers trained on synthetic data showed minimal AUROC drops
(<0.04), indicating preserved decision-relevant patterns. These results
position Organ-Agents as a credible, interpretable, and generalizable digital
twin for precision diagnosis, treatment simulation, and hypothesis testing in
critical care.

</details>


### [54] [Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization](https://arxiv.org/abs/2508.14385)
*Kim Hammar,Tao Li*

Main category: cs.LG

TL;DR: MOBAL是一种在线贝叶斯学习方法，用于网络安全事件响应规划，能够在模型不准确的情况下通过迭代学习优化响应决策。


<details>
  <summary>Details</summary>
Motivation: 现有的网络安全事件响应决策支持框架需要详细的系统模型，但在实际攻击中信息往往不完整或不准确，限制了其实用性。

Method: 提出MOBAL方法：通过贝叶斯学习迭代修正模型猜想，将推测模型量化为有限马尔可夫模型，利用动态规划进行高效的在线响应规划。

Result: 理论证明贝叶斯学习相对于信息反馈具有渐近一致性，建立了模型错误设定和量化误差的界限。在CAGE-2基准测试中，MOBAL在适应性和鲁棒性方面优于现有方法。

Conclusion: MOBAL方法有效解决了网络安全事件响应中的模型不准确问题，通过在线学习和模型量化实现了更优的决策支持。

Abstract: Effective responses to cyberattacks require fast decisions, even when
information about the attack is incomplete or inaccurate. However, most
decision-support frameworks for incident response rely on a detailed system
model that describes the incident, which restricts their practical utility. In
this paper, we address this limitation and present an online method for
incident response planning under model misspecification, which we call MOBAL:
Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture
about the model through Bayesian learning as new information becomes available,
which facilitates model adaptation as the incident unfolds. To determine
effective responses online, we quantize the conjectured model into a finite
Markov model, which enables efficient response planning through dynamic
programming. We prove that Bayesian learning is asymptotically consistent with
respect to the information feedback. Additionally, we establish bounds on
misspecification and quantization errors. Experiments on the CAGE-2 benchmark
show that MOBAL outperforms the state of the art in terms of adaptability and
robustness to model misspecification.

</details>


### [55] [Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states](https://arxiv.org/abs/2508.14413)
*Samarth Gupta,Raghudeep Gadde,Rui Chen,Aleix M. Martinez*

Main category: cs.LG

TL;DR: 本文挑战了扩散模型需要大量潜在状态/时间步数的假设，证明通过精心选择噪声调度，仅用少量甚至单个潜在状态就能达到与传统大量状态相当的性能，并提出解耦模型实现4-6倍的收敛加速。


<details>
  <summary>Details</summary>
Motivation: 挑战扩散模型需要大量潜在状态的基本假设，探索在少量甚至单个潜在状态下实现高质量生成的可能性，以提高训练效率和收敛速度。

Method: 通过精心选择噪声调度，在少量潜在状态（T~32）下训练扩散模型，并进一步推进到单个潜在状态的完全解耦，通过组合多个独立训练的单潜在状态模型来生成高质量样本。

Result: 实验表明，使用32个潜在状态的模型性能与使用1000个潜在状态的模型相当；解耦模型在两个不同数据集上实现了4-6倍的收敛加速。

Conclusion: 扩散模型并不需要大量潜在状态，通过适当的噪声调度和模型解耦，可以在保持生成质量的同时显著提高训练效率和收敛速度。

Abstract: We challenge a fundamental assumption of diffusion models, namely, that a
large number of latent-states or time-steps is required for training so that
the reverse generative process is close to a Gaussian. We first show that with
careful selection of a noise schedule, diffusion models trained over a small
number of latent states (i.e. $T \sim 32$) match the performance of models
trained over a much large number of latent states ($T \sim 1,000$). Second, we
push this limit (on the minimum number of latent states required) to a single
latent-state, which we refer to as complete disentanglement in T-space. We show
that high quality samples can be easily generated by the disentangled model
obtained by combining several independently trained single latent-state models.
We provide extensive experiments to show that the proposed disentangled model
provides 4-6$\times$ faster convergence measured across a variety of metrics on
two different datasets.

</details>


### [56] [Personalized Counterfactual Framework: Generating Potential Outcomes from Wearable Data](https://arxiv.org/abs/2508.14432)
*Ajan Subramanian,Amir M. Rahmani*

Main category: cs.LG

TL;DR: 提出了一个从可穿戴设备数据学习个性化反事实模型的框架，通过多模态相似性分析和时间PC算法来探索生活方式改变对个体生理结果的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器数据为个性化健康监测提供了机会，但从其复杂的纵向数据流中获取可操作的见解具有挑战性。需要一种方法来理解个体特定的生活方式选择可能带来的生理结果。

Method: 1) 通过多模态相似性分析用相似患者数据增强个体数据集；2) 使用时间PC算法发现预测关系，建模t-1时刻变量对t时刻生理变化的影响；3) 基于这些关系训练梯度提升机来量化个体特异性效应；4) 构建反事实引擎来预测假设干预下的生理轨迹

Result: 评估显示合理的预测准确性（平均心率MAE为4.71 bpm）和高反事实合理性（中位数0.9643）。干预结果显示出个体对假设生活方式改变反应的显著差异性。

Conclusion: 该框架为探索个性化健康动态和生成关于个体对生活方式改变反应的假设提供了工具，展示了在个性化健康洞察方面的潜力。

Abstract: Wearable sensor data offer opportunities for personalized health monitoring,
yet deriving actionable insights from their complex, longitudinal data streams
is challenging. This paper introduces a framework to learn personalized
counterfactual models from multivariate wearable data. This enables exploring
what-if scenarios to understand potential individual-specific outcomes of
lifestyle choices. Our approach first augments individual datasets with data
from similar patients via multi-modal similarity analysis. We then use a
temporal PC (Peter-Clark) algorithm adaptation to discover predictive
relationships, modeling how variables at time t-1 influence physiological
changes at time t. Gradient Boosting Machines are trained on these discovered
relationships to quantify individual-specific effects. These models drive a
counterfactual engine projecting physiological trajectories under hypothetical
interventions (e.g., activity or sleep changes). We evaluate the framework via
one-step-ahead predictive validation and by assessing the plausibility and
impact of interventions. Evaluation showed reasonable predictive accuracy
(e.g., mean heart rate MAE 4.71 bpm) and high counterfactual plausibility
(median 0.9643). Crucially, these interventions highlighted significant
inter-individual variability in response to hypothetical lifestyle changes,
showing the framework's potential for personalized insights. This work provides
a tool to explore personalized health dynamics and generate hypotheses on
individual responses to lifestyle changes.

</details>


### [57] [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
*Shuaijie She,Yu Bao,Yu Lu,Lu Xu,Tao Li,Wenhao Zhu,Shujian Huang,Shanbo Cheng,Lu Lu,Yuxuan Wang*

Main category: cs.LG

TL;DR: DuPO是一个基于对偶学习的偏好优化框架，通过广义对偶性生成无标注反馈，解决了RLVR对昂贵标签的依赖和传统对偶学习仅限于严格对偶任务对的限制。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习需要验证奖励（RLVR）对昂贵标注的依赖以及仅限于可验证任务的局限性，同时突破传统对偶学习只能处理严格对偶任务对（如翻译和回译）的限制。

Method: 将原始任务输入分解为已知和未知组件，构建对偶任务来使用原始输出和已知信息重建未知部分（如通过数学解反推隐藏变量），将重建质量作为自监督奖励来优化原始任务。

Result: 在756个翻译方向上平均提升2.13 COMET分数，在三个数学推理挑战基准上平均提升6.4个百分点，作为推理时重排序器性能提升9.3个百分点。

Conclusion: DuPO是一个可扩展、通用且无需标注的LLM优化范式，通过广义对偶学习实现了有效的自监督偏好优化。

Abstract: We present DuPO, a dual learning-based preference optimization framework that
generates annotation-free feedback via a generalized duality. DuPO addresses
two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s
reliance on costly labels and applicability restricted to verifiable tasks, and
traditional dual learning's restriction to strictly dual task pairs (e.g.,
translation and back-translation). Specifically, DuPO decomposes a primal
task's input into known and unknown components, then constructs its dual task
to reconstruct the unknown part using the primal output and known information
(e.g., reversing math solutions to recover hidden variables), broadening
applicability to non-invertible tasks. The quality of this reconstruction
serves as a self-supervised reward to optimize the primal task, synergizing
with LLMs' ability to instantiate both tasks via a single model. Empirically,
DuPO achieves substantial gains across diverse tasks: it enhances the average
translation quality by 2.13 COMET over 756 directions, boosts the mathematical
reasoning accuracy by an average of 6.4 points on three challenge benchmarks,
and enhances performance by 9.3 points as an inference-time reranker (trading
computation for accuracy). These results position DuPO as a scalable, general,
and annotation-free paradigm for LLM optimization.

</details>


### [58] [Fast Symbolic Regression Benchmarking](https://arxiv.org/abs/2508.14481)
*Viktor Martinek*

Main category: cs.LG

TL;DR: 本文提出了一种改进的符号回归基准测试方法，通过引入可接受表达式列表和早期终止回调机制，显著提高了基准测试的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归基准测试过度强调恢复特定表达式形式，或仅依赖计算机代数系统评估成功，且在发现表达式后仍继续搜索，存在效率低下和评估不准确的问题。

Method: 使用精心策划的可接受表达式列表和早期终止回调机制，基于Yoshitomo等人提出的SRSD基准问题，对SymbolicRegression.jl和TiSR两个符号回归包进行基准测试。

Result: 新方法将SymbolicRegression.jl的重新发现率从26.7%提升至44.7%，计算开销减少41.2%；TiSR的重新发现率达到69.4%，时间节省63%。

Conclusion: 提出的基准测试方法显著提高了符号回归算法的评估效率和准确性，为符号回归研究提供了更可靠的基准测试框架。

Abstract: Symbolic regression (SR) uncovers mathematical models from data. Several
benchmarks have been proposed to compare the performance of SR algorithms.
However, existing ground-truth rediscovery benchmarks overemphasize the
recovery of "the one" expression form or rely solely on computer algebra
systems (such as SymPy) to assess success. Furthermore, existing benchmarks
continue the expression search even after its discovery. We improve upon these
issues by introducing curated lists of acceptable expressions, and a callback
mechanism for early termination. As a starting point, we use the symbolic
regression for scientific discovery (SRSD) benchmark problems proposed by
Yoshitomo et al., and benchmark the two SR packages SymbolicRegression.jl and
TiSR. The new benchmarking method increases the rediscovery rate of
SymbolicRegression.jl from 26.7%, as reported by Yoshitomo et at., to 44.7%.
Performing the benchmark takes 41.2% less computational expense. TiSR's
rediscovery rate is 69.4%, while performing the benchmark saves 63% time.

</details>


### [59] [On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines](https://arxiv.org/abs/2508.14482)
*Alexander Geiger,Lars Wagner,Daniel Rueckert,Dirk Wilhelm,Alissa Jell*

Main category: cs.LG

TL;DR: 提出使用反事实生成方法为医学图像解释性分析提供更有意义的基线输入，替代传统的全零基线，以产生更忠实和医学相关的特征归因。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学领域需要可解释的输出，但现有路径归因方法使用的基线输入（如全零）在医学上下文中缺乏语义意义，且缺失信息本身可能具有临床价值。

Method: 使用变分自编码器生成反事实基线，这些基线是临床正常但与输入接近的反事实样本，能更准确地表示医学特征的有意义缺失。该方法与生成模型无关，可适用于任何合适的反事实方法。

Result: 在三个不同的医学数据集上评估，实证表明反事实基线相比标准基线选择能产生更忠实和医学相关的归因结果。

Conclusion: 反事实引导的基线选择方法为医学数据解释性分析提供了更原则性的途径，解决了传统基线在医学语境中的语义无意义问题。

Abstract: The explainability of deep learning models remains a significant challenge,
particularly in the medical domain where interpretable outputs are critical for
clinical trust and transparency. Path attribution methods such as Integrated
Gradients rely on a baseline input representing the absence of relevant
features ("missingness"). Commonly used baselines, such as all-zero inputs, are
often semantically meaningless, especially in medical contexts where
missingness can itself be informative. While alternative baseline choices have
been explored, existing methods lack a principled approach to dynamically
select baselines tailored to each input. In this work, we examine the notion of
missingness in the medical setting, analyze its implications for baseline
selection, and introduce a counterfactual-guided approach to address the
limitations of conventional baselines. We argue that a clinically normal but
input-close counterfactual represents a more accurate representation of a
meaningful absence of features in medical data. To implement this, we use a
Variational Autoencoder to generate counterfactual baselines, though our
concept is generative-model-agnostic and can be applied with any suitable
counterfactual method. We evaluate the approach on three distinct medical data
sets and empirically demonstrate that counterfactual baselines yield more
faithful and medically relevant attributions compared to standard baseline
choices.

</details>


### [60] [Semantic Energy: Detecting LLM Hallucination Beyond Entropy](https://arxiv.org/abs/2508.14496)
*Huan Ma,Jiadong Pan,Jing Liu,Yan Chen,Joey Tianyi Zhou,Guangyu Wang,Qinghua Hu,Hua Wu,Changqing Zhang,Haifeng Wang*

Main category: cs.LG

TL;DR: 提出了Semantic Energy框架，通过直接在倒数第二层logits上操作，结合语义聚类和玻尔兹曼能量分布，改进了LLM幻觉检测和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易产生幻觉，现有基于语义熵的方法依赖softmax后概率，无法捕捉模型内在不确定性，在某些场景下效果不佳。

Method: 引入Semantic Energy框架，利用LLM倒数第二层logits的固有置信度，结合语义聚类和玻尔兹曼能量分布来估计不确定性。

Result: 在多个基准测试中，Semantic Energy显著提升了幻觉检测和不确定性估计性能。

Conclusion: Semantic Energy为下游应用（如幻觉检测）提供了更可靠的不确定性信号，解决了语义熵方法的局限性。

Abstract: Large Language Models (LLMs) are being increasingly deployed in real-world
applications, but they remain susceptible to hallucinations, which produce
fluent yet incorrect responses and lead to erroneous decision-making.
Uncertainty estimation is a feasible approach to detect such hallucinations.
For example, semantic entropy estimates uncertainty by considering the semantic
diversity across multiple sampled responses, thus identifying hallucinations.
However, semantic entropy relies on post-softmax probabilities and fails to
capture the model's inherent uncertainty, causing it to be ineffective in
certain scenarios. To address this issue, we introduce Semantic Energy, a novel
uncertainty estimation framework that leverages the inherent confidence of LLMs
by operating directly on logits of penultimate layer. By combining semantic
clustering with a Boltzmann-inspired energy distribution, our method better
captures uncertainty in cases where semantic entropy fails. Experiments across
multiple benchmarks show that Semantic Energy significantly improves
hallucination detection and uncertainty estimation, offering more reliable
signals for downstream applications such as hallucination detection.

</details>


### [61] [Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes](https://arxiv.org/abs/2508.14499)
*Majid Mohammadi,Krikamol Muandet,Ilaria Tiddi,Annette Ten Teije,Siu Lun Chau*

Main category: cs.LG

TL;DR: 本文提出了一种在二次时间内精确计算FANOVA高斯过程Shapley值的方法，解决了传统方法计算复杂度高的问题，为概率模型提供了可扩展且包含不确定性的解释。


<details>
  <summary>Details</summary>
Motivation: Shapley值作为特征重要性归因的 principled 方法，在概率模型（如高斯过程）中计算复杂度极高，限制了实际应用。需要解决计算效率问题并提供不确定性感知的解释。

Method: 针对FANOVA GP模型，利用闭式Möbius表示和递归算法（受牛顿恒等式启发），在二次时间内精确计算局部和全局Shapley值，包括期望贡献和不确定性。

Result: 实现了在二次时间内精确计算Shapley值，能够捕获特征的期望贡献和不确定性，为概率模型提供了可扩展、公理合理且不确定性感知的解释。

Conclusion: 该方法显著提升了可解释AI的实用性，为结构化概率模型提供了更高效、更全面的解释能力，解决了Shapley值计算复杂度的瓶颈问题。

Abstract: Shapley values are widely recognized as a principled method for attributing
importance to input features in machine learning. However, the exact
computation of Shapley values scales exponentially with the number of features,
severely limiting the practical application of this powerful approach. The
challenge is further compounded when the predictive model is probabilistic - as
in Gaussian processes (GPs) - where the outputs are random variables rather
than point estimates, necessitating additional computational effort in modeling
higher-order moments. In this work, we demonstrate that for an important class
of GPs known as FANOVA GP, which explicitly models all main effects and
interactions, *exact* Shapley attributions for both local and global
explanations can be computed in *quadratic time*. For local, instance-wise
explanations, we define a stochastic cooperative game over function components
and compute the exact stochastic Shapley value in quadratic time only,
capturing both the expected contribution and uncertainty. For global
explanations, we introduce a deterministic, variance-based value function and
compute exact Shapley values that quantify each feature's contribution to the
model's overall sensitivity. Our methods leverage a closed-form (stochastic)
M\"{o}bius representation of the FANOVA decomposition and introduce recursive
algorithms, inspired by Newton's identities, to efficiently compute the mean
and variance of Shapley values. Our work enhances the utility of explainable
AI, as demonstrated by empirical studies, by providing more scalable,
axiomatically sound, and uncertainty-aware explanations for predictions
generated by structured probabilistic models.

</details>


### [62] [Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2508.14503)
*Lian Lian,Yilin Li,Song Han,Renzi Meng,Sibo Wang,Ming Wang*

Main category: cs.LG

TL;DR: 提出基于Transformer架构的多尺度特征感知异常检测方法，用于云服务环境中的时序建模和尺度感知特征表示，在多个关键指标上优于主流基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决云服务环境中时序建模和尺度感知特征表示的局限性，提升异常检测在复杂云环境中的性能。

Method: 使用改进的Transformer模块进行时序建模，通过自注意力机制捕获长程依赖关系；引入多尺度特征构建路径提取不同粒度的时间特征；设计注意力加权融合模块动态调整各尺度贡献度；构建标准化多维时间序列并采用位置编码增强时间感知。

Result: 在精确率、召回率、AUC和F1-score等关键指标上优于主流基线模型，在各种扰动条件下保持强稳定性和检测性能。

Conclusion: 该方法在复杂云环境中表现出优异的异常检测能力，具有强大的稳定性和检测性能。

Abstract: This study proposes an anomaly detection method based on the Transformer
architecture with integrated multiscale feature perception, aiming to address
the limitations of temporal modeling and scale-aware feature representation in
cloud service environments. The method first employs an improved Transformer
module to perform temporal modeling on high-dimensional monitoring data, using
a self-attention mechanism to capture long-range dependencies and contextual
semantics. Then, a multiscale feature construction path is introduced to
extract temporal features at different granularities through downsampling and
parallel encoding. An attention-weighted fusion module is designed to
dynamically adjust the contribution of each scale to the final decision,
enhancing the model's robustness in anomaly pattern modeling. In the input
modeling stage, standardized multidimensional time series are constructed,
covering core signals such as CPU utilization, memory usage, and task
scheduling states, while positional encoding is used to strengthen the model's
temporal awareness. A systematic experimental setup is designed to evaluate
performance, including comparative experiments and hyperparameter sensitivity
analysis, focusing on the impact of optimizers, learning rates, anomaly ratios,
and noise levels. Experimental results show that the proposed method
outperforms mainstream baseline models in key metrics, including precision,
recall, AUC, and F1-score, and maintains strong stability and detection
performance under various perturbation conditions, demonstrating its superior
capability in complex cloud environments.

</details>


### [63] [Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism](https://arxiv.org/abs/2508.14523)
*Kevin Riehl,Shaimaa K. El-Baklish,Anastasios Kouvelas,Michail A. Makridis*

Main category: cs.LG

TL;DR: 提出了Great GATsBi框架，这是一个基于领域知识的混合多模态自行车轨迹预测模型，结合物理建模和社会建模来处理自行车运动的双重特性。


<details>
  <summary>Details</summary>
Motivation: 自行车在交通事故中占比较高但研究较少，现有工作主要关注行人和机动车，需要专门针对自行车的轨迹预测方法来提高道路安全。

Method: 使用图注意力网络建模社会交互，结合物理模型（短期预测）和社会模型（长期预测）的集成方法，包含历史轨迹和预期未来轨迹数据。

Result: 提出的集成模型在短期和长期预测方面都表现出色，超越了现有最先进方法的性能，并通过大规模骑行实验验证了框架的有效性。

Conclusion: 该混合框架成功解决了自行车轨迹预测问题，证明了结合物理和社会建模的方法在处理自行车独特运动特性方面的有效性，为道路安全应用提供了重要工具。

Abstract: Accurate prediction of road user movement is increasingly required by many
applications ranging from advanced driver assistance systems to autonomous
driving, and especially crucial for road safety. Even though most traffic
accident fatalities account to bicycles, they have received little attention,
as previous work focused mainly on pedestrians and motorized vehicles. In this
work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal
trajectory prediction framework for bicycles. The model incorporates both
physics-based modeling (inspired by motorized vehicles) and social-based
modeling (inspired by pedestrian movements) to explicitly account for the dual
nature of bicycle movement. The social interactions are modeled with a graph
attention network, and include decayed historical, but also anticipated, future
trajectory data of a bicycles neighborhood, following recent insights from
psychological and social studies. The results indicate that the proposed
ensemble of physics models -- performing well in the short-term predictions --
and social models -- performing well in the long-term predictions -- exceeds
state-of-the-art performance. We also conducted a controlled mass-cycling
experiment to demonstrate the framework's performance when forecasting bicycle
trajectories and modeling social interactions with road users.

</details>


### [64] [Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks](https://arxiv.org/abs/2508.14536)
*Saman Yazdannik,Morteza Tayefi,Shamim Sanisales*

Main category: cs.LG

TL;DR: 提出基于切比雪夫多项式的DQN改进架构Ch-DQN，在CartPole-v1任务中比标准DQN性能提升39%，但多项式阶数选择很关键


<details>
  <summary>Details</summary>
Motivation: 标准多层感知机在近似复杂价值函数时效率不足，需要更有效的特征表示方法来提升DQN性能

Method: 将切比雪夫多项式基集成到DQN框架中，利用其强大的函数逼近能力创建更有效的特征表示

Result: 在CartPole-v1基准测试中，Ch-DQN（N=4）比参数量相当的标准DQN性能提升约39%，但高阶多项式（N=8）反而有害

Conclusion: 验证了正交多项式基在深度强化学习中的潜力，同时强调了模型复杂度选择的重要性

Abstract: The performance of Deep Q-Networks (DQN) is critically dependent on the
ability of its underlying neural network to accurately approximate the
action-value function. Standard function approximators, such as multi-layer
perceptrons, may struggle to efficiently represent the complex value landscapes
inherent in many reinforcement learning problems. This paper introduces a novel
architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev
polynomial basis into the DQN framework to create a more effective feature
representation. By leveraging the powerful function approximation properties of
Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more
efficiently and achieve higher performance. We evaluate our proposed model on
the CartPole-v1 benchmark and compare it against a standard DQN with a
comparable number of parameters. Our results demonstrate that the Ch-DQN with a
moderate polynomial degree (N=4) achieves significantly better asymptotic
performance, outperforming the baseline by approximately 39\%. However, we also
find that the choice of polynomial degree is a critical hyperparameter, as a
high degree (N=8) can be detrimental to learning. This work validates the
potential of using orthogonal polynomial bases in deep reinforcement learning
while also highlighting the trade-offs involved in model complexity.

</details>


### [65] [Adaptively Robust LLM Inference Optimization under Prediction Uncertainty](https://arxiv.org/abs/2508.14544)
*Zixi Chen,Yinyu Ye,Zijie Zhou*

Main category: cs.LG

TL;DR: 提出两种LLM推理调度算法：保守算法A_max基于预测输出长度上限防止内存溢出，但性能较差；自适应算法A_min基于预测下限动态调整，在保证性能的同时实现对数级竞争比。


<details>
  <summary>Details</summary>
Motivation: LLM推理是在线多任务服务过程，能耗高且输出长度未知，需要高效调度算法来降低延迟和功耗。

Method: 使用机器学习预测输出长度区间，设计A_max保守算法和A_min自适应算法，后者动态调整输出长度估计。

Result: A_min算法性能接近后见调度器，具有高效性和鲁棒性，且仅需预测区间下限（比上限更容易预测）。

Conclusion: A_min算法通过动态调整输出长度预测，在不确定环境下实现了高效的LLM推理调度，具有实际应用价值。

Abstract: We study the problem of optimizing Large Language Model (LLM) inference
scheduling to minimize total latency. LLM inference is an online and multi-task
service process and also heavily energy consuming by which a pre-trained LLM
processes input requests and generates output tokens sequentially. Therefore,
it is vital to improve its scheduling efficiency and reduce the power
consumption while a great amount of prompt requests are arriving. A key
challenge in LLM inference scheduling is that while the prompt length is known
upon arrival, the output length, which critically impacts memory usage and
processing time, is unknown. To address this uncertainty, we propose algorithms
that leverage machine learning to predict output lengths, assuming the
prediction provides an interval classification (min-max range) for each
request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which
schedules requests based on the upper bound of predicted output lengths to
prevent memory overflow. However, this approach is overly conservative: as
prediction accuracy decreases, performance degrades significantly due to
potential overestimation. To overcome this limitation, we propose
$\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted
lower bound as the output length and dynamically refines this estimate during
inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale
competitive ratio. Through numerical simulations, we demonstrate that
$\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler,
highlighting both its efficiency and robustness in practical scenarios.
Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the
prediction interval--an advantageous design choice since upper bounds on output
length are typically more challenging to predict accurately.

</details>


### [66] [A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression](https://arxiv.org/abs/2508.14576)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TL;DR: 本研究探讨了回归问题中基于密度比估计的公平性测量方法对底层估计算法的敏感性，发现不同密度比估计算法会显著影响公平性测量结果，甚至产生不一致的结论。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的算法偏见问题日益受到关注，现有研究将回归公平性测量建模为密度比估计问题，但未研究不同密度比估计算法对公平性测量结果的影响。

Method: 开发了多种基于不同密度比估计算法核心的公平性测量方法，并系统比较这些核心算法对公平性测量结果的影响。

Result: 实验结果表明，密度比估计算法的选择会显著影响公平性测量结果，甚至导致对不同算法相对公平性评价的不一致结论。

Conclusion: 基于密度比估计的回归公平性测量方法存在重大可靠性问题，需要进一步研究来提高其可靠性。

Abstract: The prevalence of algorithmic bias in Machine Learning (ML)-driven approaches
has inspired growing research on measuring and mitigating bias in the ML
domain. Accordingly, prior research studied how to measure fairness in
regression which is a complex problem. In particular, recent research proposed
to formulate it as a density-ratio estimation problem and relied on a Logistic
Regression-driven probabilistic classifier-based approach to solve it. However,
there are several other methods to estimate a density ratio, and to the best of
our knowledge, prior work did not study the sensitivity of such fairness
measurement methods to the choice of underlying density ratio estimation
algorithm. To fill this gap, this paper develops a set of fairness measurement
methods with various density-ratio estimation cores and thoroughly investigates
how different cores would affect the achieved level of fairness. Our
experimental results show that the choice of density-ratio estimation core
could significantly affect the outcome of fairness measurement method, and
even, generate inconsistent results with respect to the relative fairness of
various algorithms. These observations suggest major issues with density-ratio
estimation based fairness measurement in regression and a need for further
research to enhance their reliability.

</details>


### [67] [DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning](https://arxiv.org/abs/2508.14600)
*Xudong Wang,Guoming Tang,Junyu Xue,Srinivasan Keshav,Tongxin Li,Chris Ding*

Main category: cs.LG

TL;DR: DualNILM是一个基于Transformer的多任务学习框架，用于解决分布式能源系统下的非侵入式负荷监测问题，能够同时识别电器状态和注入能量。


<details>
  <summary>Details</summary>
Motivation: 随着太阳能电池板等分布式能源的普及，传统的NILM方法仅依赖电表数据，无法有效处理来自分布式能源的注入能量，导致性能下降。

Method: 采用深度多任务学习框架，结合sequence-to-point和sequence-to-sequence策略，基于Transformer架构捕获多尺度时间依赖关系。

Result: 在包含电器能耗和能量注入的自收集和合成数据集上验证，DualNILM在双重任务上表现优异，远超传统方法。

Conclusion: DualNILM有效解决了分布式能源环境下的NILM挑战，为智能家居和建筑应用提供了更准确的电器级能耗监测方案。

Abstract: Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain
fine-grained appliance-level energy consumption in smart homes and building
applications. However, the increasing adoption of behind-the-meter energy
sources, such as solar panels and battery storage, poses new challenges for
conventional NILM methods that rely solely on at-the-meter data. The injected
energy from the behind-the-meter sources can obscure the power signatures of
individual appliances, leading to a significant decline in NILM performance. To
address this challenge, we present DualNILM, a deep multi-task learning
framework designed for the dual tasks of appliance state recognition and
injected energy identification in NILM. By integrating sequence-to-point and
sequence-to-sequence strategies within a Transformer-based architecture,
DualNILM can effectively capture multi-scale temporal dependencies in the
aggregate power consumption patterns, allowing for accurate appliance state
recognition and energy injection identification. We conduct validation of
DualNILM using both self-collected and synthesized open NILM datasets that
include both appliance-level energy consumption and energy injection. Extensive
experimental results demonstrate that DualNILM maintains an excellent
performance for the dual tasks in NILM, much outperforming conventional
methods.

</details>


### [68] [Measuring IIA Violations in Similarity Choices with Bayesian Models](https://arxiv.org/abs/2508.14615)
*Hugo Sales Corrêa,Suryanarayana Sankagiri,Daniel Ratton Figueiredo,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 本文提出了两种统计方法来检验相似性选择中的IIA假设，发现显著违反IIA的现象，表明需要开发考虑情境效应的新相似性选择模型。


<details>
  <summary>Details</summary>
Motivation: 相似性选择数据在信息检索和嵌入学习等场景中很常见，但经典的基于度量的模型假设了无关选项独立性(IIA)。虽然IIA违反已在许多离散选择设置中被发现，但相似性选择设置却很少受到关注，因为目标依赖性质使IIA测试变得复杂。

Method: 提出了两种统计测试方法：经典的拟合优度测试和基于后验预测检查(PPC)框架的贝叶斯对应方法。贝叶斯方法能够量化IIA违反的程度而不仅仅是显著性。还设计了新的PPC测试来检验群体同质性。

Result: 在两个数据集上都确认了显著的IIA违反，发现随机生成的选择集和专门设计的选择集具有可比程度的违反。群体同质性测试显示群体确实是同质的，表明IIA违反是由选择集内的情境效应驱动的。

Conclusion: 研究结果强调了需要开发能够解释此类情境效应的新相似性选择模型，因为经典的IIA假设在相似性选择设置中不成立。

Abstract: Similarity choice data occur when humans make choices among alternatives
based on their similarity to a target, e.g., in the context of information
retrieval and in embedding learning settings. Classical metric-based models of
similarity choice assume independence of irrelevant alternatives (IIA), a
property that allows for a simpler formulation. While IIA violations have been
detected in many discrete choice settings, the similarity choice setting has
received scant attention. This is because the target-dependent nature of the
choice complicates IIA testing. We propose two statistical methods to test for
IIA: a classical goodness-of-fit test and a Bayesian counterpart based on the
framework of Posterior Predictive Checks (PPC). This Bayesian approach, our
main technical contribution, quantifies the degree of IIA violation beyond its
mere significance. We curate two datasets: one with choice sets designed to
elicit IIA violations, and another with randomly generated choice sets from the
same item universe. Our tests confirmed significant IIA violations on both
datasets, and notably, we find a comparable degree of violation between them.
Further, we devise a new PPC test for population homogeneity. Results show that
the population is indeed homogenous, suggesting that the IIA violations are
driven by context effects -- specifically, interactions within the choice sets.
These results highlight the need for new similarity choice models that account
for such context effects.

</details>


### [69] [A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification](https://arxiv.org/abs/2508.14618)
*Amin Noroozi,Sandaruwan K. Sethunge,Elham Norouzi,Phat T. Phan,Kavinda U. Waduge,Md. Arafatur Rahman*

Main category: cs.LG

TL;DR: 本研究提出了一个模糊增强可解释AI框架(FEXAI)，结合模糊逻辑、机器学习和SHAP分析，用于分析连续下降操作(CDO)性能影响因素，并开发可解释的分类规则。


<details>
  <summary>Details</summary>
Motivation: 连续下降操作(CDO)具有节能减排和提升乘客舒适度等优势，但现有研究缺乏对其性能影响因素的系统分析，且相关方法缺乏航空领域所需的透明度和可解释性。

Method: 收集1094个航班的ADS-B数据(29个特征)，应用机器学习模型和SHAP分析分类CDO依从性并排序特征重要性，然后使用最重要的3个特征构建模糊规则分类器。

Result: 所有模型分类准确率超过90%，识别出平均下降速率、下降段数量和平均航向变化是最重要的CDO性能预测因子，FEXAI生成了可读性强的操作规则。

Conclusion: FEXAI方法为操作决策支持提供了新途径，可集成到航空工具中实现实时建议，确保在不同操作条件下维持CDO依从性。

Abstract: Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that
avoid level-offs, reducing fuel burn, emissions, and noise while improving
efficiency and passenger comfort. Despite its operational and environmental
benefits, limited research has systematically examined the factors influencing
CDO performance. Moreover, many existing methods in related areas, such as
trajectory optimization, lack the transparency required in aviation, where
explainability is critical for safety and stakeholder trust. This study
addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI)
framework that integrates fuzzy logic with machine learning and SHapley
Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive
dataset of 29 features, including 11 operational and 18 weather-related
features, was collected from 1,094 flights using Automatic Dependent
Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then
applied to classify flights' CDO adherence levels and rank features by
importance. The three most influential features, as identified by SHAP scores,
were then used to construct a fuzzy rule-based classifier, enabling the
extraction of interpretable fuzzy rules. All models achieved classification
accuracies above 90%, with FEXAI providing meaningful, human-readable rules for
operational users. Results indicated that the average descent rate within the
arrival route, the number of descent segments, and the average change in
directional heading during descent were the strongest predictors of CDO
performance. The FEXAI method proposed in this study presents a novel pathway
for operational decision support and could be integrated into aviation tools to
enable real-time advisories that maintain CDO adherence under varying
operational conditions.

</details>


### [70] [Clinical semantics for lung cancer prediction](https://arxiv.org/abs/2508.14627)
*Luis H. John,Jan A. Kors,Jenna M. Reps,Peter R. Rijnbeek,Egill A. Fridgeirsson*

Main category: cs.LG

TL;DR: 该研究通过将SNOMED医学术语层次结构映射到双曲空间，生成Poincaré嵌入，并将其整合到深度学习模型中，以改进肺癌发病预测。


<details>
  <summary>Details</summary>
Motivation: 现有的临床预测模型通常忽略临床概念之间的语义关系，该研究旨在通过整合领域特定的语义信息来改善肺癌发病预测。

Method: 从SNOMED分类法中构建临床知识图谱，通过黎曼随机梯度下降生成Poincaré嵌入，然后将这些嵌入整合到ResNet和Transformer深度学习架构中。

Result: 使用预训练的Poincaré嵌入相比使用随机初始化欧几里得嵌入的基线模型，在区分性能上获得了适度但一致的改进。ResNet模型（特别是使用10维Poincaré嵌入的）显示出更好的校准性能，而Transformer模型在不同配置下保持稳定的校准。

Conclusion: 将临床知识图谱嵌入双曲空间并整合到深度学习模型中，可以通过保留用于预测的临床术语的层次结构来改善肺癌发病预测，这展示了将数据驱动特征提取与既定临床知识相结合的可行方法。

Abstract: Background: Existing clinical prediction models often represent patient data
using features that ignore the semantic relationships between clinical
concepts. This study integrates domain-specific semantic information by mapping
the SNOMED medical term hierarchy into a low-dimensional hyperbolic space using
Poincar\'e embeddings, with the aim of improving lung cancer onset prediction.
  Methods: Using a retrospective cohort from the Optum EHR dataset, we derived
a clinical knowledge graph from the SNOMED taxonomy and generated Poincar\'e
embeddings via Riemannian stochastic gradient descent. These embeddings were
then incorporated into two deep learning architectures, a ResNet and a
Transformer model. Models were evaluated for discrimination (area under the
receiver operating characteristic curve) and calibration (average absolute
difference between observed and predicted probabilities) performance.
  Results: Incorporating pre-trained Poincar\'e embeddings resulted in modest
and consistent improvements in discrimination performance compared to baseline
models using randomly initialized Euclidean embeddings. ResNet models,
particularly those using a 10-dimensional Poincar\'e embedding, showed enhanced
calibration, whereas Transformer models maintained stable calibration across
configurations.
  Discussion: Embedding clinical knowledge graphs into hyperbolic space and
integrating these representations into deep learning models can improve lung
cancer onset prediction by preserving the hierarchical structure of clinical
terminologies used for prediction. This approach demonstrates a feasible method
for combining data-driven feature extraction with established clinical
knowledge.

</details>


### [71] [Understanding Data Influence with Differential Approximation](https://arxiv.org/abs/2508.14648)
*Haoru Tan,Sitong Wu,Xiuzhe Wu,Wang Wang,Bo Zhao,Zeke Xie,Gui-Song Xia,Xiaojuan Qi*

Main category: cs.LG

TL;DR: Diff-In是一种新的样本影响力近似方法，通过累积连续训练步骤中的影响力差异来估计样本影响力，无需模型凸性假设，计算效率高且精度优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有数据分析工具精度不足，往往假设神经网络损失函数是凸的，这些限制使得现有方法难以有效实施

Method: 提出Diff-In方法，将样本影响力表述为连续训练迭代中变化的累积和，采用二阶近似高精度估计差异项，通过计算Hessian矩阵和梯度的乘积来保持计算效率

Result: 理论分析和大量实验证明Diff-In在多个基准数据集上的三种数据中心任务（数据清洗、数据删除、核心集选择）中表现优异，能够扩展到百万级数据点

Conclusion: Diff-In方法在保持计算效率的同时显著提高了样本影响力估计的准确性，为大规模数据分析和处理提供了有效工具

Abstract: Data plays a pivotal role in the groundbreaking advancements in artificial
intelligence. The quantitative analysis of data significantly contributes to
model training, enhancing both the efficiency and quality of data utilization.
However, existing data analysis tools often lag in accuracy. For instance, many
of these tools even assume that the loss function of neural networks is convex.
These limitations make it challenging to implement current methods effectively.
In this paper, we introduce a new formulation to approximate a sample's
influence by accumulating the differences in influence between consecutive
learning steps, which we term Diff-In. Specifically, we formulate the
sample-wise influence as the cumulative sum of its changes/differences across
successive training iterations. By employing second-order approximations, we
approximate these difference terms with high accuracy while eliminating the
need for model convexity required by existing methods. Despite being a
second-order method, Diff-In maintains computational complexity comparable to
that of first-order methods and remains scalable. This efficiency is achieved
by computing the product of the Hessian and gradient, which can be efficiently
approximated using finite differences of first-order gradients. We assess the
approximation accuracy of Diff-In both theoretically and empirically. Our
theoretical analysis demonstrates that Diff-In achieves significantly lower
approximation error compared to existing influence estimators. Extensive
experiments further confirm its superior performance across multiple benchmark
datasets in three data-centric tasks: data cleaning, data deletion, and coreset
selection. Notably, our experiments on data pruning for large-scale
vision-language pre-training show that Diff-In can scale to millions of data
points and outperforms strong baselines.

</details>


### [72] [ELATE: Evolutionary Language model for Automated Time-series Engineering](https://arxiv.org/abs/2508.14667)
*Andrew Murray,Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TL;DR: ELATE是一个基于进化框架和语言模型的自动化时间序列特征工程方法，通过统计指标和特征重要性指导特征生成，平均提升预测精度8.4%。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列特征工程依赖人工且耗时，现有自动化方法计算成本高且缺乏领域知识，需要更智能的自动化解决方案。

Method: 结合进化框架和语言模型，使用时间序列统计指标和特征重要性来指导和剪枝特征，语言模型生成上下文相关的特征变换。

Result: 实验表明ELATE在不同领域平均提高预测精度8.4%。

Conclusion: ELATE成功实现了智能化的时间序列特征工程自动化，显著提升了预测性能。

Abstract: Time-series prediction involves forecasting future values using machine
learning models. Feature engineering, whereby existing features are transformed
to make new ones, is critical for enhancing model performance, but is often
manual and time-intensive. Existing automation attempts rely on exhaustive
enumeration, which can be computationally costly and lacks domain-specific
insights. We introduce ELATE (Evolutionary Language model for Automated
Time-series Engineering), which leverages a language model within an
evolutionary framework to automate feature engineering for time-series data.
ELATE employs time-series statistical measures and feature importance metrics
to guide and prune features, while the language model proposes new,
contextually relevant feature transformations. Our experiments demonstrate that
ELATE improves forecasting accuracy by an average of 8.4% across various
domains.

</details>


### [73] [Improving Fairness in Graph Neural Networks via Counterfactual Debiasing](https://arxiv.org/abs/2508.14683)
*Zengyi Wo,Chang Liu,Yumeng Wang,Minglai Shao,Wenjun Wang*

Main category: cs.LG

TL;DR: Fair-ICD：一种基于反事实数据增强的图神经网络公平性优化方法，通过创建多样化的邻域来学习无偏节点表示，并使用对抗判别器减少预测偏差，在保持高预测性能的同时显著提升公平性指标。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在建模图结构数据方面表现出色，但与其他机器学习模型类似，GNNs在预测中可能基于种族、性别等属性表现出偏见。图结构和消息传递机制可能会加剧这种偏见。现有的去偏方法（如边丢弃或特征掩码）可能会无意中消除非敏感特征，导致预测准确性和公平性之间的平衡受损。

Method: 提出Fair-ICD方法：1）使用反事实数据增强创建多样化邻域，在消息传递前生成反事实数据；2）从增强图中学习无偏节点表示；3）使用对抗判别器来减少传统GNN分类器的预测偏差。该方法在适度条件下确保GNNs的公平性。

Result: 在标准数据集上使用三种GNN骨干网络进行的实验表明，Fair-ICD显著提高了公平性指标，同时保持了较高的预测性能。

Conclusion: Fair-ICD通过反事实数据增强和对抗学习相结合的方法，有效解决了GNNs中的偏见问题，在公平性和预测准确性之间取得了良好的平衡，为图神经网络的公平性研究提供了新的解决方案。

Abstract: Graph Neural Networks (GNNs) have been successful in modeling
graph-structured data. However, similar to other machine learning models, GNNs
can exhibit bias in predictions based on attributes like race and gender.
Moreover, bias in GNNs can be exacerbated by the graph structure and
message-passing mechanisms. Recent cutting-edge methods propose mitigating bias
by filtering out sensitive information from input or representations, like edge
dropping or feature masking. Yet, we argue that such strategies may
unintentionally eliminate non-sensitive features, leading to a compromised
balance between predictive accuracy and fairness. To tackle this challenge, we
present a novel approach utilizing counterfactual data augmentation for bias
mitigation. This method involves creating diverse neighborhoods using
counterfactuals before message passing, facilitating unbiased node
representations learning from the augmented graph. Subsequently, an adversarial
discriminator is employed to diminish bias in predictions by conventional GNN
classifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs
under moderate conditions. Experiments on standard datasets using three GNN
backbones demonstrate that Fair-ICD notably enhances fairness metrics while
preserving high predictive performance.

</details>


### [74] [Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum](https://arxiv.org/abs/2508.14684)
*Zengyi Wo,Wenjun Wang,Minglai Shao,Chang Liu,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: 提出基于因果边分离的谱神经网络CES2-GAD，用于解决异配图中异常检测问题，通过分离同配和异配边并使用混合谱滤波器捕获信号


<details>
  <summary>Details</summary>
Motivation: 现实世界中异常实体往往通过添加合法连接隐藏异常链接，形成异配结构，现有GNN方法难以处理。空间域方法忽略了节点结构编码、特征与环境间的复杂关系，且谱域异配问题研究有限

Method: 1) 使用因果干预将原图分离为同配和异配边；2) 使用多种混合谱滤波器从分割图中捕获信号；3) 将多信号表示拼接后输入分类器进行异常预测

Result: 在真实数据集上的大量实验证明了所提方法的有效性

Conclusion: 通过分析不同异配程度节点的谱分布，发现异常节点的异配性导致谱能量从低频向高频转移，基于此提出的CES2-GAD方法能有效处理异配图中的异常检测问题

Abstract: In the real world, anomalous entities often add more legitimate connections
while hiding direct links with other anomalous entities, leading to
heterophilic structures in anomalous networks that most GNN-based techniques
fail to address. Several works have been proposed to tackle this issue in the
spatial domain. However, these methods overlook the complex relationships
between node structure encoding, node features, and their contextual
environment and rely on principled guidance, research on solving spectral
domain heterophilic problems remains limited. This study analyzes the spectral
distribution of nodes with different heterophilic degrees and discovers that
the heterophily of anomalous nodes causes the spectral energy to shift from low
to high frequencies. To address the above challenges, we propose a spectral
neural network CES2-GAD based on causal edge separation for anomaly detection
on heterophilic graphs. Firstly, CES2-GAD will separate the original graph into
homophilic and heterophilic edges using causal interventions. Subsequently,
various hybrid-spectrum filters are used to capture signals from the segmented
graphs. Finally, representations from multiple signals are concatenated and
input into a classifier to predict anomalies. Extensive experiments with
real-world datasets have proven the effectiveness of the method we proposed.

</details>


### [75] [AFABench: A Generic Framework for Benchmarking Active Feature Acquisition](https://arxiv.org/abs/2508.14734)
*Valter Schütz,Han Wu,Reza Rezvan,Linus Aronsson,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 提出了AFABench，首个主动特征获取（AFA）的标准化基准框架，包含多样化数据集并评估了各类AFA方法，揭示了贪婪策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中获取所有特征成本高昂，AFA方法虽多但缺乏标准化评估基准，阻碍了公平比较和系统评估。

Method: 构建包含合成和真实数据集的基准框架AFABench，支持多种获取策略，实现了静态、贪婪和强化学习等代表性算法，并设计了AFAContext数据集测试前瞻能力。

Result: 评估了不同AFA策略的关键权衡，揭示了贪婪选择策略的局限性，为未来研究提供了实用见解。

Conclusion: AFABench填补了AFA领域基准测试的空白，促进了方法的公平比较和系统评估，代码已开源供社区使用。

Abstract: In many real-world scenarios, acquiring all features of a data instance can
be expensive or impractical due to monetary cost, latency, or privacy concerns.
Active Feature Acquisition (AFA) addresses this challenge by dynamically
selecting a subset of informative features for each data instance, trading
predictive performance against acquisition cost. While numerous methods have
been proposed for AFA, ranging from greedy information-theoretic strategies to
non-myopic reinforcement learning approaches, fair and systematic evaluation of
these methods has been hindered by the lack of standardized benchmarks. In this
paper, we introduce AFABench, the first benchmark framework for AFA. Our
benchmark includes a diverse set of synthetic and real-world datasets, supports
a wide range of acquisition policies, and provides a modular design that
enables easy integration of new methods and tasks. We implement and evaluate
representative algorithms from all major categories, including static, greedy,
and reinforcement learning-based approaches. To test the lookahead capabilities
of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed
to expose the limitations of greedy selection. Our results highlight key
trade-offs between different AFA strategies and provide actionable insights for
future research. The benchmark code is available at:
https://github.com/Linusaronsson/AFA-Benchmark.

</details>


### [76] [CaTE Data Curation for Trustworthy AI](https://arxiv.org/abs/2508.14741)
*Mary Versa Clemens-Sewall,Christopher Cervantes,Emma Rafkin,J. Neil Otte,Tom Magelinski,Libby Lewis,Michelle Liu,Dana Udwin,Monique Kirkman-Bey*

Main category: cs.LG

TL;DR: 这篇报告提供了在AI系统数据精炼阶段提升可信质量的实践指南，包括一系列步骤、工具和最佳实践。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI系统在数据精炼阶段的可信质挑战，提供系统化的实践指南和工具支持。

Method: 通过定义数据、数据精炼阶段和可信质概念，描述一系列核心步骤，分析各步骤的优缺点、前提条件、结果和开源工具实现。

Result: 综合了来自学术文献的数据精炼工具和方法，形成了一套多样化但协调一致的实践方法集。

Conclusion: 该报告为AI开发团队提供了完整的可信质数据精炼框架，能够有效提升AI系统的可靠性和信任度。

Abstract: This report provides practical guidance to teams designing or developing
AI-enabled systems for how to promote trustworthiness during the data curation
phase of development. In this report, the authors first define data, the data
curation phase, and trustworthiness. We then describe a series of steps that
the development team, especially data scientists, can take to build a
trustworthy AI-enabled system. We enumerate the sequence of core steps and
trace parallel paths where alternatives exist. The descriptions of these steps
include strengths, weaknesses, preconditions, outcomes, and relevant
open-source software tool implementations. In total, this report is a synthesis
of data curation tools and approaches from relevant academic literature, and
our goal is to equip readers with a diverse yet coherent set of practices for
improving AI trustworthiness.

</details>


### [77] [MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding](https://arxiv.org/abs/2508.14746)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出了D-GSR新范式，通过下游任务数据直接优化图结构，并开发MissionHD超维计算框架来执行图结构精炼，在视频异常检测任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有图结构精炼方法不适用于从大语言模型生成的新型、无数据集的图结构，这些图与下游视觉任务（如视频异常检测）存在不对齐问题

Method: 提出数据驱动的图结构精炼（D-GSR）范式，使用MissionHD超维计算框架，通过高效的编码-解码过程，在下游任务信号的指导下精炼图结构

Result: 在具有挑战性的视频异常检测和视频异常识别基准测试中，使用精炼后的图结构获得了显著的性能改进

Conclusion: 该方法作为有效的预处理步骤，能够有效优化大语言模型生成的图结构，使其更好地与下游视觉任务对齐

Abstract: Reasoning graphs from Large Language Models (LLMs) are often misaligned with
downstream visual tasks such as video anomaly detection (VAD). Existing Graph
Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less
graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly
optimizes graph structure using downstream task data, and propose MissionHD, a
hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses
an efficient encode-decode process to refine the graph, guided by the
downstream task signal. Experiments on challenging VAD and VAR benchmarks show
significant performance improvements when using our refined graphs, validating
our approach as an effective pre-processing step.

</details>


### [78] [Cross-Modality Controlled Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2508.14748)
*Yunzhe Zhang,Yifei Wang,Khanh Vinh Nguyen,Pengyu Hong*

Main category: cs.LG

TL;DR: CMCM-DLM是一个基于预训练扩散模型的跨模态分子生成方法，通过两个可训练模块（SCM和PCM）分阶段注入结构和性质约束，无需重新训练即可支持多模态约束。


<details>
  <summary>Details</summary>
Motivation: 现有SMILES扩散模型仅支持单模态约束，需要为每个新约束重新训练模型，而实际应用中需要处理多模态约束且可能动态增加新约束。

Method: 在预训练扩散模型基础上添加结构控制模块（SCM）和性质控制模块（PCM），分两阶段生成：第一阶段注入结构约束锚定分子骨架，第二阶段引入性质约束精炼分子性质。

Result: 在多个数据集上的实验证明了该方法的效率和适应性，在药物发现应用中展现出显著优势。

Conclusion: CMCM-DLM能够有效支持跨模态约束并动态整合新约束，无需重新训练，为分子生成提供了灵活高效的解决方案。

Abstract: Current SMILES-based diffusion models for molecule generation typically
support only unimodal constraint. They inject conditioning signals at the start
of the training process and require retraining a new model from scratch
whenever the constraint changes. However, real-world applications often involve
multiple constraints across different modalities, and additional constraints
may emerge over the course of a study. This raises a challenge: how to extend a
pre-trained diffusion model not only to support cross-modality constraints but
also to incorporate new ones without retraining. To tackle this problem, we
propose the Cross-Modality Controlled Molecule Generation with Diffusion
Language Model (CMCM-DLM), demonstrated by two distinct cross modalities:
molecular structure and chemical properties. Our approach builds upon a
pre-trained diffusion model, incorporating two trainable modules, the Structure
Control Module (SCM) and the Property Control Module (PCM), and operates in two
distinct phases during the generation process. In Phase I, we employs the SCM
to inject structural constraints during the early diffusion steps, effectively
anchoring the molecular backbone. Phase II builds on this by further
introducing PCM to guide the later stages of inference to refine the generated
molecules, ensuring their chemical properties match the specified targets.
Experimental results on multiple datasets demonstrate the efficiency and
adaptability of our approach, highlighting CMCM-DLM's significant advancement
in molecular generation for drug discovery applications.

</details>


### [79] [HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents](https://arxiv.org/abs/2508.14751)
*Thomas Carta,Clément Romac,Loris Gaven,Pierre-Yves Oudeyer,Olivier Sigaud,Sylvain Lamprier*

Main category: cs.LG

TL;DR: HERAKLES是一个分层自目标AI代理框架，通过将已掌握的目标编译到低级策略中，动态扩展高级策略可用的子目标空间，利用LLM作为高级控制器来实现目标分解和泛化。


<details>
  <summary>Details</summary>
Motivation: 解决开放环境中AI代理需要学习日益复杂、抽象和异构目标的问题，同时控制目标复杂性的增长，避免样本和计算复杂度的爆炸。

Method: 采用两层分层强化学习架构：低级策略使用小型快速神经网络执行编译后的技能，高级策略使用LLM进行目标分解和泛化；通过动态编译已掌握目标来扩展子目标空间。

Result: 在Crafter环境中验证，HERAKLES能够有效随目标复杂性扩展，通过技能编译提高样本效率，并使代理能够随时间稳健适应新挑战。

Conclusion: HERAKLES框架通过分层架构和动态技能编译，成功解决了开放环境中目标复杂性增长的问题，为构建更强大的自目标AI代理提供了有效方案。

Abstract: Open-ended AI agents need to be able to learn efficiently goals of increasing
complexity, abstraction and heterogeneity over their lifetime. Beyond sampling
efficiently their own goals, autotelic agents specifically need to be able to
keep the growing complexity of goals under control, limiting the associated
growth in sample and computational complexity. To adress this challenge, recent
approaches have leveraged hierarchical reinforcement learning (HRL) and
language, capitalizing on its compositional and combinatorial generalization
capabilities to acquire temporally extended reusable behaviours. Existing
approaches use expert defined spaces of subgoals over which they instantiate a
hierarchy, and often assume pre-trained associated low-level policies. Such
designs are inadequate in open-ended scenarios, where goal spaces naturally
diversify across a broad spectrum of difficulties. We introduce HERAKLES, a
framework that enables a two-level hierarchical autotelic agent to continuously
compile mastered goals into the low-level policy, executed by a small, fast
neural network, dynamically expanding the set of subgoals available to the
high-level policy. We train a Large Language Model (LLM) to serve as the
high-level controller, exploiting its strengths in goal decomposition and
generalization to operate effectively over this evolving subgoal space. We
evaluate HERAKLES in the open-ended Crafter environment and show that it scales
effectively with goal complexity, improves sample efficiency through skill
compilation, and enables the agent to adapt robustly to novel challenges over
time.

</details>


### [80] [PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning](https://arxiv.org/abs/2508.14765)
*Ruheng Wang,Hang Zhang,Trieu Nguyen,Shasha Feng,Hao-Wei Pang,Xiang Yu,Li Xiao,Peter Zhiping Zhang*

Main category: cs.LG

TL;DR: PepThink-R1是一个结合大语言模型、思维链微调和强化学习的肽设计框架，通过显式单体级修改推理来生成具有优化药理特性的环肽，在脂溶性、稳定性和暴露度方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 治疗性肽设计面临序列空间巨大、实验数据有限和现有生成模型可解释性差等挑战，需要开发能够进行可解释设计选择并优化多种药理特性的新方法。

Method: 整合大语言模型(LLMs)与思维链(CoT)监督微调和强化学习(RL)，在序列生成过程中显式推理单体级修改，通过定制奖励函数平衡化学有效性和属性改进。

Result: PepThink-R1生成的环肽在脂溶性、稳定性和暴露度方面显著增强，在优化成功率和可解释性上都优于通用LLMs(如GPT-5)和领域特定基线模型。

Conclusion: 这是首个结合显式推理与RL驱动属性控制的基于LLM的肽设计框架，为治疗发现提供了可靠且透明的肽优化方法。

Abstract: Designing therapeutic peptides with tailored properties is hindered by the
vastness of sequence space, limited experimental data, and poor
interpretability of current generative models. To address these challenges, we
introduce PepThink-R1, a generative framework that integrates large language
models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and
reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly
reasons about monomer-level modifications during sequence generation, enabling
interpretable design choices while optimizing for multiple pharmacological
properties. Guided by a tailored reward function balancing chemical validity
and property improvements, the model autonomously explores diverse sequence
variants. We demonstrate that PepThink-R1 generates cyclic peptides with
significantly enhanced lipophilicity, stability, and exposure, outperforming
existing general LLMs (e.g., GPT-5) and domain-specific baseline in both
optimization success and interpretability. To our knowledge, this is the first
LLM-based peptide design framework that combines explicit reasoning with
RL-driven property control, marking a step toward reliable and transparent
peptide optimization for therapeutic discovery.

</details>


### [81] [Context Steering: A New Paradigm for Compression-based Embeddings by Synthesizing Relevant Information Features](https://arxiv.org/abs/2508.14780)
*Guillermo Sarasa Durán,Ana Granados Fontecha,Francisco de Borja Rodríguez Ortíz*

Main category: cs.LG

TL;DR: 提出了一种名为"上下文引导"的新方法，通过主动指导特征塑造过程来改进基于压缩的距离度量，使其更好地适应特定分类或聚类任务


<details>
  <summary>Details</summary>
Motivation: 基于压缩的距离度量虽然灵活且领域无关，但难以与具体任务对齐，特别是在复杂聚类或分类场景中，因为相似性特征是从数据中衍生而非预定义的

Method: 通过系统分析每个对象在聚类框架内如何影响关系上下文，主动"引导"特征塑造过程，生成定制化的嵌入来隔离和放大类别区分信息

Result: 在异构数据集（从文本到真实世界音频）上的实验验证了上下文引导方法的鲁棒性和通用性，为常见的转导方法提供了有效替代方案

Conclusion: 该方法标志着应用的根本转变：从仅仅发现固有数据结构转向主动塑造针对特定目标定制的特征空间

Abstract: Compression-based distances (CD) offer a flexible and domain-agnostic means
of measuring similarity by identifying implicit information through
redundancies between data objects. However, as similarity features are derived
from the data, rather than defined as an input, it often proves difficult to
align with the task at hand, particularly in complex clustering or
classification settings. To address this issue, we introduce "context
steering," a novel methodology that actively guides the feature-shaping
process. Instead of passively accepting the emergent data structure (typically
a hierarchy derived from clustering CDs), our approach "steers" the process by
systematically analyzing how each object influences the relational context
within a clustering framework. This process generates a custom-tailored
embedding that isolates and amplifies class-distinctive information. We
validate the capabilities of this strategy using Normalized Compression
Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical
clustering, providing an effective alternative to common transductive methods.
Experimental results across heterogeneous datasets-from text to real-world
audio-validate the robustness and generality of context steering, marking a
fundamental shift in their application: from merely discovering inherent data
structures to actively shaping a feature space tailored to a specific
objective.

</details>


### [82] [Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method](https://arxiv.org/abs/2508.14783)
*Suleyman Olcay Polat,Poli A. Nemkova,Mark V. Albert*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model distillation enables the transfer of knowledge from large-scale models
to compact student models, facilitating deployment in resource-constrained
environments. However, conventional distillation approaches often suffer from
computational overhead and limited generalization. We propose a novel adaptive
distillation framework that dynamically augments training data in regions of
high student model loss. Using UMAP-based dimensionality reduction and nearest
neighbor sampling, our method identifies underperforming regions in the
embedding space and generates targeted synthetic examples to guide student
learning. To further improve efficiency, we introduce a lightweight
teacher-student interface that bypasses the teacher's input layer, enabling
direct distillation on vectorized representations. Experiments across standard
NLP benchmarks demonstrate that our 66M-parameter student model consistently
matches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%
on SST-2, while training with fewer epochs. These results highlight the promise
of loss-aware data augmentation and vectorized distillation for efficient and
effective model compression.

</details>


### [83] [A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects](https://arxiv.org/abs/2508.14801)
*Azim Ahmadzadeh,Rohan Adhyapak,Armin Iraji,Kartik Chaurasiya,V Aparna,Petrus C. Martens*

Main category: cs.LG

TL;DR: 本文提供了一个领域无关的图像标注项目准备指南，重点关注科学图像标注，旨在降低人工标注成本并提高质量


<details>
  <summary>Details</summary>
Motivation: 尽管对人工标注图像数据的需求很高，但管理复杂且成本高昂的标注项目仍然讨论不足，缺乏实用的指导方针

Method: 基于作者在管理大型人工标注项目中的丰富经验，讨论成功度量、标注对象、项目目标、数据可用性和团队角色等基本概念，并分析各种人类偏见，推荐提高标注质量和效率的工具技术

Result: 提出了一个全面的标注项目准备框架，涵盖了从数据收集到资源分配、从偏见缓解到标注人员培训等各个环节

Conclusion: 该指南旨在鼓励进一步研究和框架开发，为各个领域创建全面的知识库，从而降低人工标注项目的成本

Abstract: Despite the high demand for manually annotated image data, managing complex
and costly annotation projects remains under-discussed. This is partly due to
the fact that leading such projects requires dealing with a set of diverse and
interconnected challenges which often fall outside the expertise of specific
domain experts, leaving practical guidelines scarce. These challenges range
widely from data collection to resource allocation and recruitment, from
mitigation of biases to effective training of the annotators. This paper
provides a domain-agnostic preparation guide for annotation projects, with a
focus on scientific imagery. Drawing from the authors' extensive experience in
managing a large manual annotation project, it addresses fundamental concepts
including success measures, annotation subjects, project goals, data
availability, and essential team roles. Additionally, it discusses various
human biases and recommends tools and technologies to improve annotation
quality and efficiency. The goal is to encourage further research and
frameworks for creating a comprehensive knowledge base to reduce the costs of
manual annotation projects across various fields.

</details>


### [84] [Source-Guided Flow Matching](https://arxiv.org/abs/2508.14807)
*Zifan Wang,Alice Harting,Matthieu Barreau,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出了Source-Guided Flow Matching (SGFM)框架，通过直接修改源分布而非概率流向量场来实现生成模型引导，保持预训练向量场不变，将引导问题转化为源分布采样问题。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型引导方法通过修改概率流向量场实现，但这种方法可能不够灵活且计算复杂。作者希望开发一种更直接、灵活的引导框架。

Method: SGFM框架直接修改源分布，保持预训练向量场完整。将引导问题转化为从源分布采样的明确定义问题，允许用户根据具体问题灵活选择采样方法。

Result: 理论证明SGFM能精确恢复目标分布，提供了使用近似采样器和近似向量场时的Wasserstein误差界限。在合成2D基准、图像数据集和物理信息生成任务上验证了有效性。

Conclusion: SGFM框架提供了更灵活、精确的生成模型引导方法，与最优流匹配模型良好集成，保持了向量场生成的直接传输映射特性。

Abstract: Guidance of generative models is typically achieved by modifying the
probability flow vector field through the addition of a guidance field. In this
paper, we instead propose the Source-Guided Flow Matching (SGFM) framework,
which modifies the source distribution directly while keeping the pre-trained
vector field intact. This reduces the guidance problem to a well-defined
problem of sampling from the source distribution. We theoretically show that
SGFM recovers the desired target distribution exactly. Furthermore, we provide
bounds on the Wasserstein error for the generated distribution when using an
approximate sampler of the source distribution and an approximate vector field.
The key benefit of our approach is that it allows the user to flexibly choose
the sampling method depending on their specific problem. To illustrate this, we
systematically compare different sampling methods and discuss conditions for
asymptotically exact guidance. Moreover, our framework integrates well with
optimal flow matching models since the straight transport map generated by the
vector field is preserved. Experimental results on synthetic 2D benchmarks,
image datasets, and physics-informed generative tasks demonstrate the
effectiveness and flexibility of the proposed framework.

</details>


### [85] [Enhancing Contrastive Link Prediction With Edge Balancing Augmentation](https://arxiv.org/abs/2508.14808)
*Chen-Hao Chang,Hui-Ju Hung,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 本文提出了Edge Balancing Augmentation (EBA)方法和Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA)框架，通过理论分析和节点度调整来改进图对比学习在链接预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的链接预测方法存在两个主要问题：缺乏理论分析基础，以及对节点度考虑不足。本文旨在填补这些研究空白。

Method: 1) 首次为链接预测中的对比学习提供形式化理论分析；2) 提出EBA图增强方法调整节点度分布；3) 开发CoEBA框架整合EBA和新对比损失函数。

Result: 在8个基准数据集上的实验表明，CoEBA方法显著优于其他最先进的链接预测模型。

Conclusion: 本文通过理论指导和节点度平衡的增强策略，有效提升了对比学习在链接预测任务中的性能，为相关研究提供了新的理论基础和实践框架。

Abstract: Link prediction is one of the most fundamental tasks in graph mining, which
motivates the recent studies of leveraging contrastive learning to enhance the
performance. However, we observe two major weaknesses of these studies: i) the
lack of theoretical analysis for contrastive learning on link prediction, and
ii) inadequate consideration of node degrees in contrastive learning. To
address the above weaknesses, we provide the first formal theoretical analysis
for contrastive learning on link prediction, where our analysis results can
generalize to the autoencoder-based link prediction models with contrastive
learning. Motivated by our analysis results, we propose a new graph
augmentation approach, Edge Balancing Augmentation (EBA), which adjusts the
node degrees in the graph as the augmentation. We then propose a new approach,
named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),
that integrates the proposed EBA and the proposed new contrastive losses to
improve the model performance. We conduct experiments on 8 benchmark datasets.
The results demonstrate that our proposed CoEBA significantly outperforms the
other state-of-the-art link prediction models.

</details>


### [86] [Successive Halving with Learning Curve Prediction via Latent Kronecker Gaussian Processes](https://arxiv.org/abs/2508.14818)
*Jihao Andreas Lin,Nicolas Mayoraz,Steffen Rendle,Dima Kuzmin,Emil Praun,Berivan Isik*

Main category: cs.LG

TL;DR: 论文研究了使用潜在克罗内克高斯过程预测学习曲线来指导Successive Halving算法，以避免过早剪枝慢启动的超参数配置。虽然预测方法表现有竞争力，但需要完整学习曲线作为训练数据，不如直接为标准方法投入更多资源有效。


<details>
  <summary>Details</summary>
Motivation: Successive Halving算法依赖中间性能值做资源分配决策，可能导致过早剪枝那些初始表现不佳但最终会成为最佳候选的慢启动配置。

Method: 使用基于潜在克罗内克高斯过程的学习曲线预测来指导Successive Halving算法，与基于当前性能值的标准方法进行比较。

Result: 预测方法虽然能达到有竞争力的性能，但相比为标准方法投入更多资源并非帕累托最优，因为它需要完全观测的学习曲线作为训练数据。

Conclusion: 基于学习曲线预测的方法有潜力，但需要完整学习曲线数据的限制可以通过利用现有学习曲线数据来缓解。

Abstract: Successive Halving is a popular algorithm for hyperparameter optimization
which allocates exponentially more resources to promising candidates. However,
the algorithm typically relies on intermediate performance values to make
resource allocation decisions, which can cause it to prematurely prune slow
starters that would eventually become the best candidate. We investigate
whether guiding Successive Halving with learning curve predictions based on
Latent Kronecker Gaussian Processes can overcome this limitation. In a
large-scale empirical study involving different neural network architectures
and a click prediction dataset, we compare this predictive approach to the
standard approach based on current performance values. Our experiments show
that, although the predictive approach achieves competitive performance, it is
not Pareto optimal compared to investing more resources into the standard
approach, because it requires fully observed learning curves as training data.
However, this downside could be mitigated by leveraging existing learning curve
data.

</details>


### [87] [On Defining Neural Averaging](https://arxiv.org/abs/2508.14832)
*Su Hyeong Lee,Richard Ngo*

Main category: cs.LG

TL;DR: 本文提出了Amortized Model Ensembling (AME)框架，通过将模型差异视为伪梯度来指导神经网络权重更新，实现无需训练数据的模型平均化，在分布外场景下优于传统模型融合方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何从多个在不相交数据分片上预训练的模型中合成单一神经网络，仅使用最终权重且无需访问训练数据，探索神经网络平均化的定义和方法。

Method: 提出Amortized Model Ensembling (AME)框架，将模型差异作为伪梯度进行元优化，这是一种数据无关的模型权重聚合方法，能够恢复模型融合并支持更灵活的自适应集成策略。

Result: 实验表明AME产生的平均神经网络解决方案在性能上优于单个专家模型和模型融合基线，特别是在分布外设置中表现突出。

Conclusion: AME提供了一个原则性和可推广的数据无关模型权重聚合概念，为神经网络平均化提供了新的定义和方法论基础。

Abstract: What does it even mean to average neural networks? We investigate the problem
of synthesizing a single neural network from a collection of pretrained models,
each trained on disjoint data shards, using only their final weights and no
access to training data. In forming a definition of neural averaging, we take
insight from model soup, which appears to aggregate multiple models into a
singular model while enhancing generalization performance. In this work, we
reinterpret model souping as a special case of a broader framework: Amortized
Model Ensembling (AME) for neural averaging, a data-free meta-optimization
approach that treats model differences as pseudogradients to guide neural
weight updates. We show that this perspective not only recovers model soup but
enables more expressive and adaptive ensembling strategies. Empirically, AME
produces averaged neural solutions that outperform both individual experts and
model soup baselines, especially in out-of-distribution settings. Our results
suggest a principled and generalizable notion of data-free model weight
aggregation and defines, in one sense, how to perform neural averaging.

</details>


### [88] [Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations](https://arxiv.org/abs/2508.14844)
*Murat Isik,Mandeep Kaur Saggi,Humaira Gowher,Sabre Kais*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模态量子机器学习框架，通过整合蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像表示四种生化模态，显著提升了酶功能分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测酶功能是计算生物学的主要挑战之一，特别是对于结构注释有限或序列同源性较低的酶。现有方法往往依赖单一模态，无法充分捕捉酶功能的立体电子相互作用。

Method: 采用量子视觉变换器(QVT)主干网络，配备模态特定编码器和统一的交叉注意力融合模块。整合图特征和空间模式，捕捉酶功能背后的关键立体电子相互作用。

Result: 多模态QVT模型达到85.1%的top-1准确率，显著优于仅使用序列的基线方法，并比其他QML模型表现更好。

Conclusion: 该多模态量子机器学习框架通过整合多种生化信息源，有效提升了酶功能预测的准确性，为计算酶学提供了新的解决方案。

Abstract: Accurately predicting enzyme functionality remains one of the major
challenges in computational biology, particularly for enzymes with limited
structural annotations or sequence homology. We present a novel multimodal
Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC)
classification by integrating four complementary biochemical modalities:
protein sequence embeddings, quantum-derived electronic descriptors, molecular
graph structures, and 2D molecular image representations. Quantum Vision
Transformer (QVT) backbone equipped with modality-specific encoders and a
unified cross-attention fusion module. By integrating graph features and
spatial patterns, our method captures key stereoelectronic interactions behind
enzyme function. Experimental results demonstrate that our multimodal QVT model
achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a
substantial margin and achieving better performance results compared to other
QML models.

</details>


### [89] [Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent](https://arxiv.org/abs/2508.14853)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 提出一种基于指数梯度下降和Bregman投影的固有优化方法，直接优化松弛的one-hot编码来生成对抗性后缀，有效破解多个大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在关键应用中部署时，尽管有RLHF等对齐技术，但仍容易受到对抗性触发器的越狱攻击。现有方法要么在离散令牌空间搜索效率低，要么优化连续嵌入但无法直接应用于专有模型。

Method: 使用指数梯度下降和Bregman投影直接优化松弛的one-hot编码，确保每个令牌的优化one-hot编码始终保持在概率单纯形内。

Result: 在5个开源LLM和4个对抗行为数据集上评估，相比3个最先进基线方法，获得了更高的成功率和更快的收敛速度。还能生成跨多个提示的通用对抗后缀，并展示优化的后缀在不同LLM间的可迁移性。

Conclusion: 提出的固有优化方法有效解决了LLM越狱攻击问题，在攻击效果和效率方面都优于现有方法，具有实际应用价值。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, ensuring their robustness and safety alignment remains a major
challenge. Despite the overall success of alignment techniques such as
reinforcement learning from human feedback (RLHF) on typical prompts, LLMs
remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers
appended to user prompts. Most existing jailbreak methods either rely on
inefficient searches over discrete token spaces or direct optimization of
continuous embeddings. While continuous embeddings can be given directly to
selected open-source models as input, doing so is not feasible for proprietary
models. On the other hand, projecting these embeddings back into valid discrete
tokens introduces additional complexity and often reduces attack effectiveness.
We propose an intrinsic optimization method which directly optimizes relaxed
one-hot encodings of the adversarial suffix tokens using exponentiated gradient
descent coupled with Bregman projection, ensuring that the optimized one-hot
encoding of each token always remains within the probability simplex. We
provide theoretical proof of convergence for our proposed method and implement
an efficient algorithm that effectively jailbreaks several widely used LLMs.
Our method achieves higher success rates and faster convergence compared to
three state-of-the-art baselines, evaluated on five open-source LLMs and four
adversarial behavior datasets curated for evaluating jailbreak methods. In
addition to individual prompt attacks, we also generate universal adversarial
suffixes effective across multiple prompts and demonstrate transferability of
optimized suffixes to different LLMs.

</details>


### [90] [Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning](https://arxiv.org/abs/2508.14859)
*Jiafeng Xiong,Rizos Sakellariou*

Main category: cs.LG

TL;DR: GTGIB是一个结合图结构学习和时序图信息瓶颈的框架，用于动态网络的归纳表示学习，通过结构增强和信息瓶颈正则化有效处理未见节点和噪声信息


<details>
  <summary>Details</summary>
Motivation: 时序图学习中面临两个主要挑战：有效表示未见节点以及减轻噪声或冗余图信息的影响

Method: 提出GTGIB框架，整合图结构学习和时序图信息瓶颈，设计两步GSL结构增强器优化节点邻域，通过变分近似推导可处理的TGIB目标函数进行正则化

Result: 在四个真实数据集上的链接预测任务中，GTGIB在归纳设置下优于现有方法，在转导设置下也有显著且一致的改进

Conclusion: GTGIB通过结构增强和信息瓶颈正则化的结合，有效解决了时序图学习中的关键挑战，为动态网络表示学习提供了有效解决方案

Abstract: Temporal graph learning is crucial for dynamic networks where nodes and edges
evolve over time and new nodes continuously join the system. Inductive
representation learning in such settings faces two major challenges:
effectively representing unseen nodes and mitigating noisy or redundant graph
information. We propose GTGIB, a versatile framework that integrates Graph
Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We
design a novel two-step GSL-based structural enhancer to enrich and optimize
node neighborhoods and demonstrate its effectiveness and efficiency through
theoretical proofs and experiments. The TGIB refines the optimized graph by
extending the information bottleneck principle to temporal graphs, regularizing
both edges and features based on our derived tractable TGIB objective function
via variational approximation, enabling stable and efficient optimization.
GTGIB-based models are evaluated to predict links on four real-world datasets;
they outperform existing methods in all datasets under the inductive setting,
with significant and consistent improvement in the transductive setting.

</details>


### [91] [Squeezed Diffusion Models](https://arxiv.org/abs/2508.14871)
*Jyotirmai Singh,Samar Khanna,James Burgess*

Main category: cs.LG

TL;DR: 本文提出了挤压扩散模型(SDM)，通过数据感知的各向异性噪声缩放来改进扩散模型，在多个数据集上实现了FID提升15%的效果


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用各向同性高斯噪声，忽略了数据结构信息。受量子压缩态根据海森堡不确定性原理重新分布不确定性的启发，作者希望通过对主成分方向进行各向异性噪声缩放来更好地学习重要数据特征

Method: 提出了两种配置：(1)海森堡扩散模型-在主轴上缩放噪声并在正交方向进行逆缩放；(2)标准SDM变体-仅缩放主轴。在CIFAR-10/100和CelebA-64数据集上进行实验

Result: 反直觉地发现，适度的反挤压（在主轴上增加方差）能持续改善FID达15%，并将精度-召回边界推向更高召回率

Conclusion: 简单的数据感知噪声整形可以在不改变架构的情况下提供稳健的生成增益，证明了数据相关噪声缩放的有效性

Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding
structure in the data. Motivated by the way quantum squeezed states
redistribute uncertainty according to the Heisenberg uncertainty principle, we
introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically
along the principal component of the training distribution. As squeezing
enhances the signal-to-noise ratio in physics, we hypothesize that scaling
noise in a data-dependent manner can better assist diffusion models in learning
important data features. We study two configurations: (i) a Heisenberg
diffusion model that compensates the scaling on the principal axis with inverse
scaling on orthogonal directions and (ii) a standard SDM variant that scales
only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,
mild antisqueezing - i.e. increasing variance on the principal axis -
consistently improves FID by up to 15% and shifts the precision-recall frontier
toward higher recall. Our results demonstrate that simple, data-aware noise
shaping can deliver robust generative gains without architectural changes.

</details>


### [92] [Compute-Optimal Scaling for Value-Based Deep RL](https://arxiv.org/abs/2508.14881)
*Preston Fu,Oleh Rybkin,Zhiyuan Zhou,Michal Nauman,Pieter Abbeel,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 这篇论文研究深度强化学习中的计算优化扩展问题，探索如何在固定计算预算下通过调整模型容量和更新比例（UTD）来最大化样本效率，发现大批处理对小模型有害而对大模型无影响的TD过拟合现象。


<details>
  <summary>Details</summary>
Motivation: 随着模型变大和训练成本增加，需要在固定计算预算下完成计算优化扩展。虽然语言模型领域已有相关研究，但强化学习领域的计算扩展研究较少。

Method: 研究在线价值基于深度强化学习方法的计算扩展，重点分析模型容量和更新比例（UTD）两个资源分配轴。通过实验探索不同模型大小、批处大小和UTD比例的相互作用。

Result: 发现了TD过拟合现象：增加批处大小会快速损害小模型的Q函数准确性，但大模型中没有这种效应，从而能够有效利用大批处进行扩展。提供了选择批处大小和UTD的指导原则。

Conclusion: 研究结果为深度强化学习的计算优化扩展提供了基础指南，类似于监督学习中的相关研究，但适配于TD学习特性。

Abstract: As models grow larger and training them becomes expensive, it becomes
increasingly important to scale training recipes not just to larger models and
more data, but to do so in a compute-optimal manner that extracts maximal
performance per unit of compute. While such scaling has been well studied for
language modeling, reinforcement learning (RL) has received less attention in
this regard. In this paper, we investigate compute scaling for online,
value-based deep RL. These methods present two primary axes for compute
allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed
compute budget, we ask: how should resources be partitioned across these axes
to maximize sample efficiency? Our analysis reveals a nuanced interplay between
model size, batch size, and UTD. In particular, we identify a phenomenon we
call TD-overfitting: increasing the batch quickly harms Q-function accuracy for
small models, but this effect is absent in large models, enabling effective use
of large batch size at scale. We provide a mental model for understanding this
phenomenon and build guidelines for choosing batch size and UTD to optimize
compute usage. Our findings provide a grounded starting point for
compute-optimal scaling in deep RL, mirroring studies in supervised learning
but adapted to TD learning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [93] [Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots](https://arxiv.org/abs/2508.14247)
*Saswata Jana,Subhajit Pramanick,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 本文研究有限可见度的自洽机器人群体如何在图中部署形成最小顶点覆盖，针对树结构和一般图提出了多种算法，实现了时间最优和内存最优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在有限可见度的机器人群体中，如何利用局部知识将机器人部署到满足特定属性（如最小顶点覆盖）的顶点上，这是一个自然且具有挑战性的问题。

Method: 使用有限可见度的发光机器人，从特殊顶点（门）顺序进入图。针对树结构提出了单门和多门算法，针对一般图提出了需要额外内存的算法，所有算法都在O(|E|)时间内运行。

Result: 建立了机器人可见范围和时间复杂度的下界（Ω(|E|)）。对树结构实现了时间和内存最优的单门算法，以及内存最优的多门算法。对一般图提出了需要O(logΔ)额外内存的算法。

Conclusion: 成功证明了有限可见度机器人可以在异步调度器下形成最小顶点覆盖，特别是在树结构上实现了最优解，为机器人群体在图上的部署问题提供了有效的解决方案。

Abstract: In a connected graph with an autonomous robot swarm with limited visibility,
it is natural to ask whether the robots can be deployed to certain vertices
satisfying a given property using only local knowledge. This paper
affirmatively answers the question with a set of \emph{myopic} (finite
visibility range) luminous robots with the aim of \emph{filling a minimal
vertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special
vertices, called \emph{doors}, through which robots enter sequentially.
Starting from the doors, the goal of the robots is to settle on a set of
vertices that forms a minimal vertex cover of $G$ under the asynchronous
($\mathcal{ASYNC}$) scheduler. We are also interested in achieving the
\emph{minimum vertex cover} (MinVC, which is NP-hard \cite{Karp1972} for
general graphs) for a specific graph class using the myopic robots. We
establish lower bounds on the visibility range for the robots and on the time
complexity (which is $\Omega(|E|)$). We present two algorithms for trees: one
for single door, which is both time and memory-optimal, and the other for
multiple doors, which is memory-optimal and achieves time-optimality when the
number of doors is a constant. Interestingly, our technique achieves MinVC on
trees with a single door. We then move to the general graph, where we present
two algorithms, one for the single door and the other for the multiple doors
with an extra memory of $O(\log \Delta)$ for the robots, where $\Delta$ is the
maximum degree of $G$. All our algorithms run in $O(|E|)$ epochs.

</details>


### [94] [Pure Data Spaces](https://arxiv.org/abs/2508.14271)
*Saul Youssef*

Main category: cs.DC

TL;DR: 本文提出了'纯数据'作为数学和计算的基础框架，基于'有限序列'而非逻辑或类型，发展出空间理论并通过自同态半环研究空间结构。


<details>
  <summary>Details</summary>
Motivation: 旨在建立基于有限序列而非传统逻辑或类型的数学基础框架，探索纯数据空间中的数学对象如何有机地'生长'出来。

Method: 采用'纯数据'公理化框架，通过最小组合定义从纯数据基质中'有机生长'出空间，研究空间的自同态半环结构。

Result: 成功从纯数据框架中衍生出经典数学对象，包括自然数、整数、有理数、布尔空间、矩阵代数、高斯整数、四元数以及非结合代数如整数八元数。

Conclusion: 纯数据框架为数学和计算提供了新的基础，展示了经典数学对象可以从简单的组合定义中自然涌现，为理论发展和新探索指明了方向。

Abstract: In a previous work, "pure data" is proposed as an axiomatic foundation for
mathematics and computing, based on "finite sequence" as the foundational
concept rather than based on logic or type. Within this framework, objects with
mathematical meaning are "data" and collections of mathematical objects must
then be associative data, called a "space." A space is then the basic
collection in this framework analogous to sets in Set Theory or objects in
Category Theory. A theory of spaces is developed,where spaces are studied via
their semiring of endomorphisms. To illustrate these concepts, and as a way of
exploring the implications of the framework, pure data spaces are "grown
organically" from the substrate of pure data with minimal combinatoric
definitions. Familiar objects from classical mathematics emerge this way,
including natural numbers, integers, rational numbers, boolean spaces, matrix
algebras, Gaussian Integers, Quaternions, and non-associative algebras like the
Integer Octonions. Insights from these examples are discussed with a view
towards new directions in theory and new exploration.

</details>


### [95] [SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319)
*Parshan Javanrood,Matei Ripeanu*

Main category: cs.DC

TL;DR: SSSP-Del是一个新的顶点中心、异步、完全分布式的动态单源最短路径算法，能够同时处理边插入和删除操作，在共享内存架构中提供低延迟查询结果


<details>
  <summary>Details</summary>
Motivation: 现代图数据规模大且动态变化，传统SSSP算法无法高效处理拓扑变化，现有动态算法往往不能同时处理边增删、分布式内存操作和低延迟查询

Method: 提出SSSP-Del算法，采用顶点中心、异步、完全分布式架构，在共享内存环境中处理边插入和删除流

Result: 在包含数百万顶点的大型真实世界和合成图上进行全面评估，分析了结果延迟、解决方案稳定性和吞吐量

Conclusion: SSSP-Del算法成功解决了动态图SSSP查询的挑战，能够高效处理大规模动态图的边变化并提供低延迟查询

Abstract: Modern graphs are both large and dynamic, presenting significant challenges
for fundamental queries, such as the Single-Source Shortest Path (SSSP)
problem. Naively recomputing the SSSP tree after each topology change is
prohibitively expensive, causing on-demand computation to suffer from high
latency. Existing dynamic SSSP algorithms often cannot simultaneously handle
both edge additions and deletions, operate in distributed memory, and provide
low-latency query results. To address these challenges, this paper presents
SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm
for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm
processes streams of both edge insertions and deletions. We conduct a
comprehensive evaluation on large real-world and synthetic graphs with millions
of vertices, and provide a thorough analysis by evaluating result latency,
solution stability, and throughput.

</details>


### [96] [A Hierarchical Sharded Blockchain Balancing Performance and Availability](https://arxiv.org/abs/2508.14457)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: PyloChain是一个分层分片区块链系统，通过本地链并行执行本地交易和主链处理全局交易，在保证可用性的同时提升性能，相比现有技术吞吐量提高1.49倍，延迟降低2.63倍。


<details>
  <summary>Details</summary>
Motivation: 现有区块链分片技术往往以牺牲可用性为代价来追求性能，少数服务器故障就会导致数据不可用，需要一种能够平衡可用性和性能的分片解决方案。

Method: 采用分层架构：多个本地链并行执行本地交易，主链使用DAG内存池确保本地区块可用性，并通过BFT共识处理跨分片交易。应用简单调度技术减少本地交易中止，提供细粒度审计机制。

Result: 实现了PyloChain系统并进行了评估，相比最先进的平衡分层分片区块链，吞吐量提高1.49倍，延迟降低2.63倍，证明了其性能可扩展性。

Conclusion: PyloChain通过分层分片架构成功平衡了区块链的可用性和性能，为大规模区块链网络提供了可行的解决方案。

Abstract: Blockchain networks offer decentralization, transparency, and immutability
for managing critical data but encounter scalability problems as the number of
network members and transaction issuers grows. Sharding is considered a
promising solution to enhance blockchain scalability. However, most existing
blockchain sharding techniques prioritize performance at the cost of
availability (e.g., a failure in a few servers holding a shard leads to data
unavailability). In this paper, we propose PyloChain, a hierarchical sharded
blockchain that balances availability and performance. PyloChain consists of
multiple lower-level local chains and one higher-level main chain. Each local
chain speculatively executes local transactions to achieve high parallelism
across multiple local chains. The main chain leverages a directed-acyclic-graph
(DAG)-based mempool to guarantee local block availability and to enable
efficient Byzantine Fault Tolerance (BFT) consensus to execute global (or
cross-shard) transactions within a collocated sharding. PyloChain speculatively
executes local transactions across multiple local chains to achieve high
parallelism. In order to reduce the number of aborted local transactions,
PyloChain applies a simple scheduling technique to handle global transactions
in the main chain. PyloChain provides a fine-grained auditing mechanism to
mitigate faulty higher-level members by externalizing main chain operations to
lower-level local members. We implemented and evaluated PyloChain,
demonstrating its performance scalability with 1.49x higher throughput and
2.63x faster latency compared to the state-of-the-art balanced hierarchical
sharded blockchain.

</details>


### [97] [Auditable Shared Objects: From Registers to Synchronization Primitives](https://arxiv.org/abs/2508.14506)
*Hagit Attiya,Antonio Fernández Anta,Alessia Milani,Alexandre Rapetti,Corentin Travers*

Main category: cs.DC

TL;DR: 本文扩展了可审计性概念到多写入者寄存器，提出了O(n+m)步复杂度的实现，并展示了其与共识数的关系，以及扩展到LL/SC原语和拒绝列表的实现。


<details>
  <summary>Details</summary>
Motivation: 现有可审计性研究主要针对单写入者寄存器，需要将其扩展到更通用的多写入者共享对象，为数据所有者提供更好的数据访问控制。

Method: 使用(n+m)-滑动寄存器实现可审计的n写入者m读取者读写寄存器，分析其共识数需求，并扩展到LL/SC原语和拒绝列表实现。

Result: 成功实现了步复杂度为O(n+m)的可审计多写入者寄存器，证明了m+n的共识数是必要的，并展示了向LL/SC和访问控制对象的扩展能力。

Conclusion: 可审计性可以有效地扩展到多写入者共享对象，为构建更强大的数据访问控制系统提供了理论基础和实现方法。

Abstract: Auditability allows to track operations performed on a shared object,
recording who accessed which information. This gives data owners more control
on their data. Initially studied in the context of single-writer registers,
this work extends the notion of auditability to other shared objects, and
studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide
an implementation of an auditable $n$-writer $m$-reader read / write register,
with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding
registers, which have consensus number $m+n$. We show that this consensus
number is necessary. The implementation extends naturally to support an
auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a
primitive that supports efficient implementation of many shared objects.
Finally, we relate auditable registers to other access control objects, by
implementing an anti-flickering deny list from auditable registers.

</details>


### [98] [Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection](https://arxiv.org/abs/2508.14524)
*Krishnendu Chatterjee,Jan Matyáš Křišťan,Stefan Schmid,Jakub Svoboda,Michelle Yeo*

Main category: cs.DC

TL;DR: 本文提出了一个支付通道网络(PCN)的优化算法，通过设计网络拓扑和交易决策来最小化通道创建成本和交易拒绝成本，实现了O(p)的近似比。


<details>
  <summary>Details</summary>
Motivation: 支付通道网络虽然能缓解区块链扩展性问题，但需要精心设计网络拓扑来最大化交易吞吐量，同时用户需要做出最优的交易转发决策来延长通道寿命。

Method: 提出了一个近似算法来处理p个参与方的交易序列，通过设计PCN拓扑、通道容量配置和交易接受/拒绝决策来最小化总成本。

Result: 开发了一个O(p)近似算法，在特定交易分布假设下可进一步降低到O(√p)近似比，并通过闪电网络的实证研究验证了方法和假设。

Conclusion: 该算法为PCN网络优化提供了有效的理论框架和实用解决方案，能够显著降低运营成本并提高网络效率。

Abstract: Payment channel networks (PCNs) are a promising technology that alleviates
blockchain scalability by shifting the transaction load from the blockchain to
the PCN. Nevertheless, the network topology has to be carefully designed to
maximise the transaction throughput in PCNs. Additionally, users in PCNs also
have to make optimal decisions on which transactions to forward and which to
reject to prolong the lifetime of their channels. In this work, we consider an
input sequence of transactions over $p$ parties. Each transaction consists of a
transaction size, source, and target, and can be either accepted or rejected
(entailing a cost). The goal is to design a PCN topology among the $p$
cooperating parties, along with the channel capacities, and then output a
decision for each transaction in the sequence to minimise the cost of creating
and augmenting channels, as well as the cost of rejecting transactions. Our
main contribution is an $\mathcal{O}(p)$ approximation algorithm for the
problem with $p$ parties. We further show that with some assumptions on the
distribution of transactions, we can reduce the approximation ratio to
$\mathcal{O}(\sqrt{p})$. We complement our theoretical analysis with an
empirical study of our assumptions and approach in the context of the Lightning
Network.

</details>


### [99] [A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows](https://arxiv.org/abs/2508.14625)
*Kathleen West,Youssef Moawad,Fabian Lehmann,Vasilis Bountris,Ulf Leser,Yehia Elkhatib,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 该研究分析了科学工作流的碳排放问题，展示了碳感知执行策略（时间转移、暂停/恢复、资源缩放）的减排潜力，时间转移可减少80%排放，资源缩放可减少67%排放。


<details>
  <summary>Details</summary>
Motivation: 科学工作流通常计算密集且运行时间长，导致大量能源消耗和碳排放。虽然已有许多碳感知计算方法，但专门针对科学工作流的研究较少，而科学工作流具有延迟容忍、可中断、可扩展和异构性等特点，为碳感知计算提供了重要机会。

Method: 研究首先量化了7个真实Nextflow工作流在不同集群基础设施上的碳足迹，使用平均和边际碳强度数据。然后系统评估了碳感知时间转移、工作流暂停恢复、以及工作流和任务级别的资源缩放策略。

Result: 时间转移策略能够减少超过80%的碳排放，资源缩放策略能够减少67%的碳排放，显示了碳感知工作流执行的巨大减排潜力。

Conclusion: 科学工作流执行存在显著的碳减排机会，通过采用碳感知的时间调度和资源管理策略，可以大幅降低碳排放，为绿色科学计算提供了有效途径。

Abstract: Scientific workflows are widely used to automate scientific data analysis and
often involve computationally intensive processing of large datasets on compute
clusters. As such, their execution tends to be long-running and
resource-intensive, resulting in substantial energy consumption and, depending
on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware
computing methods have been proposed, yet little work has focused specifically
on scientific workflows, even though they present a substantial opportunity for
carbon-aware computing because they are often significantly delay tolerant,
efficiently interruptible, highly scalable and widely heterogeneous. In this
study, we first exemplify the problem of carbon emissions associated with
running scientific workflows, and then show the potential for carbon-aware
workflow execution. For this, we estimate the carbon footprint of seven
real-world Nextflow workflows executed on different cluster infrastructures
using both average and marginal carbon intensity data. Furthermore, we
systematically evaluate the impact of carbon-aware temporal shifting, and the
pausing and resuming of the workflow. Moreover, we apply resource scaling to
workflows and workflow tasks. Finally, we report the potential reduction in
overall carbon emissions, with temporal shifting capable of decreasing
emissions by over 80%, and resource scaling capable of decreasing emissions by
67%.

</details>


### [100] [DAG it off: Latency Prefers No Common Coins](https://arxiv.org/abs/2508.14716)
*Amores-Sesar Ignacio,Grøndal Viktor,Holmgård Adam,Ottendal Mads*

Main category: cs.DC

TL;DR: Black Marlin是首个在部分同步设置中无需可靠广播和公共币原语的DAG拜占庭原子广播协议，实现了3轮通信延迟的最优性能


<details>
  <summary>Details</summary>
Motivation: 现有DAG拜占庭协议依赖可靠广播和公共币原语，存在性能瓶颈，需要开发更高效的基础协议

Method: 基于有向无环图(DAG)设计的新型拜占庭原子广播协议，移除了可靠广播和公共币原语依赖，采用部分同步网络模型

Result: 实现3轮通信延迟(拜占庭故障下4.25轮)，保持最优通信和摊销通信复杂度，吞吐量和延迟均优于现有DAG协议

Conclusion: Black Marlin证明了无需传统原语即可实现高性能拜占庭共识，为分布式系统提供了更高效的底层协议解决方案

Abstract: We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based
Byzantine atomic broadcast protocol in a partially synchronous setting that
successfully forgoes the reliable broadcast and common coin primitives. Black
Marlin achieves the optimal latency of 3 rounds of communication (4.25 with
Byzantine faults) while maintaining optimal communication and amortized
communication complexities. We present a formal security analysis of the
protocol, accompanied by empirical evidence that Black Marlin outperforms
state-of-the-art DAG-based protocols in both throughput and latency.

</details>


### [101] [MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems](https://arxiv.org/abs/2508.14830)
*Kushagra Agrawal,Polat Goktas,Anjan Bandopadhyay,Debolina Ghosh,Junali Jasmine Jena,Mahendra Kumar Gourisaria*

Main category: cs.DC

TL;DR: 提出了MOHAF框架，一个分布式多目标资源分配机制，在IoT环境中同时优化成本、服务质量、能效和公平性，相比传统方法显著提升了分配效率和公平性。


<details>
  <summary>Details</summary>
Motivation: IoT生态系统快速发展，传统集中式机制和单目标拍卖模型无法在动态分布式环境中提供平衡的系统性能，需要解决异构资源高效分配问题。

Method: 采用分层聚类降低计算复杂度，结合贪心次模优化策略保证(1-1/e)近似比，使用动态定价机制实时适应资源利用率。

Result: 在Google集群数据上测试显示，MOHAF分配效率(0.263)显著优于Greedy(0.185)、First-Price(0.138)和Random(0.101)方法，同时实现完美公平性(Jain指数=1.000)。

Conclusion: MOHAF具有近线性可扩展性、理论保证和强大实证性能，为大规模IoT部署提供了实用且适应性强的解决方案，有效平衡了效率、公平性和可持续性。

Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the
challenge of efficiently allocating heterogeneous resources in highly dynamic,
distributed environments. Conventional centralized mechanisms and
single-objective auction models, focusing solely on metrics such as cost
minimization or revenue maximization, struggle to deliver balanced system
performance. This paper proposes the Multi-Objective Hierarchical Auction
Framework (MOHAF), a distributed resource allocation mechanism that jointly
optimizes cost, Quality of Service (QoS), energy efficiency, and fairness.
MOHAF integrates hierarchical clustering to reduce computational complexity
with a greedy, submodular optimization strategy that guarantees a (1-1/e)
approximation ratio. A dynamic pricing mechanism adapts in real time to
resource utilization, enhancing market stability and allocation quality.
Extensive experiments on the Google Cluster Data trace, comprising 3,553
requests and 888 resources, demonstrate MOHAF's superior allocation efficiency
(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)
auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation
studies reveal the critical influence of cost and QoS components in sustaining
balanced multi-objective outcomes. With near-linear scalability, theoretical
guarantees, and robust empirical performance, MOHAF offers a practical and
adaptable solution for large-scale IoT deployments, effectively reconciling
efficiency, equity, and sustainability in distributed resource coordination.

</details>


### [102] [Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach](https://arxiv.org/abs/2508.14848)
*Qiao Zhang,Rabab Alomairy,Dali Wang,Zhuowei Gu,Qinglei Cao*

Main category: cs.DC

TL;DR: 提出自适应混合精度GEMM框架，支持细粒度块级精度格式，利用PaRSEC运行时系统在多架构上实现良好性能扩展


<details>
  <summary>Details</summary>
Motivation: 随着低精度算术硬件的发展，需要重新评估数值算法以利用混合精度计算，提升性能和能效

Method: 开发自适应混合精度GEMM框架，在细粒度块/瓦片级别支持不同精度格式，使用PaRSEC运行时系统进行跨架构负载均衡

Result: 在ARM CPU的Fugaku超算、Nvidia GPU的A100 DGX和AMD GPU的Frontier超算上均表现出良好的性能扩展性

Conclusion: 通过桥接算法进步和硬件创新，该研究旨在提升计算效率和精度，推动各种应用的变革性进展

Abstract: General Matrix Multiplication (GEMM) is a critical operation underpinning a
wide range of applications in high-performance computing (HPC) and artificial
intelligence (AI). The emergence of hardware optimized for low-precision
arithmetic necessitates a reevaluation of numerical algorithms to leverage
mixed-precision computations, achieving improved performance and energy
efficiency. This research introduces an adaptive mixed-precision GEMM framework
that supports different precision formats at fine-grained tile/block levels. We
utilize the PaRSEC runtime system to balance workloads across various
architectures. The performance scales well on ARM CPU-based Fugaku
supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier
supercomputer. This research aims to enhance computational efficiency and
accuracy by bridging algorithmic advancements and hardware innovations, driving
transformative progress in various applications.

</details>


### [103] [The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace](https://arxiv.org/abs/2508.14883)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 本文分析了云虚拟机组合的成本优化，发现通过从不同市场空间购买异构虚拟机组合，并在运行时迁移虚拟机，可以显著降低成本。研究使用亚马逊定价数据和Bitbrains数据中心真实数据集验证了结果。


<details>
  <summary>Details</summary>
Motivation: 随着云提供商推出新型虚拟机交易方式，消费者需要从多个市场空间购买虚拟机组合（云组合）。行业研究需要确定创建最优组合的最佳实践和指南。

Method: 使用亚马逊的定价数据和Bitbrains数据中心的真实虚拟机利用率数据集进行成本分析，并使用第二个Bitbrains数据集进行结果验证。

Result: 研究表明：1）只有创建从不同市场空间购买的异构虚拟机组合才能达到成本最优；2）在运行时迁移虚拟机具有成本效益，特别是对运行6小时到1年的虚拟机；3）大多数虚拟机资源从未被利用，存在巨大成本优化潜力。

Conclusion: 云虚拟机组合的成本优化需要采用跨市场空间的异构采购策略和运行时迁移技术，未充分利用的资源代表了重要的未来成本优化机会。

Abstract: In recent years, cloud providers have introduced novel approaches for trading
virtual machines. For example, Virtustream introduced so-called muVMs to charge
cloud computing resources while other providers such as Google, Microsoft, or
Amazon re-invented their marketspaces. Today, the market leader Amazon runs six
marketspaces for trading virtual machines. Consumers can purchase bundles of
virtual machines, which are called cloud-portfolios, from multiple marketspaces
and providers. An industry-relevant field of research is to identify best
practices and guidelines on how such optimal portfolios are created. In the
paper at hand, a cost analysis of cloud portfolios is presented. Therefore,
pricing data from Amazon was used as well as a real virtual machine utilization
dataset from the Bitbrains datacenter. The results show that a cost optimum can
only be reached if heterogeneous portfolios are created where virtual machines
are purchased from different marketspaces. Additionally, the cost-benefit of
migrating virtual machines to different marketplaces during runtime is
presented. Such migrations are especially cost-effective for virtual machines
of cloud-portfolios which run between 6 hours and 1 year. The paper further
shows that most of the resources of virtual machines are never utilized by
consumers, which represents a significant future potential for cost
optimization. For the validation of the results, a second dataset of the
Bitbrains datacenter was used, which contains utility data of virtual machines
from a different domain of application.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [104] [MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging](https://arxiv.org/abs/2508.14053)
*Jinwei Tang,Jiayin Qin,Nuo Xu,Pragnya Sudershan Nalla,Yu Cao,Yang,Zhao,Caiwen Ding*

Main category: cs.AR

TL;DR: MAHL是一个基于大型语言模型的层次化chiplet设计生成框架，通过六个智能体协作实现AI算法-硬件映射，显著提高了chiplet设计的生成准确性和PPA优化效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI等工作负载规模和算法复杂度的增加，高维度设计挑战日益突出。虽然LLM在HDL生成方面表现出色，但在2.5D集成和chiplet设计中面临扁平化设计、高验证成本和参数优化不精确等问题。

Method: 提出MAHL框架，包含六个协作智能体：层次化描述生成、检索增强代码生成、多样化验证流程、多粒度设计空间探索等组件，共同优化PPA（功耗、性能、面积）。

Result: 实验显示MAHL显著提高了简单RTL设计的生成准确性，在最佳情况下将真实chiplet设计的生成准确率（Pass@5）从0提升到0.72。与最先进的CLARIE（基于专家）相比，在某些优化目标下达到相当或更优的PPA结果。

Conclusion: MAHL框架成功解决了LLM驱动chiplet设计的关键挑战，通过层次化方法和多智能体协作，实现了高效的chiplet设计生成和PPA优化，为先进芯片设计提供了有前景的解决方案。

Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity,
the primary challenge lies in their high dimensionality, encompassing computing
cores, array sizes, and memory hierarchies. To overcome these obstacles,
innovative approaches are required. Agile chip design has already benefited
from machine learning integration at various stages, including logic synthesis,
placement, and routing. With Large Language Models (LLMs) recently
demonstrating impressive proficiency in Hardware Description Language (HDL)
generation, it is promising to extend their abilities to 2.5D integration, an
advanced technique that saves area overhead and development costs. However,
LLM-driven chiplet design faces challenges such as flatten design, high
validation cost and imprecise parameter optimization, which limit its chiplet
design capability. To address this, we propose MAHL, a hierarchical LLM-based
chiplet design generation framework that features six agents which
collaboratively enable AI algorithm-hardware mapping, including hierarchical
description generation, retrieval-augmented code generation, diverseflow-based
validation, and multi-granularity design space exploration. These components
together enhance the efficient generation of chiplet design with optimized
Power, Performance and Area (PPA). Experiments show that MAHL not only
significantly improves the generation accuracy of simple RTL design, but also
increases the generation accuracy of real-world chiplet design, evaluated by
Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case
scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves
comparable or even superior PPA results under certain optimization objectives.

</details>


### [105] [Revisit Choice Network for Synthesis and Technology Mapping](https://arxiv.org/abs/2508.14068)
*Chen Chen,Jiaqi Yin,Cunxi Yu*

Main category: cs.AR

TL;DR: Cristal是一种新颖的布尔选择网络构建方法，通过代表性逻辑锥搜索、结构变异和优先级选择等技术，构建更少但更高质量的选择节点，在技术映射后阶段显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统选择网络构建方法忽视选择质量的问题，传统方法虽然能识别功能等价节点但可能无法有效提升技术映射效果。

Method: 提出包含代表性逻辑锥搜索、通过等式饱和进行结构变异生成多样化选择结构、优先级排序选择以及选择网络构建和验证的新流程。

Result: 在延迟导向模式下平均减少3.85%面积/8.35%延迟，面积导向模式下减少0.11%面积/2.74%延迟，大规模案例运行时间减少63.77%。

Conclusion: Cristal通过构建高质量选择节点，在布尔选择网络构建方面显著优于现有技术，为布尔优化和等价检查提供了更有效的解决方案。

Abstract: Choice network construction is a critical technique for alleviating
structural bias issues in Boolean optimization, equivalence checking, and
technology mapping. Previous works on lossless synthesis utilize independent
optimization to generate multiple snapshots, and use simulation and SAT solvers
to identify functionally equivalent nodes. These nodes are then merged into a
subject graph with choice nodes. However, such methods often neglect the
quality of these choices, raising the question of whether they truly contribute
to effective technology mapping.
  This paper introduces Cristal, a novel methodology and framework for
constructing Boolean choice networks. Specifically, Cristal introduces a new
flow of choice network-based synthesis and mapping, including representative
logic cone search, structural mutation for generating diverse choice structures
via equality saturation, and priority-ranking choice selection along with
choice network construction and validation. Through these techniques, Cristal
constructs fewer but higher-quality choices.
  Our experimental results demonstrate that Cristal outperforms the
state-of-the-art Boolean choice network construction implemented in ABC in the
post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in
delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime
reduction on large-scale cases across a diverse set of combinational circuits
from the IWLS 2005, ISCAS'89, and EPFL benchmark suites.

</details>


### [106] [AI Agents for Photonic Integrated Circuit Design Automation](https://arxiv.org/abs/2508.14123)
*Ankita Sharma,YuQi Fu,Vahid Ansari,Rishabh Iyer,Fiona Kuang,Kashish Mistry,Raisa Islam Aishy,Sara Ahmad,Joaquin Matres,Dirk R. Englund,Joyce K. S. Poon*

Main category: cs.AR

TL;DR: PhIDO是一个多智能体框架，可将自然语言光子集成电路设计请求转换为布局掩模文件。在102个设计测试中，单器件设计成功率最高达91%，15组件以下设计的端到端成功率约57%。


<details>
  <summary>Details</summary>
Motivation: 开发能够将自然语言描述自动转换为光子集成电路布局的智能设计系统，以加速光子芯片的开发流程。

Method: 使用多智能体框架PhIDO，比较了7种大型语言模型的推理能力，测试了从单器件到112组件的光子集成电路设计。

Result: 单器件设计成功率最高91%；15组件以下设计中，o1、Gemini-2.5-pro和Claude Opus 4达到约57%的端到端成功率，其中Gemini-2.5-pro所需输出token最少且成本最低。

Conclusion: 下一步工作包括标准化知识表示、扩展数据集、增强验证能力和实现机器人自动化，以推进自主光子集成电路开发。

Abstract: We present Photonics Intelligent Design and Optimization (PhIDO), a
multi-agent framework that converts natural-language photonic integrated
circuit (PIC) design requests into layout mask files. We compare 7 reasoning
large language models for PhIDO using a testbench of 102 design descriptions
that ranged from single devices to 112-component PICs. The success rate for
single-device designs was up to 91%. For design queries with less than or equal
to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest
end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro
requiring the fewest output tokens and lowest cost. The next steps toward
autonomous PIC development include standardized knowledge representations,
expanded datasets, extended verification, and robotic automation.

</details>


### [107] [Cross-Layer Design of Vector-Symbolic Computing: Bridging Cognition and Brain-Inspired Hardware Acceleration](https://arxiv.org/abs/2508.14245)
*Shuting Du,Mohamed Ibrahim,Zishen Wan,Luqi Zheng,Boheng Zhao,Zhenkun Fan,Che-Kai Liu,Tushar Krishna,Arijit Raychowdhury,Haitong Li*

Main category: cs.AR

TL;DR: 该论文旨在弥合向量符号架构(VSA)的理论软件探索与高效硬件架构开发之间的鸿沟，从协同设计角度为研究人员提供见解，包括VSA原理介绍、硬件技术分析、跨层设计方法，并提出了首个内存计算层次认知硬件系统。


<details>
  <summary>Details</summary>
Motivation: 尽管VSA在各种认知应用中广泛部署且硬件解决方案不断发展，但关于VSA硬件与算法融合的全面统一论述仍然有限，需要弥合理论软件探索与高效硬件架构开发之间的差距。

Method: 1) 介绍向量符号计算原理和核心数学操作；2) 深入分析VSA的硬件技术(模拟、混合信号、数字电路)；3) 提出VSA跨层设计方法；4) 提出首个内存计算层次认知硬件系统作为具体演示。

Result: 通过详细分析不同硬件实现的性能特征和权衡，提取了任意VSA公式的开发设计指南，展示了协同设计方法在效率、灵活性和可扩展性方面的优势。

Conclusion: 论文为VSA的硬件/软件协同设计提供了系统框架和具体实现，并讨论了未来探索的开放性研究挑战，推动了VSA技术在认知计算领域的进一步发展。

Abstract: Vector Symbolic Architectures (VSAs) have been widely deployed in various
cognitive applications due to their simple and efficient operations. The
widespread adoption of VSAs has, in turn, spurred the development of numerous
hardware solutions aimed at optimizing their performance. Despite these
advancements, a comprehensive and unified discourse on the convergence of
hardware and algorithms in the context of VSAs remains somewhat limited. The
paper aims to bridge the gap between theoretical software-level explorations
and the development of efficient hardware architectures and emerging technology
fabrics for VSAs, providing insights from the co-design aspect for researchers
from either side. First, we introduce the principles of vector-symbolic
computing, including its core mathematical operations and learning paradigms.
Second, we provide an in-depth discussion on hardware technologies for VSAs,
analyzing analog, mixed-signal, and digital circuit design styles. We compare
hardware implementations of VSAs by carrying out detailed analysis of their
performance characteristics and tradeoffs, allowing us to extract design
guidelines for the development of arbitrary VSA formulations. Third, we discuss
a methodology for cross-layer design of VSAs that identifies synergies across
layers and explores key ingredients for hardware/software co-design of VSAs.
Finally, as a concrete demonstration of this methodology, we propose the first
in-memory computing hierarchical cognition hardware system, showcasing the
efficiency, flexibility, and scalability of this co-design approach. The paper
concludes with a discussion of open research challenges for future
explorations.

</details>


### [108] [Power Stabilization for AI Training Datacenters](https://arxiv.org/abs/2508.14318)
*Esha Choukse,Brijesh Warrier,Scot Heath,Luz Belmont,April Zhao,Hassan Ali Khan,Brian Harry,Matthew Kappel,Russell J. Hewett,Kushal Datta,Yu Pei,Caroline Lichtenberger,John Siegler,David Lukofsky,Zaid Kahn,Gurpreet Sahota,Andy Sullivan,Charles Frederick,Hien Thai,Rebecca Naughton,Daniel Jurnove,Justin Harp,Reid Carper,Nithish Mahalingam,Srini Varkala,Alok Gautam Kumbhare,Satyajit Desai,Venkatesh Ramamurthy,Praneeth Gottumukkala,Girish Bhatia,Kelsey Wildstone,Laurentiu Olariu,Mohammed Ayna,Mike Kendrick,Ricardo Bianchini,Aaron Hurst,Reza Zamani,Xin Li,Gene Oden,Rory Carmichael,Tom Li,Apoorv Gupta,Nilesh Dattani,Lawrence Marwong,Rob Nertney,Jeff Liott,Miro Enev,Divya Ramakrishnan,Ian Buck,Jonah Alben*

Main category: cs.AR

TL;DR: 大规模AI训练工作负载存在显著的功率波动问题，计算阶段和通信阶段的功率差异导致电网安全隐患，需要跨软件、硬件和数据中心基础设施的多层次解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI训练规模扩展到数万个GPU，训练过程中的功率波动问题日益严重。计算密集型阶段和通信阶段的功率差异导致大幅功率摆动，这些摆动的频率可能与电网关键频率共振，对电网基础设施造成物理损害，因此需要稳定AI训练工作负载的功率。

Method: 提出了跨软件、GPU硬件和数据中心基础设施的创新解决方案，包括多层次的干预措施。使用真实硬件和微软内部云功率模拟器对这些解决方案进行严格测试，评估其在真实条件下的有效性。

Result: 研究通过生产数据展示了功率波动的挑战，分析了各种解决方案的优缺点，最终提出了一个多管齐下的方法来解决功率稳定问题。

Conclusion: 为了解决AI训练规模扩展带来的电网安全隐患，需要采用跨软件、硬件和数据中心基础设施的综合方法来实现功率稳定，确保大规模AI训练的安全性和可持续性。

Abstract: Large Artificial Intelligence (AI) training workloads spanning several tens
of thousands of GPUs present unique power management challenges. These arise
due to the high variability in power consumption during the training. Given the
synchronous nature of these jobs, during every iteration there is a
computation-heavy phase, where each GPU works on the local data, and a
communication-heavy phase where all the GPUs synchronize on the data. Because
compute-heavy phases require much more power than communication phases, large
power swings occur. The amplitude of these power swings is ever increasing with
the increase in the size of training jobs. An even bigger challenge arises from
the frequency spectrum of these power swings which, if harmonized with critical
frequencies of utilities, can cause physical damage to the power grid
infrastructure. Therefore, to continue scaling AI training workloads safely, we
need to stabilize the power of such workloads. This paper introduces the
challenge with production data and explores innovative solutions across the
stack: software, GPU hardware, and datacenter infrastructure. We present the
pros and cons of each of these approaches and finally present a multi-pronged
approach to solving the challenge. The proposed solutions are rigorously tested
using a combination of real hardware and Microsoft's in-house cloud power
simulator, providing critical insights into the efficacy of these interventions
under real-world conditions.

</details>


### [109] [Computing-In-Memory Dataflow for Minimal Buffer Traffic](https://arxiv.org/abs/2508.14375)
*Choongseok Song,Doo Seok Jeong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的存内计算(CIM)数据流，专门优化深度卷积操作，通过最大化数据重用和提高内存利用率，将缓冲区流量减少77.4-87.0%，从而大幅降低了数据传输能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 存内计算(CIM)虽然能够提高边缘AI设备的能效，但在加速深度卷积操作时存在CIM内存利用率低和缓冲区流量大的问题，而后者对延迟和能耗有重要影响但被忽视。

Method: 提出一种基于坚实理论基础的新题CIM数据流方案，通过最大化数据重用和改善内存利用率来优化深度卷积操作的缓冲区流量。

Result: 在MobileNet和EfficientNet模型上实验，新数据流将缓冲区流量减少77.4-87.0%，数据传输能耗总体降低10.1-17.9%，延迟降低15.6-27.8%，超过传统权重静态数据流。

Conclusion: 该研究提供了一种高效的CIM数据流方案，成功解决了深度卷积在CIM平台上的性能瓶颈，为边缘AI设备提供了更高能效的解决方案。

Abstract: Computing-In-Memory (CIM) offers a potential solution to the memory wall
issue and can achieve high energy efficiency by minimizing data movement,
making it a promising architecture for edge AI devices. Lightweight models like
MobileNet and EfficientNet, which utilize depthwise convolution for feature
extraction, have been developed for these devices. However, CIM macros often
face challenges in accelerating depthwise convolution, including
underutilization of CIM memory and heavy buffer traffic. The latter, in
particular, has been overlooked despite its significant impact on latency and
energy consumption. To address this, we introduce a novel CIM dataflow that
significantly reduces buffer traffic by maximizing data reuse and improving
memory utilization during depthwise convolution. The proposed dataflow is
grounded in solid theoretical principles, fully demonstrated in this paper.
When applied to MobileNet and EfficientNet models, our dataflow reduces buffer
traffic by 77.4-87.0%, leading to a total reduction in data traffic energy and
latency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline
(conventional weight-stationary dataflow).

</details>


### [110] [Wit-HW: Bug Localization in Hardware Design Code via Witness Test Case Generation](https://arxiv.org/abs/2508.14414)
*Ruiyang Ma,Daikang Kuang,Ziqian Liu,Jiaxi Zhang,Ping Fan,Guojie Luo*

Main category: cs.AR

TL;DR: 提出Wit-HW框架，通过生成有效的见证测试用例来增强硬件bug定位，显著优于现有技术


<details>
  <summary>Details</summary>
Motivation: 现有硬件调试技术仅使用单个bug触发测试用例，无法有效分析复杂硬件系统和找出bug根本原因

Method: 将硬件bug定位问题转化为测试生成问题，使用基于频谱的方法分析通过和失败测试用例的执行差异，采用基于突变的策略生成有效见证测试用例

Result: 在41个bug上，Wit-HW在Top-1、Top-5、Top-10排名中分别有效定位49%、73%、88%的bug，显著优于最先进技术

Conclusion: Wit-HW框架通过生成见证测试用例有效提升了硬件bug定位能力，在真实项目中展现出稳健性能

Abstract: Debugging hardware designs requires significant manual effort during hardware
development. After engineers identify a bug-triggering test case in
simulation-based hardware verification, they usually spend considerable time
analyzing the execution trace to localize the bug. Although numerous automated
hardware debugging techniques exist, they are not applicable to large designs
and deep bugs. A primary reason for their limitations is that these techniques
only utilize the information of a single bug-triggering test case for bug
localization, which prevents them from effectively analyzing intricate hardware
systems and figure out the root cause of bugs. To solve this problem, in this
paper, we transform the hardware bug localization problem into a test
generation problem, aiming to find a set of effective witness test cases beyond
the initial bug-triggering test case to enhance hardware bug localization.
Witness test cases refer to the cases that do not trigger the bug in the faulty
design. By analyzing the execution differences between passing and failing test
cases with spectrum-based method, we can eliminate innocent design statements
and localize the buggy ones. To further refine the suspicious area, we define
the criteria for effective witness test cases and use a mutation-based strategy
to generate such test cases. Based on this approach, we propose an automated
hardware bug localization framework named Wit-HW. We evaluate Wit-HW on 41 bugs
from various hardware designs. The experimental results show that Wit-HW
effectively localize 49%, 73%, 88% bugs within Top-1, Top-5, Top-10 ranks,
significantly outperforming state-of-the-art bug localization techniques.
Additionally, we evaluate Wit-HW on 13 real-world bugs collected from
open-source hardware projects, showcasing the robust performance of our method.

</details>


### [111] [An Open-Source HW-SW Co-Development Framework Enabling Efficient Multi-Accelerator Systems](https://arxiv.org/abs/2508.14582)
*Ryan Albert Antonio,Joren Dumoulin,Xiaoling Yi,Josse Van Delm,Yunhao Deng,Guilherme Paim,Marian Verhelst*

Main category: cs.AR

TL;DR: SNAX是一个开源的HW-SW集成框架，通过新颖的混合耦合方案实现高效的多加速器平台，在低功耗异构SoC中实现10倍以上的神经网络性能提升和90%以上的加速器利用率。


<details>
  <summary>Details</summary>
Motivation: 异构加速器计算集群是AI工作负载的高效解决方案，但当前的集成策略往往牺牲数据移动效率并存在硬件软件兼容性问题，缺乏统一平衡性能和易用性的方法。

Method: 提出SNAX框架，采用松散耦合异步控制和紧密耦合数据访问的混合耦合方案，提供可重用硬件模块和基于MLIR的可定制编译器，自动化关键系统管理任务。

Result: 在低功耗异构SoC中，SNAX实现了10倍以上的神经网络性能提升，同时在全系统运行中保持90%以上的加速器利用率，易于集成和编程。

Conclusion: SNAX框架能够快速开发和部署定制的多加速器计算集群，有效解决了异构加速器集成中的数据移动效率和兼容性问题。

Abstract: Heterogeneous accelerator-centric compute clusters are emerging as efficient
solutions for diverse AI workloads. However, current integration strategies
often compromise data movement efficiency and encounter compatibility issues in
hardware and software. This prevents a unified approach that balances
performance and ease of use. To this end, we present SNAX, an open-source
integrated HW-SW framework enabling efficient multi-accelerator platforms
through a novel hybrid-coupling scheme, consisting of loosely coupled
asynchronous control and tightly coupled data access. SNAX brings reusable
hardware modules designed to enhance compute accelerator utilization, and its
customizable MLIR-based compiler to automate key system management tasks,
jointly enabling rapid development and deployment of customized
multi-accelerator compute clusters. Through extensive experimentation, we
demonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.
Accelerators can easily be integrated and programmed to achieve > 10x
improvement in neural network performance compared to other accelerator systems
while maintaining accelerator utilization of > 90% in full system operation.

</details>


### [112] [ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems](https://arxiv.org/abs/2508.14798)
*Soumyo Bhattacharjee,Federico Villani,Christian Vogt,Andrea Cossettini,Luca Benini*

Main category: cs.AR

TL;DR: 开源JESD204B接收器IP核心，为超声系统提供高速、低延迟、低资源占用的解决方案


<details>
  <summary>Details</summary>
Motivation: 传统LVDS接口无法满足超声系统对高速、多通道同步的需求，而商业JESD204B IP核心存在专利、成本高、资源占用大等问题

Method: 使用可综合SystemVerilog开发开源接收器IP核心，支持4个12.8Gb/s GTH/GTY通道，采用模块化数据路径设计，包括每辆弹性缓冲区、SYSREF锁定LMFC生成器和可选LFSR解加密功能

Result: 仅占用107个配置逻辑块（约437个LUT），资源占用比商业IP减少79%，支持周期准确AXI-Stream数据输出和确定性Subclass 1延迟，通过30分钟稳定流水测试

Conclusion: ListenToJESD204B为高性能超声系统提供了一个高效、经济、可扩展的开源JESD204B接收解决方案，在协议兼容性、性能和稳定性方面都表现优异

Abstract: The demand for hundreds of tightly synchronized channels operating at tens of
MSPS in ultrasound systems exceeds conventional low-voltage differential
signaling links' bandwidth, pin count, and latency. Although the JESD204B
serial interface mitigates these limitations, commercial FPGA IP cores are
proprietary, costly, and resource-intensive. We present ListenToJESD204B, an
open-source receiver IP core released under a permissive Solderpad 0.51 license
for AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable
SystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides
cycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It
occupies only 107 configurable logic blocks (approximately 437 LUTs),
representing a 79\% reduction compared to comparable commercially available IP.
A modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC
generation, and optional LFSR descrambling facilitates scaling to high lane
counts. We verified protocol compliance through simulation against the Xilinx
JESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block
stability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s
links for 30 minutes with no errors.

</details>
