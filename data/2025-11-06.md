<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 72]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 本文总结了EuroHPC JU CEEC项目在测量、分析和优化欧洲主要HPC系统能耗方面的经验，通过CFD应用案例展示了加速器和混合精度技术在降低能耗方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算系统面临能效挑战，计算需求增长和架构复杂性导致显著的能源足迹，需要系统性的能耗测量和优化方法。

Method: 使用代表性CFD应用（waLBerla、FLEXI/GALÆXI、Neko、NekRS）在多种架构上进行案例研究，评估能耗和计算时间指标，包括CPU和GPU分区。

Result: 结果表明加速器和混合精度技术能够显著降低能耗，同时保持计算精度，在不同HPC系统上验证了能效提升效果。

Conclusion: 需要在HPC系统上促进能耗测量，以提高意识、教育社区，并采取行动实现更可持续的百亿亿次计算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [2] [Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots](https://arxiv.org/abs/2511.03286)
*Ehud Shapiro*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Global digital platforms are software systems designed to serve entire
populations, with some already serving billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes communication impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they satisfy basic correctness
properties. We discuss informally additional global platforms -- currencies,
``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus

</details>


### [3] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: UMDAM提出了一种统一的内存亲和性数据布局和DRAM地址映射方案，专门针对NPU-PIM协同执行优化，显著提升边缘设备上LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: LLM在边缘设备NPU上部署时，解码阶段内存密集度高，性能受限。NPU-PIM协同执行面临数据布局不匹配、带宽损失和冗余存储等挑战。

Method: 采用列主序、基于分块的数据布局和可配置的DRAM映射策略，确保与NPU计算兼容的同时最大化PIM效率，不引入额外内存开销或带宽损失。

Result: 在OPT模型上的评估显示，UMDAM将首token时间(TTFT)降低达3.0倍，末token时间(TTLT)降低2.18倍。

Conclusion: UMDAM显著提升了边缘设备上端到端LLM推理效率，为NPU-PIM协同执行提供了有效的解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [4] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [5] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文提出了一种基于谱空间的拓扑编码方法，用于分析分布式计算中基于轮次的全信息对手模型，并给出了无色任务可解性的特征化。


<details>
  <summary>Details</summary>
Motivation: 统一分布式计算中的拓扑方法，为消息对手模型提供更一般的可计算性理论框架。

Method: 使用谱空间和Alexandrov拓扑来表示分布式协议的执行状态，通过投影极限定义极限对象，并应用Stone对偶性。

Result: 得到了一个通用的分布式可计算性定理：当且仅当存在与Δ兼容的谱映射时，无色任务在紧致对手下可解。

Conclusion: 该框架统一了许多已知的无色可计算性定理，并解释了彩色和非彩色模型具有相同计算能力的原因。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [6] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文扩展了分布式计算中任务可解性的拓扑特征，将无色可计算性定理推广到输入相关对手模型，并给出了k-集合协议在条件基核心相关对手下的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定对手模型，而实际分布式系统中对手行为可能依赖于输入配置。本文旨在研究输入相关对手模型下的任务可解性特征。

Method: 使用拓扑框架和几何构造方法，将CG-24的特征推广到输入相关对手，分析核心弹性对手的计算能力等价性，并建立条件基核心相关对手下k-集合协议的可解性特征。

Result: 证明了输入相关对手的可计算性特征，核心弹性对手在IIS模型中的计算能力等价性，以及k-集合协议在条件基核心相关对手下的充要可解条件。

Conclusion: 本文为分布式任务的可解性提供了更一般的拓扑特征，特别在输入相关对手和条件基对手设置下建立了完整的理论框架，简化了证明过程而不改变任务的计算能力。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [LogicSparse: Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators](https://arxiv.org/abs/2511.03079)
*Changhong Li,Biswajit Basu,Shreejith Shanker*

Main category: cs.AR

TL;DR: 该论文提出了一个将非结构化稀疏性嵌入数据流加速器的框架，无需专用稀疏引擎即可保持并行性，在LeNet-5上实现了51.6倍压缩和1.23倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型在资源受限的边缘设备上性能受限，虽然量化和剪枝可以缓解这一问题，但非结构化稀疏性由于不规则内存访问而未被充分利用。

Method: 引入一个将非结构化稀疏性嵌入数据流加速器的框架，无需专用稀疏引擎即可保持并行性，并采用硬件感知的剪枝策略进一步提高效率。

Result: 在LeNet-5上，该框架仅使用5.12%的LUT就实现了51.6倍的压缩和1.23倍的吞吐量提升，有效利用了非结构化稀疏性进行QNN加速。

Conclusion: 该框架成功地将非结构化稀疏性嵌入数据流加速器，显著提升了QNN在FPGA上的推理效率和资源利用率。

Abstract: FPGAs have been shown to be a promising platform for deploying Quantised
Neural Networks (QNNs) with high-speed, low-latency, and energy-efficient
inference. However, the complexity of modern deep-learning models limits the
performance on resource-constrained edge devices. While quantisation and
pruning alleviate these challenges, unstructured sparsity remains
underexploited due to irregular memory access. This work introduces a framework
that embeds unstructured sparsity into dataflow accelerators, eliminating the
need for dedicated sparse engines and preserving parallelism. A hardware-aware
pruning strategy is introduced to improve efficiency and design flow further.
On LeNet-5, the framework attains 51.6 x compression and 1.23 x throughput
improvement using only 5.12% of LUTs, effectively exploiting unstructured
sparsity for QNN acceleration.

</details>


### [8] [An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM](https://arxiv.org/abs/2511.03203)
*Deyang Yu,Chenchen Liu,Chuanjie Zhang,Xiao Fang,Weisheng Zhao*

Main category: cs.AR

TL;DR: 提出了一种基于自旋轨道转矩MRAM的事件驱动脉冲处理计算内存宏，采用混合串并联单元结构实现高效矩阵向量乘法，显著提升了能效。


<details>
  <summary>Details</summary>
Motivation: 现有MRAM计算内存设计依赖复杂的模拟电路，导致能耗过高，需要更高效的解决方案。

Method: 使用SOT-MRAM交叉阵列，采用混合串并联单元结构支持矩阵向量乘法，通过轻量级电路将信号编码为脉冲，避免传统模拟电路。

Result: 在28nm工艺下实现，峰值能效达到243.6 TOPS/W，显著优于现有设计。

Conclusion: 该SOT-MRAM计算内存宏通过事件驱动脉冲处理和混合结构设计，实现了高能效的计算内存解决方案。

Abstract: The application of Magnetic Random-Access Memory (MRAM) in
computing-in-memory (CIM) has gained significant attention. However, existing
designs often suffer from high energy consumption due to their reliance on
complex analog circuits for computation. In this work, we present a Spin-Orbit-
Torque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking
processing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid
series-parallel cell structure to efficiently support matrix-vector
multiplication (MVM). Signal information is (en) decoded as spikes using
lightweight circuits, eliminating the need for conventional area- and
powerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in
28nm technology, and experimental results show that it achieves a peak energy
efficiency of 243.6 TOPS/W, significantly outperforming existing designs.

</details>


### [9] [Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics](https://arxiv.org/abs/2511.03427)
*Florentia Afentaki,Maha Shatta,Konstantinos Balaskas,Georgios Panagopoulos,Georgios Zervakis,Mehdi B. Tahoori*

Main category: cs.AR

TL;DR: 提出了首个柔性电子中的混合核混合信号SVM设计，通过联合优化方法平衡线性核和RBF核的优势，在保持高精度的同时大幅降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 柔性电子技术受限于大特征尺寸，难以实现高集成度的机器学习电路。现有SVM设计在硬件成本和精度之间存在权衡：线性核硬件开销小但性能差，RBF核精度高但硬件成本过高。

Method: 采用混合核和混合信号设计，通过联合优化方法训练混合核SVM，并将二元SVM分类器映射到合适的核函数（线性/RBF）和域（数字/模拟），在最大化精度的同时减少昂贵的RBF分类器数量。

Result: 相比最先进的单核线性SVM，精度提高7.7%；相比数字RBF实现，面积和功耗分别平均降低108倍和17倍。

Conclusion: 混合核混合信号SVM设计成功平衡了柔性电子中硬件成本与精度的权衡，为柔性电子实现机器学习电路提供了可行方案。

Abstract: Flexible Electronics (FE) have emerged as a promising alternative to
silicon-based technologies, offering on-demand low-cost fabrication,
conformality, and sustainability. However, their large feature sizes severely
limit integration density, imposing strict area and power constraints, thus
prohibiting the realization of Machine Learning (ML) circuits, which can
significantly enhance the capabilities of relevant near-sensor applications.
Support Vector Machines (SVMs) offer high accuracy in such applications at
relatively low computational complexity, satisfying FE technologies'
constraints. Existing SVM designs rely solely on linear or Radial Basis
Function (RBF) kernels, forcing a trade-off between hardware costs and
accuracy. Linear kernels, implemented digitally, minimize overhead but
sacrifice performance, while the more accurate RBF kernels are prohibitively
large in digital, and their analog realization contains inherent functional
approximation. In this work, we propose the first mixed-kernel and mixed-signal
SVM design in FE, which unifies the advantages of both implementations and
balances the cost/accuracy trade-off. To that end, we introduce a
co-optimization approach that trains our mixed-kernel SVMs and maps binary SVM
classifiers to the appropriate kernel (linear/RBF) and domain (digital/analog),
aiming to maximize accuracy whilst reducing the number of costly RBF
classifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art
single-kernel linear SVMs, and reduce area and power by 108x and 17x on average
compared to digital RBF implementations.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [NF-SecRIS: RIS-Assisted Near-Field Physical Layer Security via Secure Location Modulation](https://arxiv.org/abs/2511.02949)
*Zhendong Wang,Chenyang Meng,Jun Yang,Jiayuan Wang,Yin Li,Linshan Jiang,Jin Zhang*

Main category: cs.ET

TL;DR: 提出了NF-SecRIS系统，这是首个基于超大规模可重构智能表面的近场通信系统，实现了距离-角度二维物理层安全。


<details>
  <summary>Details</summary>
Motivation: 6G无线网络对物理层安全通信要求极高，现有方案通常只能在角度维度实现一维物理层安全，无法在距离维度实现安全。

Method: 提出安全位置调制方案，以极低复杂度合成RIS的近场时空编码模式，确保只有合法用户能接收原始星座图，而其他位置或角度的窃听者只能接收混淆星座图。

Result: 实验结果显示合法用户的误码率低于10^{-4}，而其他距离或角度的窃听者误码率超过40%。

Conclusion: 验证了近场通信中二维物理层安全的实现。

Abstract: The 6G wireless networks impose extremely high requirements on physical layer
secure communication. However, the existing solutions usually can only achieve
one-dimensional physical layer security (PLS) in the angle dimension, and
cannot achieve PLS in the range dimension. In this paper, we propose the
NF-SecRIS system, the first range-angle-dependent (2D) PLS near-field
communication system based on ultra-large-scale reconfigurable intelligent
surface (RIS). We propose the secure location modulation scheme to synthesize
the near-field spatial-temporal coding pattern of RIS with extremely low
complexity. It ensures that only legitimate user can receive the raw
constellations, while potential eavesdroppers at other ranges or angles can
only receive the obfuscated constellations. NF-SecRIS operates without
requiring synchronization with either transmitter or receiver. We implement a
prototype of NF-SecRIS and conduct comprehensive experiments with multiple
modulation schemes. The results show that the bit error rate (BER) of
legitimate user is below 10^{-4}, while eavesdroppers at other ranges or angles
suffer from BER exceeding 40%. It validates the implementation of 2D PLS in
near-field communications.

</details>


### [11] [QAGT-MLP: An Attention-Based Graph Transformer for Small and Large-Scale Quantum Error Mitigation](https://arxiv.org/abs/2511.03119)
*Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.ET

TL;DR: QAGT-MLP是一种基于注意力机制的图变换器，用于量子误差缓解，通过融合全局结构和局部光锥邻域特征来预测噪声缓解后的量子电路输出值。


<details>
  <summary>Details</summary>
Motivation: 现有量子误差缓解方法存在执行或校准开销大、难以扩展到大规模深度电路的问题，需要一种既准确又简单高效的误差缓解技术。

Method: 将量子电路编码为图结构，节点代表门实例，边捕获量子比特连接性和因果邻接关系，使用双路径注意力模块提取全局结构上下文和局部光锥上下文特征，结合电路级描述符特征和噪声期望值，通过轻量级MLP预测噪声缓解值。

Result: 在100量子比特的Trotter化一维横向场伊辛模型电路上，QAGT-MLP在平均误差和误差变异性方面优于现有学习方法，在相同测量预算下展现出强大的有效性和实际应用性。

Conclusion: QAGT-MLP通过注意力机制融合全局和局部特征，在不增加噪声缩放或资源需求的情况下实现高质量误差缓解，为现代和未来量子工作负载提供了可扩展且实用的误差缓解路径。

Abstract: Noisy quantum devices demand error-mitigation techniques to be accurate yet
simple and efficient in terms of number of shots and processing time. Many
established approaches (e.g., extrapolation and quasi-probability cancellation)
impose substantial execution or calibration overheads, while existing
learning-based methods have difficulty scaling to large and deep circuits. In
this research, we introduce QAGT-MLP: an attention-based graph transformer
tailored for small- and large-scale quantum error mitigation (QEM). QAGT-MLP
encodes each quantum circuit as a graph whose nodes represent gate instances
and whose edges capture qubit connectivity and causal adjacency. A dual-path
attention module extracts features around measured qubits at two scales or
contexts: 1) graph-wide global structural context; and 2) fine-grained local
lightcone context. These learned representations are concatenated with
circuit-level descriptor features and the circuit noisy expected values, then
they are passed to a lightweight MLP to predict the noise-mitigated values. On
large-scale 100-qubit Trotterized 1D Transverse-Field Ising Models -- TFIM
circuits -- the proposed QAGT-MLP outperformed state-of-the-art learning
baselines in terms of mean error and error variability, demonstrating strong
validity and applicability in real-world QEM scenarios under matched shot
budgets. By using attention to fuse global structures with local lightcone
neighborhoods, QAGT-MLP achieves high mitigation quality without the increasing
noise scaling or resource demand required by classical QEM pipelines, while
still offering a scalable and practical path to QEM in modern and future
quantum workloads.

</details>


### [12] [LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol](https://arxiv.org/abs/2511.03706)
*Yu-Erh Pan,Ayesha Siddika Nipu*

Main category: cs.ET

TL;DR: 提出了一种基于LLM增强的空气监测界面（AMI），通过Model Context Protocol（MCP）集成实时传感器数据和对话界面，有效减少LLM幻觉问题，提供准确的环境监测服务。


<details>
  <summary>Details</summary>
Motivation: 传统空气质量监测系统难以被非专业用户理解，存在可视化复杂、交互性有限和部署成本高等问题。LLM虽然提供了新机会，但在安全关键领域存在幻觉风险。

Method: 采用Django后端、响应式用户仪表板和安全的MCP服务器架构，将系统功能作为可发现工具暴露给LLM，使其成为主动操作者而非被动响应者。

Result: 专家评估显示高事实准确性（4.78/5）、完整性（4.82/5）和最小化幻觉（4.84/5），支持者间可靠性分析。

Conclusion: 将LLM与标准化工具协议结合，可为实时环境监测创建可靠、安全且用户友好的界面。

Abstract: Air quality monitoring is central to environmental sustainability and public
health, yet traditional systems remain difficult for non-expert users to
interpret due to complex visualizations, limited interactivity, and high
deployment costs. Recent advances in Large Language Models (LLMs) offer new
opportunities to make sensor data more accessible, but their tendency to
produce hallucinations limits reliability in safety-critical domains. To
address these challenges, we present an LLM-enhanced Air Monitoring Interface
(AMI) that integrates real-time sensor data with a conversational interface via
the Model Context Protocol (MCP). Our system grounds LLM outputs in live
environmental data, enabling accurate, context-aware responses while reducing
hallucination risk. The architecture combines a Django-based backend, a
responsive user dashboard, and a secure MCP server that exposes system
functions as discoverable tools, allowing the LLM to act as an active operator
rather than a passive responder. Expert evaluation demonstrated high factual
accuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a
scale of 5, supported by inter-rater reliability analysis. These results
highlight the potential of combining LLMs with standardized tool protocols to
create reliable, secure, and user-friendly interfaces for real-time
environmental monitoring.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [13] [PerfDojo: Automated ML Library Generation for Heterogeneous Architectures](https://arxiv.org/abs/2511.03586)
*Andrei Ivanov,Siyuan Shen,Gioele Gottardo,Marcin Chrapek,Afif Boudaoud,Timo Schneider,Luca Benini,Torsten Hoefler*

Main category: cs.PF

TL;DR: PerfLLM是一个利用大语言模型和强化学习的自动优化方法，通过人类可读的代码表示实现跨硬件架构的性能优化。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型复杂度增加和硬件架构多样化使得性能优化变得困难，现有方法依赖硬件特定启发式方法且缺乏可解释性。

Method: 提出PerfLLM方法，结合LLM和RL，使用PerfDojo环境将优化构建为RL游戏，采用数学启发的代码表示保证语义有效性。

Result: 在多种CPU（x86、Arm、RISC-V）和GPU架构上实现了显著的性能提升。

Conclusion: PerfLLM能够在无需先验硬件知识的情况下实现有效的性能优化，促进人类分析和RL代理训练。

Abstract: The increasing complexity of machine learning models and the proliferation of
diverse hardware architectures (CPUs, GPUs, accelerators) make achieving
optimal performance a significant challenge. Heterogeneity in instruction sets,
specialized kernel requirements for different data types and model features
(e.g., sparsity, quantization), and architecture-specific optimizations
complicate performance tuning. Manual optimization is resource-intensive, while
existing automatic approaches often rely on complex hardware-specific
heuristics and uninterpretable intermediate representations, hindering
performance portability. We introduce PerfLLM, a novel automatic optimization
methodology leveraging Large Language Models (LLMs) and Reinforcement Learning
(RL). Central to this is PerfDojo, an environment framing optimization as an RL
game using a human-readable, mathematically-inspired code representation that
guarantees semantic validity through transformations. This allows effective
optimization without prior hardware knowledge, facilitating both human analysis
and RL agent training. We demonstrate PerfLLM's ability to achieve significant
performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: FATE是一个新的形式代数定理评估基准系列，包含FATE-H和FATE-X两个组件，各100个问题，难度从本科练习到超过博士资格考试水平。评估显示当前LLM在此基准上表现远差于竞赛数学，最佳模型在FATE-H上仅3%准确率，FATE-X上为0%。


<details>
  <summary>Details</summary>
Motivation: 现有基于竞赛的数学基准无法反映现代数学研究的深度、广度和抽象性，需要创建更接近研究水平的数学推理基准。

Method: 引入FATE基准系列，包含FATE-H和FATE-X两个组件，各100个抽象代数和交换代数问题，难度跨度从本科到博士水平。采用两阶段评估方法，分别评估自然语言推理和形式化推理能力。

Result: 最佳LLM模型在FATE-H上仅达到3%准确率（pass@64），在FATE-X上为0%。模型自然语言推理能力显著优于形式化推理能力。专门化证明器在自然语言阶段表现不如通用模型。

Conclusion: FATE为研究级形式数学推理提供了稳健且具有挑战性的基准，揭示了当前LLM在高级数学推理方面的显著差距。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [15] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm是一个用于动态群组形成的框架，通过轻量级图卷积网络捕捉高阶用户信息，支持实时群组形成和动态调整群组数量。


<details>
  <summary>Details</summary>
Motivation: 现有的群组推荐系统主要关注推荐准确性，但假设群组是静态或预定义的，不适合动态的现实场景。因此需要解决群组形成这一核心挑战。

Method: 使用轻量级GCN架构捕捉高阶结构信号，通过随机聚类学习实现无需重新训练的自适应群组重构，并利用对比学习在动态条件下优化群组。

Result: 在多个数据集上的实验表明，DeepForm在群组形成质量、效率和推荐准确性方面均优于各种基线方法。

Conclusion: DeepForm框架成功解决了动态群组形成问题，在满足实时性和自适应需求的同时，提升了群组推荐系统的整体性能。

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [16] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体LLM的模拟电路尺寸设计框架AnaFlow，通过智能体协作实现高效、可解释的自动化电路设计，解决了传统方法仿真成本高和缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计主要依赖手工过程，设计周期长且易出错。现有AI方法需要大量耗时仿真且缺乏可解释性，阻碍了工具的广泛应用。

Method: 采用多智能体工作流，专门化的LLM智能体协作解释电路拓扑、理解设计目标，并通过可解释的推理迭代优化电路设计参数，结合自适应仿真策略提高样本效率。

Result: AnaFlow框架在两个复杂度不同的电路上验证，能够完全自动完成尺寸设计任务，优于纯贝叶斯优化和强化学习方法，并能从优化历史中学习避免错误、加速收敛。

Conclusion: 该框架为模拟设计空间探索提供了强大工具，代表了模拟EDA的新范式，其中AI智能体作为透明的设计助手发挥作用。

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [17] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 通过预训练在公开ARC任务上的小规模递归模型，可以在计算限制内高效微调，在竞赛中达到6.67%的得分。


<details>
  <summary>Details</summary>
Motivation: 解决TRM方法在ARC竞赛中计算资源超限的问题，探索在允许计算限制内实现有效微调的方法。

Method: 首先在1280个公开任务上预训练7M参数递归神经网络，然后在竞赛任务上进行12500步梯度更新的完整微调。

Result: 预训练模型在公开评估集上获得约10%得分，微调后在半私有评估任务上达到6.67%的得分。

Conclusion: 从预训练的小型递归模型出发，通过完整微调而非LoRA或任务嵌入微调，可以在计算限制内实现有效的竞赛表现。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [18] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 提出AI辅助框架预测北印度洋潜在渔区，使用海面温度和叶绿素浓度等海洋参数，旨在提高渔区识别准确性并支持可持续渔业。


<details>
  <summary>Details</summary>
Motivation: 北印度洋（包括阿拉伯海和孟加拉湾）是沿海社区重要生计来源，但渔民常面临难以定位高产渔区的不确定性。

Method: 使用AI辅助框架，基于海面温度和叶绿素浓度等海洋学参数预测潜在渔区。

Result: 初步结果表明该框架能帮助渔民减少搜索时间、降低燃料消耗并促进高效资源利用。

Conclusion: 该AI框架能有效提升渔区预测准确性，为可持续渔业实践提供区域特定见解。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [19] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: 提出了一种结合数字孪生和图神经网络的统一框架，用于可扩展的、数据驱动的路面健康监测和预测性维护，通过图结构建模路面段和空间关系，实现非线性能退化预测和主动干预。


<details>
  <summary>Details</summary>
Motivation: 传统路面管理系统主要被动响应，缺乏实时智能用于故障预防和最优维护规划，需要解决复杂空间依赖、变化环境条件和非线性退化等问题。

Method: 将路面段和空间关系建模为图节点和边，利用实时无人机、传感器和激光雷达数据流输入数字孪生，使用归纳图神经网络从图结构输入中学习退化模式。

Result: 在真实世界启发数据集上训练，模型R2达到0.3798，优于基线回归器，有效捕捉非线性退化，并开发了交互式仪表板和强化学习模块。

Conclusion: 数字孪生与图神经网络集成提高了预测精度，建立了持续改进的闭环反馈循环，为主动、智能和可持续的路面管理奠定了基础。

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [20] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 提出使用大语言模型进行零样本、单样本和少样本学习，通过角色扮演和逐步推理来检测和清理可穿戴物联网系统中的人类活动识别数据中毒攻击。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备在物联网中的广泛应用需要可靠的人类活动识别技术，但机器学习模型容易受到数据中毒攻击。传统防御方法需要大量标注数据和特定任务训练，难以适应动态的物联网环境。

Method: 使用大语言模型进行中毒检测和清理，采用角色扮演提示让LLM作为专家评估传感器异常，以及逐步推理指导LLM推断中毒指标和可能的清洁替代方案。

Result: 框架在检测准确性、清理质量、延迟和通信成本方面进行了广泛评估，证明了LLM在提高可穿戴物联网系统安全性和可靠性方面的实用性。

Conclusion: 该方法减少了对大量数据集的依赖，实现了实时、稳健且适应性强的防御机制，展示了LLM在保护可穿戴物联网系统安全方面的有效性。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [21] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 使用Llama 3.1-405B大语言模型对引用特定基因组数据集的科学文献进行零样本数据使用案例分类，无需人工标注即可生成结构化标签，F1分数达到0.674。


<details>
  <summary>Details</summary>
Motivation: 近年来需要识别数据集与引用它们的科学文献之间的关联，了解数据如何被使用。传统方法需要昂贵的人工标注和训练数据集开发，而预训练大语言模型提供了规模化描述数据使用案例的潜在途径。

Method: 应用开源LLM Llama 3.1-405B对已知引用特定基因组数据集的出版物生成结构化数据使用案例标签，采用零样本分类方法，无需预定义类别，并引入了新的评估框架来验证方法效果。

Result: 原始模型在零样本数据引用分类任务中达到0.674的F1分数，表明该方法具有潜力。

Conclusion: 虽然结果有希望，但受到数据可用性、提示过拟合、计算基础设施以及进行负责任性能评估所需费用等障碍的限制。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [22] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文针对ROGUE多臂老虎机框架提出ROGUE-TS算法，通过概率裁剪平衡个性化推荐与群体效应学习，在微随机试验中实现低遗憾和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在ROGUE框架下过度强调利用而忽视探索，限制了群体层面效应的估计能力，这在微随机试验中尤为重要。

Method: 开发了ROGUE-TS汤普森采样算法，并引入概率裁剪程序来平衡个性化与群体学习，量化了遗憾与最小探索概率之间的权衡。

Result: 在两个微随机试验数据集上的验证表明，该方法比现有方法获得更低遗憾，并通过裁剪程序保持高统计功效而不显著增加遗憾。

Conclusion: 该框架为设计微随机试验的研究人员提供了平衡个性化与统计有效性的实用指导，能够可靠检测治疗效果同时考虑个体行为动态。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [23] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: 提出UserAlign方法，通过少量成对响应比较来获取用户偏好，基于逻辑赌博机理论框架从固定响应池中选择个性化响应，实现生成模型与用户偏好的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有个性化对齐方法要么需要大量用户偏好查询，要么要求偏好以文本形式明确指定，这在实际应用中存在限制。

Method: 基于逻辑赌博机中的最佳臂识别理论框架，将用户反馈视为一致且无噪声的，通过少量成对响应比较快速识别最佳响应。

Result: 在个性化文本和图像生成等多个任务上的实验结果表明，UserAlign在实现个性化对齐方面具有有效性。

Conclusion: UserAlign提供了一种高效的推理时个性化对齐方法，仅需少量用户偏好查询即可实现生成模型与用户偏好的有效对齐。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [24] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 将期望信息价值(EVOI)集成到Bootstrapped DQN框架中，通过衡量不同网络头之间的意见差异来指导探索，在稀疏奖励的复杂环境中提升探索效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于随机局部策略噪声的探索策略（如ε-greedy和Boltzmann探索）在高维状态和稀疏奖励环境中难以有效平衡探索与利用。

Method: 开发了两种新算法，将期望信息价值集成到Bootstrapped DQN中，利用信息价值估计衡量不同网络头之间的意见差异，引导探索到最有潜力的区域。

Result: 在复杂稀疏奖励的Atari游戏中表现出更高的性能，更好地利用了随机网络初始化产生的不确定性，且没有引入额外超参数。

Conclusion: 基于期望信息价值的探索方法能够有效提升深度强化学习在稀疏奖励环境中的探索能力，同时保持算法简洁性。

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [25] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的超材料设计框架，通过多尺度神经表示学习连续的两尺度结构表示，解决了异质超材料设计中的兼容性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统超材料设计方法面临巨大设计空间和相邻单元兼容性要求，数据驱动方法受限于固定微结构库且需要额外后处理。需要一种能自动确保单元间无缝连接的设计框架。

Method: 使用多尺度神经表示，神经网络同时输入全局和局部坐标，输出表示多尺度结构的隐式场。在训练中使用兼容性损失项强制相邻单元间的连接性。

Result: 该方法能够产生任意高分辨率的超材料设计，实现无限上采样用于制造或仿真。在力学超材料设计、负泊松比和力学隐身问题上展示了有效性。

Conclusion: 提出的神经网络框架能够生成具有兼容单元几何形状的多尺度结构，无需预定义数据集，在机器人、生物工程和航空航天领域具有应用潜力。

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [26] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: GraphBSI是一种基于贝叶斯样本推理(BSI)的单次图生成模型，通过迭代优化分布参数空间中的图信念来处理离散结构，在分子和合成图生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在分子生成、知识图谱和网络分析中很重要，但其离散、无序的特性使传统生成模型难以处理，因此需要离散扩散和流匹配模型。

Method: GraphBSI基于贝叶斯样本推理(BSI)，在分布参数空间中迭代优化图信念，而不是直接演化样本。将BSI表述为随机微分方程(SDE)，并推导出通过分数函数近似保持边缘分布的噪声控制SDE族。

Result: 在分子和合成图生成的标准基准Moses和GuacaMol上，GraphBSI表现出最先进的性能，优于现有的单次图生成模型。

Conclusion: GraphBSI提供了一种处理离散图结构的有效方法，理论分析揭示了与贝叶斯流网络和扩散模型的联系，在多个基准测试中取得了优越性能。

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [27] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 提出了自适应无传感器监控方法，通过残差校正框架修正无传感器模型的系统偏差，在温度预测上MAE从2.43°C降至2.24-2.31°C，湿度预测从7.99%降至5.72-7.09%。


<details>
  <summary>Details</summary>
Motivation: 传统无传感器监控无法整合遥测信息和校正系统误差，导致预测结果与实时数据差异显著，造成用户困惑。

Method: 引入残差校正方法，在观测到实时遥测数据后校正无传感器模型的系统偏差，称为自适应无传感器监控。

Result: 在348万数据点上评估，温度MAE降至2.24-2.31°C（基准2.43°C），湿度MAE降至5.72-7.09%（基准7.99%），RMSE也相应改善。

Conclusion: 自适应无传感器模型实现了更准确的货物监控、早期风险检测，并减少了对全球航运中完全连接的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [28] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 提出了一种新的分布优化算法DADO，能够利用设计变量的可分解性结构来提高优化效率


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的科学工程中，需要根据用户指定属性设计离散对象（如蛋白质、电路等）。当前分布优化算法无法利用设计变量的可分解性结构，限制了优化效率

Method: 使用基于连接树的软因子化搜索分布，通过图消息传递协调跨链接因子的优化

Result: DADO算法能够有效利用设计变量的可分解性，提高离散设计空间的优化效率

Conclusion: DADO为可分解设计问题提供了一种高效的分布优化方法，有望在蛋白质设计、电路布局等科学应用中发挥重要作用

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [29] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 使用Vision Transformer (ViT)架构从隐含波动率曲面预测资产未来30天的已实现波动率，探索将Transformer模型应用于期权数据的可行性。


<details>
  <summary>Details</summary>
Motivation: 金融机器学习研究表明深度学习能够学习高度非线性关系，在金融预测中优于简单方法。虽然Transformer架构如Informer在金融时间序列预测中表现出潜力，但将其应用于期权数据的研究仍较少。

Method: 训练Vision Transformer (ViT)架构，该架构通常用于现代图像识别和分类系统，从单日的隐含波动率曲面（增强日期信息）预测资产未来30天的已实现波动率。

Result: ViT能够从IV曲面中学习季节性模式和非线性特征，表明这是一个有前景的模型开发方向。

Conclusion: 这项初步研究表明将Transformer模型应用于期权数据具有潜力，ViT能够有效捕捉隐含波动率曲面中的复杂模式。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [30] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 本文提出了首个针对目标驱动交互的无监督评估指标，利用未标注交互数据的统计特性和微调LLM来适应分布变化，无需人工标注即可评估用户目标、目标完成度和LLM不确定性。


<details>
  <summary>Details</summary>
Motivation: 企业应用中LLM驱动的AI代理与人类进行目标驱动交互时，评估面临诸多挑战：数据复杂且无标注、人工标注难以规模化、定制指标无法检测未知错误、LLM评估结果不可靠。

Method: 利用未标注交互数据的统计特性，通过微调LLM来适应分布变化，开发了用户目标标注、目标完成度测量和LLM不确定性量化的无监督指标。

Result: 在开放域和任务特定交互数据上验证了方法的有效性，能够在不依赖人工生成理想响应的情况下进行可靠评估。

Conclusion: 提出的无监督评估框架为解决目标驱动交互系统的评估难题提供了可行方案，特别是在缺乏标注数据的情况下仍能进行有效评估。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [31] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 本文提出了一个几何框架来理解基于Transformer的语言模型，将其与广义相对论进行类比。注意力机制在表示空间中诱导出有效度量，注意力作为离散连接实现值向量在token间的并行传输。通过实验验证了嵌入空间曲率的存在及其影响。


<details>
  <summary>Details</summary>
Motivation: 将Transformer架构与广义相对论进行类比，为理解语言模型提供新的几何视角，探索注意力机制如何塑造表示空间的曲率结构。

Method: 设计三个实验：(1)可视化完整段落的曲率景观；(2)通过模拟验证尖锐/平坦角度和长度-弦比异常；(3)受爱因斯坦日食实验启发，在受控上下文编辑下探测嵌入轨迹的偏转。

Result: 实验证实了嵌入空间曲率的存在：局部转向角度在token和层间变化；尖锐/平坦角度和长度-弦比异常无法用维度或偶然性解释；上下文编辑导致可测量的、意义一致的嵌入轨迹弯曲。

Conclusion: Transformer语言模型确实表现出类似广义相对论的几何特性，注意力机制诱导的曲率塑造了token嵌入在表示空间中的演化轨迹。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [32] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: 提出了一种基于图同态失真的图相似性度量方法，能够完全表征图结构，并可作为完整的图嵌入。通过采样方法高效计算该度量，且该度量具有度量性质。


<details>
  <summary>Details</summary>
Motivation: 长期以来，图神经网络的表达能力仅通过组合性质来衡量，这限制了图相似性度量的发展。本文旨在提供一种原则性的方法来度量带顶点属性图之间的相似性。

Method: 提出图同态失真度量，通过采样方法高效计算该度量，避免了图规范化问题。该度量在期望上确保完整性，并能转化为度量空间。

Result: 图同态失真在BREC数据集上完全区分了4-WL无法区分的图，在ZINC-12k数据集上优于先前基于同态的方法。

Conclusion: 图同态失真为图的表征开辟了新途径，将图论传统扩展到新的前沿领域。

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [33] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 提出了MSUCB算法，将中位数均值估计器首次应用于带腐败的bandit设置，在无腐败时实现最优对数遗憾，在腐败时遗憾仅随总腐败量线性增加。


<details>
  <summary>Details</summary>
Motivation: 在线学习排序系统容易受到点击欺诈和操纵的影响，这些腐败反馈会误导学习过程并降低用户体验。

Method: 使用新颖的中位数均值估计器，在无腐败时表现如标准均值，在腐败时通过中位数步骤过滤异常值和腐败样本。

Result: 在真实世界数据集上的实验表明，该方法始终优于先前方法，对两种最先进方法的遗憾改进分别达到97.35%和91.60%。

Conclusion: MSUCB算法在无腐败时实现最优性能，在腐败时表现出优雅的性能下降，具有强大的鲁棒性。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [34] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 该论文提出了SparKer方法，通过稀疏、局部性和竞争性三个原则构建自组织局部核，用于在缺乏先验信息的情况下检测高维表示空间中的异常。


<details>
  <summary>Details</summary>
Motivation: 现代AI能够提取丰富的表示，但这些表示的统计特性难以控制，导致异常检测方法在弱信号或罕见异常情况下失效，需要开发在最小先验信息下仍能有效检测异常的方法。

Method: 提出SparKer方法，使用稀疏高斯核集成，在半监督Neyman-Pearson框架下训练，局部建模可能包含异常的样本与无异常参考样本之间的似然比。

Result: 实验表明，仅包含少量核的集成就能在数千维表示空间中识别统计显著的异常位置，证明了方法的可解释性、效率和可扩展性。

Conclusion: 基于稀疏性、局部性和竞争性三个结构原则的自组织局部核方法能够有效解决高维表示空间中的异常检测问题，在科学发现、开放世界新颖性检测等多个领域具有应用价值。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [35] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出了DiCoDe框架，通过投影通用引导和批评器蒸馏机制，解决agent-environment协同设计中的可扩展性和样本效率问题，在多个多智能体环境设计基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前协同设计方法难以扩展到高维环境设计空间，且在联合优化中面临移动目标导致的样本效率低下问题，限制了在实际应用中的部署。

Method: DiCoDe框架包含两个核心创新：投影通用引导（PUG）采样技术，用于探索满足硬约束的奖励最大化环境分布；批评器蒸馏机制，通过共享强化学习批评器知识，使扩散模型能够适应演化的智能体策略。

Result: 在仓库自动化、多智能体路径规划和风电场优化等基准测试中，方法始终优于现有技术，如在仓库设置中实现了39%更高的奖励和66%更少的模拟样本。

Conclusion: DiCoDe为agent-environment协同设计设立了新标准，是向现实世界领域应用协同设计的重要进展。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [36] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 提出改进的CTF-IDF算法和更快的IRLBA降维方法，在文本分析中替代深度学习，显著降低计算复杂度和碳足迹，同时保持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在文本分析领域广泛应用导致计算资源和能耗急剧增加，产生严重的碳足迹问题，需要寻找更高效环保的替代方案。

Method: 改进传统TF-IDF算法提出CTF-IDF，结合IRLBA算法进行降维处理，构建经典机器学习文本分析流程。

Result: 相比深度学习方法，显著降低了时间复杂度和计算资源消耗，碳足迹大幅减少，同时模型准确率仅有轻微下降。

Conclusion: 经典机器学习方法结合CTF-IDF和IRLBA可以在文本分析中有效替代深度学习，实现更高效环保的解决方案。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [37] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: 提出了一种新的梯度估计方法，通过操纵上游雅可比矩阵来减少偏差和方差，在更宽的网络中表现更好，有望扩展到大型网络。


<details>
  <summary>Details</summary>
Motivation: 反向传播需要两次前向传播和存储中间激活值，而现有的前向模式自动微分方法由于估计方差高难以扩展到小型网络之外，且现有减少方差的方法引入了显著偏差。

Method: 通过操纵上游雅可比矩阵计算猜测方向，减少梯度估计的偏差和方差。

Result: 该方法显示出有希望的结果，随着网络宽度的增加性能更好。

Conclusion: 该方法通过分析偏差和方差及其与神经网络梯度低维结构的关系，有望扩展到更大的网络。

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [38] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff是首个在整个生成轨迹中强制执行Fokker-Planck方程物理学的抗体生成器，通过FPE残差损失确保物理一致性，在CDR设计任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有抗体生成模型存在两个核心挑战：缺乏动力学一致性导致物理上不可行的结构，以及由于数据稀缺和结构偏差导致的泛化能力差。

Method: 在SE(3)-等变扩散框架中引入FPE物理约束，在CDR几何混合流形上最小化FPE残差损失，将局部学习的去噪分数组装成全局一致的概率流。

Result: 在RAbD基准测试中创下新纪录：CDR-H3设计的RMSD为0.99Å（比AbX提升25%），接触氨基酸恢复率为39.91%；六CDR协同设计中全链RMSD降低约15%，CDR-H3环的氨基酸恢复率达到45.67%。

Conclusion: 通过将生成动力学与物理定律对齐，FP-AbDiff增强了鲁棒性和泛化能力，为物理忠实且功能可行的抗体设计建立了原则性方法。

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [39] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文提出了基于增强重叠理论的自监督对比学习分析框架，揭示了数据增强如何通过增加类内样本支持重叠来促进对比学习的效果，并开发了无需额外模块的无监督表示评估指标。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习虽然取得了显著成功，但其工作机制尚不明确。现有理论大多基于条件独立性假设，这在实践中往往不成立，因此需要更实用的理论框架来解释对比学习的有效性。

Method: 首先基于条件独立性假设给出最紧边界，然后放松该假设，提出更实用的增强重叠假设，推导出下游性能的渐近闭边界。通过分析发现，激进的数据增强会使类内样本的支持区域更加重叠，从而对齐正样本（同一样本的增强视图）就能使对比学习将类内样本聚类在一起。

Result: 从增强重叠的新视角开发了对比学习表示的无监督评估指标，该指标与下游性能高度一致，几乎不需要依赖额外模块。

Conclusion: 增强重叠理论为理解对比学习的工作机制提供了新视角，所提出的无监督评估指标能够有效预测下游任务性能，为对比学习的研究和应用提供了理论指导和实用工具。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [40] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 提出了Static Deceptor (StaDec)和Dynamic Deceptor (DyDec)两种攻击框架，用于系统生成动态和自适应的对抗样本，评估LLM在敏感任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当将LLM应用于敏感任务时，需要全面评估其对抗输入的鲁棒性，但目前缺乏系统性的评估方法。

Method: 利用LLM的理解能力，通过自动化的LLM驱动流程生成语义相似但能有效欺骗目标LLM的对抗样本，无需依赖外部启发式方法。

Result: 攻击方法能随着LLM的进步而进化，并在攻击者未知的模型间表现出强迁移性。

Conclusion: 这项工作为LLM的鲁棒性自评估提供了系统性方法。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [41] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了一种名为自适应分位数重校准（AQR）的测试时适应技术，通过通道级分位数对齐来修改预激活分布，无需目标域先验知识或模型重训练，在多种架构上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统领域适应方法依赖目标域先验知识或需要模型重训练的问题，以及现有测试时适应方法无法捕获复杂激活分布且局限于特定归一化层的局限性。

Method: AQR通过通道级分位数对齐来修改预激活分布，采用鲁棒的尾部校准策略处理不同批量大小下的分布尾部估计问题，利用训练时计算的源域统计量实现无监督适应。

Result: 在CIFAR-10-C、CIFAR-100-C和ImageNet-C数据集上的实验表明，AQR在多种架构下实现了鲁棒的适应性能，优于现有的测试时适应基线方法。

Conclusion: AQR在动态和不可预测数据分布的真实场景中具有部署潜力，能够有效提升深度学习模型在分布偏移情况下的泛化能力。

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [42] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: 提出了Forecast2Anomaly (F2A)框架，通过联合预测-异常损失和检索增强生成模块，使时间序列基础模型具备零样本异常预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有异常预测方法局限于特定系统，无法适应随时间演变的异常模式；而预训练时间序列基础模型虽具有强泛化能力，但尚未用于异常预测任务。

Method: 1. 联合预测-异常损失：微调TSFMs以准确预测异常时间点的未来信号；2. 检索增强生成模块：检索历史相关时段并基于此进行预测，动态适应推理时的分布变化。

Result: 在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A始终优于最先进的方法。

Conclusion: F2A通过目标微调和动态检索的结合，弥合了鲁棒TSFM零样本预测与零样本异常预测之间的差距，为实际应用提供了可扩展的解决方案。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [43] [UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems](https://arxiv.org/abs/2511.03168)
*Tingzhu Bi,Yicheng Pan,Xinrui Jiang,Huize Sun,Meng Ma,Ping Wang*

Main category: cs.LG

TL;DR: UnCLe是一种新颖的深度学习动态因果发现方法，通过解耦器和重耦合器网络分解时间序列，使用自回归依赖矩阵学习变量间依赖关系，通过时间扰动分析动态因果影响。


<details>
  <summary>Details</summary>
Motivation: 从观测时间序列中发现因果关系对于理解复杂系统至关重要。现实世界系统通常表现出动态因果关系，即关系随时间演变，需要时间分辨的因果图来准确捕捉这些时间动态。

Method: 使用解耦器和重耦合器网络将输入时间序列分解为语义表示，通过自回归依赖矩阵学习变量间依赖关系，通过分析时间扰动引起的数据点预测误差来估计动态因果影响。

Result: 在静态因果发现基准测试中优于现有最先进方法，更重要的是在合成和真实世界动态系统（如人体运动）中能够准确捕捉和表示演化的时间因果关系。

Conclusion: UnCLe为揭示复杂现象中潜在的时变机制提供了一种有前景的方法。

Abstract: Uncovering cause-effect relationships from observational time series is
fundamental to understanding complex systems. While many methods infer static
causal graphs, real-world systems often exhibit dynamic causality-where
relationships evolve over time. Accurately capturing these temporal dynamics
requires time-resolved causal graphs. We propose UnCLe, a novel deep learning
method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler
and Recoupler networks to disentangle input time series into semantic
representations and learns inter-variable dependencies via auto-regressive
Dependency Matrices. It estimates dynamic causal influences by analyzing
datapoint-wise prediction errors induced by temporal perturbations. Extensive
experiments demonstrate that UnCLe not only outperforms state-of-the-art
baselines on static causal discovery benchmarks but, more importantly, exhibits
a unique capability to accurately capture and represent evolving temporal
causality in both synthetic and real-world dynamic systems (e.g., human
motion). UnCLe offers a promising approach for revealing the underlying,
time-varying mechanisms of complex phenomena.

</details>


### [44] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出了PSD框架，用于无监督发现周期性技能，通过将状态映射到圆形潜在空间来编码周期性，能够学习具有不同周期的技能并在下游任务中取得良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法往往忽略技能的周期性特征，而许多机器人任务（特别是运动任务）需要在不同时间尺度上执行周期性行为，因此发现多样化周期性技能至关重要。

Method: PSD框架训练编码器将状态映射到圆形潜在空间，在潜在表示中自然编码周期性。通过捕捉时间距离，能够有效学习具有不同周期的技能。

Result: PSD能够在复杂机器人任务中学习具有不同周期的技能，即使在基于像素的观察下也能工作。这些学习到的技能在下游任务（如跨栏）中表现出色，与现有技能发现方法结合还能提供更多样化的行为。

Conclusion: PSD框架成功实现了无监督周期性技能发现，为机器人任务提供了有效的周期性行为学习方案，并能与其他方法结合扩展agent的行为库。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [45] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 提出了一种基于熵理论的线性注意力机制，通过熵值相等性实现线性复杂度，在保持性能的同时显著降低计算和内存开销


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制因二次计算复杂度难以扩展到长序列，限制了在时间序列建模中的应用

Method: 利用熵作为严格凹函数的理论特性，开发线性复杂度算法计算点积分布熵值，实现基于熵相等的线性注意力机制

Result: 在四个时空数据集上的实验表明，该方法在预测性能上具有竞争力或更优，同时大幅减少了内存使用和计算时间

Conclusion: 注意力机制的有效性可能主要源于获得适度且平衡的权重分布，而非softmax的非线性特性

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [46] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于copula的多模态学习框架，通过建模不同模态间的复杂交互来学习联合分布，能够有效处理缺失模态问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在多种数据模态（如医疗记录、医学图像和临床笔记），现有方法主要依赖拼接或Kronecker积，过度简化了模态间的交互结构，需要建模更复杂的交互关系。

Method: 提出copula驱动的多模态学习框架，将copula模型解释为有效对齐模态边际分布的工具，假设每个模态服从高斯混合分布，在联合分布上应用copula模型。

Result: 在公开MIMIC数据集上的大量实验表明，该模型优于其他竞争方法，能够为缺失模态生成准确表示。

Conclusion: copula驱动的多模态学习框架能够有效捕捉模态间的复杂交互，在缺失模态情况下仍能生成准确表示，性能优于现有方法。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [47] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文采用概率U-Net进行气候统计降尺度，结合确定性U-Net主干和变分潜在空间来捕捉随机不确定性，评估了四种训练目标在降水和温度降尺度任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 气候模型受限于计算成本，通常只能产生粗空间分辨率的输出，而许多气候变化影响研究需要更精细的尺度，统计降尺度可以弥补这一差距。

Method: 使用概率U-Net架构，结合确定性U-Net主干和变分潜在空间，评估了四种训练目标（afCRPS和WMSE-MS-SSIM的三种设置）在16倍降尺度任务中的表现。

Result: 主要发现是WMSE-MS-SSIM在特定设置下对极端值表现良好，而afCRPS能更好地捕捉跨尺度的空间变异性。

Conclusion: 不同的训练目标在气候统计降尺度任务中各有优势，需要根据具体应用需求选择合适的损失函数。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [48] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 该研究提出了一种VAE-MLP模型框架，通过变分自编码器提取8维潜在向量，然后系统评估了两种量化策略(QAT和PTQ)在IoT僵尸网络检测中的性能影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习检测方法在IoT僵尸网络攻击中表现出色，但计算强度限制了在资源受限的IoT设备上的部署，因此需要轻量级检测模型。

Method: 使用预训练VAE编码器从高维训练数据中提取8维潜在向量，训练MLP分类器，然后系统评估QAT和PTQ两种量化策略。

Result: PTQ在检测准确率上仅轻微下降，实现6倍加速和21倍尺寸缩减；QAT准确率下降更明显，实现3倍加速和24倍压缩。

Conclusion: 量化技术为设备级IoT僵尸网络检测提供了实用解决方案，特别是PTQ策略在保持性能的同时显著提升了效率。

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [49] [Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning](https://arxiv.org/abs/2511.03238)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习识别能够长期提高城市生活质量的适应气候变化路径，结合综合评估模型分析城市洪水问题。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致城市洪水频率和严重性增加，影响生活质量，需要制定能够应对不确定性和复杂性的适应策略。

Method: 采用强化学习结合综合评估模型，整合降雨预测、洪水模型、交通可达性和生活质量指数。

Result: 初步结果表明该方法能够学习最优适应措施，优于其他现实和实际规划策略。

Conclusion: 该框架为城市洪水适应规划提供了有效工具，可公开获取使用。

Abstract: Urban flooding is expected to increase in frequency and severity as a
consequence of climate change, causing wide-ranging impacts that include a
decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise
adaptation strategies that can cope with the uncertain nature of climate change
and the complex and dynamic nature of urban flooding. Reinforcement Learning
(RL) holds significant promise in tackling such complex, dynamic, and uncertain
problems. Because of this, we use RL to identify which climate adaptation
pathways lead to a higher QoL in the long term. We do this using an Integrated
Assessment Model (IAM) which combines a rainfall projection model, a flood
model, a transport accessibility model, and a quality of life index. Our
preliminary results suggest that this approach can be used to learn optimal
adaptation measures and it outperforms other realistic and real-world planning
strategies. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [50] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: FCDC将数据收集建模为闭环控制问题，通过在线概率模型和反馈机制动态调节样本保留，实现更平衡的数据集和显著减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统主要受限于数据质量和多样性，传统开环数据收集方式积累冗余样本，导致存储效率低、标注成本高和泛化能力有限。

Method: FCDC使用在线概率模型近似已收集数据分布状态，基于似然和马氏距离等反馈信号自适应调节样本保留，动态平衡探索与利用。

Result: 在真实数据流实验中，FCDC产生更平衡的数据集（提升25.9%），同时减少数据存储39.8%。

Conclusion: 数据收集本身可以主动控制，将收集从被动管道阶段转变为数据中心AI核心的自调节、反馈驱动过程。

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [51] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: 提出IGNO生成神经算子框架，统一解决基于点测量和算子值数据的PDE反问题，无需标记训练对，通过物理约束和潜在空间优化实现准确、稳定的反演。


<details>
  <summary>Details</summary>
Motivation: 解决PDE控制的反问题在稀疏、噪声测量或高维、不连续系数情况下具有挑战性，现有深度学习方法需要大量标记数据或局限于特定测量类型。

Method: IGNO将高维系数场编码到低维潜在空间，通过神经算子解码器重建系数和PDE解，训练仅依赖PDE残差的物理约束，反演通过潜在空间梯度优化加速。

Result: 在多种挑战性反问题中，包括从解测量恢复不连续系数和EIT问题，IGNO均实现准确、稳定、可扩展的反演，在严重噪声下表现优异，优于现有最优方法。

Conclusion: IGNO为计算科学领域挑战性反问题提供了统一强大的框架，具有良好泛化能力。

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [52] [Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways](https://arxiv.org/abs/2511.03243)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习在气候变化背景下制定洪水适应政策，通过集成评估模型分析不同优先级（生活质量vs经济影响）对政策决策的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致洪水频率和严重性增加，需要有效的适应政策制定。现有政策制定中存在长期气候影响的不确定性和未明确说明的规范性选择问题。

Method: 提出强化学习框架，结合集成评估模型（IAM）连接降雨洪水模型，计算洪水对生活质量、交通和基础设施的影响。

Result: 结果显示，优先考虑生活质量而非经济影响的模型会导致更多适应支出，并在研究区域内更均匀地分配支出。

Conclusion: 强化学习是识别不确定条件下适应路径的有用工具，同时允许明确建模和比较不同的适应优先级，规范性假设会显著改变适应政策。

Abstract: Climate change will cause an increase in the frequency and severity of flood
events, prompting the need for cohesive adaptation policymaking. Designing
effective adaptation policies, however, depends on managing the uncertainty of
long-term climate impacts. Meanwhile, such policies can feature important
normative choices that are not always made explicit. We propose that
Reinforcement Learning (RL) can be a useful tool to both identify adaptation
pathways under uncertain conditions while it also allows for the explicit
modelling (and consequent comparison) of different adaptation priorities (e.g.
economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link
together a rainfall and flood model, and compute the impacts of flooding in
terms of quality of life (QoL), transportation, and infrastructure damage. Our
results show that models prioritising QoL over economic impacts results in more
adaptation spending as well as a more even distribution of spending over the
study area, highlighting the extent to which such normative assumptions can
alter adaptation policy. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [53] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: 提出了GMoPE框架，将混合专家架构与基于提示的图学习结合，通过专家专用提示向量和结构感知路由实现跨领域泛化，显著降低适应成本。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在跨领域和任务泛化能力有限，存在负迁移、可扩展性问题和适应成本高的挑战。

Method: GMoPE框架整合混合专家架构与提示学习，使用专家专用提示向量和结构感知路由，引入软正交约束促进专家多样性，采用仅提示微调策略。

Result: 在各种预训练策略和下游任务中，GMoPE持续优于现有方法，性能接近全参数微调，但适应开销大幅降低。

Conclusion: GMoPE为推进通用且高效的图基础模型提供了一个原则性和可扩展的框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [54] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 该论文分析了经典熵最小化(EM)的内在机制，将其解耦为两个相反作用的部分，揭示了其局限性，并提出自适应解耦熵最小化(AdaDEM)方法来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 经典熵最小化(EM)在机器学习中虽然有益于减少类别重叠、缩小领域差距和限制不确定性，但其潜力有限。作者希望研究EM的内在机制，揭示其局限性并改进该方法。

Method: 将经典EM解耦为两个部分：聚类聚合驱动因子(CADF)和梯度缓解校准器(GMC)。提出AdaDEM方法，通过归一化CADF带来的奖励，并使用边际熵校准器(MEC)替代GMC。

Result: AdaDEM在噪声和动态环境中的各种不完美监督学习任务中表现优于DEM*(经典EM的上界变体)，取得了优越的性能。

Conclusion: 通过解耦经典EM并引入自适应机制，AdaDEM有效解决了经典EM的局限性，在多个任务中实现了更好的性能。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [55] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 扩散语言模型在数据受限时通过更多训练轮次超越自回归模型，这种优势源于任意顺序建模、迭代双向去噪的超密集计算和内置蒙特卡洛增强


<details>
  <summary>Details</summary>
Motivation: 研究在严格控制的预训练设置下，当独特数据有限时，扩散语言模型相比自回归模型的性能表现

Method: 在匹配的计算预算和数据集下对比扩散语言模型和自回归模型，分析交叉点出现的条件及其影响因素

Result: 1.7B参数的扩散语言模型在10B独特Python token上训练，使用1.5T token计算预算时超越了自回归模型；1B参数的扩散语言模型仅用1B token就达到了HellaSwag 56%和MMLU 33%的准确率

Conclusion: 扩散语言模型在数据受限场景下具有优势，验证交叉熵上升不一定意味着下游性能下降

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [56] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的自适应限流策略，在Kubernetes环境中相比传统固定阈值策略提升23.7%吞吐量，降低31.4% P99延迟


<details>
  <summary>Details</summary>
Motivation: 传统限流算法难以适应动态流量模式和变化的系统负载，需要更智能的限流机制来确保系统稳定性和服务质量

Method: 结合深度Q网络和异步优势演员-评论家算法的混合架构，将限流决策过程建模为马尔可夫决策过程，通过环境交互学习最优限流策略

Result: 高负载场景下吞吐量提升23.7%，P99延迟降低31.4%；90天生产部署处理5亿日请求，服务降级事件减少82%，人工干预减少68%

Conclusion: 基于深度强化学习的自适应限流策略能有效平衡系统吞吐量和服务延迟，显著提升微服务架构下的系统性能和稳定性

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [57] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 提出了一种新的多参考对齐算法，通过概率建模和相对位姿作为干扰变量来消除全局对称性，实现更直接的解决方案和更好的收敛性。


<details>
  <summary>Details</summary>
Motivation: 从分子成像到无线通信，从多个未对齐观测中对齐和重建信号对系统性能至关重要。多参考对齐问题出现在许多实际问题中，如冷冻电镜、计算机视觉和无线通信系统。

Method: 使用概率方法建模多参考对齐问题，将相对位姿作为干扰变量进行边缘化处理，从而消除问题的全局对称性。通过循环一致性避免集中式方法的立方复杂度缩放。

Result: 所提出的算法在实验设置中实现了更低的重建误差，通过去中心化方法显著节省了计算成本。

Conclusion: 该方法通过消除全局对称性和利用循环一致性，为多参考对齐问题提供了更有效和计算效率更高的解决方案。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [58] [Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices](https://arxiv.org/abs/2511.03285)
*Qingyuan Zhang,Ning Lyu,Le Liu,Yuxi Wang,Ziyu Cheng,Cancan Hua*

Main category: cs.LG

TL;DR: 提出了一个结合图神经网络和时间建模的统一框架，用于微服务架构中的异常检测和根因追踪，在动态拓扑和复杂环境下表现出高准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决微服务架构中异常检测和根因追踪的问题，为分布式系统的智能运维提供技术路径。

Method: 将微服务调用链抽象为有向图，使用图卷积聚合节点特征建模依赖关系，引入门控循环单元建模时间演化，定义节点和路径级别的异常评分函数实现从局部异常检测到全局调用链追踪的统一建模。

Result: 在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。

Conclusion: 为微服务异常检测提供了新的技术路径，为分布式系统智能运维奠定了方法论基础。

Abstract: This study addresses the problem of anomaly detection and root cause tracing
in microservice architectures and proposes a unified framework that combines
graph neural networks with temporal modeling. The microservice call chain is
abstracted as a directed graph, where multidimensional features of nodes and
edges are used to construct a service topology representation, and graph
convolution is applied to aggregate features across nodes and model
dependencies, capturing complex structural relationships among services. On
this basis, gated recurrent units are introduced to model the temporal
evolution of call chains, and multi-layer stacking and concatenation operations
are used to jointly obtain structural and temporal representations, improving
the ability to identify anomaly patterns. Furthermore, anomaly scoring
functions at both the node and path levels are defined to achieve unified
modeling from local anomaly detection to global call chain tracing, which
enables the identification of abnormal service nodes and the reconstruction of
potential anomaly propagation paths. Sensitivity experiments are then designed
from multiple dimensions, including hyperparameters, environmental
disturbances, and data distribution, to evaluate the framework, and results
show that it outperforms baseline methods in key metrics such as AUC, ACC,
Recall, and F1-Score, maintaining high accuracy and stability under dynamic
topologies and complex environments. This research not only provides a new
technical path for anomaly detection in microservices but also lays a
methodological foundation for intelligent operations in distributed systems.

</details>


### [59] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种用于连续公平性的核方法，将零空间投影推广到核方法中，适用于连续保护属性，在支持向量回归中表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在日常社会生活中的广泛应用，公平性成为重要议题。现有文献主要关注离散属性，而连续属性（特别是回归问题中的连续公平性）研究稀缺。

Method: 将迭代零空间投影方法推广到核方法，提出模型和公平性评分无关的核嵌入方法，适用于连续保护属性。

Result: 与支持向量回归结合使用时，该方法在多个数据集上表现出竞争力或改进的性能。

Conclusion: 该方法显著扩展了连续公平性的应用范围，为处理连续保护属性提供了有效的核方法解决方案。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [60] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: SORTD是一个新颖框架，用于高效枚举Rashomon集合中的决策树（性能相似但结构不同的树），相比现有技术将运行时间减少两个数量级，支持可分离和全序目标函数。


<details>
  <summary>Details</summary>
Motivation: 稀疏决策树学习提供准确且可解释的预测模型，但单一"最佳"树可能无法满足所有需求。Rashomon集合（性能相似但结构不同的树集合）可以增强变量重要性分析、丰富解释性，并让用户根据偏好选择更简单或满足公平性等标准的树。

Method: 提出SORTD框架，按目标值顺序枚举Rashomon集合中的树，实现随时可用的行为。支持任何可分离和全序的目标函数，并支持使用其他可分离（部分有序）目标函数进行后评估。

Result: 实验显示SORTD相比现有技术将运行时间减少最多两个数量级，能够更高效地计算Rashomon集合。

Conclusion: SORTD使探索Rashomon集合在实际应用中更加实用，为高风险决策提供更多样化的可解释模型选择。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [61] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 提出了一种无需数据的模块化多标签意图识别管道DMTC，通过提示工程生成合成查询、Sentence-T5编码和在线焦点对比损失训练，在交通领域应用中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大量标注数据且难以处理细粒度多标签分类，需要消除昂贵的数据收集需求并提高多标签意图理解的准确性。

Method: 三阶段管道：1) 使用提示工程引导LLM生成多样化合成查询；2) 用Sentence-T5模型编码文本查询获得语义嵌入；3) 使用新颖的在线焦点对比损失训练轻量级分类器。

Result: 在海上交通应用中，DMTC实现了5.35%的汉明损失和95.92%的AUC，优于最先进的多标签分类器和LLM基线。Sentence-T5嵌入比替代编码器提高子集准确率至少3.29%，OFC损失相比标准对比目标带来额外0.98%增益。

Conclusion: 该系统能够无缝地将用户查询路由到特定任务模块，为无需昂贵人工标注的完全自主意图感知智能体奠定了基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [62] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 提出了一个统一的数据-模型耦合市场，将数据集和模型交易作为单一系统处理，通过双向映射机制实现数据卖家、模型生产者和模型买家之间的同时对称定价。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型经济的兴起使得训练数据集和预训练模型市场相互交织，但现有定价方法仍将数据和模型交易分离，或依赖偏向一方的代理中心化流程，缺乏同时对称的机制。

Method: 构建统一的数据-模型耦合市场，包含供给端映射（将数据集支付转换为买家可见的模型报价）和需求端映射（通过Shapley分配将买家价格传播回数据集），形成连接四个交互的闭环系统。

Result: 证明联合算子是一个标准干扰函数，保证均衡价格的存在性、唯一性和全局收敛性。实验显示相比代理中心和单边基线方法，该方法能高效收敛并提高公平性。

Conclusion: 该统一市场机制能够有效解决数据与模型市场的耦合定价问题，为机器学习模型经济提供了更公平高效的定价方案。

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [63] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: 提出了Adaptable HER框架，将后见经验回放与AlphaZero结合，通过重新标记搜索树中的失败轨迹作为监督学习信号，解决稀疏奖励环境下的训练问题，在方程发现等任务中表现优于纯监督或强化学习方法。


<details>
  <summary>Details</summary>
Motivation: AlphaZero的蒙特卡洛树搜索系统在稀疏奖励环境下早期训练阶段存在网络无法提供有效指导的问题，需要改进训练方法。

Method: 引入Adaptable HER框架，将后见经验回放与AlphaZero集成，允许灵活调整HER属性如重新标记目标、策略目标和轨迹选择。

Result: 实验表明修改HER的可能性是有益的，在方程发现等任务中超越了纯监督或强化学习的性能。

Conclusion: Adaptable HER框架成功解决了AlphaZero在稀疏奖励环境中的训练挑战，提供了更灵活有效的训练方法。

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [64] [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)
*Mihriban Kocak Balik,Pekka Marttinen,Negar Safinianaini*

Main category: cs.LG

TL;DR: POEMS是一个可解释的多组学集成框架，通过稀疏解码和专家乘积模型保持预测性能的同时提供可解释性，解决了深度生成模型中预测性能与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决多组学数据集成中深度生成模型在预测性能和可解释性之间的权衡问题，大多数模型要么牺牲可解释性追求预测性能，要么通过线性化解码器来强制可解释性但削弱了非线性表达能力。

Method: 使用稀疏连接将特征映射到潜在因子，通过专家乘积模型实现跨组学关联的共享潜在空间，采用门控网络自适应计算各组学在表示学习中的贡献，并提出了高效的稀疏解码器。

Result: 在癌症亚型分类案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了新颖的解释能力，证明基于生物标志物的洞察和预测准确性可以在多组学表示学习中并存。

Conclusion: POEMS框架成功证明了在多组学表示学习中，生物标志物洞察和预测准确性可以共存，无需线性化网络任何部分即可提供可解释性。

Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for
unraveling the complexity of diseases; yet, most deep generative models either
prioritize predictive performance at the expense of interpretability or enforce
interpretability by linearizing the decoder, thereby weakening the network's
nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:
Product Of Experts for Interpretable Multiomics Integration using Sparse
Decoding, an unsupervised probabilistic framework that preserves predictive
performance while providing interpretability. POEMS provides interpretability
without linearizing any part of the network by 1) mapping features to latent
factors using sparse connections, which directly translates to biomarker
discovery, 2) allowing for cross-omic associations through a shared latent
space using product of experts model, and 3) reporting contributions of each
omic by a gating network that adaptively computes their influence in the
representation learning. Additionally, we present an efficient sparse decoder.
In a cancer subtyping case study, POEMS achieves competitive clustering and
classification performance while offering our novel set of interpretations,
demonstrating that biomarker based insight and predictive accuracy can coexist
in multiomics representation learning.

</details>


### [65] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 提出了一种将已知群对称性融入基于核的强化学习的理论算法框架，通过不变核编码奖励和转移动态的不变性，显著提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 许多现实强化学习问题存在固有对称性，可利用这些对称性提高学习效率。

Method: 提出对称感知的乐观最小二乘值迭代变体，使用不变核编码奖励和转移动态的不变性。

Result: 建立了不变RKHS的最大信息增益和覆盖数的新界限，在定制Frozen Lake环境和2D布局设计问题上验证了理论改进。

Conclusion: 对称感知强化学习比标准核方法性能显著更好，凸显了结构先验在设计更样本高效强化学习算法中的价值。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [66] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost是一个高效的检索增强生成系统，通过准确性保持的上下文重用实现高缓存复用率，在不牺牲准确性的前提下将预填充性能提升1.5-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG缓存技术要么保持准确性但缓存复用率低，要么提高复用率但牺牲推理质量，需要在保持准确性的同时提高缓存效率。

Method: 通过检测并发会话和多轮交互中的重叠检索项，使用高效的上下文索引、排序和去重来最大化复用，同时通过轻量级上下文提示保持推理保真度。

Result: 与现有最先进方法相比，预填充性能提升1.5-3倍，同时在多样化RAG和智能AI工作负载中保持甚至提高了推理准确性。

Conclusion: RAGBoost是一个高效的RAG系统，成功解决了缓存复用与准确性之间的权衡问题，能够无缝集成到现有LLM推理引擎中。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [67] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: NAP是一种基于注意力的模型，通过三轴注意力机制（时间、空间、预测器级）学习组合多个预测流，能够适应不同的输入维度，在零样本泛化方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 多导睡眠图信号具有高度异质性，现有模型依赖固定的模态或通道子集，未能充分利用其固有的多模态特性。

Method: 通过冻结的预训练单通道模型生成预测，然后使用三轴注意力机制（时间、空间、预测器级）来聚合这些预测输出。

Result: NAP持续优于单个预测器和简单集成方法，在多个数据集上实现了最先进的零样本泛化性能。

Conclusion: 该方法虽然以自动睡眠分期为背景展示，但可扩展到其他多模态生理应用。

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [68] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来解决机器学习中的核心悖论：何时使用更少数据反而更好。研究表明，在某些条件下，精心筛选的小数据集可以优于完整数据集，并提供了数据大小和质量相关的相变曲线。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中的核心矛盾：传统扩展定律认为"越多越好"，但最近的研究如LIMO和s1显示，精心筛选的小数据集反而能获得更好的性能。需要理论框架来解释何时以及为什么使用更少数据能改善泛化能力。

Method: 研究数据筛选策略，其中不完美的预言机根据样本难度和正确性来选择训练样本。推导了在标签无关和标签感知筛选规则下测试误差的精确扩展定律曲线，并分析了数据大小和质量相关的相变条件。

Result: 理论分析表明，在某些条件下，小型筛选数据集可以超越完整数据集。在ImageNet上的实证结果验证了理论预测，确认了筛选何时能提高准确性甚至缓解模型崩溃。框架还为LLM数学推理中观察到的矛盾筛选策略提供了原则性解释。

Conclusion: 本文提供了一个理论框架，解释了何时使用更少数据反而更好，挑战了传统的"越多越好"扩展定律。研究表明，通过适当的数据筛选策略，小型数据集在某些条件下可以实现更好的泛化性能。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [69] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO作为PPO的可扩展替代方案，通过轨迹组间比较估计优势值而无需学习critic。研究发现：学习critic对长时域任务仍必要；GRPO适合高折扣因子；小分组优于大分组。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO这种无需学习critic的策略梯度方法的有效性，探索学习baseline在策略梯度方法中的必要性。

Method: 在经典单任务RL环境中系统研究GRPO，通过控制变量实验分离baseline、折扣因子和分组采样的影响。

Result: 1) 除CartPole等短时域任务外，所有无critic baseline均不如PPO；2) GRPO适合高折扣因子(0.99)，但HalfCheetah适合中等折扣(0.9)；3) 小分组优于大分组。

Conclusion: 揭示了无critic方法在经典控制任务中的局限性，以及它们在特定条件下作为学习价值函数替代方案的可行性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [70] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: 提出了一种新的拜占庭鲁棒联邦学习优化问题，将聚合权重作为可学习参数与全局模型参数联合优化，在异构数据和大量恶意客户端场景下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 联邦学习中恶意（拜占庭）客户端的存在对系统鲁棒性构成重大挑战，特别是在客户端数据分布异构的情况下

Method: 开发了具有强收敛保证的交替最小化算法，将聚合权重作为可学习参数与全局模型参数联合优化

Result: 在各种数据集和攻击场景下的实验结果表明，该方法在高度异构数据和大量恶意客户端设置下始终优于现有方法

Conclusion: 所提出的拜占庭鲁棒联邦学习方法在对抗性攻击下具有更强的鲁棒性和性能表现

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [71] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana Pérez-Neira,Miguel Ángel Lagunas*

Main category: cs.LG

TL;DR: 扩展了基于离散余弦变换(DCT)参数化的表达性神经网络(ENN)，强调其效率、可解释性和剪枝能力，通过DCT参数化实现结构化表示和神经元功能识别，提出高效剪枝策略，在分类和隐式神经表示任务中达到SOTA精度且参数少，可安全剪枝40%激活系数。


<details>
  <summary>Details</summary>
Motivation: 在之前证明ENN具有强大表达能力的基础上，进一步探索其在效率、可解释性和剪枝方面的优势，将信号处理概念系统整合到神经网络设计中。

Method: 使用DCT参数化多层感知机的自适应激活函数，通过DCT的结构化和去相关表示揭示神经元功能角色，提出基于DCT系数的高效剪枝策略。

Result: 在分类和隐式神经表示任务中达到最先进精度，同时保持较少的参数数量，由于DCT基的正交性和有界性，可以安全剪枝高达40%的激活系数而性能损失可忽略。

Conclusion: ENN框架将信号处理概念原则性地整合到神经网络设计中，在表达能力、紧凑性和可解释性之间实现了平衡的权衡。

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [72] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 该论文研究了在随机凸优化设置下平坦最小值与泛化性能之间的关系，发现平坦最小值可能泛化很差，而尖锐最小值反而能最优泛化。同时分析了两种锐度感知算法的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 理解学习算法的泛化行为是学习理论的核心目标。最近的研究表明学习算法在实践中成功是因为收敛到平坦最小值，但这一解释在基础随机凸优化设置下的有效性尚不明确。

Method: 在非负β-平滑目标的随机凸优化框架下，分析平坦最小值与泛化性能的关系，并研究两种锐度感知算法（SA-GD和SAM）的泛化表现。

Result: 发现平坦经验最小值可能产生Ω(1)的总体风险，而尖锐最小值反而能最优泛化。SA-GD能快速收敛到平坦最小值但泛化性能差，SAM可能收敛到尖锐最小值且泛化性能同样差。

Conclusion: 在随机凸优化设置中，平坦最小值与良好泛化性能之间的关联并不成立，锐度感知算法可能无法保证良好的泛化性能。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [73] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本文对模仿学习的最新进展进行了系统性综述，提出了新的分类法来反映当前研究现状和发展趋势，并分析了代表性工作的优缺点、评估实践以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，模仿学习在多个领域的能力和可扩展性显著提升，但面临着泛化性、协变量偏移和演示质量等长期挑战，需要对这些最新进展进行系统性总结。

Method: 提出了一种新颖的分类法，与现有分类不同，更好地反映了模仿学习研究现状和发展趋势；对代表性工作进行了批判性分析，包括其优势、局限性和评估实践。

Result: 系统梳理了模仿学习的最新进展，包括方法创新、趋势分析和实际应用；建立了能够更好反映当前研究格局的新分类体系。

Conclusion: 模仿学习领域正在快速发展，但仍面临关键挑战，需要进一步研究来解决泛化性、协变量偏移等问题，并提出了未来的研究方向。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [74] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma是一个用于表格预测的LLM模型，通过科学记数法处理数值、目标插补预训练和n-gram检索选择示例，在语义丰富的分类任务上达到SOTA，但在回归任务上数据量大时落后传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决将预训练LLM应用于表格预测时的两个实际问题：不稳定的数值标记化和有限的上下文大小。

Method: 1. 使用带符号科学记数法规范化数值；2. 在12B Gemma 3模型上继续预训练，采用目标插补目标；3. 使用紧凑的n-gram检索选择信息丰富的示例以适应128k标记窗口。

Result: 在语义丰富的基准测试中，TabGemma在分类任务上（无论是低数据还是高数据情况）建立了新的SOTA，并且随着上下文行数增加而单调改进。对于回归任务，在小样本量时具有竞争力，但随着数据增长落后于传统方法。

Conclusion: 当配备专门的数值处理和上下文检索时，LLMs可以成为有效的表格上下文学习器，特别是在高度语义化的任务上，同时需要在数值建模和长上下文扩展方面进一步改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [75] [Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations](https://arxiv.org/abs/2511.03578)
*Mainak Singha*

Main category: cs.LG

TL;DR: 提出了约束投影学习(CPL)框架，通过在物理约束集上投影网络输出来确保神经网络求解偏微分方程时严格遵守物理定律，包括守恒律、熵条件等。


<details>
  <summary>Details</summary>
Motivation: 神经网络求解偏微分方程时常违反物理定律，如破坏质量守恒、产生虚假震荡等，需要确保求解过程始终满足物理约束。

Method: CPL框架在每次网络更新时将输出投影到物理约束集的交集上，包括守恒、Rankine-Hugoniot平衡、熵和正定性约束，并采用总变差阻尼和滚动课程学习来增强稳定性。

Result: 在Burgers和Euler系统上，CPL能够产生稳定且物理合法的解，守恒律达到机器精度，总变差不增长，熵和误差有界。

Conclusion: CPL使物理约束成为学习过程的内在属性，而非依赖神经网络自行满足，为物理约束下的神经网络求解提供了可靠框架。

Abstract: Neural networks can approximate solutions to partial differential equations,
but they often break the very laws they are meant to model-creating mass from
nowhere, drifting shocks, or violating conservation and entropy. We address
this by training within the laws of physics rather than beside them. Our
framework, called Constraint-Projected Learning (CPL), keeps every update
physically admissible by projecting network outputs onto the intersection of
constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and
positivity. The projection is differentiable and adds only about 10%
computational overhead, making it fully compatible with back-propagation. We
further stabilize training with total-variation damping (TVD) to suppress small
oscillations and a rollout curriculum that enforces consistency over long
prediction horizons. Together, these mechanisms eliminate both hard and soft
violations: conservation holds at machine precision, total-variation growth
vanishes, and entropy and error remain bounded. On Burgers and Euler systems,
CPL produces stable, physically lawful solutions without loss of accuracy.
Instead of hoping neural solvers will respect physics, CPL makes that behavior
an intrinsic property of the learning process.

</details>


### [76] [Tensor-Efficient High-Dimensional Q-learning](https://arxiv.org/abs/2511.03595)
*Junyi Wu,Dan Li*

Main category: cs.LG

TL;DR: 提出了Tensor-Efficient Q-Learning (TEQL)，通过改进的低秩张量分解和探索机制，在高维强化学习中提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 高维强化学习面临计算复杂和样本效率低的问题，传统Q学习算法受维度灾难影响，需要更参数高效的方法。

Method: 使用改进的块坐标下降进行低秩张量分解，结合近似误差和访问计数上置信界的探索策略，以及频率惩罚项减少过拟合。

Result: 在经典控制任务中，TEQL在样本效率和总奖励方面优于传统矩阵方法和深度强化学习方法。

Conclusion: TEQL适合采样成本高的资源受限应用，如太空和医疗领域。

Abstract: High-dimensional reinforcement learning faces challenges with complex
calculations and low sample efficiency in large state-action spaces. Q-learning
algorithms struggle particularly with the curse of dimensionality, where the
number of state-action pairs grows exponentially with problem size. While
neural network-based approaches like Deep Q-Networks have shown success, recent
tensor-based methods using low-rank decomposition offer more
parameter-efficient alternatives. Building upon existing tensor-based methods,
we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor
decomposition via improved block coordinate descent on discretized state-action
spaces, incorporating novel exploration and regularization mechanisms. The key
innovation is an exploration strategy that combines approximation error with
visit count-based upper confidence bound to prioritize actions with high
uncertainty, avoiding wasteful random exploration. Additionally, we incorporate
a frequency-based penalty term in the objective function to encourage
exploration of less-visited state-action pairs and reduce overfitting to
frequently visited regions. Empirical results on classic control tasks
demonstrate that TEQL outperforms conventional matrix-based methods and deep RL
approaches in both sample efficiency and total rewards, making it suitable for
resource-constrained applications, such as space and healthcare where sampling
costs are high.

</details>


### [77] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 提出深度隐式模仿强化学习框架，结合深度强化学习和仅观察数据集的隐式模仿学习，解决传统模仿学习需要完整状态-动作演示和最优专家的限制。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要完整的状态-动作演示和最优专家，这严重限制了实际应用，因为许多现实场景仅提供状态观察而没有对应动作，且专家表现往往次优。

Method: 主要算法DIIQN采用动作推断机制通过在线探索重建专家动作，并集成动态置信机制自适应平衡专家引导和自主学习。HA-DIIQN算法处理专家和智能体具有不同动作集的情况，引入不可行性检测机制和桥接过程。

Result: DIIQN相比标准DQN获得高达130%的更高回合回报，持续优于现有无法超越专家表现的隐式模仿方法。在异构动作设置中，HA-DIIQN学习速度比基线快64%，可利用传统方法无法使用的专家数据集。

Conclusion: 该框架通过结合深度强化学习和隐式模仿学习，有效解决了传统模仿学习的局限性，使智能体能够利用专家指导加速训练同时保持超越次优专家表现的能力。

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [78] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 使用Lean 4定理证明器基于Mathlib库形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是最早且最有影响力的强化学习算法，研究它们的收敛性不仅是RL领域早期发展的主要课题，如今也受到越来越多的关注

Method: 基于Robbins-Siegmund定理的统一框架来形式化验证几乎必然收敛性，该框架可轻松扩展到收敛速率和其他收敛模式

Result: 成功形式化验证了Q学习和线性TD学习的几乎必然收敛性

Conclusion: 这项工作为完全形式化收敛RL结果迈出了重要一步

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [79] [Financial Management System for SMEs: Real-World Deployment of Accounts Receivable and Cash Flow Prediction](https://arxiv.org/abs/2511.03631)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 开发了一个面向中小企业的集成财务预测系统，包含应收账款预测和现金流预测功能，专门针对资源有限的小企业设计。


<details>
  <summary>Details</summary>
Motivation: 解决中小企业特别是自由职业者和早期企业面临的财务挑战，这些企业资源有限、客户基础小、数据可用性受限，现有企业级财务工具无法满足其实际需求。

Method: 系统包含两个关键组件：预测发票付款延迟的二元分类模型，以及处理不完整和有限历史数据的多模块现金流预测模型。原型系统已作为Web应用程序实现并集成到Cluee平台。

Result: 成功开发并部署了原型系统，证明了该系统在现实世界中小企业财务管理中的实际可行性。

Conclusion: 该集成财务预测系统有效填补了企业级财务工具与中小企业实际需求之间的空白，为资源受限的小企业提供了实用的财务管理解决方案。

Abstract: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage
businesses, face unique financial management challenges due to limited
resources, small customer bases, and constrained data availability. This paper
presents the development and deployment of an integrated financial prediction
system that combines accounts receivable prediction and cash flow forecasting
specifically designed for SME operational constraints. Our system addresses the
gap between enterprise-focused financial tools and the practical needs of
freelancers and small businesses. The solution integrates two key components: a
binary classification model for predicting invoice payment delays, and a
multi-module cash flow forecasting model that handles incomplete and limited
historical data. A prototype system has been implemented and deployed as a web
application with integration into Cluee's platform, a startup providing
financial management tools for freelancers, demonstrating practical feasibility
for real-world SME financial management.

</details>


### [80] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: nanoTabPFN是一个简化的表格基础模型实现，相比复杂的TabPFN v2，它更轻量、易于理解，能在单GPU上1分钟内完成预训练，性能与传统机器学习基线相当。


<details>
  <summary>Details</summary>
Motivation: 现有的表格基础模型实现过于复杂（超过1万行代码），缺乏架构文档和代码质量，难以理解和适应新实验，阻碍了学生和研究人员的使用。

Method: 开发了简化的TabPFN v2架构实现和训练循环，使用预生成的训练数据，大幅减少了代码复杂度和计算资源需求。

Result: 在小数据设置下，nanoTabPFN在单GPU上1分钟预训练即可达到与传统机器学习基线相当的性能，比TabPFN v2预训练快160,000倍。

Conclusion: nanoTabPFN使表格基础模型的预训练对教育目的变得可行，消除了对大型计算资源的需求，使该技术更易于访问。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [81] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: 本研究提出了一个机器学习驱动的框架，用于检测医疗物联网设备中的恶意网络攻击和设备故障异常。通过评估8种机器学习模型在三种学习范式下的表现，发现XGBoost在异常检测中达到99%准确率，KNN在攻击检测中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备的集成带来了严重的安全性和可靠性挑战，增加了网络威胁和操作异常的风险，需要有效的检测方法来保障医疗设备的安全运行和患者数据安全。

Method: 使用包含20万条记录的数据集，评估了8种机器学习模型：监督学习（XGBoost、KNN）、半监督学习（GAN、VAE）和无监督学习（One-Class SVM、Isolation Forest、GNN、LSTM Autoencoders），采用F1分数、精确率、召回率、准确率、ROC-AUC和计算效率等多指标进行综合评估。

Result: XGBoost在异常检测中达到99%准确率且计算开销最小（0.04秒），Isolation Forest在精确率和召回率间取得良好平衡。在攻击检测中，KNN实现近乎完美的精确率、召回率和F1分数，计算成本最低（0.05秒），VAE达到97%准确率。GAN表现最差，计算成本最高且准确率最低。

Conclusion: 该框架通过有效的异常检测策略增强了医疗物联网安全性，能够早期检测网络威胁和设备故障，预防数据泄露，减少系统停机时间，确保医疗设备持续安全运行，最终保护患者健康和信任。

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [82] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: 对深度Q网络在有限环境中的研究，重点分析ε-贪婪探索策略和优先经验回放的影响，通过实验评估不同ε衰减策略对学习效率和收敛行为的影响。


<details>
  <summary>Details</summary>
Motivation: 研究深度Q网络中探索策略与经验回放机制的相互作用，为资源受限环境下的强化学习提供实用建议。

Method: 通过系统实验评估不同ε衰减策略和三种经验回放策略（均匀、无回放、优先回放）对DQN学习效果的影响。

Result: 优先经验回放能带来更快的收敛速度和更高的回报，实验结果表明探索策略与记忆管理之间存在权衡关系。

Conclusion: 研究揭示了DQN训练中探索策略与记忆管理之间的相互作用，为资源受限环境下的强化学习提供了实用指导。

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [83] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文提出了更灵活的后验校准方法，通过结构化正则化、鲁棒预处理和高效优化来管理偏差-方差权衡，在多项分类校准中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 参数化校准函数基于逻辑回归，可以用于二分类和多分类，但现有方法表达能力有限，且多分类校准面临参数过多和过拟合的挑战。

Method: 使用结构化正则化、鲁棒预处理和高效优化来管理偏差-方差权衡，开发了比标准温度缩放更灵活的参数化校准方法。

Result: 新方法在多项分类校准中显著优于现有的基于逻辑回归的校准技术，包括温度缩放、向量缩放和矩阵缩放。

Conclusion: 提出的校准方法通过有效管理偏差-方差权衡，提供了比现有方法更准确的概率估计，并有高效的开源实现。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [84] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ是一种离线到在线强化学习框架，通过行为一致性信号和自适应约束机制，实现从离线训练到在线微调的平稳过渡，提高策略在动态环境中的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习策略在部署到动态环境时，由于分布偏移和未见状态-动作对上的不可靠价值估计，往往表现不佳。需要一种能够平滑可靠地从离线训练过渡到在线微调的方法。

Method: BAQ框架利用离线数据中的隐式行为模型提供行为一致性信号，采用双目标损失函数：(i)在不确定性高时使在线策略向离线行为对齐，(ii)随着在线经验积累逐渐放松约束。

Result: 在标准基准测试中，BAQ始终优于先前的离线到在线强化学习方法，实现了更快的恢复速度、改进的鲁棒性和更高的整体性能。

Conclusion: 隐式行为适应是一种原则性且实用的解决方案，能够实现可靠的现实世界策略部署。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [85] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出了一种基于收缩估计的基线方法，用于改进强化学习中策略梯度估计器的方差，特别适用于可验证奖励的强化学习场景。


<details>
  <summary>Details</summary>
Motivation: 在可验证奖励的强化学习中，通常使用每个提示的实证平均值作为基线来稳定训练。然而，在低生成数量的典型情况下，这些估计可能不够准确。受Stein悖论的启发，作者希望通过收缩估计器结合每个提示和跨提示的均值来提高估计精度。

Method: 提出使用收缩估计器，将每个提示的均值与跨提示的均值相结合，构建一个基于收缩的基线。该方法无需额外超参数或计算，可直接替代现有的每个提示均值基线。

Result: 理论上证明了收缩基线能够产生更低方差的策略梯度估计器。实证结果显示，收缩基线始终优于标准实证均值基线，导致梯度更新的方差更低，训练稳定性更好。

Conclusion: 收缩基线方法在可验证奖励的强化学习中有效降低了策略梯度估计的方差，提高了训练稳定性，且易于实现，无需额外计算成本。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>
