<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 82]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 提出基于思维深度(DoT)的课程学习方法，通过统计教师模型推理步骤数量来定义难度，构建从浅到深的训练课程，在推理任务上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 需要一种与推理能力对齐、可扩展且可解释的难度信号来指导LLM的课程学习

Method: 将思维深度(DoT)定义为教师模型推理轨迹中的离散步骤数量，构建按DoT排序的从浅到深课程，并制定规模化应用方案

Result: 提出三个可验证假设：DoT与推理基准难度相关、DoT排序课程优于长度或评分排序课程、难度在不同教师模型间具有鲁棒性

Conclusion: 该方法为推理中心训练提供了认知基础扎实、可解释的课程学习框架，推动LLM训练向更符合人类认知的方式发展

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [2] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 一种多模态机器学习框架，结合语义嵌入和关注机制，用于预测海上浮体的偏移轨迹，在多种时间距积分享现优称性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测海上浮体偏移对搜救救援等时间敏感场景至关重要，但依靠传统物理模型和机器学习方法有限。

Method: 通过实验收集环境和物理数据，使用Navier-Stokes模型训练卷积神经网络估算拉力和升力系数，然后结合语言模型编码的文本描述，使用关注机制的LSTM和Transformer模型预测偏移轨迹。

Result: 在1、3、5和10秒多个时间距上，多模态模型与传统方法性能相当，但能够进行更长期预测而非单步预测。

Conclusion: 多模态建模策略能够在动态海上条件下提供准确且适应性强的浮体偏移预测。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [3] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 基于机器学习的石油气气藏生产预测方法，使用生产注入数据预测产量，免需地质模型和流体性质信息


<details>
  <summary>Details</summary>
Motivation: 解决石油气气藏工程中生产预测的靠谱性和岩矿-流体系统行为变化预测的挑战

Method: 采用监督学习方法（回归和神经网络），通过生产注入变量相关性分析和数据处理，处理概念漏淀问题，先使用组合模型数据验证方法论

Result: 开发了可靠的快速响应预测器，能处理实际生产限制和处理单元问题，应用于巴西盖层下盆地实际案例

Conclusion: 该方法能够支持气藏管理决策，预测不良行为，优化生产注入参数，分析概率事件影响，最大化石油重收率

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [4] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 使用手机应用使用数据在1秒内进行抑郁症检测的最小化系统，通过机器学习模型达到82.4%检测准确率


<details>
  <summary>Details</summary>
Motivation: 现有抑郁检测系统需要长时间数据收集，不适合早期识别需求，需要开发一种最快速的最小化检测系统

Method: 开发了在1秒内获取过去7天应用使用数据的工具，使用多种机器学习模型和特征选择方法，包括稳定特征选择和Boruta全相关特征选择

Result: 轻量梯度提升机模型使用稳定特征选择方法，检测准确率达82.4%，精度为75%，F1分数为78.5%。堆叠模型使用约5个特征，最高精度达77.4%，平衡准确率为77.9%

Conclusion: 该系统速度快、资源消耗少，特别适合资源稀缺的发展中国家和地区使用，为抑郁症早期识别提供了一种高效解决方案

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [5] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: NeuroPathX是一个可解释的深度学习框架，通过交叉注意力机制融合脑部MRI和遗传数据，使用稀疏性和通路相似性损失函数提高可解释性，在自闭症和阿尔茨海默病中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统成像遗传学方法局限于简单线性模型或缺乏可解释性的黑盒技术，无法有效捕捉脑部结构与遗传变异之间的复杂相互作用。

Method: 提出NeuroPathX框架，采用早期融合策略和交叉注意力机制，引入两个注意力矩阵损失函数：稀疏性损失（聚焦最显著交互）和通路相似性损失（确保队列间一致表示）。

Result: 在自闭症谱系障碍和阿尔茨海默病上的验证表明，NeuroPathX优于竞争基线方法，并揭示了与疾病相关的生物学合理关联。

Conclusion: NeuroPathX有潜力推动对复杂脑部疾病的理解，其代码已开源。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [6] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出了SALMAN框架，通过Distance Mapping Distortion (DMD)度量来评估Transformer语言模型在输入扰动下的鲁棒性，无需修改模型参数或复杂扰动启发式方法。


<details>
  <summary>Details</summary>
Motivation: 随着预训练Transformer语言模型规模增大和部署广泛，其在输入扰动下的鲁棒性成为紧迫问题。现有方法通常在小参数模型和大语言模型(LLMs)之间存在差异，且依赖劳动密集型的样本特定对抗设计。

Method: 提出统一的局部(样本级)鲁棒性框架SALMAN，核心是新颖的Distance Mapping Distortion (DMD)度量，通过比较输入到输出的距离映射来评估样本易感性，具有近线性复杂度。

Result: 在攻击效率和鲁棒训练方面取得了显著提升，证明了该框架的有效性。

Conclusion: SALMAN框架作为一个实用的、模型无关的工具，可用于提升基于Transformer的NLP系统的可靠性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [7] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 提出了一个统一框架，结合算子值再生核希尔伯特空间和基于核的Koopman算子方法，用于学习向量值函数的时空动力学。


<details>
  <summary>Details</summary>
Motivation: 需要一种非参数、数据驱动的方法来估计复杂的时变向量场，同时保持时空结构，支持高维非线性系统的高效降阶建模和长期预测。

Method: 结合算子值再生核希尔伯特空间(OV-RKHS)与基于核的Koopman算子方法，建立时间依赖的OV-RKHS插值表示定理，推导光滑向量场的Sobolev型近似边界。

Result: 提供了核Koopman算子近似的谱收敛保证，支持高效的降阶建模和长期预测能力。

Conclusion: 该框架为时空机器学习中的预测、控制和不确定性量化提供了理论基础扎实的工具。

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [8] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: CoPE是一种轻量级复数位置编码方法，用复数嵌入替代传统位置编码，实部捕捉语义内容，虚部编码位置信息，在Transformer中实现更优性能且计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法在长序列建模中存在长期衰减问题，且计算复杂度较高，需要一种既能有效编码位置信息又能保持计算效率的新方法。

Method: 提出CoPE复数位置编码架构，使用复数嵌入（实部为语义内容，虚部为位置信息），在第一层引入相位感知注意力机制捕捉位置依赖模式，后续层使用标准注意力机制。

Result: 在GLUE基准测试中表现优于RoPE、正弦和可学习位置编码，不出现长期衰减现象，且与线性注意力兼容。

Conclusion: CoPE通过复数编码方式有效整合内容和位置信息，在保持高性能的同时降低了计算复杂度，为位置编码提供了新的有效解决方案。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [9] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: DPO性能主要受选择回应质量影响，拒绝回应影响相对有限，理论分析和实验验证了这一发现


<details>
  <summary>Details</summary>
Motivation: 系统研究DPO对偏好数据分布的敏感性，探索哪些数据特征对DPO性能最为关键

Method: 统一理论分析DPO的最优回应分布，并通过多种任务的广泛实验验证理论发现

Result: 选择回应质量对DPO性能起主导作用，拒绝回应影响相对有限，在线DPO等效于对选择回应的监督微调

Conclusion: 提高选择回应质量是构建高效DPO偏好数据集的关键，为LLM对齐实践提供了重要见解

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [10] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: ProtoEHR是一个可解释的分层原型学习框架，充分利用EHR数据的多层次结构来提升医疗预测性能，在多个临床任务上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗AI研究往往只关注电子健康记录(EHR)数据的孤立组成部分，限制了预测性能和可解释性。需要开发能够充分利用EHR丰富多层次结构的框架。

Method: 提出ProtoEHR框架：1)利用大语言模型提取医疗代码语义关系构建医学知识图谱；2)设计分层表示学习框架，在医疗代码、医院就诊和患者三个层次捕获上下文表示；3)在每个层次融入原型信息以捕捉内在相似性和提升泛化能力。

Result: 在两个公共数据集上评估五个临床重要任务（死亡率预测、再入院预测、住院时长预测、药物推荐和表型预测），结果显示ProtoEHR相比文献基线方法能够做出更准确、鲁棒和可解释的预测。

Conclusion: ProtoEHR框架能够充分利用EHR数据的多层次结构，在保持高预测准确性的同时提供代码、就诊和患者三个层次的可解释性洞察，有助于医疗预测决策。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [11] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: 这篇论文提出了DualSparse-MoE系统，通过综合动态张量级计算丢弃和静态神经元级重构，在仅损失0.08%-0.28%准确性的情况下，实现了近25%计算量减少和1.41x的速提升。


<details>
  <summary>Details</summary>
Motivation: 虽然专家混合模型(MoE)通过稀疏激活提高了效率，但仍面临大规模计算和不可预测激活模式的挑战。识别到张量级和神经元级的双重稀疏性是关键因素。

Method: 提出训练后专家分区方法，在不需要重新训练的情况下导入张量级稀疏性。构建DualSparse-MoE系统，结合动态张量级计算丢弃和静态神经元级重构。

Result: 在三种主流MoE模型上，实现约25%的计算量丢弃时，平均准确性仅下降0.08%-0.28%，同时计算速度按比例提升。结合负载不平衡感知后，实现了1.41x的模块速度提升，准确性仅下降0.5%。

Conclusion: 该方法通过合理利用双重稀疏性，在保持模型数学一致性的前提下，显著提升了MoE模型的部署效率和性能，为大规模语言模型的高效运行提供了有效解决方案。

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [12] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 基于民聊学习框架的机器学习模型，通过学生早期学业表现和数字参与模式预测远程教育中的高风险学生，达到了85% AUC成绩，同时避免数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 远程教育中的高退学和失败率是重大挑战，需要及早识别高风险学生以提供及时支持，但实际应用遇到数据隐私和机构数捠的障碍。

Method: 使用OULAD数据集，构建基于民聊学习框架的机器学习模型，比较逻辑回归和深度神经网络两种模型复杂度，并进行数据平衡处理。

Result: 最终模型在识别高风险学生方面达到约85% ROC AUC分数，显示出强大的预测能力。

Conclusion: 民聊学习方法为教育机构提供了一种实用且可扩展的方案，能够在保护数据隐私的前提下构建有效的早期预警系统，支持主动学生支持。

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [13] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL是一个针对大语言模型强化学习的系统，通过利用历史rollout数据的相似性，使用推测解码和两级调度策略，实现了2.6倍的性能提升，同时保持训练准确性。


<details>
  <summary>Details</summary>
Motivation: 当前RL系统存在GPU利用率低的问题，主要原因是rollout阶段占主导地位和rollout长度不平衡导致的GPU气泡。传统异步执行和截断方法虽然能部分缓解，但可能牺牲训练准确性。

Method: 提出RhymeRL系统，包含两个关键创新：1) HistoSpec - 利用历史rollout token序列相似性的推测解码推理引擎；2) HistoPipe - 利用历史rollout分布相似性的两级调度策略来平衡工作负载。

Result: 在真实生产环境中评估，展示了从几十到数千个GPU的可扩展性，相比现有方法实现了2.6倍的性能提升。

Conclusion: RhymeRL在不影响准确性或修改RL范式的前提下，显著提高了RL训练效率，解决了GPU利用率低的问题。

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [14] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: ZTFed-MAS2S是一个零信任联邦学习框架，结合多头注意力序列到序列插补模型，用于风电场数据缺失值处理，通过可验证差分隐私和信任感知聚合确保安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 风电场数据常因传感器故障和传输不稳定而缺失，联邦学习虽然能保护隐私但易受异常更新和参数交换中的隐私泄露影响，特别是在开放工业环境中需要零信任机制。

Method: 提出ZTFed-MAS2S框架，整合可验证差分隐私与非交互式零知识证明、保密性和完整性验证机制，采用动态信任感知聚合机制，通过相似图传播信任，并使用稀疏化和量化压缩降低通信开销。MAS2S模型捕获风功率数据的长期依赖关系进行准确插补。

Result: 在真实风电场数据集上的大量实验验证了ZTFed-MAS2S在联邦学习性能和缺失数据插补方面的优越性。

Conclusion: ZTFed-MAS2S是能源领域实际应用中安全高效的有效解决方案。

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [15] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: FedProtoKD是一种异构联邦学习方法，通过增强的双知识蒸馏机制和对比学习来解决原型聚合中的边缘收缩问题，在非IID数据场景下显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的原型异构联邦学习方法在服务器端使用加权平均聚合原型，导致全局知识次优和原型边缘收缩问题，特别是在模型异构和数据极度非IID的场景下影响模型性能。

Method: 提出FedProtoKD方法，采用增强的双知识蒸馏机制（客户端logits和原型特征表示），使用基于对比学习的可训练服务器原型和类别自适应原型边缘来解决原型边缘收缩问题，并通过样本与类别代表原型的接近度评估公共样本重要性。

Result: 在各种设置下平均准确率提升1.13%至34.13%，显著优于现有的最先进异构联邦学习方法。

Conclusion: FedProtoKD通过创新的知识蒸馏和原型优化机制，有效解决了异构联邦学习中的原型聚合问题，为处理非IID数据和模型异构性提供了有效解决方案。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [16] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: HSIC是当前最先进的统计依赖性评估方法，但计算复杂度高（O(n^2.37)），不适用于大数据样本。本文提出HCR（分层相关重建）作为线性成本替代方案，具有更高的依赖性灵敏度，并能提供联合分布模型。


<details>
  <summary>Details</summary>
Motivation: 解决HSIC方法在大数据样本下计算复杂度高的问题，提供更高效的统计依赖性评估方法。

Method: 使用HCR（分层相关重建）方法，通过混合矩特征描述依赖性，从相关性和同方差性开始，允许近似互信息。单个依赖性特征可在O(n)线性时间内计算。

Result: HCR方法在测试中表现出比HSIC更高的依赖性灵敏度，同时计算成本显著降低（线性复杂度）。

Conclusion: HCR是HSIC的实用替代方案，具有线性计算复杂度、更高的灵敏度，并能提供联合分布模型，适用于大规模数据分析。

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [17] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文综述了低秩张量分解在深度神经网络理论分析中的基础作用，包括表达能力、可学习性、计算复杂性、泛化能力和可识别性等方面，旨在统一不同学科的研究视角。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络表现出色但缺乏数学理论基础，低秩张量分解因其与神经网络的紧密联系和丰富的理论结果，成为解释神经网络性能的理论工具。

Method: 通过综述性方法，整合计算机科学、数学等不同学科的研究成果，系统分析低秩张量分解在神经网络理论中的各种应用。

Result: 展示了低秩张量方法在解释神经网络表达能力、算法可学习性、计算复杂性、泛化性能和模型可识别性等方面的基础性作用。

Conclusion: 低秩张量分解为深度神经网络理论提供了统一的理论框架，开启了更广阔的跨学科研究视角，对理解神经网络性能具有重要意义。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [18] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 结合变分状态内在奖励(VSIMR)和大型语言模型(LLM)内在奖励的策略，在稀疏奖励环境中显著提升强化学习代理的性能和采样效率


<details>
  <summary>Details</summary>
Motivation: 解决极端稀疏奖励环境中传统强化学习因正反馈稀少而学习困难的问题

Method: 集成VSIMR(使用VAE奖励状态新颖性)和LLM基于环境目标描述生成奖励信号的方法，在MiniGrid DoorKey环境中使用A2C代理实现

Result: 组合策略相比单独使用任一策略或标准A2C代理，显著提高了代理性能和采样效率，标准A2C代理完全无法学习

Conclusion: VSIMR驱动状态探索，LLM奖励促进目标导向利用，两种策略在环境和任务的不同方面形成有效互补

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [19] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于全局高斯过程的多局部二次模型方法，通过梯度和Hessian矩阵构建局部模型，解决了高维贝叶斯优化中采样效率低和梯度消失问题，显著提升了TuRBO方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统TuRBO方法使用局部高斯过程虽然能实现目标函数的异质建模并避免过度探索，但采样效率相比全局高斯过程有所降低。同时高维空间中高斯过程的梯度消失问题也需要解决。

Method: 使用全局高斯过程的梯度和Hessian矩阵构建多个局部二次模型，通过求解边界约束二次规划问题选择新的采样点，解决了高维空间中的梯度消失问题。

Result: 实验结果表明该方法有效提升了TuRBO的效能，在合成函数和实际应用中均优于多种高维贝叶斯优化技术。

Conclusion: 所提出的方法在保持异质建模能力的同时显著提高了采样效率，为高维贝叶斯优化提供了有效的解决方案。

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [20] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出了一个针对Verilog代码生成的强化学习框架，包含高质量数据集构建、奖励机制改进和平衡学习策略，在硬件描述语言生成任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 硬件描述语言（如Verilog）由于并发语义、语法刚性和仿真复杂性等原因，在代码生成领域研究不足，需要专门的方法来解决这些挑战。

Method: 构建了Veribench-53K高质量数据集，提出基于回溯的重新评分机制来改善稀疏和噪声奖励信号，采用样本平衡权重策略防止灾难性遗忘和过拟合，构建了策略模型和奖励模型协同进化的迭代RL流程。

Result: 在Verilog生成任务中实现了最先进的性能，在测试通过率、功能正确性和编译鲁棒性方面都有显著提升。

Conclusion: 强化学习方法在硬件中心领域的结构化代码生成中具有巨大潜力，通过高质量数据集和RL优化的结合可以超越依赖大规模闭源模型蒸馏的方法。

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [21] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [22] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 本文研究了数据增强策略对机器遗忘性能的影响，发现适当的增强设计能显著提升遗忘效果，使遗忘后模型性能更接近重新训练的模型。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练好的模型中移除特定数据的影响，同时保持对剩余数据的性能。虽然已有研究指出记忆化与增强之间存在联系，但系统性增强设计在机器遗忘中的作用尚未得到充分研究。

Method: 研究了不同数据增强策略（包括TrivialAug等）对三种遗忘方法（SalUn、随机标签和微调）性能的影响，在CIFAR-10和CIFAR-100数据集上进行了实验，并考虑了不同的遗忘率。

Result: 实验结果显示，适当的增强设计能显著提高遗忘效果，使用TrivialAug增强时，平均差距遗忘指标最多可减少40.12%，使遗忘后模型性能更接近重新训练的模型。

Conclusion: 数据增强不仅有助于减少记忆化，还在实现隐私保护和高效遗忘方面发挥着关键作用，系统性增强设计是提升机器遗忘性能的重要因素。

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [23] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: 提出基于强化学习的初始化策略来解决VQA中的贫瘠高原问题，通过RL预训练生成有利的初始参数，显著提升收敛速度和最终解质量


<details>
  <summary>Details</summary>
Motivation: 变分量子算法(VQAs)在近期量子设备应用中面临贫瘠高原问题，即梯度随系统规模或电路深度增加而指数衰减，阻碍训练效果

Method: 探索多种RL算法（确定性策略梯度、软演员-评论家、近端策略优化等）生成电路参数作为动作，在标准梯度优化前最小化VQA成本函数

Result: 在各种噪声条件和任务下的数值实验表明，RL初始化方法显著提高了收敛速度和最终解质量，多种RL算法都能获得可比性能提升

Conclusion: 该方法为将机器学习技术整合到量子算法设计提供了有前景的途径，RL驱动的参数初始化可以加速VQAs的可扩展性和实际部署

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [24] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 本文提出了一种将任意电路精确转换为ReLU神经网络的方法，证明了神经网络可以完美模拟任何推理任务，包括布尔逻辑、动态规划、数学表示等，且无需近似或舍入。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在完美训练条件下能够执行何种形式推理的核心问题，量化神经网络的推理能力边界。

Method: 提出系统元算法，通过迭代将电路中的每个门替换为规范的ReLU MLP模拟器，构建前馈神经网络来精确模拟原始电路。

Result: 证明神经网络可以精确模拟任何数字电路（包括布尔门、热带电路、算术门等），网络规模与电路复杂度成正比，计算图结构反映原始电路结构。

Conclusion: 神经网络能够模拟任何推理任务，将算法运行时间转换为空间复杂度，结果比经典通用近似定理更强大，可应用于最短路径算法、图灵机模拟等场景。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [25] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: BTW是一个双层级、非参数化的多模态权重框架，通过KL散度和互信息动态调整训练中的模态重要性，无需额外参数即可处理任意数量模态，显著提升回归和分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如部分信息分解难以扩展到两个以上模态，且缺乏实例级控制能力。当额外模态引入噪声多于补充信息时，MoE模型的有效性不明确。

Method: 提出BTW框架：使用实例级KL散度衡量单模态与多模态预测的差异，计算每样本权重；使用模态级互信息估计全局对齐，计算模态级权重。双层级非参数化方法。

Result: 在情感回归和临床分类任务上的大量实验表明，该方法显著改善了回归性能和多类分类准确率。

Conclusion: BTW框架有效解决了多模态学习中噪声模态的问题，提供了可扩展的实例级控制，无需额外参数即可提升模型性能。

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [26] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 提出了一种名为counterfactual masking的新框架，用于分子属性预测的可解释性分析，通过用生成模型生成的合理化学片段替换掩码子结构，提供更真实和可操作的解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码策略的可解释性方法往往产生不符合分子分布的不可靠解释，需要一种能够保持分子真实性的解释框架。

Method: 使用生成模型训练完成分子图，采样化学合理的片段来替换掩码子结构，评估相对于数据分布中反事实分子的预测结果。

Result: 该方法在多个数据集和属性预测任务中产生了更稳健、分布一致的解释，并为分子设计提供了可操作的见解。

Conclusion: 该方法弥合了可解释性与分子设计之间的差距，为化学中的可解释机器学习提供了原则性和生成性的路径。

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [27] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 本文扩展了Levie关于图神经网络在图on信号空间分析的工作，解决了原论文在实用场景中的局限性，包括多维信号扩展、带readout的MPNNs Lipschitz连续性、改进泛化界以及非对称图on分析。


<details>
  <summary>Details</summary>
Motivation: Levie的论文虽然通过图on信号分析为MPNNs提供了理论框架，但在实际图机器学习应用中存在局限性，如仅支持一维信号、缺乏readout机制分析、泛化界不够紧致等。

Method: 1) 将分析扩展到多维图on信号 2) 扩展Lipschitz连续性到带readout的MPNNs 3) 使用鲁棒性泛化界改进泛化边界 4) 扩展到非对称图on和核的分析

Result: 提出了更全面的图on信号分析框架，能够处理更广泛的实用场景，包括多维属性图、带readout的MPNN架构，并获得了更紧致的泛化保证。

Conclusion: 通过多项技术扩展和改进，显著提升了图on信号分析在图神经网络理论分析中的实用性和适用范围，为实际图机器学习应用提供了更坚实的理论基础。

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [28] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: SPF框架通过随机推前策略实现多步学习，在保持单步训练的同时提升长期预测精度，降低内存需求


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在复杂系统中长期精度下降，自回归训练需要大量GPU内存且可能牺牲短期性能

Method: 构建模型预测的补充数据集，通过随机获取策略结合真实数据，平衡短期和长期性能，减少过拟合

Result: 在Burgers方程和浅水基准测试中，SPF比自回归方法获得更高的长期精度，同时降低内存需求

Conclusion: SPF框架在资源受限和复杂模拟中具有应用前景，实现了内存效率和预测精度的平衡

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [29] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 稀疏自编码器(SAEs)在蛋白质功能预测和设计中，即使只有24个序列的少量数据，也能超越或媲美ESM2基线模型，在83%的情况下产生顶级适应性变异体。


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺（低N）情况下从氨基酸序列预测蛋白质功能的挑战，特别是在只有少量标记数据时机器学习指导蛋白质设计的局限性。

Method: 使用在微调ESM2嵌入上训练的稀疏自编码器(SAEs)，评估其在各种适应性外推和蛋白质工程任务中的表现，通过分解蛋白质语言模型的嵌入来获得可解释的潜在变量。

Result: SAEs在适应性预测方面始终优于或与ESM2基线竞争，其稀疏潜在空间编码了紧凑且具有生物学意义的表示，能够从有限数据中更有效地泛化。

Conclusion: SAEs为低数据情况下的蛋白质功能预测和设计提供了有效的解决方案，通过利用pLM表示中的生物基序，显著提高了蛋白质工程的成功率。

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [30] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: DrugReasoner是基于LLaMA架构的推理型大语言模型，通过GRPO微调来预测小分子药物批准概率，在保持竞争力的预测准确性的同时提供可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程复杂且资源密集，需要早期预测批准结果以优化研究投资。传统机器学习方法虽然有效但可解释性有限，限制了其实际影响。

Method: 基于LLaMA架构构建DrugReasoner模型，使用GRPO进行微调，整合分子描述符和与结构相似化合物的比较推理，生成预测结果、逐步推理过程和置信度分数。

Result: 在验证集上AUC为0.732，F1分数为0.729；测试集上AUC为0.725，F1分数为0.718。在外部独立数据集上AUC为0.728，F1分数为0.774，优于传统基线方法和ChemAP模型。

Conclusion: DrugReasoner不仅提供竞争力的预测准确性，还通过推理输出增强透明度，解决了AI辅助药物发现中的关键瓶颈，展示了推理增强型LLM在药物决策中的潜力。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [31] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 提出一种具有稀疏频谱的线性交易头寸方法，通过Krasnosel'skiħ-Mann固定点算法优化，能够探索预测矩阵的更大频谱区域，在各种情况下获得良好且稳健的性能。


<details>
  <summary>Details</summary>
Motivation: 主投资组合方法在基于信号的交易中可能不够多样化，无法充分探索预测矩阵的关键特征，且在不同情况下缺乏稳健性。

Method: 提出稀疏频谱线性交易头寸方法，开发Krasnosel'skiħ-Mann固定点算法进行优化，该算法具有下降性质和线性收敛速度。

Result: 该方法在预测矩阵的频谱区域探索方面表现更好，在各种情况下都能获得良好且稳健的交易性能。

Conclusion: 所提出的稀疏频谱线性交易头寸方法通过新颖的优化算法，有效解决了主投资组合方法的局限，在理论和实验上都取得了显著成果。

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [32] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 本文提出了一种结合多尺度特征提取和不确定性估计的无监督域适配方法，通过混合输入架构和证据学习机制，在时间序列数据上实现了更好的域适配效果和预测信心检验。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列数据中训练和测试集间分布偏移的问题，提高模型在未标注测试数据上的汇聚性能和稳健性。

Method: 采用多尺度混合输入架构提取不同尺度特征，减少域间特征差异；基于证据学习引入不确定性感知机制，通过Dirichlet先验分布对标签进行预测和不确定性估计。

Result: 在多个标准数据集上达到了状态前沿性能，预测信心检验错误（ECE）显著降低，表明预测信心更加准确。

Conclusion: 结合混合输入架构和不确定性感知机制的方法在时间序列数据的无监督域适配中具有高效性和强镇健性。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [33] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: STRATA-TS是一个针对城市时间序列预测的选择性迁移学习框架，通过目标感知检索和大型语言模型推理，解决数据稀缺城市的数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 城市预测面临严重的数据不平衡问题：少数城市有密集的长期记录，而许多其他城市只有短期或不完整的历史数据。直接迁移不可靠，因为只有有限的源模式真正有益于目标域，而不加选择的迁移可能会引入噪声和负迁移。

Method: STRATA-TS结合领域适应性检索和推理能力强大的大型模型，采用基于补丁的时间编码器识别与目标查询语义和动态对齐的源子序列，然后将检索到的样本注入检索引导的推理阶段，由LLM对目标输入和检索支持进行结构化推理。

Result: 在新加坡、诺丁汉和格拉斯哥的三个停车可用性数据集上的广泛实验表明，STRATA-TS始终优于强大的预测和迁移基线，同时提供可解释的知识迁移路径。

Conclusion: 该框架通过选择性迁移和推理能力，有效解决了数据稀缺环境下的城市预测问题，并通过监督微调实现了高效部署。

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [34] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: BDVAE是一种深度生成模型，通过模态和通路特异性编码器整合转录组和基因组数据，准确预测免疫检查点抑制剂治疗反应，并揭示连续性耐药机制。


<details>
  <summary>Details</summary>
Motivation: 免疫检查点抑制剂治疗效果差异大，现有机器学习模型缺乏可解释性且未能有效利用多组学数据的生物结构，需要开发能够揭示耐药生物学机制的可解释模型。

Method: 提出生物解缠变分自编码器(BDVAE)，采用模块化编码器架构结合变分推断，学习与免疫、基因组和代谢过程相关的生物学意义潜在特征。

Result: 在366名患者的泛癌队列中，BDVAE准确预测治疗反应(AUC-ROC=0.94)，发现关键耐药机制包括免疫抑制、代谢转变和神经元信号传导，揭示耐药是连续性生物谱而非严格二元状态。

Conclusion: BDVAE证明了生物结构化机器学习在阐明复杂耐药模式和指导精准免疫治疗策略方面的价值，能够生成可解释的临床相关见解。

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [35] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 提出了一个新颖的多模态金融风险评估框架，整合文本情感分析和基于高管声带动力学的副语言线索，能够解释43.8%的30天已实现波动率方差。


<details>
  <summary>Details</summary>
Motivation: 金融市场信息不对称问题严重，传统文本分析方法效果有限，需要从高管语音中提取更可靠的生物特征信号来识别隐藏的企业不确定性。

Method: 使用物理信息声学模型(PIAM)从收益电话会议原始音频中提取情感特征，将声学和文本情感状态投影到三维情感状态标签空间(紧张度、稳定性、唤醒度)，分析高管在脚本化演讲和自发问答环节的情感动态变化。

Result: 多模态特征无法预测股票方向性收益，但能解释高达43.8%的30天已实现波动率样本外方差。情感动态变化，特别是CFO的文本稳定性降低和声学不稳定性增加，以及CEO的唤醒度变化，是波动率预测的关键驱动因素。

Conclusion: 多模态方法显著优于仅使用财务数据的基准模型，声学和文本模态具有互补性，为投资者和监管机构提供了增强市场可解释性和识别隐藏企业不确定性的强大工具。

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [36] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: FFT MoE是一个新颖的联邦微调框架，用稀疏专家混合（MoE）适配器替代LoRA，解决了异构联邦学习环境中结构不兼容和适应性问题，通过轻量级门控网络和异构感知辅助损失实现更好的泛化性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型向AGI发展，在隐私和资源约束下进行微调变得至关重要。联邦学习通过联邦微调（FFT）提供了解决方案，但现有的LoRA-based FFT在异构FL环境中面临结构不兼容和非IID数据适应性问题。

Method: 提出FFT MoE框架，用稀疏MoE适配器替代LoRA，每个客户端训练轻量级门控网络选择激活个性化专家子集。引入异构感知辅助损失动态正则化路由分布，确保专家多样性和平衡利用。

Result: 在IID和非IID条件下的广泛实验表明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线方法。

Conclusion: FFT MoE通过MoE适配器和异构感知路由机制有效解决了联邦微调中的异构性问题，为资源受限的边缘设备提供了高效的协作模型适应方案。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [37] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 本文发现现有的近似机器遗忘方法可能损害差分隐私模型中保留样本的隐私，提出了新的隐私审计标准和高效的MIA方法A-LiRA，证明需要差分隐私遗忘算法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法假设保留数据不受影响，但隐私洋葱效应表明这种假设可能不正确，特别是在差分隐私模型中，需要全面审计遗忘和保留样本的隐私风险。

Method: 提出基于差分隐私和成员推理攻击的隐私标准，开发高效MIA方法A-LiRA（利用数据增强减少影子模型训练成本），对现有近似遗忘算法进行实验审计。

Result: 实验发现现有近似机器遗忘算法可能无意中损害差分隐私模型中保留样本的隐私，证明需要开发差分隐私遗忘算法。

Conclusion: 需要开发差分隐私的机器遗忘算法来确保保留样本的隐私保护，现有方法在差分隐私模型中存在隐私风险。

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [38] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 本文研究了MoE模型稀疏性对记忆和推理能力的影响，发现在固定计算预算下，记忆能力随总参数增加而提升，而推理能力会饱和甚至下降。


<details>
  <summary>Details</summary>
Motivation: 现有经验缩放定律主要针对密集模型，而MoE模型引入了新的稀疏维度，需要研究稀疏性如何影响不同能力体系（记忆vs推理）。

Method: 训练不同总参数、激活参数和top-k路由的MoE Transformer家族，固定计算预算，记录预训练损失、下游任务损失和准确率。

Result: 记忆基准随总参数单调改善，推理性能会饱和甚至回归；改变top-k路由影响很小；强化学习和额外测试计算无法挽救过度稀疏模型的推理缺陷。

Conclusion: MoE稀疏性对记忆和推理能力有不同影响，推理能力存在最优稀疏度，过度稀疏会损害推理性能，即使训练损失持续改善。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [39] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: LRTab是一种新颖的提示学习方法，通过从训练数据中检索相关信息来结合微调和零样本提示的优势，在表格推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理方法要么需要微调LLMs（泛化性差），要么使用零样本提示（无法充分利用训练数据），需要一种能结合两者优势的方法。

Method: 首先通过提示获取训练数据的思维链响应，对错误的思维链预测提示条件以避免错误，然后在推理时检索最相关的提示条件作为额外上下文。

Result: 在WikiTQ和Tabfact数据集上的实验表明，LRTab具有可解释性、成本效益高，并且能够超越之前的基线方法。

Conclusion: LRTab成功地将训练数据的学习与提示检索相结合，为表格理解任务提供了一种有效且高效的解决方案。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [40] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 提出了一种端到端深度学习框架，结合自编码器和多层感知机，用于ICU脓毒症早期检测，在三个ICU队列中准确率分别达到74.6%、80.6%和93.5%，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖人工特征工程，难以处理电子健康记录中不规则、不完整的时间序列数据，需要开发更有效的脓毒症早期检测方法。

Method: 使用无监督自编码器进行自动特征提取，结合多层感知机分类器；采用定制下采样策略提取高信息密度片段，使用非重叠动态滑动窗口机制进行实时推理；预处理时间序列数据为固定维度向量并包含缺失值指示器。

Result: 在三个ICU队列中分别获得74.6%、80.6%和93.5%的准确率，持续优于传统机器学习基线方法（朴素贝叶斯、SVM、随机森林、XGBoost）。

Conclusion: 该端到端框架在异质ICU环境中展现出优异的鲁棒性、泛化能力和临床实用性，适用于脓毒症的早期检测。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [41] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: 这篇论文提出了一种统一框架，结合统计物理、编码理论咋代数拓扑学，通过在Nishimori温度下运行随机铅伊斯模型，实现了高效的多类图像分类。该方法将1280维特征压缩到32-64维，在ImageNet-10和ImageNet-100上达到了独特的性能。


<details>
  <summary>Details</summary>
Motivation: 将高维特征向量的分类问题与统计物理中的随机铅伊斯模型相结合，通过拓扑学指导的图设计来提高分类效果和效率。

Method: 使用冻结的MobileNetV2背链提取高维特征，将其解释为散布在MET-QC-LDPC图上的旋旋，构成RBIM模型。在Nishimori温度下运行，利用二次插值和牛顿校正高效估计温度。通过拓扑学指导设计球面和地面图集合来压制有害的捕捉集。

Result: 在40倍参数压缩的情况下，在ImageNet-10上达到98.7%的准确率，在ImageNet-100上达到82.7%的准确率，显示出独特的性能。温度估计算法比二分法快6倍。

Conclusion: 拓扑学指导的图设计能够产生高效的、受物理启发的嵌入表达，在大规模压缩后仍能保持独特的分类性能。

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [42] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: StructRTL是一个基于控制数据流图的结构感知自监督学习框架，通过结合结构学习和跨阶段监督，显著提升了RTL设计质量估计的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的方法忽略了RTL代码的结构语义，而控制数据流图能更好地暴露设计结构特征，为质量估计提供更丰富的线索。

Method: 提出结构感知的图自监督学习框架，从CDFG学习结构感知表示，并采用知识蒸馏策略将后映射网表的低级洞察转移到CDFG预测器中。

Result: 在各种质量估计任务上显著优于现有方法，建立了新的最先进结果。

Conclusion: 结合结构学习和跨阶段监督的方法在RTL设计质量估计中非常有效，证明了结构信息的重要性。

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [43] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: FLAegis是一个两阶段防御框架，使用符号时间序列变换和谱聚类来检测拜占庭客户端，并通过FFT聚合提高联邦学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性限制了训练过程的可见性，依赖客户端诚实性，容易被恶意拜占庭客户端通过提交虚假模型更新来毒化训练过程。

Method: 采用两阶段防御框架：1）使用符号时间序列变换（SAX）放大良性模型与恶意模型的差异；2）使用谱聚类准确检测对抗行为；3）集成基于FFT的鲁棒聚合函数作为最终防御层。

Result: 在五种投毒攻击（从简单标签翻转到自适应优化策略）的严格评估中，该方法在检测精度和最终模型准确性方面均优于最先进的防御方法，即使在强对抗条件下也能保持高性能。

Conclusion: FLAegis框架有效提高了联邦学习系统对拜占庭攻击的鲁棒性，通过多阶段防御机制成功识别和缓解恶意客户端的影响。

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [44] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 本文分析了离线强化学习和离线逆强化学习中Bellman残差最小化(BRM)方法的统计性能，提出了新的稳定性分析和样本复杂度界限


<details>
  <summary>Details</summary>
Motivation: 当前离线RL和离线IRL实践中难以有效执行Bellman一致性，虽然BRM方法已有全局收敛的随机梯度下降-上升算法，但其在离线设置下的统计行为尚未充分研究

Method: 引入单个Lyapunov势能函数来耦合相邻数据集上的SGDA运行，获得O(1/n)的平均参数稳定性界限，并将该稳定性常数转化为BRM的O(1/n)超额风险界限

Result: 获得了比凸-凹鞍点问题已知最佳样本复杂度指数高一倍的稳定性界限，且结果适用于标准神经网络参数化和小批量SGD，无需方差缩减、额外正则化或对minibatch采样的限制性独立假设

Conclusion: 本文填补了BRM方法在离线设置下的统计空白，为Bellman残差最小化提供了理论保证，推动了离线强化学习理论的发展

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [45] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 提出基于约束缩减的MILP模型简化方法，通过识别关键紧约束并转为等式来加速求解，相比现有方法提升解质量50%以上，减少计算时间17.47%


<details>
  <summary>Details</summary>
Motivation: 现有模型简化方法主要基于变量缩减，而约束缩减这一对偶视角被忽视，但将不等式约束转为等式同样能降低MILP复杂度

Method: 首先标记最优解处的紧约束作为候选关键约束，设计启发式规则选择关键子集；提出多模态表示技术，利用实例级和抽象级MILP表述信息来学习关键紧约束

Result: 相比最先进方法，解质量提升超过50%，计算时间减少17.47%

Conclusion: 约束缩减是有效的MILP模型简化方法，多模态表示技术能有效识别关键约束，显著提升求解效率和解质量

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [46] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: UltraMemV2是一种改进的内存层架构，通过五项关键改进实现了与8专家MoE模型相当的性能，同时显著降低了内存访问成本，在内存密集型任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 混合专家(MoE)模型虽然通过激活参数子集实现了高效性，但在推理过程中存在高内存访问成本的问题。现有的内存层架构如UltraMem只能匹配2专家MoE模型的性能，无法达到最先进的8专家配置水平。

Method: 提出了五项关键改进：1) 在每个transformer块中集成内存层；2) 使用单线性投影简化值扩展；3) 采用基于FFN的值处理；4) 实施原则性参数初始化；5) 重新平衡内存到FFN的计算比例。

Result: UltraMemV2在相同计算和参数条件下实现了与8专家MoE模型的性能相当，但内存访问显著降低。在内存密集型任务上表现优异：长上下文记忆+1.6分，多轮记忆+6.2分，上下文学习+7.9分。验证了激活密度比总稀疏参数数量对性能影响更大。

Conclusion: UltraMemV2使内存层架构达到了与最先进MoE模型相当的性能，为高效稀疏计算提供了一个有吸引力的替代方案。

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [47] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Helen Pervez,Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: 提出了Governance-as-a-Service (GaaS)框架，作为分布式AI系统的模块化治理层，通过声明式规则和信任因子机制在运行时监管智能体行为，无需修改模型内部结构。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向分布式生态系统发展，现有监管机制存在反应性、脆弱性和不可审计性问题，缺乏可扩展的解耦治理方案。

Method: 采用声明式规则和信任因子评分机制，支持强制、规范和自适应干预，在运行时拦截和评估智能体输出，实现分级执法和动态信任调节。

Result: 在三个模拟场景中使用开源模型测试，GaaS能可靠阻止高风险行为，保持系统吞吐量，信任分数有效跟踪规则遵守情况，隔离不可信组件。

Conclusion: GaaS将治理定位为运行时服务，为可互操作的智能体生态系统建立基础设施级别的对齐机制，不是教导智能体伦理，而是强制执行。

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [48] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: HGNN-DDI是一个基于异构图神经网络的药物相互作用预测模型，通过整合多种药物相关数据源，在预测准确性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用(DDIs)是临床实践中的主要问题，可能导致疗效降低或严重不良反应。传统计算方法难以捕捉药物、靶点和生物实体之间的复杂关系。

Method: 提出HGNN-DDI异构图神经网络模型，通过图表示学习建模异质生物医学网络，实现跨不同节点和边类型的有效信息传播。

Result: 在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于最先进的基线方法。

Conclusion: HGNN-DDI显示出支持更安全药物开发和精准医学的潜力，为解决药物相互作用预测问题提供了有效解决方案。

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [49] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文研究联邦学习中客户端标签集异质性问题，比较了公开标签集和私有标签集两种设置，提出了适应私有标签集的标准联邦学习方法，实验表明这些方法在保持隐私的同时能达到与公开标签集相似的性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中客户端标签集异质性很常见，但很少在联邦学习中被研究。特别是当客户端只与中央服务器共享私有标签集时，学习算法面临更多约束，这是一个更难解决的问题。

Method: 将分类器组合问题的经典方法应用于联邦学习，采用集中调优；调整常见联邦学习方法以适应私有标签集设置；通过实验比较不同方法在公开和私有标签设置下的性能。

Result: 减少每个客户端可用的标签数量会显著损害所有方法的性能。集中调优客户端模型以进行表示对齐可以缓解这一问题，但通常以更高方差为代价。提出的标准联邦学习方法适应方案在私有标签设置下表现良好，与标准方法在公开设置下的性能相似。

Conclusion: 客户端可以在几乎不损失模型准确性的情况下享受更高的隐私保护，提出的方法适应方案在私有标签联邦学习设置中有效可行。

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [50] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: SWiFT是一种高效的机器学习去偏框架，通过软掩码权重微调技术，只需少量外部数据和少量训练轮次即可改善模型公平性，同时保持判别性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在医疗等敏感领域存在偏见问题，现有去偏方法需要原始训练数据、大量重新训练，且在公平性和性能之间存在权衡。

Method: 首先识别模型参数对偏见和预测性能的相对贡献，然后通过两步微调过程，根据参数贡献定义不同的梯度流来更新每个参数。

Result: 在三个偏见敏感属性和六个医学数据集上的实验表明，SWiFT能持续减少模型偏见，在公平性和准确性指标上达到或超越最先进方法，并展现出更好的泛化能力。

Conclusion: SWiFT提供了一种高效、低成本的去偏解决方案，在保持模型性能的同时显著改善公平性，具有很好的实际应用价值。

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [51] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 基于深度强化学习的恶意软件检测方法，通过马尔可夫决策过程同时优化分类性能和拒绝高风险样本，提高对概念漏洒的适应能力


<details>
  <summary>Details</summary>
Motivation: 传统分类器在恶意软件检测中面临概念漏洒、标签预算有限和预测不确定性等挑战，需要新方法来同时优化检测性能和适应变化

Method: 将恶意软件检测形式化为一步马尔可夫决策过程，训练深度强化学习代理，同时优化样本分类性能和拒绝高风险样本进行手动标签

Result: 在Android恶意软件数据集上，DRMD代理在分类、拒绝和主动学习各种设置下均实现了AUT性能提升（分别为5.18±5.44、14.49±12.86、10.06±10.81），显示出更好的概念漏洒适应能力

Conclusion: 这是首次证明深度强化学习可以在Android恶意软件动态环境中实现有效的检测和提高对概念漏洒的弹性能力

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [52] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 提出了一种新的上下文决斗bandit模型，用户消费推荐物品后与历史物品进行比较，通过矩阵浓度界限证明获得O(√T)遗憾保证


<details>
  <summary>Details</summary>
Motivation: 现有上下文决斗bandit模型只捕获用户导航时的隐式选择，但用户消费物品后能提供更可靠的反馈，需要新模型来利用这种比较查询

Method: 算法每次推荐一个物品，用户消费后要求与历史消费记录中的另一个物品进行比较，通过初始随机探索阶段积累丰富历史，利用矩阵浓度界限进行分析

Result: 理论证明获得O(√T)的遗憾上界，模拟显示重用历史物品进行比较比仅比较同时推荐物品能显著降低遗憾

Conclusion: 提出的新bandit模型能有效利用用户消费后的可靠反馈，通过重用历史比较物品实现更好的性能，为推荐系统提供了新的交互范式

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [53] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: C-Flat是一种针对持续学习的平坦损失景观优化方法，通过促进更平坦的最小值来提高模型稳定性和性能，并提供了高效的C-Flat++变体。


<details>
  <summary>Details</summary>
Motivation: 持续学习中需要在敏感性和稳定性之间取得平衡。虽然现有的锐度感知最小化方法在持续学习中有效，但仅依赖零阶锐度可能在特定设置下偏好更尖锐的最小值，导致解决方案不够鲁棒和次优。

Method: 提出了C-Flat方法，专门为持续学习设计以促进更平坦的损失景观。该方法具有即插即用兼容性，可轻松集成到现有代码流程中。还提出了C-Flat++框架，利用选择性平坦性驱动提升，显著降低更新成本。

Result: 实验结果表明，C-Flat在多种持续学习方法、数据集和场景中都能一致提升性能。C-Flat++在保持有效性的同时显著降低了所需的更新成本。

Conclusion: C-Flat和C-Flat++为持续学习提供了有效的平坦性优化解决方案，能够改善模型在持续学习任务中的表现，同时保持计算效率。

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [54] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: MOCHA是一个新颖的框架，用于在时间点过程中发现多阶动态因果关系，通过建模时变有向无环图和端到端可微分框架，在事件预测和因果结构发现方面都取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖静态或一阶因果结构，忽略了因果关系的多阶性和时变特性，无法充分建模真实世界事件序列中的复杂因果依赖关系。

Method: 提出MOCHA框架，将多阶影响建模为潜在时变图上的多跳因果路径，使用时变有向无环图（DAG）和可学习结构权重，施加无环性和稀疏性约束，设计端到端可微分框架联合建模因果发现和时间点过程动态。

Result: 在真实世界数据集上的广泛实验表明，MOCHA不仅在事件预测方面达到了最先进的性能，而且揭示了有意义且可解释的因果结构。

Conclusion: MOCHA框架成功解决了时间点过程中多阶动态因果关系的发现问题，为建模复杂事件序列提供了有效的解决方案，同时在预测性能和结构可解释性方面都表现出色。

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [55] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: HAEPO是一种新的探索性策略优化方法，通过历史聚合和轨迹级软最大化来增强长时域任务的探索能力，相比PPO、GRPO和DPO具有更好的收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DPO和GRPO在长时域任务中探索能力有限，需要一种能够更好利用完整轨迹历史并促进广泛探索的新方法。

Method: HAEPO将轨迹压缩为累积对数似然，应用Plackett-Luce软最大化获得与回报成正比的归一化权重，并加入熵正则化和软KL惩罚来稳定训练。

Result: 实验表明HAEPO收敛快、探索充分、与真实奖励对齐紧密，在多样化任务中表现优于或相当于PPO、GRPO和DPO。

Conclusion: HAEPO提供了一个稳定且可解释的框架，通过显式利用完整轨迹历史来平衡探索和稳定性。

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [56] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: pyFAST是一个基于PyTorch的时间序列分析框架，专注于模块化设计和复杂数据处理，支持多源数据、稀疏数据和不规则数据的处理，并提供丰富的模型库和训练工具。


<details>
  <summary>Details</summary>
Motivation: 现有Python时间序列库在模块化、不规则数据、多源数据和稀疏数据支持方面存在局限性，需要更灵活、高效和可扩展的框架来支持研究需求。

Method: 采用数据与模型计算解耦的设计，构建数据引擎支持多源加载、蛋白质序列处理、动态归一化等功能；集成LLM架构进行稀疏数据融合；提供稀疏指标、专用损失函数和训练工具。

Result: 开发了一个紧凑而强大的时间序列研究平台，支持线性模型、CNN、RNN、Transformer、GNN等多种模型，并在GitHub上以MIT许可证发布。

Conclusion: pyFAST为时间序列研究提供了一个模块化、高效且功能丰富的框架，特别适合处理复杂数据场景和快速实验需求。

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [57] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: DNP是一种改进的神经过程变体，通过结合全局和距离感知的局部潜在结构来提升不确定性估计能力，解决了标准神经过程在校准不确定性和捕捉局部数据依赖方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准神经过程主要依赖全局潜在变量，在校准不确定性和捕捉局部数据依赖方面存在困难，需要一种能够同时建模任务级变化和输入相似性的方法。

Method: 引入全局潜在变量建模任务级变化，局部潜在变量在距离保持的潜在空间中捕捉输入相似性，通过bi-Lipschitz正则化限制输入关系的扭曲并保持相对距离。

Result: DNP在回归和分类任务中实现了强大的预测性能和改进的不确定性校准，能够更有效地区分分布内和分布外数据。

Conclusion: 距离感知神经过程通过结合全局和局部潜在结构，显著提升了不确定性估计的质量和数据依赖关系的建模能力。

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [58] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 该方法在联邦学习中保持模型性能的同时，提供了比基线方法更强的模型参数保护


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中模型性能与参数隐私保护之间的平衡问题，在保持模型有效性的同时增强参数安全性

Method: 未在摘要中明确说明具体方法，但提到通过某种创新方法实现性能保持和隐私增强

Result: 实验结果表明该方法在各种模型和任务中都能保持强大的模型性能，同时相比基线方法实现了更好的模型参数保护

Conclusion: 该方法成功地在联邦学习环境中平衡了模型性能与隐私保护，为联邦学习提供了更安全的解决方案

Abstract: Experimental results across various models and tasks demonstrate that our
approach not only maintains strong model performance in federated learning
settings but also achieves enhanced protection of model parameters compared to
baseline methods.

</details>


### [59] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 本文分析了具有一般非线性动态函数的神经ODE的泛化误差界，首次为这类模型建立了理论保证


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注线性动态函数或依赖于采样间隔的神经控制ODE，缺乏对一般非线性动态函数神经ODE泛化性能的理论分析

Method: 在动态函数关于状态变量Lipschitz连续的条件下，证明神经ODE解具有有界变差，并基于此建立时间依赖和时间独立情况的泛化界

Result: 成功推导出一般非线性动态函数神经ODE的泛化误差界，并分析了过参数化和域约束对界的影响

Conclusion: 这是首次为具有一般非线性动态的神经ODE建立泛化界，为这类连续深度模型的理论理解提供了重要贡献

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [60] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: HierCVAE是一个结合分层注意力机制和条件变分自编码器的新架构，用于复杂系统的时间建模，在预测精度和不确定性校准方面显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 复杂系统中的时间建模需要捕捉多个时间尺度的依赖关系并管理固有不确定性，现有方法在这方面存在不足

Method: 采用三层注意力结构（局部、全局、跨时间）结合多模态条件编码，在潜在空间中使用ResFormer块，并通过预测头提供显式不确定性量化

Result: 在能源消耗数据集上评估显示，预测精度提高15-40%，不确定性校准优于最先进方法，在长期预测和复杂多变量依赖方面表现优异

Conclusion: HierCVAE通过分层注意力机制和条件变分自编码器的集成，有效解决了复杂时间建模中的多尺度依赖和不确定性管理问题

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [61] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文提出了一种基于能量视角的流匹配方法，用于分子结构生成，通过迭代映射随机配置到目标结构，在蛋白质对接和骨架生成任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 分子结构生成在分子对接、蛋白质折叠和分子设计等生物应用中至关重要。现有生成模型如扩散模型和流匹配已取得进展，但需要从能量角度改进训练和推理

Method: 采用能量视角的流匹配方法，通过深度网络学习映射函数，迭代地将随机配置（源分布样本）映射到目标结构（数据流形中的点）

Result: 在蛋白质对接和蛋白质骨架生成实验中，该方法在相同计算预算下优于最近的流匹配和扩散模型基线

Conclusion: 提出的能量视角流匹配方法概念简单、实证有效，具有理论依据，并与幂等性、稳定性等基本特性以及AlphaFold中的结构精炼技术有有趣联系

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [62] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: 本文比较了三种模型（多元Probit、多元Bernoulli和分阶段Logit）在估计多标签条件标签协方差方面的性能，发现多元Probit模型在错误率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多标签数据分析需要先检验标签依赖性，但由于标签值依赖于协变量，无法直接测量标签独立性，需要通过条件标签协方差来评估。

Method: 使用三种模型（多元Probit、多元Bernoulli和分阶段Logit）来估计恒定和依赖的多标签条件协方差，并通过实验观察各模型的条件协方差测量能力。

Result: 所有模型在测量恒定和依赖协方差方面表现相当，但都会错误地将恒定协方差检测为依赖协方差。多元Probit模型的错误率最低。

Conclusion: 虽然三种模型都能较好地测量条件协方差，但都存在误检问题。多元Probit模型在三种模型中表现最优，错误率最低。

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [63] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文研究Koopman表示在混沌动力系统中的泛化能力，重点关注其在预测和控制任务间的可迁移性。使用Lorenz系统作为测试平台，提出三阶段方法，结果显示Koopman嵌入优于基准方法，且预训练权重在微调时固定不会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究Koopman表示在混沌动力系统中的泛化能力和跨任务可迁移性，为物理信息机器学习中的多任务学习提供基础。

Method: 三阶段方法：1）通过自编码学习Koopman嵌入；2）在下一状态预测上预训练transformer；3）针对安全关键控制进行微调。使用Lorenz系统作为测试平台。

Result: Koopman嵌入优于标准和物理信息PCA基线，实现准确且数据高效的性能。预训练transformer权重在微调时固定不会导致性能下降，表明学习到的表示捕获了可重用的动力结构。

Conclusion: Koopman嵌入可作为物理信息机器学习中多任务学习的基础，学习到的表示能够捕获可重用的动力结构而非任务特定模式。

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [64] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: PAX-TS是一个模型无关的后处理算法，用于解释时间序列预测模型及其预测结果，通过局部输入扰动提供多粒度解释，并能表征多元时间序列的跨通道相关性。


<details>
  <summary>Details</summary>
Motivation: 现代预测模型通常不透明且不提供预测解释，而现有的后处理可解释性方法（如LIME）不适用于预测场景，需要专门的时间序列预测解释方法。

Method: 基于局部输入扰动的模型无关后处理算法，通过扰动输入时间序列并观察预测变化来生成解释，支持多粒度解释和跨通道相关性分析。

Result: 在7种算法和10个不同数据集上的基准测试表明，PAX-TS能有效捕捉模型行为，高性能和低性能算法的解释存在差异，并识别出6类重复出现的模式，这些模式是性能的指标。

Conclusion: PAX-TS能够以不同详细程度说明时间序列预测模型的机制，其解释可用于回答关于预测的实际问题，为时间序列预测提供了有效的可解释性解决方案。

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [65] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: STDiff是一种基于条件去噪扩散模型的时间序列缺失值填补方法，通过逐步生成缺失值，基于最近已知状态和相关控制输入，在工业数据上表现优于传统窗口方法


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法将缺失值填补视为固定时间窗口内的模式完成任务，这在工业系统中往往失效，因为工业系统动态受控制动作驱动、高度非平稳且可能存在长时间连续缺失

Method: 使用条件去噪扩散模型，具有与控制理论一致的自因果偏置，基于最近已知状态和相关控制或环境输入逐步生成缺失值

Result: 在模拟缺失块的公共污水处理数据集上始终实现最低误差，优势随缺失长度增加而增强；在真实工业数据集上产生动态合理的轨迹，而基于窗口的模型往往过度平滑

Conclusion: 支持动态感知、显式条件化的填补方法作为工业时间序列的稳健方法，讨论了计算权衡和向更广泛领域的扩展

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [66] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 该论文提出了一种基于弹簧和杆件的简单物理系统，能够通过势能最小化来近似任意连续函数，性能与多层感知机相当，并发现了热力学学习障碍现象。


<details>
  <summary>Details</summary>
Motivation: 从物理角度研究学习过程，探索简单的弹簧-杆件动力系统如何通过能量最小化来实现函数逼近和学习。

Method: 使用杆件模拟分段线性函数逼近，用弹簧势能编码均方误差损失函数，通过耗散收敛到最小能量配置。

Result: 系统在回归任务中表现与多层感知机相当，发现了热力学学习障碍现象，即当系统自由能变化达到一定阈值时无法学习。

Conclusion: 这种简单物理模型有助于从物理角度更好地理解学习系统，揭示了环境波动导致的热力学学习限制。

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [67] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 该研究探索了从资源角度进行下一活动预测的预测性过程监控方法，通过比较四种模型和三种编码策略，发现LightGBM和Transformer模型在2-gram活动迁移编码下表现最佳，而随机森林在组合编码下表现最好，该方法为资源分配和人力规划提供了新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前预测性过程监控研究主要从控制流角度出发，而资源信息在下一活动预测中的作用尚未得到充分探索。资源角度的预测能够帮助改善工作组织、平衡工作负荷、预测容量，以及通过分析个人行为来提供个性化的员工支持。

Method: 研究评估了四种预测模型（LightGBM、Transformer、随机森林等）和三种编码策略（包括2-gram活动迁移编码、活动重复特征组合编码等），在四个实际数据集上进行了实验分析。

Result: 结果显示LightGBM和Transformer模型在2-gram活动迁移编码下表现最佳，而随机森林模型在组合2-gram迁移和活动重复特征的编码下表现最好。组合编码策略获得了最高的平均准确率。

Conclusion: 该研究证明了资源角度的下一活动预测的潜力，为预测性过程监控领域开启了新的研究方向，能够支持更智能的资源分配、战略性人力规划和个性化员工支持。

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [68] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 提出基于主动学习和相似性搜索的异常检测框架，使用注意力自编码器处理APT检测中的类别不平衡问题，通过相似性度量选择样本提升检测精度和标签效率


<details>
  <summary>Details</summary>
Motivation: APT攻击具有隐蔽性强和检测数据集类别极度不平衡的特点，传统方法难以有效检测，需要新的主动学习框架来应对这些挑战

Method: 基于注意力自编码器的主动学习框架，利用特征空间相似性搜索识别正常和异常样本，通过多种相似性度量方法迭代优化决策空间

Result: 在包括DARPA透明计算APT追踪在内的多个数据集上验证，证明相似性度量的选择显著影响模型收敛速度、异常检测准确率和标签使用效率

Conclusion: 为威胁情报和网络防御领域的主动学习流水线提供了相似性函数选择的可操作指导，相似性度量对APT检测性能具有关键影响

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [69] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: GradStop是一种基于梯度信息的随机早停方法，无需验证集即可防止过拟合，特别适用于数据有限场景


<details>
  <summary>Details</summary>
Motivation: 传统早停方法需要验证集，这会减少训练数据量。为了解决这个问题，作者希望利用梯度下降算法自然产生的梯度信息来实现早停

Method: 通过梯度信息估计贝叶斯后验分布，将早停问题定义为从该后验分布中采样，并使用近似后验来获得停止准则

Result: 实验表明GradStop在测试数据上实现了较小的损失，性能优于基于验证集的停止准则，特别在数据有限场景中表现优异

Conclusion: GradStop是一种有效的早停方法，能够充分利用全部数据进行训练，计算开销小，可轻松集成到梯度下降库中

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents {\sc gradstop}, a novel stochastic early stopping method that only
uses information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
{\sc gradstop} achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [70] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文深入研究了现代循环模型（如状态空间模型）在关联回忆任务中的表现，发现学习率选择对性能影响重大，揭示了循环模型与Transformer在宽度和深度扩展上的不同特性，并分析了单层Transformer的训练动态和架构组件对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管现代循环深度学习模型具有次二次复杂度优势，但近期研究表明它们在推理和记忆任务上可能不如Transformer。本文旨在深入分析关联回忆基准测试中这些模型的性能差异，特别是扩展性和优化问题的影响。

Method: 通过系统实验分析不同学习率对现代循环模型性能的影响；比较循环模型和基于注意力模型在宽度与深度扩展时的表现差异；深入研究单层Transformer的训练动态；通过架构消融研究分析Transformer和Mamba模型中各组件对性能和优化稳定性的影响。

Result: 发现学习率选择对现代循环模型性能至关重要；循环模型和注意力模型在宽度与深度扩展上表现出相反的优势模式；单层Transformer虽然性能较差，但其训练动态却惊人地类似于双层模型中观察到的归纳头形成现象；架构组件对模型的性能和优化稳定性有显著影响。

Conclusion: 现代循环模型的训练稳定性需要进一步研究，学习率的敏感性问题可能严重影响先前报道的性能结果；循环模型和Transformer在架构扩展上具有不同的优势特性；单层Transformer展现出有趣的训练动态，为理解模型内部机制提供了新视角。

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [71] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 通过透明的机器学习架构和HazBinLoss函数，解决了地震地动模型的黑盒问题和数据不平衡问题


<details>
  <summary>Details</summary>
Motivation: 传统机器学习地震地动模型存在黑盒特性和数据不平衡问题，限制了在重要决策中的应用

Method: 使用透明的ML架构，每个输入参数独立处理并线性相加，采用HazBinLoss函数给关键近场大震级记录赋予更高权重

Result: 模型能够抓取已知的地震学原理，与现有GMMs相比有相似的性能但更透明

Conclusion: 该框架促进了基于机器学习的风险评估和灾害规划的更广泛应用

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [72] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: GNN可以通过符号回归生成数值格式，从两节点图训练中推导出一阶有限体积格式，并在无监督设置下重新发现高阶格式。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在传统近似角色之外的能力，验证其能否在训练域外（如更大或结构不同的图）生成数值格式，并超越传统物理信息神经网络。

Method: 使用两节点图训练GNN，结合符号回归分析网络学到的表示；在无监督设置下使用类似PINN的残差损失训练GNN；扩展到2跳和2层GNN以发现高阶格式。

Result: GNN能够准确推导出一阶有限体积格式（误差为O(ε)），通过符号回归重新发现标准格式的解析表达式；无监督训练成功恢复一阶格式；2跳GNN发现二阶修正项，2层GNN发现经典二阶中点格式。

Conclusion: GNN不仅是强大的近似器，还能作为数值方法开发的主动贡献者，为科学计算开辟了新范式，证明其能够自主发现已知和新的数值格式。

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [73] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 基于信息理论的联邦遗忘学习框架，通过二阶Hessian信息识别敏感参数并选择性重置，实现高效的数据遗忘，在保护隐私的同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 隐私法规要求从深度学习模型中删除数据，这在联邦学习中尤为困难，因为数据保留在客户端，使得完全重训练或协调更新往往不可行

Method: 使用二阶Hessian信息识别和选择性重置对遗忘数据最敏感的参教，然后进行最小化的联邦重训练，这种模型无关的方法支持类别和客户端级别的遗忘

Result: 在基准数据集上表现出强大的隐私保护能力（MIA攻击成功率接近随机猜测，类别知识被有效擦除）和高性能（相对于重训练基准的归一化准确率约0.9），在针对性后门攻击场景中能有效消除恶意触发器

Conclusion: 为联邦学习中的数据遗忘提供了一个实用的解决方案，在效率和隐私保护之间取得了良好平衡

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [74] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: TRIGON是一个新颖的图重布线框架，通过从多个图视图中学习选择相关三角形来构建丰富的非平面三角剖分，有效解决了GNN中的过压缩和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络(GNN)的性能受到图拓扑固有问题的限制，特别是过压缩(oversquashing)和过平滑(oversmoothing)问题。现有的图重布线方法需要改进以更有效地促进信息传播。

Method: TRIGON框架通过联合优化三角形选择和下游分类性能，从多个图视图中学习选择相关三角形来构建非平面三角剖分，从而重布线图结构。

Result: 实验结果表明，TRIGON在节点分类任务上优于现有最先进方法，在多种同质性和异质性基准测试中都表现出色，产生的重布线图具有更小的直径、更大的谱间隙和更低的有效电阻。

Conclusion: TRIGON通过创新的三角剖分学习方法有效改善了图拓扑结构，为GNN性能提升提供了新的有效途径，特别是在处理图拓扑固有局限性方面表现出显著优势。

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [75] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: APT-LLM是一种针对任意精度大语言模型的综合加速方案，通过新型数据格式、矩阵乘法方法、内存管理系统和内核映射方法，在GPU上实现了显著的推理加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算需求巨大，限制了部署和实时性能。现有的量化方法在GPU上实现超低位宽量化面临Tensor Core支持有限、内存管理低效和内核优化不灵活等挑战。

Method: 提出APT-LLM方案：1）双极INT数据格式实现高效无损转换和并行计算；2）位级矩阵分解重组的任意精度矩阵乘法方法；3）基于数据恢复的内存管理系统；4）动态选择最优内核参数的内核映射方法。

Result: 在LLM推理中，相比FP16基线实现最高3.99倍加速，相比NVIDIA CUTLASS INT4加速实现最高2.16倍加速。在RTX 4090和H800上分别实现2.44倍和1.65倍加速。

Conclusion: APT-LLM有效解决了GPU上任意精度LLM加速的挑战，为超低位宽量化模型的部署提供了高效的解决方案。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [76] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一个约束优化框架，统一扩散模型的对齐和组合方法，通过拉格朗日对偶训练算法确保生成样本满足奖励约束并保持与预训练模型的接近性


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在对齐和组合时无法保证生成样本同时具备所有期望属性，存在权衡问题

Method: 约束优化框架结合拉格朗日对偶训练算法，强制对齐模型满足奖励约束并保持与预训练模型的接近性

Result: 实验证明该方法在图像生成中有效，能够满足约束条件并在对齐和组合任务上优于等权重方法

Conclusion: 提出的约束优化框架成功解决了扩散模型多目标优化中的权衡问题，为对齐和组合提供了理论保证和实用算法

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [77] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 提出结合概率众包建模和主动学习的新框架，处理多标注者噪声反馈，通过熵基查询选择优先获取最有价值的人类反馈，在多个环境中验证有效性


<details>
  <summary>Details</summary>
Motivation: 基于偏好的强化学习依赖人类反馈，但高质量人类输入成本高且稀缺，特别是在专家反馈稀少或错误代价高的领域需要更高效的反馈利用策略

Method: 扩展Advise算法支持多训练器，在线估计其可靠性，结合熵基查询选择来优先请求对最不确定的智能体动作的反馈，使用概率众包建模处理噪声多标注者反馈

Result: 在Taxi、Pacman、Frozen Lake等2D游戏和血糖控制任务中验证，在不确定轨迹上获取反馈的智能体学习更快，在血糖控制任务中超越基线方法

Conclusion: 提出的结合概率众包建模和主动学习的框架能有效处理噪声人类反馈，提高反馈利用效率，在复杂任务中展现优势

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [78] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 本文研究了基于三层玻尔兹曼机的密集联想记忆模型，通过统计力学分析推导了鞍点方程，提出了新的正则化方案使训练更稳定，并开发了网络增长算法来降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 密集联想记忆模型因其对对抗样本的鲁棒性以及与transformer注意力机制、生成扩散模型等前沿机器学习范式的紧密联系而受到关注。

Method: 使用三层玻尔兹曼机构建DAM模型，通过统计力学分析推导鞍点方程，提出新的正则化方案，并实现基于鞍点层次结构的网络增长算法。

Result: DAM模型能够学习到可解释的监督和无监督分类解决方案，新正则化方案显著提高了训练稳定性，网络增长算法大幅降低了计算成本。

Conclusion: 该研究为密集联想记忆模型提供了理论分析和实用改进，通过鞍点层次结构实现了高效训练，推动了DAM模型在实际应用中的发展。

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [79] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出了一种基于假设检验的塔式鲁棒性评估方法，用于解决现有深度学习模型鲁棒性评估中计算成本与测量精度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的深度学习应用中，现有鲁棒性评估方法存在计算成本与测量精度之间的显著权衡，限制了实际应用价值。

Method: 提出塔式鲁棒性评估方法，这是一种基于假设检验的新型实用度量标准，用于定量评估概率鲁棒性。

Result: 广泛的比较评估展示了所提出方法的优势和适用性。

Conclusion: 该方法推进了对安全关键深度学习应用中模型鲁棒性的系统理解和增强。

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [80] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 提出了一种新的模糊感知序数情感表示框架，通过建模情感模糊度的变化率来同时捕捉情感标注的模糊性和情感轨迹的时序动态特性。


<details>
  <summary>Details</summary>
Motivation: 现有连续情感识别方法要么忽略情感的模糊性，要么将模糊性视为独立且静态的时间变量，无法充分处理情感固有的模糊性和动态特性。

Method: 引入模糊感知序数情感表示框架，通过情感模糊度的变化率来建模情感模糊性，在RECOLA和GameVibe两个情感语料库上测试有界（唤醒度、效价）和无界（参与度）连续情感轨迹。

Result: 序数表示在无界标签上优于传统模糊感知模型，获得最高的CCC和SDA分数；在有界轨迹上，序数表示在SDA方面表现优异，显示出其在捕捉标注情感轨迹相对变化方面的卓越能力。

Conclusion: 模糊感知序数表示框架能够有效建模情感标注的模糊性和情感轨迹的时序动态特性，特别是在处理无界情感标签和捕捉相对变化方面表现出色。

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce \emph{ambiguity-aware
ordinal} emotion representations, a novel framework that captures both the
ambiguity present in emotion annotation and the inherent temporal dynamics of
emotional traces. Specifically, we propose approaches that model emotion
ambiguity through its rate of change. We evaluate our framework on two
affective corpora -- RECOLA and GameVibe -- testing our proposed approaches on
both bounded (arousal, valence) and unbounded (engagement) continuous traces.
Our results demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [81] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: 本文首次从理论上证明了工具集成推理(TIR)能够从根本上扩展大语言模型的能力边界，通过工具打破了纯文本模型的能力天花板，并提出了ASPO算法来优化工具使用行为。


<details>
  <summary>Details</summary>
Motivation: 虽然工具集成的大语言模型显示出巨大潜力，但缺乏解释为什么这种范式有效的理论原理。本文旨在提供首个形式化证明，阐明TIR扩展模型能力的根本原因。

Method: 引入优势塑造策略优化(ASPO)算法，直接修改优势函数来指导策略行为而不影响训练稳定性。在数学基准测试中使用Python解释器作为外部工具进行综合实验。

Result: TIR模型在pass@k指标上显著优于纯文本模型，优势不仅限于计算密集型问题，还扩展到需要抽象洞察力的问题。ASPO带来了更早的代码调用和更多交互轮次的改进工具使用行为。

Conclusion: 本研究首次为TIR的成功提供了原理性解释，将关注点从工具是否有效转向了工具如何以及为什么能够实现更强大的推理能力，揭示了模型学习使用工具进行思考的新兴认知模式。

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [82] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 提出了Token Order Prediction (TOP)方法替代Multi-Token Prediction (MTP)，通过排序学习损失来预测token顺序而非精确token，在多个NLP基准测试中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: MTP作为辅助目标在语言模型训练中表现不一致，在标准NLP基准测试中表现不佳，作者认为MTP的精确未来token预测作为辅助损失过于困难

Method: 提出Token Order Prediction (TOP)，使用学习排序损失训练模型按接近度对即将到来的token进行排序，相比MTP仅需增加单个unembedding层而非多个transformer层

Result: 在340M、1.8B和7B参数规模的模型预训练中，TOP在八个标准NLP基准测试中整体表现优于NTP和MTP，即使在较大规模下也保持优势

Conclusion: TOP是一种有效的替代MTP的辅助训练目标，通过降低预测难度（从精确token预测到顺序预测）实现了更好的性能表现

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [83] [Experiences with Model Context Protocol Servers for Science and High Performance Computing](https://arxiv.org/abs/2508.18489)
*Haochen Pan,Ryan Chard,Reid Mello,Christopher Grams,Tanjin He,Alexander Brace,Owen Price Skelly,Will Engler,Hayden Holbrook,Song Young Oh,Maxime Gonthier,Michael Papka,Ben Blaiszik,Kyle Chard,Ian Foster*

Main category: cs.DC

TL;DR: 这篇论文介绍了使用Model Context Protocol (MCP)作为统一接口，使科学研究基础设施更易于大语言模型代理发现和调用的方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型代理在科学工作流中遇到的问题：异构API接口和安全模型导致的障碍，使得研究能力难以被代理发现和使用。

Method: 通过在成熟服务（如Globus转移、计算、搜索，计算设施状态API，Octopus事件架构，以及Garden和Galaxy等领域特定工具）上实现薄层MCP服务器，作为统一接口。

Result: 通过计算化学、生物信息学、量子化学和文件系统监控等案例研究，证明了MCP导向架构在实践中的可行性。

Conclusion: 总结了实践经验，并提出了在代理导向科学中评估和信任方面的开放性挑战。

Abstract: Large language model (LLM)-powered agents are increasingly used to plan and
execute scientific workflows, yet most research cyberinfrastructure (CI)
exposes heterogeneous APIs and implements security models that present barriers
for use by agents. We report on our experience using the Model Context Protocol
(MCP) as a unifying interface that makes research capabilities discoverable,
invokable, and composable. Our approach is pragmatic: we implement thin MCP
servers over mature services, including Globus Transfer, Compute, and Search;
status APIs exposed by computing facilities; Octopus event fabric; and
domain-specific tools such as Garden and Galaxy. We use case studies in
computational chemistry, bioinformatics, quantum chemistry, and filesystem
monitoring to illustrate how this MCP-oriented architecture can be used in
practice. We distill lessons learned and outline open challenges in evaluation
and trust for agent-led science.

</details>


### [84] [CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator](https://arxiv.org/abs/2508.19073)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Bulat Ibragimov,Florina M. Ciorba,Pınar Tözün*

Main category: cs.DC

TL;DR: CARMA是一个服务器级别的任务协同定位感知资源管理系统，通过ML-based GPU内存估计和协同定位策略来解决DL训练任务在GPU上协同定位时的内存不足和性能干扰问题，显著提升GPU利用率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 企业级基础设施研究表明，深度学习训练核心资源GPU经常利用率低下。任务协同定位可以解决这一问题，但会带来内存不足崩溃和性能干扰两个挑战。

Method: 提出CARMA系统，包含GPUMemNet（基于ML的GPU内存估计框架）来最小化内存错误，引入协同定位策略限制GPU利用率以减少干扰，并提供任务崩溃后的恢复机制。

Result: 评估显示CARMA使GPU利用率提升39.3%，端到端执行时间减少约26.7%，GPU能耗降低约14.2%。

Conclusion: CARMA有效解决了DL训练任务协同定位的挑战，显著提升了GPU资源利用率、执行效率和能源效率。

Abstract: Studies conducted on enterprise-scale infrastructure have shown that GPUs --
the core computational resource for deep learning (DL) training -- are often
significantly underutilized. DL task collocation on GPUs is an opportunity to
address this challenge. However, it may result in (1) out-of-memory crashes for
the subsequently arriving task and (2) slowdowns for all tasks sharing the GPU
due to resource interference. The former challenge poses a threat to
robustness, while the latter affects the quality of service and energy
efficiency.
  We propose CARMA, a server-scale task-level collocation-aware resource
management system that handles both collocation challenges. CARMA encompasses
GPUMemNet, a novel ML-based GPU memory estimator framework for DL training
tasks, to minimize out-of-memory errors and introduces collocation policies
that cap GPU utilization to minimize interference. Furthermore, CARMA
introduces a recovery method to ensure robust restart of tasks that crash. Our
evaluation on traces modeled after real-world DL training task traces shows
that CARMA increases the GPU utilization over time by 39.3\%, decreases the
end-to-end execution time by $\sim$26.7\%, and reduces the GPU energy use by
$\sim$14.2\%.

</details>


### [85] [Managing Multi Instance GPUs for High Throughput and Energy Savings](https://arxiv.org/abs/2508.18556)
*Abhijeet Saraha,Yuanbo Li,Chris Porter,Santosh Pande*

Main category: cs.DC

TL;DR: 本文针对现代GPU（如A100、H100等）的并发性能优化挑战，提出了动态分区调度方案，包括内存估计、分区融合/分裂和进程重启机制，在科学计算和机器学习工作负载上实现了显著的吞吐量和能效提升。


<details>
  <summary>Details</summary>
Motivation: 现代GPU虽然提供强大的性能和安全性隔离功能，但由于芯片分区复杂约束，充分利用其并发性具有挑战性。需要开发有效的分区调度方案来优化各种工作负载的性能和能效。

Method: 开发了动态内存估计、分区融合、分区裂变等技术，支持进程重启以从内存错误中恢复，并采用早期重启作为优化手段。

Result: 通用工作负载获得6.20倍吞吐量提升和5.93倍能效改进；机器学习工作负载在A100 GPU上实现1.59倍吞吐量和1.12倍能效提升；LLM工作负载达到1.43倍吞吐量提升和1.11倍节能。

Conclusion: 提出的动态分区调度方案能有效利用现代GPU的并发能力，显著提升各种工作负载的性能和能源效率，特别是在大规模机器学习模型如LLM上表现优异。

Abstract: Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper
series (H100, H200) offer performance as well as security isolation features.
They also support a good amount of concurrency, but taking advantage of it can
be quite challenging due to the complex constraints on partitioning the chip.
  In this work, we develop partitioning and scheduling schemes for a variety of
workloads, ranging from scientific to modern ML workloads, including LLMs. We
develop several schemes involving dynamic memory estimation, partition fusion
and partition fission. We also support process restart to recover from
out-of-memory errors for workloads and early restart as an optimization. This
approach yields up to 6.20x throughput and 5.93x energy improvements for
general workloads; and we see 1.59x and 1.12x improvement to throughput and
energy, respectively, for ML workloads on an A100 GPU. We leverage this
technique on LLM workloads and show good improvements, including up to 1.43x
throughput improvement and 1.11x energy savings.

</details>


### [86] [Strata: Hierarchical Context Caching for Long Context Language Model Serving](https://arxiv.org/abs/2508.18572)
*Zhiqiang Xie,Ziyi Xu,Mark Zhao,Yuwei An,Vikram Sharma Mailthody,Scott Mahlke,Michael Garland,Christos Kozyrakis*

Main category: cs.DC

TL;DR: Strata是一个分层上下文缓存框架，通过GPU辅助I/O和缓存感知调度，解决了长上下文LLM服务中的KV缓存存储和I/O瓶颈问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM上下文窗口不断扩大，KV缓存存储需求超过GPU内存容量，导致分层缓存成为必要。但现有系统在将大缓存传输回GPU时面临I/O碎片化和调度延迟问题，造成性能瓶颈。

Method: Strata采用GPU辅助I/O技术解决KV缓存碎片化问题，解耦GPU和CPU内存布局，并使用缓存感知请求调度来平衡计算与I/O延迟，将不可避免的停顿与补充任务重叠执行。

Result: 在SGLang上构建并部署到生产环境，Strata相比vLLM + LMCache实现了5倍的首token时间降低，相比NVIDIA TensorRT-LLM有3.75倍加速，且不影响短上下文性能。

Conclusion: Strata框架有效解决了长上下文LLM服务中的缓存存储和I/O瓶颈，显著提升了服务性能，为实际生产部署提供了可行的解决方案。

Abstract: Large Language Models (LLMs) with expanding context windows face significant
performance hurdles. While caching key-value (KV) states is critical for
avoiding redundant computation, the storage footprint of long-context caches
quickly exceeds GPU memory capacity, forcing production systems to adopt
hierarchical caching across memory hierarchies. However, transferring large
cached contexts back to the GPU introduces severe performance bottlenecks:
fragmented I/O from paged layouts prevents full bandwidth utilization, and
existing schedulers fail to account for cache-loading delays, leaving systems
loading-bound rather than compute-bound. We present Strata, a hierarchical
context caching framework designed for efficient long context LLM serving.
Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling
GPU and CPU memory layouts and employs cache-aware request scheduling to
balance compute with I/O latency and overlapping unavoidable stalls with
complementary tasks. Built on SGLang and deployed in production, Strata
achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache
and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without
degrading short-context performance.

</details>


### [87] [Examining MPI and its Extensions for Asynchronous Multithreaded Communication](https://arxiv.org/abs/2508.18667)
*Jiakun Yan,Marc Snir,Yanfei Guo*

Main category: cs.DC

TL;DR: 本文评估了MPI的两个扩展（VCI和Continuation）在AMT运行时HPX中的性能表现，发现虽然这些扩展相比标准MPI能提升性能，但在多VCI设置下的多线程消息速率和实际系统有效性方面仍存在限制。


<details>
  <summary>Details</summary>
Motivation: 随着HPC架构复杂化和不规则科学算法的广泛采用，需要为异步多线程通信提供高效支持，特别是在异步多任务(AMT)系统中。MPI原始规范设计时未考虑这种通信模式，需要评估新扩展的实际效果。

Method: 首先使用基于HPX底层通信机制的MPI级微基准测试测量扩展的峰值性能潜力，然后将这些扩展集成到HPX中评估其在真实场景中的有效性。

Result: 结果显示这些扩展相比标准MPI能提升性能，但当前continuation提案限制了多VCI设置下的最大多线程消息速率，且推荐的每线程单VCI模式在实际系统中因注意力问题而效果不佳。

Conclusion: 需要改进VCI内部线程效率以实现可扩展的多线程通信，从而充分发挥近期MPI扩展的效益。

Abstract: The increasing complexity of HPC architectures and the growing adoption of
irregular scientific algorithms demand efficient support for asynchronous,
multithreaded communication. This need is especially pronounced with
Asynchronous Many-Task (AMT) systems. This communication pattern was not a
consideration during the design of the original MPI specification. The MPI
community has recently introduced several extensions to address these evolving
requirements. This work evaluates two such extensions, the Virtual
Communication Interface (VCI) and the Continuation extensions, in the context
of an established AMT runtime HPX. We begin by using an MPI-level
microbenchmark, modeled from HPX's low-level communication mechanism, to
measure the peak performance potential of these extensions. We then integrate
them into HPX to evaluate their effectiveness in real-world scenarios. Our
results show that while these extensions can enhance performance compared to
standard MPI, areas for improvement remain. The current continuation proposal
limits the maximum multithreaded message rate achievable in the multi-VCI
setting. Furthermore, the recommended one-VCI-per-thread mode proves
ineffective in real-world systems due to the attentiveness problem. These
findings underscore the importance of improving intra-VCI threading efficiency
to achieve scalable multithreaded communication and fully realize the benefits
of recent MPI extensions.

</details>


### [88] [ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive](https://arxiv.org/abs/2508.18850)
*Xinhao Luo,Zihan Liu,Yangjie Zhou,Shihan Fang,Ziyu Huang,Yu Feng,Chen Zhang,Shixuan Sun,Zhenzhe Zheng,Jingwen Leng,Minyi Guo*

Main category: cs.DC

TL;DR: 基于NVIDIA Hopper架构的集群通信原语ClusterReduce和ClusterGather，通过ClusterFusion框架将LLM解码的多个阶段融合为单个内核，减少内存访问和内核启动开销，实现1.61x的延迟提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM解码存在高延迟问题，主要因为运算符扩散执行导致内存交换和缓存缺失，同时现代GPU架构的集群共享内存功能缺乏高级抽象支持。

Method: 设计ClusterReduce和ClusterGather两种集群通信原语，在芯片上实现高速数据交换和约简，然后构建ClusterFusion框架，将QKV投影、注意力和输出投影等解码阶段融合为单个内核执行。

Result: 在NVIDIA H100 GPU上评测，相比最先进的推理框架，ClusterFusion在不同模型和配置下平均实现了1.61倍的突突端到突端延迚t提升。

Conclusion: 通过集群通信原语和融合执行框架，可以充分利用现代GPU架构的集群共享内存能力，有效解决LLM解码的延迚t问题，为大模型推理提供了高效的软硬件协同解决方案。

Abstract: Large language model (LLM) decoding suffers from high latency due to
fragmented execution across operators and heavy reliance on off-chip memory for
data exchange and reduction. This execution model limits opportunities for
fusion and incurs significant memory traffic and kernel launch overhead. While
modern architectures such as NVIDIA Hopper provide distributed shared memory
and low-latency intra-cluster interconnects, they expose only low-level data
movement instructions, lacking structured abstractions for collective on-chip
communication. To bridge this software-hardware gap, we introduce two
cluster-level communication primitives, ClusterReduce and ClusterGather, which
abstract common communication patterns and enable structured, high-speed data
exchange and reduction between thread blocks within a cluster, allowing
intermediate results to be on-chip without involving off-chip memory. Building
on these abstractions, we design ClusterFusion, an execution framework that
schedules communication and computation jointly to expand operator fusion scope
by composing decoding stages such as QKV Projection, Attention, and Output
Projection into a single fused kernels. Evaluations on H100 GPUs show that
ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on
average in end-to-end latency across different models and configurations. The
source code is available at https://github.com/xinhao-luo/ClusterFusion.

</details>


### [89] [SIREN: Software Identification and Recognition in HPC Systems](https://arxiv.org/abs/2508.18950)
*Thomas Jakobsche,Fredrik Robertsén,Jessica R. Jones,Utz-Uwe Haus,Florina M. Ciorba*

Main category: cs.DC

TL;DR: SIREN是一个进程级数据收集框架，使用模糊哈希技术来识别和识别HPC系统中的软件，解决了传统基于作业或文件名方法不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: HPC系统需要应用特定的洞察来分析日益复杂和多样化的工作负载，但传统的基于作业或文件名的识别方法对于用户提供的任意名称不可靠。

Method: 引入SIREN框架，通过收集进程元数据、环境信息和可执行文件模糊哈希，使用模糊哈希技术检测可执行文件的相似性，即使版本或编译方法发生变化。

Result: 在LUMI系统上的首次选择加入部署活动显示，SIREN能够提供软件使用洞察、识别已知应用的重复执行，以及基于相似性识别未知应用。

Conclusion: SIREN通过模糊哈希技术提高了HPC系统的可观测性，为系统优化和安全改进提供了有效工具。

Abstract: HPC systems use monitoring and operational data analytics to ensure
efficiency, performance, and orderly operations. Application-specific insights
are crucial for analyzing the increasing complexity and diversity of HPC
workloads, particularly through the identification of unknown software and
recognition of repeated executions, which facilitate system optimization and
security improvements. However, traditional identification methods using job or
file names are unreliable for arbitrary user-provided names (a.out). Fuzzy
hashing of executables detects similarities despite changes in executable
version or compilation approach while preserving privacy and file integrity,
overcoming these limitations. We introduce SIREN, a process-level data
collection framework for software identification and recognition. SIREN
improves observability in HPC by enabling analysis of process metadata,
environment information, and executable fuzzy hashes. Findings from a first
opt-in deployment campaign on LUMI show SIREN's ability to provide insights
into software usage, recognition of repeated executions of known applications,
and similarity-based identification of unknown applications.

</details>


### [90] [Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale](https://arxiv.org/abs/2508.18969)
*Zhuoqiang Guo,Runze Mao,Lijun Liu,Guangming Tan,Weile Jia,Zhi X. Chen*

Main category: cs.DC

TL;DR: 优化DeepFlame超临界火焰模拟软件，在Sunway和Fugaku超算上实现6180亿和1540亿网格的超临界液氧/甲烷湍流燃烧模拟，计算能力提升三个数量级


<details>
  <summary>Details</summary>
Motivation: 数十年来，包含详细化学反应和真实流体传输的超临界火焰模拟仅限于百万级网格，限制了物理系统的空间和时间尺度分辨率

Method: 从并行计算、计算效率和I/O性能三个角度优化DeepFlame软件，结合深度神经网络同时保持真实流体的力学和化学精度

Result: 在Sunway和Fugaku超算上分别达到439/1186和187/316 PFlop/s的峰值性能，实现前所未有的求解时间，计算能力超越现有水平三个数量级

Conclusion: 这一突破使高保真超临界火焰建模成为下一代火箭推进和超高能量密度系统的关键设计工具，首次实现>100个液氧/甲烷喷射器的火箭发动机燃烧实际模拟

Abstract: For decades, supercritical flame simulations incorporating detailed chemistry
and real-fluid transport have been limited to millions of cells, constraining
the resolved spatial and temporal scales of the physical system. We optimize
the supercritical flame simulation software DeepFlame -- which incorporates
deep neural networks while retaining the real-fluid mechanical and chemical
accuracy -- from three perspectives: parallel computing, computational
efficiency, and I/O performance. Our highly optimized DeepFlame achieves
supercritical liquid oxygen/methane (LOX/\ce{CH4}) turbulent combustion
simulation of up to 618 and 154 billion cells with unprecedented
time-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\%/21.8\% and
37.4\%/31.8\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304
nodes) and Fugaku (73,728 nodes) supercomputers, respectively. This
computational capability surpasses existing capacities by three orders of
magnitude, enabling the first practical simulation of rocket engine combustion
with >100 LOX/\ce{CH4} injectors. This breakthrough establishes high-fidelity
supercritical flame modeling as a critical design tool for next-generation
rocket propulsion and ultra-high energy density systems.

</details>


### [91] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: FLUX是一个联邦学习系统，专门用于在资源受限设备上高效微调MoE架构的大语言模型，通过量化分析、专家合并和动态角色分配实现了4.75倍的加速


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型联邦微调方法存在计算资源需求大、系统假设不实际等问题，无法在消费级GPU等资源受限环境下有效工作

Method: 提出三个关键技术：1）基于量化的本地分析估计专家激活；2）自适应层感知专家合并减少资源消耗；3）探索-利用策略的动态专家角色分配

Result: 在LLaMA-MoE和DeepSeek-MoE模型上实验表明，FLUX相比现有方法显著提升性能，实现了最高4.75倍的时间-准确率加速

Conclusion: FLUX系统成功解决了MoE模型在资源受限环境下的联邦微调挑战，为边缘设备上的大模型部署提供了可行方案

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


### [92] [Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance](https://arxiv.org/abs/2508.19138)
*Nicolas Vetsch,Alexander Maeder,Vincent Maillou,Anders Winka,Jiang Cao,Grzegorz Kwasniewski,Leonard Deuschle,Torsten Hoefler,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: 提出了首个能够处理实验尺度纳米带场效应晶体管(NRFET)的NEGF+GW计算方案QuaTrEx，通过创新的空间域分解方法实现了84,480原子规模的计算，在超级计算机上展现出优异的扩展性和exascale性能。


<details>
  <summary>Details</summary>
Motivation: 随着纳米电子器件尺寸缩小至几纳米，强电子-电子相互作用变得至关重要，传统DFT+NEGF方法需要扩展到GW近似来准确描述这些量子效应，但计算量巨大。

Method: 开发了NEGF+GW计算方案QuaTrEx，采用新颖的空间域分解策略，能够处理实验尺度的NRFET几何结构，支持多达84,480个原子的计算。

Result: 在Alps和Frontier超级计算机上实现了>80%的弱扩展效率，在42,240个原子规模上达到了1.15 Eflop/s的exascale FP64性能。

Conclusion: QuaTrEx是首个能够处理实验尺度NRFET器件的NEGF+GW实现，为解决纳米尺度电子器件中的强关联效应提供了强大的计算工具。

Abstract: Designing nanoscale electronic devices such as the currently manufactured
nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools
capturing all relevant quantum mechanical effects. State-of-the-art approaches
combine the non-equilibrium Green's function (NEGF) formalism and density
functional theory (DFT). However, as device dimensions do not exceed a few
nanometers anymore, electrons are confined in ultra-small volumes, giving rise
to strong electron-electron interactions. To account for these critical
effects, DFT+NEGF solvers should be extended with the GW approximation, which
massively increases their computational intensity. Here, we present the first
implementation of the NEGF+GW scheme capable of handling NRFET geometries with
dimensions comparable to experiments. This package, called QuaTrEx, makes use
of a novel spatial domain decomposition scheme, can treat devices made of up to
84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80%
weak scaling efficiency), and sustains an exascale FP64 performance on 42,240
atoms (1.15 Eflop/s).

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [93] [Exact Persistent Stochastic Non-Interference](https://arxiv.org/abs/2508.19110)
*Carla Piazza,Riccardo Romanello,Sabina Rossi*

Main category: cs.PF

TL;DR: 本文从性能角度重新审视PSNI，提出基于弱精确等价的新特征化方法EPSNI，扩展了精确等价并放松对内部动作的处理，为随机系统的非干扰分析提供了更稳健的基础。


<details>
  <summary>Details</summary>
Motivation: PSNI（持久随机非干扰）用于捕捉随机进程代数中的定量安全属性，但需要从性能导向的角度重新审视，以提供对定量可观测量的精确控制。

Method: 引入弱精确等价，扩展精确等价并放松对内部τ动作的处理，基于此定义EPSNI（精确PSNI），展示其与PSNI相同的互模拟特征和展开式特征。

Result: EPSNI具有与PSNI相同的互模拟基础和展开式特征化，并享有类似的组合性质，证明弱精确等价是分析随机系统非干扰的稳健基础。

Conclusion: 弱精确等价为随机系统中的非干扰推理提供了强有力的理论基础，EPSNI作为PSNI的变体在保持原有性质的同时提供了更好的性能导向分析能力。

Abstract: Persistent Stochastic Non-Interference (PSNI) was introduced to capture a
quantitative security property in stochastic process algebras, ensuring that a
high-level process does not influence the observable behaviour of a low-level
component, as formalised via lumpable bisimulation. In this work, we revisit
PSNI from a performance-oriented perspective and propose a new characterisation
based on a refined behavioural relation. We introduce \emph{weak-exact
equivalence}, which extends exact equivalence with a relaxed treatment of
internal (\(\tau\)) actions, enabling precise control over quantitative
observables while accommodating unobservable transitions. Based on this, we
define \emph{Exact PSNI} (EPSNI), a variant of PSNI characterised via
weak-exact equivalence. We show that EPSNI admits the same bisimulation-based
and unwinding-style characterisations as PSNI, and enjoys analogous
compositionality properties. These results confirm weak-exact equivalence as a
robust foundation for reasoning about non-interference in stochastic systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [94] [SeDA: Secure and Efficient DNN Accelerators with Hardware/Software Synergy](https://arxiv.org/abs/2508.18924)
*Wei Xuan,Zhongrui Wang,Lang Feng,Ning Lin,Zihao Xuan,Rongliang Fu,Tsung-Yi Ho,Yuzhong Jiao,Luhong Liang*

Main category: cs.AR

TL;DR: SeDA是一种针对DNN加速器的安全框架，通过带宽感知加密、最优块粒度和多级完整性验证机制，显著降低性能开销和内存访问开销


<details>
  <summary>Details</summary>
Motivation: 当前DNN加速器安全方案需要大量硬件资源并产生显著的片外内存访问开销，在自动驾驶、医疗和金融等关键应用中需要更高效的解决方案

Method: 1) 带宽感知加密机制提高硬件资源效率 2) 通过层内和层间分块模式确定最优块粒度 3) 多级完整性验证机制最小化或消除内存访问开销

Result: 实验结果显示SeDA在服务器和边缘神经处理单元上均降低了超过12%的性能开销，同时保证了强大的可扩展性

Conclusion: SeDA提供了一种高效且可扩展的DNN加速器安全解决方案，在保证机密性和完整性的同时显著降低了资源消耗和性能开销

Abstract: Ensuring the confidentiality and integrity of DNN accelerators is paramount
across various scenarios spanning autonomous driving, healthcare, and finance.
However, current security approaches typically require extensive hardware
resources, and incur significant off-chip memory access overheads. This paper
introduces SeDA, which utilizes 1) a bandwidth-aware encryption mechanism to
improve hardware resource efficiency, 2) optimal block granularity through
intra-layer and inter-layer tiling patterns, and 3) a multi-level integrity
verification mechanism that minimizes, or even eliminates, memory access
overheads. Experimental results show that SeDA decreases performance overhead
by over 12% for both server and edge neural processing units (NPUs), while
ensuring robust scalability.

</details>


### [95] [TaiBai: A fully programmable brain-inspired processor with topology-aware efficiency](https://arxiv.org/abs/2508.18961)
*Qianpeng Li,Yu Song,Xin Liu,Wenna Song,Boshi Zhao,Zhichao Wang,Aoxin Chen,Tielin Zhang,Liang Chen*

Main category: cs.AR

TL;DR: TaiBai是一款事件驱动的可编程脑启发处理器，通过层次化拓扑编码、多粒度指令集和协同设计编译器，实现了比NVIDIA RTX 3090 GPU高200倍以上的能效


<details>
  <summary>Details</summary>
Motivation: 现有脑启发芯片存在网络拓扑刚性约束和神经元可编程性有限的问题，限制了其适应性，需要开发更灵活高效的解决方案

Method: 采用时间空间脉冲稀疏性优化带宽和计算开销，设计层次化拓扑编码方案支持任意网络架构，开发多粒度指令集实现神经元和突触可编程性，构建协同设计编译器优化任务映射

Result: 在语音识别、ECG分类和脑机接口解码等任务中，TaiBai芯片上的脉冲神经网络实现了比NVIDIA RTX 3090 GPU高200倍以上的能效，同时保持相当的准确率

Conclusion: TaiBai证明了其作为可扩展、可编程和超高效解决方案的巨大潜力，适用于多尺度脑模拟和脑启发计算

Abstract: Brain-inspired computing has emerged as a promising paradigm to overcome the
energy-efficiency limitations of conventional intelligent systems by emulating
the brain's partitioned architecture and event-driven sparse computation.
However, existing brain-inspired chips often suffer from rigid network topology
constraints and limited neuronal programmability, hindering their adaptability.
To address these challenges, we present TaiBai, an event-driven, programmable
many-core brain-inspired processor that leverages temporal and spatial spike
sparsity to minimize bandwidth and computational overhead. TaiBai chip contains
three key features: First, a brain-inspired hierarchical topology encoding
scheme is designed to flexibly support arbitrary network architectures while
slashing storage overhead for large-scale networks; Second, a multi-granularity
instruction set enables programmability of brain-like spiking neuron or
synapses with various dynamics and on-chip learning rules; Third, a co-designed
compiler stack optimizes task mapping and resource allocation. After evaluating
across various tasks, such as speech recognition, ECG classification, and
cross-day brain-computer interface decoding, we found spiking neural networks
embedded on the TaiBai chip could achieve more than 200 times higher energy
efficiency than a standard NVIDIA RTX 3090 GPU at a comparable accuracy. These
results demonstrated its high potentiation as a scalable, programmable, and
ultra-efficient solution for both multi-scale brain simulation and
brain-inspired computation.

</details>


### [96] [Building an Open CGRA Ecosystem for Agile Innovation](https://arxiv.org/abs/2508.19090)
*Rohan Juneja,Pranav Dangi,Thilini Kaushalya Bandara,Zhaoying Li,Dhananjaya Wijerathne,Li-Shiuan Peh,Tulika Mitra*

Main category: cs.AR

TL;DR: 这篇论文提出了一个开放源的粗粒度可重构架构(CGRA)生态系统，包含HyCUBE CGRA、PACE SoC和Morpher设计框架，以支持超层优化和边缘计算应用。


<details>
  <summary>Details</summary>
Motivation: 现代计算工作负荷需要硬件软件协同设计来满足性能和能源目标，而开放平台和CGRA能够提供灵活的基础。

Method: 开发了三个核心组件：1) HyCUBE - 具有可重构单周期多跳互联的CGRA；2) PACE - 将HyCUBE嵌入RISC-V SoC的芯片；3) Morpher - 完全开源的架构适配型CGRA设计框架。

Result: 构建了一个完整的开源CGRA生态系统，支持架构探索、编译、模拟和验证，为空间计算提供了可扩展的基础。

Conclusion: 通过在各层面实现开放性，低了创新门槛，支持可复现研究。建议为CGRAs和空间加速器建立统一的抽象层，以实现架构可移植性和编译器创新。

Abstract: Modern computing workloads, particularly in AI and edge applications, demand
hardware-software co-design to meet aggressive performance and energy targets.
Such co-design benefits from open and agile platforms that replace closed,
vertically integrated development with modular, community-driven ecosystems.
Coarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance
of flexibility and efficiency are particularly well-suited for this paradigm.
When built on open-source hardware generators and software toolchains, CGRAs
provide a compelling foundation for architectural exploration, cross-layer
optimization, and real-world deployment. In this paper, we will present an open
CGRA ecosystem that we have developed to support agile innovation across the
stack. Our contributions include HyCUBE, a CGRA with a reconfigurable
single-cycle multi-hop interconnect for efficient data movement; PACE, which
embeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing;
and Morpher, a fully open-source, architecture-adaptive CGRA design framework
that supports design space exploration, compilation, simulation, and
validation. By embracing openness at every layer, we aim to lower barriers to
innovation, enable reproducible research, and demonstrate how CGRAs can anchor
the next wave of agile hardware development. We will conclude with a call for a
unified abstraction layer for CGRAs and spatial accelerators, one that
decouples hardware specialization from software development. Such a
representation would unlock architectural portability, compiler innovation, and
a scalable, open foundation for spatial computing.

</details>
