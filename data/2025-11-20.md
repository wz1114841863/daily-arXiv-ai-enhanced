<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 56]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2511.15015)
*Kexin Chu,Dawei Xiang,Zixu Shen,Yiwei Yang,Zecheng Liu,Wei Zhang*

Main category: cs.PF

TL;DR: DynaExq是一个运行时系统，通过动态管理专家精度来解决MoE模型在消费级GPU上的内存限制问题，相比静态量化方法显著提升精度。


<details>
  <summary>Details</summary>
Motivation: MoE模型在消费级GPU上部署受限于非活跃专家的大内存占用，静态后训练量化无法适应变化的激活模式，导致激进压缩下的精度损失。

Method: 结合热感知精度控制器、全异步精度切换流水线和无碎片内存池机制，将专家精度作为动态管理的首要资源。

Result: 在Qwen3-30B和Qwen3-80B MoE模型及六个基准测试中，DynaExq在单张RTX 5090和A6000 GPU上部署大型LLM，相比静态低精度基线精度提升高达4.03分。

Conclusion: 自适应、工作负载感知的量化是内存受限MoE服务的有效策略。

Abstract: Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.
  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.

</details>


### [2] [A Latency-Constrained, Gated Recurrent Unit (GRU) Implementation in the Versal AI Engine](https://arxiv.org/abs/2511.15626)
*M. Sapkas,A. Triossi,M. Zanetti*

Main category: cs.PF

TL;DR: 使用AMD Xilinx Versal AIE加速GRU推理，提出工作负载分配框架和AIE-PL混合设计，用于实时应用


<details>
  <summary>Details</summary>
Motivation: 为延迟受限应用（如物理实验中的在线预处理）提供可适应的神经网络部署方案，替代传统固定功能算法

Method: 提出自定义工作负载分配框架，在AIE向量处理器间分配计算，并设计AIE与可编程逻辑的混合架构

Result: 展示了在实时环境中部署可适应神经网络的潜力，提高了计算效率

Conclusion: AIE-PL混合设计为延迟受限的实时应用提供了灵活高效的GRU推理加速方案

Abstract: This work explores the use of the AMD Xilinx Versal Adaptable Intelligent Engine(AIE) to accelerate Gated Recurrent Unit (GRU) inference for latency-Constrained applications. We present a custom workload distribution framework across the AIE's vector processors and propose a hybrid AIE - Programmable Logic (PL) design to optimize computational efficiency. Our approach highlights the potential of deploying adaptable neural networks in real-time environments such as online preprocessing in the readout chain of a physics experiment, offering a flexible alternative to traditional fixed-function algorithms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [3] [Anchor-and-Connect: Robotic Aerial Base Stations Transforming 6G Infrastructure](https://arxiv.org/abs/2511.15341)
*Wen Shang,Yuan Liao,Vasilis Friderikos,Halim Yanikomeroglu*

Main category: cs.ET

TL;DR: 提出机器人空中基站(RABS)概念，通过配备能量中性锚定末端执行器，能够自主抓握或栖息在高层城市地貌上，解决传统空中基站因电池限制导致的续航时间短的问题。


<details>
  <summary>Details</summary>
Motivation: 传统空中基站(ABS)因无人机电池限制导致续航时间极短，严重限制了实际应用。需要克服这一基本限制，推动更广泛采用。

Method: 设计配备能量中性锚定末端执行器的机器人空中基站，能够自主抓握或栖息在高层城市地貌上，实现节能的锚定操作。

Result: RABS可提供数小时的无缝无线连接，而传统悬停式ABS只能提供几分钟服务。延长了服务能力，使其能够集成到无线接入网络中，在需要时增强网络容量。

Conclusion: RABS通过锚定操作显著延长服务时间，解决了传统空中基站的续航瓶颈，为无线通信网络提供了更实用的非地面基础设施解决方案。

Abstract: Despite the significant attention that aerial base stations (ABSs) have received recently, their practical implementation is severely weakened by their limited endurance due to the battery constraints of drones. To overcome this fundamental limitation and barrier for wider adoption, we propose the concept of robotic aerial base stations (RABSs) that are equipped with energy-neutral anchoring end-effectors able to autonomously grasp or perch on tall urban landforms. Thanks to the energy-efficient anchoring operation, RABSs could offer seamless wireless connectivity for multiple hours compared to minutes of the typical hovering-based ABSs. Therefore, the prolonged service capabilities of RABSs allowing them to integrate into the radio access network and augment the network capacity where and when needed. To set the scene, we discuss the key components of the proposed RABS concept including hardware, workflow, communication considerations, and regulation issues. Then, the advantages of RABSs are highlighted which is followed by case studies that compare RABSs with terrestrial micro BSs and other types of non-terrestrial communication infrastructure, such as hovering-based, tethered, and laser-powered ABSs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852)
*Mingkun Yu,Heming Zhong,Dan Huang,Yutong Lu,Jiazhi Jiang*

Main category: cs.DC

TL;DR: PolyKAN是一个GPU加速的Kolmogorov-Arnold网络算子库，通过四种优化技术实现了比现有实现快1.2-12倍的推理和训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有KAN并行实现的GPU利用率低，阻碍了其在实际应用中的采用，特别是在AI for Science领域。

Method: 设计了四种正交优化技术：查找表线性插值、2D平铺、两阶段归约和系数布局重排序，将多项式KAN层的前向和反向传播融合为优化的CUDA内核。

Result: 在高端和消费级GPU上，使用Chebyshev KAN变体进行测试，在语音、音频增强和表格回归任务上实现了1.2-10倍推理加速和1.4-12倍训练加速，且精度相同。

Conclusion: PolyKAN是首个通用的开源KAN实现，显著提升了KAN在GPU上的计算效率，为AI for Science应用提供了实用的高性能工具。

Abstract: Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\times$ faster inference and $1.4$--$12\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.

</details>


### [5] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 提出了RemoteOptiGraph建模抽象，用于在分布式内存环境中构建和求解优化问题，通过InterWorkerEdges管理跨工作节点的链接约束，支持Benders和拉格朗日分解等元算法。


<details>
  <summary>Details</summary>
Motivation: 为分布式内存系统提供统一的优化问题建模方法，避免定制化建模方式，并支持开发能利用分布式内存结构的通用元算法。

Method: 扩展Plasmo.jl中的OptiGraph模型，引入RemoteOptiGraph抽象和InterWorkerEdges，将优化问题表示为超图，节点定义模块化子问题，边编码节点间的代数链接约束。

Result: 在美国西部混合整数容量扩展模型（超过1200万变量和约束）上测试，RemoteOptiGraph结合Benders分解比无分解求解快7.5倍。

Conclusion: RemoteOptiGraph为分布式内存环境提供了灵活统一的优化建模框架，显著提升了大规模优化问题的求解效率。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo$.$jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo$.$jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [6] [GPU-Initiated Networking for NCCL](https://arxiv.org/abs/2511.15076)
*Khaled Hamidouche,John Bachan,Pak Markthub,Peter-Jan Gootzen,Elena Agostini,Sylvain Jeaugey,Aamir Shafi,Georgios Theodorakis,Manjunath Gorentla Venkata*

Main category: cs.DC

TL;DR: NCCL 2.28引入了设备API，包括GPU发起的网络(GIN)功能，支持设备端直接发起通信操作，消除了CPU协调开销，特别适用于MoE架构等需要计算与通信紧密集成的应用。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载（尤其是MoE架构）需要低延迟、细粒度的GPU间通信，传统的主机发起通信模式存在CPU协调开销，设备发起通信可以更好地满足计算与通信紧密集成的需求。

Method: GIN采用三层架构：1）NCCL核心主机端API用于设备通信器设置和集体内存窗口注册；2）设备端API支持从CUDA内核调用远程内存操作；3）网络插件架构提供双重语义（GPUDirect异步内核发起和代理）以支持广泛硬件。

Result: 通过集成DeepEP MoE通信库的全面基准测试表明，GIN在NCCL统一运行时中提供设备发起通信，将低延迟操作与NCCL的集体算法和生产基础设施相结合。

Conclusion: GIN架构成功实现了设备端直接发起通信，为现代AI工作负载特别是MoE架构提供了高效的低延迟通信解决方案。

Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.

</details>


### [7] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle是一个双层共识架构，通过BB-Core层实现高性能和低延迟，BB-Guard层提供去中心化时间戳、主动错误检测和同步恢复路径，在保持强安全性和活跃性的同时实现亚秒级最终确认。


<details>
  <summary>Details</summary>
Motivation: 解决区块链共识在安全性、延迟和去中心化之间的三难困境，高吞吐量系统通常需要牺牲去中心化或对强对手的鲁棒性，而高度去中心化和安全的系统往往性能较低。

Method: 提出双层共识架构：BB-Core层采用n=5f+1协议，以部分容错性换取低最终确认延迟；BB-Guard层提供去中心化时间戳、主动错误检测和同步恢复机制。

Result: 实验显示BB-Core相比Mysticeti减少20-25%的延迟，系统在温和同步假设下保持强安全性和活跃性，实现亚秒级最终确认和高吞吐量。

Conclusion: BlueBottle通过双层架构成功平衡了区块链共识的三难困境，在保持去中心化和安全性的同时实现了高性能和低延迟。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


### [8] [Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies](https://arxiv.org/abs/2511.15388)
*Lucianna Kiffer,Lioba Heimbach,Dennis Trautwein,Yann Vonlanthen,Oliver Gasser*

Main category: cs.DC

TL;DR: 本文首次对36个公共区块链网络进行了纵向跨网络测量研究，揭示了区块链P2P网络层的现状、规模和特征差异。


<details>
  <summary>Details</summary>
Motivation: 区块链的去中心化应用和金融系统日益扩展，但其底层P2P网络层（除少数顶级生态系统外）大多不透明，缺乏系统性测量研究。

Method: 部署15个主动爬虫，结合社区爬虫数据，进行9个月的纵向测量；利用以太坊发现协议推断19个辅助网络；开发互联网范围扫描方法，通过简单协议特定负载快速识别响应节点。

Result: 发现网络规模差异巨大（从不到10到超过10000个活跃节点）；量化了IPv4与IPv6使用趋势、自治系统和地理集中度；描述了节点流失、昼夜行为和发现协议的覆盖范围。

Conclusion: 研究揭示了区块链网络在弹性、去中心化和可观测性方面的关键差异，并建立了一个可扩展测量去中心化网络的通用框架，为持续监控和透明评估区块链基础设施奠定了基础。

Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.

</details>


### [9] [When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit](https://arxiv.org/abs/2511.15421)
*Ethan Hicks,Joseph Oglio,Mikhail Nesterenko,Gokarna Sharma*

Main category: cs.DC

TL;DR: 分析比特币交易确认最终性与交易金额和用户风险承受能力的关系，提出基于区块深度和网络延迟的确认概率模型。


<details>
  <summary>Details</summary>
Motivation: 传统比特币交易确认采用固定区块深度（如6个区块），但未考虑交易金额和用户风险偏好的差异，需要更精确的确认机制。

Method: 通过模拟和实际比特币数据分析不同网络延迟下的分叉概率，建立区块深度与确认撤销概率的关系，并应用前景理论将确认概率与交易金额和用户风险承受能力关联。

Result: 建立了区块深度与交易确认撤销概率的量化关系，证明固定区块深度确认方法不够精确，应考虑交易金额和用户风险偏好。

Conclusion: 比特币交易确认应基于交易金额和用户风险承受能力动态调整，而非采用固定区块深度，这能提供更精确和个性化的确认机制。

Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.

</details>


### [10] [Proving there is a leader without naming it](https://arxiv.org/abs/2511.15491)
*Laurent Feuilloley,Josef Erik Sedláček,Martin Slávik*

Main category: cs.DC

TL;DR: 该论文研究了在特定图类中实现亚对数位数的唯一领导者本地认证，探讨了在无环图等拓扑结构中是否仍需节点标识符。


<details>
  <summary>Details</summary>
Motivation: 网络结构对本地认证复杂度有显著影响，传统领导者认证需要O(log n)位证书，但在某些图类中可能实现更高效的认证。研究旨在探索在无环图等特定拓扑中是否能实现亚对数位数的领导者认证。

Method: 通过分析小直径图、弦图、网格图和稠密图等特定图类，研究在这些拓扑结构中设计本地认证方案，探讨是否需要节点标识符以及如何实现更紧凑的证书。

Result: 在不同图类中获得了领导者认证的复杂度结果，证明了在某些特定拓扑中确实可以实现亚对数位数的认证方案。

Conclusion: 网络拓扑结构对领导者本地认证的复杂度有决定性影响，在特定图类中可以突破传统对数位数的限制，实现更高效的认证机制。

Abstract: Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.
  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\log n)$ and $O(\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).
  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC是一个面向PIM系统的数据中心化ML编译器，通过联合优化数据重排和计算代码，解决了PIM设备与主机处理器之间数据布局不匹配的问题，在多种PIM后端上显著提升了ML内核和LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: PIM设备与主机处理器需要不同的数据布局：主机需要跨DRAM bank的连续元素分布，而PIM核心需要本地bank内的数据。这导致ML内核执行中的数据重排带来显著的性能和可编程性挑战，且需要支持多样化的PIM后端。

Method: 设计DCC编译器，集成多层PIM抽象，将数据分区策略映射到计算循环分区，应用PIM特定代码优化，并利用快速准确的性能预测模型来选择最优配置，在统一调优过程中联合优化数据重排和计算代码。

Result: 在单个ML内核评估中，DCC在HBM-PIM上实现最高7.68倍加速（平均2.7倍），在AttAcc PIM后端上实现最高13.17倍加速（平均5.75倍）。在端到端LLM推理中，DCC在AttAcc上对GPT-3和LLaMA-2实现最高7.71倍加速（平均4.88倍）。

Conclusion: DCC通过数据重排和计算代码的联合优化，有效解决了PIM系统中的数据布局挑战，为多样化的PIM后端提供了系统化的ML编译优化方案。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [12] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: CoroAMU是一个硬件-软件协同设计的系统，通过优化的协程编译器和硬件异步内存单元，在分解式内存系统中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用在分解式内存系统中面临内存延迟挑战，现有协程方法在延迟隐藏效率和运行时开销之间难以平衡。

Method: 引入编译器过程优化协程代码生成，最小化上下文并合并请求，配合硬件异步内存单元的解耦内存操作和内存引导分支预测机制。

Result: 在Intel服务器处理器上比最先进协程方法快1.51倍，在FPGA模拟的分解式系统中，在200ns和800ns延迟下分别实现3.39倍和4.87倍的平均性能提升。

Conclusion: CoroAMU通过硬件-软件协同设计有效解决了分解式内存系统中的内存延迟问题，显著提升了数据密集型应用的性能。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [13] [DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution](https://arxiv.org/abs/2511.15367)
*Xin Yang,Xin Fan,Zengshi Wang,Jun Han*

Main category: cs.AR

TL;DR: DARE是一种针对稀疏深度神经网络的不规则性容忍MPU，通过密度化ISA和过滤式预执行机制，解决了稀疏DNN在内存访问和计算方面的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前硬件与算法的协同优化不足，稀疏DNN存在不规则内存访问模式导致高缓存缺失率，以及现有矩阵ISA的步幅约束限制了多个逻辑相关稀疏操作的密度化，导致MPU处理单元利用率低下。

Method: 提出DARE架构，扩展ISA以支持稀疏操作的密度化，并配备具有过滤能力的轻量级预执行机制。

Result: 实验结果显示，DARE相比基线性能提升1.04×到4.44×，能效提升1.00×到22.8×，硬件开销比NVR低3.91×。

Conclusion: DARE通过硬件-算法协同优化有效解决了稀疏DNN的不规则性问题，显著提升了性能和能效。

Abstract: Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.
  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.
  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.

</details>


### [14] [Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism](https://arxiv.org/abs/2511.15397)
*Cong Wang,Zexin Fu,Jiayi Huang,Shanshi Huang*

Main category: cs.AR

TL;DR: Hemlet是一个异构CIM芯片系统，通过集成模拟CIM、数字CIM和中间数据处理芯片来加速Vision Transformers，解决单片CIM设计的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在视觉任务中表现出色，但需要大量内存和计算资源，单片CIM设计存在可扩展性问题，而芯片设计虽然可扩展但通信成本高。

Method: 设计异构CIM芯片系统，集成模拟CIM、数字CIM和中间数据处理芯片，实现灵活的资源扩展。

Result: 提高了吞吐量并减少了通信开销。

Conclusion: Hemlet系统为Vision Transformers的高效能部署提供了可行的解决方案。

Abstract: Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.
  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove

</details>


### [15] [Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference](https://arxiv.org/abs/2511.15505)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出基于指令的FPGA多处理器协调架构，通过指令控制器和点对点同步单元实现可编程同步，支持DNN模型灵活分区和多种并行策略的动态切换。


<details>
  <summary>Details</summary>
Motivation: 解决FPGA多处理器系统中DNN推理加速的协调和同步问题，实现高效的计算资源利用和灵活的并行策略。

Method: 采用指令控制器与点对点指令同步单元，将指令分为加载、计算和存储功能组，通过编译框架将DNN模型转换为可执行指令程序。

Result: 在ResNet-50上的实验显示计算效率高达98%，吞吐效率比现有工作提升2.7倍。

Conclusion: 该架构支持设计空间探索，在单批次和多批次性能之间实现动态权衡，显著提升了FPGA多处理器系统的DNN推理效率。

Abstract: This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\%$, and throughput efficiency gains, up to $2.7\times$, over prior works across different configurations.

</details>


### [16] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: 提出开源chiplet-based RISC-V系统路线图，从Occamy双chiplet系统扩展到Ramora和Ogopogo四chiplet架构，探索超越逻辑核心RTL的开放生态


<details>
  <summary>Details</summary>
Motivation: 缩小开源RISC-V系统与专有设计之间的性能差距，面向高性能计算和人工智能应用

Method: 采用chiplet方法，从12nm FinFET的Occamy双chiplet系统开始，扩展到基于mesh-NoC的Ramora双chiplet系统，再到7nm的Ogopogo四chiplet架构

Result: 实现了最先进的计算密度，并探索了仿真、EDA、PDK和片外PHY等更广泛开放生态的可能性

Conclusion: 开源chiplet-based RISC-V系统在高性能计算领域具有巨大潜力，需要将开放理念扩展到整个芯片设计生态系统

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文研究了仅解码器Transformer中离散提示到隐藏状态的映射的注入性，证明了在温和条件下这种映射是通用且持续可注入的，并通过几何诊断方法在实际模型中验证了这一性质。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型中离散提示到隐藏状态映射的注入性，这对于理解模型表示能力和实际可逆性具有重要意义。

Method: 定义了碰撞判别式和注入层，证明了注入性的二分法，并在实际模型中使用分离边界和共Lipschitz常数等几何诊断方法进行实证研究。

Result: 在连续参数理想化下，Transformer表示是通用且持续可注入的；在实际模型中，全精度和8位量化下未发现碰撞，4位量化会导致少量碰撞和共Lipschitz估计显著缩小。

Conclusion: Transformer表示在连续参数理想化下具有通用且持续的注入性，其实际可逆性可通过简单的几何诊断方法进行探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [18] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了推导关系(DR)和推导能力(DC)的概念来评估LLMs的推理能力，并开发了DEVAL评估框架。评估发现主流LLMs在DR识别方面表现中等，但在应用DR解决问题时显著下降。提出的推导提示(DP)方法平均提升了15.2%的DC表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数据上的推理能力是一个重要但尚未充分研究的问题。人类推理能够根据输入的变化推导出相应的输出修改，这种基于抽象规则的推理模式在LLMs中尚未得到全面描述和评估。

Method: 正式定义了推导关系(DR)和推导能力(DC)概念；提出了系统性的评估框架DEVAL；评估了5个流行LLMs和1个大推理模型在7个主流任务中的表现；提出了推导提示(DP)这一新的提示工程方法。

Result: 主流LLMs如GPT-4o和Claude3.5在DR识别方面表现中等，但在应用DR有效解决问题时显著下降；推导提示(DP)方法在所有测试的LLMs中平均提升了15.2%的DC表现，优于常用的提示工程技术。

Conclusion: LLMs在推导能力方面存在显著不足，特别是在应用抽象规则解决实际问题时；推导提示方法能有效提升LLMs的推导能力，为改进LLMs的推理能力提供了有前景的方向。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [19] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出了动态嵌套层次结构，使模型能够在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，解决现有模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型（包括大语言模型）在静态任务中表现出色，但在非平稳环境中因架构僵化而难以持续适应和终身学习，存在逆行性遗忘问题。

Method: 基于嵌套学习范式，构建动态嵌套层次结构，允许模型自主调整优化层级、嵌套结构和更新频率，受神经可塑性启发实现无预定义约束的自我进化。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界和不同机制下的次线性遗憾分析，以及在语言建模、持续学习和长上下文推理中的实证演示，证明了动态嵌套层次结构的优越性能。

Conclusion: 动态嵌套层次结构为实现自适应、通用智能奠定了基础性进展，能够动态压缩上下文流并适应分布变化。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [20] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出GTPO算法解决多轮工具集成推理任务中现有RL方法奖励信号不足的问题，通过细粒度轮次奖励、基于回报的优势估计和自监督奖励塑造来提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮工具集成推理任务中面临奖励信号粗糙、训练停滞的问题，需要更精细的奖励机制来支持复杂的多轮交互推理。

Method: GTPO算法包含三个创新：轮次级奖励分配、基于回报的优势估计、以及利用生成代码自监督信号来丰富稀疏二元奖励的自监督奖励塑造。

Result: 在多样化推理基准测试中，GTPO平均比GRPO提升3.0%，证明其在复杂数学推理任务中的有效性。

Conclusion: GTPO通过细粒度的奖励机制成功解决了多轮工具集成推理中的训练挑战，为现实世界复杂数学推理提供了有效的训练方法。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [21] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了金融服务业中长序列交互、多渠道数据和多产品协调等挑战，相比传统树模型在性能和成本方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 金融服务业中的实时推荐面临独特挑战：长序列用户交互、数字和物理渠道的异构上下文、多产品协调需求，以及业务目标平衡。传统树模型虽然可解释性强，但在处理这些复杂场景时存在局限。

Method: 提出FinTRec框架，采用基于Transformer的架构处理长序列用户交互，支持跨产品信号共享，通过产品适配微调实现统一建模。

Result: 通过历史模拟和实时A/B测试，FinTRec在各项指标上持续优于生产级树模型基线，统一架构减少了训练成本和技术债务，同时提升了所有产品的离线性能。

Conclusion: FinTRec证明了Transformer架构在金融服务推荐中的可行性和有效性，是首个全面解决技术和业务考量的统一序列推荐模型研究。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [22] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出基于Transformer指导的深度强化学习方法来优化eVTOL无人机起飞轨迹，相比传统DRL训练效率提升75%，能耗优化准确率达到97.2%。


<details>
  <summary>Details</summary>
Motivation: eVTOL飞机有望缓解城市交通拥堵，但需要开发最小能耗的起飞轨迹。传统最优控制方法受限于问题维度和复杂度，而深度强化学习虽然能处理非线性系统但训练困难。

Method: 使用Transformer引导的深度强化学习方法，通过Transformer在每个时间步探索真实状态空间来缓解训练难度，控制变量包括功率和机翼角度。

Result: Transformer引导的DRL仅需457万时间步完成训练，比传统DRL的1979万时间步减少75%；在能耗优化方面达到97.2%的准确率，优于传统DRL的96.3%。

Conclusion: Transformer引导的DRL在训练效率和最优设计验证方面均优于传统DRL方法，为eVTOL飞机轨迹优化提供了有效解决方案。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [23] [Bringing Federated Learning to Space](https://arxiv.org/abs/2511.14889)
*Grace Kim,Filip Svoboda,Nicholas Lane*

Main category: cs.LG

TL;DR: 本文首次系统分析了将联邦学习算法应用于卫星星座的可行性，提出了"空间化"框架来适应轨道约束，并在768种星座配置下验证了这些算法能扩展到100颗卫星，性能接近集中式理想情况，训练周期可从数月缩短到数天。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座迅速扩展到数千颗航天器，需要分布式星载机器学习来解决下行带宽限制。联邦学习为卫星网络的协作模型训练提供了有前景的框架，但需要解决空间特有的约束条件。

Method: 提出了全面的"空间化"框架，将地面算法（FedAvg、FedProx、FedBuff）适应轨道约束，创建了轨道就绪的FL算法套件。通过768种星座配置的广泛参数扫描进行评估，包括集群大小、每集群卫星数和地面站网络的变化。

Result: 空间适应的FL算法能有效扩展到100颗卫星星座，性能接近集中式理想情况。多月的训练周期可缩短到数天，通过轨道调度和卫星集群内本地协调实现9倍加速。

Conclusion: 研究结果为未来任务设计者提供了可行的见解，使分布式星载学习能够实现更自主、有弹性和数据驱动的卫星操作。

Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.

</details>


### [24] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出了LIT框架，通过强制LLM使用外部可靠工具来解决问题，提高解决方案的可信度和可调试性


<details>
  <summary>Details</summary>
Motivation: LLM推理过程不透明，在关键领域难以信任，即使有更好选择也可能选择不可靠方案

Method: 基于现有LLM工具调用能力构建框架，让LLM选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用

Result: 创建了包含1300个问题的新基准数据集和可靠性成本函数，证明LLM能在保持任务性能的同时实现更可靠的问题解决

Conclusion: LIT框架使LLM能够实现更可靠和明智的问题解决，同时保持任务性能

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [25] [Structured Contrastive Learning for Interpretable Latent Representations](https://arxiv.org/abs/2511.14920)
*Zhengyang Shen,Hua Tu,Mayue Shi*

Main category: cs.LG

TL;DR: 提出结构化对比学习(SCL)框架，通过将潜在空间划分为不变特征、变异特征和自由特征三组，解决神经网络对语义无关变换的脆弱性问题，实现同时保证鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络对语义无关变换表现出严重脆弱性，如ECG信号75ms相位偏移导致潜在余弦相似度从1.0降至0.2，IMU传感器旋转使活动识别性能崩溃。根本原因是"放任"表示学习，只要任务性能满足，潜在空间就无约束演化。

Method: 提出结构化对比学习(SCL)框架，将潜在空间表示划分为三组语义特征：不变特征（在给定变换下保持一致）、变异特征（通过新颖变异机制主动区分变换）和自由特征（保持任务灵活性）。创建可控的推拉动态，不同潜在维度服务于不同可解释目的。变异机制通过鼓励变异特征在正对中区分来增强对比学习。

Result: 在ECG相位不变性和IMU旋转鲁棒性实验中表现优异：ECG相似度在相位偏移下从0.25提升至0.91，WISDM活动识别达到86.65%准确率和95.38%旋转一致性，始终优于传统数据增强方法。

Conclusion: 这项工作代表了从反应性数据增强到主动性结构学习的范式转变，使神经网络中的潜在表示具有可解释性，无需架构修改即可无缝集成到现有训练流程中。

Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.

</details>


### [26] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: Causal-GCN是一个基于干预的图卷积框架，通过整合do-calculus后门调整来识别对阿尔茨海默病进展具有稳定因果影响的大脑区域。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图学习模型大多保持相关性，混淆了人口统计学和遗传因素与疾病特异性特征，需要开发能够识别因果关系的模型。

Method: 将每个受试者的MRI表示为结构连接组，节点表示皮层和皮层下区域，边编码解剖连接性。通过主成分分析总结年龄、性别和APOE4基因型等混杂因素，并纳入因果调整集。训练后，通过切断传入边和改变节点特征来模拟对单个区域的干预，以估计对疾病概率的平均因果效应。

Result: 在ADNI队列的484名受试者中，Causal-GCN实现了与基线GNNs相当的性能，同时提供了可解释的因果效应排名，突出了与已建立的AD神经病理学一致的后部、扣带回和岛叶枢纽。

Conclusion: Causal-GCN框架能够识别对AD进展具有因果影响的大脑区域，为理解疾病机制提供了可解释的因果见解。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [27] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 该论文首次系统比较了四种用于医院出院摘要自动诊断编码的训练流程，发现在中等隐私预算下，从DP训练教师模型进行知识蒸馏的方法优于直接DP-SGD和DP合成数据训练，能恢复63%的非私有性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本训练中存在暴露敏感患者信息的风险，但差分隐私方法通常会严重降低诊断准确性，因此需要找到有效的隐私保护策略。

Method: 使用相同的1B参数模型和匹配的隐私预算，系统比较四种训练流程：直接DP-SGD、DP合成数据训练、知识蒸馏等，用于预测ICD-9代码。

Result: 在中等隐私预算(ε=4,6)下，知识蒸馏方法表现最佳，能恢复63%的非私有性能，同时保持强大的经验隐私保护(成员推断AUC≈0.5)。

Conclusion: 知识蒸馏是实现隐私保护临床NLP的最实用途径，不同架构在隐私-效用权衡上存在显著差异。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [28] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，将嵌入空间总结为带有可靠性指标的原型节点，并通过几何和上下文关系边连接。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中每个训练实例被孤立处理的问题，通过总结嵌入空间来统一实例检索、原型推理和图标签传播，在保持高效推理的同时提供可信解释。

Method: 构建原型节点图结构，节点标注可靠性指标，边编码几何和上下文关系，支持归纳推理和标签传播。

Result: 在合成和真实数据集（包括乳腺癌组织病理学IDC）上，GM达到与kNN和标签传播相当的准确率，但具有更好的校准性和更平滑的决策边界，且样本量少一个数量级。

Conclusion: GM通过显式建模可靠性和关系结构，为非参数学习中局部证据和全局一致性提供了原则性桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [29] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast是一个基于深度学习的电离层预测模型套件，通过图神经网络进行时空学习，能够准确预测全球总电子含量(TEC)，在风暴期和宁静期都表现出优于持续性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 电离层对GNSS精度、高频通信和航空运营至关重要，但准确预测和建模电离层变异性存在挑战，需要开发更有效的预测方法。

Method: 采用GraphCast启发的图神经网络模型，结合时空学习技术，整合多种物理驱动因素和观测数据集，进行全球TEC预测。

Result: 在保留的风暴期和宁静期条件下验证，IonCast相比持续性预测显示出更好的预测技能。

Conclusion: 通过将异构数据与可扩展的基于图的时空学习相结合，IonCast展示了机器学习如何增强对电离层变异性的物理理解，并推进空间天气业务韧性。

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [30] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 开发强化学习智能辅导系统，结合学生个体状态学习和群体信息，通过探测性干预平衡信息获取与教学干扰，在不同课程结构中提升教学效果


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统需要处理学生个体差异和部分可观测的学习过程，通过探测性干预来获取更多学生状态信息，同时避免过度干扰学习

Method: 构建动态时间序列模拟教室环境，开发强化学习ITS系统，结合探测性干预（辅导、讲座、考试）来学习学生状态并利用群体信息

Result: RL算法与启发式方法效果相似，探测性干预能显著提升隐藏信息较多情况下的表现，在测验和期中考试结构中效果优于仅期末考试结构

Conclusion: 探测性干预能有效提升智能辅导系统性能，特别是在信息获取有限的情况下，但需要在信息获取与教学干扰之间找到平衡

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [31] [Oversampling techniques for predicting COVID-19 patient length of stay](https://arxiv.org/abs/2511.15048)
*Zachariah Farahany,Jiawei Wu,K M Sajjadul Islam,Praveen Madiraju*

Main category: cs.LG

TL;DR: 使用电子健康记录和人工神经网络预测COVID-19患者住院时间，通过贝叶斯优化调参解决数据不平衡问题


<details>
  <summary>Details</summary>
Motivation: COVID-19症状严重程度差异大，需要预测患者住院时间以评估病情严重程度，这是一个数据不平衡的分类问题

Method: 使用电子健康记录数据，通过合成过采样处理数据不平衡问题，采用人工神经网络模型，使用贝叶斯优化进行超参数调优

Result: 选择具有最佳F1分数的模型进行评估和讨论

Conclusion: 该方法能够有效预测COVID-19患者的住院时间，为病情严重程度评估提供支持

Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.

</details>


### [32] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出一种结合局部和全局信息提取与注意力融合的框架，用于心律失常检测和分类，能够处理不同长度的心律失常，在MITDB和AFDB数据库上表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于心律失常的持续时间各不相同，现有的临床决策支持系统在处理不同长度的心律失常时面临挑战，需要开发能够准确检测心律失常发作起始点和持续时间的模型。

Method: 提出包含局部和全局信息提取以及基于注意力的局部-全局信息融合的框架，能够在受限输入长度内进行心律失常检测和分类。

Result: 在MITDB和AFDB数据库上，持续时间、发作和Dice分数的F1得分分别为96.45%、82.05%、96.31%和97.57%、98.31%、97.45%，性能优于基准模型。

Conclusion: 该方法能有效捕捉局部和全局信息及动态变化，准确检测心律失常并精确定位其发生时间，有助于制定更准确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [33] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发了基于病理全切片图像的TDAM-CRC多实例学习模型，用于结直肠癌精准预后预测，并通过多组学分析揭示了分子机制。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌具有高度异质性，传统的TNM分期系统无法满足个性化医疗需求，需要开发更精准的预后分层工具。

Method: 使用TCGA队列（n=581）训练TDAM-CRC模型，在独立外部队列（n=1031）验证，整合多组学数据提高模型可解释性并识别预后生物标志物。

Result: TDAM-CRC在两个队列中均实现稳健风险分层，预测性能显著优于传统临床分期系统和现有先进模型。多组学分析显示高风险亚型与代谢重编程和免疫抑制肿瘤微环境相关，发现MRPL37是连接病理特征与临床预后的关键基因。

Conclusion: TDAM-CRC为结直肠癌提供了改进的风险分层工具，揭示了新的分子靶点，并促进了个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [34] [Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection](https://arxiv.org/abs/2511.15083)
*Xiancheng Wang,Lin Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 提出Fourier-KAN-Mamba混合架构，结合傅里叶层、KAN网络和Mamba状态空间模型，用于时间序列异常检测，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在长序列建模中表现出色，但直接应用于异常检测任务时难以捕捉复杂的时间模式和非线性动态特征。

Method: 集成傅里叶层提取多尺度频率特征，KAN增强非线性表示能力，并引入时间门控机制来更好地区分正常和异常模式。

Result: 在MSL、SMAP和SWaT数据集上的大量实验表明，该方法显著优于现有的最先进方法。

Conclusion: Fourier-KAN-Mamba混合架构通过结合频域特征提取、非线性建模和选择性状态空间建模，有效提升了时间序列异常检测性能。

Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network

</details>


### [35] [Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data](https://arxiv.org/abs/2511.15112)
*Wei-hsiang Yen,Lyn Chao-ling Chen*

Main category: cs.LG

TL;DR: 本研究将深度学习方法与情感分析整合到传统商业模式分析中，以台积电为研究对象预测台湾半导体产业趋势。通过结合情感增强的时间序列数据和LSTM模型，准确预测了台积电的技术发展和全球市场威胁。


<details>
  <summary>Details</summary>
Motivation: 半导体产业市场变化快速，传统数据分析方法在处理高变化性和时间序列数据时表现不佳，需要更先进的分析方法来预测产业趋势。

Method: 收集台积电季度报告中的文本数据和时序数据，通过情感分析考虑公司内部事件和外部全球事件的影响，使用情感增强的时间序列数据训练LSTM模型进行产业趋势预测。

Result: 预测结果揭示了台积电晶圆技术的显著发展和全球市场的潜在威胁，与台积电产品发布新闻和国际新闻相符。

Conclusion: 该研究通过考虑内外事件干预，在半导体产业趋势预测方面表现准确，为研究和商业领域提供了有价值的半导体产业信息。

Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.

</details>


### [36] [Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling](https://arxiv.org/abs/2511.15125)
*Huifan Zhang,Pingqiang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯在线学习的不确定性感知框架，用于高效建模RF无源组件，相比传统机器学习方法仅需2.86%的电磁仿真时间，实现35倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的RF无源组件建模需要大量电磁仿真来覆盖几何和频率设计空间，造成计算瓶颈。

Method: 1) 具有可重构头的贝叶斯神经网络，用于联合几何-频率域建模并量化不确定性；2) 自适应采样策略，利用不确定性指导同时优化几何参数和频率域的训练数据采样。

Result: 在三种RF无源组件上验证，框架实现了准确建模，仅使用传统基于ML流程2.86%的电磁仿真时间，达到35倍加速。

Conclusion: 该不确定性感知贝叶斯在线学习框架显著提高了RF无源组件参数化建模的效率，大幅减少了计算成本。

Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.

</details>


### [37] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 开发了一种新的稀疏矩阵乘法算法，使得能够对整个Medline数据集应用自组织映射，从而更完整地映射现有医学知识。


<details>
  <summary>Details</summary>
Motivation: 现有的算法在处理整个Medline数据集时面临指数级增长的内存和处理需求限制，只能处理数据的小子集。

Method: 设计了一种新颖的稀疏矩阵乘法算法，并将其应用于整个Medline数据集的自组织映射。

Result: 成功创建了更完整的医学知识地图，并提高了随时间更新自组织映射的可行性。

Conclusion: 该算法解决了大规模数据集处理的技术瓶颈，为全面映射医学知识提供了可行方案。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [38] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 提出了GRPO-Verif算法，通过统一损失函数联合优化解决方案生成和自我验证，可调节验证信号权重，在保持推理性能的同时增强自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习显著提升了大型语言模型的推理能力，但它们仍难以一致地验证自己的推理过程，因此需要研究如何增强LLMs的自我验证能力以及这种能力是否能进一步改善推理性能。

Method: 提出GRPO-Verif算法，在统一损失函数中联合优化解决方案生成和自我验证，包含可调节验证信号权重的超参数。

Result: 实验结果表明该方法在保持推理性能相当的同时，增强了自我验证能力。

Conclusion: GRPO-Verif算法成功实现了推理能力和自我验证能力的联合优化，为解决LLMs自我验证不一致的问题提供了有效方案。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [39] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观不一致的报告，使得稳健的情感解码特别困难。需要一种能够处理标签噪声的数据高效方法。

Method: 使用表示对齐模块将EEG和面部特征嵌入共享潜在空间，强制执行模态间的语义一致性。将残差异常视为噪声引起的不一致性，在主动学习中选择性查询这些样本以获取反馈。

Result: 在ASCERTAIN数据集上的实验验证了该方法的效率和鲁棒性，展示了其作为脑机接口系统中数据高效且噪声容忍的EEG情感解码方法的潜力。

Conclusion: 该框架通过反馈驱动过程引导网络关注可靠、信息丰富的样本，减少噪声标签的影响，为EEG情感识别提供了一种有效的解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [40] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 该论文研究了复数变分自编码器中的Kähler几何结构，提出了基于复数高斯混合的Kähler势能导数方法，有效降低了计算负担并改善了潜在空间表示质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明欧几里得变分自编码器具有黎曼几何结构，但复数VAEs中的几何结构尚未充分探索。本文旨在揭示复数VAEs中的Kähler几何结构，并开发高效的几何正则化方法。

Method: 针对复数VAEs的解码器几何，推导了复数高斯正则化下的Fisher信息度量，建立了Kähler势能与相对熵的关系，提出了复数高斯混合的Kähler势能导数方法。

Result: 提出的方法能够高效计算度量，通过PSH函数减轻自动微分的大规模计算负担，在潜在空间中实现基于解码器几何的正则化，并按照加权复数体积元素进行采样。

Conclusion: 该方法在样本变异的代价下，能够产生更平滑的表示和更少的语义异常值，验证了复数VAEs中Kähler几何结构的有效性。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [41] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器利用正常数据分布建模故障差异，并引入多样性损失防止模式崩溃，显著提升了故障样本的真实性和多样性。


<details>
  <summary>Details</summary>
Motivation: 工业设备监控中故障数据稀缺，现有时间序列生成模型在少样本场景下难以捕捉故障分布，生成样本缺乏真实性和多样性。

Method: 使用扩散模型框架，包含正负差异适配器利用预训练正常数据分布建模故障差异，并引入多样性损失通过样本间差异正则化促进多样性。

Result: 实验结果表明，该方法在真实性和多样性方面显著优于传统方法，在关键基准测试中达到最先进性能。

Conclusion: 提出的少样本故障时间序列生成框架有效解决了故障数据稀缺问题，为工业设备预测性维护提供了可靠的数据增强解决方案。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [42] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种量子图注意力网络(Q-GAT)，在深度强化学习框架中用参数化量子电路替代传统MLP，减少了50%以上可训练参数，在车辆路径问题上实现了更快的收敛和约5%的路由成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统基于图神经网络的深度强化学习方法依赖参数繁多的多层感知机，存在参数过多和内存限制的问题，需要开发更紧凑高效的模型。

Method: 在DRL框架中构建量子图注意力网络，用参数化量子电路替代关键读出阶段的多层感知机，结合近端策略优化和贪心/随机解码策略。

Result: 实验表明Q-GAT相比传统GAT基线减少了50%以上可训练参数，收敛速度更快，路由成本降低约5%。

Conclusion: 参数化量子电路增强的图神经网络有望成为大规模路由和物流优化的紧凑有效求解器。

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [43] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个蒸馏框架，将掩码自回归扩散模型的扩散链压缩为单步生成，实现30倍加速并保持样本质量，同时支持强化学习后训练。


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归扩散模型推理速度慢的问题，其分层推理机制（外部AR循环+内部扩散链）不仅影响生成效率，还阻碍了强化学习后训练的实际应用。

Method: 提出基于分数的变分目标，将掩码自回归扩散模型蒸馏为单步生成；开发MARVAL-RL高效强化学习框架。

Result: 在ImageNet 256*256上，MARVAL-Huge达到FID 2.00，相比MAR-diffusion加速30倍以上；MARVAL-RL在CLIP和图像奖励分数上持续提升。

Conclusion: MARVAL为掩码自回归扩散模型的蒸馏和强化学习提供了首个实用路径，实现了快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [44] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 提出了ATPO方法，通过分析扩散大语言模型推理轨迹中的不确定性模式，动态选择关键步骤进行梯度更新，显著提升推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法均匀分配策略梯度，隐含假设所有去噪步骤同等重要，但研究发现推理轨迹中存在结构化的'困惑区域'，这些高不确定性步骤对最终结果有决定性影响。

Method: 提出自适应轨迹策略优化(ATPO)，使用混合RoEC+CM规则识别轨迹中的高杠杆步骤，动态重新分配梯度更新到这些关键步骤，而不改变RL目标、奖励或计算预算。

Result: ATPO在多个基准测试中显著提升了推理准确性和训练稳定性，证明利用轨迹动态是推进dLLM强化学习的关键。

Conclusion: 通过分析轨迹动态并选择性关注关键步骤，ATPO为扩散大语言模型的强化学习对齐提供了更有效的方法，展示了步骤级优化的重要性。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [45] [D2D Power Allocation via Quantum Graph Neural Network](https://arxiv.org/abs/2511.15246)
*Tung Giang Le,Xuan Tung Nguyen,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种全量子图神经网络(QGNN)，使用参数化量子电路实现消息传递，用于无线网络资源管理，在保持性能的同时减少参数数量并实现并行计算。


<details>
  <summary>Details</summary>
Motivation: 无线网络复杂性增加需要可扩展的资源管理方法，经典图神经网络计算成本高，量子方法有望提供更高效的解决方案。

Method: 通过量子图卷积层(QGCLs)将特征编码为量子态，使用NISQ兼容的幺正变换处理图结构，通过测量获取嵌入表示。

Result: 在D2D功率控制SINR最大化任务中，QGNN与经典方法性能相当，但参数更少且具有固有并行性。

Conclusion: 这种基于PQC的端到端GNN标志着向量子加速无线优化迈出了一步。

Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.

</details>


### [46] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出了EntroPIC方法，通过比例-积分控制动态调整正负样本的损失系数，稳定大语言模型训练中的熵值，确保有效探索和稳定进展。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以维持适当的熵水平，因为训练过程涉及正负样本混合，且在不同步骤中对熵的影响不同，容易导致模型陷入次优行为。

Method: EntroPIC方法使用比例-积分控制自适应调整正负样本的影响，通过动态调谐其损失系数来稳定训练过程中的熵。

Result: 实验结果表明该方法成功维持了期望的熵水平，为大语言模型实现了稳定且最优的强化学习训练。

Conclusion: EntroPIC方法能有效控制大规模语言模型训练中的熵，确保稳定探索和避免过早收敛到次优解。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [47] [Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling](https://arxiv.org/abs/2511.15250)
*Jin Ye,Lingmei Wang,Shujian Zhang,Haihang WU*

Main category: cs.LG

TL;DR: 提出基于改进PVTD3算法的电热联合系统智能调度方法，在新能源接入和多重不确定性下优化系统运行，显著降低综合成本和电网购电波动。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源转型和可再生能源快速发展，新能源接入和多重不确定性下的电热联合系统调度优化挑战日益突出。

Method: 采用改进的双延迟深度确定性策略梯度(PVTD3)算法，通过引入电网购电变化惩罚项实现系统优化。

Result: 在10%、20%、30%可再生能源渗透率三种典型场景下，PVTD3算法相比传统TD3算法分别降低系统综合成本6.93%、12.68%、13.59%，平均购电波动幅度降低12.8%。

Conclusion: 该算法在经济性、电网稳定性和储能设备可持续调度方面均表现出优越性能。

Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.

</details>


### [48] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT是一个统一框架，将不同网络指标建模为共享潜在网络状态的投影，通过多模态对齐和对比学习构建结构化表示，在链路估计、拓扑推断和流量预测等任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有网络断层扫描方法通常单独解决不同问题，依赖有限的任务特定信号，这限制了泛化能力和可解释性。

Method: 基于柏拉图表示假设，通过多模态对齐和对比学习学习共享潜在网络状态，在共享潜在空间中训练多个断层扫描任务。

Result: 在合成和真实数据集上的实验表明，PLATONT在链路估计、拓扑推断和流量预测方面始终优于现有方法，在不同网络条件下具有更高的准确性和更强的鲁棒性。

Conclusion: PLATONT通过构建紧凑和结构化的共享潜在表示，提高了跨任务泛化能力，为网络断层扫描提供了统一的解决方案。

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [49] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出GRPO-RM方法，将GRPO强化学习技术从大语言模型扩展到表示学习模型，通过预定义输出集和专用奖励函数来优化表示模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探索GRPO强化学习方法是否能够从大语言模型泛化到表示学习模型，以提升表示模型的性能。

Method: 建立预定义输出集替代LLM中的token序列采样，生成输出组用于GRPO的概率驱动优化，并设计专门的奖励函数以适应表示模型的特性。

Result: 在多个真实世界数据集上进行广泛实验，验证了所提方法的有效性。

Conclusion: GRPO-RM成功将GRPO技术应用于表示学习模型，证明了该方法的泛化能力和实际应用价值。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [50] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时适应框架，通过减少适应频率和数据使用来降低计算成本，同时保持准确性，特别适合资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法需要频繁适应和高计算成本，不适合资源受限的边缘环境。

Method: 提出两个关键组件：CnDRM（类和域代表内存）存储代表性样本，IoBMN（仅推理批量感知内存归一化）动态调整归一化统计量。

Result: 与五种最先进TTA算法集成，延迟降低93.12%，精度下降低于3.3%，即使只使用1%的数据流。

Conclusion: SNAP在边缘设备上具有实际应用潜力，特别适合延迟敏感的应用场景。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [51] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝，生成硬件中立的检查点，减少边缘加速器在不同后端和精度选择下的精度不一致问题。


<details>
  <summary>Details</summary>
Motivation: 专用边缘加速器依赖低位量化，但供应商编译器在缩放、裁剪和内核支持方面存在差异，导致相同浮点检查点在不同后端产生不一致的精度，迫使从业者调整标志或重构模型。

Method: 结合渐进式伪量化来对齐训练与部署的整数网格，以及反向剪枝来抑制异常值驱动的尺度膨胀同时保持可学习性。该方法与量化方案无关，无需供应商特定的图更改。

Result: 在模型和任务中，缩小了浮点与低位量化之间的差距，减少了对编译器启发式/校准的依赖，避免了每个后端的重新训练。报告了精度和边缘指标（延迟、吞吐量、能耗/推理和成本）。

Conclusion: Quant-Trim提供了一种硬件中立的解决方案，提高了量化模型在不同边缘加速器后端上的一致性和鲁棒性。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [52] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 系统研究时间序列基础模型的概念可解释性，发现早期层捕获局部时域模式，深层编码离散度和变化时间信号，但概念组合时存在干扰。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在经验上取得成功，但其内部如何表示基本时间序列概念的机制仍不清楚，需要系统研究概念可解释性。

Method: 使用分层分析、线性可恢复性测试和表示相似性度量，系统探测概念在不同层的编码情况、线性可恢复性、表示演化和概念组合处理。

Result: 早期层主要捕获局部时域模式（如AR(1)、水平偏移、趋势），深层编码离散度和变化时间信号，频谱和扭曲因子最难线性恢复；概念组合时探针性能下降，显示概念间存在干扰。

Conclusion: 原子概念能够可靠定位，但概念组合仍是挑战，这突显了当前时间序列基础模型在表示交互时间现象能力方面的关键局限性。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [53] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: KrawtchoukNet是基于离散Krawtchouk多项式的GNN滤波器，解决了传统谱图神经网络在异配图和高多项式度数下的性能崩溃问题，通过有界递归系数和可学习形状参数实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于多项式滤波器的谱图神经网络存在两个关键限制：1)在异配图上性能崩溃 2)在高多项式度数时出现过平滑问题。这些问题源于标准滤波器的静态低通特性。

Method: 提出KrawtchoukNet，基于离散Krawtchouk多项式构建GNN滤波器。通过将多项式域N固定为小常数，创建首个具有固有有界递归系数的GNN滤波器；通过使滤波器的形状参数p可学习，使滤波器能够适应图数据的频谱响应。

Result: KrawtchoukNet在K=10时达到SOTA结果，对过平滑具有出色鲁棒性；在具有挑战性的异配图基准测试（Texas、Cornell）上实现SOTA性能，明显优于GAT和APPNP等标准GNN。

Conclusion: KrawtchoukNet通过有界递归系数和自适应频谱响应，为异配图性能和高多项式度数过平滑问题提供了统一解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [54] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出LaguerreNet，一种基于连续拉盖尔多项式的新型GNN滤波器，通过可训练的alpha参数学习滤波器频谱形状，解决了异质图性能差和高阶多项式过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱图神经网络在异质图上性能不佳，且在高阶多项式时会出现过平滑现象，主要源于标准滤波器的静态低通特性。现有自适应多项式滤波器在连续域扩展和系数稳定性方面存在问题。

Method: 使用连续拉盖尔多项式构建GNN滤波器，将核心alpha参数设为可训练以学习频谱形状，并采用基于LayerNorm的稳定化技术解决多项式系数无界导致的数值不稳定问题。

Result: 1) 在挑战性异质图基准测试中达到最先进性能；2) 对过平滑具有卓越鲁棒性，性能在K=10时达到峰值，比ChebyNet崩溃点高一个数量级。

Conclusion: LaguerreNet通过连续拉盖尔多项式和稳定化技术，有效解决了谱GNN在异质图和过平滑方面的关键限制，为自适应多项式滤波器提供了可行的解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [55] [STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection](https://arxiv.org/abs/2511.15339)
*Kadir-Kaan Özer,René Ebeling,Markus Enzweiler*

Main category: cs.LG

TL;DR: STREAM-VAE是一种用于汽车遥测时序数据异常检测的变分自编码器，通过双路径编码器分离慢漂移和快尖峰信号动态，提高异常检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 汽车遥测数据同时存在慢漂移和快尖峰，标准重建方法使用单一潜在过程会混合不同时间尺度，导致尖峰被平滑或方差膨胀，削弱异常分离能力。

Method: 使用双路径编码器分离慢漂移和快尖峰信号动态，解码器将瞬时偏差与正常操作模式分开表示，专为部署设计，能在车辆监控和后端车队分析中产生稳定的异常分数。

Result: 在汽车遥测数据集和公开SMD基准测试上的实验表明，明确分离漂移和尖峰动态相比强基线方法（预测、注意力、图和VAE）提高了鲁棒性。

Conclusion: STREAM-VAE通过分离异构时间尺度的信号动态，有效解决了汽车遥测数据中慢漂移和快尖峰同时存在的异常检测挑战。

Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.

</details>


### [56] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 系统探索时间序列预测中的集成策略，提出多层堆叠框架，在50个真实数据集上验证其优越性能


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中集成方法未被充分利用，简单的线性组合仍被认为是state-of-the-art，需要系统探索更有效的集成策略

Method: 评估33个集成模型，提出多层堆叠框架，结合不同堆叠模型的优势

Result: 堆叠方法持续提高准确性，多层堆叠框架在不同预测场景下均提供优越精度

Conclusion: 基于堆叠的方法有潜力改进时间序列预测的AutoML系统

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [57] [Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction](https://arxiv.org/abs/2511.15357)
*Yinan Yu,Falk Dippel,Christina E. Lundberg,Martin Lindgren,Annika Rosengren,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 本文提出了一个成本感知预测框架，结合LLM代理进行成本效益分析，以改善机器学习预测在临床决策中的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习预测模型往往没有考虑下游价值权衡和临床可解释性，需要开发能够沟通预测应用所涉及权衡的框架。

Method: 开发了预测心力衰竭患者1年死亡率的ML模型，引入临床影响投影曲线可视化成本维度，并使用四个LLM代理生成患者特定描述。

Result: XGB模型表现最佳，AUROC为0.804，AUPRC为0.529，Brier得分为0.135。CIP曲线提供群体层面的成本构成概览，LLM生成个体层面的成本效益分析。

Conclusion: CAP框架利用LLM代理整合ML分类器结果和成本效益分析，为临床决策提供更透明和可解释的支持。

Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.

</details>


### [58] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种新的局部特征重要性方法CID，通过生成正负反事实样本、使用核密度估计建模分布，并基于分布差异度量来排序特征重要性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估个体特征重要性对理解模型决策过程至关重要，但现有方法缺乏明确的基准真值进行比较，需要替代的、有理论基础的评价指标。

Method: 生成正负反事实样本集，使用核密度估计建模其分布，基于分布差异度量对特征进行排序，该方法具有严格的数学理论基础并满足有效度量的关键性质。

Result: 与现有局部特征重要性解释方法相比，CID方法不仅提供了互补视角，而且在忠实性指标（全面性和充分性）上表现更好，能提供更忠实的系统解释。

Conclusion: CID方法作为一个有价值的模型分析工具具有巨大潜力，能够提供更可靠的特征重要性解释。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [59] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过选择性更新仅0.1%的核心参数来防止灾难性遗忘，同时保持基础模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定后训练导致的灾难性遗忘问题，在保持基础模型通用能力的同时有效学习下游领域知识，而不依赖历史数据或增加额外参数。

Method: 使用两种重要性估计器（基于Fisher信息的PIECE-F和基于二阶归一化的PIECE-S）来识别核心参数，仅选择性更新0.1%与任务最相关的参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE保持了通用能力，并在多样化下游任务中实现了最先进的持续学习性能。

Conclusion: PIECE为构建可扩展、领域自适应的基础模型提供了一条实用路径，有效避免了灾难性遗忘问题。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [60] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: EVA-Net是一个可解释的脑年龄异常检测框架，使用稀疏注意力Transformer处理长EEG序列，通过变分信息瓶颈学习鲁棒表示，并与原型网络对齐来学习健康老化流形。


<details>
  <summary>Details</summary>
Motivation: 现有脑年龄模型在处理不完美的医疗数据（如仅从健康队列学习"正常"基线）时存在困难，且标准模型通常是缺乏可解释结构的黑箱。

Method: 使用高效的稀疏注意力Transformer建模长EEG序列；采用变分信息瓶颈处理噪声和变异性；通过连续原型网络学习规范健康老化流形以实现可解释性。

Result: 在1297名健康受试者上训练，达到最先进精度；在27名MCI和AD患者队列上验证，病理组显示出显著更高的脑年龄差距和原型对齐误差。

Conclusion: EVA-Net为使用不完美医疗数据的医疗智能提供了一个可解释的框架。

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [61] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出一种基于变分拉格朗日框架的非线性非高斯状态空间模型状态估计算法，通过熵信任区域更新和动态约束实现贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 解决非线性非高斯状态空间模型中的状态估计问题，传统方法在处理这类复杂模型时存在局限性。

Method: 采用变分拉格朗日公式，将贝叶斯推断转化为序列化的熵信任区域更新；针对高斯-马尔可夫近似推导递归方案；对一般非线性非高斯模型使用广义统计线性回归和傅里叶-埃尔米特矩匹配。

Result: 开发出一类前向-后向算法，具有有利的计算复杂度，能够有效处理非线性非高斯状态估计问题。

Conclusion: 该变分框架为非线性非高斯状态估计提供了一种系统性的解决方案，通过不同的后验分解方式产生算法族，具有广泛适用性。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [62] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 分析了表格ICL模型中各层的作用，发现只有部分层共享共同表示语言，存在结构冗余，为模型压缩和可解释性提供机会


<details>
  <summary>Details</summary>
Motivation: 尽管表格ICL模型与LLMs架构相似，但各层在表格预测中的具体贡献尚不清楚，需要研究层间潜在空间的演化

Method: 通过"层作为画家"的视角分析TabPFN和TabICL模型，研究各层潜在空间的演化动态

Result: 发现只有部分层共享共同表示语言，表明存在结构冗余，与LLMs的层动态有相似之处

Conclusion: 表格ICL模型存在层间冗余，这为模型压缩和提升可解释性提供了机会

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [63] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出了一种基于时间序列基础模型（TSFM）的上下文学习分类方法，无需微调即可对训练数据之外的数据进行分类，应用于轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要微调模型的问题，利用预训练模型的扩展性，实现更广泛的AI驱动维护系统。

Method: 将频域参考信号转换为伪时间序列模式，生成对齐的协变量和目标信号，通过TSFM预测数据与预定义标签对应的概率。

Result: 该方法在不同操作条件下都表现出有效性，展示了预训练模型的可扩展性优势。

Conclusion: 该方法标志着从定制化窄AI解决方案向更广泛AI驱动维护系统的重大进展。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [64] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个公平感知的联邦学习能量最小化框架，通过联合优化设备选择、带宽分配和压缩级别，在非IID数据上实现比基线策略高79%的能耗降低和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决无线边缘系统中联邦学习面临的挑战：平衡能源效率与公平参与，同时确保高模型精度，克服异构资源、不平等客户端贡献和有限通信容量的问题。

Method: 提出FairEnergy框架，将捕获更新幅度和压缩比的贡献分数集成到联合优化中，通过松弛二元选择变量和应用拉格朗日分解处理全局带宽耦合，然后进行每个设备的子问题优化。

Result: 在非IID数据上的实验表明，FairEnergy相比基线策略实现了更高的准确率，同时能耗降低了高达79%。

Conclusion: FairEnergy框架有效解决了联邦学习在无线边缘系统中的能源效率和公平参与平衡问题，显著提升了性能表现。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [65] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出NTK引导的隐式神经教学(NINT)方法，通过动态选择最大化全局函数更新的坐标来加速隐式神经表示的训练，减少近一半训练时间同时保持或提高表示质量。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)通过多层感知机参数化连续信号，但在拟合高分辨率信号时需要优化数百万个坐标，计算成本过高。

Method: 利用神经正切核(NTK)，NINT通过计算NTK增强损失梯度的范数来评分样本，同时考虑拟合误差和异构杠杆效应(自影响和跨坐标耦合)。

Result: 实验表明NINT显著减少近一半训练时间，同时保持或改进表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过NTK引导的动态坐标选择有效加速隐式神经表示训练，为高分辨率信号建模提供了高效解决方案。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [66] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 研究了按需采样中样本复杂度和轮数复杂度之间的权衡，在多分布学习的可实现和不可知设置中建立了最优边界，并提出了新的优化框架OODS来抽象这种权衡。


<details>
  <summary>Details</summary>
Motivation: 探索多分布学习中样本复杂度和轮数复杂度之间的基本权衡关系，特别是在按需采样设置下，理解自适应采样算法在不同轮数限制下的最优性能。

Method: 提出了优化通过按需采样(OODS)框架来抽象样本自适应权衡，在可实现设置中分析r轮算法的最优样本复杂度，在不可知设置中设计接近最优的算法。

Result: 在可实现MDL中，r轮算法的最优样本复杂度约为dk^{Θ(1/r)}/ε；在不可知MDL中，提出了在Õ(√k)轮内达到Õ((d+k)/ε²)样本复杂度的算法；建立了OODS设置中轮数复杂度的紧边界。

Conclusion: 按需采样中的样本-轮数权衡是固有的，实现亚多项式轮数复杂度需要绕过OODS固有硬度的新技术，所提出的框架统一了大多数现有MDL算法并建立了基本极限。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [67] [PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles](https://arxiv.org/abs/2511.15522)
*Yinan Yu,Samuel Scheidegger*

Main category: cs.LG

TL;DR: PCARNN-DCBF是一种结合物理编码控制仿射残差神经网络和基于预览的离散控制屏障函数的新方法，用于地面车辆的运行时地理围栏，在保持可验证控制结构的同时实现高保真学习。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案难以在可验证控制的结构要求与高保真学习之间取得平衡，而运行时地理围栏对于强制执行操作设计域至关重要。

Method: 引入PCARNN-DCBF管道，将物理编码控制仿射残差神经网络与基于预览的离散控制屏障函数集成，明确保持车辆动力学的控制仿射结构，并通过实时二次规划强制执行多边形保持约束。

Result: 在CARLA中对电动和燃烧平台进行的实验表明，这种结构保持方法显著优于分析和非结构化神经基线。

Conclusion: PCARNN-DCBF通过保持控制仿射结构，成功实现了高保真学习与可验证控制要求的协调，为地面车辆的运行时地理围栏提供了有效解决方案。

Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.

</details>


### [68] [CODE: A global approach to ODE dynamics learning](https://arxiv.org/abs/2511.15619)
*Nils Wildt,Daniel M. Tartakovsky,Sergey Oladyshkin,Wolfgang Nowak*

Main category: cs.LG

TL;DR: 本文提出ChaosODE（CODE），一种多项式混沌ODE展开方法，用于从稀疏采样数据中学习常微分方程的动力系统。与传统神经网络或核方法相比，CODE在噪声数据和外推预测方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统ODE建模需要密集测量数据，但实际应用中通常只能获得稀疏采样数据。现有基于神经网络或核方法的数据驱动方法在稀疏数据和噪声条件下外推能力较差。

Method: 使用任意多项式混沌展开（aPCE）来表示ODE的右侧，构建全局正交多项式表示的动力系统。

Result: 在Lotka-Volterra系统上的实验表明，CODE在噪声水平、初始条件和长期预测方面表现优异，即使在未见过的初始条件下也展现出卓越的外推能力，优于NeuralODE和KernelODE方法。

Conclusion: CODE方法在稀疏数据和噪声条件下具有更好的鲁棒性和外推能力，为动力系统学习提供了更可靠的解决方案，并提供了实用的优化指南。

Abstract: Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.

</details>


### [69] [Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges](https://arxiv.org/abs/2511.15652)
*Kim N. Nolle,Ivana Dusparic,Rhodri Cusack,Vinny Cahill*

Main category: cs.LG

TL;DR: 本文通过自动驾驶环境中的实验，揭示了持续强化学习(CRL)面临的挑战，包括环境抽象、超参数敏感、灾难性遗忘和神经网络容量利用等问题，并提出了需要解决的研究问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习在非平稳环境（如自动驾驶）中很重要，但将CL成功应用于强化学习仍然是一个开放问题，需要探索CRL在实际场景中的挑战。

Method: 在自动驾驶环境中，使用PPO算法让智能体依次学习四种不同角度的停车场景，模拟持续学习环境，通过实验识别CRL面临的挑战。

Result: 实验揭示了CRL的四个主要挑战：寻找合适的环境抽象、对超参数的过度敏感、灾难性遗忘问题、神经网络容量的有效利用。

Conclusion: CRL需要解决这些挑战才能创建鲁棒系统，质疑神经网络在CL中的适用性，并强调需要计算机科学和神经科学的跨学科研究。

Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.

</details>


### [70] [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)
*Cheng Yin,Yankai Lin,Wang Xu,Sikyuen Tam,Xiangrui Zeng,Zhiyuan Liu,Zhouping Yin*

Main category: cs.LG

TL;DR: DeepThinkVLA通过混合注意力解码器和两阶段训练策略，解决了视觉-语言-动作模型中思维链推理与机器人动作生成之间的架构冲突，在LIBERO基准上达到97.0%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型使用单一自回归解码器同时处理顺序的思维链推理和高维并行机器人动作，这种架构不匹配导致运动控制性能下降，无法建立思维与动作之间的强因果关系。

Method: 采用混合注意力解码器：用因果注意力生成顺序思维链，然后切换到双向注意力快速并行解码动作向量。配合两阶段训练：先用监督微调教授基础推理，再用强化学习通过任务成功奖励对齐推理-动作序列与期望结果。

Result: 在LIBERO基准上达到97.0%的成功率。消融实验显示混合架构比标准解码器性能提升15.5%，强化学习阶段提供关键的2%提升以确保最佳性能。

Conclusion: DeepThinkVLA通过架构创新和训练策略的协同作用，成功解决了VLA模型中思维与动作的冲突，实现了最先进的性能表现。

Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

</details>


### [71] [Walrus: A Cross-Domain Foundation Model for Continuum Dynamics](https://arxiv.org/abs/2511.15684)
*Michael McCabe,Payel Mukhopadhyay,Tanya Marwah,Bruno Regaldo-Saint Blancard,Francois Rozet,Cristiana Diaconu,Lucas Meyer,Kaze W. K. Wong,Hadi Sotoudeh,Alberto Bietti,Irina Espejo,Rio Fear,Siavash Golkar,Tom Hehir,Keiya Hirashima,Geraud Krawezik,Francois Lanusse,Rudy Morel,Ruben Ohana,Liam Parker,Mariel Pettee,Jeff Shen,Kyunghyun Cho,Miles Cranmer,Shirley Ho*

Main category: cs.LG

TL;DR: Walrus是一个基于Transformer的基础模型，主要用于流体类连续介质动力学模拟，通过谐波分析稳定化、负载均衡分布式训练和计算自适应标记化等方法，在19个多样化场景上预训练，在短期和长期预测任务上表现优于先前的基础模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言和视觉领域取得了巨大成功，但在物理模拟领域仍面临挑战。数据异质性、不稳定的长期动力学、不同分辨率和维度等问题阻碍了从多样化动力学数据中有效学习。

Method: 开发了Walrus模型，采用谐波分析稳定化方法、负载均衡的2D和3D分布式训练策略、计算自适应标记化等技术，在19个涵盖天体物理、地球科学、流变学、等离子体物理、声学和经典流体等领域的多样化场景上进行预训练。

Result: 实验表明，Walrus在下游任务的短期和长期预测中均优于先前的基础模型，消融研究证实了所提方法在预测稳定性、训练吞吐量和迁移性能方面的价值。

Conclusion: Walrus成功克服了物理模拟中的关键挑战，为连续介质动力学提供了一个强大的基础模型，代码和权重已开源供社区使用。

Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.

</details>


### [72] [The Impact of Quantization on Large Reasoning Model Reinforcement Learning](https://arxiv.org/abs/2511.15694)
*Medha Kumar,Zifei Xu,Xin Wang,Tristan Webb*

Main category: cs.LG

TL;DR: 量化感知的强化学习训练对大型推理模型的推理性能产生负面影响，而后训练量化和QLoRA方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究量化（特别是后训练量化和量化感知训练）如何影响大型推理模型在强化学习环境下的推理能力，这是一个尚未充分探索的问题。

Method: 通过系统实验比较后强化学习量化模型与量化感知强化学习优化模型在数学基准测试上的推理性能差异。

Result: 发现量化感知强化学习训练对学习过程产生负面影响，而后训练量化和QLoRA方法能够获得更好的性能表现。

Conclusion: 在大型推理模型的强化学习训练中，应避免使用量化感知的强化学习方法，而优先考虑后训练量化或QLoRA等替代方案。

Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.

</details>
