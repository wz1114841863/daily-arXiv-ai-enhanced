<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 89]
- [cs.DC](#cs.DC) [Total: 13]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 这篇论文指出MICRO 2024最佳论文亚军(Mess论文)中关于Ramulator 2.0模拟器的结果存在错误且不可复现，通过正确配置模拟器证明了Mess论文的关键贡献声明是错误的。


<details>
  <summary>Details</summary>
Motivation: 纠正Mess论文中关于内存系统性能评估的错误结果，防止不准确和误导性结果的传播，维护科学记录的可靠性。

Method: 通过重新配置和使用Ramulator 2.0模拟器，对比分析Mess论文中的模拟结果，识别配置错误和统计方法问题。

Result: 发现Mess论文在Ramulator 2.0配置和使用中存在多个简单人为错误，正确配置后模拟结果与真实系统特征相符；同时发现DAMOV模拟使用了错误的统计指标。

Conclusion: 强调严谨验证模拟结果的重要性，建议计算机架构社区纠正Mess论文的错误，并对评审和制品评估过程的完整性提出质疑。

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [2] [Impact of AI-Triage on Radiologist Report Turnaround Time: Real-World Time-Savings and Insights from Model Predictions](https://arxiv.org/abs/2510.15237)
*Yee Lam Elim Thompson,Jonathan Fergus,Jonathan Chung,Jana G. Delfino,Weijie Chen,Gary M. Levine,Frank W. Samuelson*

Main category: cs.PF

TL;DR: 该研究量化了AI分诊设备对肺栓塞CTPA检查报告周转时间的影响，发现工作时间段有显著时间节省（22.2分钟），而下班时间段无显著节省。


<details>
  <summary>Details</summary>
Motivation: 评估AI分诊设备在临床工作流程中对报告周转时间的影响，并量化工作流程参数对时间节省的作用。

Method: 回顾性分析11252例成人CTPA检查，分为AI部署前后两个时期。比较PE阳性检查的报告周转时间，并使用计算模型预测时间节省。

Result: 工作时间段平均TAT从68.9分钟降至46.7分钟，节省22.2分钟（p=0.004）；下班时间段无显著节省（2.82分钟，p=0.345）。模型预测与观察结果一致。

Conclusion: 考虑和量化临床工作流程有助于准确评估AI分诊设备部署后预期的时间节省效果。

Abstract: Objective: To quantify the impact of workflow parameters on time-savings in
report turnaround time (TAT) due to an AI-triage device that prioritized
pulmonary embolism (PE) in chest CT pulmonary angiography (CTPA) exams.
Methods: This retrospective study analyzed 11252 adult CTPA exams conducted for
suspected PE at a single tertiary academic medical center. Data was divided
into two periods: pre-AI and post-AI. For PE-positive exams, TAT - defined as
the duration from patient scan completion to the first preliminary report
completion - was compared between the two periods. Time-savings were reported
separately for work-hour and off-hour cohorts. To characterize radiologist
workflow, 527234 records were retrieved from the PACS and workflow parameters
such as exam inter-arrival time and radiologist read-time extracted. These
parameters were input into a computational model to predict time-savings
following deployment of an AI-triage device and to study the impact of workflow
parameters. Results: The pre-AI dataset included 4694 chest CTPA exams with
13.3% being PE-positive. The post-AI dataset comprised 6558 exams with 16.2%
being PE-positive. The mean TAT for pre-AI and post-AI during work hours are
68.9 [95% CI" 55.0, 82.8] and 46.7 [38.1, 55.2] minutes respectively, and those
during off-hours are 44.8 [33.7, 55.9] and 42.0 [33.6, 50.3] minutes.
Clinically-observed time-savings during work hours (22.2 [95% CI: 5.85, 38.6]
minutes) were significant (p=0.004), while off-hour (2.82 [-11.1, 16.7]
minutes) were not (p=0.345). Observed time-savings aligned with model
predictions (29.6 [95% range: 23.2, 38.1] minutes for work hours; 2.10 [1.76,
2.58] minutes for off-hours). Discussion: Consideration and quantification of
clinical workflow contribute to an accurate assessment of the expected
time-savings in TAT following deployment of an AI-triage device.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective](https://arxiv.org/abs/2503.01933)
*Rakshit Aralimatti,Syed Abdul Gaffar Shakhadri,Kruthika KR,Kartik Basavaraj Angadi*

Main category: cs.LG

TL;DR: Shakti系列小型语言模型（100M、250M、500M参数）针对边缘设备部署挑战，通过高效架构、量化技术和负责任AI原则，在智能手机、物联网等设备上实现本地智能。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备部署大型语言模型时面临的高计算需求、能耗和数据隐私风险等挑战。

Method: 结合高效架构设计、量化技术和负责任AI原则，开发了Shakti-100M、Shakti-250M和Shakti-500M三个小型语言模型。

Result: 在通用任务（MMLU、Hellaswag）和专业领域（医疗、金融、法律）的基准测试中表现优异，证明精心设计的小型模型在边缘AI场景中能够满足甚至超越预期。

Conclusion: 经过精心工程设计和微调的紧凑模型能够在现实世界边缘AI场景中取得出色表现，为边缘设备智能部署提供了可行解决方案。

Abstract: Deploying large scale language models on edge devices faces inherent
challenges such as high computational demands, energy consumption, and
potential data privacy risks. This paper introduces the Shakti Small Language
Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these
constraints headon. By combining efficient architectures, quantization
techniques, and responsible AI principles, the Shakti series enables on-device
intelligence for smartphones, smart appliances, IoT systems, and beyond. We
provide comprehensive insights into their design philosophy, training
pipelines, and benchmark performance on both general tasks (e.g., MMLU,
Hellaswag) and specialized domains (healthcare, finance, and legal). Our
findings illustrate that compact models, when carefully engineered and
fine-tuned, can meet and often exceed expectations in real-world edge-AI
scenarios.

</details>


### [4] [Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction](https://arxiv.org/abs/2501.01087)
*Syed Tahir Hussain Rizvi,Neel Kanwal,Muddasar Naeem*

Main category: cs.LG

TL;DR: 提出了一种基于高斯激活的线性模型GLinear，用于多元时间序列预测，该模型利用周期性模式，在数据效率更高的情况下实现了比现有线性模型和Transformer模型更好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测领域存在争议：Transformer模型虽然擅长处理长序列，但可能难以保持时间序列数据中的时序关系。近期研究表明，简单的线性模型可能比复杂的Transformer模型表现更好或至少具有竞争力。

Method: 提出Gaussian-activated Linear model (GLinear)，一种数据效率高的架构，通过利用周期性模式来提高多元时间序列预测的准确性。该模型需要的历史数据比其他最先进的线性预测器更少。

Result: 在ETTh1、Electricity、Traffic和Weather四个数据集上的评估表明，GLinear在大多数多元时间序列预测情况下优于现有的线性架构（如NLinear、DLinear、RLinear）和基于Transformer的时间序列预测器（Autoformer），在其他情况下也具有竞争力。

Conclusion: GLinear模型为开发更简单、更复杂的数据和计算效率高的时间序列分析架构开辟了新的研究前沿。

Abstract: Time Series Forecasting (TSF) is an important application across many fields.
There is a debate about whether Transformers, despite being good at
understanding long sequences, struggle with preserving temporal relationships
in time series data. Recent research suggests that simpler linear models might
outperform or at least provide competitive performance compared to complex
Transformer-based models for TSF tasks. In this paper, we propose a novel
data-efficient architecture, \textit{Gaussian-activated Linear model
(GLinear)}, for multivariate TSF that exploits periodic patterns to provide
better accuracy. It achieves higher prediction accuracy while requiring less
historical data than other state-of-the-art linear predictors. Four different
datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the
performance of the proposed predictor. A performance comparison with
state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear)
and transformer-based time series predictors (Autoformer) shows that the
GLinear, despite being data efficient, outperforms the existing architectures
in most cases of multivariate TSF while being competitive in others. We hope
that the proposed GLinear model opens new fronts of research and development of
simpler and more sophisticated architectures for data and computationally
efficient time-series analysis. The source code is publicly available on
GitHub.

</details>


### [5] [Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators](https://arxiv.org/abs/2510.14983)
*Oskar Triebe,Fletcher Passow,Simon Wittner,Leonie Wagner,Julio Arend,Tao Sun,Chad Zanocco,Marek Miltner,Arezou Ghesmati,Chen-Hao Tsai,Christoph Bergmeir,Ram Rajagopal*

Main category: cs.LG

TL;DR: 开发了一个多级电力负荷预测系统，能够从区域级扩展到节点级预测，提高预测精度和可解释性，帮助电网运营商更好地管理可再生能源带来的负荷不确定性。


<details>
  <summary>Details</summary>
Motivation: 可持续能源发展增加了电力负荷的不确定性，输电系统运营商需要更高空间分辨率的负荷预测，从区域聚合扩展到单个节点，但节点负荷预测精度较低且管理困难。

Method: 设计多级预测系统，使用可解释和可扩展的预测模型，评估解决节点负荷异质性和波动性的方案，采用完全并行化的单模型预测工作流。

Result: 区域预测的准确性和可解释性得到改善，节点预测有显著提升，运营商能够以前所未有的置信度和准确性调整预测，并精确诊断原本不透明的错误。

Conclusion: 多级预测系统使输电系统运营商能够逐步将区域运营扩展到节点预测，有效应对可再生能源发展带来的负荷不确定性挑战。

Abstract: The reliability of local power grid infrastructure is challenged by
sustainable energy developments increasing electric load uncertainty.
Transmission System Operators (TSOs) need load forecasts of higher spatial
resolution, extending current forecasting operations from zonal aggregates to
individual nodes. However, nodal loads are less accurate to forecast and
require a large number of individual forecasts, which are hard to manage for
the human experts assessing risks in the control room's daily operations
(operator). In collaboration with a TSO, we design a multi-level system that
meets the needs of operators for hourly day-ahead load forecasting. Utilizing a
uniquely extensive dataset of zonal and nodal net loads, we experimentally
evaluate our system components. First, we develop an interpretable and scalable
forecasting model that allows for TSOs to gradually extend zonal operations to
include nodal forecasts. Second, we evaluate solutions to address the
heterogeneity and volatility of nodal load, subject to a trade-off. Third, our
system is manageable with a fully parallelized single-model forecasting
workflow. Our results show accuracy and interpretability improvements for zonal
forecasts, and substantial improvements for nodal forecasts. In practice, our
multi-level forecasting system allows operators to adjust forecasts with
unprecedented confidence and accuracy, and to diagnose otherwise opaque errors
precisely.

</details>


### [6] [TangledFeatures: Robust Feature Selection in Highly Correlated Spaces](https://arxiv.org/abs/2510.15005)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: 提出了TangledFeatures框架，用于在相关特征空间中进行特征选择，通过识别纠缠预测因子组中的代表性特征来减少冗余并保留解释力。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法主要关注预测准确性，但在存在相关预测因子的情况下性能会下降，需要一种能够处理相关特征空间的方法。

Method: TangledFeatures框架识别相关预测因子组中的代表性特征，减少冗余同时保持解释能力。

Result: 在Alanine Dipeptide数据集上应用该框架预测主链扭转角，所选特征对应于结构上有意义的原子间距离，能够解释这些角度的变化。

Conclusion: TangledFeatures提供比传统选择技术更可解释和稳定的分析基础，可直接应用于下游模型。

Abstract: Feature selection is a fundamental step in model development, shaping both
predictive performance and interpretability. Yet, most widely used methods
focus on predictive accuracy, and their performance degrades in the presence of
correlated predictors. To address this gap, we introduce TangledFeatures, a
framework for feature selection in correlated feature spaces. It identifies
representative features from groups of entangled predictors, reducing
redundancy while retaining explanatory power. The resulting feature subset can
be directly applied in downstream models, offering a more interpretable and
stable basis for analysis compared to traditional selection techniques. We
demonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying
it to the prediction of backbone torsional angles and show that the selected
features correspond to structurally meaningful intra-atomic distances that
explain variation in these angles.

</details>


### [7] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: 提出了一种改进的C51分布强化学习算法ES-C51，用Expected Sarsa更新替换贪婪Q学习更新，通过softmax结合所有动作信息来解决动作期望奖励相似时的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统C51算法使用贪婪Q学习更新，当多个动作期望奖励相似但分布不同时，会导致学习不稳定。需要一种更稳定的方法来处理这种情况。

Method: 将C51的贪婪Q学习更新替换为Expected Sarsa更新，使用softmax计算结合所有可能动作的信息，而不是仅依赖单个最佳动作。

Result: 在Gym经典控制环境和Atari-10游戏中评估，ES-C51在多数环境中优于改进的QL-C51（将C51的探索策略从ε-greedy改为softmax）。

Conclusion: ES-C51通过Expected Sarsa更新有效解决了动作期望奖励相似时的不稳定性问题，能够学习到性能更高的策略。

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [8] [Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines](https://arxiv.org/abs/2510.15010)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Balamurugan Balusamy,Wathiq Mansoor*

Main category: cs.LG

TL;DR: 提出了一种基于集成深度学习的无监督异常检测框架，用于风力涡轮机的早期故障检测，结合VAE、LSTM自编码器和Transformer架构，在真实SCADA数据上实现了0.947的AUC-ROC和提前48小时的故障检测。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机可靠性对可再生能源行业至关重要，早期故障检测能显著减少停机时间和维护成本。

Method: 集成变分自编码器(VAE)、LSTM自编码器和Transformer架构，结合特征工程提取时域、统计和频域特征，通过集成评分和自适应阈值进行无监督异常检测。

Result: 在包含89年真实涡轮机数据的CARE数据集上评估，AUC-ROC达到0.947，能够提前48小时检测故障。

Conclusion: 该方法通过预测性维护显著减少涡轮机故障，提高大规模风能部署的运营效率，具有重要的社会价值。

Abstract: Wind turbine reliability is critical to the growing renewable energy sector,
where early fault detection significantly reduces downtime and maintenance
costs. This paper introduces a novel ensemble-based deep learning framework for
unsupervised anomaly detection in wind turbines. The method integrates
Variational Autoencoders (VAE), LSTM Autoencoders, and Transformer
architectures, each capturing different temporal and contextual patterns from
high-dimensional SCADA data. A unique feature engineering pipeline extracts
temporal, statistical, and frequency-domain indicators, which are then
processed by the deep models. Ensemble scoring combines model predictions,
followed by adaptive thresholding to detect operational anomalies without
requiring labeled fault data. Evaluated on the CARE dataset containing 89 years
of real-world turbine data across three wind farms, the proposed method
achieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to
failure. This approach offers significant societal value by enabling predictive
maintenance, reducing turbine failures, and enhancing operational efficiency in
large-scale wind energy deployments.

</details>


### [9] [AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.15038)
*Lingkai Kong,Molei Tao,Yang Liu,Bryan Wang,Jinmiao Fu,Chien-Chih Wang,Huidong Liu*

Main category: cs.LG

TL;DR: AlignFlow是一种基于半离散最优传输(SDOT)的流生成模型训练方法，通过将噪声空间划分为Laguerre单元并映射到数据点，提高了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于最优传输的流生成模型方法使用小批量采样估计传输计划，这限制了其在大规模高维数据集上的可扩展性。

Method: 利用半离散最优传输(SDOT)在噪声分布和数据点之间建立显式最优对齐，通过划分噪声空间为Laguerre单元，每个单元映射到对应的数据点。

Result: AlignFlow能够扩展到大型数据集和模型架构，计算开销可忽略，显著提升了多种最先进流生成模型算法的性能。

Conclusion: AlignFlow是一种可即插即用的组件，通过半离散最优传输有效提升了流生成模型的训练效率和性能。

Abstract: Flow-based Generative Models (FGMs) effectively transform noise into complex
data distributions. Incorporating Optimal Transport (OT) to couple noise and
data during FGM training has been shown to improve the straightness of flow
trajectories, enabling more effective inference. However, existing OT-based
methods estimate the OT plan using (mini-)batches of sampled noise and data
points, which limits their scalability to large and high-dimensional datasets
in FGMs. This paper introduces AlignFlow, a novel approach that leverages
Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by
establishing an explicit, optimal alignment between noise distribution and data
points with guaranteed convergence. SDOT computes a transport map by
partitioning the noise space into Laguerre cells, each mapped to a
corresponding data point. During FGM training, i.i.d. noise samples are paired
with data points via the SDOT map. AlignFlow scales well to large datasets and
model architectures with negligible computational overhead. Experimental
results show that AlignFlow improves the performance of a wide range of
state-of-the-art FGM algorithms and can be integrated as a plug-and-play
component. Code is available at: https://github.com/konglk1203/AlignFlow.

</details>


### [10] [IQNN-CS: Interpretable Quantum Neural Network for Credit Scoring](https://arxiv.org/abs/2510.15044)
*Abdul Samad Khan,Nouhaila Innan,Aeysha Khalique,Muhammad Shafique*

Main category: cs.LG

TL;DR: 提出IQNN-CS可解释量子神经网络框架，用于多类别信用风险分类，结合变分量子神经网络和事后解释技术，引入ICAA指标量化跨类别归因差异，在真实信用数据集上验证了稳定训练、竞争性预测性能和增强的可解释性。


<details>
  <summary>Details</summary>
Motivation: 信用评分是金融服务中的高风险任务，量子机器学习虽然提供新计算能力，但其黑盒特性在需要透明度和信任的领域面临挑战。

Method: IQNN-CS框架结合变分量子神经网络和针对结构化数据的事后解释技术，引入Inter-Class Attribution Alignment (ICAA)指标量化跨预测类别的归因差异。

Result: 在两个真实世界信用数据集上的评估显示，IQNN-CS具有稳定的训练动态、竞争性的预测性能和增强的可解释性。

Conclusion: 为金融决策中的透明和可问责量子机器学习模型提供了实用路径。

Abstract: Credit scoring is a high-stakes task in financial services, where model
decisions directly impact individuals' access to credit and are subject to
strict regulatory scrutiny. While Quantum Machine Learning (QML) offers new
computational capabilities, its black-box nature poses challenges for adoption
in domains that demand transparency and trust. In this work, we present
IQNN-CS, an interpretable quantum neural network framework designed for
multiclass credit risk classification. The architecture combines a variational
QNN with a suite of post-hoc explanation techniques tailored for structured
data. To address the lack of structured interpretability in QML, we introduce
Inter-Class Attribution Alignment (ICAA), a novel metric that quantifies
attribution divergence across predicted classes, revealing how the model
distinguishes between credit risk categories. Evaluated on two real-world
credit datasets, IQNN-CS demonstrates stable training dynamics, competitive
predictive performance, and enhanced interpretability. Our results highlight a
practical path toward transparent and accountable QML models for financial
decision-making.

</details>


### [11] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: SPA框架通过自监督微调学习世界模型，显著提升LLM智能体在分布外环境中的性能


<details>
  <summary>Details</summary>
Motivation: LLM智能体在分布外环境中表现不佳，难以将内部知识与环境动态对齐，传统RL训练难以扩展

Method: 将世界模型分解为状态表示和转移建模，通过自监督微调阶段学习世界模型，然后在策略优化前模拟未来状态

Result: 在Sokoban、FrozenLake和Sudoku等环境中显著提升性能，Sokoban成功率从25.6%提升至59.8%，FrozenLake得分从22.1%提升至70.9%

Conclusion: 为LLM智能体配备内部世界模型能更好地对齐推理与环境动态，改善决策能力

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [12] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 该论文提出了多层可配置时变马尔可夫决策过程（MCTVMDP），其中智能体不仅可以通过原始动作优化策略，还可以通过上层模型改变动作主动修改环境动态模型，从而提升长期奖励。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习假设环境是给定或固定的，但现实中智能体可能具有主动修改环境动态的能力。研究这种主动环境配置的智能体可以突破被动适应的限制，通过改变底层模型来获得更高奖励。

Method: 引入MCTVMDP框架，包含上层MDP用于配置模型改变动作，下层MDP具有可配置的非平稳转移函数。智能体需要联合优化上层配置策略和下层原始动作策略。

Result: 提出了一个能够处理主动环境配置问题的理论框架，允许智能体通过修改环境动态来优化长期奖励。

Conclusion: MCTVMDP为研究具有主动环境修改能力的智能体提供了理论基础，扩展了传统强化学习的边界，使智能体能够通过配置环境动态来获得更好的性能。

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [13] [Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models](https://arxiv.org/abs/2510.15061)
*Samuel Paech,Allen Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: Antislop框架通过采样器、自动化流水线和FTPO微调方法，有效检测和消除LLM输出中的重复短语模式，在保持性能的同时显著减少AI生成文本的特征性表达。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型广泛使用导致输出中出现特征性重复短语（称为"slop"），这降低了输出质量并使AI生成文本容易被识别。

Method: 结合三种创新方法：Antislop采样器（推理时回溯抑制不需要的字符串）、自动化流水线（分析模型特定slop并生成训练数据）、FTPO微调方法（在单个token级别调整logits）。

Result: 成功抑制8000+个模式，而token禁止方法在2000个模式时就失效；FTPO实现90%的slop减少，同时在GSM8K、MMLU和创意写作任务中保持或提升性能。

Conclusion: Antislop框架能有效减少LLM输出中的重复模式，显著改善文本质量，而DPO方法虽然能实现较弱的抑制但会导致写作质量和词汇多样性显著下降。

Abstract: Widespread LLM adoption has introduced characteristic repetitive phraseology,
termed ``slop,'' which degrades output quality and makes AI-generated text
immediately recognizable. We present Antislop, a comprehensive framework
providing tools to both detect and eliminate these overused patterns. Our
approach combines three innovations: (1) The Antislop Sampler, which uses
backtracking to suppress unwanted strings at inference time without destroying
vocabulary; (2) An automated pipeline that profiles model-specific slop against
human baselines and generates training data; (3) Final Token Preference
Optimization (FTPO), a novel fine-tuning method that operates on individual
tokens, surgically adjusting logits wherever a banned pattern has appeared in
an inference trace. We demonstrate that some slop patterns appear over
1,000$\times$ more frequently in LLM output than human text. The Antislop
Sampler successfully suppresses 8,000+ patterns while maintaining quality,
whereas token banning becomes unusable at just 2,000. Most importantly, FTPO
achieves 90\% slop reduction while maintaining or improving performance in
cross-domain evals including GSM8K, MMLU, and creative writing tasks. In
contrast, DPO suffers significant degradation in writing quality and lexical
diversity despite achieving weaker suppression. We release all code and results
under MIT license: https://github.com/sam-paech/auto-antislop.

</details>


### [14] [Physics-informed data-driven machine health monitoring for two-photon lithography](https://arxiv.org/abs/2510.15075)
*Sixian Jia,Zhiqiao Dong,Chenhui Shao*

Main category: cs.LG

TL;DR: 提出了三种基于物理信息数据驱动模型的双光子光刻系统健康监测方法，能够准确及时地监控机器健康状况，提高维护效率。


<details>
  <summary>Details</summary>
Motivation: 当前双光子光刻系统的维护主要依赖经验而非基于机器健康状况的智能监控，导致维护不及时或过度维护的问题。

Method: 结合物理信息数据驱动的结构尺寸预测模型与统计方法，开发了三种能够处理不同泛化能力水平的健康监测方法。

Result: 在包含六种工艺参数组合和六种结构尺寸的全面实验数据集上测试，所有方法都达到了高精度，表现出优秀的有效性、鲁棒性和泛化能力。

Conclusion: 这些成果为实现双光子光刻系统基于状态的维护迈出了重要一步。

Abstract: Two-photon lithography (TPL) is a sophisticated additive manufacturing
technology for creating three-dimensional (3D) micro- and nano-structures.
Maintaining the health of TPL systems is critical for ensuring consistent
fabrication quality. Current maintenance practices often rely on experience
rather than informed monitoring of machine health, resulting in either untimely
maintenance that causes machine downtime and poor-quality fabrication, or
unnecessary maintenance that leads to inefficiencies and avoidable downtime. To
address this gap, this paper presents three methods for accurate and timely
monitoring of TPL machine health. Through integrating physics-informed
data-driven predictive models for structure dimensions with statistical
approaches, the proposed methods are able to handle increasingly complex
scenarios featuring different levels of generalizability. A comprehensive
experimental dataset that encompasses six process parameter combinations and
six structure dimensions under two machine health conditions was collected to
evaluate the effectiveness of the proposed approaches. Across all test
scenarios, the approaches are shown to achieve high accuracies, demonstrating
excellent effectiveness, robustness, and generalizability. These results
represent a significant step toward condition-based maintenance for TPL
systems.

</details>


### [15] [Online Correlation Clustering: Simultaneously Optimizing All $\ell_p$-norms](https://arxiv.org/abs/2510.15076)
*Sami Davies,Benjamin Moseley,Heather Newman*

Main category: cs.LG

TL;DR: 该论文提出了首个在线相关聚类算法，能够在AOS模型中同时近似所有ℓp范数目标，将离线的"全范数"保证扩展到在线设置。


<details>
  <summary>Details</summary>
Motivation: 相关聚类的ℓp范数目标在最小化总分歧（ℓ1范数）和确保个体节点公平性（ℓ∞范数）之间存在基本权衡。离线设置中可以同时近似所有范数，但标准随机顺序在线模型存在根本性分离，需要超越最坏情况的模型。

Method: 在在线带样本（AOS）模型中，给定输入的一小部分作为样本，设计单一算法产生一个聚类，该聚类能同时为所有ℓp范数提供竞争性保证。

Result: 算法在AOS模型中：对所有ℓp范数高概率O(log⁴n)竞争比；对ℓ∞范数高概率O(logn)竞争比；对ℓ1范数期望O(1)竞争比。同时证明了标准RO模型中ℓ∞范数的Ω(n¹/³)下界。

Conclusion: 成功将离线的"全范数"保证扩展到在线世界，证明了AOS模型的有效性，并提供了近乎紧的竞争比下界。

Abstract: The $\ell_p$-norm objectives for correlation clustering present a fundamental
trade-off between minimizing total disagreements (the $\ell_1$-norm) and
ensuring fairness to individual nodes (the $\ell_\infty$-norm). Surprisingly,
in the offline setting it is possible to simultaneously approximate all
$\ell_p$-norms with a single clustering. Can this powerful guarantee be
achieved in an online setting? This paper provides the first affirmative
answer. We present a single algorithm for the online-with-a-sample (AOS) model
that, given a small constant fraction of the input as a sample, produces one
clustering that is simultaneously $O(\log^4 n)$-competitive for all
$\ell_p$-norms with high probability, $O(\log n)$-competitive for the
$\ell_\infty$-norm with high probability, and $O(1)$-competitive for the
$\ell_1$-norm in expectation. This work successfully translates the offline
"all-norms" guarantee to the online world.
  Our setting is motivated by a new hardness result that demonstrates a
fundamental separation between these objectives in the standard random-order
(RO) online model. Namely, while the $\ell_1$-norm is trivially
$O(1)$-approximable in the RO model, we prove that any algorithm in the RO
model for the fairness-promoting $\ell_\infty$-norm must have a competitive
ratio of at least $\Omega(n^{1/3})$. This highlights the necessity of a
different beyond-worst-case model. We complement our algorithm with lower
bounds, showing our competitive ratios for the $\ell_1$- and $\ell_\infty$-
norms are nearly tight in the AOS model.

</details>


### [16] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: TempO是一个基于流匹配的潜在模型，利用稀疏条件和通道折叠处理3D时空场，通过时间条件傅里叶层捕捉多尺度模式，在PDE动力学预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预测高维PDE控制动力学是生成建模的核心挑战，现有自回归和扩散方法存在累积误差和离散化伪影，限制了长期物理一致性预测。

Method: 提出TempO潜在流匹配模型，采用稀疏条件和通道折叠高效处理3D时空场，使用时间条件傅里叶层捕捉多尺度模式。

Result: 在三个基准PDE数据集上优于最先进基线，谱分析显示能更好恢复多尺度动力学，效率研究表明相比基于注意力或卷积回归器具有参数和内存轻量设计。

Conclusion: 流匹配为PDE动力学预测提供了高效确定性采样的自然替代方案，TempO在精度和效率方面均表现优异。

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [17] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: DLER通过强化学习优化推理语言模型的输出长度，在保持准确性的同时大幅缩短响应长度，实现了最先进的准确率-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前推理语言模型虽然性能强大，但生成了不必要的冗长输出，最大化每个token的智能度（准确率相对于响应长度）仍是一个未解决的问题。

Method: 提出DLER训练配方，结合批量奖励归一化、更高裁剪、动态采样和简单的截断长度惩罚，解决强化学习优化中的三个关键挑战：优势估计偏差大、熵崩溃和稀疏奖励信号。

Result: DLER将输出长度减少超过70%，同时超越了所有先前基线的准确率。DLER-7B相比DeepSeek-R1-7B生成多个简洁响应，准确率提高28%且延迟更低。

Conclusion: DLER通过改进的强化学习优化实现了推理语言模型的准确率-效率平衡，并提出了难度感知DLER和更新选择性合并方法以进一步提升效率和适用性。

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [18] [Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework](https://arxiv.org/abs/2510.15127)
*David J. Albers,Tell D. Bennett,Jana de Wiljes,Bradford J. Smith,Peter D. Sottile,J. N. Stroh*

Main category: cs.LG

TL;DR: 开发了一个基于进化博弈论的框架来分析机械通气患者-呼吸机-护理系统的数据，为机械通气优化和个性化提供定量分析基础。


<details>
  <summary>Details</summary>
Motivation: 需要分析重症监护中异质性患者-呼吸机系统的数据，以理解机械通气和辅助护理决策对患者结果的影响，改进重症呼吸管理。

Method: 采用进化博弈论分析呼吸行为，为概率和随机方法（如强化学习）提供定量前驱分析，并在合成数据和真实ICU数据上进行验证。

Result: 建立了可扩展的分析方法，能够揭示数据生成过程的复杂性，为机械通气决策的状态转换模型开发奠定了基础。

Conclusion: 这是实现机械通气优化和个性化的重要一步，为基于经验和博弈论元素的机械通气决策模拟提供了发展潜力。

Abstract: Identifying the effects of mechanical ventilation strategies and protocols in
critical care requires analyzing data from heterogeneous patient-ventilator
systems within the context of the clinical decision-making environment. This
research develops a framework to help understand the consequences of mechanical
ventilation (MV) and adjunct care decisions on patient outcome from
observations of critical care patients receiving MV. Developing an
understanding of and improving critical care respiratory management requires
the analysis of existing secondary-use clinical data to generate hypotheses
about advantageous variations and adaptations of current care. This work
introduces a perspective of the joint patient-ventilator-care systems
(so-called J6) to develop a scalable method for analyzing data and trajectories
of these complex systems. To that end, breath behaviors are analyzed using
evolutionary game theory (EGT), which generates the necessary quantitative
precursors for deeper analysis through probabilistic and stochastic machinery
such as reinforcement learning. This result is one step along the pathway
toward MV optimization and personalization. The EGT-based process is
analytically validated on synthetic data to reveal potential caveats before
proceeding to real-world ICU data applications that expose complexities of the
data-generating process J6. The discussion includes potential developments
toward a state transition model for the simulating effects of MV decision using
empirical and game-theoretic elements.

</details>


### [19] [A Simple Method for PMF Estimation on Large Supports](https://arxiv.org/abs/2510.15132)
*Alex Shtoff*

Main category: cs.LG

TL;DR: 提出一种基于图拉普拉斯算子的非参数概率质量函数估计方法，通过数据依赖的低通滤波处理多模态和重尾分布，计算高效且无需过多调参。


<details>
  <summary>Details</summary>
Motivation: 针对大型离散支撑集上的多模态、重尾概率质量函数估计问题，传统方法在处理噪声和保持粗粒度结构方面存在挑战。

Method: 将经验PMF视为线图上的信号，构建对称三对角算子（路径图拉普拉斯算子扰动），计算最小特征值对应的特征向量，将经验PMF投影到低维子空间进行平滑估计。

Result: 在合成和真实重尾数据上，该方法能有效保持粗粒度结构并抑制采样噪声，在目标场景下优于logspline和高斯KDE基线方法。

Conclusion: 该方法实现简洁、计算可靠快速，适合自动化流水线和大规模探索性分析，但存在已知失效模式（如突变不连续性）。

Abstract: We study nonparametric estimation of a probability mass function (PMF) on a
large discrete support, where the PMF is multi-modal and heavy-tailed. The core
idea is to treat the empirical PMF as a signal on a line graph and apply a
data-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal
operator, the path graph Laplacian perturbed with a diagonal matrix built from
the empirical PMF, then compute the eigenvectors, corresponding to the smallest
feq eigenvalues. Projecting the empirical PMF onto this low dimensional
subspace produces a smooth, multi-modal estimate that preserves coarse
structure while suppressing noise. A light post-processing step of clipping and
re-normalizing yields a valid PMF.
  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the
computation is reliable and runs time and memory proportional to the support
times the dimension of the desired low-dimensional supspace. We also provide a
practical, data-driven rule for selecting the dimension based on an
orthogonal-series risk estimate, so the method "just works" with minimal
tuning. On synthetic and real heavy-tailed examples, the approach preserves
coarse structure while suppressing sampling noise, compares favorably to
logspline and Gaussian-KDE baselines in the intended regimes. However, it has
known failure modes (e.g., abrupt discontinuities). The method is short to
implement, robust across sample sizes, and suitable for automated pipelines and
exploratory analysis at scale because of its reliability and speed.

</details>


### [20] [Predicting the Unpredictable: Reproducible BiLSTM Forecasting of Incident Counts in the Global Terrorism Database (GTD)](https://arxiv.org/abs/2510.15136)
*Oluwasegun Adegoke*

Main category: cs.LG

TL;DR: 使用双向LSTM模型进行恐怖主义事件周计数短期预测，在测试集上RMSE达到6.38，优于LSTM-Attention和线性滞后回归基线，并分析了时间记忆、训练历史长度等关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究恐怖主义事件周计数的短期预测，建立可复现的预测流程，评估深度学习模型相对于传统方法的性能优势。

Method: 构建包含固定时间分割的可复现管道，使用双向LSTM模型，并与季节性朴素、线性/ARIMA模型以及LSTM-Attention基线进行比较。

Result: 双向LSTM在测试集上RMSE为6.38，比LSTM-Attention提升30.6%，比线性滞后回归基线提升35.4%，在MAE和MAPE指标上也有并行改进。

Conclusion: 研究表明使用长历史数据训练的模型泛化能力最好，适中的回看窗口（20-30周）提供强上下文，双向编码对捕捉窗口内的积累和后续模式至关重要。

Abstract: We study short-horizon forecasting of weekly terrorism incident counts using
the Global Terrorism Database (GTD, 1970--2016). We build a reproducible
pipeline with fixed time-based splits and evaluate a Bidirectional LSTM
(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a
deep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE
6.38, outperforming LSTM-Attention (9.19; +30.6\%) and a linear lag-regression
baseline (+35.4\% RMSE gain), with parallel improvements in MAE and MAPE.
Ablations varying temporal memory, training-history length, spatial grain,
lookback size, and feature groups show that models trained on long historical
data generalize best; a moderate lookback (20--30 weeks) provides strong
context; and bidirectional encoding is critical for capturing both build-up and
aftermath patterns within the window. Feature-group analysis indicates that
short-horizon structure (lagged counts and rolling statistics) contributes
most, with geographic and casualty features adding incremental lift. We release
code, configs, and compact result tables, and provide a data/ethics statement
documenting GTD licensing and research-only use. Overall, the study offers a
transparent, baseline-beating reference for GTD incident forecasting.

</details>


### [21] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: 该论文首次为连续时间强化学习中的策略迁移提供了理论证明，证明了在连续时间线性二次调节器（LQR）中，一个最优策略可以作为相关LQR问题的近最优初始化，同时保持原始算法的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务上从头训练效率低下，而迁移学习在大语言模型中已证明成功。本文旨在探索如何通过预训练模型提升强化学习效率，特别是在连续时间设置中填补现有文献的空白。

Method: 研究策略迁移方法，在熵正则化的连续时间线性二次调节器（LQR）中，使用相关源任务的策略初始化目标RL任务的学习。提出了一种新的连续时间LQR策略学习算法。

Result: 证明了策略迁移在连续时间RL中的有效性，提出的算法实现了全局线性和局部超线性收敛。作为分析副产品，还通过LQR连接推导了一类连续时间基于分数的扩散模型的稳定性。

Conclusion: 研究展示了迁移学习在连续时间强化学习中的理论保证和算法优势，将先前工作从离散时间扩展到连续时间设置，为RL效率提升提供了新的理论支持。

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [22] [A simple mean field model of feature learning](https://arxiv.org/abs/2510.15174)
*Niclas Göring,Chris Mingard,Yoonsoo Nam,Ard Louis*

Main category: cs.LG

TL;DR: 本文通过统计物理方法推导了双层非线性网络在随机梯度朗之万动力学训练下的贝叶斯后验的均值场理论，揭示了有限宽度下网络与目标函数对齐的对称性破缺相变，并发现了自增强输入特征选择机制对泛化性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 特征学习（神经网络在训练过程中调整内部表示）的机制仍然缺乏深入理解，需要发展理论框架来解释有限宽度网络中的特征学习现象。

Method: 使用统计物理方法推导了双层非线性网络在SGLD训练下的自洽均值场理论，并在基本均值场理论基础上引入了自增强输入特征选择机制。

Result: 无限宽度下理论退化为核岭回归，有限宽度下预测了对称性破缺相变；基本均值场理论能半定量预测特征学习的出现，但低估了相变后的泛化改进；加入自增强特征选择机制后能定量匹配SGLD训练网络的学习曲线。

Conclusion: 自增强输入特征选择是特征学习的关键机制，将其纳入均值场理论能提供对特征学习的机制性理解并定量描述网络训练行为。

Abstract: Feature learning (FL), where neural networks adapt their internal
representations during training, remains poorly understood. Using methods from
statistical physics, we derive a tractable, self-consistent mean-field (MF)
theory for the Bayesian posterior of two-layer non-linear networks trained with
stochastic gradient Langevin dynamics (SGLD). At infinite width, this theory
reduces to kernel ridge regression, but at finite width it predicts a symmetry
breaking phase transition where networks abruptly align with target functions.
While the basic MF theory provides theoretical insight into the emergence of FL
in the finite-width regime, semi-quantitatively predicting the onset of FL with
noise or sample size, it substantially underestimates the improvements in
generalisation after the transition. We trace this discrepancy to a key
mechanism absent from the plain MF description: \textit{self-reinforcing input
feature selection}. Incorporating this mechanism into the MF theory allows us
to quantitatively match the learning curves of SGLD-trained networks and
provides mechanistic insight into FL.

</details>


### [23] [Finding geodesics with the Deep Ritz method](https://arxiv.org/abs/2510.15177)
*Conor Rowan*

Main category: cs.LG

TL;DR: 该论文提出测地线问题特别适合深度Ritz方法，并通过三个数值实例验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 测地线问题在物理和工程中广泛存在，但科学机器学习社区对其关注较少。作者认为这些问题的简单几何结构、变分结构和自然非线性使其特别适合深度Ritz方法。

Method: 使用深度Ritz方法解决测地线问题，通过三个数值实例进行验证：路径规划、光学和固体力学。

Result: 通过三个不同领域的数值实验，证明了深度Ritz方法在解决测地线问题上的有效性。

Conclusion: 测地线问题是深度Ritz方法的一个有前景的应用方向，也是未来科学机器学习研究的一个富有成果的方向。

Abstract: Geodesic problems involve computing trajectories between prescribed initial
and final states to minimize a user-defined measure of distance, cost, or
energy. They arise throughout physics and engineering -- for instance, in
determining optimal paths through complex environments, modeling light
propagation in refractive media, and the study of spacetime trajectories in
control theory and general relativity. Despite their ubiquity, the scientific
machine learning (SciML) community has given relatively little attention to
investigating its methods in the context of these problems. In this work, we
argue that given their simple geometry, variational structure, and natural
nonlinearity, geodesic problems are particularly well-suited for the Deep Ritz
method. We substantiate this claim with three numerical examples drawn from
path planning, optics, and solid mechanics. Our goal is not to provide an
exhaustive study of geodesic problems, but rather to identify a promising
application of the Deep Ritz method and a fruitful direction for future SciML
research.

</details>


### [24] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 提出一个两阶段模型，整合临床和影像信息来改进髋部骨折风险预测，相比传统方法具有更高敏感性和更少漏诊。


<details>
  <summary>Details</summary>
Motivation: 传统工具如DXA T-score和FRAX在髋部骨折风险评估中缺乏敏感性，特别是对于无既往骨折或骨量减少的高风险个体。

Method: 使用MrOS、SOF和UK Biobank数据，构建两阶段模型：第一阶段使用临床、人口统计学和功能变量进行筛查，第二阶段整合DXA影像特征进行精炼。

Result: 经过内外验证，该模型在不同队列中表现一致且适应性强，相比T-score和FRAX具有更高敏感性和更少漏诊病例。

Conclusion: 该两阶段框架为早期髋部骨折风险评估提供了一种成本效益高且个性化的方法。

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [25] [Automotive Crash Dynamics Modeling Accelerated with Machine Learning](https://arxiv.org/abs/2510.15201)
*Mohammad Amin Nabian,Sudeep Chavare,Deepak Akhare,Rishikesh Ranade,Ram Cherukuri,Srinivas Tadepalli*

Main category: cs.LG

TL;DR: 本研究探索了基于机器学习的代理模型来预测汽车碰撞场景中的结构变形，使用NVIDIA PhysicsNeMo框架比较了MeshGraphNet和Transolver两种神经网络架构，以及三种瞬态动力学建模策略，在包含150个详细有限元仿真的BIW碰撞数据集上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的汽车耐撞性评估依赖计算昂贵且耗时的有限元仿真，需要开发更高效的预测方法。由于机器学习在结构碰撞动力学中的应用研究有限，本研究旨在探索各种建模方法的可行性和工程实用性。

Method: 研究了MeshGraphNet和Transolver两种最先进的神经网络架构，以及三种瞬态动力学建模策略：时间条件法、标准自回归方法和增强稳定性的自回归方案。使用包含150个LS-DYNA有限元仿真的BIW碰撞数据集，以未变形网格几何和组件特性作为输入预测变形网格的时空演化。

Result: 模型能够以合理的保真度捕捉整体变形趋势，证明了机器学习在结构碰撞动力学中应用的可行性。虽然尚未达到完整有限元仿真的精度，但计算成本降低了数个数量级。

Conclusion: 机器学习代理模型在汽车碰撞动力学预测中具有可行性，能够显著降低计算成本，支持快速设计探索和早期优化，为耐撞性评估提供了高效替代方案。

Abstract: Crashworthiness assessment is a critical aspect of automotive design,
traditionally relying on high-fidelity finite element (FE) simulations that are
computationally expensive and time-consuming. This work presents an exploratory
comparative study on developing machine learning-based surrogate models for
efficient prediction of structural deformation in crash scenarios using the
NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine
learning to structural crash dynamics, the primary contribution lies in
demonstrating the feasibility and engineering utility of the various modeling
approaches explored in this work. We investigate two state-of-the-art neural
network architectures for modeling crash dynamics: MeshGraphNet, and
Transolver. Additionally, we examine three strategies for modeling transient
dynamics: time-conditional, the standard Autoregressive approach, and a
stability-enhanced Autoregressive scheme incorporating rollout-based training.
The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset
comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a
structurally rich vehicle assembly with over 200 components, including 38 key
components featuring variable thickness distributions to capture realistic
manufacturing variability. Each model utilizes the undeformed mesh geometry and
component characteristics as inputs to predict the spatiotemporal evolution of
the deformed mesh during the crash sequence. Evaluation results show that the
models capture the overall deformation trends with reasonable fidelity,
demonstrating the feasibility of applying machine learning to structural crash
dynamics. Although not yet matching full FE accuracy, the models achieve
orders-of-magnitude reductions in computational cost, enabling rapid design
exploration and early-stage optimization in crashworthiness evaluation.

</details>


### [26] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: 该论文研究了表示几何和归一化对基于马氏距离的OOD检测方法性能的影响，提出了径向缩放L2归一化方法，通过控制特征空间的径向几何来显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 虽然马氏距离方法在OOD检测中广泛使用，但表示几何和归一化对其性能的影响尚未被充分理解，这限制了其下游应用。

Method: 在多种图像基础模型、数据集和距离归一化方案上进行全面实证研究，分析理想几何条件，提出径向缩放L2归一化方法，通过可调参数控制特征空间的径向几何。

Result: 研究表明马氏距离方法并非普遍可靠；光谱和内在维度指标能准确预测模型的OOD性能；径向缩放L2归一化能显著提升OOD检测性能。

Conclusion: 通过桥接表示几何、归一化和OOD性能之间的差距，为设计更有效可靠的深度学习模型提供了新见解。

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [27] [ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning](https://arxiv.org/abs/2510.15211)
*Yongchan Kwon,Shang Zhu,Federico Bianchi,Kaitlyn Zhou,James Zou*

Main category: cs.LG

TL;DR: 提出了ReasonIF基准来评估大型推理模型在推理过程中遵循用户指令的能力，发现现有模型表现不佳，并探索了两种改进方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中遵循用户指令对于可靠性、安全性和可控性至关重要，但目前缺乏系统评估方法。

Method: 构建ReasonIF基准，包含6类指令提示，评估多个开源LRM，并探索多轮推理和推理指令微调(RIF)两种改进策略。

Result: 现有模型在推理指令遵循方面表现较差，最高指令遵循分数低于0.25；任务难度增加时表现更差；RIF方法将GPT-OSS-20B的分数从0.11提升到0.27。

Conclusion: 推理指令遵循是LRM的重要能力但现有模型表现不足，RIF方法有效但仍有很大改进空间。

Abstract: The ability of large language models (LLMs) to follow user instructions is
central to their reliability, safety, and usefulness. While prior studies
assess instruction adherence in the model's main responses, we argue that it is
also critical for large reasoning models (LRMs) to follow user instructions
throughout their reasoning process. Reasoning instruction following makes LRMs
more controllable and transparent, while reducing risks of undesirable
shortcuts, hallucinations, or reward hacking within reasoning traces. To
evaluate this dimension, we introduce ReasonIF, a systematic benchmark for
assessing reasoning instruction following. ReasonIF includes six categories of
instruction prompts, spanning multilingual reasoning, formatting and length
control. Across many open-source LRMs including GPT-OSS, Qwen3, and
DeepSeek-R1, we find substantial failures in reasoning instruction adherence:
the highest instruction following score (IFS) remains below 0.25, meaning that
fewer than $25\%$ of reasoning traces comply with the given instructions.
Notably, as task difficulty increases, reasoning instruction following degrades
further. We also explore two strategies to enhance reasoning instruction
fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning
(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to
0.27, indicating measurable progress but leaving ample room for improvement.

</details>


### [28] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [29] [Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025](https://arxiv.org/abs/2510.15217)
*Emily Alsentzer,Marie-Laure Charpignon,Bill Chen,Niharika D'Souza,Jason Fries,Yixing Jiang,Aparajita Kashyap,Chanwoo Kim,Simon Lee,Aishwarya Mandyam,Ashery Christopher Mbilinyi,Nikita Mehandru,Nitish Nagesh,Brighton Nuwagira,Emma Pierson,Arvind Pillai,Akane Sano,Tanveer Syeda-Mahmood,Shashank Yadav,Elias Adhanom,Muhammad Umar Afza,Amelia Archer,Suhana Bedi,Vasiliki Bikia,Trenton Chang,George H. Chen,Winston Chen,Erica Chiang,Edward Choi,Octavia Ciora,Paz Dozie-Nnamah,Shaza Elsharief,Matthew Engelhard,Ali Eshragh,Jean Feng,Josh Fessel,Scott Fleming,Kei Sen Fong,Thomas Frost,Soham Gadgil,Judy Gichoya,Leeor Hershkovich,Sujeong Im,Bhavya Jain,Vincent Jeanselme,Furong Jia,Qixuan,Jin,Yuxuan Jin,Daniel Kapash,Geetika Kapoor,Behdokht Kiafar,Matthias Kleiner,Stefan Kraft,Annika Kumar,Daeun Kyung,Zhongyuan Liang,Joanna Lin,Qianchu,Liu,Chang Liu,Hongzhou Luan,Chris Lunt,Leopoldo Julían Lechuga López,Matthew B. A. McDermott,Shahriar Noroozizadeh,Connor O'Brien,YongKyung Oh,Mixail Ota,Stephen Pfohl,Meagan Pi,Tanmoy Sarkar Pias,Emma Rocheteau,Avishaan Sethi,Toru Shirakawa,Anita Silver,Neha Simha,Kamile Stankeviciute,Max Sunog,Peter Szolovits,Shengpu Tang,Jialu Tang,Aaron Tierney,John Valdovinos,Byron Wallace,Will Ke Wang,Peter Washington,Jeremy Weiss,Daniel Wolfe,Emily Wong,Hye Sun Yun,Xiaoman Zhang,Xiao Yu Cindy Zhang,Hayoung Jeong,Kaveri A. Thakoor*

Main category: cs.LG

TL;DR: CHIL 2025会议举办了8个研究圆桌会议，聚焦机器学习与医疗交叉领域的关键话题，包括可解释性、公平性、因果推断等主题。


<details>
  <summary>Details</summary>
Motivation: 促进机器学习与医疗交叉领域的协作对话，探讨关键挑战和新兴机遇，推动该领域的可行动方向。

Method: 通过由资深和初级主席主持的小组圆桌会议形式，鼓励开放交流、知识好奇和包容性参与。

Result: 成功举办了8个圆桌会议，涉及19位主席，覆盖了可解释性、公平性、因果推断、领域适应、基础模型等多个重要主题。

Conclusion: 研究圆桌会议为机器学习与医疗交叉领域提供了有效的协作平台，促进了该领域的集体创新和发展。

Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),
hosted by the Association for Health Learning and Inference (AHLI), was held in
person on June 25-27, 2025, at the University of California, Berkeley, in
Berkeley, California, USA. As part of this year's program, we hosted Research
Roundtables to catalyze collaborative, small-group dialogue around critical,
timely topics at the intersection of machine learning and healthcare. Each
roundtable was moderated by a team of senior and junior chairs who fostered
open exchange, intellectual curiosity, and inclusive engagement. The sessions
emphasized rigorous discussion of key challenges, exploration of emerging
opportunities, and collective ideation toward actionable directions in the
field. In total, eight roundtables were held by 19 roundtable chairs on topics
of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,
and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning
from Small Medical Data," "Multimodal Methods," and "Scalable, Translational
Healthcare Solutions."

</details>


### [30] [Machine Learning for Early Detection of Meningitis: Stacked Ensemble Learning with EHR data](https://arxiv.org/abs/2510.15218)
*Han Ouyang,Jesse Hamilton,Saeed Amal*

Main category: cs.LG

TL;DR: 该研究使用MIMIC-III数据库中的脑膜炎患者和非脑膜炎患者数据，通过特征选择和集成学习方法构建脑膜炎诊断模型，在模拟急诊室场景下取得了优异的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 开发能够在真实急诊室场景下有效诊断脑膜炎的AI工具，解决脑膜炎诊断的临床挑战。

Method: 使用214名脑膜炎患者和46,303名非脑膜炎患者数据，经过数据预处理和两阶段特征选择，选择临床相关特征，训练随机森林、LightGBM和深度神经网络作为基础模型，然后通过集成学习（使用逻辑回归作为元模型）整合基础模型输出。

Result: 集成学习模型在两个测试集上均表现出色：测试集1的AUC为0.9637，测试集2的AUC为0.9472。

Conclusion: 虽然直接将诊断工具部署给临床医生具有挑战性，但该研究为未来使用集成学习的AI驱动脑膜炎诊断方法铺平了道路。

Abstract: We utilized a cohort of 214 meningitis patients and 46,303 non-meningitis
patients from the MIMIC-III database. After extensive data preprocessing, which
included ICD-based cohort selection, one-hot encoding of coding, and a
two-stage feature selection process (for both the training set and the testing
sets), clinically relevant features such as gender and high-risk ICD codes
(including subarachnoid hemorrhage, secondary malignant neoplasm of the brain,
and generalized epilepsy) are selected. Overall, these clinically reasonable
and temporally adherent features provided excellent modeling performance. Three
models (Random Forest, LightGBM, and Deep Neural Networks (DNN) are trained as
base models for Ensemble Learning. Base model outputs are aggregated and
stacked into a meta model (Logistic Regression) that uses the base model
outputs as input values in training. Ultimately, soldier outputs (AUC of
Testing Set 1: 0.9637, AUC of Testing Set 2: 0.9472) are obtained through
ensemble learning.
  We created a challenging condition for diagnosing meningitis, simulating a
real-world ER (Emergency Room) scenario to enhance clinical use in real-world
applications. While directly deploying a diagnostic tool that clinicians can
use is challenging, this paper paves the way for a potential future AI-driven
diagnostic approach for meningitis using Ensemble Learning.

</details>


### [31] [Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)](https://arxiv.org/abs/2510.15219)
*Patricia Medina,Rasika Karkare*

Main category: cs.LG

TL;DR: 本文扩展了先前关于使用乘积系数增强3D LiDAR点云分类的研究，通过结合自编码器表示和KNN分类器，在PCA基线和早期框架基础上实现了持续的性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩展先前研究，探索如何通过结合乘积系数与自编码器表示来进一步提升LiDAR点云分类性能，并研究不同层级乘积系数对分类效果的影响。

Method: 将乘积系数与自编码器表示相结合，使用KNN分类器进行分类，并逐级添加乘积系数来研究其对性能的影响。

Result: 结果显示，结合乘积系数与自编码器表示在PCA基线和早期框架基础上实现了持续的性能提升，且更丰富的系数集系统性地提高了类别可分性和整体准确率。

Conclusion: 结合分层乘积系数特征与自编码器能够有效推动LiDAR分类性能的进一步提升。

Abstract: This work extends our previous study on enhancing 3D LiDAR point-cloud
classification with product coefficients
\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic
descriptors that complement the original spatial Lidar features. Here, we show
that combining product coefficients with an autoencoder representation and a
KNN classifier delivers consistent performance gains over both PCA-based
baselines and our earlier framework. We also investigate the effect of adding
product coefficients level by level, revealing a clear trend: richer sets of
coefficients systematically improve class separability and overall accuracy.
The results highlight the value of combining hierarchical product-coefficient
features with autoencoders to push LiDAR classification performance further.

</details>


### [32] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: 提出熵正则化信任衰减方法，通过应力感知指数倾斜处理分布漂移下的序列决策问题，实现动态遗憾保证和鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 研究在分布漂移下的序列决策问题，传统方法在分布变化时性能下降，需要开发能够适应分布变化的鲁棒算法。

Method: 提出熵正则化信任衰减方法，在信念更新和镜像下降决策中注入应力感知指数倾斜，通过Fenchel对偶等价性统一信念倾斜和决策倾斜。

Result: 证明了高概率敏感性边界和动态遗憾保证，在KL漂移路径长度下达到$\tilde{O}(\sqrt{T})$遗憾，实现每切换$O(1)$遗憾，而无应力更新会产生$\Omega(1)$尾部。

Conclusion: 该框架统一了动态遗憾分析、分布鲁棒目标和KL正则化控制，提供了一种单一应力自适应更新方法，可扩展到二阶更新、老虎机反馈、异常值处理等场景。

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [33] [FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain](https://arxiv.org/abs/2510.15232)
*Tiansheng Hu,Tongyan Hu,Liuyang Bai,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.LG

TL;DR: FinTrust是一个专门评估金融领域LLM可信赖性的基准测试，涵盖多个对齐维度的细粒度任务，发现专有模型在安全性方面表现更好，开源模型在特定领域有优势，但所有模型在法律意识方面都存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 由于金融领域的高风险和高利害关系特性，在真实金融应用中部署LLM面临挑战，需要评估其可信赖性。

Method: 开发FinTrust基准测试，基于实际情境设计广泛的对齐问题，为每个可信赖性评估维度创建细粒度任务，评估了11个LLM。

Result: 专有模型如o4-mini在大多数任务中表现更好，开源模型如DeepSeek-V3在特定领域如行业级公平性有优势，所有模型在受托责任对齐和披露等挑战性任务上表现不佳。

Conclusion: FinTrust可作为金融领域LLM可信赖性评估的有价值基准，当前LLM在法律意识方面存在显著差距，需要进一步改进。

Abstract: Recent LLMs have demonstrated promising ability in solving finance related
problems. However, applying LLMs in real-world finance application remains
challenging due to its high risk and high stakes property. This paper
introduces FinTrust, a comprehensive benchmark specifically designed for
evaluating the trustworthiness of LLMs in finance applications. Our benchmark
focuses on a wide range of alignment issues based on practical context and
features fine-grained tasks for each dimension of trustworthiness evaluation.
We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini
outperforms in most tasks such as safety while open-source models like
DeepSeek-V3 have advantage in specific areas like industry-level fairness. For
challenging task like fiduciary alignment and disclosure, all LLMs fall short,
showing a significant gap in legal awareness. We believe that FinTrust can be a
valuable benchmark for LLMs' trustworthiness evaluation in finance domain.

</details>


### [34] [Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction](https://arxiv.org/abs/2510.15233)
*Amitesh Badkul,Lei Xie*

Main category: cs.LG

TL;DR: 提出TESSERA方法，在蛋白质-配体亲和力预测中提供可靠的、信息丰富的个体不确定性量化，在分布偏移下仍能保持接近名义覆盖率和最佳覆盖宽度权衡。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习社区缺乏可靠、信息丰富且个体化的不确定性量化方法，这阻碍了AI/ML在风险敏感领域的有效应用，特别是在药物发现中的蛋白质-配体亲和力预测面临异质噪声、不平衡化学空间和分布偏移等挑战。

Method: TESSERA方法结合专家混合模型的多样性和保形校准，提供具有可靠覆盖保证的个体不确定性，生成信息丰富且自适应的预测区间宽度来跟踪绝对误差。

Result: 在独立同分布和基于支架的分布外分割下，TESSERA达到接近名义覆盖率和最佳覆盖宽度权衡（CWC），同时保持竞争性的适应性（最低AUSE），大小分层覆盖证实区间大小合适。

Conclusion: 通过统一专家混合模型多样性与保形校准，TESSERA提供可信、紧凑且自适应的不确定性，适合药物发现管道中的选择性预测和下游决策制定。

Abstract: Reliable, informative, and individual uncertainty quantification (UQ) remains
missing in current ML community. This hinders the effective application of
AI/ML to risk-sensitive domains. Most methods either fail to provide coverage
on new data, inflate intervals so broadly that they are not actionable, or
assign uncertainties that do not track actual error, especially under a
distribution shift. In high-stakes drug discovery, protein-ligand affinity
(PLI) prediction is especially challenging as assay noise is heterogeneous,
chemical space is imbalanced and large, and practical evaluations routinely
involve distribution shift. In this work, we introduce a novel uncertainty
quantification method, Trustworthy Expert Split-conformal with Scaled
Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides
per-sample uncertainty with reliable coverage guarantee, informative and
adaptive prediction interval widths that track the absolute error. We evaluate
on protein-ligand binding affinity prediction under both independent and
identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)
splits, comparing against strong UQ baselines. TESSERA attains near-nominal
coverage and the best coverage-width trade-off as measured by the
Coverage-Width Criterion (CWC), while maintaining competitive adaptivity
(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage
(SSC) further confirms that intervals are right-sized, indicating width
increases when data are scarce or noisy, and remain tight when predictions are
reliable. By unifying Mixture of Expert (MoE) diversity with conformal
calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties
that are well-suited to selective prediction and downstream decision-making in
the drug-discovery pipeline and other applications.

</details>


### [35] [Dual-Weighted Reinforcement Learning for Generative Preference Modeling](https://arxiv.org/abs/2510.15242)
*Shengyu Feng,Yun He,Shuang Ma,Beibin Li,Yuanhao Xiong,Vincent Li,Karishma Mandyam,Julian Katz-Samuels,Shengjie Bi,Licheng Yu,Hejia Zhang,Karthik Abinav Sankararaman,Han Fang,Riham Mansour,Yiming Yang,Manaal Faruqui*

Main category: cs.LG

TL;DR: DWRL是一个新的偏好建模框架，通过双权重强化学习目标将思维链推理与Bradley-Terry模型结合，在非可验证任务上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 强化学习在可验证任务上扩展思维链推理有效，但在更一般的非可验证偏好任务上仍然具有挑战性且研究不足。

Method: 提出双权重强化学习(DWRL)，通过实例级错位权重和组级条件偏好分数来近似Bradley-Terry模型的最大似然目标，训练生成式偏好模型。

Result: 在多个基准和模型规模上，DWRL持续优于生成式偏好模型基线和标量模型，同时产生连贯、可解释的思维。

Conclusion: DWRL作为一个通用框架，将推理增强的偏好学习扩展到可验证任务之外。

Abstract: Reinforcement learning (RL) has recently proven effective at scaling
chain-of-thought (CoT) reasoning in large language models on tasks with
verifiable answers. However, extending RL to more general non-verifiable tasks,
typically in the format of human preference pairs, remains both challenging and
underexplored. In this work, we propose Dual-Weighted Reinforcement Learning
(DWRL), a new framework for preference modeling that integrates CoT reasoning
with the Bradley-Terry (BT) model via a dual-weighted RL objective that
preserves preference-modeling inductive bias. DWRL approximates the
maximum-likelihood objective of the BT model with two complementary weights: an
instance-wise misalignment weight, which emphasizes under-trained pairs
misaligned with human preference, and a group-wise (self-normalized)
conditional preference score, which promotes promising thoughts. In this paper,
we apply DWRL to preference modeling by training generative preference models
(GPMs) to first generate a thought and then predict the human preference score.
Across multiple benchmarks and model scales (Llama3 and Qwen2.5), DWRL
consistently outperforms both GPM baselines and scalar models, while producing
coherent, interpretable thoughts. In summary, our results position DWRL as a
general framework for reasoning-enhanced preference learning beyond verifiable
tasks.

</details>


### [36] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: 使用Transformer框架预测候鸟迁徙轨迹终点的疾病风险，整合GPS追踪、疫情记录和地理空间数据，在测试集上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 准确预测禽类疾病暴发对野生动物保护和公共卫生至关重要，需要开发有效的早期预警系统。

Method: 整合多源数据集（GPS追踪、疫情记录、地理空间数据），使用H3地理空间编码处理坐标，通过Transformer模型学习候鸟移动序列的时空依赖性来估计终点疾病风险。

Result: 在测试集上表现优异：准确率0.9821、AUC 0.9803、平均精度0.9299、F1分数0.8836。

Conclusion: Transformer架构在禽类疾病监测早期预警系统中具有巨大潜力，能够支持及时的干预和预防策略。

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [37] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero通过将零样本提示优化构建为鲁棒贝叶斯优化，解决了现有提示搜索方法在分布偏移和对抗评估下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示搜索方法（如InstructZero）在单一评估分布下优化期望性能，导致在分布偏移和对抗评估时性能下降，提示难以迁移。

Method: 使用f-散度球在评估分布周围定义模糊集，通过鲁棒获取规则最大化最坏情况期望效用，同时保持贝叶斯搜索的查询效率。

Result: 在形式化重写任务中准确率从61.3%提升至85-90%，代码调试在域偏移下提升约25个百分点，稳定任务保持96%以上准确率。

Conclusion: DRO-InstructZero将分布鲁棒优化与提示学习相结合，为现实世界不确定性下的可靠、可迁移提示对齐提供了即插即用的通用方法。

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [38] [Robust Layerwise Scaling Rules by Proper Weight Decay Tuning](https://arxiv.org/abs/2510.15262)
*Zhiyuan Fan,Yifeng Liu,Qingyue Zhao,Angela Yuan,Quanquan Gu*

Main category: cs.LG

TL;DR: 本文提出了AdamW优化器的权重衰减缩放规则，解决了现代尺度不变架构中μP方法在训练稳定状态下学习率传递失效的问题，实现了跨宽度零-shot超参数迁移。


<details>
  <summary>Details</summary>
Motivation: 现代尺度不变架构训练快速进入优化器主导的稳定状态，归一化层造成反向尺度敏感性，使得有效学习率变得宽度依赖，从而破坏了μP方法的学习率传递能力。

Method: 引入AdamW的权重衰减缩放规则，通过观察矩阵参数奇异值谱的缩放行为，提出经验性的权重衰减缩放规则λ₂∝√d，结合向量参数在η₁=Θ_d(1)和λ₁=0的训练，保持子层增益的宽度不变性。

Result: 在LLaMA风格Transformer和最小合成设置中验证了该规则的有效性，实现了从代理宽度到目标宽度的学习率和权重衰减的零-shot迁移，消除了每个宽度的超参数搜索需求。

Conclusion: 该研究通过显式控制优化器设置的稳定状态尺度，将μP方法扩展到近初始化阶段之外，为AdamW下的宽度鲁棒超参数迁移提供了实用方案。

Abstract: Empirical scaling laws prescribe how to allocate parameters, data, and
compute, while maximal-update parameterization ($\mu$P) enables learning-rate
transfer across widths by equalizing early-time update magnitudes. However, in
modern scale-invariant architectures, training quickly enters an
optimizer-governed steady state where normalization layers create backward
scale sensitivity and the effective learning rate becomes width dependent,
degrading $\mu$P transfer. We address this by introducing a weight-decay
scaling rule for AdamW that preserves sublayer gain across widths. Empirically,
the singular-value spectrum of each matrix parameter scales in norm as
$\sqrt{\eta/\lambda}$ with an approximately invariant shape; under width
scaling $d$, we observe that the top singular value scales approximately as
$\sqrt{\eta/\lambda}\cdot d^{0.75}$. Combining this observation with the $\mu$P
learning-rate rule $\eta_2\propto d^{-1}$ for matrix-like parameters implies an
empirical weight-decay scaling rule $\lambda_2\propto \sqrt{d}$ that
approximately keeps sublayer gains width invariant. Together with vector-like
parameters trained at $\eta_1=\Theta_d(1)$ and $\lambda_1=0$, this yields
\emph{zero-shot} transfer of both learning rate and weight decay from proxy to
target widths, removing per-width sweeps. We validate the rule on LLaMA-style
Transformers and in a minimal synthetic setting, and we provide a simple
diagnostic, matching top singular values, to check sublayer-gain invariance.
Our results extend $\mu$P beyond the near-init regime by explicitly controlling
steady-state scales set by the optimizer, offering a practical recipe for
width-robust hyperparameter transfer under AdamW.

</details>


### [39] [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
*Emam Hossain,Muhammad Hasan Ferdous,Devon Dunmire,Aneesh Subramanian,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出RIC-TSC框架，将因果发现融入时间序列分类，在格陵兰冰上湖演化预测中实现比相关性基线高12.59%的准确率，提升分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前地球观测中的时空模型过度依赖相关性特征，在异质域间迁移能力差。因果建模能发现稳定不变的关系，提高分布偏移下的鲁棒性和泛化性。

Method: 使用多模态卫星数据，通过J-PCMCI+进行区域特定和不变的因果发现，将验证的预测因子及其时间滞后输入轻量级分类器。

Result: 在两个对比融化季节的1000个标记湖泊上，因果模型在分布外评估中比相关性基线准确率提高达12.59%。

Conclusion: 因果发现不仅是特征选择手段，更是构建可泛化和机制基础的地球表面动态过程模型的途径。

Abstract: Causal modeling offers a principled foundation for uncovering stable,
invariant relationships in time-series data, thereby improving robustness and
generalization under distribution shifts. Yet its potential is underutilized in
spatiotemporal Earth observation, where models often depend on purely
correlational features that fail to transfer across heterogeneous domains. We
propose RIC-TSC, a regionally-informed causal time-series classification
framework that embeds lag-aware causal discovery directly into sequence
modeling, enabling both predictive accuracy and scientific interpretability.
Using multi-modal satellite and reanalysis data-including Sentinel-1 microwave
backscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA
meteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify
region-specific and invariant predictors of supraglacial lake evolution in
Greenland. Causal graphs are estimated globally and per basin, with validated
predictors and their time lags supplied to lightweight classifiers. On a
balanced benchmark of 1000 manually labeled lakes from two contrasting melt
seasons (2018-2019), causal models achieve up to 12.59% higher accuracy than
correlation-based baselines under out-of-distribution evaluation. These results
show that causal discovery is not only a means of feature selection but also a
pathway to generalizable and mechanistically grounded models of dynamic Earth
surface processes.

</details>


### [40] [Semi-Supervised Regression with Heteroscedastic Pseudo-Labels](https://arxiv.org/abs/2510.15266)
*Xueqing Sun,Renzhen Wang,Quanziang Wang,Yichen Wu,Xixi Jia,Deyu Meng*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的伪标签框架，通过双层优化动态调整伪标签影响，解决了半监督回归中伪标签可靠性评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 半监督回归中的伪标签方法相对未被充分探索，由于连续输出和异方差噪声的存在，难以评估伪标签可靠性，导致错误累积和过拟合问题。

Method: 采用不确定性感知的伪标签框架，从双层优化角度动态调整伪标签影响，联合最小化所有数据的经验风险和优化不确定性估计以增强泛化能力。

Result: 在多个基准半监督回归数据集上的实验验证了该方法的有效性，相比现有方法展现出更优越的鲁棒性和性能表现。

Conclusion: 所提出的不确定性感知伪标签框架能有效缓解不可靠伪标签的影响，为半监督回归提供了有效的解决方案。

Abstract: Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet
its application to semi-supervised regression (SSR) remains relatively
under-explored. Unlike classification, where pseudo-labels are discrete and
confidence-based filtering is effective, SSR involves continuous outputs with
heteroscedastic noise, making it challenging to assess pseudo-label
reliability. As a result, naive pseudo-labeling can lead to error accumulation
and overfitting to incorrect labels. To address this, we propose an
uncertainty-aware pseudo-labeling framework that dynamically adjusts
pseudo-label influence from a bi-level optimization perspective. By jointly
minimizing empirical risk over all data and optimizing uncertainty estimates to
enhance generalization on labeled data, our method effectively mitigates the
impact of unreliable pseudo-labels. We provide theoretical insights and
extensive experiments to validate our approach across various benchmark SSR
datasets, and the results demonstrate superior robustness and performance
compared to existing methods. Our code is available at
https://github.com/sxq/Heteroscedastic-Pseudo-Labels.

</details>


### [41] [Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition](https://arxiv.org/abs/2510.15280)
*Fan Liu,Jindong Han,Tengfei Lyu,Weijia Zhang,Zhe-Rui Yang,Lu Dai,Cancheng Liu,Hao Liu*

Main category: cs.LG

TL;DR: 基础模型正在推动科学发现范式的转变，从增强现有工作流到人机协作，最终实现自主科学发现。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型不仅仅是增强现有科学方法，而是在重新定义科学实践方式，分析其对科学范式的根本性影响。

Method: 提出三阶段框架：元科学整合、人机混合共创、自主科学发现，并基于此框架回顾当前应用和新兴能力。

Result: 识别了基础模型在科学发现中的风险，并指出了未来发展方向，为科学界理解其变革性作用提供支持。

Conclusion: 基础模型正在催化科学范式的转变，需要科学界共同反思科学发现的未来，并应对相关风险和挑战。

Abstract: Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the
landscape of scientific research. Beyond accelerating tasks such as hypothesis
generation, experimental design, and result interpretation, they prompt a more
fundamental question: Are FMs merely enhancing existing scientific
methodologies, or are they redefining the way science is conducted? In this
paper, we argue that FMs are catalyzing a transition toward a new scientific
paradigm. We introduce a three-stage framework to describe this evolution: (1)
Meta-Scientific Integration, where FMs enhance workflows within traditional
paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active
collaborators in problem formulation, reasoning, and discovery; and (3)
Autonomous Scientific Discovery, where FMs operate as independent agents
capable of generating new scientific knowledge with minimal human intervention.
Through this lens, we review current applications and emerging capabilities of
FMs across existing scientific paradigms. We further identify risks and future
directions for FM-enabled scientific discovery. This position paper aims to
support the scientific community in understanding the transformative role of
FMs and to foster reflection on the future of scientific discovery. Our project
is available at
https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.

</details>


### [42] [Small Ensemble-based Data Assimilation: A Machine Learning-Enhanced Data Assimilation Method with Limited Ensemble Size](https://arxiv.org/abs/2510.15284)
*Zhilin Li,Zhou Yao,Xianglong Li,Zeng Liu,Zhaokuan Lu,Shanlin Xu,Seungnam Kim,Guangyao Wang*

Main category: cs.LG

TL;DR: 提出了一种结合集成卡尔曼滤波和全连接神经网络的机器学习数据同化方法，使用较小集成规模获得初步分析状态，再用神经网络预测修正项来提升精度。


<details>
  <summary>Details</summary>
Motivation: 传统集成数据同化方法在分析精度和计算效率之间存在权衡，大集成规模提高精度但增加计算成本。

Method: 使用较小集成规模的EnKF生成初步分析状态，然后通过全连接神经网络学习并预测修正项来弥补集成规模不足导致的性能下降。

Result: 在Lorenz系统和非线性海洋波场模拟实验中，新方法在相同集成规模下比传统EnKF精度更高，且额外计算成本可忽略。

Conclusion: EnKF-FCNN方法能够有效提升数据同化精度，适应性强，可与不同模型和集成数据同化方法结合使用。

Abstract: Ensemble-based data assimilation (DA) methods have become increasingly
popular due to their inherent ability to address nonlinear dynamic problems.
However, these methods often face a trade-off between analysis accuracy and
computational efficiency, as larger ensemble sizes required for higher accuracy
also lead to greater computational cost. In this study, we propose a novel
machine learning-based data assimilation approach that combines the traditional
ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN).
Specifically, our method uses a relatively small ensemble size to generate
preliminary yet suboptimal analysis states via EnKF. A FCNN is then employed to
learn and predict correction terms for these states, thereby mitigating the
performance degradation induced by the limited ensemble size. We evaluate the
performance of our proposed EnKF-FCNN method through numerical experiments
involving Lorenz systems and nonlinear ocean wave field simulations. The
results consistently demonstrate that the new method achieves higher accuracy
than traditional EnKF with the same ensemble size, while incurring negligible
additional computational cost. Moreover, the EnKF-FCNN method is adaptable to
diverse applications through coupling with different models and the use of
alternative ensemble-based DA methods.

</details>


### [43] [Identifying internal patterns in (1+1)-dimensional directed percolation using neural networks](https://arxiv.org/abs/2510.15294)
*Danil Parkhomenko,Pavel Ovchinnikov,Konstantin Soldatov,Vitalii Kapitan,Gennady Y. Chitov*

Main category: cs.LG

TL;DR: 提出基于CNN、TCN和GRU组合的神经网络方法，用于自动检测(1+1)维复制过程中的相变和分类隐藏渗流模式。


<details>
  <summary>Details</summary>
Motivation: 开发能够直接从原始配置数据中自动检测相变和分类隐藏渗流模式的深度学习方法，避免手动特征提取。

Method: 结合CNN、TCN和GRU网络，直接在原始配置数据上进行训练，无需手动特征提取。

Result: 网络成功重现了相图并为配置分配了相标签，证明深度架构能够从数值实验原始数据中提取层次结构。

Conclusion: 深度神经网络架构能够有效从原始数值实验数据中提取层次结构信息，实现相变的自动检测和隐藏渗流模式的分类。

Abstract: In this paper we present a neural network-based method for the automatic
detection of phase transitions and classification of hidden percolation
patterns in a (1+1)-dimensional replication process. The proposed network model
is based on the combination of CNN, TCN and GRU networks, which are trained
directly on raw configurations without any manual feature extraction. The
network reproduces the phase diagram and assigns phase labels to
configurations. It shows that deep architectures are capable of extracting
hierarchical structures from the raw data of numerical experiments.

</details>


### [44] [DFCA: Decentralized Federated Clustering Algorithm](https://arxiv.org/abs/2510.15300)
*Jonas Kirch,Sebastian Becker,Tiago Koketsu Rodrigues,Stefan Harmeling*

Main category: cs.LG

TL;DR: DFCA是一种完全去中心化的聚类联邦学习算法，无需中央服务器协调，通过顺序运行平均聚合邻居模型，在稀疏连接下仍能保持与集中式IFCA相当的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有聚类联邦学习方法依赖中央服务器协调模型更新，存在瓶颈和单点故障问题，限制了在现实去中心化学习场景中的应用。

Method: DFCA使用顺序运行平均来聚合邻居模型作为更新到达，提供通信高效的批量聚合替代方案，同时保持聚类性能。

Result: 实验表明DFCA在各种数据集上优于其他去中心化算法，性能与集中式IFCA相当，即使在稀疏连接下也表现出鲁棒性。

Conclusion: DFCA展示了在动态现实世界去中心化网络中应用的鲁棒性和实用性。

Abstract: Clustered Federated Learning has emerged as an effective approach for
handling heterogeneous data across clients by partitioning them into clusters
with similar or identical data distributions. However, most existing methods,
including the Iterative Federated Clustering Algorithm (IFCA), rely on a
central server to coordinate model updates, which creates a bottleneck and a
single point of failure, limiting their applicability in more realistic
decentralized learning settings. In this work, we introduce DFCA, a fully
decentralized clustered FL algorithm that enables clients to collaboratively
train cluster-specific models without central coordination. DFCA uses a
sequential running average to aggregate models from neighbors as updates
arrive, providing a communication-efficient alternative to batch aggregation
while maintaining clustering performance. Our experiments on various datasets
demonstrate that DFCA outperforms other decentralized algorithms and performs
comparably to centralized IFCA, even under sparse connectivity, highlighting
its robustness and practicality for dynamic real-world decentralized networks.

</details>


### [45] [On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions](https://arxiv.org/abs/2510.15327)
*Zailin Ma,Jiansheng Yang,Yaodong Yang*

Main category: cs.LG

TL;DR: 本文研究了可学习激活函数的随机特征模型(RFLAF)的泛化性质，通过数据依赖的采样方案显著减少了学习所需特征数量的理论界，并提出了加权采样算法。


<details>
  <summary>Details</summary>
Motivation: 研究RFLAF模型的泛化性质，旨在通过改进采样方案来减少学习所需的特征数量，提高模型效率。

Method: 采用数据依赖的采样方案生成特征，包括普通采样方案和杠杆加权方案，并提出近似核查找算法来学习加权RFLAF。

Result: 通过加权采样，MSE损失情况下的特征数界从Ω(1/ε²)改进到Ω̃((1/ε)^{1/t})，甚至当Gram矩阵有限秩时达到Ω(1)；Lipschitz损失情况从Ω(1/ε²)改进到Ω̃((1/ε²)^{1/t})。

Conclusion: 加权RFLAF在显著减少特征数量的情况下仍能达到相同性能，验证了理论的有效性和该方法的实用性。

Abstract: This paper studies the generalization properties of a recently proposed
kernel method, the Random Feature models with Learnable Activation Functions
(RFLAF). By applying a data-dependent sampling scheme for generating features,
we provide by far the sharpest bounds on the required number of features for
learning RFLAF in both the regression and classification tasks. We provide a
unified theorem that describes the complexity of the feature number $s$, and
discuss the results for the plain sampling scheme and the data-dependent
leverage weighted scheme. Through weighted sampling, the bound on $s$ in the
MSE loss case is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon)^{1/t})$ in general $(t\geq 1)$, and even to
$\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss
case, the bound is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also
propose an algorithm to find an approximate kernel and then apply the leverage
weighted sampling. Empirical results show that the weighted RFLAF achieves the
same performances with a significantly fewer number of features compared to the
plainly sampled RFLAF, validating our theories and the effectiveness of this
method.

</details>


### [46] [Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks](https://arxiv.org/abs/2510.15333)
*Yuyuan Feng,Bin Ma,Enyan Dai*

Main category: cs.LG

TL;DR: 提出基于Mixture of Experts架构的统一防御框架，同时抵御图神经网络的后门攻击、边操纵和节点注入攻击，通过逻辑多样性损失和鲁棒性感知路由器实现多威胁防护。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络防御方法通常只针对单一攻击类型，缺乏统一的多威胁防御方案，需要解决这一局限性。

Method: 使用MoE架构，设计MI-based逻辑多样性损失使专家关注不同邻域结构，并引入鲁棒性感知路由器识别扰动模式并自适应路由节点到相应鲁棒专家。

Result: 在各种对抗设置下的广泛实验表明，该方法在多种图对抗攻击下始终实现优越的鲁棒性。

Conclusion: 提出的统一防御框架能有效同时防御多种图对抗攻击，具有可扩展性和实用性。

Abstract: Extensive research has highlighted the vulnerability of graph neural networks
(GNNs) to adversarial attacks, including manipulation, node injection, and the
recently emerging threat of backdoor attacks. However, existing defenses
typically focus on a single type of attack, lacking a unified approach to
simultaneously defend against multiple threats. In this work, we leverage the
flexibility of the Mixture of Experts (MoE) architecture to design a scalable
and unified framework for defending against backdoor, edge manipulation, and
node injection attacks. Specifically, we propose an MI-based logic diversity
loss to encourage individual experts to focus on distinct neighborhood
structures in their decision processes, thus ensuring a sufficient subset of
experts remains unaffected under perturbations in local structures. Moreover,
we introduce a robustness-aware router that identifies perturbation patterns
and adaptively routes perturbed nodes to corresponding robust experts.
Extensive experiments conducted under various adversarial settings demonstrate
that our method consistently achieves superior robustness against multiple
graph adversarial attacks.

</details>


### [47] [Sequence Modeling with Spectral Mean Flows](https://arxiv.org/abs/2510.15366)
*Jinwoo Kim,Max Beier,Petar Bevanda,Nayun Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 提出了一种基于算子理论的序列建模新方法，通过谱分解和MMD梯度流在希尔伯特空间中嵌入序列分布，避免了传统的随机递归，实现了可扩展的生成建模。


<details>
  <summary>Details</summary>
Motivation: 传统序列建模方法在处理高度非线性和概率性状态动态时面临挑战，而算子理论提供了将动态视为希尔伯特空间线性映射的视角，但目前被忽视。

Method: 采用算子理论视角，将完整序列分布嵌入到乘积希尔伯特空间中作为张量；提出谱均值流算法，包括基于谱分解的可扩展张量网络架构，以及扩展到时间相关希尔伯特空间的MMD梯度流。

Result: 在多个时间序列建模数据集上展示了有竞争力的结果，代码已开源。

Conclusion: 该工作将算子理论与现代深度学习相结合，为序列建模提供了新的理论框架和实用算法，在可扩展性和采样效率方面具有优势。

Abstract: A key question in sequence modeling with neural networks is how to represent
and learn highly nonlinear and probabilistic state dynamics. Operator theory
views such dynamics as linear maps on Hilbert spaces containing mean embedding
vectors of distributions, offering an appealing but currently overlooked
perspective. We propose a new approach to sequence modeling based on an
operator-theoretic view of a hidden Markov model (HMM). Instead of
materializing stochastic recurrence, we embed the full sequence distribution as
a tensor in the product Hilbert space. A generative process is then defined as
maximum mean discrepancy (MMD) gradient flow in the space of sequences. To
overcome challenges with large tensors and slow sampling convergence, we
introduce spectral mean flows, a novel tractable algorithm integrating two core
concepts. First, we propose a new neural architecture by leveraging spectral
decomposition of linear operators to derive a scalable tensor network
decomposition of sequence mean embeddings. Second, we extend MMD gradient flows
to time-dependent Hilbert spaces and connect them to flow matching via the
continuity equation, enabling simulation-free learning and faster sampling. We
demonstrate competitive results on a range of time-series modeling datasets.
Code is available at https://github.com/jw9730/spectral-mean-flow.

</details>


### [48] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: BREEZE是一个改进的零样本强化学习框架，通过行为正则化、任务条件扩散模型和注意力架构，解决了现有方法在表达性和外推误差方面的问题，提升了学习稳定性和策略提取能力。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本强化学习方法（如FB表示）存在表达性不足和离线学习中外推误差导致表示偏差的问题，这影响了性能表现。

Method: BREEZE引入行为正则化将策略优化转化为稳定的样本内学习范式，使用任务条件扩散模型提取策略以生成高质量多模态动作分布，并采用基于注意力的表达性架构进行表示建模。

Result: 在ExORL和D4RL Kitchen数据集上的实验表明，BREEZE实现了最佳或接近最佳的性能，同时展现出比现有离线零样本RL方法更优越的鲁棒性。

Conclusion: BREEZE通过同时增强学习稳定性、策略提取能力和表示学习质量，有效提升了零样本强化学习的性能表现。

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [49] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: SWFP框架通过离散化流匹配推理过程，将其与最优传输的JKO原理对齐，实现了预训练流策略的稳定在线微调。


<details>
  <summary>Details</summary>
Motivation: 行为克隆的流/扩散策略虽然擅长从演示中学习复杂技能，但对分布偏移很敏感，标准RL方法难以微调这些模型。

Method: 将全局流分解为一系列小增量变换，每个步骤对应JKO更新，通过熵正则化确保稳定在线适应。

Result: 实验证明SWFP在多个机器人控制基准测试中展现出增强的稳定性、效率和优越的适应性能。

Conclusion: SWFP提供了一种高效算法，通过级联小流块微调预训练流，具有训练简单快速、计算内存成本低和可证明稳定性等优势。

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [50] [Geometric Mixture Models for Electrolyte Conductivity Prediction](https://arxiv.org/abs/2510.15403)
*Anyi Li,Jiacheng Cen,Songyou Li,Mingze Li,Yang Yu,Wenbing Huang*

Main category: cs.LG

TL;DR: GeoMix是一个几何感知框架，通过保持Set-SE(3)等变性来准确预测电解质系统的离子电导率，在CALiSol和DiffMix数据集上优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前电解质研究面临两个基本挑战：缺乏高质量标准化基准，以及对混合物系统中几何结构和分子间相互作用的建模不足。

Method: 首先重组并增强CALiSol和DiffMix电解质数据集，加入分子几何图表示；然后提出GeoMix框架，其核心是几何交互网络(GIN)，这是一个专门为分子间几何消息传递设计的等变模块。

Result: 综合实验表明，GeoMix在两个数据集上始终优于多种基线方法（包括MLPs、GNNs和几何GNNs），验证了跨分子几何交互和等变消息传递对准确性质预测的重要性。

Conclusion: 这项工作不仅为电解质研究建立了新基准，还提供了一个通用的几何学习框架，可推进能源材料、药物开发等领域中混合物系统的建模。

Abstract: Accurate prediction of ionic conductivity in electrolyte systems is crucial
for advancing numerous scientific and technological applications. While
significant progress has been made, current research faces two fundamental
challenges: (1) the lack of high-quality standardized benchmarks, and (2)
inadequate modeling of geometric structure and intermolecular interactions in
mixture systems. To address these limitations, we first reorganize and enhance
the CALiSol and DiffMix electrolyte datasets by incorporating geometric graph
representations of molecules. We then propose GeoMix, a novel geometry-aware
framework that preserves Set-SE(3) equivariance-an essential but challenging
property for mixture systems. At the heart of GeoMix lies the Geometric
Interaction Network (GIN), an equivariant module specifically designed for
intermolecular geometric message passing. Comprehensive experiments demonstrate
that GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,
and geometric GNNs) across both datasets, validating the importance of
cross-molecular geometric interactions and equivariant message passing for
accurate property prediction. This work not only establishes new benchmarks for
electrolyte research but also provides a general geometric learning framework
that advances modeling of mixture systems in energy materials, pharmaceutical
development, and beyond.

</details>


### [51] [Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing](https://arxiv.org/abs/2510.15404)
*Christopher Salazar,Krithika Manohar,Ashis G. Banerjee*

Main category: cs.LG

TL;DR: 提出了WORK-DMD方法，结合随机傅里叶特征和在线动态模态分解，用于实时流数据预测，在计算受限环境下实现高精度、自适应性和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决实时流数据预测中的关键挑战：处理非平稳动态、在严格计算限制下运行、快速适应而不发生灾难性遗忘。现有方法在准确性、适应性和效率之间存在权衡。

Method: 使用随机傅里叶特征进行显式特征映射，结合在线动态模态分解捕获非线性动态。采用Sherman-Morrison更新和滚动窗口机制，仅使用当前数据进行连续适应。

Result: 在多个领域的基准数据集上，WORK-DMD比几种最先进的在线预测方法获得更高精度，仅需单次数据遍历，在短期预测中表现尤其出色。

Conclusion: 将核评估与自适应矩阵更新相结合，在最小数据需求下实现强大预测性能，为流式预测应用提供了深度学习的实用替代方案。

Abstract: Real-time forecasting from streaming data poses critical challenges: handling
non-stationary dynamics, operating under strict computational limits, and
adapting rapidly without catastrophic forgetting. However, many existing
approaches face trade-offs between accuracy, adaptability, and efficiency,
particularly when deployed in constrained computing environments. We introduce
WORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method
that combines Random Fourier Features with online Dynamic Mode Decomposition to
capture nonlinear dynamics through explicit feature mapping, while preserving
fixed computational cost and competitive predictive accuracy across evolving
data. WORK-DMD employs Sherman-Morrison updates within rolling windows,
enabling continuous adaptation to evolving dynamics from only current data,
eliminating the need for lengthy training or large storage requirements for
historical data. Experiments on benchmark datasets across several domains show
that WORK-DMD achieves higher accuracy than several state-of-the-art online
forecasting methods, while requiring only a single pass through the data and
demonstrating particularly strong performance in short-term forecasting. Our
results show that combining kernel evaluations with adaptive matrix updates
achieves strong predictive performance with minimal data requirements. This
sample efficiency offers a practical alternative to deep learning for streaming
forecasting applications.

</details>


### [52] [ParaFormer: Shallow Parallel Transformers with Progressive Approximation](https://arxiv.org/abs/2510.15425)
*Wei Wang,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: ParaFormer是一种浅层Transformer架构，通过并行分支结构实现真正的结构和计算并行，解决了深层模型训练时间长、推理延迟高的问题。


<details>
  <summary>Details</summary>
Motivation: 解决深层Transformer模型带来的训练时间长、推理延迟高以及在资源受限设备上不实用的问题。

Method: 将标准Transformer建模为闭式函数逼近器，通过并行分支组织层，算法强制层间协作，实现渐进逼近。

Result: ParaFormer在性能上超越标准Transformer如ViT，支持15.07倍模型压缩，在多GPU部署中比FairScale快3.30倍。

Conclusion: 基于通用逼近定理的闭式Transformer公式不仅解释了"深度信念"，还为设计高效Transformer架构开辟了新途径。

Abstract: The widespread 'deeper is better' philosophy has driven the creation of
architectures like ResNet and Transformer, which achieve high performance by
stacking numerous layers. However, increasing model depth comes with challenges
such as longer training times, higher inference latency, and impracticality on
resource-constrained devices. To address these issues, we propose ParaFormer, a
shallow Transformer architecture designed for true parallelism in both
structure and computation. By formulating standard Transformers as function
approximators in closed-form, our theoretical analysis shows that their
performance relies on inter-layer collaboration for progressive approximation,
rather than depth itself. While deep Transformers enforce this collaboration
through sequential designs, we demonstrate that such collaboration is not
inherently tied to sequential structures. ParaFormer removes the sequential
constraint by organizing layers into parallel branches, enforcing inter-layer
collaboration algorithmically. Specifically, we implement progressive
approximation, ensuring that each new branch further reduces the loss from
preceding branches, enabling faster convergence. Extensive experiments validate
ParaFormer's effectiveness, outperforming standard Transformers like ViT.
Moreover, ParaFormer supports up to 15.07x model compression and facilitates
model expansion for adaptive continuous learning. Experimental results on
multi-GPU deployment demonstrate that ParaFormer is 3.30x faster than widely
used parallelism solutions such as FairScale. These advancements stem from our
closed-form formulation of Transformers based on the Universal Approximation
Theorem, which not only explains the ``depth belief'' but also opens new
avenues for designing efficient Transformer architectures. Source code:
https://(open-upon-acceptance)

</details>


### [53] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: 该论文研究了如何设计安全、样本高效且鲁棒的强化学习方法，主要关注排序推荐系统和文本到图像扩散模型两个应用领域，提出了多种理论保证和算法改进。


<details>
  <summary>Details</summary>
Motivation: 为了解决强化学习在真实世界应用中的安全性、样本效率和鲁棒性问题，特别是在排序推荐和生成模型等关键领域，需要开发具有理论保证的算法。

Method: 采用上下文多臂老虎机框架，开发了曝光泛化边界理论、双重鲁棒估计器、基线校正框架，以及结合PPO和REINFORCE的LOOP算法。

Result: 提出的方法能够保证不劣于记录策略的性能，即使在稀疏反馈下也能安全部署；优化了离策略学习可靠性；在文本到图像生成中实现了PPO级别的样本效率同时更好地对齐文本属性。

Conclusion: 通过理论分析和算法创新，该研究为强化学习在排序推荐和生成模型等领域的实际应用提供了安全、高效且鲁棒的解决方案。

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [54] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 本文提出了首个基于置信度估计的理论框架来分析采样型测试时扩展方法，揭示了自一致性和困惑度方法的局限性，并提出了混合方法RPC，通过困惑度一致性和推理剪枝来降低推理错误。


<details>
  <summary>Details</summary>
Motivation: 尽管采样型测试时扩展方法在实践中取得了成功，但其理论基础仍然不足。本文旨在填补这一空白，为这类方法提供理论分析框架。

Method: 提出了RPC混合方法，包含两个关键组件：困惑度一致性（结合自一致性和困惑度的优势）和推理剪枝（消除低概率推理路径）。

Result: 在七个基准数据集上的实验表明，RPC能够将估计误差收敛速度从线性提升到指数级，同时减少50%的采样成本，达到与自一致性相当的推理性能。

Conclusion: RPC方法在降低推理错误方面具有强大潜力，不仅提高了置信度可靠性，还显著降低了计算成本。

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [55] [Particle Dynamics for Latent-Variable Energy-Based Models](https://arxiv.org/abs/2510.15447)
*Shiqin Tang,Shuxin Zhuang,Rong Feng,Runsheng Yu,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: 提出一种基于鞍点优化的潜变量能量模型训练方法，将最大似然训练重新表述为潜变量和联合流形上的分布鞍点问题，使用耦合Wasserstein梯度流进行优化。


<details>
  <summary>Details</summary>
Motivation: 潜变量能量模型能够表达生成建模并捕捉隐藏结构，但传统训练方法存在局限性。本文旨在开发无需判别器或辅助网络的训练算法，提供理论保证和更紧的变分下界。

Method: 将最大似然训练重新表述为鞍点问题，使用耦合Wasserstein梯度流进行优化，交替进行过阻尼Langevin更新和随机参数上升，无需判别器或辅助网络。

Result: 在标准平滑性和耗散性假设下证明了算法的存在性和收敛性，在KL散度和Wasserstein-2距离上获得衰减率。在物理系统数值近似上表现优于可比方法。

Conclusion: 提出的鞍点视角为潜变量能量模型训练提供了理论保证和更紧的变分下界，在保持表达力的同时简化了训练过程，在数值实验中表现优异。

Abstract: Latent-variable energy-based models (LVEBMs) assign a single normalized
energy to joint pairs of observed data and latent variables, offering
expressive generative modeling while capturing hidden structure. We recast
maximum-likelihood training as a saddle problem over distributions on the
latent and joint manifolds and view the inner updates as coupled Wasserstein
gradient flows. The resulting algorithm alternates overdamped Langevin updates
for a joint negative pool and for conditional latent particles with stochastic
parameter ascent, requiring no discriminator or auxiliary networks. We prove
existence and convergence under standard smoothness and dissipativity
assumptions, with decay rates in KL divergence and Wasserstein-2 distance. The
saddle-point view further yields an ELBO strictly tighter than bounds obtained
with restricted amortized posteriors. Our method is evaluated on numerical
approximations of physical systems and performs competitively against
comparable approaches.

</details>


### [56] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 提出一种新方法，将时序逻辑因果图融入概率奖励机中，以加速策略学习并促进任务规范迁移到新环境。


<details>
  <summary>Details</summary>
Motivation: 强化学习在奖励反馈稀疏且依赖于环境中复杂事件序列的任务中表现不佳。概率奖励机虽然能捕捉奖励信号中的时间依赖性，但难以手动修改和设计，阻碍了利用高层因果知识和在不同因果结构环境中迁移奖励形式化。

Method: 将时序逻辑因果图形式表示的因果信息整合到奖励形式化中，利用有限状态结构加速学习。

Result: 提供了收敛到最优策略的理论结果，并通过实验证明了该方法的优势。

Conclusion: 该方法能有效加速策略学习，并帮助将任务规范迁移到具有不同因果结构的新环境中。

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [57] [Learning to Answer from Correct Demonstrations](https://arxiv.org/abs/2510.15464)
*Nirmit Joshi,Gene Li,Siddharth Bhandari,Shiva Prasad Kasiviswanathan,Cong Ma,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文研究了从正确演示中学习生成答案的问题，提出了一种基于低基数奖励模型假设的新方法，相比传统的最大似然估计方法具有更好的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设演示者属于低复杂度策略类，但在实际中奖励模型（指定哪些答案正确）属于低基数类可能是更弱的假设。当奖励模型为低基数时，最大似然估计方法可能失败。

Method: 将问题形式化为上下文多臂老虎机中的离线模仿学习，提出了一种基于低基数奖励模型假设的新方法，该方法只需要对数级别的样本复杂度。

Result: 证明了最大似然估计方法在低基数奖励模型情况下可能失败，而新方法在样本复杂度上具有对数级别的优势。

Conclusion: 当从正确演示中学习时，应该超越传统的似然最大化方法，特别是在奖励模型为低基数的情况下。

Abstract: We study the problem of learning to generate an answer (or completion) to a
question (or prompt), where there could be multiple correct answers, any one of
which is acceptable at test time. Learning is based on demonstrations of some
correct answer to each training question, as in Supervised Fine Tuning (SFT).
We formalize the problem as offline imitation learning in contextual bandits,
with demonstrations from some optimal policy, without explicitly observed
rewards. Prior work assumes that the demonstrator belongs to a low-complexity
policy class, which motivates maximum likelihood estimation (i.e., log-loss
minimization). In contrast, we propose relying only on the reward model
(specifying which answers are correct) being in a low-cardinality class, which
we argue is a weaker assumption. We show that likelihood maximization methods
can fail in this case, and instead devise an alternative novel approach that
learns with sample complexity logarithmic in the cardinality of the reward
class. Our work motivates looking beyond likelihood maximization when learning
from correct demonstrations.

</details>


### [58] [Adversary-Free Counterfactual Prediction via Information-Regularized Representations](https://arxiv.org/abs/2510.15479)
*Shiqin Tang,Rong Feng,Shuxin Zhuang,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: 提出一种基于信息论的因果推断方法，通过最小化表示与治疗变量之间的互信息来消除分配偏差，无需对抗训练。


<details>
  <summary>Details</summary>
Motivation: 解决因果推断中的分配偏差问题，传统对抗训练方法存在不稳定性和调参困难。

Method: 学习一个随机表示Z来预测结果，同时最小化I(Z;T)，推导出可处理的变分目标上界信息项，并与监督解码器耦合。

Result: 在数值模拟和真实临床数据集上评估，在似然度、反事实误差和政策评估指标上表现优于现有方法。

Conclusion: 该方法在避免对抗训练不稳定性的同时，在多个评估指标上取得了良好性能。

Abstract: We study counterfactual prediction under assignment bias and propose a
mathematically grounded, information-theoretic approach that removes
treatment-covariate dependence without adversarial training. Starting from a
bound that links the counterfactual-factual risk gap to mutual information, we
learn a stochastic representation Z that is predictive of outcomes while
minimizing I(Z; T). We derive a tractable variational objective that
upper-bounds the information term and couples it with a supervised decoder,
yielding a stable, provably motivated training criterion. The framework extends
naturally to dynamic settings by applying the information penalty to sequential
representations at each decision time. We evaluate the method on controlled
numerical simulations and a real-world clinical dataset, comparing against
recent state-of-the-art balancing, reweighting, and adversarial baselines.
Across metrics of likelihood, counterfactual error, and policy evaluation, our
approach performs favorably while avoiding the training instabilities and
tuning burden of adversarial schemes.

</details>


### [59] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: 提出OffSim框架，通过离线专家轨迹学习环境动态和奖励函数，无需手动设计模拟器和奖励函数，可离线训练策略


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要手动开发模拟器和定义奖励函数，过程耗时费力，需要更高效的替代方案

Method: 基于离线逆强化学习的模型框架，联合优化高熵转移模型和IRL奖励函数，增强探索性和奖励泛化能力

Result: 在MuJoCo实验中，OffSim相比现有离线IRL方法取得显著性能提升，验证了其有效性和鲁棒性

Conclusion: OffSim框架成功解决了手动设计模拟器和奖励函数的痛点，为离线强化学习提供了高效解决方案

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [60] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出了SESA（顺序采样框架），通过顺序生成多样化的解决方案草图来缓解强化学习中探索有限和熵崩溃的问题，提升LLM的推理多样性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练大型语言模型时存在探索有限和熵崩溃的问题，模型容易收敛到狭窄的解决方案集合，导致采样多样性丧失，阻碍性能进一步提升。

Method: SESA框架采用顺序采样方法，首先生成多样化的解决方案草图，然后将其扩展为完整的推理路径。通过将每个新输出条件于先前输出，促进整个过程的多样性。

Result: 在合成任务中，顺序采样在路径多样性和从崩溃中恢复方面持续优于传统RL方法。在三个智能体基准测试中，SESA将成功率分别提升了+0.25、+0.42和+0.07（相对基线RL最高提升211%）。

Conclusion: SESA为探索提供了一种结构化方法，为RL训练的LLM实现更有效和多样化的推理铺平了道路。

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [61] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出KME-CLIP方法，通过再生核希尔伯特空间的内积来改进CLIP中的相似度计算机制，更好地逼近点间互信息(PMI)


<details>
  <summary>Details</summary>
Motivation: 现有CLIP及其变体未能充分利用PMI的线性结构，而理论研究表明多模态配对的最优相似度度量应对应于PMI

Method: 利用再生核希尔伯特空间的内积来近似PMI，提出KME-CLIP方法

Result: 理论上证明该方法能以任意精度逼近PMI，实验表明在多个检索和分类任务中整体优于标准CLIP

Conclusion: KME-CLIP通过更好地利用PMI的线性结构，在多模态对比预训练中改进了相似度计算机制

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [62] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: 本文证明Transformer语言模型在离散输入到连续表示的映射中是单射的，这意味着可以从模型的表示中精确恢复输入文本。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即Transformer组件（如非线性激活和归一化）的非单射性会阻止从模型表示中精确恢复输入。

Method: 数学证明Transformer语言模型在初始化和训练过程中保持单射性；通过数十亿次碰撞测试验证；提出SipIt算法，可高效地从隐藏激活中重建精确输入文本。

Result: 在六个最先进的语言模型上进行测试，未观察到碰撞；SipIt算法在实践中实现了精确可逆性，并具有线性时间保证。

Conclusion: 单射性是语言模型的基本且可利用的属性，对透明度、可解释性和安全部署具有直接意义。

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


### [63] [Revisiting Knowledge Distillation: The Hidden Role of Dataset Size](https://arxiv.org/abs/2510.15516)
*Giulia Lanzillotta,Felix Sarnthein,Gil Kur,Thomas Hofmann,Bobby He*

Main category: cs.LG

TL;DR: 知识蒸馏在低数据量场景下效果更显著，这种新发现的性质被称为蒸馏的数据效率。研究否定了蒸馏可理解为标签平滑的假设，支持了暗知识假说。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注知识蒸馏的模型大小和泛化能力，但缺乏对数据集大小这一维度的系统研究。本文旨在探索数据集大小如何影响蒸馏效果。

Method: 在广泛的数据集、任务和神经网络架构上进行实验套件测试，分析不同数据集大小下蒸馏的表现，并验证现有蒸馏理论在不同数据量下的预测能力。

Result: 实验表明蒸馏效果在低数据量场景下不仅保持而且增强，数据集大小可能是影响蒸馏机制的一个基本但被忽视的变量。

Conclusion: 数据集大小是理解知识蒸馏机制的关键因素，蒸馏在低数据量场景下表现出更强的数据效率，这为蒸馏理论提供了新的视角。

Abstract: The concept of knowledge distillation (KD) describes the training of a
student model from a teacher model and is a widely adopted technique in deep
learning. However, it is still not clear how and why distillation works.
Previous studies focus on two central aspects of distillation: model size, and
generalisation. In this work we study distillation in a third dimension:
dataset size. We present a suite of experiments across a wide range of
datasets, tasks and neural architectures, demonstrating that the effect of
distillation is not only preserved but amplified in low-data regimes. We call
this newly discovered property the data efficiency of distillation. Equipped
with this new perspective, we test the predictive power of existing theories of
KD as we vary the dataset size. Our results disprove the hypothesis that
distillation can be understood as label smoothing, and provide further evidence
in support of the dark knowledge hypothesis. Finally, we analyse the impact of
modelling factors such as the objective, scale and relative number of samples
on the observed phenomenon. Ultimately, this work reveals that the dataset size
may be a fundamental but overlooked variable in the mechanisms underpinning
distillation.

</details>


### [64] [Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation](https://arxiv.org/abs/2510.15535)
*Abhay Kumar Dwivedi,Shanu Saklani,Soumya Dutta*

Main category: cs.LG

TL;DR: 该论文开发了一种用于多变量数据集的压缩神经表示方法，通过参数共享实现所有变量的同时学习，在数据压缩、重建质量、可视化效果等方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在科学可视化任务中的广泛应用，以及隐式神经表示在构建压缩数据模型方面的成功，激发了开发多变量数据集压缩神经表示的需求。

Method: 使用单一网络通过参数共享同时学习所有数据变量的表示，实现高效的数据压缩。

Result: 通过全面评估，该方法在重建数据质量、渲染和可视化质量、变量间依赖关系保持以及存储效率方面表现出优越性能。

Conclusion: 该方法为多变量数据集提供了高效的压缩神经表示解决方案，在多个关键指标上达到最先进水平。

Abstract: The extensive adoption of Deep Neural Networks has led to their increased
utilization in challenging scientific visualization tasks. Recent advancements
in building compressed data models using implicit neural representations have
shown promising results for tasks like spatiotemporal volume visualization and
super-resolution. Inspired by these successes, we develop compressed neural
representations for multivariate datasets containing tens to hundreds of
variables. Our approach utilizes a single network to learn representations for
all data variables simultaneously through parameter sharing. This allows us to
achieve state-of-the-art data compression. Through comprehensive evaluations,
we demonstrate superior performance in terms of reconstructed data quality,
rendering and visualization quality, preservation of dependency information
among variables, and storage efficiency.

</details>


### [65] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: MC Dropout不确定性在脑肿瘤MRI分割中与分割误差的相关性较弱，特别是在肿瘤边界区域，表明该方法在医学图像分割中的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然MC Dropout被广泛用于估计模型不确定性，但其在识别分割错误（尤其是肿瘤边界附近）方面的有效性尚不清楚。

Method: 使用U-Net在四种数据增强设置下进行2D脑肿瘤MRI分割，通过50次随机前向传播计算不确定性，并使用Pearson和Spearman系数与像素级误差进行相关性分析。

Result: 结果显示全局相关性较弱（r≈0.30-0.38），边界相关性可忽略（|r|<0.05），不同增强方法间的差异虽统计显著但缺乏实际意义。

Conclusion: MC Dropout不确定性在边界错误定位方面提供有限线索，强调了医学图像分割中需要替代或混合不确定性估计方法。

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [66] [Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems](https://arxiv.org/abs/2510.15555)
*Sibo Xiao*

Main category: cs.LG

TL;DR: 提出了战略双重稳健（SDR）估计器，将战略均衡建模与双重稳健估计相结合，用于战略环境中的因果推断。


<details>
  <summary>Details</summary>
Motivation: 解决由战略代理行为引起的内生处理分配问题，在保持双重稳健性的同时纳入战略考量。

Method: 整合战略均衡建模与双重稳健估计框架，在战略无混淆性假设下进行理论分析。

Result: 实证评估显示SDR优于基线方法，在不同战略强度下实现7.6%-29.3%的偏差减少，并保持与代理群体规模的稳健可扩展性。

Conclusion: 该框架为代理对干预做出战略响应时的可靠因果推断提供了原则性方法。

Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework
that integrates strategic equilibrium modeling with doubly robust estimation
for causal inference in strategic environments. SDR addresses endogenous
treatment assignment arising from strategic agent behavior, maintaining double
robustness while incorporating strategic considerations. Theoretical analysis
confirms SDR's consistency and asymptotic normality under strategic
unconfoundedness. Empirical evaluations demonstrate SDR's superior performance
over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying
strategic strengths and maintaining robust scalability with agent populations.
The framework provides a principled approach for reliable causal inference when
agents respond strategically to interventions.

</details>


### [67] [On the Neural Feature Ansatz for Deep Neural Networks](https://arxiv.org/abs/2510.15563)
*Edward Tansley,Estelle Massart,Coralia Cartis*

Main category: cs.LG

TL;DR: 本文扩展了神经特征假设(NFA)到多层线性网络，证明NFA指数与网络深度相关(α=1/L)，并研究了不平衡初始化和权重衰减对NFA的影响，同时指出非线性激活函数下NFA可能不成立。


<details>
  <summary>Details</summary>
Motivation: 理解特征学习是建立深度神经网络数学基础的重要开放问题。神经特征假设(NFA)描述了训练后第一层权重Gram矩阵与输入梯度外积平均值(AGOP)的关系，需要扩展到更深的网络架构。

Method: 使用梯度流动力学和平衡权重初始化，分析多层线性网络的训练动态。通过理论证明和数值实验验证NFA在不同网络深度、优化算法、权重衰减率和初始化方案下的表现。

Result: 对于L层线性网络，NFA在整个训练过程中成立，指数α=1/L。对于不平衡初始化，应用权重衰减时NFA渐近成立。非线性激活函数下NFA可能不成立，即使网络能完美拟合训练数据。

Conclusion: NFA的指数与网络深度相关，为理解深度网络特征学习提供了理论依据。权重衰减有助于在不平衡初始化下维持NFA，但非线性激活可能破坏这一关系。

Abstract: Understanding feature learning is an important open question in establishing
a mathematical foundation for deep neural networks. The Neural Feature Ansatz
(NFA) states that after training, the Gram matrix of the first-layer weights of
a deep neural network is proportional to some power $\alpha>0$ of the average
gradient outer product (AGOP) of this network with respect to its inputs.
Assuming gradient flow dynamics with balanced weight initialization, the NFA
was proven to hold throughout training for two-layer linear networks with
exponent $\alpha = 1/2$ (Radhakrishnan et al., 2024). We extend this result to
networks with $L \geq 2$ layers, showing that the NFA holds with exponent
$\alpha = 1/L$, thus demonstrating a depth dependency of the NFA. Furthermore,
we prove that for unbalanced initialization, the NFA holds asymptotically
through training if weight decay is applied. We also provide counterexamples
showing that the NFA does not hold for some network architectures with
nonlinear activations, even when these networks fit arbitrarily well the
training data. We thoroughly validate our theoretical results through numerical
experiments across a variety of optimization algorithms, weight decay rates and
initialization schemes.

</details>


### [68] [Attn-JGNN: Attention Enhanced Join-Graph Neural Networks](https://arxiv.org/abs/2510.15583)
*Jixin Zhang,Yong Lai*

Main category: cs.LG

TL;DR: 提出了Attention Enhanced Join-Graph Neural Networks(Attn-JGNN)模型用于解决#SAT问题，通过注意力机制提升求解精度


<details>
  <summary>Details</summary>
Motivation: 改进#SAT问题的求解准确性，通过结合图神经网络和注意力机制来优化概率推理过程

Method: 使用树分解将CNF公式编码为连接图，在连接图上进行迭代消息传递，应用簇内和簇间注意力机制来关注关键变量和簇

Result: 实验表明Attn-JGNN模型比其他神经网络方法取得了更好的结果

Conclusion: 注意力增强的连接图神经网络能有效提升#SAT问题的求解精度，通过关注关键变量和减少冗余计算来优化推理过程

Abstract: We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) model
for solving #SAT problems, which significantly improves the solving accuracy.
Inspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNN
uses tree decomposition to encode the CNF formula into a join-graph, then
performs iterative message passing on the join-graph, and finally approximates
the model number by learning partition functions. In order to further improve
the accuracy of the solution, we apply the attention mechanism in and between
clusters of the join-graphs, which makes Attn-JGNN pay more attention to the
key variables and clusters in probabilistic inference, and reduces the
redundant calculation. Finally, our experiments show that our Attn-JGNN model
achieves better results than other neural network methods.

</details>


### [69] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: GRATING是一个无需训练的高效推理系统，通过渐进式聚类剪枝和内存优化策略，在保持精度的同时显著降低语义top-K选择的延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 语义top-K选择在边缘设备上的延迟和内存需求主导了端到端预算，需要更高效的推理方法。

Method: 基于序列级稀疏性观察，提出整体前向传播和渐进式聚类剪枝，采用双层滑动窗口和分块执行来优化内存使用。

Result: 在0.6B到8B参数的reranker上，GRATING在微基准测试中延迟降低达89.0%，峰值内存降低达94.9%，在三个真实应用中延迟降低11.6%-51.0%，内存降低18.6%-77.8%。

Conclusion: GRATING通过利用相对排名特性和序列级稀疏性，实现了显著的效率提升，增强了边缘AI服务的可部署性。

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [70] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: 提出了CQD-SHAP框架，通过Shapley值计算复杂查询中各部分对特定答案排名的贡献，提高神经符号复杂查询回答的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有复杂查询回答方法多为黑盒模型，用户难以信任。虽然神经符号方法如CQD允许跟踪中间结果，但无法解释查询不同部分的重要性。

Method: 基于合作博弈论中的Shapley值，计算查询各部分对答案排名的贡献，满足所有基本Shapley公理。

Result: 在必要性和充分性解释的自动评估中，与多种基线方法比较，显示该方法对大多数查询类型有效。

Conclusion: CQD-SHAP能够解释神经预测器从不完整知识图谱中推断新知识的价值，相比仅依赖现有事实的符号方法更具优势。

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [71] [Decentralized Parameter-Free Online Learning](https://arxiv.org/abs/2510.15644)
*Tomas Ortega,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 提出了首个无需参数调优的去中心化在线学习算法，通过多智能体投币赌博和去中心化在线学习的结合实现次线性网络遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化在线学习中需要超参数调优的问题，开发无需参数调整就能获得次线性网络遗憾保证的算法。

Method: 通过gossip步骤将多智能体投币赌博与去中心化在线学习相结合，引入新的"投注函数"公式来简化多智能体遗憾分析。

Result: 算法实现了次线性网络遗憾界限，在合成和真实数据集上的实验验证了性能。

Conclusion: 该算法家族适用于分布式感知、去中心化优化和协作机器学习应用，是首个无需参数调优的去中心化在线学习算法。

Abstract: We propose the first parameter-free decentralized online learning algorithms
with network regret guarantees, which achieve sublinear regret without
requiring hyperparameter tuning. This family of algorithms connects multi-agent
coin-betting and decentralized online learning via gossip steps. To enable our
decentralized analysis, we introduce a novel "betting function" formulation for
coin-betting that simplifies the multi-agent regret analysis. Our analysis
shows sublinear network regret bounds and is validated through experiments on
synthetic and real datasets. This family of algorithms is applicable to
distributed sensing, decentralized optimization, and collaborative ML
applications.

</details>


### [72] [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)
*Ziqian Li,Kang Liu,Yongcun Song,Hangrui Yue,Enrique Zuazua*

Main category: cs.LG

TL;DR: 提出NODE-ONet框架，结合神经ODE和算子学习，通过物理编码增强PDE求解的泛化能力和数值效率


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法忽略PDE的领域知识，导致难以捕捉时间动态和在训练时间范围外的泛化问题

Method: 采用编码器-解码器架构：编码器空间离散化输入函数，神经ODE捕捉潜在时间动态，解码器重构物理空间解；提出物理编码神经ODE融入PDE特定物理特性

Result: 在非线性扩散-反应和Navier-Stokes方程上的数值实验显示高精度、计算效率和超出训练时间范围的预测能力

Conclusion: 该框架具有适应不同编码器/解码器的灵活性，能跨相关PDE族泛化，是科学机器学习中可扩展的物理编码工具

Abstract: Operator learning has emerged as a promising paradigm for developing
efficient surrogate models to solve partial differential equations (PDEs).
However, existing approaches often overlook the domain knowledge inherent in
the underlying PDEs and hence suffer from challenges in capturing temporal
dynamics and generalization issues beyond training time frames. This paper
introduces a deep neural ordinary differential equation (ODE) operator network
framework, termed NODE-ONet, to alleviate these limitations. The framework
adopts an encoder-decoder architecture comprising three core components: an
encoder that spatially discretizes input functions, a neural ODE capturing
latent temporal dynamics, and a decoder reconstructing solutions in physical
spaces. Theoretically, error analysis for the encoder-decoder architecture is
investigated. Computationally, we propose novel physics-encoded neural ODEs to
incorporate PDE-specific physical properties. Such well-designed neural ODEs
significantly reduce the framework's complexity while enhancing numerical
efficiency, robustness, applicability, and generalization capacity. Numerical
experiments on nonlinear diffusion-reaction and Navier-Stokes equations
demonstrate high accuracy, computational efficiency, and prediction
capabilities beyond training time frames. Additionally, the framework's
flexibility to accommodate diverse encoders/decoders and its ability to
generalize across related PDE families further underscore its potential as a
scalable, physics-encoded tool for scientific machine learning.

</details>


### [73] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: 提出了一种基于指令级位运算的Tsetlin机器高效软件实现，通过早期退出机制和文字重排序策略，在ARM处理器上实现推理时间减少96.71%


<details>
  <summary>Details</summary>
Motivation: Tsetlin机器在CPU等资源受限设备上具有高速推理潜力，其逻辑驱动操作适合现代CPU架构的并行执行

Method: 使用指令级位运算进行紧凑模型表示和加速处理；引入早期退出机制避免不必要计算；提出文字重排序策略最大化早期退出可能性

Result: 在ARM处理器上的实验显示，推理时间相比传统整数实现减少96.71%，同时保持相当的代码密度

Conclusion: 所提出的优化实现显著提升了Tsetlin机器的推理效率，适用于资源受限设备

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [74] [WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables](https://arxiv.org/abs/2510.15655)
*Lino Gerlach,Liv Våge,Thore Gerlach,Elliott Kauffman*

Main category: cs.LG

TL;DR: 提出WARP-LUTs方法，通过Walsh变换辅助的概率查找表来高效学习逻辑门组合，相比DLGNs训练更快、参数更少，在CIFAR-10上保持相似精度。


<details>
  <summary>Details</summary>
Motivation: 现有乘法自由模型如DLGNs在训练时计算成本高，且难以扩展到更多输入的逻辑块，需要更高效的梯度学习方法。

Method: 使用Walsh变换辅助的概率查找表(WARP-LUTs)，通过梯度下降学习逻辑门组合，大幅减少可训练参数数量。

Result: 在CIFAR-10数据集上比DLGNs收敛显著更快，同时保持可比的准确率。

Conclusion: WARP-LUTs为高效机器学习提供了有前景的方法，有望扩展到更高输入逻辑块，在现代FPGA上实现极高效部署和实时科学应用。

Abstract: Fast and efficient machine learning is of growing interest to the scientific
community and has spurred significant research into novel model architectures
and hardware-aware design. Recent hard? and software co-design approaches have
demonstrated impressive results with entirely multiplication-free models.
Differentiable Logic Gate Networks (DLGNs), for instance, provide a
gradient-based framework for learning optimal combinations of low-level logic
gates, setting state-of-the-art trade-offs between accuracy, resource usage,
and latency. However, these models suffer from high computational cost during
training and do not generalize well to logic blocks with more inputs. In this
work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables
(WARP-LUTs) - a novel gradient-based method that efficiently learns
combinations of logic gates with substantially fewer trainable parameters. We
demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10
compared to DLGNs, while maintaining comparable accuracy. Furthermore, our
approach suggests potential for extension to higher-input logic blocks,
motivating future research on extremely efficient deployment on modern FPGAs
and its real-time science applications.

</details>


### [75] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: CarBoN是一个测试时校准框架，通过自适应调整温度参数和偏移向量来引导语言模型生成更可靠的推理路径，在减少采样次数的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Best-of-N采样在N增加时收益递减，存在效率问题。需要在不重新训练LLM的情况下，通过测试时校准来更有效地提升推理任务性能。

Method: 提出两阶段方法：先探索解空间，然后通过输入特定的温度T和加性偏移向量δ来校准logits，引导生成更可靠的推理路径。

Result: 在MATH-500和AIME-2024上的实验显示，CarBoN效率提升显著，达到相同准确率所需的采样次数减少4倍，且在固定预算下通常获得更高准确率。

Conclusion: CarBoN框架有效提升了推理任务的测试时效率，温度T和偏移向量δ在平衡输出多样性和正确性方面发挥互补作用，该框架也可推广到束搜索等步级采样策略。

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [76] [KS-Net: Multi-layer network model for determining the rotor type from motor parameters in interior PMSMs](https://arxiv.org/abs/2510.15688)
*Kivanc Dogan,Ahmet Orhan*

Main category: cs.LG

TL;DR: 该研究使用机器学习方法对IPMSM转子形状进行分类，替代传统有限元分析方法。Cubic SVM和Quadratic SVM算法达到100%准确率，自定义深度学习模型KS-Net达到99.98%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统有限元分析方法计算成本高，需要寻找更快速、成本更低的替代方案来分类IPMSM转子形状。

Method: 使用机器学习方法（包括Cubic SVM、Quadratic SVM、Fine KNN、Cosine KNN、Fine Tree和自定义KS-Net模型）基于电磁参数对转子形状进行分类，采用10折交叉验证和9000个样本的平衡数据集。

Result: Cubic SVM和Quadratic SVM算法完美分类所有样本，准确率100%；KS-Net模型准确率99.98%，仅有两个误分类；所有方法都表现出色。

Conclusion: 数据驱动方法可以高精度预测IPMSM转子形状，为电机设计过程加速、自动化转子识别系统和数据驱动故障诊断提供了快速且经济有效的替代方案。

Abstract: The demand for high efficiency and precise control in electric drive systems
has led to the widespread adoption of Interior Permanent Magnet Synchronous
Motors (IPMSMs). The performance of these motors is significantly influenced by
rotor geometry. Traditionally, rotor shape analysis has been conducted using
the finite element method (FEM), which involves high computational costs. This
study aims to classify the rotor shape (2D type, V type, Nabla type) of IPMSMs
using electromagnetic parameters through machine learning-based methods and to
demonstrate the applicability of this approach as an alternative to classical
methods. In this context, a custom deep learning model, KS-Net, developed by
the user, was comparatively evaluated against Cubic SVM, Quadratic SVM, Fine
KNN, Cosine KNN, and Fine Tree algorithms. The balanced dataset, consisting of
9,000 samples, was tested using 10-fold cross-validation, and performance
metrics such as accuracy, precision, recall, and F1-score were employed. The
results indicate that the Cubic SVM and Quadratic SVM algorithms classified all
samples flawlessly, achieving 100% accuracy, while the KS-Net model achieved
99.98% accuracy with only two misclassifications, demonstrating competitiveness
with classical methods. This study shows that the rotor shape of IPMSMs can be
predicted with high accuracy using data-driven approaches, offering a fast and
cost-effective alternative to FEM-based analyses. The findings provide a solid
foundation for accelerating motor design processes, developing automated rotor
identification systems, and enabling data-driven fault diagnosis in engineering
applications.

</details>


### [77] [Constrained Adversarial Perturbation](https://arxiv.org/abs/2510.15699)
*Virendra Nishad,Bhaskar Mukhoty,Hilal AlQuabeh,Sandeep K. Shukla,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 提出了一种针对约束特征空间的通用对抗扰动方法CAP，通过增强拉格朗日最小最大优化来强制执行领域特定约束，在金融、IT网络等场景中实现了更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有通用对抗扰动方法忽略了领域特定约束，导致生成的对抗样本不真实或容易被检测，限制了实际应用。

Method: 提出CAP算法，基于增强拉格朗日最小最大优化框架，使用梯度交替优化策略来强制执行多个复杂约束。

Result: 在金融、IT网络和网络物理系统等多个领域评估，CAP实现了更高的攻击成功率，同时显著减少了运行时间。

Conclusion: CAP方法有效解决了约束特征空间中的对抗攻击问题，并能泛化到个体对抗扰动，还提供了从数据中学习特征约束的原则性方法。

Abstract: Deep neural networks have achieved remarkable success in a wide range of
classification tasks. However, they remain highly susceptible to adversarial
examples - inputs that are subtly perturbed to induce misclassification while
appearing unchanged to humans. Among various attack strategies, Universal
Adversarial Perturbations (UAPs) have emerged as a powerful tool for both
stress testing model robustness and facilitating scalable adversarial training.
Despite their effectiveness, most existing UAP methods neglect domain specific
constraints that govern feature relationships. Violating such constraints, such
as debt to income ratios in credit scoring or packet flow invariants in network
communication, can render adversarial examples implausible or easily
detectable, thereby limiting their real world applicability.
  In this work, we advance universal adversarial attacks to constrained feature
spaces by formulating an augmented Lagrangian based min max optimization
problem that enforces multiple, potentially complex constraints of varying
importance. We propose Constrained Adversarial Perturbation (CAP), an efficient
algorithm that solves this problem using a gradient based alternating
optimization strategy. We evaluate CAP across diverse domains including
finance, IT networks, and cyber physical systems, and demonstrate that it
achieves higher attack success rates while significantly reducing runtime
compared to existing baselines. Our approach also generalizes seamlessly to
individual adversarial perturbations, where we observe similar strong
performance gains. Finally, we introduce a principled procedure for learning
feature constraints directly from data, enabling broad applicability across
domains with structured input spaces.

</details>


### [78] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer是一个通过强化学习训练的语言模型，专门用于简化Lean证明，无需人工监督，能显著压缩证明长度并提高验证效率。


<details>
  <summary>Details</summary>
Motivation: 神经网络定理证明生成的证明过长，难以理解，限制了数学洞察力，而现有方法难以处理RL训练证明器生成的超长证明。

Method: 通过专家迭代和强化学习训练ProofOptimizer模型，使用Lean验证简化并提供训练信号，在推理时采用迭代证明缩短工作流。

Result: 在标准基准测试中，ProofOptimizer显著压缩了证明长度：miniF2F减少87%，PutnamBench减少57%，IMO 2025证明减少49%，同时提高了Lean验证速度。

Conclusion: ProofOptimizer能有效简化证明，不仅使证明更简洁，还提高了验证效率，并且简化后的证明作为训练数据能进一步提升下游证明器的性能。

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [79] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: ProSh是一种无模型的安全强化学习算法，通过风险预算增强状态空间，使用学习到的成本评论家对策略分布施加屏蔽，确保所有采样动作在期望意义上保持安全。


<details>
  <summary>Details</summary>
Motivation: 开发既性能最优又安全可靠的强化学习系统，为部署提供正式的安全保证。

Method: 在约束MDP状态空间中添加风险预算，通过学习的成本评论家对智能体策略分布施加屏蔽，确保采样动作的安全性。

Result: 在确定性环境中保持最优性，提供仅依赖于备份评论家准确度的成本期望上界，实验表明在训练期间也能保证安全性。

Conclusion: ProSh是一种有效的无模型安全强化学习方法，在适当假设下能够在训练和部署时提供正式的安全保证。

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [80] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 提出了一种基于自动机反馈的强化学习方法，用DFA生成的偏好替代显式奖励函数，无需手动设计奖励，在具有时间依赖性的任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理具有复杂历史依赖奖励结构的环境时面临挑战，需要手动设计奖励函数，这既困难又耗时。

Method: 利用确定性有限自动机(DFA)结构生成轨迹偏好来学习奖励函数，包含静态方法（直接使用学习到的奖励函数进行策略优化）和动态方法（通过迭代更新持续优化奖励函数和策略）。

Result: 在离散和连续环境中的实验表明，该方法能够学习具有时间依赖性的任务的有效策略，优于传统奖励工程和基于自动机的基线方法。

Conclusion: 基于自动机的偏好方法在处理非马尔可夫奖励方面具有优势，为传统奖励建模提供了可扩展、高效且独立于人工的替代方案，并提供了收敛保证。

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


### [81] [A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis](https://arxiv.org/abs/2510.15750)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: 本文评估了图神经网络和3D U-Net作为参数化I梁有限元分析的替代模型，发现GNN显著优于U-Net，其中MPNN和Graph Transformer表现最佳，物理信息神经网络框架显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 有限元分析计算成本高，不适合设计优化问题，需要寻找深度学习模型作为替代方案。

Method: 使用图神经网络和3D U-Net作为FEA替代模型，引入基于Navier-Cauchy方程的物理信息神经网络框架，并采用课程学习策略（先数据预训练后物理信息微调）。

Result: GNN明显优于U-Net，MPNN和Graph Transformer分别达到3.5%和2.6%的相对L2误差，PINN框架将误差降低高达11.3%。MPNN PINN在预测性能、模型大小和推理速度之间提供了最佳平衡。

Conclusion: PINN增强的MPNN提供了最实用的解决方案，在保持高精度的同时具有更好的推理效率。

Abstract: Although Finite Element Analysis (FEA) is an integral part of the product
design lifecycle, the analysis is computationally expensive, making it
unsuitable for many design optimization problems. The deep learning models can
be a great solution. However, selecting the architecture that emulates the FEA
with great accuracy is a challenge. This paper presents a comprehensive
evaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEA
of parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)
framework, governed by the Navier Cauchy equations, to enforce physical laws.
Crucially, we demonstrate that a curriculum learning strategy, pretraining on
data followed by physics informed fine tuning, is essential for stabilizing
training. Our results show that GNNs fundamentally outperform the U-Net. Even
the worst performer among GNNs, the GCN framework, achieved a relative L2 error
of 8.7% while the best framework among U Net, U Net with attention mechanism
trained on high resolution data, achieved 13.0% score. Among the graph-based
architectures, the Message Passing Neural Networks (MPNN) and Graph
Transformers achieved the highest accuracy, achieving a relative L2 score of
3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)
significantly improved the generalization, reducing error by up to 11.3% on
high-signal tasks. While the Graph Transformer is the most accurate model, it
is more 37.5% slower during inference when compared to second best model, MPNN
PINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.
It offers a good compromise between predictive performance, model size, and
inference speed.

</details>


### [82] [SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive Mixup and Neural Collapse](https://arxiv.org/abs/2510.15751)
*Trung-Anh Dang,Vincent Nguyen,Ngoc-Son Vu,Christel Vrain*

Main category: cs.LG

TL;DR: 提出Sphere-Adaptive Mixup (SAMix)方法，通过自适应混合策略改进基于神经崩溃的持续学习方法，不仅提升性能还改善模型校准。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注减轻遗忘和提高准确率，但忽视了网络校准这一重要方面。神经崩溃现象在持续学习中具有优势，但很少有工作致力于改进持续学习模型的校准可靠性。

Method: 提出SAMix方法，这是一种针对神经崩溃方法设计的自适应混合策略。它根据神经崩溃下特征空间的几何特性调整混合过程，确保更鲁棒的规范化和对齐。

Result: 实验表明SAMix显著提升性能，在持续学习中超越现有最优方法，同时改善模型校准。提高了跨任务准确率和预测的可靠性。

Conclusion: SAMix是持续学习系统稳健性的有前景的进展，能够同时提升准确率和预测可靠性。

Abstract: While most continual learning methods focus on mitigating forgetting and
improving accuracy, they often overlook the critical aspect of network
calibration, despite its importance. Neural collapse, a phenomenon where
last-layer features collapse to their class means, has demonstrated advantages
in continual learning by reducing feature-classifier misalignment. Few works
aim to improve the calibration of continual models for more reliable
predictions. Our work goes a step further by proposing a novel method that not
only enhances calibration but also improves performance by reducing
overconfidence, mitigating forgetting, and increasing accuracy. We introduce
Sphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural
collapse-based methods. SAMix adapts the mixing process to the geometric
properties of feature spaces under neural collapse, ensuring more robust
regularization and alignment. Experiments show that SAMix significantly boosts
performance, surpassing SOTA methods in continual learning while also improving
model calibration. SAMix enhances both across-task accuracy and the broader
reliability of predictions, making it a promising advancement for robust
continual learning systems.

</details>


### [83] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI是一个模块化、经济实惠的AI平台，集成了六个AI模块，通过优化摄像头布局、视听监控、实时分析预警、鸡蛋计数、生产预测和推荐系统，为中小型家禽养殖场提供全面的智能化管理解决方案。


<details>
  <summary>Details</summary>
Motivation: 中小型家禽养殖场缺乏负担得起的集成工具进行持续监控和决策，主要依赖手动、反应式检查，难以在满足生产力目标的同时确保动物福利和环境合规。

Method: 使用进化算法优化摄像头布局实现全覆盖，集成视听监控模块提取福利指标，边缘视觉模型实现实时鸡蛋计数，预测模型进行产量和饲料消耗预测，推荐模块结合天气预报指导环境调整。

Result: 现场试验显示在树莓派5上实现100%鸡蛋计数准确率，稳健的异常检测和可靠的短期预测，填补了孤立试点工具与可扩展农场智能之间的差距。

Conclusion: PoultryFI通过低成本传感、边缘分析和规范性AI的结合，实现了对鸡群的持续监控、生产预测和性能优化，使生产者能够主动保障福利和盈利能力。

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


### [84] [Cavity Duplexer Tuning with 1d Resnet-like Neural Networks](https://arxiv.org/abs/2510.15796)
*Anton Raskovalov*

Main category: cs.LG

TL;DR: 提出了一种用于调谐具有大量调节螺钉的腔体双工器的机器学习方法，使用监督学习框架和1D ResNet架构，结合S参数曲线特征处理，能够在每个螺钉4-5次旋转内达到接近调谐状态


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法效果不佳，需要开发更有效的机器学习方法来调谐具有大量调节螺钉的腔体双工器

Method: 采用监督学习框架，构建包含1D ResNet骨干网络和S参数曲线特征（如曲线形状、峰值位置和幅度）处理的神经网络架构，结合外部控制算法

Result: 该方法能够在每个螺钉仅需4-5次旋转的情况下，使双工器达到接近调谐的状态

Conclusion: 所提出的监督学习方法和神经网络架构能够有效解决腔体双工器的调谐问题，显著优于传统强化学习方法

Abstract: This paper presents machine learning method for tuning of cavity duplexer
with a large amount of adjustment screws. After testing we declined
conventional reinforcement learning approach and reformulated our task in the
supervised learning setup. The suggested neural network architecture includes
1d ResNet-like backbone and processing of some additional information about
S-parameters, like the shape of curve and peaks positions and amplitudes. This
neural network with external control algorithm is capable to reach almost the
tuned state of the duplexer within 4-5 rotations per screw.

</details>


### [85] [AB-UPT for Automotive and Aerospace Applications](https://arxiv.org/abs/2510.15808)
*Benedikt Alkin,Richard Kurle,Louis Serrano,Dennis Just,Johannes Brandstetter*

Main category: cs.LG

TL;DR: AB-UPT在汽车和飞机CFD模拟中表现出色，相比传统数值求解器计算量大幅减少，在单GPU上一天内可训练完成，为工业级应用铺平道路。


<details>
  <summary>Details</summary>
Motivation: 扩展AB-UPT在汽车和飞机计算流体动力学模拟中的实证评估用例，结合高质量数据生成和最先进的神经代理模型。

Method: 使用Luminary Cloud平台生成SHIFT-SUV（汽车）和SHIFT-Wing（飞机）数据集，采用AB-UPT模型进行CFD模拟，与基于Transformer的基线模型进行对比。

Result: AB-UPT在两个数据集上都优于之前的SOTA Transformer基线模型，能够从简单的各向同性网格几何表示中在几秒内近乎完美地预测集成空气动力学力。

Conclusion: AB-UPT在汽车和飞机CFD模拟中表现优异，计算效率高，训练成本低，具有工业级应用的潜力。

Abstract: The recently proposed Anchored-Branched Universal Physics Transformers
(AB-UPT) shows strong capabilities to replicate automotive computational fluid
dynamics simulations requiring orders of magnitudes less compute than
traditional numerical solvers. In this technical report, we add two new
datasets to the body of empirically evaluated use-cases of AB-UPT, combining
high-quality data generation with state-of-the-art neural surrogates. Both
datasets were generated with the Luminary Cloud platform containing automotives
(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data
generation. Next, we show favorable performances of AB-UPT against previous
state-of-the-art transformer-based baselines on both datasets, followed by
extensive qualitative and quantitative evaluations of our best AB-UPT model.
AB-UPT shows strong performances across the board. Notably, it obtains near
perfect prediction of integrated aerodynamic forces within seconds from a
simple isotopically tesselate geometry representation and is trainable within a
day on a single GPU, paving the way for industry-scale applications.

</details>


### [86] [Chronos-2: From Univariate to Universal Forecasting](https://arxiv.org/abs/2510.15821)
*Abdul Fatir Ansari,Oleksandr Shchur,Jaris Küken,Andreas Auer,Boran Han,Pedro Mercado,Syama Sundar Rangapuram,Huibin Shen,Lorenzo Stella,Xiyuan Zhang,Mononito Goswami,Shubham Kapoor,Danielle C. Maddix,Pablo Guerron,Tony Hu,Junming Yin,Nick Erickson,Prateek Mutalik Desai,Hao Wang,Huzefa Rangwala,George Karypis,Yuyang Wang,Michael Bohlke-Schneider*

Main category: cs.LG

TL;DR: Chronos-2是一个预训练的时间序列模型，能够以零样本方式处理单变量、多变量和协变量预测任务，通过群组注意力机制实现上下文学习，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练时间序列模型主要关注单变量预测，限制了在现实场景中处理多变量数据和协变量的应用。

Method: 使用群组注意力机制促进跨多个时间序列的上下文学习，通过在合成数据集上训练来获得通用能力。

Result: 在fev-bench、GIFT-Eval和Chronos Benchmark II三个基准测试中表现最优，在涉及协变量的任务中显著优于基线模型。

Conclusion: Chronos-2的上下文学习能力使其成为可在现实预测管道中直接使用的通用预测模型。

Abstract: Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate forecasting, limiting their
applicability in real-world scenarios where multivariate data and covariates
play a crucial role. We present Chronos-2, a pretrained model capable of
handling univariate, multivariate, and covariate-informed forecasting tasks in
a zero-shot manner. Chronos-2 employs a group attention mechanism that
facilitates in-context learning (ICL) through efficient information sharing
across multiple time series within a group, which may represent sets of related
series, variates of a multivariate series, or targets and covariates in a
forecasting task. These general capabilities are achieved through training on
synthetic datasets that impose diverse multivariate structures on univariate
series. Chronos-2 delivers state-of-the-art performance across three
comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On
fev-bench, which emphasizes multivariate and covariate-informed forecasting,
Chronos-2's universal ICL capabilities lead to substantial improvements over
existing models. On tasks involving covariates, it consistently outperforms
baselines by a wide margin. Case studies in the energy and retail domains
further highlight its practical advantages. The in-context learning
capabilities of Chronos-2 establish it as a general-purpose forecasting model
that can be used "as is" in real-world forecasting pipelines.

</details>


### [87] [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)
*Dominik Kallusky,Vinay Rao,Vishal Nandavanam,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 论文提出SNOO优化器，通过将Nesterov动量应用于Lookahead优化器的伪梯度，在非分布式设置中实现了1.5-2.5倍的计算效率提升，且改进效果随模型规模增大而增强。


<details>
  <summary>Details</summary>
Motivation: DiLoCo优化器在非分布式设置中意外地优于AdamW，研究发现其有效性主要源于对伪梯度应用Nesterov动量，这启发了开发SNOO优化器来提升训练效率。

Method: 提出Step-K Nesterov Outer Optimizer (SNOO)，在Lookahead双循环框架中，对多个内优化器步骤产生的伪梯度应用Nesterov动量来更新慢权重。

Result: SNOO在非分布式设置中实现了1.5-2.5倍的计算因子增益，训练FLOPs规模达1e23，且改进效果随模型规模增大而增加。

Conclusion: SNOO因其最小的计算和内存开销以及与模型分片的兼容性，成为适用于包括AdamW和Muon在内的各种内优化器的实用增强方法。

Abstract: The rapid development of large language models (LLMs) has driven the demand
for more efficient optimization techniques. Among these, the Lookahead family
of optimizers employs a two-loop framework, maintaining fast and slow sets of
model weights. Multiple inner optimizer steps on the fast weights produce a
trajectory - the pseudo-gradient - that is used to update the slow weights.
DiLoCo, a notable example originally designed for distributed training, applies
Nesterov momentum to the averaged pseudo-gradient from multiple workers,
claiming to even outperform AdamW in a non-distributed setup. In this paper, we
empirically show that DiLoCo's surprising effectiveness stems primarily from
applying Nesterov momentum to the pseudo-gradient, which improves training in a
non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov
Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains
of 1.5 - 2.5$\times$ in a non-distributed setting up to a scale of 1e23
training FLOPs, with improvements that increase with model size. Because of its
minimal compute and memory overhead and compatibility with model sharding, SNOO
is a practical enhancement for a variety of inner optimizers, including AdamW
and Muon.

</details>


### [88] [FIDDLE: Reinforcement Learning for Quantum Fidelity Enhancement](https://arxiv.org/abs/2510.15833)
*Hoang M. Ngo,Tamer Kahveci,My T. Thai*

Main category: cs.LG

TL;DR: FIDDLE是一个量子电路路由优化框架，通过高斯过程代理模型和强化学习直接最大化过程保真度，显著提升噪声量子设备的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前量子设备受限于噪声，降低了可靠性。在量子电路编译过程中，特别是在路由阶段，如何提高过程保真度是一个关键挑战。

Method: 提出FIDDLE学习框架，包含两个模块：基于高斯过程的代理模型用于有限训练样本下的过程保真度估计，以及强化学习模块用于路由优化。

Result: FIDDLE的代理模型相比现有学习技术能提供更好的过程保真度估计，端到端框架在各种噪声模型下显著提高了量子电路的过程保真度。

Conclusion: 这是首个直接最大化过程保真度的方法，超越了依赖电路深度或门数等间接指标的传统方法，在量子计算可靠性方面取得了重要进展。

Abstract: Quantum computing has the potential to revolutionize fields like quantum
optimization and quantum machine learning. However, current quantum devices are
hindered by noise, reducing their reliability. A key challenge in gate-based
quantum computing is improving the reliability of quantum circuits, measured by
process fidelity, during the transpilation process, particularly in the routing
stage. In this paper, we address the Fidelity Maximization in Routing Stage
(FMRS) problem by introducing FIDDLE, a novel learning framework comprising two
modules: a Gaussian Process-based surrogate model to estimate process fidelity
with limited training samples and a reinforcement learning module to optimize
routing. Our approach is the first to directly maximize process fidelity,
outperforming traditional methods that rely on indirect metrics such as circuit
depth or gate count. We rigorously evaluate FIDDLE by comparing it with
state-of-the-art fidelity estimation techniques and routing optimization
methods. The results demonstrate that our proposed surrogate model is able to
provide a better estimation on the process fidelity compared to existing
learning techniques, and our end-to-end framework significantly improves the
process fidelity of quantum circuits across various noise models.

</details>


### [89] [Transfer Orthology Networks](https://arxiv.org/abs/2510.15837)
*Vikash Singh*

Main category: cs.LG

TL;DR: TRON是一种新颖的跨物种迁移学习神经网络架构，利用同源关系构建二分图来指导知识迁移，通过物种转换层将源物种基因表达映射到目标物种基因空间。


<details>
  <summary>Details</summary>
Motivation: 解决跨物种迁移学习问题，利用同源关系实现生物基础的知识迁移，提高转录组数据的有效利用。

Method: 在预训练前馈神经网络前添加学习的物种转换层，该层权重通过同源关系的二分图邻接矩阵进行掩码，学习源物种到目标物种基因表达的线性变换。

Result: TRON提供了生物基础且可解释的跨物种迁移学习方法，转换层的学习权重可用于解释功能同源性。

Conclusion: TRON为跨物种迁移学习开辟了新途径，目前正在收集跨物种转录组/表型数据以进行实验验证。

Abstract: We present Transfer Orthology Networks (TRON), a novel neural network
architecture designed for cross-species transfer learning. TRON leverages
orthologous relationships, represented as a bipartite graph between species, to
guide knowledge transfer. Specifically, we prepend a learned species conversion
layer, whose weights are masked by the biadjacency matrix of this bipartite
graph, to a pre-trained feedforward neural network that predicts a phenotype
from gene expression data in a source species. This allows for efficient
transfer of knowledge to a target species by learning a linear transformation
that maps gene expression from the source to the target species' gene space.
The learned weights of this conversion layer offer a potential avenue for
interpreting functional orthology, providing insights into how genes across
species contribute to the phenotype of interest. TRON offers a biologically
grounded and interpretable approach to cross-species transfer learning, paving
the way for more effective utilization of available transcriptomic data. We are
in the process of collecting cross-species transcriptomic/phenotypic data to
gain experimental validation of the TRON architecture.

</details>


### [90] [Learning Correlated Reward Models: Statistical Barriers and Opportunities](https://arxiv.org/abs/2510.15839)
*Yeshwanth Cherapanamjeri,Constantinos Daskalakis,Gabriele Farina,Sobhan Mohammadpour*

Main category: cs.LG

TL;DR: 本文研究了学习相关probit模型的统计和计算挑战，发现传统的成对偏好数据无法学习相关性信息，而三选一偏好数据能够克服这一局限，并提出了高效估计器。


<details>
  <summary>Details</summary>
Motivation: 随机效用模型在人类反馈强化学习中很重要，但许多技术依赖无关选项独立性假设，这限制了人类偏好的精细建模。避免该假设的模型缺乏统计和计算保证。

Method: 研究相关probit模型的学习挑战，分析不同数据收集范式（成对偏好vs三选一偏好）的有效性，并开发统计和计算高效的估计器。

Result: 证明成对偏好数据无法学习相关性信息，而三选一偏好数据能够有效学习相关效用，提出的估计器具有接近最优性能。

Conclusion: 高阶偏好数据在建模相关效用方面具有优势，能够实现更精细的人类偏好建模，提高个性化效果。

Abstract: Random Utility Models (RUMs) are a classical framework for modeling user
preferences and play a key role in reward modeling for Reinforcement Learning
from Human Feedback (RLHF). However, a crucial shortcoming of many of these
techniques is the Independence of Irrelevant Alternatives (IIA) assumption,
which collapses \emph{all} human preferences to a universal underlying utility
function, yielding a coarse approximation of the range of human preferences. On
the other hand, statistical and computational guarantees for models avoiding
this assumption are scarce. In this paper, we investigate the statistical and
computational challenges of learning a \emph{correlated} probit model, a
fundamental RUM that avoids the IIA assumption. First, we establish that the
classical data collection paradigm of pairwise preference data is
\emph{fundamentally insufficient} to learn correlational information,
explaining the lack of statistical and computational guarantees in this
setting. Next, we demonstrate that \emph{best-of-three} preference data
provably overcomes these shortcomings, and devise a statistically and
computationally efficient estimator with near-optimal performance. These
results highlight the benefits of higher-order preference data in learning
correlated utilities, allowing for more fine-grained modeling of human
preferences. Finally, we validate these theoretical guarantees on several
real-world datasets, demonstrating improved personalization of human
preferences.

</details>


### [91] [Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch](https://arxiv.org/abs/2510.15850)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出了一种混合求解器，结合优化代理和经典求解器，利用对偶理论来保证预测的最优性差距，在无法认证最优性时回退到经典求解器，实现了1000倍以上的加速同时保证最大2%的最优性差距。


<details>
  <summary>Details</summary>
Motivation: 现有优化代理虽然平均最优性差距低于1%，但存在最坏情况下最优性差距显著增大的问题，难以在实际中信任其预测结果。

Method: 提出混合求解器，利用对偶理论高效地约束预测的最优性差距，对于无法认证最优性的查询回退到经典求解器；并提出结合原始和对偶代理训练的替代训练程序以提高混合求解器的加速效果。

Result: 在大规模传输系统上的实验表明，混合求解器具有高度可扩展性，相比并行化单纯形求解器实现了超过1000倍的加速，同时保证最大2%的最优性差距。

Conclusion: 该混合求解器在优化代理和经典求解器之间取得了平衡，实现了可信赖的部署，并基于用户定义的最优性阈值提供了可解释的速度-最优性权衡。

Abstract: Recent research has shown that optimization proxies can be trained to high
fidelity, achieving average optimality gaps under 1% for large-scale problems.
However, worst-case analyses show that there exist in-distribution queries that
result in orders of magnitude higher optimality gap, making it difficult to
trust the predictions in practice. This paper aims at striking a balance
between classical solvers and optimization proxies in order to enable
trustworthy deployments with interpretable speed-optimality tradeoffs based on
a user-defined optimality threshold. To this end, the paper proposes a hybrid
solver that leverages duality theory to efficiently bound the optimality gap of
predictions, falling back to a classical solver for queries where optimality
cannot be certified. To improve the achieved speedup of the hybrid solver, the
paper proposes an alternative training procedure that combines the primal and
dual proxy training. Experiments on large-scale transmission systems show that
the hybrid solver is highly scalable. The proposed hybrid solver achieves
speedups of over 1000x compared to a parallelized simplex-based solver while
guaranteeing a maximum optimality gap of 2%.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [92] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive hash table是一个高性能、可动态调整大小的GPU哈希表，通过warp协作和缓存对齐的桶布局设计，在保持95%高负载因子的同时，提供比现有GPU哈希表高1.5-2倍的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表在处理并发更新、高负载因子和不规则内存访问模式时存在性能瓶颈，需要一种能适应不同工作负载且无需全局重哈希的高性能解决方案。

Method: 采用缓存对齐的打包桶布局、warp同步并发协议（WABC和WCME）、负载因子感知的动态调整策略，以及四步插入策略（替换、声明提交、有界布谷鸟驱逐、溢出存储后备）。

Result: 在NVIDIA RTX 4090上，Hive哈希表在混合插入-删除-查找工作负载下比最先进的GPU哈希表（Slab-Hash、DyCuckoo、WarpCore）吞吐量高1.5-2倍，在平衡工作负载下达到35亿次更新/秒和近40亿次查找/秒。

Conclusion: Hive哈希表通过创新的warp协作设计、高效的并发控制和动态调整策略，为GPU加速数据处理提供了可扩展且高效的哈希表解决方案。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [93] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO是一个新的区块链执行引擎，结合乐观并发控制(OCC)和对象数据模型，通过四项核心创新解决高竞争负载下的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着区块链共识算法效率提升，执行层已成为新的性能瓶颈，特别是在高竞争负载下。现有的并行执行框架（乐观或悲观并发控制）在高竞争负载下性能都会下降。

Method: NEMO结合OCC和对象数据模型，引入四项创新：1) 仅使用自有对象的贪婪提交规则；2) 精细化依赖处理以减少重执行；3) 使用静态可推导的读/写提示指导执行；4) 基于优先级的调度器，优先处理能解锁其他事务的事务。

Result: 模拟执行实验表明，NEMO显著减少了冗余计算，在16个工作线程下，吞吐量比最先进的OCC方法Block-STM高42%，比悲观并发控制基线高61%。

Conclusion: NEMO通过结合OCC和对象数据模型，并引入四项核心创新，有效解决了区块链执行层在高竞争负载下的性能瓶颈问题，显著提升了吞吐量。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [94] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 开发了基于Kubernetes的Charm++ HPC应用弹性调度器，支持动态扩缩容以最大化云资源利用率并减少高优先级作业响应时间


<details>
  <summary>Details</summary>
Motivation: 云环境中的HPC应用需要动态扩缩容能力来优化资源利用，但传统MPI等并行编程模型缺乏原生支持，Charm++的迁移对象范式天然支持动态重缩放

Method: 创建Kubernetes operator运行Charm++应用，开发基于优先级的弹性作业调度器，根据集群状态动态调整作业规模

Result: 弹性调度器在最小化开销的情况下重缩放HPC作业，相比传统静态调度器展现出显著的性能提升

Conclusion: Charm++与Kubernetes结合为云环境HPC应用提供了高效的动态扩缩容解决方案，显著改善了资源利用率和作业响应时间

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [95] [Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks](https://arxiv.org/abs/2510.15215)
*Zhimin Qiu,Feng Liu,Yuxiao Wang,Chenrui Hu,Ziyu Cheng,Di Wu*

Main category: cs.DC

TL;DR: 提出基于图神经网络的分布式后端系统流量预测方法，通过图卷积和门控循环结构整合时空特征，显著提升预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉分布式系统中复杂的依赖关系和动态特征，需要更有效的建模方法来处理服务间的交互和时空演化。

Method: 将系统抽象为图结构，节点特征表示流量和资源状态，边表示服务交互。使用图卷积进行多阶特征传播，门控循环结构建模历史序列，时空联合建模模块融合图表示和时间依赖，解码器生成未来流量预测。

Result: 在公开分布式系统日志上的实验表明，该方法在不同预测时域和模型深度下均表现稳定且误差较低，MSE、RMSE、MAE和MAPE指标均优于主流基线方法。

Conclusion: 该方法显著提高了分布式后端系统流量预测的准确性和鲁棒性，验证了图神经网络在复杂系统建模中的潜力。

Abstract: This paper addresses the problem of traffic prediction in distributed backend
systems and proposes a graph neural network based modeling approach to overcome
the limitations of traditional models in capturing complex dependencies and
dynamic features. The system is abstracted as a graph with nodes and edges,
where node features represent traffic and resource states, and adjacency
relations describe service interactions. A graph convolution mechanism enables
multi order propagation and aggregation of node features, while a gated
recurrent structure models historical sequences dynamically, thus integrating
spatial structures with temporal evolution. A spatiotemporal joint modeling
module further fuses graph representation with temporal dependency, and a
decoder generates future traffic predictions. The model is trained with mean
squared error to minimize deviations from actual values. Experiments based on
public distributed system logs construct combined inputs of node features,
topology, and sequences, and compare the proposed method with mainstream
baselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed method
achieves stable performance and low error across different prediction horizons
and model depths, significantly improving the accuracy and robustness of
traffic forecasting in distributed backend systems and verifying the potential
of graph neural networks in complex system modeling.

</details>


### [96] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan控制器通过主动调节LLM应用输出长度来应对系统负载变化，在H100 GPU测试平台上实现8倍延迟降低和25%能耗减少


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用对底层基础设施负载不敏感，导致推理延迟增加和用户体验下降

Method: 开发beLLMan控制器，使LLM基础设施能主动向应用发送信号调节输出长度以响应系统负载变化

Result: 在真实H100 GPU测试平台上，beLLMan将端到端延迟降低8倍，能耗减少25%，同时处理请求量增加19%

Conclusion: beLLMan能有效控制LLM推理延迟并降低能耗，提升系统效率

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [97] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨本地与云端仿真环境的权衡，分析计算基础设施设置对执行性能和数据安全的影响，旨在提高远程仿真可信度并促进虚拟原型技术的采用。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统快速发展和AI算法复杂性增加，需要基于虚拟原型技术的强大软硬件协同设计方法。市场上仿真解决方案多样但各有优缺点，远程计算资源的普及为操作策略带来新可能性。

Method: 研究本地与云端仿真环境的二分法，重点关注可扩展性与隐私之间的权衡，分析计算基础设施设置对执行性能和数据安全的影响。

Result: 探讨了嵌入式AI开发工作流程，强调高效仿真在优化算法中的关键作用。

Conclusion: 通过提出的解决方案，可持续提高对远程仿真的信任度，促进虚拟原型实践的采用。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [98] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 本文研究离散负载均衡问题，提出一种基于匹配的局部平衡方案，能在任意图上快速达到恒定偏差（discrepancy=3），且时间复杂度和连续负载均衡的谱边界相匹配。


<details>
  <summary>Details</summary>
Motivation: 研究离散负载均衡问题，目标是证明离散负载均衡可以达到与连续负载均衡相似的性能，解决之前工作中偏差常数大且仅适用于规则图的问题。

Method: 提出基于匹配的局部平衡方案，包括三种模型：匹配模型（每轮随机生成新匹配）、平衡电路模型（应用固定匹配序列）和异步模型（随机选择边进行负载均衡）。在每轮中，匹配节点平均其令牌数，若和为奇数则随机分配多余令牌。

Result: 证明该离散平衡方案以高概率在渐进匹配连续负载均衡谱边界的时间轮数内达到偏差为3的负载分布，适用于任意图结构。

Conclusion: 离散负载均衡在一般模型下并不比连续负载均衡更难，且能实现小常数偏差，改进了先前工作的局限性。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [99] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 提出了UWFQ调度器，在Spark框架中实现用户级公平调度，通过虚拟公平排队系统和运行时分区技术，显著降低小作业响应时间并确保用户间资源公平分配。


<details>
  <summary>Details</summary>
Motivation: Spark内置调度器在工业分析环境中难以同时维护用户级公平性和低平均响应时间，现有解决方案偏向提交更多作业的用户，且缺乏对动态用户工作负载的适应性。

Method: 设计UWFQ调度器，模拟虚拟公平排队系统，基于预估完成时间调度作业，并引入运行时分区技术动态调整任务粒度以解决任务倾斜和优先级反转问题。

Result: UWFQ相比现有Spark调度器和最先进公平调度算法，将小作业的平均响应时间降低了高达74%。

Conclusion: UWFQ调度器在保持用户间公平性的同时，显著提升了作业性能，特别是在多用户共享的Spark环境中表现出色。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [100] [Retrofitting Service Dependency Discovery in Distributed Systems](https://arxiv.org/abs/2510.15490)
*Diogo Landau,Gijs Blanken,Jorge Barbosa,Nishant Saurabh*

Main category: cs.DC

TL;DR: 提出了一种名为XXXX的新型运行时系统，用于构建进程级服务依赖图，能够在复杂网络路由机制（包括NAT）下准确推断服务依赖关系，无需源代码插桩。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统中复杂的服务依赖关系容易导致级联故障，而NAT等网络路由技术会模糊实际服务主机，使现有基于网络元数据的运行时方法无法准确推断服务依赖关系。

Method: 通过在TCP数据包头部非破坏性地注入元数据来维护协议正确性，即使在没有接收代理的情况下也不会影响现有TCP连接，支持部分部署。

Result: 在9种场景下与3种最先进系统进行比较，涉及3种网络配置和3个微服务基准测试。XXXX是唯一在所有网络配置中表现一致的方法，在大多数场景下精度和召回率均达到100%。

Conclusion: XXXX系统能够有效解决NAT环境下的服务依赖图构建问题，具有非破坏性、协议兼容和部分部署友好的特点，在准确性和鲁棒性方面优于现有方法。

Abstract: Modern distributed systems rely on complex networks of interconnected
services, creating direct or indirect dependencies that can propagate faults
and cause cascading failures. To localize the root cause of performance
degradation in these environments, constructing a service dependency graph is
highly beneficial. However, building an accurate service dependency graph is
impaired by complex routing techniques, such as Network Address Translation
(NAT), an essential mechanism for connecting services across networks. NAT
obfuscates the actual hosts running the services, causing existing run-time
approaches that passively observe network metadata to fail in accurately
inferring service dependencies. To this end, this paper introduces XXXX, a
novel run-time system for constructing process-level service dependency graphs.
It operates without source code instrumentation and remains resilient under
complex network routing mechanisms, including NAT. XXXX implements a
non-disruptive method of injecting metadata onto a TCP packet's header that
maintains protocol correctness across host boundaries. In other words, if no
receiving agent is present, the instrumentation leaves existing TCP connections
unaffected, ensuring non-disruptive operation when it is partially deployed
across hosts. We evaluated XXXX extensively against three state-of-the-art
systems across nine scenarios, involving three network configurations
(NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX
was the only approach that performed consistently across networking
configurations. With regards to correctness, it performed on par with, or
better than, the state-of-the-art with precision and recall values of 100% in
the majority of the scenarios.

</details>


### [101] [PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training](https://arxiv.org/abs/2510.15596)
*Alicia Golden,Michael Kuchnik,Samuel Hsia,Zachary DeVito,Gu-Yeon Wei,David Brooks,Carole-Jean Wu*

Main category: cs.DC

TL;DR: PRISM是一个考虑大规模分布式训练随机特性的性能建模框架，通过统计方法为训练时间提供概率保证，帮助优化并行化策略和系统设计以减少性能变异性。


<details>
  <summary>Details</summary>
Motivation: 随着GPU训练规模扩展到数万级别，训练过程中的性能变异性不可避免，在64k GPU规模下已观察到9%的GPU时间变异性，需要理解和管理这种随机性对训练效率的影响。

Method: 提出PRISM性能建模框架，使用统计方法量化训练时间的概率保证，通过分析不同平台上的GPU微基准测试和GEMM工作负载，探索并行化方法和训练系统的设计空间。

Result: PRISM验证显示训练时间预测准确度达到20.8% KS距离，通过优化计算节点布局可获得1.26倍性能提升，发现优化AllGather和ReduceScatter等通信内核对减少训练步骤时间变异性贡献最大。

Conclusion: PRISM框架能够有效建模大规模分布式训练的性能变异性，为系统优化提供指导，通信内核优化是减少训练时间变异性的关键因素。

Abstract: Large model training beyond tens of thousands of GPUs is an uncharted
territory. At such scales, disruptions to the training process are not a matter
of if, but a matter of when -- a stochastic process degrading training
productivity. Dynamic runtime variation will become increasingly more frequent
as training scales up and GPUs are operated in increasingly power-limited and
thermally-stressed environments. At the 64k GPU scale, we already observed 9%
GPU time variability for frontier foundation model training. To understand
potential causes of variability, we analyze GPU microbenchmarks at scale across
a variety of platforms, showing up to 14% variation in GPU performance on GEMM
workloads depending on training hardware and deployed environment.
  Motivated by our analysis and the large design space around performance
variability, we present PRISM -- a performance modeling framework that
considers the stochastic nature of the large-scale distributed training. The
core of PRISM is the statistical method that provides a quantifiable measure
for probabilistic guarantees on training time. Using PRISM, we explore the
design and optimization space of distributed training, from parallelization
methods to next-generation training systems. PRISM is validated with
real-system measurement, showing training time prediction accuracy with 20.8%
Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending on
computation node placement, up to 1.26x performance improvement potential is
available if we factor in sensitivities of parallelization strategies to
variation. In addition, we use PRISM to identify kernels to optimize for
reducing performance variability and predict probability of slow-down for
large-scale jobs where variation is magnified. We find optimizing communication
kernels, such as AllGather and ReduceScatter, contribute most to minimizing
variability in training step time.

</details>


### [102] [GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters](https://arxiv.org/abs/2510.15652)
*Ahmad Raeisi,Mahdi Dolati,Sina Darabi,Sadegh Talebi,Patrick Eugster,Ahmad Khonsari*

Main category: cs.DC

TL;DR: 提出基于学习的异构集群资源管理架构，使用两个神经网络在线分配机器学习工作负载，最小化能耗并满足性能要求


<details>
  <summary>Details</summary>
Motivation: 机器学习对计算资源需求增长，异构硬件集群中设备能力、年限和能效差异大，升级最新硬件不可行，需要可持续利用现有混合代际资源

Method: 使用两个神经网络：第一个提供新模型在不同硬件上的利用率和对共置模型影响的初始估计；优化器基于估计分配资源；部署后监控实际性能，通过第二个神经网络用这些数据改进预测

Result: 开发出自适应迭代方法，随时间学习在异构深度学习集群中做出更有效的资源分配决策

Conclusion: 该学习型架构能够持续改进对硬件利用和共置场景的预测，实现异构集群中资源分配的优化

Abstract: The growing demand for computational resources in machine learning has made
efficient resource allocation a critical challenge, especially in heterogeneous
hardware clusters where devices vary in capability, age, and energy efficiency.
Upgrading to the latest hardware is often infeasible, making sustainable use of
existing, mixed-generation resources essential. In this paper, we propose a
learning-based architecture for managing machine learning workloads in
heterogeneous clusters. The system operates online, allocating resources to
incoming training or inference requests while minimizing energy consumption and
meeting performance requirements. It uses two neural networks: the first
provides initial estimates of how well a new model will utilize different
hardware types and how it will affect co-located models. An optimizer then
allocates resources based on these estimates. After deployment, the system
monitors real performance and uses this data to refine its predictions via a
second neural network. This updated model improves estimates not only for the
current hardware but also for hardware not initially allocated and for
co-location scenarios not yet observed. The result is an adaptive, iterative
approach that learns over time to make more effective resource allocation
decisions in heterogeneous deep learning clusters.

</details>


### [103] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 该论文证明了分布式量子计算中Lovász局部引理问题的下界为2^Ω(log* n)，这是该领域首个超常数下界结果


<details>
  <summary>Details</summary>
Motivation: 研究分布式量子计算中的Lovász局部引理问题，解决近期提出的开放性问题，填补该领域下界分析的空白

Method: 开发全新的下界技术，在比量子LOCAL模型更强的随机在线LOCAL模型中分析sinkless orientation这一LLL特例

Result: 获得了sinkless orientation和分布式LLL的2^Ω(log* n)下界，这是这些模型中的首个超常数下界

Conclusion: 提出的新技术有望成为证明后量子时代局部性研究中重要问题下界的首个通用技术

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [104] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: Funky是一个面向云原生应用的FPGA感知编排引擎，通过FPGA虚拟化、状态管理和编排组件解决了FPGA在云环境中缺乏虚拟化、隔离和抢占支持的问题。


<details>
  <summary>Details</summary>
Motivation: FPGA在云原生环境中的采用面临障碍，因为FPGA缺乏虚拟化、隔离和抢占支持，云提供商无法提供FPGA编排服务，导致可扩展性、灵活性和弹性不足。

Method: Funky通过三个核心贡献实现：(1) FPGA虚拟化创建轻量级沙箱；(2) FPGA状态管理支持任务抢占和检查点；(3) 遵循行业标准CRI/OCI规范的FPGA感知编排组件。

Result: 评估显示Funky只需修改3.4%源代码即可移植23个OpenCL应用，OCI镜像比AMD的FPGA容器小28.7倍，性能开销仅7.4%，同时提供强隔离和分布式FPGA编排。

Conclusion: Funky成功实现了高性能、高利用率的FPGA云原生编排，在大规模集群中展示了良好的可扩展性、容错能力和调度效率。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>
