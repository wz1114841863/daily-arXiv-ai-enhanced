<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 50]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Ordering-based Causal Discovery via Generalized Score Matching](https://arxiv.org/abs/2601.16249)
*Vy Vo,He Zhao,Trung Le,Edwin V. Bonilla,Dinh Phung*

Main category: cs.LG

TL;DR: 提出基于离散评分函数的叶节点判别准则，扩展评分匹配框架用于离散数据的因果发现，通过叶节点检测识别拓扑序，再通过边剪枝恢复图结构。


<details>
  <summary>Details</summary>
Motivation: 从纯观测数据学习DAG结构是跨科学领域的长期挑战。现有基于评分匹配的因果发现方法主要针对连续数据，需要扩展到离散数据场景。

Method: 扩展评分匹配框架到离散数据，提出基于离散评分函数的叶节点判别准则，通过叶节点检测识别拓扑序，然后进行边剪枝恢复图结构。

Result: 模拟和真实世界实验表明，该方法能够从观测的离散数据中准确推断真实因果序，识别出的排序能显著提升现有因果发现基线的准确性，在几乎所有设置中都有效。

Conclusion: 提出的基于离散评分函数的叶节点判别准则成功扩展了评分匹配框架到离散数据因果发现，为从纯观测离散数据学习DAG结构提供了有效方法。

Abstract: Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.

</details>


### [2] [Student Mental Health Screening via Fitbit Data Collected During the COVID-19 Pandemic](https://arxiv.org/abs/2601.16324)
*Rebecca Lopez,Avantika Shrestha,ML Tlachac,Kevin Hickey,Xingtong Guo,Shichao Liu,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 利用Fitbit可穿戴设备数据，通过机器学习模型筛查大学生抑郁、焦虑和压力，探索不同生理模态的最佳数据聚合水平和筛查效果。


<details>
  <summary>Details</summary>
Motivation: 大学生面临高压力导致焦虑抑郁高发，现有研究在心理测量工具多样性、生理模态种类和时间序列参数方面存在局限，需要更全面的可穿戴设备心理健康监测方案。

Method: 收集疫情期间大学生的StudentMEH Fitbit数据集，使用预测性机器学习模型，评估不同Fitbit模态（心率、睡眠等）对抑郁、焦虑和压力的筛查能力。

Result: 心率模态在压力筛查中F1分数达0.77，睡眠模态在抑郁筛查中达0.78，焦虑筛查F1分数最高达0.79，显示生理模态在心理健康筛查中的潜力。

Conclusion: 可穿戴设备支持持续心理健康监测具有潜力，识别最佳数据聚合水平和适当模态对不同心理疾病的筛查至关重要。

Abstract: College students experience many stressors, resulting in high levels of anxiety and depression. Wearable technology provides unobtrusive sensor data that can be used for the early detection of mental illness. However, current research is limited concerning the variety of psychological instruments administered, physiological modalities, and time series parameters. In this research, we collect the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from students at our institution during the pandemic. We provide a comprehensive assessment of the ability of predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities. Our findings indicate potential in physiological modalities such as heart rate and sleep to screen for mental illness with the F1 scores as high as 0.79 for anxiety, the former modality reaching 0.77 for stress screening, and the latter modality achieving 0.78 for depression. This research highlights the potential of wearable devices to support continuous mental health monitoring, the importance of identifying best data aggregation levels and appropriate modalities for screening for different mental ailments.

</details>


### [3] [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)
*Felipe Tobar,Elsa Cazelles*

Main category: cs.LG

TL;DR: 提出一种基于数据低维线性投影的高斯过程训练新目标函数——投影似然(PL)，相比精确GP训练和变分稀疏GP方法，在精度和计算效率上表现更优


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程训练在大规模数据集上计算复杂度高，需要更高效且保持准确性的训练方法

Method: 使用数据低维线性投影构建投影似然(PL)训练目标，推导信息损失闭式解，采用单位球面上的随机投影减少信息损失

Result: 投影似然方法在不同优化器、核函数和中等规模数据集上，相比精确GP训练和变分稀疏GP方法，在准确性和计算效率方面表现更优

Conclusion: 投影似然为高斯过程训练提供了一种高效且准确的替代方案，特别适用于中等规模数据集

Abstract: We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random projections on the unit sphere. We show the superiority of the PL, in terms of accuracy and computational efficiency, over the exact GP training and the variational free energy approach to sparse GPs over different optimisers, kernels and datasets of moderately large sizes.

</details>


### [4] [Analyzing Neural Network Information Flow Using Differential Geometry](https://arxiv.org/abs/2601.16366)
*Shuhang Tan,Jayson Sia,Paul Bogdan,Radoslav Ivanov*

Main category: cs.LG

TL;DR: 该论文从图论视角重新审视神经网络数据流问题，使用Ollivier-Ricci曲率识别关键连接，提出神经曲率概念用于模型分析


<details>
  <summary>Details</summary>
Motivation: 传统神经网络数据流分析基于信息论，本文希望通过图论视角提供新的分析工具，特别是利用图曲率来识别对模型性能至关重要的连接，为符号神经网络分析（如鲁棒性分析、模型修复）提供支持

Method: 1) 基于神经网络结构构建图，引入基于Ollivier-Ricci曲率的神经曲率概念；2) 根据输入样本的激活模式计算曲率；3) 使用神经曲率对边的重要性进行排序，通过剪枝实验验证方法有效性

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上的实验表明，移除负曲率边会快速降低神经网络性能，而正曲率边影响很小。与现有剪枝方法相比，该方法能识别更多不重要的边

Conclusion: 图曲率（特别是Ollivier-Ricci曲率）为神经网络数据流分析提供了有效的图论视角，神经曲率能够可靠地识别对模型功能至关重要的连接，为模型分析和优化提供了新工具

Abstract: This paper provides a fresh view of the neural network (NN) data flow problem, i.e., identifying the NN connections that are most important for the performance of the full model, through the lens of graph theory. Understanding the NN data flow provides a tool for symbolic NN analysis, e.g.,~robustness analysis or model repair. Unlike the standard approach to NN data flow analysis, which is based on information theory, we employ the notion of graph curvature, specifically Ollivier-Ricci curvature (ORC). The ORC has been successfully used to identify important graph edges in various domains such as road traffic analysis, biological and social networks. In particular, edges with negative ORC are considered bottlenecks and as such are critical to the graph's overall connectivity, whereas positive-ORC edges are not essential. We use this intuition for the case of NNs as well: we 1)~construct a graph induced by the NN structure and introduce the notion of neural curvature (NC) based on the ORC; 2)~calculate curvatures based on activation patterns for a set of input examples; 3)~aim to demonstrate that NC can indeed be used to rank edges according to their importance for the overall NN functionality. We evaluate our method through pruning experiments and show that removing negative-ORC edges quickly degrades the overall NN performance, whereas positive-ORC edges have little impact. The proposed method is evaluated on a variety of models trained on three image datasets, namely MNIST, CIFAR-10 and CIFAR-100. The results indicate that our method can identify a larger number of unimportant edges as compared to state-of-the-art pruning methods.

</details>


### [5] [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)
*Sihan Zeng,Sujay Bhatt,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 提出单循环一阶actor-critic算法，通过惩罚重构解决双层优化问题，其中上层优化平滑函数，下层是MDP策略优化，引入衰减熵正则实现无偏梯度估计。


<details>
  <summary>Details</summary>
Motivation: 现有双层优化和RL方法需要二阶信息、强正则化或低效的嵌套循环，需要更高效的单循环一阶算法来解决上层目标依赖下层最优策略的双层优化问题。

Method: 提出单循环一阶actor-critic算法，通过惩罚重构将双层问题转化为单层问题，在下层RL目标中引入衰减熵正则，实现渐进无偏的上层超梯度估计。

Result: 在特殊Polyak-Lojasiewicz条件下，通过新颖的下层残差分析，证明了算法在有限时间和有限样本下收敛到原始无正则双层优化问题的驻点。

Conclusion: 该方法在GridWorld目标位置问题和基于人类反馈的强化学习（RLHF）快乐推文生成任务上验证了性能，为双层优化提供了高效的单循环解决方案。

Abstract: We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).

</details>


### [6] [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)
*Zhaochun Li,Mingyang Yi,Yue Wang,Shisheng Cui,Yong Liu*

Main category: cs.LG

TL;DR: 该论文为RLHF对齐LLMs建立了泛化理论，在线性奖励模型下通过算法稳定性框架证明了策略模型的经验最优解具有O(n^{-1/2})的泛化界，结果可推广到梯度上升算法。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF及其变体在实践中被证明有效对齐大语言模型与人类意图，但这些方法在高维设置下的理论泛化性质尚未得到充分探索。现有工作主要基于奖励模型最大似然估计的一致性，而本文旨在建立与端到端学习实践一致的理论框架。

Method: 在线性奖励模型假设下，通过算法稳定性框架分析RLHF的泛化性质。在关键的特征覆盖条件下，证明策略模型经验最优解的泛化界，并将结果推广到梯度上升(GA)和随机梯度上升(SGA)算法获得的参数。

Result: 在特征覆盖条件下，策略模型经验最优解具有O(n^{-1/2})的泛化界。该结果可扩展到梯度上升算法获得的参数，为RLHF后LLMs观察到的经验泛化现象提供了新的理论证据。

Conclusion: 本文为RLHF对齐LLMs建立了端到端学习的泛化理论框架，在线性奖励模型和特征覆盖条件下证明了策略模型的泛化性能，为实践中观察到的RLHF泛化现象提供了理论支持。

Abstract: Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.

</details>


### [7] [Reasoning-Enhanced Rare-Event Prediction with Balanced Outcome Correction](https://arxiv.org/abs/2601.16406)
*Vitaly Bulgakov,Alexander Turchin*

Main category: cs.LG

TL;DR: LPCORP：一个两阶段框架，通过推理增强预测和基于置信度的结果修正来解决极端类别不平衡问题，在医疗和消费服务领域显著提升稀有事件预测性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融、可靠性工程等领域，稀有事件预测至关重要，但极端类别不平衡导致传统模型偏向多数类预测，限制了召回率、校准和实际应用价值。

Method: 提出LPCORP两阶段框架：1）推理模型从叙事输入生成增强预测；2）轻量级逻辑回归分类器评估并选择性修正这些输出，以减轻流行度驱动的偏差。

Result: 在真实医疗和消费服务数据集上，该方法将高度不平衡设置转化为平衡设置，同时保留原始样本数量且无需任何重采样策略。测试集评估显示性能显著提升，特别是在低流行度数据中已知薄弱的精确度方面。成本降低分析显示某些情况下预防性干预可减少50%以上费用。

Conclusion: LPCORP通过结合推理增强预测和置信度修正，有效解决了极端类别不平衡问题，在保持样本完整性的同时显著提升了稀有事件预测的精确度和实际应用价值。

Abstract: Rare-event prediction is critical in domains such as healthcare, finance, reliability engineering, customer support, aviation safety, where positive outcomes are infrequent yet potentially catastrophic. Extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness. We propose LPCORP (Low-Prevalence CORrector for Prediction)*, a two-stage framework that combines reasoningenhanced prediction with confidence-based outcome correction. A reasoning model first produces enriched predictions from narrative inputs, after which a lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias. We evaluate LPCORP on real-world datasets from medical and consumer service domains. The results show that this method transforms a highly imbalanced setting into a well-balanced one while preserving the original number of samples and without applying any resampling strategies. Test-set evaluation demonstrates substantially improved performance, particularly in precision, which is a known weakness in low-prevalence data. We further provide a costreduction analysis comparing the expenses associated with rare-event damage control without preventive measures to those incurred when low-cost, prediction-based preventive interventions are applied that showed more than 50% reduction in some cases. * Patent pending: U.S. Provisional 63/933,518, filed 8 December 2025.

</details>


### [8] [A Refinement of Vapnik--Chervonenkis' Theorem](https://arxiv.org/abs/2601.16411)
*A. Iosevich,A. Vagharshakyan,E. Wyman*

Main category: cs.LG

TL;DR: 该论文对经典的VC定理进行了改进，使用正态近似配合Berry-Esseen误差控制，在ε√n较大时获得了比传统VC估计更精确的中度偏差估计。


<details>
  <summary>Details</summary>
Motivation: 经典的VC定理是机器学习中的基础性成果，它建立了经验概率向理论概率一致收敛的充分条件，并给出了收敛速率的估计。作者希望改进经典证明中的概率部分，获得更精确的收敛速率估计。

Method: 作者重新审视了经典VC定理证明中的概率部分。不同于传统方法在最后一步应用Hoeffding不等式，他们采用了带有显式Berry-Esseen误差控制的正态近似方法。

Result: 该方法获得了比传统VC估计更精确的中度偏差估计。当ε√n较大时，在主导指数项中获得了额外的(ε√n)^{-1}阶因子，从而改进了收敛速率的估计精度。

Conclusion: 通过使用正态近似配合Berry-Esseen误差控制，作者改进了经典的VC定理估计，在ε√n较大的情况下获得了更精确的中度偏差估计，为机器学习中的收敛性分析提供了更精细的工具。

Abstract: Vapnik--Chervonenkis' theorem is a seminal result in machine learning. It establishes sufficient conditions for empirical probabilities to converge to theoretical probabilities, uniformly over families of events. It also provides an estimate for the rate of such uniform convergence.
  We revisit the probabilistic component of the classical argument. Instead of applying Hoeffding's inequality at the final step, we use a normal approximation with explicit Berry--Esseen error control. This yields a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order $(\varepsilon\sqrt{n})^{-1}$ in the leading exponential term when $\varepsilon\sqrt{n}$ is large.

</details>


### [9] [PyHealth 2.0: A Comprehensive Open-Source Toolkit for Accessible and Reproducible Clinical Deep Learning](https://arxiv.org/abs/2601.16414)
*John Wu,Yongda Fan,Zhenbang Wu,Paul Landes,Eric Schrock,Sayeed Sajjad Razin,Arjun Chatterjee,Naveen Baskaran,Joshua Steier,Andrea Fitzpatrick,Bilal Arif,Rian Atri,Jathurshan Pradeepkumar,Siddhartha Laghuvarapu,Junyi Gao,Adam R. Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: PyHealth 2.0是一个增强的临床深度学习工具包，旨在通过7行代码实现预测建模，解决临床AI研究中基线复现困难、计算成本高和领域专业知识需求等障碍。


<details>
  <summary>Details</summary>
Motivation: 临床AI研究面临基线复现困难、计算成本高、需要领域专业知识等持续障碍，这些限制了研究的可及性和可重复性。

Method: 开发PyHealth 2.0工具包，整合15+数据集、20+临床任务、25+模型、5+可解释性方法和不确定性量化（包括符合预测），支持信号、影像和电子健康记录等多种临床数据模态，并实现5+医疗编码标准的转换。

Result: PyHealth 2.0提供高达39倍的处理速度提升和20倍的内存使用降低，支持从16GB笔记本电脑到生产系统的多种计算资源，拥有400+成员的活跃开源社区，提供多语言支持（通过RHealth）。

Conclusion: PyHealth 2.0建立了一个开源基础和社区，推动可访问、可重复的医疗AI研究，可通过pip install pyhealth获取。

Abstract: Difficulty replicating baselines, high computational costs, and required domain expertise create persistent barriers to clinical AI research. To address these challenges, we introduce PyHealth 2.0, an enhanced clinical deep learning toolkit that enables predictive modeling in as few as 7 lines of code. PyHealth 2.0 offers three key contributions: (1) a comprehensive toolkit addressing reproducibility and compatibility challenges by unifying 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction within a single framework that supports diverse clinical data modalities - signals, imaging, and electronic health records - with translation of 5+ medical coding standards; (2) accessibility-focused design accommodating multimodal data and diverse computational resources with up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems; and (3) an active open-source community of 400+ members lowering domain expertise barriers through extensive documentation, reproducible research contributions, and collaborations with academic health systems and industry partners, including multi-language support via RHealth. PyHealth 2.0 establishes an open-source foundation and community advancing accessible, reproducible healthcare AI. Available at pip install pyhealth.

</details>


### [10] [Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance](https://arxiv.org/abs/2601.16425)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 该论文比较了贝叶斯实验设计（BED）中两种效用函数准则：KL散度和Wasserstein距离。通过玩具示例和源反演问题，发现KL散度在无模型误差时收敛更快，而Wasserstein距离在存在模型误差时提供更稳健的序列BED结果。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯实验设计（BED）为复杂物理系统的实验设计提供了原则性框架，但效用函数的选择一直是个活跃的研究课题。虽然KL散度是最常见的选择，但最近的研究提出了Wasserstein距离作为替代方案。本文旨在系统比较这两种准则，阐明它们在不同情况下的优劣。

Method: 首先使用玩具示例说明Wasserstein距离的问题：固定形状后验的Wasserstein距离值取决于其主要质量在支撑集中的相对位置，可能产生与信息增益无关的虚假奖励。然后通过BED文献中的经典源反演问题，系统比较这两种准则在不同条件下的表现。

Result: 研究发现：1）Wasserstein距离存在位置依赖性问题，特别是在使用非信息先验（如均匀分布）时；2）在没有模型误差的情况下，KL散度倾向于导致更快的收敛；3）如果模型误差不可忽略，Wasserstein度量提供更稳健的序列BED结果。

Conclusion: 本文阐明了KL散度和Wasserstein度量作为效用函数的权衡关系，为实际BED应用中如何选择合适的准则提供了指导。选择取决于具体应用场景：无模型误差时KL散度更优，存在模型误差时Wasserstein距离更稳健。

Abstract: Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.

</details>


### [11] [Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction](https://arxiv.org/abs/2601.16426)
*Shuang Wu,Meijie Wang,Lun Yu*

Main category: cs.LG

TL;DR: 该论文研究气味相关性质建模中的蒸汽压和气味阈值两个任务，采用骨架分割评估OOD能力，比较不同分子图特征和GNN架构，并提出"安全多任务"训练方法提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究气味相关性质建模中的两个关键任务：蒸汽压(VP)和气味阈值(OP)，重点关注模型在分布外(OOD)场景下的泛化能力，并探索有效的多任务学习方法。

Method: 采用Bemis-Murcko骨架分割评估OOD能力；引入A20/E17分子图特征(20维原子特征+17维键特征)；系统比较GINE和PNA图神经网络架构；提出"安全多任务"训练方法，以VP为主任务、OP为辅助任务，采用延迟激活+梯度裁剪+小权重策略。

Result: VP任务中，PNA架构在验证集上达到MSE≈0.21；OP单任务在相同骨架分割下，使用A20/E17特征和鲁棒训练达到MSE≈0.60-0.61；提出的"安全多任务"方法在避免损害主任务的同时，获得了最佳的VP泛化性能。

Conclusion: 该研究提供了完整的可重复实验、消融研究和误差相似性分析，证明了所提方法的有效性，同时讨论了数据噪声的影响和方法局限性，为气味相关性质建模提供了实用的解决方案。

Abstract: We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model's out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA backbones. The results show: for VP, PNA with a simple regression head achieves Val MSE $\approx$ 0.21 (normalized space); for the OP single task under the same scaffold split, using A20/E17 with robust training (Huber/winsor) achieves Val MSE $\approx$ 0.60-0.61. For multitask training, we propose a **"safe multitask"** approach: VP as the primary task and OP as the auxiliary task, using delayed activation + gradient clipping + small weight, which avoids harming the primary task and simultaneously yields the best VP generalization performance. This paper provides complete reproducible experiments, ablation studies, and error-similarity analysis while discussing the impact of data noise and method limitations.

</details>


### [12] [Endless Terminals: Scaling RL Environments for Terminal Agents](https://arxiv.org/abs/2601.16443)
*Kanishk Gandhi,Shivam Garg,Noah D. Goodman,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: Endless Terminals是一个完全自主的管道，能够程序化生成终端使用任务，无需人工标注。使用简单的PPO训练代理，在终端任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前终端基准测试主要用于评估而非训练，强化学习需要一个可扩展的管道而不仅仅是数据集。环境是自改进代理的瓶颈。

Method: 提出Endless Terminals管道，包含四个阶段：生成多样化任务描述、构建和验证容器化环境、生成完成测试、筛选可解任务。使用简单的PPO训练代理，仅使用二元回合级奖励，没有检索、多代理协调或专用工具。

Result: 获得了3255个任务，涵盖文件操作、日志管理、数据处理、脚本编写和数据库操作。训练后模型在保留开发集上显著提升：Llama-3.2-3B从4.0%提升到18.2%，Qwen2.5-7B从10.7%提升到53.3%，Qwen3-8B-openthinker-sft从42.6%提升到59.0%。在TerminalBench 2.0上也有显著提升。

Conclusion: 当环境可扩展时，简单的强化学习就能成功。Endless Terminals展示了程序化生成训练环境的价值，即使使用简单的训练方法也能显著提升代理性能。

Abstract: Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.

</details>


### [13] [Brownian ReLU(Br-ReLU): A New Activation Function for a Long-Short Term Memory (LSTM) Network](https://arxiv.org/abs/2601.16446)
*George Awiakye-Marfo,Elijah Agbosu,Victoria Mawuena Barns,Samuel Asante Gyamerah*

Main category: cs.LG

TL;DR: 该论文提出了BrownianReLU激活函数，通过布朗运动引入随机性来解决传统ReLU类激活函数在金融时间序列中梯度不稳定的问题，在LSTM网络中实现了更好的梯度传播和学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数如ReLU、LeakyReLU和PReLU在处理噪声、非平稳的金融时间序列时经常出现梯度不稳定问题，这限制了深度学习模型在金融预测中的性能。

Method: 提出BrownianReLU激活函数，基于布朗运动引入随机性，使用蒙特卡洛模拟实现平滑的自适应响应，特别针对负输入值缓解"死亡ReLU"问题，并在LSTM网络中应用。

Result: 在苹果、GCB、标普500等金融时间序列以及LendingClub贷款分类数据上的实验表明，BrownianReLU相比传统激活函数获得了更低的均方误差和更高的R²值，显示出更好的预测准确性和泛化能力。

Conclusion: BrownianReLU激活函数能有效改善金融时间序列建模中的梯度传播和学习稳定性，虽然分类任务中ROC-AUC指标有限，但激活函数选择显著影响准确性与敏感性的权衡，BrownianReLU提供了实际有意义的性能提升。

Abstract: Deep learning models are effective for sequential data modeling, yet commonly used activation functions such as ReLU, LeakyReLU, and PReLU often exhibit gradient instability when applied to noisy, non-stationary financial time series. This study introduces BrownianReLU, a stochastic activation function induced by Brownian motion that enhances gradient propagation and learning stability in Long Short-Term Memory (LSTM) networks. Using Monte Carlo simulation, BrownianReLU provides a smooth, adaptive response for negative inputs, mitigating the dying ReLU problem. The proposed activation is evaluated on financial time series from Apple, GCB, and the S&P 500, as well as LendingClub loan data for classification. Results show consistently lower Mean Squared Error and higher $R^2$ values, indicating improved predictive accuracy and generalization. Although ROC-AUC metric is limited in classification tasks, activation choice significantly affects the trade-off between accuracy and sensitivity, with Brownian ReLU and the selected activation functions yielding practically meaningful performance.

</details>


### [14] [On the Expressive Power of Floating-Point Transformers](https://arxiv.org/abs/2601.16450)
*Sejun Park,Yeachan Park,Geonho Hwang*

Main category: cs.LG

TL;DR: 浮点Transformer在有限精度下表现出与传统理论不同的表达能力：即使没有位置编码也能表示非置换等变函数，在序列长度有限时可表示所有置换等变函数，但长序列时则不能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer表达能力研究基于实数参数和精确运算，但实际计算机实现使用有限精度浮点数和含舍入误差的机器运算。需要研究浮点Transformer在现实计算约束下的表达能力。

Method: 研究浮点Transformer使用浮点参数和浮点运算时的表示能力。分析其在有无位置编码、不同序列长度下的表现，并探索最小等变结构。

Result: 1) 浮点Transformer即使没有位置编码也能表示一类非置换等变函数；2) 序列长度有限时可表示所有置换等变函数，但长序列时不能；3) 发现浮点Transformer的最小等变结构；4) 所有非平凡加法位置编码都会损害浮点Transformer的表示能力。

Conclusion: 浮点运算的有限精度特性显著改变了Transformer的理论表达能力，使其在现实计算约束下表现出与理想实数模型不同的行为，这对Transformer的理论理解和实际应用有重要启示。

Abstract: The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.

</details>


### [15] [On the Effects of Adversarial Perturbations on Distribution Robustness](https://arxiv.org/abs/2601.16464)
*Yipei Wang,Zhaoying Pan,Xiaoqian Wang*

Main category: cs.LG

TL;DR: 本文通过理论分析揭示了对抗鲁棒性与分布鲁棒性之间的权衡关系，并发现了一个微妙现象：在适度偏置的数据上，ℓ∞扰动可以提升分布鲁棒性，特别是在特征可分性较高时。


<details>
  <summary>Details</summary>
Motivation: 对抗鲁棒性和分布鲁棒性都旨在确保模型的可靠性能，但先前工作揭示了二者之间存在权衡。对抗训练可能增加对虚假特征的依赖，从而损害分布鲁棒性，特别是在某些代表性不足的子群体上。本文旨在深入理解这种权衡关系及其影响因素。

Method: 通过理论分析，研究在扰动数据上训练的模型，为每步对抗训练提供了一个可处理的替代方案。分析重点关注ℓ∞扰动对分布鲁棒性的影响，并探讨特征可分性在其中的作用。

Result: 研究发现：1）对抗鲁棒性和分布鲁棒性之间存在权衡关系；2）一个微妙现象：在适度偏置的数据上，ℓ∞扰动可以增加分布鲁棒性；3）在高度偏斜的数据上，当简单性偏置诱导对核心特征的依赖（表现为更高的特征可分性）时，分布鲁棒性的增益仍然存在。

Conclusion: 理论分析扩展了对权衡关系的理解，强调了权衡与特征可分性之间的相互作用。尽管在许多情况下权衡关系仍然存在，但忽视特征可分性的作用可能导致对鲁棒性的误导性结论。研究为理解对抗训练对分布鲁棒性的复杂影响提供了新的视角。

Abstract: Adversarial robustness refers to a model's ability to resist perturbation of inputs, while distribution robustness evaluates the performance of the model under data shifts. Although both aim to ensure reliable performance, prior work has revealed a tradeoff in distribution and adversarial robustness. Specifically, adversarial training might increase reliance on spurious features, which can harm distribution robustness, especially the performance on some underrepresented subgroups. We present a theoretical analysis of adversarial and distribution robustness that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data. In addition to the tradeoff, our work further identified a nuanced phenomenon that $\ell_\infty$ perturbations on data with moderate bias can yield an increase in distribution robustness. Moreover, the gain in distribution robustness remains on highly skewed data when simplicity bias induces reliance on the core feature, characterized as greater feature separability. Our theoretical analysis extends the understanding of the tradeoff by highlighting the interplay of the tradeoff and the feature separability. Despite the tradeoff that persists in many cases, overlooking the role of feature separability may lead to misleading conclusions about robustness.

</details>


### [16] [A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study](https://arxiv.org/abs/2601.16467)
*Maxwell Reynolds,Chaitanya Srinivasan,Vijay Cherupally,Michael Leone,Ke Yu,Li Sun,Tigmanshu Chaudhary,Andreas Pfenning,Kayhan Batmanghelich*

Main category: cs.LG

TL;DR: R-NCE自监督学习框架从结构MRI中提取比传统手工特征更强大的阿尔茨海默病生物标志物，在疾病分类、转化预测和淀粉样蛋白状态预测方面表现更优，且具有生物学相关性。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病早期检测和监测需要敏感且具有生物学基础的生物标志物。结构MRI虽然广泛可用，但通常依赖手工特征（如皮质厚度或体积），自监督学习方法在疾病分类等任务中表现不佳，需要开发更有效的框架。

Method: 提出残差噪声对比估计（R-NCE）自监督学习框架，整合辅助的FreeSurfer特征，同时最大化增强不变性信息。通过对比学习从结构MRI中提取特征，并与传统特征进行比较。

Result: R-NCE在多个基准测试中优于传统特征和现有自监督学习方法，包括AD转化预测。R-NCE衍生的脑年龄差距（BAG）测量显示高遗传性，与MAPT和IRAG1基因相关，在星形胶质细胞和少突胶质细胞中富集，表明对神经退行性和脑血管过程敏感。

Conclusion: R-NCE能够从结构MRI中发现比传统手工特征更强大且具有生物学相关性的阿尔茨海默病生物标志物，为早期检测和监测提供了有前景的方法。

Abstract: Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.

</details>


### [17] [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)
*Shenghong Cai,Yiqun Zhang,Xiaopeng Luo,Yiu-Ming Cheung,Hong Jia,Peng Liu*

Main category: cs.LG

TL;DR: 提出MCDC方法，通过多粒度竞争惩罚学习(MGCPL)和基于编码的聚类聚合(CAME)来处理分类数据的嵌套粒度聚类问题，具有线性时间复杂度和良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 分类数据在聚类分析中面临挑战，因为分类特征具有有限的定性可能值，在隐式离散距离空间中普遍存在嵌套粒度聚类效应（小紧凑聚类形成更大聚类），但无法像欧氏距离那样明确定义距离空间。

Method: 提出MCDC方法，包含两个核心组件：1) MGCPL算法让潜在聚类通过竞争惩罚学习在不同粒度上交互调整和收敛；2) CAME策略基于MGCPL编码将数据对象编码为嵌入表示，然后在嵌入空间进行最终聚类。

Result: MCDC能够自动探索多粒度聚类的嵌套分布，对各种领域的分类数据集具有高度鲁棒性。得益于线性时间复杂度，可扩展到大规模数据集，并可用于预分区数据集或计算节点以提升分布式计算性能。实验证明其在多个真实公共数据集上优于最先进方法。

Conclusion: 提出的MCDC方法通过MGCPL和CAME有效解决了分类数据聚类中的嵌套粒度聚类问题，具有自动探索多粒度分布、鲁棒性强、可扩展性好等优势，在分类数据聚类任务中表现出优越性能。

Abstract: Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data objects frequently overlap in space or subspace to form small compact clusters, and similar small clusters often form larger clusters. However, the distance space cannot be well-defined like the Euclidean distance due to the qualitative categorical data values, which brings great challenges to the cluster analysis of categorical data. In view of this, we design a Multi-Granular Competitive Penalization Learning (MGCPL) algorithm to allow potential clusters to interactively tune themselves and converge in stages with different numbers of naturally compact clusters. To leverage MGCPL, we also propose a Cluster Aggregation strategy based on MGCPL Encoding (CAME) to first encode the data objects according to the learned multi-granular distributions, and then perform final clustering on the embeddings. It turns out that the proposed MGCPL-guided Categorical Data Clustering (MCDC) approach is competent in automatically exploring the nested distribution of multi-granular clusters and highly robust to categorical data sets from various domains. Benefiting from its linear time complexity, MCDC is scalable to large-scale data sets and promising in pre-partitioning data sets or compute nodes for boosting distributed computing. Extensive experiments with statistical evidence demonstrate its superiority compared to state-of-the-art counterparts on various real public data sets.

</details>


### [18] [BoostFGL: Boosting Fairness in Federated Graph Learning](https://arxiv.org/abs/2601.16496)
*Zekai Chen,Kairui Yang,Xunkai Li,Henan Sun,Zhihan Zhang,Jia Li,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: BoostFGL是一个联邦图学习框架，通过客户端节点增强、拓扑增强和服务器端模型增强机制，解决联邦图学习中存在的公平性问题，显著提升弱势节点组的性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法虽然整体准确率高，但会掩盖弱势节点组的严重性能退化问题。这种不公平性源于三个耦合因素：标签偏向多数模式、消息传播中的拓扑混淆，以及困难客户端更新的聚合稀释。

Method: BoostFGL采用增强式框架，包含三个协调机制：1) 客户端节点增强：重塑本地训练信号，强调系统性服务不足的节点；2) 客户端拓扑增强：重新分配传播重点到可靠但未充分利用的结构，减弱误导性邻域；3) 服务器端模型增强：执行难度和可靠性感知的聚合，保留困难客户端的有效更新同时稳定全局模型。

Result: 在9个数据集上的广泛实验表明，BoostFGL显著提升了公平性，将Overall-F1提高了8.43%，同时在整体性能上保持与强基线方法的竞争力。

Conclusion: BoostFGL有效解决了联邦图学习中的公平性问题，通过协调的增强机制平衡了整体性能和弱势节点组的公平性，为公平感知的联邦图学习提供了有效解决方案。

Abstract: Federated graph learning (FGL) enables collaborative training of graph neural networks (GNNs) across decentralized subgraphs without exposing raw data. While existing FGL methods often achieve high overall accuracy, we show that this average performance can conceal severe degradation on disadvantaged node groups. From a fairness perspective, these disparities arise systematically from three coupled sources: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients. To address this, we propose \textbf{BoostFGL}, a boosting-style framework for fairness-aware FGL. BoostFGL introduces three coordinated mechanisms: \ding{182} \emph{Client-side node boosting}, which reshapes local training signals to emphasize systematically under-served nodes; \ding{183} \emph{Client-side topology boosting}, which reallocates propagation emphasis toward reliable yet underused structures and attenuates misleading neighborhoods; and \ding{184} \emph{Server-side model boosting}, which performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model. Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43\%, while preserving competitive overall performance against strong FGL baselines.

</details>


### [19] [kNN-Graph: An adaptive graph model for $k$-nearest neighbors](https://arxiv.org/abs/2601.16509)
*Jiaye Li,Gang Chen,Hang Xu,Shichao Zhang*

Main category: cs.LG

TL;DR: 提出一种自适应图模型，将HNSW图与预计算投票机制结合，将邻居选择和加权的计算负担完全转移到训练阶段，实现推理速度显著提升而不损失分类精度。


<details>
  <summary>Details</summary>
Motivation: kNN算法在大规模应用中面临推理速度与精度的计算权衡问题，现有近似最近邻解决方案虽然加速检索但会降低分类精度，且缺乏自适应选择最优邻居数量(k)的能力。

Method: 提出自适应图模型，整合分层可导航小世界(HNSW)图与预计算投票机制。高层图实现快速导航，低层图编码精确的节点特定决策边界和自适应邻居数量，将邻居选择和加权的计算完全转移到训练阶段。

Result: 在六个不同数据集上对八个最先进基线进行基准测试，该架构显著加速推理速度，实现实时性能，同时不损害分类精度。

Conclusion: 为解决kNN长期存在的推理瓶颈提供了可扩展、鲁棒的解决方案，为基于图的非参数学习建立了新的结构范式。

Abstract: The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning.

</details>


### [20] [Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)
*Enes Arda,Semih Cayci,Atilla Eryilmaz*

Main category: cs.LG

TL;DR: 浅层Transformer在核机制下训练时，宽度仅需与样本量对数相关，优化误差与序列长度无关，但内存需求随序列长度增长。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer为何表现优异仍具挑战性，因其非凸优化特性。本文旨在分析浅层Transformer在核机制下的优化特性，并与循环架构对比。

Method: 分析具有m个独立头的浅层Transformer，使用投影梯度下降在核机制下训练。理论分析结合数值验证，在教师-学生设置中检验预测的缩放规律。

Result: 发现两个关键结果：(1) 非渐近保证所需的宽度仅与样本量n成对数关系；(2) 优化误差与序列长度T无关。这与循环架构形成鲜明对比，后者优化误差可能随T指数增长。

Conclusion: Transformer在优化方面优于循环架构，代价是内存需求随序列长度增长。理论预测的缩放规律在数值实验中得到了验证。

Abstract: Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.

</details>


### [21] [Rethinking Large Language Models For Irregular Time Series Classification In Critical Care](https://arxiv.org/abs/2601.16516)
*Feixiang Zheng,Yu Wu,Cecilia Mascolo,Ting Dang*

Main category: cs.LG

TL;DR: 研究评估了LLM在ICU不规则时间序列数据上的应用，发现编码器设计比对齐策略更重要，但LLM方法训练时间长且在小样本场景下表现不佳


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在时间序列建模中显示出潜力，但其在ICU不规则数据（高缺失率）上的有效性尚未充分探索，需要系统评估LLM在ICU时间序列分析中的关键组件

Method: 建立系统测试平台，评估时间序列编码器和多模态对齐策略对LLM方法的影响，在ICU基准数据集上对比监督和自监督基线方法

Result: 编码器设计比对齐策略更关键，能显式建模不规则性的编码器性能提升12.8%；对齐策略影响较小，最佳策略提升2.9%；但LLM方法训练时间至少长10倍，小样本学习场景下表现不佳

Conclusion: LLM在ICU不规则时间序列分析中既有潜力也有局限，编码器设计是关键，但当前方法训练成本高且在小数据场景下效果有限

Abstract: Time series data from the Intensive Care Unit (ICU) provides critical information for patient monitoring. While recent advancements in applying Large Language Models (LLMs) to time series modeling (TSM) have shown great promise, their effectiveness on the irregular ICU data, characterized by particularly high rates of missing values, remains largely unexplored. This work investigates two key components underlying the success of LLMs for TSM: the time series encoder and the multimodal alignment strategy. To this end, we establish a systematic testbed to evaluate their impact across various state-of-the-art LLM-based methods on benchmark ICU datasets against strong supervised and self-supervised baselines. Results reveal that the encoder design is more critical than the alignment strategy. Encoders that explicitly model irregularity achieve substantial performance gains, yielding an average AUPRC increase of $12.8\%$ over the vanilla Transformer. While less impactful, the alignment strategy is also noteworthy, with the best-performing semantically rich, fusion-based strategy achieving a modest $2.9\%$ improvement over cross-attention. However, LLM-based methods require at least 10$\times$ longer training than the best-performing irregular supervised models, while delivering only comparable performance. They also underperform in data-scarce few-shot learning settings. These findings highlight both the promise and current limitations of LLMs for irregular ICU time series. The code is available at https://github.com/mHealthUnimelb/LLMTS.

</details>


### [22] [DANCE: Dynamic, Available, Neighbor-gated Condensation for Federated Text-Attributed Graphs](https://arxiv.org/abs/2601.16519)
*Zekai Chen,Haodong Lu,Xunkai Li,Henan Sun,Jia Li,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: DANCE提出了一种新的TAG-FGL范式，通过轮次式模型内循环图压缩刷新和可追溯证据包，解决了现有方法在计算开销、性能次优和可解释性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本属性图的联邦学习方法面临三个主要挑战：1) LLM处理长文本的高计算开销；2) 一次性图压缩导致性能次优；3) LLM压缩过程缺乏可解释性，难以追踪预测来源。

Method: DANCE采用轮次式、模型内循环的图压缩刷新策略，利用最新全局模型动态更新压缩表示；同时保存可本地检查的证据包，将预测追溯到特定邻居和源文本片段，增强可解释性。

Result: 在8个TAG数据集上，DANCE在8%压缩率下将准确率提升2.33%，同时比基线方法减少33.42%的token使用量。

Conclusion: DANCE通过动态图压缩和可追溯证据机制，有效解决了TAG-FGL中的计算开销、性能优化和可解释性问题，为实际应用提供了可行的解决方案。

Abstract: Federated graph learning (FGL) enables collaborative training on graph data across multiple clients. With the rise of large language models (LLMs), textual attributes in FGL graphs are gaining attention. Text-attributed graph federated learning (TAG-FGL) improves FGL by explicitly leveraging LLMs to process and integrate these textual features. However, current TAG-FGL methods face three main challenges: \textbf{(1) Overhead.} LLMs for processing long texts incur high token and computation costs. To make TAG-FGL practical, we introduce graph condensation (GC) to reduce computation load, but this choice also brings new issues. \textbf{(2) Suboptimal.} To reduce LLM overhead, we introduce GC into TAG-FGL by compressing multi-hop texts/neighborhoods into a condensed core with fixed LLM surrogates. However, this one-shot condensation is often not client-adaptive, leading to suboptimal performance. \textbf{(3) Interpretability.} LLM-based condensation further introduces a black-box bottleneck: summaries lack faithful attribution and clear grounding to specific source spans, making local inspection and auditing difficult. To address the above issues, we propose \textbf{DANCE}, a new TAG-FGL paradigm with GC. To improve \textbf{suboptimal} performance, DANCE performs round-wise, model-in-the-loop condensation refresh using the latest global model. To enhance \textbf{interpretability}, DANCE preserves provenance by storing locally inspectable evidence packs that trace predictions to selected neighbors and source text spans. Across 8 TAG datasets, DANCE improves accuracy by \textbf{2.33\%} at an \textbf{8\%} condensation ratio, with \textbf{33.42\%} fewer tokens than baselines.

</details>


### [23] [Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs](https://arxiv.org/abs/2601.16527)
*Xianya Fang,Feiyang Ren,Xiang Chen,Yu Tian,Zhen Bi,Haiyang Yu,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: SARE方法通过几何稳定化解决多模态大语言模型的对象幻觉问题，相比传统遗忘方法具有更强的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在对象幻觉问题，描述不存在的实体，损害可靠性。现有遗忘方法存在结构脆弱性缺陷，只能实现表面抑制，模型容易陷入尖锐最小值，在轻量级再学习后幻觉会灾难性重现。

Method: 提出SARE方法，将遗忘建模为有针对性最小-最大优化问题，使用Targeted-SAM机制显式地平坦化幻觉概念周围的损失景观。通过在最坏参数扰动下抑制幻觉，确保对权重变化的鲁棒移除。

Result: SARE在遗忘效果上显著优于基线方法，同时保持一般生成质量。关键的是，它能持久抑制幻觉对抗再学习和参数更新，验证了几何稳定化的有效性。

Conclusion: 通过几何稳定化方法解决多模态大语言模型的对象幻觉问题，SARE框架实现了鲁棒且持久的幻觉抑制，为模型可靠性提供了有效解决方案。

Abstract: Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

</details>


### [24] [A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics](https://arxiv.org/abs/2601.16531)
*Tao Lin*

Main category: cs.LG

TL;DR: 高频键碰撞不是Engram式条件记忆的主要瓶颈，消除碰撞的改进方案并未带来一致的验证损失提升，碰撞反而可能提供有益的隐式正则化。


<details>
  <summary>Details</summary>
Motivation: 研究高频键碰撞是否是Engram式条件记忆的主要瓶颈，探索通过消除碰撞是否能提升模型性能。

Method: 引入Engram-Nine，一个无碰撞的热层扩展，使用最小完美哈希函数映射最频繁的n-gram，同时保留原始多头部哈希查找作为冷层。在严格等参数设置下，通过路由分层评估（将每个token损失分解为热/冷层贡献）进行分析。

Result: 无碰撞设计并未一致改善验证损失。发现训练中存在一致的"热到冷优势翻转"现象：热（高频）位置初始损失较低，但冷位置最终超越它们。无碰撞配置比有碰撞基线更早发生翻转，表明碰撞起到隐式正则化作用。还发现门控不匹配问题：门控早期学习偏好热位置，但即使翻转后这种偏好仍然持续，将更高权重分配给损失更高的位置。

Conclusion: 单纯提高查找精度并不能保证更好的训练结果。主要限制可能在于门控信用分配而非索引准确性，碰撞引起的噪声可能提供有益的正则化，不应简单消除。

Abstract: We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.
  Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent "hot-to-cold advantage flip" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.
  Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.

</details>


### [25] [Understanding and Improving UMAP with Geometric and Topological Priors: The JORC-UMAP Algorithm](https://arxiv.org/abs/2601.16552)
*Xiaobin Li,Run Zhang*

Main category: cs.LG

TL;DR: JORC-UMAP：通过引入Ollivier-Ricci曲率和Jaccard相似度先验，改进UMAP的几何感知能力，减少拓扑撕裂和结构塌陷。


<details>
  <summary>Details</summary>
Motivation: UMAP在可视化高维数据时，其局部欧几里得距离假设经常无法捕捉内在流形几何结构，导致拓扑撕裂和结构塌陷。研究发现UMAP对k近邻图的敏感性是主要原因。

Method: 引入Ollivier-Ricci曲率作为几何先验，加强几何瓶颈处的边连接并减少冗余链接；同时结合Jaccard相似度作为拓扑先验确保邻域一致性，形成JORC-UMAP方法。

Result: 在合成和真实数据集上的实验表明，JORC-UMAP比标准UMAP和其他降维方法更有效地减少撕裂和塌陷，通过SVM准确率和三元组保持分数衡量，同时保持计算效率。

Conclusion: JORC-UMAP为UMAP提供了几何感知增强，能够更忠实地可视化数据，区分真实流形结构和虚假连接。

Abstract: Nonlinear dimensionality reduction techniques, particularly UMAP, are widely used for visualizing high-dimensional data. However, UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. We identify UMAP's sensitivity to the k-nearest neighbor graph as a key cause. To address this, we introduce Ollivier-Ricci curvature as a geometric prior, reinforcing edges at geometric bottlenecks and reducing redundant links. Since curvature estimation is noise-sensitive, we also incorporate a topological prior using Jaccard similarity to ensure neighborhood consistency. The resulting method, JORC-UMAP, better distinguishes true manifold structure from spurious connections. Experiments on synthetic and real-world datasets show that JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other DR methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency. This work offers a geometry-aware enhancement to UMAP for more faithful data visualization.

</details>


### [26] [Process-Tensor Tomography of SGD: Measuring Non-Markovian Memory via Back-Flow of Distinguishability](https://arxiv.org/abs/2601.16563)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 提出将神经训练视为过程张量，引入基于可区分性回流的训练记忆见证方法，证明实际SGD偏离马尔可夫理想化，为优化器、课程和调度比较提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 传统上神经网络训练常被理想化为马尔可夫过程，但实际训练中可能存在记忆效应。作者希望建立一种原则性的诊断方法来量化训练过程中的非马尔可夫性，验证"数据顺序重要"这一经验观察。

Method: 将训练建模为过程张量，通过可控干预序列（批次选择、数据增强、优化器微步）来观察训练结果。提出基于可区分性回流的记忆见证方法：在两步协议中比较单次干预与两次干预后的结果分布差异，当Δ_BF = D_2 - D_1 > 0时证明存在非马尔可夫性。使用TV/JS/Hellinger距离在固定探针集上测量softmax预测分布。

Result: 观察到一致的正向回流效应，具有紧密的自举置信区间。在更高动量、更大批次重叠和更多微步下效应增强，在因果中断（重置优化器状态）下效应消失，证明效应源于优化器/数据状态记忆。见证方法对TV/JS/Hellinger距离均稳健，计算成本低，无需架构修改。

Conclusion: 该工作提供了原则性诊断和实证证据，表明实际SGD训练偏离马尔可夫理想化。提出的框架为比较优化器、课程安排和调度策略提供了统一平台，将"数据顺序重要"转化为可测试的操作符。探索性案例展示了微观信号如何指导课程排序设计。

Abstract: This work proposes neural training as a \emph{process tensor}: a multi-time map that takes a sequence of controllable instruments (batch choices, augmentations, optimizer micro-steps) and returns an observable of the trained model. Building on this operational lens, we introduce a simple, model-agnostic witness of training memory based on \emph{back-flow of distinguishability}. In a controlled two-step protocol, we compare outcome distributions after one intervention versus two; the increase $Δ_{\mathrm{BF}} = D_2 - D_1>0$ (with $D\in\{\mathrm{TV}, \mathrm{JS}, \mathrm{H}\}$ measured on softmax predictions over a fixed probe set) certifies non-Markovianity. We observe consistent positive back-flow with tight bootstrap confidence intervals, amplification under higher momentum, larger batch overlap, and more micro-steps, and collapse under a \emph{causal break} (resetting optimizer state), directly attributing the effect to optimizer/data-state memory. The witness is robust across TV/JS/Hellinger, inexpensive to compute, and requires no architectural changes. We position this as a \emph{measurement} contribution: a principled diagnostic and empirical evidence that practical SGD deviates from the Markov idealization. An exploratory case study illustrates how the micro-level signal can inform curriculum orderings. "Data order matters" turns into a testable operator with confidence bounds, our framework offers a common stage to compare optimizers, curricula, and schedules through their induced training memory.

</details>


### [27] [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)
*Abdurahman Maarouf,Alket Bakiaj,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出kNN-ICL框架，利用LLM进行上下文学习预测早期初创公司成功，无需模型训练，仅需少量标注示例，在数据稀缺环境下优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 早期初创公司成功预测面临数据稀缺挑战，传统机器学习方法需要大量标注数据，而VC公司通常只有几十个早期初创公司的信息，限制了传统方法的有效性。

Method: 提出kNN-ICL框架，基于k近邻选择最相似的历史初创公司作为上下文示例，利用大型语言模型进行预测，无需模型训练，仅需少量标注示例。

Result: 使用Crunchbase真实数据，kNN-ICL方法在预测准确率上优于监督机器学习基线和普通上下文学习方法，仅需50个示例即可达到较高的平衡准确率。

Conclusion: 上下文学习可以作为VC公司在数据稀缺环境下的决策工具，kNN-ICL框架为早期初创公司成功预测提供了有效的解决方案。

Abstract: Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.

</details>


### [28] [Integrating Meteorological and Operational Data: A Novel Approach to Understanding Railway Delays in Finland](https://arxiv.org/abs/2601.16592)
*Vinicius Pozzobon Borin,Jean Michel de Souza Sant'Ana,Usama Raheel,Nurul Huda Mahmood*

Main category: cs.LG

TL;DR: 首个公开的芬兰铁路运营与气象观测数据集（2018-2024），整合了运营数据和209个气象站观测，包含28个特征、3850万条观测记录，用于铁路延误预测和天气影响分析。


<details>
  <summary>Details</summary>
Motivation: 现有数据集很少将气象信息与铁路运营数据整合，特别是在北欧地区，天气对铁路可靠性有重要影响。需要综合数据集来研究天气对铁路运营的影响。

Method: 整合芬兰Digitraffic铁路交通服务的运营指标和209个环境监测站的气象观测数据，使用Haversine距离进行时空对齐。包含28个工程特征，采用空间回退算法处理缺失数据、时间特征的循环编码、天气数据的稳健缩放。

Result: 数据集包含约3850万条观测记录，覆盖芬兰5915公里铁路网络。分析显示明显的季节性模式，冬季延误率超过25%，中北部芬兰存在高延误走廊的地理聚类。XGBoost回归基线实验在预测站点特定延误时达到2.73分钟的平均绝对误差。

Conclusion: 该数据集为铁路运营研究提供了灵活资源，支持列车延误预测、天气影响评估和基础设施脆弱性映射等多种应用，展示了在机器学习应用中的实用性。

Abstract: Train delays result from complex interactions between operational, technical, and environmental factors. While weather impacts railway reliability, particularly in Nordic regions, existing datasets rarely integrate meteorological information with operational train data. This study presents the first publicly available dataset combining Finnish railway operations with synchronized meteorological observations from 2018-2024. The dataset integrates operational metrics from Finland Digitraffic Railway Traffic Service with weather measurements from 209 environmental monitoring stations, using spatial-temporal alignment via Haversine distance. It encompasses 28 engineered features across operational variables and meteorological measurements, covering approximately 38.5 million observations from Finland's 5,915-kilometer rail network. Preprocessing includes strategic missing data handling through spatial fallback algorithms, cyclical encoding of temporal features, and robust scaling of weather data to address sensor outliers. Analysis reveals distinct seasonal patterns, with winter months exhibiting delay rates exceeding 25\% and geographic clustering of high-delay corridors in central and northern Finland. Furthermore, the work demonstrates applications of the data set in analysing the reliability of railway traffic in Finland. A baseline experiment using XGBoost regression achieved a Mean Absolute Error of 2.73 minutes for predicting station-specific delays, demonstrating the dataset's utility for machine learning applications. The dataset enables diverse applications, including train delay prediction, weather impact assessment, and infrastructure vulnerability mapping, providing researchers with a flexible resource for machine learning applications in railway operations research.

</details>


### [29] [E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory](https://arxiv.org/abs/2601.16622)
*Lin Huang,Chengxiang Huang,Ziang Wang,Yiyue Du,Chu Wang,Haocheng Lu,Yunyang Li,Xiaoli Liu,Arthur Jiang,Jia Zhang*

Main category: cs.LG

TL;DR: E2Former-V2通过代数稀疏化和硬件感知执行解决了EGNNs的可扩展性瓶颈，在保持预测性能的同时显著加速推理


<details>
  <summary>Details</summary>
Motivation: 当前等变图神经网络(EGNNs)在建模3D原子系统时面临可扩展性瓶颈，因为需要在每条边上显式构建几何特征或进行密集张量积运算

Method: 提出等变轴对齐稀疏化(EAAS)，利用SO(3)→SO(2)基变换将密集张量收缩转换为稀疏奇偶重索引操作；引入基于Triton内核的即时等变注意力机制，消除边张量并最大化SRAM利用率

Result: 在SPICE和OMol25数据集上保持可比预测性能的同时，内核实现相比标准版本获得20倍的TFLOPS提升，证明大型等变变压器可以在普通GPU平台上高效训练

Conclusion: E2Former-V2通过代数稀疏化和硬件优化成功解决了EGNNs的可扩展性问题，为大规模等变模型训练提供了高效解决方案

Abstract: Equivariant Graph Neural Networks (EGNNs) have become a widely used approach for modeling 3D atomistic systems. However, mainstream architectures face critical scalability bottlenecks due to the explicit construction of geometric features or dense tensor products on \textit{every} edge. To overcome this, we introduce \textbf{E2Former-V2}, a scalable architecture that integrates algebraic sparsity with hardware-aware execution. We first propose \textbf{E}quivariant \textbf{A}xis-\textbf{A}ligned \textbf{S}parsification (EAAS). EAAS builds on Wigner-$6j$ convolution by exploiting an $\mathrm{SO}(3) \rightarrow \mathrm{SO}(2)$ change of basis to transform computationally expensive dense tensor contractions into efficient, sparse parity re-indexing operations. Building on this representation, we introduce \textbf{On-the-Fly Equivariant Attention}, a fully node-centric mechanism implemented via a custom fused Triton kernel. By eliminating materialized edge tensors and maximizing SRAM utilization, our kernel achieves a \textbf{20$\times$ improvement in TFLOPS} compared to standard implementations. Extensive experiments on the SPICE and OMol25 datasets demonstrate that E2Former-V2 maintains comparable predictive performance while notably accelerating inference. This work demonstrates that large equivariant transformers can be trained efficiently using widely accessible GPU platforms. The code is avalible at https://github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.

</details>


### [30] [Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting](https://arxiv.org/abs/2601.16632)
*Haonan Yang,Jianchao Tang,Zhuo Li*

Main category: cs.LG

TL;DR: DPAD是一个模型无关的辅助框架，通过双原型自适应解耦机制，为时间序列预测模型提供模式解耦和上下文感知能力，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法通常学习静态的平均表示，缺乏动态解耦复杂交织时间模式的能力，无法进行上下文感知的适应。

Method: 提出双原型自适应解耦框架（DPAD），包含：1）动态双原型库（DDP），包括具有强时间先验的常见模式库和动态记忆关键罕见事件的罕见模式库；2）双路径上下文感知路由机制（DPC），从DDP中选择性检索上下文特定模式表示；3）解耦引导损失（DGLoss），确保每个原型库专注于其指定角色。

Result: 综合实验表明，DPAD在各种真实世界基准测试中，能够一致地提升最先进模型的预测性能和可靠性。

Conclusion: DPAD通过模式解耦和上下文感知适应，有效解决了现有方法学习静态平均表示的问题，为时间序列预测模型提供了强大的辅助能力。

Abstract: Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

</details>


### [31] [Provably Robust Bayesian Counterfactual Explanations under Model Changes](https://arxiv.org/abs/2601.16659)
*Jamie Duell,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出PSCE方法，生成具有概率安全保证的反事实解释，确保在高置信度和低预测方差下对模型更新的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型频繁更新，现有反事实解释容易失效或不可靠，需要能够适应模型变化的鲁棒解释方法。

Method: 基于贝叶斯原理的PSCE方法，生成δ-安全（高预测置信度）和ε-鲁棒（低预测方差）的反事实解释，通过不确定性感知约束集成到优化框架中。

Result: 在多个数据集上验证，相比现有贝叶斯反事实方法，PSCE生成的反事实解释更具合理性、区分性，且对模型变化具有可证明的鲁棒性。

Conclusion: PSCE方法为反事实解释提供了形式化的概率保证，能够在模型更新时保持解释的有效性和可靠性，解决了现有方法在动态环境中的局限性。

Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $δ$-safe, to ensure high predictive confidence, and $ε$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\langle δ, ε\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.

</details>


### [32] [Dynamic Expert-Guided Model Averaging for Causal Discovery](https://arxiv.org/abs/2601.16715)
*Adrick Tench,Thomas Demeester*

Main category: cs.LG

TL;DR: 提出一种利用动态专家知识（包括LLMs）来集成多种因果发现算法的灵活模型平均方法，在临床因果发现中表现出色


<details>
  <summary>Details</summary>
Motivation: 医疗领域需要准确的因果模型来增强预测模型的可解释性，支持反事实推理和治疗效果估计。但现有因果发现算法众多且没有明确最佳选择，同时现实应用常违反算法假设，过度依赖专家知识

Method: 提出灵活的模型平均方法，利用动态请求的专家知识（包括LLMs作为专家）来集成多样化的因果发现算法

Result: 实验证明该方法在使用不完美专家（如LLMs）的情况下，在干净和噪声数据上都有效。分析了不同专家正确度的影响，评估了LLMs在临床因果发现中的能力

Conclusion: 该方法为实际应用提供了有价值的见解，特别是在临床因果发现中，能够有效利用专家知识（包括LLMs）来集成多种算法，提高因果发现的准确性和鲁棒性

Abstract: Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance of competitive algorithms makes ensembling a natural choice for practical applications. At the same time, real-world use cases frequently face challenges that violate the assumptions of common causal discovery algorithms, forcing heavy reliance on expert knowledge. Inspired by recent work on dynamically requested expert knowledge and LLMs as experts, we present a flexible model averaging method leveraging dynamically requested expert knowledge to ensemble a diverse array of causal discovery algorithms. Experiments demonstrate the efficacy of our method with imperfect experts such as LLMs on both clean and noisy data. We also analyze the impact of different degrees of expert correctness and assess the capabilities of LLMs for clinical causal discovery, providing valuable insights for practitioners.

</details>


### [33] [Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing](https://arxiv.org/abs/2601.16812)
*Francesca Lanzillotta,Chiara Albisani,Davide Pucci,Daniele Baracchi,Alessandro Piva,Matteo Lapucci*

Main category: cs.LG

TL;DR: 提出一种序列惩罚方法，将数据样本处理要求作为严格约束而非任意惩罚项，在深度学习场景中具有收敛保证且实际可行。


<details>
  <summary>Details</summary>
Motivation: 在许多学习任务中，对单个数据样本的处理要求应该被形式化为优化问题中的严格约束，而不是通过任意惩罚项来实现。现有方法通常使用惩罚项来处理这些要求，但这种方法不够严谨。

Method: 提出一种序列惩罚方法，能够正确处理约束条件。该方法在深度学习场景中具有合理的假设条件，并提供了收敛性保证。

Result: 该方法在图像处理任务上的实验结果表明，该方法是实际可行的，能够在实践中使用。

Conclusion: 序列惩罚方法能够有效地将数据样本处理要求作为严格约束纳入学习过程，在深度学习场景中具有理论保证和实际可行性。

Abstract: In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.

</details>


### [34] [Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results](https://arxiv.org/abs/2601.16830)
*Andrew Thompson,Miles McCrory*

Main category: cs.LG

TL;DR: 本文推导了单隐藏层ReLU MLP在输入为多元高斯分布时输出均值和方差的精确解析表达式，无需级数展开


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖级数展开来近似MLP输出的不确定性传播，缺乏精确解析解。本文旨在为单隐藏层ReLU MLP提供输入为高斯分布时输出统计特性的精确表达式。

Method: 针对单隐藏层ReLU激活的多层感知机，在输入服从多元高斯分布的条件下，推导输出均值和方差的精确数学表达式。方法不依赖级数展开近似，而是直接解析求解。

Result: 获得了MLP输出均值和方差的精确解析表达式，为不确定性传播提供了严格的理论基础，相比级数展开方法具有更高的准确性。

Conclusion: 本文成功推导了单隐藏层ReLU MLP在输入为高斯分布时输出统计特性的精确解析解，为神经网络不确定性量化提供了新的理论工具。

Abstract: We give analytical results for propagation of uncertainty through trained multi-layer perceptrons (MLPs) with a single hidden layer and ReLU activation functions. More precisely, we give expressions for the mean and variance of the output when the input is multivariate Gaussian. In contrast to previous results, we obtain exact expressions without resort to a series expansion.

</details>


### [35] [Calibrated Probabilistic Interpolation for GEDI Biomass](https://arxiv.org/abs/2601.16834)
*Robin Young,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 该论文提出使用注意力神经过程（ANPs）替代传统机器学习方法，用于从GEDI激光雷达数据生成可靠的墙到墙生物量地图，通过显式建模空间协方差函数来改善不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如随机森林和XGBoost）在处理GEDI稀疏激光雷达观测数据时存在局限性：它们将空间预测视为独立事件，无法适应异质景观的变化难度，并且无法产生校准良好的预测区间。主要问题在于混淆了集成方差与偶然不确定性，并忽略了局部空间上下文。

Method: 引入注意力神经过程（ANPs），这是一种概率元学习框架，能够显式地将预测条件化于局部观测集和地理空间基础模型嵌入。与静态集成不同，ANPs学习灵活的空间协方差函数，使不确定性估计在复杂景观中扩展，在均匀区域收缩。该方法还展示了少样本适应能力，通过最小化本地数据实现跨区域性能恢复。

Result: 在从热带亚马逊森林到北方和高山生态系统的五个不同生物群落中验证了ANPs方法。结果显示ANPs在保持竞争性准确度的同时，实现了接近理想的不确定性校准。少样本适应实验表明，模型使用最小本地数据即可恢复跨区域转移中的大部分性能差距。

Conclusion: 该工作为大陆尺度地球观测提供了一个可扩展、理论严谨的替代方案，取代了传统的集成方差方法。ANPs框架能够更好地处理异质景观中的不确定性估计，为可靠的生物量制图提供了更强大的工具。

Abstract: Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.
  To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.

</details>


### [36] [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)
*Henri Nikoleit,Ankit Anand,Anurag Murty Naredla,Heiko Röglin*

Main category: cs.LG

TL;DR: 研究人员通过人类与LLM协作改进FunSearch算法输出，在组合优化问题上取得了突破性进展，打破了多项长期未改进的下界记录。


<details>
  <summary>Details</summary>
Motivation: 探索人类与大型语言模型（LLM）协作在理论计算机科学开放问题中的潜力，特别是在组合优化领域。研究人员希望利用LLM的初始模式发现能力，结合人类专家的数学严谨性，突破长期未改进的问题下界。

Method: 采用FunSearch算法（Romera-Paredes等人，Nature 2023）生成初始输出，然后通过人类专家迭代精炼这些输出。针对组合优化问题，特别是生成对抗性实例（使标准启发式算法表现不佳的实例），通过人类-LLM协作改进构造方法。

Result: 在多个组合优化问题上取得了最先进的下界结果：分层k-中值聚类、装箱问题、背包问题以及Lovász汽油问题的推广。其中一些问题已经十多年没有显著改进，尽管期间有间断性关注。这些突破展示了人类-LLM协作在打破长期障碍方面的有效性。

Conclusion: LLM提供了关键的初始模式，但人类专业知识对于将这些模式转化为数学严谨且有洞察力的构造至关重要。这项工作强调了LLM在数学和计算机科学研究中作为强大协作工具的价值，展示了人类专家监督如何有效从基于LLM的进化方法中提取算法洞察。

Abstract: We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.
  Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.

</details>


### [37] [Provably Learning Attention with Queries](https://arxiv.org/abs/2601.16873)
*Satwik Bhattamishra,Kulin Shah,Michael Hahn,Varun Kanade*

Main category: cs.LG

TL;DR: 本文研究通过黑盒访问Transformer序列模型输出的学习问题，针对单头注意力模型提出了精确学习算法，并分析了多头注意力的不可识别性问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何仅通过黑盒访问Transformer模型的输出（即自适应查询输入序列并观察实值输出）来学习其参数，这对于理解模型内部机制和模型逆向工程具有重要意义。

Method: 1. 针对单头softmax注意力回归器，提出基于O(d²)查询的基本算法；2. 利用压缩感知思想，针对头维度r≪d的情况，提出随机化算法仅需O(rd)查询；3. 分析噪声条件下的鲁棒性，在温和的范数和边界条件下仍可实现多项式查询的ε精度估计；4. 证明多头注意力参数在值查询下一般不可识别。

Result: 1. 单头注意力参数可通过O(d²)查询精确学习；2. 若存在ReLU前馈网络学习算法，则可扩展学习单层Transformer；3. 在r≪d时，通过压缩感知仅需O(rd)查询；4. 噪声条件下仍可实现多项式查询的ε精度估计；5. 证明多头注意力参数在值查询下不可识别，需要额外结构假设。

Conclusion: 单头注意力模型可通过高效算法从黑盒输出中学习参数，而多头注意力由于参数不可识别性需要额外假设。研究为Transformer模型的可解释性和逆向工程提供了理论基础。

Abstract: We study the problem of learning Transformer-based sequence models with black-box access to their outputs. In this setting, a learner may adaptively query the oracle with any sequence of vectors and observe the corresponding real-valued output. We begin with the simplest case, a single-head softmax-attention regressor. We show that for a model with width $d$, there is an elementary algorithm to learn the parameters of single-head attention exactly with $O(d^2)$ queries. Further, we show that if there exists an algorithm to learn ReLU feedforward networks (FFNs), then the single-head algorithm can be easily adapted to learn one-layer Transformers with single-head attention. Next, motivated by the regime where the head dimension $r \ll d$, we provide a randomised algorithm that learns single-head attention-based models with $O(rd)$ queries via compressed sensing arguments. We also study robustness to noisy oracle access, proving that under mild norm and margin conditions, the parameters can be estimated to $\varepsilon$ accuracy with a polynomial number of queries even when outputs are only provided up to additive tolerance. Finally, we show that multi-head attention parameters are not identifiable from value queries in general -- distinct parameterisations can induce the same input-output map. Hence, guarantees analogous to the single-head setting are impossible without additional structural assumptions.

</details>


### [38] [Theory of Minimal Weight Perturbations in Deep Networks and its Applications for Low-Rank Activated Backdoor Attacks](https://arxiv.org/abs/2601.16880)
*Bethan Evans,Jared Tanner*

Main category: cs.LG

TL;DR: 论文推导了深度神经网络实现指定输出变化所需的最小范数权重扰动，分析了其大小决定因素，并将单层精确公式与多层Lipschitz常数鲁棒性保证对比，发现两者同阶。应用这些结果到精度修改触发的后门攻击，建立了可证明的压缩阈值，并实证显示低秩压缩能可靠激活潜在后门同时保持全精度准确率。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络对权重扰动的敏感性，特别是为实现特定输出变化所需的最小扰动，这对于理解模型鲁棒性、防御后门攻击以及模型压缩具有重要意义。

Method: 推导了单层神经网络实现指定输出变化所需的最小范数权重扰动的精确公式，并与基于多层Lipschitz常数的通用鲁棒性保证进行对比分析。将理论结果应用于精度修改触发的后门攻击场景。

Result: 单层精确公式与多层Lipschitz常数保证具有相同阶数，表明两者保证效果相似。建立了可证明的压缩阈值，低于该阈值时精度修改触发的后门攻击无法成功。实证显示低秩压缩能可靠激活潜在后门同时保持全精度准确率。

Conclusion: 论文揭示了反向传播的边界如何控制层间敏感性，并提供了关于实现期望输出变化所需最小参数更新的可证明保证。这些结果为理解神经网络鲁棒性、防御后门攻击以及安全模型压缩提供了理论基础。

Abstract: The minimal norm weight perturbations of DNNs required to achieve a specified change in output are derived and the factors determining its size are discussed. These single-layer exact formulae are contrasted with more generic multi-layer Lipschitz constant based robustness guarantees; both are observed to be of the same order which indicates similar efficacy in their guarantees. These results are applied to precision-modification-activated backdoor attacks, establishing provable compression thresholds below which such attacks cannot succeed, and show empirically that low-rank compression can reliably activate latent backdoors while preserving full-precision accuracy. These expressions reveal how back-propagated margins govern layer-wise sensitivity and provide certifiable guarantees on the smallest parameter updates consistent with a desired output shift.

</details>


### [39] [Multigrade Neural Network Approximation](https://arxiv.org/abs/2601.16884)
*Shijun Zhang,Zuowei Shen,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL提出了一种逐层训练深度网络的方法，通过冻结已学习的层并训练新的残差块来减少近似误差，实现了可解释的层次化精炼过程，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练面临高度非凸和病态优化问题，而浅层网络（特别是单隐藏层ReLU模型）具有凸优化重构和全局保证。MGDL旨在利用这一洞察，通过逐层训练来改善稳定性并扩展到深度网络。

Method: MGDL采用逐层训练策略：冻结已学习的层，每个新的残差块仅用于减少剩余的近似误差。建立了算子理论基础，证明对于任何连续目标函数，存在固定宽度的多级ReLU方案，其残差在各级间严格递减并一致收敛到零。

Result: 理论证明MGDL能够保证残差在各级间严格递减并一致收敛到零，这是首次为逐层训练提供可证明的近似误差消失的理论保证。数值实验进一步验证了理论结果。

Conclusion: MGDL提供了一个有原则的深度网络结构化误差精炼框架，通过逐层训练实现了可解释的层次化精炼过程，并提供了严格的理论保证，为解决深度网络训练挑战提供了新思路。

Abstract: We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-hidden-layer $\texttt{ReLU}$ models, training admits convex reformulations with global guarantees, motivating learning paradigms that improve stability while scaling to depth. MGDL builds upon this insight by training deep networks grade by grade: previously learned grades are frozen, and each new residual block is trained solely to reduce the remaining approximation error, yielding an interpretable and stable hierarchical refinement process. We develop an operator-theoretic foundation for MGDL and prove that, for any continuous target function, there exists a fixed-width multigrade $\texttt{ReLU}$ scheme whose residuals decrease strictly across grades and converge uniformly to zero. To the best of our knowledge, this work provides the first rigorous theoretical guarantee that grade-wise training yields provable vanishing approximation error in deep networks. Numerical experiments further illustrate the theoretical results.

</details>


### [40] [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)
*Antesh Upadhyay,Sang Bin Moon,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: FedSGM是一个统一的联邦约束优化框架，解决了联邦学习中的四个主要挑战：函数约束、通信瓶颈、本地更新和部分客户端参与。该框架基于切换梯度方法，提供无投影、仅原变量的更新，避免了昂贵的对偶变量调优或内部求解器。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临四个主要挑战：1）函数约束的处理；2）通信瓶颈问题；3）本地多步更新的管理；4）部分客户端参与的现实场景。现有方法未能统一解决这些问题，特别是对于约束优化问题，需要同时处理这些挑战。

Method: 基于切换梯度方法，FedSGM采用无投影、仅原变量的更新策略。为处理通信限制，引入了双向误差反馈机制来纠正压缩引入的偏差，并明确理解压缩噪声与多步本地更新之间的相互作用。还提出了软切换版本以稳定在可行性边界附近的更新。

Result: 理论分析表明，平均迭代达到标准的O(1/√T)收敛速率，并提供了额外的高概率边界，将优化进展与部分参与引起的采样噪声解耦。实验验证了FedSGM在Neyman-Pearson分类和约束马尔可夫决策过程任务上的有效性。

Conclusion: FedSGM是第一个统一处理函数约束、压缩、多步本地更新和部分客户端参与的框架，为约束联邦学习建立了理论基础。该框架在理论和实验上都表现出色，为解决现实世界联邦学习中的约束优化问题提供了有效方案。

Abstract: We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\boldsymbol{\mathcal{O}}(1/\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.

</details>


### [41] [Embedding -based Crop Type Classification in the Groundnut Basin of Senegal](https://arxiv.org/abs/2601.16900)
*Madeline C. Lisaius,Srinivasan Keshav,Andrew Blake,Clement Atzberger*

Main category: cs.LG

TL;DR: 该研究评估了基于地理空间基础模型嵌入的方法在小农户作物类型制图中的应用，发现TESSERA方法在性能、合理性、可迁移性和可访问性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 目前大多数基于卫星的作物类型制图方法不适合小农户条件，需要开发适合小农户区域的实用方法。

Method: 建立了四个标准的评估框架（性能、合理性、可迁移性、可访问性），比较了TESSERA和AlphaEarth等地理空间基础模型嵌入方法与现有基线方法在塞内加尔花生盆地地区的表现。

Result: TESSERA方法在所有标准中表现最佳，在一个时间迁移示例中比次优方法准确率高28%，表明TESSERA嵌入是塞内加尔作物类型分类和制图的有效方法。

Conclusion: 基于TESSERA嵌入的方法满足小农户作物类型制图的需求标准，是适用于塞内加尔等小农户地区的有效解决方案。

Abstract: Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.

</details>


### [42] [GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints](https://arxiv.org/abs/2601.16905)
*Andy Zhu,Rongzhe Wei,Yupu Gu,Pan Li*

Main category: cs.LG

TL;DR: GRIP框架通过几何约束保持MoE模型路由稳定性，防止传统遗忘方法利用路由器漏洞进行表面遗忘，强制从专家参数中真正擦除知识。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法无法有效应用于MoE架构，因为它们利用路由器漏洞进行表面遗忘（通过操纵路由器将查询从知识专家处重定向），而不是真正擦除知识，导致模型效用损失和浅层遗忘。

Method: 提出几何路由不变性保持（GRIP）框架，核心是通过将路由器梯度更新投影到专家特定零空间来实现几何约束。这解耦了路由稳定性与参数刚性：离散专家选择保持稳定以保留知识，而连续路由器参数在零空间内保持可塑性，允许模型进行必要的内部重构以满足遗忘目标。

Result: 在大规模MoE模型上的实验表明，GRIP适配器消除了专家选择偏移（实现超过95%的路由稳定性），同时保持了所有测试遗忘方法的效用。通过防止现有算法利用MoE模型的路由器漏洞，GRIP将现有密集架构的遗忘研究适配到MoE架构。

Conclusion: GRIP作为一个算法无关的适配器框架，通过几何约束路由器参数更新，强制遗忘优化直接从专家参数中擦除知识，而不是利用路由器操纵的捷径，从而实现了对MoE架构的有效机器学习遗忘。

Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.

</details>


### [43] [The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning](https://arxiv.org/abs/2601.16906)
*Calarina Muslimani,Yunshu Du,Kenta Kawamoto,Kaushik Subramanian,Peter Stone,Peter Wurman*

Main category: cs.LG

TL;DR: TAC指标既能指导人工奖励函数调优，又能作为可微损失函数直接学习奖励模型，解决RL中奖励函数设计的难题。


<details>
  <summary>Details</summary>
Motivation: 强化学习成功依赖于准确反映任务目标的奖励函数，但人工设计奖励函数耗时且易出错。需要工具支持RL从业者设计合适的奖励权重，并探索更高效的奖励学习方法。

Method: 1) 使用轨迹对齐系数(TAC)评估奖励函数与专家偏好的匹配度；2) 进行人因研究，比较有/无TAC指导时RL从业者调优奖励函数的效果；3) 提出Soft-TAC作为TAC的可微近似，用作损失函数从人类偏好数据中学习奖励模型。

Result: 1) TAC指导下的奖励调优产生性能更好的奖励函数，且参与者认知负荷更低；2) 但人工设计仍耗时；3) Soft-TAC训练的奖励模型在Gran Turismo 7中成功捕捉偏好特定目标，产生比交叉熵损失更明显不同的行为策略。

Conclusion: TAC既可作为奖励调优的实用指导工具，也可作为复杂领域中奖励学习的目标函数，为解决RL奖励设计问题提供了双重解决方案。

Abstract: The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.

</details>


### [44] [Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces](https://arxiv.org/abs/2601.16907)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 本文提出一种基于等渗回归的单调校准方法，解决预训练嵌入空间中余弦相似度绝对值的系统性误校准问题，同时保持其排序相关性和局部稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练嵌入空间中的原始余弦相似度虽然与人类判断有强排序相关性，但由于各向异性导致绝对值系统性误校准：无论实际语义相关性如何，相似度分数都集中在狭窄的高相似度区间，限制了其作为定量度量的可解释性。

Method: 使用基于人类相似度判断训练的等渗回归构建单调变换，实现近乎完美的校准，同时保持排序相关性和局部稳定性（在七种扰动类型上达到98%）。该方法不改变余弦相似度本身，而是通过单调校准恢复其绝对值的可解释性。

Result: 该方法在保持排序相关性的同时实现了近乎完美的校准，局部稳定性达到98%（跨七种扰动类型）。作者将等渗校准表征为保序重参数化，并证明所有基于顺序的构造（角度排序、最近邻、阈值图和基于分位数的决策）在此变换下保持不变。

Conclusion: 等渗校准方法有效解决了余弦相似度绝对值的误校准问题，恢复了其作为定量度量的可解释性，同时保持了原有的排序特性和几何结构，无需重新计算所有嵌入向量。

Abstract: While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.
  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.
  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.

</details>


### [45] [Group-realizable multi-group learning by minimizing empirical risk](https://arxiv.org/abs/2601.16922)
*Navid Ardeshir,Samuel Deng,Daniel Hsu,Jingwen Liu*

Main category: cs.LG

TL;DR: 多组学习在组可实现设置下的样本复杂度优于不可知设置，即使组族无限但具有有限VC维。改进通过组可实现概念类的经验风险最小化实现，但计算不可行，建议使用非适当学习替代。


<details>
  <summary>Details</summary>
Motivation: 研究多组学习在不同设置下的样本复杂度差异，探索在组族无限但具有有限VC维的情况下，组可实现设置是否比不可知设置具有更好的样本效率。

Method: 使用组可实现概念类的经验风险最小化方法，分析其样本复杂度。同时证明该方法计算不可行，并提出基于非适当学习的替代方案。

Result: 在组可实现设置下，即使组族无限但具有有限VC维，多组学习的样本复杂度确实优于不可知设置。然而，实现组可实现概念类的经验风险最小化是计算不可行的。

Conclusion: 多组学习在组可实现设置下具有更好的样本效率，但需要采用非适当学习等替代方法来解决计算不可行性问题。

Abstract: The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach is also shown to be computationally intractable, and an alternative approach is suggested based on improper learning.

</details>


### [46] [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](https://arxiv.org/abs/2601.16936)
*Anton Zamyatin,Patrick Indri,Sagar Malhotra,Thomas Gärtner*

Main category: cs.LG

TL;DR: BatchEnsemble虽然旨在以低成本提供集成式不确定性估计，但实际表现更像单一模型而非真正集成，在准确率、校准和OOD检测方面与单一模型基线相近，未能达到Deep Ensembles的性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限和低延迟场景中，需要高效获取不确定性估计。Deep Ensembles能提供稳健的认知不确定性，但需要训练多个完整模型，成本过高。

Method: BatchEnsemble通过在共享基础网络上应用学习的秩-1扰动，旨在以远低于Deep Ensembles的参数和内存成本提供集成式不确定性估计。

Result: BatchEnsemble不仅表现不如Deep Ensembles，在CIFAR10/10C/SVHN上的准确率、校准和OOD检测方面与单一模型基线相近。MNIST上的控制研究发现成员在函数和参数空间几乎相同，表明实现不同预测模式的能力有限。

Conclusion: BatchEnsemble的行为更像单一模型而非真正集成，未能有效提供集成式不确定性估计的优势。

Abstract: In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.

</details>


### [47] [3D Molecule Generation from Rigid Motifs via SE(3) Flows](https://arxiv.org/abs/2601.16955)
*Roman Poletukhin,Marcel Kollovieh,Eike Eberhard,Stephan Günnemann*

Main category: cs.LG

TL;DR: 该论文提出了一种基于刚性基元的三维分子生成方法，将分子表示为刚性基元集合，使用SE(3)-等变生成模型，相比原子级方法实现了更快的生成速度和更紧凑的表示。


<details>
  <summary>Details</summary>
Motivation: 当前三维分子生成通常在原子级别进行，而分子图生成技术常考虑片段作为结构单元。受蛋白质结构生成中框架方法的启发，作者希望将片段化思想扩展到三维分子生成，将分子视为刚性基元集合，以提高生成效率和表示紧凑性。

Method: 将分子表示为刚性基元（刚性体基序）集合，采用SE(3)-等变生成模型进行三维分子生成。该方法基于框架的蛋白质结构生成技术，将片段化思想扩展到三维空间，实现了从刚性基元进行从头分子生成。

Result: 在多个基准测试中取得与最先进方法相当或更优的结果，在GEOM-Drugs上原子稳定性表现更佳。相比标准原子级方法，生成步骤减少2-10倍，分子表示压缩3.5倍。

Conclusion: 基于刚性基元的三维分子生成方法在保持生成质量的同时，显著提高了生成效率和表示紧凑性，为分子设计提供了更高效的工具。

Abstract: Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.

</details>


### [48] [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)
*Mahdi Karami,Ali Ghodsi*

Main category: cs.LG

TL;DR: ARMD模型通过将掩码扩散过程重构为块级因果模型，统一了自回归模型的训练效率和扩散模型的并行生成能力，在语言建模基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在语言建模中存在性能差距，需要更多训练迭代。研究旨在弥合自回归模型和扩散模型之间的差距，结合两者的优势。

Method: 将掩码扩散过程重构为块级因果模型，设计严格因果、置换等变架构，支持单次并行前向计算所有条件概率。采用渐进置换训练方案和跨步并行生成策略。

Result: 在标准语言建模基准上达到最先进性能，超越现有扩散基线，显著减少训练步骤，为并行文本生成设立新基准。

Conclusion: ARMD模型有效弥合了并行和顺序解码之间的性能差距，统一了自回归训练效率和扩散并行生成的优势。

Abstract: Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.

</details>


### [49] [Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection](https://arxiv.org/abs/2601.16976)
*Estela Sánchez-Carballo,Francisco M. Melgarejo-Meseguer,José Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: 使用潜在扩散模型(LDM)进行物联网攻击数据增强，解决机器学习入侵检测系统中的类别不平衡问题，相比现有方法在样本质量、多样性和计算效率方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 物联网环境中基于机器学习的入侵检测系统面临严重的类别不平衡问题（良性流量远多于攻击流量），现有数据增强方法在样本保真度、多样性和计算效率方面存在不足。

Method: 提出使用潜在扩散模型(LDM)进行物联网攻击数据增强，在潜在空间而非原始数据空间进行扩散过程，并与最先进的基线方法进行全面比较。

Result: LDM生成样本平衡训练数据后，入侵检测系统性能显著提升：DDoS和Mirai攻击的F1分数达到0.99，在所有评估的攻击类型中一致优于竞争方法；同时保持了特征依赖关系，生成多样样本，采样时间比直接在数据空间操作的扩散模型减少约25%。

Conclusion: 潜在扩散模型是合成物联网攻击数据的有效且可扩展解决方案，能显著减轻物联网场景中基于机器学习的入侵检测系统的类别不平衡影响。

Abstract: Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.

</details>


### [50] [A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs](https://arxiv.org/abs/2601.16979)
*Dayal Singh Kalra,Jean-Christophe Gagnon-Audet,Andrey Gromov,Ishita Mediratta,Kelvin Niu,Alexander H Miller,Michael Shvartsman*

Main category: cs.LG

TL;DR: 提出了一种计算高效的临界锐度(λc)度量，用于替代计算昂贵的Hessian锐度，首次在7B参数规模上展示了渐进锐化和稳定性边缘现象，并引入相对临界锐度分析预训练到微调的转换。


<details>
  <summary>Details</summary>
Motivation: 损失函数曲率演化对分析神经网络训练动态至关重要，但传统Hessian锐度(λmax^H)计算成本过高，无法应用于大语言模型。需要开发可扩展的曲率度量方法。

Method: 提出临界锐度(λc)，仅需少于10次前向传播即可计算，利用更新方向Δθ。进一步引入相对临界锐度(λc^{1→2})，量化优化一个损失函数时的另一个损失函数的曲率。

Result: 首次在7B参数规模的OLMo-2模型上展示了渐进锐化和稳定性边缘现象，覆盖预训练和中期训练。相对临界锐度成功分析了预训练到微调的转换，并指导数据混合策略。

Conclusion: 临界锐度为实践者提供了实用的曲率动态诊断工具，指导大规模训练中的数据组合选择。研究表明可扩展的曲率度量能为大规模训练提供可操作的见解。

Abstract: Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\textit{relative critical sharpness}$ ($λ_c^{1\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [51] [AERO: Adaptive and Efficient Runtime-Aware OTA Updates for Energy-Harvesting IoT](https://arxiv.org/abs/2601.16935)
*Wei Wei,Jingye Xu,Sahidul Islam,Dakai Zhu,Chen Pan,Mimi Xie*

Main category: cs.AR

TL;DR: AERO是一种自适应、高效、运行时感知的OTA更新机制，专门为能量收集物联网设备设计，通过将更新任务集成到DAG中并智能调度，解决间歇供电环境下的更新一致性问题。


<details>
  <summary>Details</summary>
Motivation: 能量收集物联网设备面临间歇性能源供应，导致任务执行中断，传统OTA更新机制依赖重启且开销大，不适合间歇供电系统。现有实时OTA更新技术虽减少重启开销，但缺乏确保更新与运行时执行交互一致性的机制。

Method: 提出AERO机制：将更新任务集成到设备的有向无环图(DAG)中，在能量和时间约束下与常规任务一起调度；通过识别更新影响的执行区域并动态调整依赖关系，确保一致的更新集成，同时适应间歇性能源可用性。

Result: 在代表性工作负载上的实验表明，相比现有实时更新方法，AERO在更新可靠性和效率方面都有显著提升。

Conclusion: AERO为能量收集物联网设备提供了一种自适应、高效、运行时感知的OTA更新解决方案，能够确保更新一致性并适应间歇性能源环境，解决了该领域的关键挑战。

Abstract: Energy-harvesting (EH) Internet of Things (IoT) devices operate under intermittent energy availability, which disrupts task execution and makes energy-intensive over-the-air (OTA) updates particularly challenging. Conventional OTA update mechanisms rely on reboots and incur significant overhead, rendering them unsuitable for intermittently powered systems. Recent live OTA update techniques reduce reboot overhead but still lack mechanisms to ensure consistency when updates interact with runtime execution. This paper presents AERO, an Adaptive and Efficient Runtime-Aware OTA update mechanism that integrates update tasks into the device's Directed Acyclic Graph (DAG) and schedules them alongside routine tasks under energy and timing constraints. By identifying update-affected execution regions and dynamically adjusting dependencies, AERO ensures consistent up date integration while adapting to intermittent energy availability. Experiments on representative workloads demonstrate improved update reliability and efficiency compared to existing live update approaches.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [52] [Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple](https://arxiv.org/abs/2601.16294)
*Evangelos Georganas,Alexander Heinecke,Pradeep Dubey*

Main category: cs.DC

TL;DR: 该论文提出使用广义希尔伯特曲线（空间填充曲线）进行矩阵乘法计算空间划分，实现平台无关和形状无关的高数据局部性方案，并扩展到通信避免算法，在多个CPU平台上超越厂商库性能。


<details>
  <summary>Details</summary>
Motivation: 现代CPU平台上的矩阵乘法加速器具有高FLOP/Byte机器平衡，使得实现最优矩阵乘法具有挑战性。厂商库需要针对不同平台（核心数、内存层次、缓存大小）和矩阵形状进行繁琐调优，导致性能"玻璃下巴"问题。

Method: 使用广义希尔伯特曲线（空间填充曲线）将多维计算空间映射到一维顺序，保持高维空间中邻近点在低维顺序中的接近性。通过SFC划分矩阵乘法计算空间，实现平台无关和形状无关的数据局部性方案，并扩展到通信避免算法以减少关键路径上的通信/数据移动。

Result: 在多个CPU平台上实现了最先进的结果，代码紧凑（约30行），对于一系列GEMM形状，几何平均加速比超过厂商库高达2倍。

Conclusion: 空间填充曲线方法能够有效解决矩阵乘法中的繁琐调优问题，提供平台无关和形状无关的高性能方案，通过通信避免算法进一步优化数据移动，在实际应用中显著超越现有厂商库性能。

Abstract: General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance "glass jaws". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes.

</details>


### [53] [Consensus In Asynchrony](https://arxiv.org/abs/2601.16460)
*Ivan Klianev*

Main category: cs.DC

TL;DR: 该论文证明了事件同步足以解决异步环境中的确定性容错共识问题，提出了一个能达成有效向量共识的算法，并重新审视了FLP不可能性定理的隐含假设。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为在异步系统中无法实现确定性容错共识（FLP不可能性定理），但作者认为FLP定理依赖于某些隐含假设，这些假设可能在实际中不成立，因此探索在事件同步条件下实现共识的可能性。

Method: 提出了一个基于事件同步的算法，该算法能够容忍一个节点崩溃，并达成有效的向量共识。同时分析了FLP定理的隐含假设，区分了数据无关和数据相关两种共识类型。

Result: 算法实现了安全性、活性和容错性（容忍一个崩溃）。实验结果表明FLP定理的第三个隐含假设缺乏证据支持，从而为异步系统中的确定性共识提供了可能性。

Conclusion: 事件同步足以实现异步系统中的确定性容错共识。FLP不可能性定理依赖于三个隐含假设，其中第三个假设缺乏实证支持，这为绕过FLP限制提供了理论依据。

Abstract: We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support.

</details>


### [54] [W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs](https://arxiv.org/abs/2601.16536)
*Yuanhong He,Peiyu Niu,Jun Chen,Chenchen Zhang,Chao Yang*

Main category: cs.DC

TL;DR: 本文提出了首个针对华为昇腾910 NPU的W4A16矩阵乘法内核，通过向量核心动态反量化、立方核心GEMM和Split-K并行化，在LLM解码场景下实现1.01-1.74倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，W4A16量化对减少内存占用至关重要，但在华为昇腾910 NPU上高效部署面临挑战，因为该架构缺乏原生混合精度支持且采用解耦计算架构。

Method: 设计专门针对昇腾910 NPU的W4A16矩阵乘法内核：利用向量核心进行INT4到FP16的实时反量化，立方核心执行高吞吐量GEMM，采用Split-K并行化缓解内存延迟。

Result: 在不同矩阵形状和批大小下评估，当K>>N（LLM解码典型场景）时优于数据并行方法，加速比1.01-1.74倍。分析发现主要瓶颈是权重的额外全局内存传输而非反量化计算，W4A16相比原生FP16最大加速1.48倍。

Conclusion: 该方法为在各种领域专用加速器上高效部署量化大语言模型奠定了坚实基础，并提供了有价值的见解。

Abstract: As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators.

</details>


### [55] [Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2601.16635)
*Julian Legler*

Main category: cs.DC

TL;DR: GOXN是一个用于Kubernetes微服务的能耗实验引擎，能够在服务级别量化计算、网络和存储能耗，研究发现忽略网络和存储能耗会导致辅助服务能耗低估高达63%


<details>
  <summary>Details</summary>
Motivation: 虽然云原生环境中已经能够进行细粒度的能耗测量（如容器或进程级别），但微服务应用的服务级别能耗测量仍然不足。分布式设置中必须包含计算、网络和存储能耗，否则会低估实际能耗

Method: 开发了GOXN（Green Observability eXperiment eNginE）能耗实验引擎，用于Kubernetes微服务。使用Kepler和cAdvisor收集指标，通过累加式能耗模型从容器级别数据推导出服务级别能耗。在OpenTelemetry Demo上评估不同配置（监控、追踪、服务网格）和稳定合成负载

Result: 排除网络和存储能耗会导致辅助服务能耗低估高达63%。高追踪负载会使能耗主导从计算转向网络和存储。服务级别能耗测量能够更准确地反映微服务应用的能耗分布

Conclusion: 微服务应用的服务级别能耗测量需要综合考虑计算、网络和存储能耗。GOXN引擎提供了一种有效的量化方法，揭示了在分布式环境中忽略网络和存储能耗会导致显著低估，特别是在高追踪负载下能耗分布会发生变化

Abstract: Recent advancements enable fine-grained energy measurements in cloud-native environments (e.g., at container or process level) beyond traditional coarse-grained scopes. However, service-level energy measurement for microservice-based applications remains underexplored. Such measurements must include compute, network, and storage energy to avoid underestimating consumption in distributed setups. We present GOXN (Green Observability eXperiment eNginE), an energy experimentation engine for Kubernetes-based microservices that quantifies compute, network, and storage energy at the service level. Using GOXN, we evaluated the OpenTelemetry Demo under varying configurations (monitoring, tracing, service mesh) and steady synthetic load, collecting metrics from Kepler and cAdvisor. Our additive energy model derives service-level energy from container-level data. Results show that excluding network and storage can underestimate auxiliary-service energy by up to 63%, and that high tracing loads shift energy dominance toward network and storage.

</details>


### [56] [GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms](https://arxiv.org/abs/2601.16637)
*Jun Doi,Tomonori Shirakawa,Yukio Kawashima,Seiji Yunoki,Hiroshi Horii*

Main category: cs.DC

TL;DR: 基于Thrust库的GPU加速SBD实现，相比CPU执行获得最高40倍加速，显著减少SQD迭代总时间


<details>
  <summary>Details</summary>
Motivation: SBD在基于样本的量子对角化(SQD)中扮演核心角色，其迭代对角化过程构成了主要的经典计算负载。为了加速这一关键计算瓶颈，需要利用现代GPU架构的高性能计算能力。

Method: 使用Thrust库实现GPU加速的SBD，通过重构关键组件（配置处理、激发生成、矩阵向量操作）为细粒度数据并行原语和扁平化的GPU友好数据布局，有效利用现代GPU架构。

Result: Thrust-based SBD在实验中实现了最高约40倍的加速比（相比CPU执行），显著减少了SQD迭代的总运行时间。

Conclusion: GPU原生并行原语为加速基于SQD的量子-经典工作流程提供了简单、可移植且高性能的基础框架。

Abstract: Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\sim$40$\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows.

</details>


### [57] [DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers](https://arxiv.org/abs/2601.16956)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: DataStates-LLM：针对大规模Transformer模型训练的新型检查点架构，通过状态提供者解耦状态抽象与数据移动，利用参数不变性实现异步快照，显著提升检查点吞吐量并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有检查点方案将模型状态视为不透明的二进制数据块，忽略了数据结构的"3D异质性"（内存位置、分片数量、数据类型、序列化需求差异），导致运行时开销大，包括阻塞的设备到主机传输、数据无关的序列化以及存储I/O争用。

Method: 引入DataStates-LLM架构，通过状态提供者解耦状态抽象与数据移动。利用前向和反向传播期间模型参数的不变性，执行"惰性"、非阻塞的异步快照。通过状态提供者有效合并碎片化的异构分片，并将元数据序列化与批量张量I/O重叠。

Result: 在256个A100-40GB GPU上评估高达700亿参数的模型。DataStates-LLM相比最先进解决方案实现了高达4倍的检查点吞吐量提升，并将端到端训练时间减少高达2.2倍，有效缓解了极端规模LLM训练中的序列化和异质性瓶颈。

Conclusion: DataStates-LLM通过创新的状态提供者架构和异步快照机制，显著优化了大规模Transformer模型训练的检查点性能，解决了现有方案在处理异构分布式状态时的效率瓶颈问题。

Abstract: The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.

</details>
