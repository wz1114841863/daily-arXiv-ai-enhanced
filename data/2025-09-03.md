<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 208]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.DC](#cs.DC) [Total: 21]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor Decomposition](https://arxiv.org/abs/2509.00280)
*Ahmed E. Helal,Fabio Checconi,Jan Laukemann,Yongseok Soh,Jesmin Jahan Tithi,Fabrizio Petrini,Jee Choi*

Main category: cs.LG

TL;DR: ReLATE是一个基于强化学习的自适应张量编码框架，能够自动为不规则张量形状和可变数据分布构建高效的稀疏张量表示，无需标记训练样本，相比专家设计的格式实现最高2倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统专家设计的稀疏张量格式无法适应不规则张量形状和高度可变的数据分布，导致张量分解在高维稀疏数据分析中面临性能和内存访问模式的挑战。

Method: 采用强化学习框架，通过自主代理与张量分解环境直接交互发现优化编码，结合无模型和基于模型的混合算法，使用规则驱动的动作掩码和动态信息动作过滤机制确保功能正确性。

Result: 在多样化稀疏张量数据集上 consistently 优于专家设计的格式，实现最高2倍加速，几何平均加速比为1.4-1.46倍。

Conclusion: ReLATE框架通过自适应学习能够为不规则张量形状和数据分布自动生成高效的稀疏表示，显著提升张量分解性能。

Abstract: Tensor decomposition (TD) is essential for analyzing high-dimensional sparse
data, yet its irregular computations and memory-access patterns pose major
performance challenges on modern parallel processors. Prior works rely on
expert-designed sparse tensor formats that fail to adapt to irregular tensor
shapes and/or highly variable data distributions. We present the
reinforcement-learned adaptive tensor encoding (ReLATE) framework, a novel
learning-augmented method that automatically constructs efficient sparse tensor
representations without labeled training samples. ReLATE employs an autonomous
agent that discovers optimized tensor encodings through direct interaction with
the TD environment, leveraging a hybrid model-free and model-based algorithm to
learn from both real and imagined actions. Moreover, ReLATE introduces
rule-driven action masking and dynamics-informed action filtering mechanisms
that ensure functionally correct tensor encoding with bounded execution time,
even during early learning stages. By automatically adapting to both irregular
tensor shapes and data distributions, ReLATE generates sparse tensor
representations that consistently outperform expert-designed formats across
diverse sparse tensor data sets, achieving up to 2X speedup compared to the
best sparse format, with a geometric-mean speedup of 1.4-1.46X.

</details>


### [2] [An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment](https://arxiv.org/abs/2509.00560)
*Can Cui,Zilong Fu,Penghe Huang,Yuanyuan Li,Wu Deng,Dongyan Li*

Main category: cs.LG

TL;DR: 这篇论文提出了一种从图神经网络向Kolmogorov-Arnold网络的知识蓄粉框架SA-DSD，通过改进的FR-KAN+学生模型和自适应重权损失机制，在大幅减少参数量和推理时间的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备环境中深度学习模型部署需要高效的模型压缩和推理速度，但MLP模型在捕捉图神经网络的复杂邻居依赖关系方面遇到了挑战，限制了其在边缘环境中的性能。

Method: 提出SA-DSD框架，改进Fourier KAN (FR-KAN)并用其替换MLP作为学生模型。通过引入可学习频率基准和相位移机制，以及算法优化，提高了非线性拟合能力并降低计算复杂度。构建基于教师-学生预测一致性的边际级采样概率矩阵，设计自适应重权损失机制。

Result: 在6个实际数据集上的实验显示，SA-DSD相比三种GNN教师模型性能提升3.05%-3.62%，相比FR-KAN+模型提升15.61%。与关键基准模型相比，参数量减少16.96倍，推理时间减少55.75%。

Conclusion: SA-DSD框架通过从GNN向KAN的知识蓄粉，有效解决了边缘设备中模型压缩和推理速度的挑战，在保持高性能的同时大幅减少计算资源消耗，为消费电子产品中深度学习模型的部署提供了有效解决方案。

Abstract: Knowledge distillation (KD) is crucial for deploying deep learning models in
resource-constrained edge environments, particularly within the consumer
electronics sector, including smart home devices, wearable technology, and
mobile terminals. These applications place higher demands on model compression
and inference speed, necessitating the transfer of knowledge from Graph Neural
Networks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However,
due to their fixed activation functions and fully connected architecture, MLPs
face challenges in rapidly capturing the complex neighborhood dependencies
learned by GNNs, thereby limiting their performance in edge environments. To
address these limitations, this paper introduces an innovative from GNNs to
Kolmogorov-Arnold Networks (KANs) knowledge distillation framework-Self
Attention Dynamic Sampling Distillation (SA-DSD). This study improved Fourier
KAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model.
Through the incorporation of learnable frequency bases and phase-shift
mechanisms, along with algorithmic optimization, FR-KAN significantly improves
its nonlinear fitting capability while effectively reducing computational
complexity. Building on this, a margin-level sampling probability matrix, based
on teacher-student prediction consistency, is constructed, and an adaptive
weighted loss mechanism is designed to mitigate performance degradation in the
student model due to the lack of explicit neighborhood aggregation. Extensive
experiments conducted on six real-world datasets demonstrate that SA-DSD
achieves performance improvements of 3.05%-3.62% over three GNN teacher models
and 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmark
models, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75%
decrease in inference time.

</details>


### [3] [DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing](https://arxiv.org/abs/2509.02197)
*Afif Boudaoud,Alexandru Calotoiu,Marcin Copik,Torsten Hoefler*

Main category: cs.LG

TL;DR: DaCe AD是一个无需代码修改的通用高效自动微分引擎，使用ILP算法优化存储与重计算权衡，在NPBench基准测试中平均比JAX快92倍以上


<details>
  <summary>Details</summary>
Motivation: 现有AD框架存在编程语言支持有限、需要代码修改、科学计算性能不足、存储策略简单等限制，迫使领域科学家手动计算大型问题的梯度

Method: 使用基于整数线性规划(ILP)的新算法，在给定内存约束下优化存储与重计算的权衡，实现最大性能

Result: 在NPBench高性能计算基准测试套件上，平均比当前最先进的通用AD框架JAX快92倍以上，且无需任何代码修改

Conclusion: DaCe AD提供了一个通用高效的自动微分解决方案，解决了现有AD框架的主要限制，特别适用于科学计算领域的大规模问题

Abstract: Automatic differentiation (AD) is a set of techniques that systematically
applies the chain rule to compute the gradients of functions without requiring
human intervention. Although the fundamentals of this technology were
established decades ago, it is experiencing a renaissance as it plays a key
role in efficiently computing gradients for backpropagation in machine learning
algorithms. AD is also crucial for many applications in scientific computing
domains, particularly emerging techniques that integrate machine learning
models within scientific simulations and schemes. Existing AD frameworks have
four main limitations: limited support of programming languages, requiring code
modifications for AD compatibility, limited performance on scientific computing
codes, and a naive store-all solution for forward-pass data required for
gradient calculations. These limitations force domain scientists to manually
compute the gradients for large problems. This work presents DaCe AD, a
general, efficient automatic differentiation engine that requires no code
modifications. DaCe AD uses a novel ILP-based algorithm to optimize the
trade-off between storing and recomputing to achieve maximum performance within
a given memory constraint. We showcase the generality of our method by applying
it to NPBench, a suite of HPC benchmarks with diverse scientific computing
patterns, where we outperform JAX, a Python framework with state-of-the-art
general AD capabilities, by more than 92 times on average without requiring any
code changes.

</details>


### [4] [Learning to Shard: RL for Co-optimizing the Parallelism Degrees and Per-operator Sharding Dimensions in Distributed LLM Inference](https://arxiv.org/abs/2509.00217)
*Ruokai Yin,Sattwik Deb Mishra,Xuan Zuo,Hokchhay Tann,Preyas Shah,Apala Guha*

Main category: cs.LG

TL;DR: Learn to Shard是首个基于强化学习的方法，用于协同优化分布式LLM推理中的粗粒度并行度和细粒度算子分片维度，相比现有方法实现了显著的性能提升


<details>
  <summary>Details</summary>
Motivation: 当前分布式LLM推理系统使用静态启发式方法分别配置并行度和算子分片维度，在模型规模扩大和硬件拓扑多样化时性能表现不佳，存在优化空间

Method: 采用基于注意力的强化学习策略，从高性能策略历史中学习，有效探索巨大的组合搜索空间，协同优化粗粒度并行度和细粒度算子分片维度

Result: 在H100集群上测试1.6T参数的MoE模型，相比元启发式基线实现了3.5倍吞吐量提升，相比Megatron启发式方法实现了1.06倍提升

Conclusion: Learn to Shard通过强化学习方法有效解决了分布式LLM推理中的并行策略优化问题，为大规模模型推理提供了高效的自动化配置方案

Abstract: Distributed LLM inference requires careful coordination of parallelization
strategies across hundreds to thousands of NPUs to meet production SLOs.
Current systems like Megatron-LM rely on static heuristics that separately
configure parallelism degrees and per-operator sharding dimensions, leaving
significant performance on the table as models scale and hardware topologies
diversify. We introduce Learn to Shard, to our knowledge, the first RL-based
approach to co-optimize both coarse-grained parallelism degrees and
fine-grained per-operator sharding dimensions for distributed LLM inference.
Our method employs an attention-based policy over an elite history that learns
from high-performing strategies to efficiently navigate the vast combinatorial
search space. Evaluated on H100 clusters with MoE models up to 1.6T parameters,
Learn to Shard achieves up to 3.5x throughput improvement over metaheuristic
baselines and 1.06x over Megatron heuristics.

</details>


### [5] [Online Identification of IT Systems through Active Causal Learning](https://arxiv.org/abs/2509.02130)
*Kim Hammar,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文提出了首个基于数据的在线IT系统因果模型识别方法——主动因果学习，使用高斯过程回归迭代估计系统变量间的因果函数，并通过基于rollout的干预策略收集系统测量数据。


<details>
  <summary>Details</summary>
Motivation: 传统IT系统因果模型依赖领域专家设计和维护，但随着现代IT系统日益复杂和动态化，这种方法面临巨大挑战。因果模型对于预测控制效果、优化操作、诊断故障等系统管理任务自动化至关重要。

Method: 采用主动因果学习方法，基于高斯过程回归迭代估计系统变量间的因果函数，通过rollout-based干预策略收集系统测量数据，实现贝叶斯最优的因果模型识别。

Result: 实验验证表明，该方法能够准确识别因果系统模型，同时对系统操作产生的干扰较低，证明了方法的有效性。

Conclusion: 该方法为复杂动态IT系统的自动化管理提供了有效的因果模型识别解决方案，实现了数据驱动的在线系统识别，具有重要的理论和实践意义。

Abstract: Identifying a causal model of an IT system is fundamental to many branches of
systems engineering and operation. Such a model can be used to predict the
effects of control actions, optimize operations, diagnose failures, detect
intrusions, etc., which is central to achieving the longstanding goal of
automating network and system management tasks. Traditionally, causal models
have been designed and maintained by domain experts. This, however, proves
increasingly challenging with the growing complexity and dynamism of modern IT
systems. In this paper, we present the first principled method for online,
data-driven identification of an IT system in the form of a causal model. The
method, which we call active causal learning, estimates causal functions that
capture the dependencies among system variables in an iterative fashion using
Gaussian process regression based on system measurements, which are collected
through a rollout-based intervention policy. We prove that this method is
optimal in the Bayesian sense and that it produces effective interventions.
Experimental validation on a testbed shows that our method enables accurate
identification of a causal system model while inducing low interference with
system operations.

</details>


### [6] [HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction](https://arxiv.org/abs/2509.02481)
*Aishwarya Sarkar,Autrin Hakimi,Xiaoqiong Chen,Hai Huang,Chaoqun Lu,Ibrahim Demir,Ali Jannesari*

Main category: cs.LG

TL;DR: HydroGAT是一个基于异构图神经网络的高分辨率洪水预测模型，通过自适应学习局部时间重要性和上游影响位置，在时空维度上同时建模水文过程，实现了精确的每小时流量预测。


<details>
  <summary>Details</summary>
Motivation: 传统洪水预测方法忽略流域拓扑信息，现有GNN方法因计算成本而降低分辨率，且时空依赖关系分开处理，无法同时捕捉对准确洪水预测至关重要的时空交互作用。

Method: 构建异质流域图，每个陆地和河流像素作为节点，通过物理水文流向和流域间关系连接；提出HydroGAT时空网络，自适应学习局部时间重要性和最具影响力的上游位置；开发分布式数据并行管道支持高分辨率训练。

Result: 在美国中西部两个流域和五个基线架构评估中，模型在每小时流量预测中达到更高的NSE（最高0.97）、改进的KGE（最高0.96）和低偏差（PBIAS在±5%内），并提供可解释的注意力图。分布式训练在64个NVIDIA A100 GPU上实现15倍加速。

Conclusion: HydroGAT通过高分辨率异质图表示和时空联合建模，显著提升了洪水预测精度，同时提供了可解释的水文过程洞察，为水资源管理提供了有效的技术解决方案。

Abstract: Accurate flood forecasting remains a challenge for water-resource management,
as it demands modeling of local, time-varying runoff drivers (e.g.,
rainfall-induced peaks, baseflow trends) and complex spatial interactions
across a river network. Traditional data-driven approaches, such as
convolutional networks and sequence-based models, ignore topological
information about the region. Graph Neural Networks (GNNs) propagate
information exactly along the river network, which is ideal for learning
hydrological routing. However, state-of-the-art GNN-based flood prediction
models collapse pixels to coarse catchment polygons as the cost of training
explodes with graph size and higher resolution. Furthermore, most existing
methods treat spatial and temporal dependencies separately, either applying
GNNs solely on spatial graphs or transformers purely on temporal sequences,
thus failing to simultaneously capture spatiotemporal interactions critical for
accurate flood prediction. We introduce a heterogenous basin graph where every
land and river pixel is a node connected by physical hydrological flow
directions and inter-catchment relationships. We propose HydroGAT, a
spatiotemporal network that adaptively learns local temporal importance and the
most influential upstream locations. Evaluated in two Midwestern US basins and
across five baseline architectures, our model achieves higher NSE (up to 0.97),
improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourly
discharge prediction, while offering interpretable attention maps that reveal
sparse, structured intercatchment influences. To support high-resolution
basin-scale training, we develop a distributed data-parallel pipeline that
scales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer,
demonstrating up to 15x speedup across machines. Our code is available at
https://github.com/swapp-lab/HydroGAT.

</details>


### [7] [Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?](https://arxiv.org/abs/2509.00026)
*Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.LG

TL;DR: 研究探讨使用传统机器学习和大型语言模型（如Llama 3.1）基于行为模式评估急诊精神病患者，以提供诊断评估。


<details>
  <summary>Details</summary>
Motivation: 精神障碍患者由于缺乏明显症状而经常被误判和误诊，在紧急情况下识别精神病问题具有挑战性但至关重要。

Method: 从德国救援站收集急诊精神病患者数据，使用包括Llama 3.1在内的各种机器学习模型分析患者行为模式。

Result: 研究评估了这些模型的预测能力是否能作为识别精神障碍患者的有效工具。

Conclusion: 机器学习和LLM模型有望成为急诊情况下识别精神障碍患者的高效诊断工具。

Abstract: Mental disorders are clinically significant patterns of behavior that are
associated with stress and/or impairment in social, occupational, or family
activities. People suffering from such disorders are often misjudged and poorly
diagnosed due to a lack of visible symptoms compared to other health
complications. During emergency situations, identifying psychiatric issues is
that's why challenging but highly required to save patients. In this paper, we
have conducted research on how traditional machine learning and large language
models (LLM) can assess these psychiatric patients based on their behavioral
patterns to provide a diagnostic assessment. Data from emergency psychiatric
patients were collected from a rescue station in Germany. Various machine
learning models, including Llama 3.1, were used with rescue patient data to
assess if the predictive capabilities of the models can serve as an efficient
tool for identifying patients with unhealthy mental disorders, especially in
rescue cases.

</details>


### [8] [Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning](https://arxiv.org/abs/2509.00027)
*Elie Thellier,Huiyu Li,Nicholas Ayache,Hervé Delingette*

Main category: cs.LG

TL;DR: 这篇论文提出了一种通过次次微调模型参数来突破医疗数据渗透攻击的防御方法，在保持模型性能的同时有效防止敏感数据泄漏。


<details>
  <summary>Details</summary>
Motivation: 医疗数据湖虽能训练强大的机器学习模型，但存在严重隐私风险，攻击者可以通过嵌入潜在表征或多任务学习来正建高保真医疗图像，带来法律和伦理风险。

Method: 提出一种简单有效的缓解策略：在模型导出时通过使用衰减的层级学习率进行微调，以突破嵌入的数据而不影响任务性能。

Result: 在DermaMNIST、ChestMNIST和MIMIC-CXR数据集上评估显示，该方法能够维持任务性能，有效突破现有的正建攻击，效果超过之前的防御方法，并使正建的数据无法用于训练。

Conclusion: 该研究提供了一种应对数据湖训练模型中数据泄漏的实用防御方法，同时适用于中心化联邦学习场景，并持续性研究揭示了适应性攻击的挑战和未来方向。

Abstract: Data lakes enable the training of powerful machine learning models on
sensitive, high-value medical datasets, but also introduce serious privacy
risks due to potential leakage of protected health information. Recent studies
show adversaries can exfiltrate training data by embedding latent
representations into model parameters or inducing memorization via multi-task
learning. These attacks disguise themselves as benign utility models while
enabling reconstruction of high-fidelity medical images, posing severe privacy
threats with legal and ethical implications. In this work, we propose a simple
yet effective mitigation strategy that perturbs model parameters at export time
through fine-tuning with a decaying layer-wise learning rate to corrupt
embedded data without degrading task performance. Evaluations on DermaMNIST,
ChestMNIST, and MIMIC-CXR show that our approach maintains utility task
performance, effectively disrupts state-of-the-art exfiltration attacks,
outperforms prior defenses, and renders exfiltrated data unusable for training.
Ablations and discussions on adaptive attacks highlight challenges and future
directions. Our findings offer a practical defense against data leakage in data
lake-trained models and centralized federated learning.

</details>


### [9] [ZeroQAT: Your Quantization-aware Training but Efficient](https://arxiv.org/abs/2509.00031)
*Qitao Tan,Xiaoying Song,Jin Lu,Guoming Li,Jun Liu,Lingzi Hong,Caiwen Ding,Jundong Li,Xiaoming Zhai,Shaoyi Huang,Wei Niu,Geng Yuan*

Main category: cs.LG

TL;DR: ZeroQAT是一种基于零阶优化的量化感知训练框架，通过前向梯度估计消除反向传播需求，在保持端到端优化优势的同时显著降低计算和内存开销


<details>
  <summary>Details</summary>
Motivation: 现有低比特后训练量化方法存在精度下降问题，而传统量化感知训练依赖反向传播导致计算成本过高，需要一种既高效又能保持精度的量化解决方案

Method: 使用零阶优化方法，通过前向梯度估计替代反向传播，联合学习量化权重、权重裁剪阈值和等效变换来减少量化误差和处理激活异常值

Result: 实验证明ZeroQAT能够达到后训练量化的效率，同时保持量化感知训练的精度，为LLMs的高质量低比特量化提供了实用解决方案

Conclusion: ZeroQAT框架成功解决了量化中效率与精度的平衡问题，为零阶优化在大型语言模型量化中的应用提供了有效途径

Abstract: Quantization is an effective technique to reduce the deployment cost of large
language models (LLMs), and post-training quantization (PTQ) has been widely
studied due to its efficiency. However, existing low-bit PTQ methods suffer
from accuracy degradation because their layer-wise optimization introduces
cumulative error propagation and misalignment between local reconstruction
objectives and downstream performance. While quantization-aware training (QAT)
provides a principled solution, its reliance on backpropagation incurs
prohibitive data, time, and memory costs, limiting its practicality. To address
these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT
framework. ZeroQAT leverages forward-only gradient estimation to eliminate the
need for backpropagation, significantly reducing computational and memory
overhead while retaining the benefits of end-to-end optimization. Moreover,
ZeroQAT jointly learns quantized weights, weight clipping thresholds, and
equivalent transformations to mitigate quantization error and handle activation
outliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ
while retaining the accuracy of QAT, offering a practical solution for
high-quality low-bit quantization of LLMs.

</details>


### [10] [Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications](https://arxiv.org/abs/2509.00034)
*Mert Sehri,Ana Cardoso,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 提出了一种基于振动数据的跨域诊断方法，使用CNN-LSTM混合深度学习模型来检测钢水浇注过程中的渣流状态，准确率达到99.10%，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 钢水浇注过程中渣流污染会造成重大经济损失，需要准确检测渣流状态以避免损失。

Method: 使用一维卷积神经网络和长短期记忆层的混合深度学习模型，处理原始时域振动信号，采用均方根预处理和选择性嵌入数据加载策略。

Result: 在16个不同域上测试，混合CNN-LSTM架构结合RMS预处理达到99.10% ± 0.30%的最高测试准确率，优于传统模型。

Conclusion: 该方法为实时渣流监测提供了实用且可扩展的解决方案，有助于提高钢铁制造的可靠性和运营效率。

Abstract: Steel casting processes are vulnerable to financial losses due to slag flow
contamination, making accurate slag flow condition detection essential. This
study introduces a novel cross-domain diagnostic method using vibration data
collected from an industrial steel foundry to identify various stages of slag
flow. A hybrid deep learning model combining one-dimensional convolutional
neural networks and long short-term memory layers is implemented, tested, and
benchmarked against a standard one-dimensional convolutional neural network.
The proposed method processes raw time-domain vibration signals from
accelerometers and evaluates performance across 16 distinct domains using a
realistic cross-domain dataset split. Results show that the hybrid
convolutional neural network and long short-term memory architecture, when
combined with root mean square preprocessing and a selective embedding data
loading strategy, achieves robust classification accuracy, outperforming
traditional models and loading techniques. The highest test accuracy of 99.10
+/- 0.30 demonstrates the method's capability for generalization and industrial
relevance. This work presents a practical and scalable solution for real-time
slag flow monitoring, contributing to improved reliability and operational
efficiency in steel manufacturing.

</details>


### [11] [Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing](https://arxiv.org/abs/2509.00035)
*Yuxuan Yin,Rebecca Chen,Boxun Xu,Chen He,Peng Li*

Main category: cs.LG

TL;DR: 提出了一种新颖的迁移学习框架，利用16nm节点的丰富历史数据来改进5nm节点的芯片最小工作电压预测精度，通过集成片上硅里程计传感器数据来表征局部工艺变化。


<details>
  <summary>Details</summary>
Motivation: 在先进工艺节点下，由于训练数据有限以及工艺变化与最小工作电压之间复杂的关系，准确预测芯片性能面临挑战。

Method: 采用迁移学习框架，利用16nm技术节点的历史数据，并集成片上硅里程计传感器数据来获取局部工艺变化的精细特征。

Result: 该方法显著提高了5nm节点的最小工作电压预测精度。

Conclusion: 所提出的迁移学习框架结合片上传感器数据，能够有效解决先进工艺节点下芯片性能预测的数据稀缺和复杂性挑战。

Abstract: Accurate prediction of chip performance is critical for ensuring energy
efficiency and reliability in semiconductor manufacturing. However, developing
minimum operating voltage ($V_{min}$) prediction models at advanced technology
nodes is challenging due to limited training data and the complex relationship
between process variations and $V_{min}$. To address these issues, we propose a
novel transfer learning framework that leverages abundant legacy data from the
16nm technology node to enable accurate $V_{min}$ prediction at the advanced
5nm node. A key innovation of our approach is the integration of input features
derived from on-chip silicon odometer sensor data, which provide fine-grained
characterization of localized process variations -- an essential factor at the
5nm node -- resulting in significantly improved prediction accuracy.

</details>


### [12] [A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](https://arxiv.org/abs/2509.00036)
*Cheng Jin,Zhenyu Xiao,Yuantao Gu*

Main category: cs.LG

TL;DR: A-FloPS是一种无需训练的高效扩散模型采样框架，通过重参数化采样轨迹和自适应速度分解，在极低函数评估次数下实现高质量生成


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成性能优异，但迭代采样过程计算成本高昂。现有训练免费加速方法受限于底层采样轨迹的低效性，需要更有效的解决方案

Method: 提出A-FloPS框架：1）将预训练扩散模型的采样轨迹重参数化为流匹配形式；2）采用自适应速度分解机制，将速度场分解为线性漂移项和残差分量；3）抑制残差分量的时间变化，恢复高阶积分精度

Result: 在条件图像生成和文本到图像合成任务中，A-FloPS在样本质量和效率方面均优于最先进的训练免费采样器。仅用5次函数评估即可实现显著更低的FID和更清晰、更连贯的图像

Conclusion: A-FloPS是一个通用有效的解决方案，可用于高质量、低延迟的生成建模，其自适应机制还能改进基于流的原生生成模型

Abstract: Diffusion models deliver state-of-the-art generative performance across
diverse modalities but remain computationally expensive due to their inherently
iterative sampling process. Existing training-free acceleration methods
typically improve numerical solvers for the reverse-time ODE, yet their
effectiveness is fundamentally constrained by the inefficiency of the
underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path
Sampler), a principled, training-free framework that reparameterizes the
sampling trajectory of any pre-trained diffusion model into a flow-matching
form and augments it with an adaptive velocity decomposition. The
reparameterization analytically maps diffusion scores to flow-compatible
velocities, yielding integration-friendly trajectories without retraining. The
adaptive mechanism further factorizes the velocity field into a linear drift
term and a residual component whose temporal variation is actively suppressed,
restoring the accuracy benefits of high-order integration even in extremely
low-NFE regimes. Extensive experiments on conditional image generation and
text-to-image synthesis show that A-FloPS consistently outperforms
state-of-the-art training-free samplers in both sample quality and efficiency.
Notably, with as few as $5$ function evaluations, A-FloPS achieves
substantially lower FID and generates sharper, more coherent images. The
adaptive mechanism also improves native flow-based generative models,
underscoring its generality. These results position A-FloPS as a versatile and
effective solution for high-quality, low-latency generative modeling.

</details>


### [13] [Exploring and Reshaping the Weight Distribution in LLM](https://arxiv.org/abs/2509.00046)
*Chunming Ye,Songzhou Li,Xu Xu*

Main category: cs.LG

TL;DR: 该论文研究发现大语言模型不同层权重间的余弦距离呈现幂律分布特征，基于此特征设计了数据生成器来重塑LoRA初始化权重，从而在不改变模型结构或训练过程的情况下提升了LoRA训练性能。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型不同层权重分布的差异及其相关性，探索这些分布特征对LoRA训练效果的影响，旨在通过理解权重分布规律来优化LoRA初始化策略。

Method: 1) 使用奇异值分解计算自注意力层和MLP层中不同投影矩阵的奇异值；2) 分析矩阵间余弦距离的概率分布，发现幂律分布特征；3) 设计结合高斯过程和帕累托分布的数据生成器模拟特定分布特征；4) 基于分布特征重塑LoRA初始化权重进行训练。

Result: 实验结果表明，该方法在不改变模型结构或训练过程的情况下，实现了LoRA训练性能的一定提升，验证了基于权重分布特征优化LoRA初始化的有效性。

Conclusion: 大语言模型不同层权重间的余弦距离具有明显的幂律分布特征，利用这一特征可以设计有效的数据生成方法来优化LoRA权重初始化，从而提升模型训练效果。

Abstract: The performance of Large Language Models is influenced by their
characteristics such as architecture, model sizes, decoding methods and so on.
Due to differences in structure or function, the weights in different layers of
large models have varying distributions. This paper explores the correlations
between different types of layers in terms of weights distribution and studies
the potential impact of these correlations on LoRA training effectiveness.
Firstly, the study reveals that in the model the cosine distances between
weights of different layers manifest power-law distribution. We extract
Query-projection, down-projection and other weight matrices from the
self-attention layers and MLP layers, calculate the singular values of the
matrices using singular value decomposition, and organize a certain number of
singular values into matrices according to projection's type. By analyzing the
probability distribution of the cosine distances between these matrices, it is
found that the cosine distances values between them have distinct power-law
distribution characteristics. Secondly, based on the results of distance
calculations and analysis across different layers of model, a qualitative
method is proposed to describe the distribution characteristics of different
models. Next, to construct weights that align with the distribution
characteristics, a data generator is designed using a combination of Gaussian
process and Pareto distribution functions. The generator is used to simulate
the generation of data that aligns with specific distribution characteristics.
Finally, based on the aforementioned distribution characteristics and data
generation method, the weights in LoRA initialization are reshaped for
training. Experimental results indicate that, without altering the model
structure or training process, this method achieves a certain improvement in
the performance of LoRA training.

</details>


### [14] [Teaching AI to Remember: Insights from Brain-Inspired Replay in Continual Learning](https://arxiv.org/abs/2509.00047)
*Jina Kim*

Main category: cs.LG

TL;DR: 该论文研究人工神经网络中的持续学习问题，特别是灾难性遗忘现象。受人类大脑记忆巩固机制启发，重点研究了内部重放机制在缓解遗忘方面的效果，发现与突触智能结合使用时效果最佳，但会降低初始任务准确性，揭示了记忆稳定性与学习可塑性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在持续学习中面临灾难性遗忘的挑战，即学习新任务时会丢失先前学到的知识。受人类大脑记忆巩固机制的启发，研究者希望探索内部重放这一大脑启发的机制如何帮助缓解这一问题。

Method: 使用CIFAR-100数据集在类增量设置下评估内部重放机制的效果，包括单独使用和与突触智能(SI)结合使用。通过似然分布、重建误差、轮廓分数和UMAP投影等多种分析方法来研究表示空间的变化。

Result: 内部重放显著减轻了遗忘现象，特别是与SI结合使用时效果更好。但代价是降低了初始任务的准确性，表明存在记忆稳定性与学习可塑性之间的权衡。分析显示内部重放增加了潜在空间中的表示重叠，可能限制了任务特异性区分。

Conclusion: 当前大脑启发的方法存在局限性，需要在保持记忆和适应能力之间找到更好的平衡。研究结果为未来持续学习系统的设计提供了方向，强调了需要开发能够同时优化记忆保留和学习新任务能力的方法。

Abstract: Artificial neural networks (ANNs) continue to face challenges in continual
learning, particularly due to catastrophic forgetting, the loss of previously
learned knowledge when acquiring new tasks. Inspired by memory consolidation in
the human brain, we investigate the internal replay mechanism proposed
by~\citep{brain_inspired_replay1}, which reactivates latent representations of
prior experiences during learning. As internal replay was identified as the
most influential component among the brain-inspired mechanisms in their
framework, it serves as the central focus of our in-depth investigation. Using
the CIFAR-100 dataset in a class-incremental setting, we evaluate the
effectiveness of internal replay, both in isolation and in combination with
Synaptic Intelligence (SI). Our experiments show that internal replay
significantly mitigates forgetting, especially when paired with SI, but at the
cost of reduced initial task accuracy, highlighting a trade-off between memory
stability and learning plasticity. Further analyses using log-likelihood
distributions, reconstruction errors, silhouette scores, and UMAP projections
reveal that internal replay increases representational overlap in latent space,
potentially limiting task-specific differentiation. These results underscore
the limitations of current brain-inspired methods and suggest future directions
for balancing retention and adaptability in continual learning systems.

</details>


### [15] [Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals](https://arxiv.org/abs/2509.00049)
*Mohammad Nooraiepour,Mohammad Masoudi,Zezhang Song,Helge Hellevang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于物理信息神经网络(PINN)的模型，用于预测氢气在泥岩、页岩和煤矿中的吸附性能，具有高精度和快速收敛特点。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法时间消耗大、容易出错且无法抓取地质异质性，而氢气吸附预测对地下氢气存储、自然氢气挖掘和核废料处置至关重要。

Method: 采用适应性物理信息神经网络框架，结合多类别特征工程，集成经典吸附线模型和热力学约束。使用深度殊差网络和多头注意力机制，通过适应损失函数和Monte Carlo dropout进行不确定性量化。

Result: 在155个样本的数据集上得到高精度结果(R2 = 0.979, RMSE = 0.045 mol/kg)，收敛速度提高67%，在泥岩、页岩和煤矿上都表现稳定，可靠性评分达85-91%。

Conclusion: 该模型框架能够加快场地选择过程，通过稳健的不确定性量化支持风险决策，验证了非线性建模方法的必要性。

Abstract: Accurate prediction of hydrogen sorption in clays, shales, and coals is vital
for advancing underground hydrogen storage, natural hydrogen exploration, and
radioactive waste containment. Traditional experimental methods, while
foundational, are time-consuming, error-prone, and limited in capturing
geological heterogeneity. This study introduces an adaptive physics-informed
neural network (PINN) framework with multi-category feature engineering to
enhance hydrogen sorption prediction. The framework integrates classical
isotherm models with thermodynamic constraints to ensure physical consistency
while leveraging deep learning flexibility. A comprehensive dataset consisting
of 155 samples, which includes 50 clays, 60 shales, and 45 coals, was employed,
incorporating diverse compositional properties and experimental conditions.
Multi-category feature engineering across seven categories captured complex
sorption dynamics. The PINN employs deep residual networks with multi-head
attention, optimized via adaptive loss functions and Monte Carlo dropout for
uncertainty quantification. K-fold cross-validation and hyperparameter
optimization achieve significant accuracy (R2 = 0.979, RMSE = 0.045 mol per kg)
with 67% faster convergence despite 15-fold increased complexity. The framework
demonstrates robust lithology-specific performance across clay minerals (R2 =
0.981), shales (R2 = 0.971), and coals (R2 = 0.978), maintaining 85-91%
reliability scores. Interpretability analysis via SHAP, accumulated local
effects, and Friedman's H-statistics reveal that hydrogen adsorption capacity
dominates predictions, while 86.7% of feature pairs exhibit strong
interactions, validating the necessity of non-linear modeling approaches. This
adaptive physics-informed framework accelerates site screening and enables
risk-informed decision-making through robust uncertainty quantification.

</details>


### [16] [Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity](https://arxiv.org/abs/2509.00050)
*David Kurtenbach,Megan Manly,Zach Metzinger*

Main category: cs.LG

TL;DR: 使用深度学习技术分析俄罗斯空间物体在乌克兰入侵前的异常活动，通过多种自编码器模型检测轨道行为变化，发现统计显著的异常模式可作为军事冲突预警指标。


<details>
  <summary>Details</summary>
Motivation: 研究俄罗斯空间物体在军事行动前的异常活动模式，旨在建立可预警未来军事冲突的指示和警告系统，通过公开的TLE数据分析俄罗斯空间行为的统计显著变化。

Method: 采用隔离森林、传统自编码器、变分自编码器、Kolmogorov Arnold网络和新型锚点损失自编码器等多种深度学习模型，基于5年数据建立基线，重点分析入侵前6个月的轨道元素异常。

Result: 研究发现俄罗斯空间物体活动存在统计显著的异常，不仅整体活动异常，还能具体识别到单个轨道元素的异常行为，为军事预警提供了详细依据。

Conclusion: 深度学习模型能有效检测空间物体的异常行为模式，这些异常可作为军事冲突的早期预警指标，该方法对未来空间态势感知和冲突预警具有重要价值。

Abstract: We apply deep learning techniques for anomaly detection to analyze activity
of Russian-owned resident space objects (RSO) prior to the Ukraine invasion and
assess the results for any findings that can be used as indications and
warnings (I&W) of aggressive military behavior for future conflicts. Through
analysis of anomalous activity, an understanding of possible tactics and
procedures can be established to assess the existence of statistically
significant changes in Russian RSO pattern of life/pattern of behavior
(PoL/PoB) using publicly available two-line element (TLE) data. This research
looks at statistical and deep learning approaches to assess anomalous activity.
The deep learning methods assessed are isolation forest (IF), traditional
autoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network
(KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is
used to establish a baseline of on-orbit activity based on a five-year data
sample. The primary investigation period focuses on the six months leading up
to the invasion date of February 24, 2022. Additional analysis looks at RSO
activity during an active combat period by sampling TLE data after the invasion
date. The deep learning autoencoder models identify anomalies based on
reconstruction errors that surpass a threshold sigma. To capture the nuance and
unique characteristics of each RSO an individual model was trained for each
observed space object. The research made an effort to prioritize explainability
and interpretability of the model results thus each observation was assessed
for anomalous behavior of the individual six orbital elements versus analyzing
the input data as a single monolithic observation. The results demonstrate not
only statistically significant anomalies of Russian RSO activity but also
details anomalous findings to the individual orbital element.

</details>


### [17] [From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis](https://arxiv.org/abs/2509.00057)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,Joao Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 本文比较了光网络中机器学习故障管理的三类不平衡处理方法，发现后处理方法在故障检测中F1分数提升最高（15.3%），而生成式AI方法在故障识别中表现最佳（24.2%提升）。不同场景下最优方法选择取决于类别重叠程度和延迟要求。


<details>
  <summary>Details</summary>
Motivation: 光网络故障管理中严重的类别不平衡问题（正常实例远多于故障案例）是主要挑战，虽然前处理和中处理技术已被广泛研究，但后处理方法尚未得到充分探索。

Method: 使用实验数据集对前处理、中处理和后处理三种类别不平衡缓解方法进行直接比较，包括阈值调整、随机欠采样、生成式AI方法、SMOTE过采样和元学习等技术。

Result: 故障检测中后处理方法（特别是阈值调整）获得最高F1分数提升（15.3%），随机欠采样推理速度最快；故障识别中生成式AI方法性能提升最大（24.2%），但后处理在多类别设置中效果有限。不同场景下最优方法不同：有类别重叠且延迟关键时SMOTE最有效，无延迟约束时元学习最佳，低重叠场景生成式AI性能最高且推理时间最短。

Conclusion: 类别不平衡缓解方法的选择应基于具体应用场景：故障检测适合后处理，故障识别适合生成式AI，同时需要考虑类别重叠程度和延迟要求来选择合适的处理技术。

Abstract: Machine learning-based failure management in optical networks has gained
significant attention in recent years. However, severe class imbalance, where
normal instances vastly outnumber failure cases, remains a considerable
challenge. While pre- and in-processing techniques have been widely studied,
post-processing methods are largely unexplored. In this work, we present a
direct comparison of pre-, in-, and post-processing approaches for class
imbalance mitigation in failure detection and identification using an
experimental dataset. For failure detection, post-processing
methods-particularly Threshold Adjustment-achieve the highest F1 score
improvement (up to 15.3%), while Random Under-Sampling provides the fastest
inference. In failure identification, GenAI methods deliver the most
substantial performance gains (up to 24.2%), whereas post-processing shows
limited impact in multi-class settings. When class overlap is present and
latency is critical, over-sampling methods such as the SMOTE are most
effective; without latency constraints, Meta-Learning yields the best results.
In low-overlap scenarios, Generative AI approaches provide the highest
performance with minimal inference time.

</details>


### [18] [T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation](https://arxiv.org/abs/2509.00066)
*Chuanxiang Yang,Yuanfeng Zhou,Guangshun Wei,Siyu Ren,Yuan Liu,Junhui Hou,Wenping Wang*

Main category: cs.LG

TL;DR: 提出T-MLP神经网络架构，通过为MLP隐藏层添加多个输出分支（尾部），实现多尺度细节层次信号表示，在多种信号表示任务中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统MLP缺乏对细节层次（LoD）表示的原生支持，只能处理单一尺度信号，无法满足图像和3D形状等多尺度信号的高效建模和传输需求。

Method: 在标准MLP基础上，为隐藏层添加多个输出分支（尾部），通过直接监督多个深度层次，使每个隐藏层学习特定LoD的目标信号，实现多尺度建模。

Result: 大量实验结果表明，T-MLP在多种信号表示任务中均优于其他神经LoD基线方法。

Conclusion: T-MLP架构成功扩展了MLP的多尺度表示能力，为细节层次信号表示提供了一种有效的神经网络解决方案。

Abstract: Level-of-detail (LoD) representation is critical for efficiently modeling and
transmitting various types of signals, such as images and 3D shapes. In this
work, we present a novel neural architecture that supports LoD signal
representation. Our architecture is based on an elaborate modification of the
widely used Multi-Layer Perceptron (MLP), which inherently operates at a single
scale and therefore lacks native support for LoD. Specifically, we introduce
the Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching
multiple output branches, also called tails, to its hidden layers, enabling
direct supervision at multiple depths. Our loss formulation and training
strategy allow each hidden layer to effectively learn a target signal at a
specific LoD, thus enabling multi-scale modeling. Extensive experimental
results show that our T-MLP outperforms other neural LoD baselines across a
variety of signal representation tasks.

</details>


### [19] [AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum](https://arxiv.org/abs/2509.00069)
*Prasasthy Balasubramanian,Dumindu Kankanamge,Ekaterina Gilman,Mourad Oussalah*

Main category: cs.LG

TL;DR: 提出了一个结合异常检测和可解释AI的框架，使用BERTViz和Captum可视化工具以及基于注意力输出的自然语言报告，在网络安全领域提供高质量解释，RoBERTa模型在HDFS数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 虽然对话AI和大型语言模型在网络安全领域应用广泛，但误报和复杂模型管理限制了信任度。可解释AI(XAI)旨在提高透明度，但许多安全分析师仍对其有效性存疑，需要开发能提供高质量解释的框架。

Method: 开发了一个结合异常检测和可解释性的框架，使用BERTViz和Captum可视化工具，生成基于注意力输出的自然语言报告。在HDFS数据集上比较了RoBERTa、Falcon-7B、DeBERTa和Mistral-7B等模型性能。

Result: RoBERTa模型表现最佳，达到99.6%的准确率，在异常检测方面优于Falcon-7B和DeBERTa，比大规模Mistral-7B更具灵活性。用户反馈证实聊天机器人易于使用，提高了对异常的理解。

Conclusion: 该框架通过减少人工努力和加速修复，有效增强了网络安全工作流程，证明了结合异常检测与高质量解释的可解释AI方法在网络安全领域的实用价值。

Abstract: Conversational AI and Large Language Models (LLMs) have become powerful tools
across domains, including cybersecurity, where they help detect threats early
and improve response times. However, challenges such as false positives and
complex model management still limit trust. Although Explainable AI (XAI) aims
to make AI decisions more transparent, many security analysts remain uncertain
about its usefulness. This study presents a framework that detects anomalies
and provides high-quality explanations through visual tools BERTViz and Captum,
combined with natural language reports based on attention outputs. This reduces
manual effort and speeds up remediation. Our comparative analysis showed that
RoBERTa offers high accuracy (99.6 %) and strong anomaly detection,
outperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility
than large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback
confirms the chatbot's ease of use and improved understanding of anomalies,
demonstrating the ability of the developed framework to strengthen
cybersecurity workflows.

</details>


### [20] [SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits](https://arxiv.org/abs/2509.00071)
*Shang Liu,Jing Wang,Wenji Fang,Zhiyao Xie*

Main category: cs.LG

TL;DR: SynCircuit是一个创新的框架，首次尝试生成功能有效的HDL格式合成电路，通过扩散模型、约束优化和MCTS搜索来解决电路数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: AI辅助IC设计方法面临电路设计数据极度稀缺的问题，特别是在公共领域，这成为开发AI辅助IC设计方法的主要瓶颈。

Method: 提出三步框架：1)使用定制化扩散生成模型处理有向循环图生成任务；2)通过约束优化确保电路有效性；3)使用蒙特卡洛树搜索优化逻辑冗余。

Result: 实验结果表明SynCircuit能够生成更真实的合成电路，并在下游电路设计任务中提升机器学习模型性能。

Conclusion: 该工作为解决电路数据稀缺问题提供了有效解决方案，生成的合成电路能够有效增强AI辅助IC设计方法的性能。

Abstract: In recent years, AI-assisted IC design methods have demonstrated great
potential, but the availability of circuit design data is extremely limited,
especially in the public domain. The lack of circuit data has become the
primary bottleneck in developing AI-assisted IC design methods. In this work,
we make the first attempt, SynCircuit, to generate new synthetic circuits with
valid functionalities in the HDL format. SynCircuit automatically generates
synthetic data using a framework with three innovative steps: 1) We propose a
customized diffusion-based generative model to resolve the Directed Cyclic
Graph (DCG) generation task, which has not been well explored in the AI
community. 2) To ensure our circuit is valid, we enforce the circuit
constraints by refining the initial graph generation outputs. 3) The Monte
Carlo tree search (MCTS) method further optimizes the logic redundancy in the
generated graph. Experimental results demonstrate that our proposed SynCircuit
can generate more realistic synthetic circuits and enhance ML model performance
in downstream circuit design tasks.

</details>


### [21] [Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis](https://arxiv.org/abs/2509.00073)
*Ankit Shetgaonkar,Dipen Pradhan,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj*

Main category: cs.LG

TL;DR: 本文综述了生成式AI（特别是大语言模型）在医疗健康领域处理复杂数据的能力、需求和应用，重点关注如何利用GenAI技术解决远程患者监测和电子健康记录数据整合带来的临床信息过载问题。


<details>
  <summary>Details</summary>
Motivation: 医疗健康领域存在大量异构数据（实时远程患者监测数据和传统电子健康记录），这些数据的复杂性和海量性给临床医生带来了严重的信息过载挑战，需要新的技术解决方案。

Method: 通过分析GenAI和LLMs的技术能力，探索其在临床数据导航、自然语言对话式临床决策支持等方面的应用潜力，同时考虑数据集成、质量保证、隐私保护等关键挑战。

Result: 提出了GenAI技术在改善临床效率、简化工作流程和个性化护理方面的应用框架，并识别了数据集成复杂性、数据质量、患者隐私、AI输出验证等关键挑战。

Conclusion: 这项工作首次系统总结了GenAI技术在管理因RPM/EHR数据复杂性导致的临床数据过载问题中的应用前景和挑战，为未来医疗AI发展提供了重要参考。

Abstract: Generative Artificial Intelligence (GenAI), particularly Large Language
Models (LLMs), offer powerful capabilities for interpreting the complex data
landscape in healthcare. In this paper, we present a comprehensive overview of
the capabilities, requirements and applications of GenAI for deriving clinical
insights and improving clinical efficiency. We first provide some background on
the forms and sources of patient data, namely real-time Remote Patient
Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The
sheer volume and heterogeneity of this combined data present significant
challenges to clinicians and contribute to information overload. In addition,
we explore the potential of LLM-powered applications for improving clinical
efficiency. These applications can enhance navigation of longitudinal patient
data and provide actionable clinical decision support through natural language
dialogue. We discuss the opportunities this presents for streamlining clinician
workflows and personalizing care, alongside critical challenges such as data
integration complexity, ensuring data quality and RPM data reliability,
maintaining patient privacy, validating AI outputs for clinical safety,
mitigating bias, and ensuring clinical acceptance. We believe this work
represents the first summarization of GenAI techniques for managing clinician
data overload due to combined RPM / EHR data complexities.

</details>


### [22] [Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor](https://arxiv.org/abs/2509.00076)
*Zachery Dahm,Konstantinos Vasili,Vasileios Theos,Konstantinos Gkouliaras,William Richards,True Miller,Brian Jowers,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 本文提出了一种多层AI/ML架构，整合IT和OT数据流来识别和区分核反应堆中的网络安全事件与操作异常，并在PUR-1研究堆上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 核工业对AI/ML应用兴趣增加，但缺乏在运行核反应堆中验证AI/ML工具可行性的研究，特别是在区分网络安全事件和操作异常方面。

Method: 开发多层AI/ML架构，整合信息技术(IT)和操作技术(OT)数据流，在PUR-1研究堆上进行实验，包含14种系统状态和1380万多个多变量数据点。

Result: AI/ML能够区分正常、异常和网络安全相关事件，即使在拒绝服务攻击等挑战条件下。结合IT和OT数据提高了分类准确性，但也带来了同步和收集方面的挑战。

Conclusion: AI/ML在核网络安全方面显示出巨大潜力，但需要进一步改进复杂事件区分和多类架构的处理能力。

Abstract: There is increased interest in applying Artificial Intelligence and Machine
Learning (AI/ML) within the nuclear industry and nuclear engineering community.
Effective implementation of AI/ML could offer benefits to the nuclear domain,
including enhanced identification of anomalies, anticipation of system
failures, and operational schedule optimization. However, limited work has been
done to investigate the feasibility and applicability of AI/ML tools in a
functioning nuclear reactor. Here, we go beyond the development of a single
model and introduce a multi-layered AI/ML architecture that integrates both
information technology and operational technology data streams to identify,
characterize, and differentiate (i) among diverse cybersecurity events and (ii)
between cyber events and other operational anomalies. Leveraging Purdue
Universitys research reactor, PUR-1, we demonstrate this architecture through a
representative use case that includes multiple concurrent false data injections
and denial-of-service attacks of increasing complexity under realistic reactor
conditions. The use case includes 14 system states (1 normal, 13 abnormal) and
over 13.8 million multi-variate operational and information technology data
points. The study demonstrated the capability of AI/ML to distinguish between
normal, abnormal, and cybersecurity-related events, even under challenging
conditions such as denial-of-service attacks. Combining operational and
information technology data improved classification accuracy but posed
challenges related to synchronization and collection during certain cyber
events. While results indicate significant promise for AI/ML in nuclear
cybersecurity, the findings also highlight the need for further refinement in
handling complex event differentiation and multi-class architectures.

</details>


### [23] [Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models](https://arxiv.org/abs/2509.00083)
*Laksh Patel,Neel Shanbhag*

Main category: cs.LG

TL;DR: GenDataCarto框架通过难度分数和记忆分数对预训练样本进行四象限划分，指导针对性数据修剪和权重调整，有效减少数据泄露同时保持生成性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型存在过拟合和无意记忆罕见训练样本的风险，这些样本可能被攻击者提取或夸大基准性能，需要数据中心的解决方案来缓解这些问题。

Method: 提出生成数据制图框架，为每个预训练样本分配难度分数（早期epoch损失）和记忆分数（遗忘事件频率），将样本划分为四个象限，指导针对性修剪和权重调整。

Result: 在仅修剪10%数据的情况下，将合成canary提取成功率降低40%以上，验证困惑度仅增加不到0.5%。

Conclusion: 基于原则的数据干预可以显著减轻数据泄露，同时对生成性能影响最小，证明了数据中心方法在生成模型安全中的有效性。

Abstract: Modern generative models risk overfitting and unintentionally memorizing rare
training examples, which can be extracted by adversaries or inflate benchmark
performance. We propose Generative Data Cartography (GenDataCarto), a
data-centric framework that assigns each pretraining sample a difficulty score
(early-epoch loss) and a memorization score (frequency of ``forget events''),
then partitions examples into four quadrants to guide targeted pruning and
up-/down-weighting. We prove that our memorization score lower-bounds classical
influence under smoothness assumptions and that down-weighting
high-memorization hotspots provably decreases the generalization gap via
uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary
extraction success by over 40\% at just 10\% data pruning, while increasing
validation perplexity by less than 0.5\%. These results demonstrate that
principled data interventions can dramatically mitigate leakage with minimal
cost to generative performance.

</details>


### [24] [Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs](https://arxiv.org/abs/2509.00084)
*Qibin Wang,Pu Zhao,Shaohan Huang,Fangkai Yang,Lu Wang,Furu Wei,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.LG

TL;DR: 提出Generative Self-Refinement (GSR)框架，通过并行生成候选答案并进行自我精炼，解决了传统测试时扩展方法在候选答案全错时无法产生正确解的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法如Best-of-N和多数投票的性能受限于候选答案质量，当所有候选答案都错误时无法产生正确解，且引入额外模型进行选择会增加部署成本。

Method: 提出GSR框架：统一模型并行生成候选答案，然后基于问题和候选答案进行自我精炼合成更优解。设计了混合训练管道，联合优化直接解题和精炼候选答案两个互补目标。

Result: 在五个数学基准测试中达到最先进性能，学习到的自我精炼技能具有模型无关性，在不同模型规模上表现稳健，并能泛化到分布外推理任务。

Conclusion: GSR框架有效提升了LLM解决复杂多步推理问题的能力，通过自我精炼机制克服了传统方法的局限性，实现了性能的显著提升。

Abstract: To further enhance the ability of Large Language Models (LLMs) to solve
complex, multi-step reasoning problems, test-time scaling (TTS) methods have
gained widespread attention. Existing approaches such as Best-of-N and majority
voting are limited as their performance depends on the quality of candidate
responses, making them unable to produce a correct solution when all candidates
are incorrect. Introducing an additional model to select the best response also
incurs significant deployment costs. To this end, we introduce Generative
Self-Refinement (GSR), a novel parallel test-time scaling framework where a
unified model first generates a set of candidate responses in parallel and then
performs self-refinement to synthesize a new superior solution based on a
prompt consisting of the problem and these candidates. However, LLMs struggle
to perform refinement effectively when prompted directly. Therefore, we design
a hybrid training pipeline by jointly optimizing for two complementary
objectives, solving problems directly and refining candidate responses.
Experimental results demonstrate that our method achieves state-of-the-art
performance across five mathematical benchmarks. We further show that this
learned self-refinement skill is a model-agnostic enhancement, robust across
different model scales and generalizing to out-of-distribution reasoning tasks.

</details>


### [25] [Centralized vs. Federated Learning for Educational Data Mining: A Comparative Study on Student Performance Prediction with SAEB Microdata](https://arxiv.org/abs/2509.00086)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 这篇论文研究了在巴西教育数据保护法规下使用联邦学习预测学生成绩的可行性，联邦模型在仅损失小部分准确性的情况下实现了数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: 教育领域的数据挖掘和AI应用遇到隐私法规的障碍，需要找到既能保护敏感学生数据隐私、又能建立预测模型的解决方案。

Method: 采用联邦学习中的FedProx算法，在50所学校模拟环境中训练深度神经网络模型，并与中心化XGBoost模型进行性能对比。

Result: 中心化模型准确率为63.96%，联邦模型达到峰值61.23%，性能损失微小但提供了强大的隐私保护。

Conclusion: 联邦学习是一种可行且有效的解决方案，能够在符合巴西数据保护法要求的前提下建立协作预测模型。

Abstract: The application of data mining and artificial intelligence in education
offers unprecedented potential for personalizing learning and early
identification of at-risk students. However, the practical use of these
techniques faces a significant barrier in privacy legislation, such as Brazil's
General Data Protection Law (LGPD), which restricts the centralization of
sensitive student data. To resolve this challenge, privacy-preserving
computational approaches are required. The present study evaluates the
feasibility and effectiveness of Federated Learning, specifically the FedProx
algorithm, to predict student performance using microdata from the Brazilian
Basic Education Assessment System (SAEB). A Deep Neural Network (DNN) model was
trained in a federated manner, simulating a scenario with 50 schools, and its
performance was rigorously benchmarked against a centralized eXtreme Gradient
Boosting (XGBoost) model. The analysis, conducted on a universe of over two
million student records, revealed that the centralized model achieved an
accuracy of 63.96%. Remarkably, the federated model reached a peak accuracy of
61.23%, demonstrating a marginal performance loss in exchange for a robust
privacy guarantee. The results indicate that Federated Learning is a viable and
effective solution for building collaborative predictive models in the
Brazilian educational context, in alignment with the requirements of the LGPD.

</details>


### [26] [Yet Unnoticed in LSTM: Binary Tree Based Input Reordering, Weight Regularization, and Gate Nonlinearization](https://arxiv.org/abs/2509.00087)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: 本文提出三种改进LSTM的方法：输入重排序、权重归一化和门非线性化，在文本分类任务中提升了LSTM的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LSTM模型虽然通过门控机制处理长期信息，但仍存在不足：无法最优关注特定旧索引信息、缺乏权重归一化研究、门控非线性化程度不够。

Method: 1) 输入重排序方法优先处理特定输入索引；2) 通过监督损失函数选择最佳Lp范数进行权重归一化；3) 使用小型前馈神经网络对门控进行非线性化增强。

Result: 在文本分类任务中，所提出的方法相比简单LSTM模型提高了准确率。

Conclusion: 输入重排序、权重归一化和门非线性化三种方法能有效提升LSTM性能，特别是在处理长期依赖关系和特定输入信息方面表现更优。

Abstract: LSTM models used in current Machine Learning literature and applications, has
a promising solution for permitting long term information using gating
mechanisms that forget and reduce effect of current input information. However,
even with this pipeline, they do not optimally focus on specific old index or
long-term information. This paper elaborates upon input reordering approaches
to prioritize certain input indices. Moreover, no LSTM based approach is found
in the literature that examines weight normalization while choosing the right
weight and exponent of Lp norms through main supervised loss function. In this
paper, we find out which norm best finds relationship between weights to either
smooth or sparsify them. Lastly, gates, as weighted representations of inputs
and states, which control reduction-extent of current input versus previous
inputs (~ state), are not nonlinearized enough (through a small FFNN). As
analogous to attention mechanisms, gates easily filter current information to
bold (emphasize on) past inputs. Nonlinearized gates can more easily tune up to
peculiar nonlinearities of specific input in the past. This type of
nonlinearization is not proposed in the literature, to the best of author's
knowledge. The proposed approaches are implemented and compared with a simple
LSTM to understand their performance in text classification tasks. The results
show they improve accuracy of LSTM.

</details>


### [27] [Learning from Peers: Collaborative Ensemble Adversarial Training](https://arxiv.org/abs/2509.00089)
*Li Dengjin,Guo Yanming,Xie Yuxiang,Li Zheng,Chen Jiangming,Li Xiaolong,Lao Mingrui*

Main category: cs.LG

TL;DR: 提出协作集成对抗训练(CEAT)，通过子模型间的预测差异自适应加权样本，增强集成模型的对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有集成对抗训练方法让子模型独立训练，忽略了子模型间的协作效益。研究发现子模型分类差异大的样本更接近集成决策边界，对集成鲁棒性影响更大

Method: CEAT方法利用子模型间的概率差异自适应分配样本权重，通过校准距离正则化，让预测差异大的样本在其他子模型训练中获得更多关注

Result: 在广泛采用的数据集上实验表明，CEAT在竞争性EAT方法中实现了最先进的性能

Conclusion: CEAT是模型无关的，可以无缝适配到各种集成方法中，具有灵活的适用性

Abstract: Ensemble Adversarial Training (EAT) attempts to enhance the robustness of
models against adversarial attacks by leveraging multiple models. However,
current EAT strategies tend to train the sub-models independently, ignoring the
cooperative benefits between sub-models. Through detailed inspections of the
process of EAT, we find that that samples with classification disparities
between sub-models are close to the decision boundary of ensemble, exerting
greater influence on the robustness of ensemble. To this end, we propose a
novel yet efficient Collaborative Ensemble Adversarial Training (CEAT), to
highlight the cooperative learning among sub-models in the ensemble. To be
specific, samples with larger predictive disparities between the sub-models
will receive greater attention during the adversarial training of the other
sub-models. CEAT leverages the probability disparities to adaptively assign
weights to different samples, by incorporating a calibrating distance
regularization. Extensive experiments on widely-adopted datasets show that our
proposed method achieves the state-of-the-art performance over competitive EAT
methods. It is noteworthy that CEAT is model-agnostic, which can be seamlessly
adapted into various ensemble methods with flexible applicability.

</details>


### [28] [Robust Detection of Synthetic Tabular Data under Schema Variability](https://arxiv.org/abs/2509.00092)
*G. Charbel N. Kindji,Elisa Fromont,Lina Maria Rojas-Barahona,Tanguy Urvoy*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的数据点级Transformer架构，用于检测生成式表格数据，在复杂的异构表格结构中显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着强大生成模型的兴起，数据真伯性成为关注点。虽然图像和文本检测方法得到了广泛研究，但表格数据的检测被忽视，而表格数据的异质性结构和测试时未见格式使得检测更加困难。

Method: 提出了一种新的数据点级Transformer架构，并结合表格适配组件，以处理变量和未见的表格格式。

Result: 新方法显著超过了之前的基准方法，AUC和准确率同时提升7个百分点。通过表格适配组件，模型进一步提升了7个准确率百分点，显示出更强的稳健性。

Conclusion: 这项工作首次提供了强有力证据，证明在实际条件下检测生成式表格数据不仅可行，而且能够高可靠地完成。

Abstract: The rise of powerful generative models has sparked concerns over data
authenticity. While detection methods have been extensively developed for
images and text, the case of tabular data, despite its ubiquity, has been
largely overlooked. Yet, detecting synthetic tabular data is especially
challenging due to its heterogeneous structure and unseen formats at test time.
We address the underexplored task of detecting synthetic tabular data in the
wild, where tables have variable and previously unseen schemas. We introduce a
novel datum-wise transformer architecture that significantly outperforms the
only previously published baseline, improving both AUC and accuracy by 7
points. By incorporating a table-adaptation component, our model gains an
additional 7 accuracy points, demonstrating enhanced robustness. This work
provides the first strong evidence that detecting synthetic tabular data in
real-world conditions is not only feasible, but can be done with high
reliability.

</details>


### [29] [Financial Decision Making using Reinforcement Learning with Dirichlet Priors and Quantum-Inspired Genetic Optimization](https://arxiv.org/abs/2509.00095)
*Prasun Nandy,Debjit Dhar,Rik Das*

Main category: cs.LG

TL;DR: 提出混合强化学习框架，结合Dirichlet随机性和量子变异遗传优化，用于动态预算分配，在苹果公司财务数据上实现高精度匹配。


<details>
  <summary>Details</summary>
Motivation: 传统预算分配模型难以处理现实世界金融数据的随机性和非线性特征，需要更智能的自适应方法。

Method: 使用强化学习代理学习研发与行政预算分配，采用Dirichlet分布模拟状态演化，量子变异遗传算法优化策略避免局部最优。

Result: 在未见财务数据上实现与实际分配高度一致（余弦相似度0.9990，KL散度0.0023）。

Conclusion: 深度强化学习、随机建模和量子启发式算法的结合为企业自适应预算提供了有前景的解决方案。

Abstract: Traditional budget allocation models struggle with the stochastic and
nonlinear nature of real-world financial data. This study proposes a hybrid
reinforcement learning (RL) framework for dynamic budget allocation, enhanced
with Dirichlet-inspired stochasticity and quantum mutation-based genetic
optimization. Using Apple Inc. quarterly financial data (2009 to 2025), the RL
agent learns to allocate budgets between Research and Development and Selling,
General and Administrative to maximize profitability while adhering to
historical spending patterns, with L2 penalties discouraging unrealistic
deviations. A Dirichlet distribution governs state evolution to simulate
shifting financial contexts. To escape local minima and improve generalization,
the trained policy is refined using genetic algorithms with quantum mutation
via parameterized qubit rotation circuits. Generation-wise rewards and
penalties are logged to visualize convergence and policy behavior. On unseen
fiscal data, the model achieves high alignment with actual allocations (cosine
similarity 0.9990, KL divergence 0.0023), demonstrating the promise of
combining deep RL, stochastic modeling, and quantum-inspired heuristics for
adaptive enterprise budgeting.

</details>


### [30] [Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs](https://arxiv.org/abs/2509.00096)
*Yao Fu,Runchao Li,Xianxuan Long,Haotian Yu,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.LG

TL;DR: 该论文发现神经网络剪枝会破坏LLM内部用于谎言检测的关键激活特征，提出了TPLO方法通过关注具有更多激活异常值和更强判别特征的层来保持剪枝后模型的谎言检测能力。


<details>
  <summary>Details</summary>
Motivation: 神经网络剪枝虽然能在低资源场景下部署LLM并保持下游任务性能，但首次发现这种剪枝会破坏LLM内部用于谎言检测的关键激活特征，这提出了一个重要问题：如何在剪枝时不牺牲这些关键的谎言检测能力？

Method: 提出了Truthful Pruning aligned by Layer-wise Outliers (TPLO)方法，同时更重视具有更多激活异常值和更强判别特征的层，并引入提示规则来丰富TruthfulQA基准以更好地校准LLM剪枝。

Result: 实验结果显示，该方法在50%稀疏度下实现了88%的幻觉检测准确率，并提高了剪枝后LLM在TruthfulQA上的性能。

Conclusion: TPLO方法能够有效保持LLM的原始性能，同时保留内部状态的关键特征，从而实现稳健的谎言检测，解决了剪枝对LLM谎言检测能力的负面影响问题。

Abstract: Neural network pruning has emerged as a promising approach for deploying LLMs
in low-resource scenarios while preserving downstream task performance.
However, for the first time, we reveal that such pruning disrupts LLMs'
internal activation features crucial for lie detection, where probing
classifiers (typically small logistic regression models) trained on these
features assess the truthfulness of LLM-generated statements. This discovery
raises a crucial open question: how can we prune LLMs without sacrificing these
critical lie detection capabilities? Our investigation further reveals that
naively adjusting layer-wise pruning sparsity based on importance inadvertently
removes crucial weights, failing to improve lie detection performance despite
its reliance on the most crucial LLM layer. To address this issue, we propose
Truthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater
emphasis on layers with more activation outliers and stronger discriminative
features simultaneously. This preserves LLMs' original performance while
retaining critical features of inner states needed for robust lie detection.
Moreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for
better calibrating LLM pruning. Empirical results show that our approach
improves the hallucination detection for pruned LLMs (achieving 88% accuracy at
50% sparsity) and enhances their performance on TruthfulQA.

</details>


### [31] [Progressive Element-wise Gradient Estimation for Neural Network Quantization](https://arxiv.org/abs/2509.00097)
*Kaiqi Zhao*

Main category: cs.LG

TL;DR: PEGE是一种替代STE的渐进式梯度估计方法，通过混合精度替换策略和离散化误差最小化，显著提升低比特量化模型的精度。


<details>
  <summary>Details</summary>
Motivation: 传统STE方法忽略了离散化误差，导致低比特量化时精度下降，需要更有效的梯度估计方法来改善量化感知训练。

Method: 提出渐进式逐元素梯度估计(PEGE)，采用对数课程驱动的混合精度替换策略，将量化训练构建为同时最小化任务损失和离散化误差的协同优化问题。

Result: 在CIFAR-10和ImageNet数据集上，PEGE在各种架构(ResNet、VGG)上均优于现有方法，使低精度模型达到甚至超过全精度模型的准确率。

Conclusion: PEGE提供了一个统一且可推广的框架，能够有效解决低比特量化中的离散化误差问题，显著提升量化模型性能。

Abstract: Neural network quantization aims to reduce the bit-widths of weights and
activations, making it a critical technique for deploying deep neural networks
on resource-constrained hardware. Most Quantization-Aware Training (QAT)
methods rely on the Straight-Through Estimator (STE) to address the
non-differentiability of discretization functions by replacing their
derivatives with that of the identity function. While effective, STE overlooks
discretization errors between continuous and quantized values, which can lead
to accuracy degradation -- especially at extremely low bit-widths. In this
paper, we propose Progressive Element-wise Gradient Estimation (PEGE), a simple
yet effective alternative to STE, which can be seamlessly integrated with any
forward propagation methods and improves the quantized model accuracy. PEGE
progressively replaces full-precision weights and activations with their
quantized counterparts via a novel logarithmic curriculum-driven
mixed-precision replacement strategy. Then it formulates QAT as a
co-optimization problem that simultaneously minimizes the task loss for
prediction and the discretization error for quantization, providing a unified
and generalizable framework. Extensive experiments on CIFAR-10 and ImageNet
across various architectures (e.g., ResNet, VGG) demonstrate that PEGE
consistently outperforms existing backpropagation methods and enables
low-precision models to match or even outperform the accuracy of their
full-precision counterparts.

</details>


### [32] [LLM-QUBO: An End-to-End Framework for Automated QUBO Transformation from Natural Language Problem Descriptions](https://arxiv.org/abs/2509.00099)
*Huixiang Zhang,Mahzabeen Emu,Salimur Choudhury*

Main category: cs.LG

TL;DR: LLM-QUBO是一个端到端框架，使用大语言模型自动将自然语言问题描述转换为QUBO格式，并结合混合量子经典Benders分解方法解决量子硬件可扩展性限制


<details>
  <summary>Details</summary>
Motivation: 量子退火在解决NP难组合优化问题方面具有潜力，但面临两个主要挑战：将问题描述手动转换为QUBO格式的复杂过程，以及当前量子硬件的可扩展性限制

Method: 利用大语言模型解析自然语言并自动生成结构化数学表示；集成混合量子经典Benders分解方法，将组合复杂的主问题编译为紧凑的QUBO格式，同时将线性结构的子问题委托给经典求解器

Result: 通过经典求解器验证了生成的QUBO的正确性和混合方法的可扩展性，建立了稳健的性能基准，证明了框架对量子硬件的准备就绪

Conclusion: 提出了一个协同计算范式，桥接经典AI和量子计算，解决了优化问题实际应用中的关键挑战，显著降低了入门门槛，为将量子设备转变为大规模实际优化挑战的可访问加速器提供了可行途径

Abstract: Quantum annealing offers a promising paradigm for solving NP-hard
combinatorial optimization problems, but its practical application is severely
hindered by two challenges: the complex, manual process of translating problem
descriptions into the requisite Quadratic Unconstrained Binary Optimization
(QUBO) format and the scalability limitations of current quantum hardware. To
address these obstacles, we propose a novel end-to-end framework, LLM-QUBO,
that automates this entire formulation-to-solution pipeline. Our system
leverages a Large Language Model (LLM) to parse natural language, automatically
generating a structured mathematical representation. To overcome hardware
limitations, we integrate a hybrid quantum-classical Benders' decomposition
method. This approach partitions the problem, compiling the combinatorial
complex master problem into a compact QUBO format, while delegating linearly
structured sub-problems to classical solvers. The correctness of the generated
QUBO and the scalability of the hybrid approach are validated using classical
solvers, establishing a robust performance baseline and demonstrating the
framework's readiness for quantum hardware. Our primary contribution is a
synergistic computing paradigm that bridges classical AI and quantum computing,
addressing key challenges in the practical application of optimization problem.
This automated workflow significantly reduces the barrier to entry, providing a
viable pathway to transform quantum devices into accessible accelerators for
large-scale, real-world optimization challenges.

</details>


### [33] [Exploiting a Mixture-of-Layers in an Electrocardiography Foundation Model](https://arxiv.org/abs/2509.00102)
*Phu X. Nguyen,Huy Phan,Hieu Pham,Christos Chatzichristos,Bert Vandenberk,Maarten De Vos*

Main category: cs.LG

TL;DR: 提出PMA方法，通过门控网络选择性融合Transformer各层表示，提升ECG下游任务性能


<details>
  <summary>Details</summary>
Motivation: 现有ECG Transformer模型仅使用最后一层表示，但理论和实证分析表明这不是最优选择，需要有效利用各层表示的多样性

Method: 1. 使用1D ViT通过掩码建模预训练ECG信号；2. 下游任务中使用门控网络选择性融合预训练模型各层表示；3. 扩展方法在预训练阶段通过分组平均聚合所有表示

Result: 提出的PMA方法能够增强表示能力并改善下游应用性能

Conclusion: 通过有效利用Transformer各层表示的多样性，可以显著提升ECG基础模型在下游任务中的表现

Abstract: Transformer-based foundation models for Electrocardiograms (ECGs) have
recently achieved impressive performance in many downstream applications.
However, the internal representations of such models across layers have not
been fully understood and exploited. An important question arises: Does the
final layer of the pre-trained Transformer model, the \emph{de facto}
representational layer, provide optimal performance for downstream tasks?
Although our answer based on empirical and theoretical analyses for this
question is negative, we propose a novel approach to leverage the
representation diversity of the model's layers effectively. Specifically, we
introduce a novel architecture called Post-pretraining Mixture-of-layers
Aggregation (PMA), which enables a flexible combination of the layer-wise
representations from the layer stack of a Transformer-based foundation model.
We first pre-train the model from ECG signals using the 1-dimensional Vision
Transformer (ViT) via masked modeling. In downstream applications, instead of
relying solely on the last layer of the model, we employ a gating network to
selectively fuse the representations from the pretrained model's layers,
thereby enhancing representation power and improving performance of the
downstream applications. In addition, we extend the proposed method to the
pretraining stage by aggregating all representations through group-wise
averaging before feeding them into the decoder-based Transformer.

</details>


### [34] [Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers](https://arxiv.org/abs/2509.00103)
*Robert MacKnight,Jose Emilio Regio,Jeffrey G. Ethier,Luke A. Baldwin,Gabe Gomes*

Main category: cs.LG

TL;DR: LLM引导的优化在化学实验参数搜索中表现优于传统贝叶斯优化，特别是在复杂分类参数空间和性能条件稀缺的情况下，同时保持更高的探索多样性。


<details>
  <summary>Details</summary>
Motivation: 传统实验化学优化使用黑盒参数空间的算法搜索，本研究探索预训练大语言模型如何改变这一范式，利用其领域知识更有效地导航化学参数空间。

Method: 使用6个完全枚举的分类反应数据集（768-5684个实验），比较LLM引导优化与贝叶斯优化和随机采样的性能，并引入拓扑无关的信息论框架量化采样多样性。

Result: 前沿LLM在5个单目标数据集上一致匹配或超越贝叶斯优化性能，优势随参数复杂性增加和高性能条件稀缺性而增强。LLM在所有数据集中保持比贝叶斯优化更高的探索熵。

Conclusion: LLM引导优化在传统方法困难的领域表现出色：需要领域理解而非数学优化的复杂分类空间，预训练领域知识实现了更有效的化学参数空间导航。

Abstract: Modern optimization in experimental chemistry employs algorithmic search
through black-box parameter spaces. Here we demonstrate that pre-trained
knowledge in large language models (LLMs) fundamentally changes this paradigm.
Using six fully enumerated categorical reaction datasets (768 - 5,684
experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian
optimization (BO) and random sampling. Frontier LLMs consistently match or
exceed BO performance across five single-objective datasets, with advantages
growing as parameter complexity increases and high-performing conditions become
scarce (<5% of space). BO retains superiority only for explicit multi-objective
trade-offs. To understand these contrasting behaviors, we introduce a
topology-agnostic information theory framework quantifying sampling diversity
throughout optimization campaigns. This analysis reveals that LLMs maintain
systematically higher exploration entropy than BO across all datasets while
achieving superior performance, with advantages most pronounced in
solution-scarce parameter spaces where high-entropy exploration typically fails
- suggesting that pre-trained domain knowledge enables more effective
navigation of chemical parameter space rather than replacing structured
exploration strategies. To enable transparent benchmarking and community
validation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a
no-code platform for side-by-side evaluation of human, algorithmic, and LLM
optimization campaigns with public leaderboards and complete trajectories. Our
findings establish that LLM-GO excels precisely where traditional methods
struggle: complex categorical spaces requiring domain understanding rather than
mathematical optimization.

</details>


### [35] [Principled Approximation Methods for Efficient and Scalable Deep Learning](https://arxiv.org/abs/2509.00174)
*Pedro Savarese*

Main category: cs.LG

TL;DR: 该论文研究深度学习的效率优化方法，包括模型压缩、架构设计和优化算法，通过可微近似解决离散约束问题，显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的计算和能耗需求急剧增长，阻碍了其部署和广泛应用，需要研究高效的近似方法来提升深度学习系统的效率。

Method: 提出三种主要方法：1）模型压缩方面，通过将离散问题转化为连续可微问题，实现剪枝和量化的梯度训练；2）架构设计方面，利用参数共享探索隐式循环架构；3）优化算法方面，研究自适应优化器的理论性质并提出快速超参数调优方法。

Result: 在图像分类、语言建模和生成建模任务上的实验结果表明，所提方法在保持甚至提升模型性能的同时，显著提高了训练和推理效率。

Conclusion: 通过可扩展和原则性的近似方法，成功解决了计算困难问题，为深度学习的高效部署提供了有效的技术方案。

Abstract: Recent progress in deep learning has been driven by increasingly larger
models. However, their computational and energy demands have grown
proportionally, creating significant barriers to their deployment and to a
wider adoption of deep learning technologies. This thesis investigates
principled approximation methods for improving the efficiency of deep learning
systems, with a particular focus on settings that involve discrete constraints
and non-differentiability.
  We study three main approaches toward improved efficiency: architecture
design, model compression, and optimization. For model compression, we propose
novel approximations for pruning and quantization that frame the underlying
discrete problem as continuous and differentiable, enabling gradient-based
training of compression schemes alongside the model's parameters. These
approximations allow for fine-grained sparsity and precision configurations,
leading to highly compact models without significant fine-tuning. In the
context of architecture design, we design an algorithm for neural architecture
search that leverages parameter sharing across layers to efficiently explore
implicitly recurrent architectures. Finally, we study adaptive optimization,
revisiting theoretical properties of widely used methods and proposing an
adaptive optimizer that allows for quick hyperparameter tuning.
  Our contributions center on tackling computationally hard problems via
scalable and principled approximations. Experimental results on image
classification, language modeling, and generative modeling tasks show that the
proposed methods provide significant improvements in terms of training and
inference efficiency while maintaining, or even improving, the model's
performance.

</details>


### [36] [FNODE: Flow-Matching for data-driven simulation of constrained multibody systems](https://arxiv.org/abs/2509.00183)
*Hongyu Wang,Jingquan Wang,Dan Negrut*

Main category: cs.LG

TL;DR: FNODE框架通过直接学习加速度向量场来提升约束多体系统的数据驱动建模性能，避免了传统神经ODE的反向传播瓶颈，在多个基准测试中表现出优越的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决约束多体系统数据驱动建模中的两个持久挑战：高计算成本和有限的长期预测准确性。传统神经ODE需要通过ODE求解器进行反向传播，这成为计算瓶颈。

Method: 提出Flow-Matching神经ODE（FNODE）框架，直接从轨迹数据学习加速度向量场。通过重新制定训练目标来监督加速度而非积分状态，使用混合FFT和有限差分方案高效计算加速度目标。

Result: 在单/三质量-弹簧-阻尼系统、双摆、滑块曲柄和车杆等多个基准测试中，FNODE始终优于MBD-NODE、LSTM和FCNN等现有方法，展现出良好的准确性、泛化能力和计算效率。

Conclusion: FNODE通过直接学习加速度场的方法有效解决了约束多体系统建模的计算瓶颈问题，为数据驱动的多体动力学建模提供了高效准确的解决方案。

Abstract: Data-driven modeling of constrained multibody systems faces two persistent
challenges: high computational cost and limited long-term prediction accuracy.
To address these issues, we introduce the Flow-Matching Neural Ordinary
Differential Equation (FNODE), a framework that learns acceleration vector
fields directly from trajectory data. By reformulating the training objective
to supervise accelerations rather than integrated states, FNODE eliminates the
need for backpropagation through an ODE solver, which represents a bottleneck
in traditional Neural ODEs. Acceleration targets are computed efficiently using
numerical differentiation techniques, including a hybrid Fast Fourier Transform
(FFT) and Finite Difference (FD) scheme. We evaluate FNODE on a diverse set of
benchmarks, including the single and triple mass-spring-damper systems, double
pendulum, slider-crank, and cart-pole. Across all cases, FNODE consistently
outperforms existing approaches such as Multi-Body Dynamic Neural ODE
(MBD-NODE), Long Short-Term Memory (LSTM) networks, and Fully Connected Neural
Networks (FCNN), demonstrating good accuracy, generalization, and computational
efficiency.

</details>


### [37] [Democratizing Agentic AI with Fast Test-Time Scaling on the Edge](https://arxiv.org/abs/2509.00195)
*Hao Mark Chen,Zhiwen Mo,Guanxi Lu,Shuang Liang,Lingxiao Ma,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: FlashTTS是一个针对边缘设备上测试时缩放(TTS)的优化服务系统，通过三种协同优化技术显著提升推理性能和效率，使小型LLM在边缘设备上达到大型云模型的准确性和延迟水平。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署AI代理对隐私和响应性至关重要，但内存限制通常只能使用推理能力较差的小型LLM。现有的测试时缩放方法在边缘硬件上开销过大，需要一种实用的解决方案。

Method: FlashTTS引入三种优化技术：1)推测性波束扩展缓解系统瓶颈；2)非对称多模型内存分配动态平衡生成和验证内存；3)动态前缀感知调度最大化KV缓存重用。作为vLLM的即插即用库实现。

Result: 在单消费级GPU(24GB)上，FlashTTS使边缘LLM匹配大型云模型的准确性和延迟。相比vLLM基线，平均吞吐量提升2.2倍，延迟降低38%-68%。

Conclusion: FlashTTS为边缘设备上高性能AI代理的普及铺平了道路，通过优化TTS技术使小型LLM在资源受限环境中实现与大型模型相当的性能。

Abstract: Deploying agentic AI on edge devices is crucial for privacy and
responsiveness, but memory constraints typically relegate these systems to
smaller Large Language Models (LLMs) with inferior reasoning capabilities.
Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more
compute during inference, but existing methods incur prohibitive overhead on
edge hardware. To overcome this, we introduce FlashTTS, a serving system that
makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces
three synergistic optimizations: (i) Speculative Beam Extension to mitigate
system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model
Memory Allocation to dynamically balance memory between generation and
verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache
reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on
a single consumer GPU (24 GB) to match the accuracy and latency of large cloud
models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x
higher goodput and reduces latency by 38%-68% compared to a vLLM baseline,
paving the way for democratized, high-performance agentic AI on edge devices.

</details>


### [38] [From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive Inference](https://arxiv.org/abs/2509.00202)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: TConstFormer通过周期性状态更新机制实现恒定大小的KV缓存和O(1)计算复杂度，在长文本推理任务中展现出显著的速度和内存效率优势


<details>
  <summary>Details</summary>
Motivation: Transformer的自回归推理存在KV缓存线性增长和O(N²d)计算复杂度问题，严重限制了其处理超长序列的能力

Method: 基于TLinFormer工作，采用创新的周期性状态更新机制，在k-1个连续步骤中执行恒定时间计算，仅在第k步执行一次线性时间的全局信息同步

Result: 理论计算和实验结果表明，TConstFormer在长文本推理任务的速度、内存效率和整体性能方面相比基线模型具有压倒性优势

Conclusion: 这一突破为高效和鲁棒的流式语言模型应用铺平了道路

Abstract: Although the Transformer has become the cornerstone of modern AI, its
autoregressive inference suffers from a linearly growing KV Cache and a
computational complexity of O(N^2 d), severely hindering its ability to process
ultra-long sequences. To overcome this limitation, this paper introduces the
TConstFormer architecture, building upon our previous work, TLinFormer.
TConstFormer employs an innovative periodic state update mechanism to achieve a
truly constant-size O(1) KV Cache. The computational complexity of this
mechanism is also O(1) in an amortized sense: it performs purely constant-time
computations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single
linear-time global information synchronization only on the $k$-th step.
Theoretical calculations and experimental results demonstrate that TConstFormer
exhibits an overwhelming advantage over baseline models in terms of speed,
memory efficiency, and overall performance on long-text inference tasks. This
breakthrough paves the way for efficient and robust streaming language model
applications.

</details>


### [39] [Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements](https://arxiv.org/abs/2509.00203)
*Xuyang Li,Mahdi Masmoudi,Rami Gharbi,Nizar Lajnef,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: Neptune是一种新的参数场推断方法，使用独立坐标神经网络从稀疏观测中准确估计非线性时空变化的PDE参数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数估计方法（如稀疏识别和PINNs）在处理非线性动态、多物理场相互作用或有限系统响应观测时表现不佳，特别是在参数呈现非线性和时空变化的情况下。

Method: Neptune采用独立坐标神经网络在物理空间或状态变量中连续表示每个参数场，能够从稀疏的系统响应测量中推断参数场。

Result: 在多个物理和生物医学问题中，Neptune仅需50个观测点就能实现稳健的参数估计，相比PINNs将参数估计误差降低了两个数量级，动态响应预测误差降低了十倍，并展现出优异的泛化能力。

Conclusion: Neptune通过实现可靠且数据高效的参数推断，有望在工程、医疗等领域产生广泛的变革性影响。

Abstract: Parameterized partial differential equations (PDEs) underpin the mathematical
modeling of complex systems in diverse domains, including engineering,
healthcare, and physics. A central challenge in using PDEs for real-world
applications is to accurately infer the parameters, particularly when the
parameters exhibit non-linear and spatiotemporal variations. Existing parameter
estimation methods, such as sparse identification and physics-informed neural
networks (PINNs), struggle in such cases, especially with nonlinear dynamics,
multiphysics interactions, or limited observations of the system response. To
address these challenges, we introduce Neptune, a general-purpose method
capable of inferring parameter fields from sparse measurements of system
responses. Neptune employs independent coordinate neural networks to
continuously represent each parameter field in physical space or in state
variables. Across various physical and biomedical problems, where direct
parameter measurements are prohibitively expensive or unattainable, Neptune
significantly outperforms existing methods, achieving robust parameter
estimation from as few as 50 observations, reducing parameter estimation errors
by two orders of magnitude and dynamic response prediction errors by a factor
of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation
capabilities, enabling accurate predictions in regimes beyond training data
where PINN fail. By facilitating reliable and data-efficient parameter
inference, Neptune promises broad transformative impacts in engineering,
healthcare, and beyond.

</details>


### [40] [Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data](https://arxiv.org/abs/2509.00221)
*Jaya Narain,Zakaria Aldeneh,Shirley Ren*

Main category: cs.LG

TL;DR: 语音基础模型（HuBERT和wav2vec 2.0）的特征提取器在可穿戴传感器时间序列任务中表现优异，超越了针对特定模态训练的自监督模型，特别是在情绪分类、心律失常检测和活动分类任务中。


<details>
  <summary>Details</summary>
Motivation: 探索语音和传感器时间序列数据在时域和频域的共性，研究语音基础模型学到的表征是否具有领域无关性，能否提升数据稀缺时间序列任务的性能。

Method: 使用HuBERT和wav2vec 2.0等语音基础模型提取特征，然后在这些特征上训练简单的探针分类器，应用于可穿戴传感器的时间序列分类任务。

Result: 语音模型的卷积特征编码器在可穿戴传感器任务中表现特别突出，在多个任务上达到了最先进的性能，且具有更好的鲁棒性。

Conclusion: 语音基础模型学习到的表征具有领域无关性，可以有效地迁移到传感器时间序列任务中，为构建通用的语音和传感器时间序列模型奠定了基础。

Abstract: Both speech and sensor time series data encode information in both the time-
and frequency- domains, like spectral powers and waveform shapelets. We show
that speech foundation models learn representations that are domain-independent
and achieve state-of-the-art performance on time series tasks from wearable
sensors. Probes trained on features extracted from HuBERT and wav2vec 2.0
outperform those extracted from self-supervised models trained directly on
modality specific datasets for mood classification, arrhythmia detection, and
activity classification tasks. We find a particularly strong relevance of the
convolutional feature encoders from speech models for wearable sensor tasks.
The methods proposed here improve performance and robustness for data-scarce
time series tasks, using simple probing methods. This work is a step towards
generalized time series models for speech and sensor data, a topic for further
exploration.

</details>


### [41] [Quantum-Optimized Selective State Space Model for Efficient Time Series Prediction](https://arxiv.org/abs/2509.00259)
*Stefan-Alexandru Jura,Mihai Udrescu,Alexandru Topirceanu*

Main category: cs.LG

TL;DR: 提出量子优化选择性状态空间模型(Q-SSM)，结合状态空间模型和变分量子门，通过量子门控机制自适应调节记忆更新，解决了长时序预测中的非平稳性、多尺度依赖和噪声鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型存在二次复杂度问题，在极长时序上性能下降；状态空间模型如S-Mamba虽然提供线性时间更新，但训练不稳定、对初始化敏感且多变量预测鲁棒性有限。需要一种既能保持高效性又能提升稳定性和鲁棒性的方法。

Method: 提出混合量子优化方法Q-SSM，集成状态空间动力学和变分量子门。使用参数化量子电路(RY-RX ansatz)的期望值来自适应调节记忆更新，替代昂贵的注意力机制，提供轻量级量子门控方案。

Result: 在ETT、Traffic和Exchange Rate三个基准数据集上验证，Q-SSM consistently优于强基线(LSTM、TCN、Reformer)、Transformer模型和S-Mamba，证明了变分量子门控在长程预测中的有效性。

Conclusion: 变分量子门控能够解决当前长程预测的局限性，实现准确且鲁棒的多变量预测，量子优化方法为时序预测提供了新的有效途径。

Abstract: Long-range time series forecasting remains challenging, as it requires
capturing non-stationary and multi-scale temporal dependencies while
maintaining noise robustness, efficiency, and stability. Transformer-based
architectures such as Autoformer and Informer improve generalization but suffer
from quadratic complexity and degraded performance on very long time horizons.
State space models, notably S-Mamba, provide linear-time updates but often face
unstable training dynamics, sensitivity to initialization, and limited
robustness for multivariate forecasting. To address such challenges, we propose
the Quantum-Optimized Selective State Space Model (Q-SSM), a hybrid
quantum-optimized approach that integrates state space dynamics with a
variational quantum gate. Instead of relying on expensive attention mechanisms,
Q-SSM employs a simple parametrized quantum circuit (RY-RX ansatz) whose
expectation values regulate memory updates adaptively. This quantum gating
mechanism improves convergence stability, enhances the modeling of long-term
dependencies, and provides a lightweight alternative to attention. We
empirically validate Q-SSM on three widely used benchmarks, i.e., ETT, Traffic,
and Exchange Rate. Results show that Q-SSM consistently improves over strong
baselines (LSTM, TCN, Reformer), Transformer-based models, and S-Mamba. These
findings demonstrate that variational quantum gating can address current
limitations in long-range forecasting, leading to accurate and robust
multivariate predictions.

</details>


### [42] [Continuously Tempered Diffusion Samplers](https://arxiv.org/abs/2509.00316)
*Ezra Erives,Bowen Jing,Peter Holderrieth,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出连续调温扩散采样器，通过引入温度分布族来改善神经采样器的探索能力，解决传统方法在复杂分布中探索不足的问题


<details>
  <summary>Details</summary>
Motivation: 传统基于退火的神经采样器在训练阶段使用部分学习的传输和退火Langevin动力学生成提案分布，但在孤立模式和其他病理性路径下探索不足，导致训练后性能下降

Method: 提出连续调温扩散采样器，利用分子动力学中的探索技术，引入不同温度下的分布族来降低高温度下的能量壁垒，在目标低温下驱动更好的探索

Result: 实证验证了改进的采样器性能，扩展的探索能力带来了性能提升

Conclusion: 连续调温扩散采样器通过改进提案分布机制，有效解决了神经采样器在复杂分布中的探索不足问题，提升了采样性能

Abstract: Annealing-based neural samplers seek to amortize sampling from unnormalized
distributions by training neural networks to transport a family of densities
interpolating from source to target. A crucial design choice in the training
phase of such samplers is the proposal distribution by which locations are
generated at which to evaluate the loss. Previous work has obtained such a
proposal distribution by combining a partially learned transport with annealed
Langevin dynamics. However, isolated modes and other pathological properties of
the annealing path imply that such proposals achieve insufficient exploration
and thereby lower performance post training. To remedy this, we propose
continuously tempered diffusion samplers, which leverage exploration techniques
developed in the context of molecular dynamics to improve proposal
distributions. Specifically, a family of distributions across different
temperatures is introduced to lower energy barriers at higher temperatures and
drive exploration at the lower temperature of interest. We empirically validate
improved sampler performance driven by extended exploration. Code is available
at https://github.com/eje24/ctds.

</details>


### [43] [Chunked TabPFN: Exact Training-Free In-Context Learning for Long-Context Tabular Data](https://arxiv.org/abs/2509.00326)
*Renat Sergazinov,Shao-An Yin*

Main category: cs.LG

TL;DR: TabPFN v2在多个表格数据基准测试中优于基于树的模型，但受限于Transformer的二次计算复杂度，无法处理超过10K上下文token。本文提出了一种tiled-block策略，使TabPFN能够在标准GPU设置下处理长上下文，无需任何预处理。


<details>
  <summary>Details</summary>
Motivation: 虽然TabPFN v2在表格数据上表现优于传统的基于树模型，但由于Transformer架构的二次计算和内存成本限制，无法处理长上下文（超过10K token）。现有方法依赖上下文压缩（如KNN选择代表性样本），但需要预处理步骤。

Method: 提出了一种tiled-block策略来计算TabPFN框架内的注意力机制。这种方法与标准GPU设置兼容，是首个无需任何预处理就能让TabPFN处理长上下文的方法。

Result: 在标准TabArena基准测试中证明了该方法的有效性，成功解决了TabPFN处理长上下文的限制。

Conclusion: tiled-block策略为TabPFN提供了处理长上下文的能力，突破了Transformer架构在表格数据处理中的计算限制，同时保持了与标准GPU硬件的兼容性。

Abstract: TabPFN v2 achieves better results than tree-based models on several tabular
benchmarks, which is notable since tree-based models are usually the strongest
choice for tabular data. However, it cannot handle more than 10K context tokens
because transformers have quadratic computation and memory costs.
  Unlike existing approaches that rely on context compression, such as
selecting representative samples via K-nearest neighbors (KNN), we introduce a
\textbf{tiled-block} strategy to compute attention within the TabPFN framework.
This design is compatible with standard GPU setups and, to the best of our
knowledge, is the first to enable TabPFN to \textbf{process long contexts
without any pre-processing}. We demonstrate the effectiveness of our approach
on the standard TabArena benchmark.

</details>


### [44] [Counterfactual Risk Minimization with IPS-Weighted BPR and Self-Normalized Evaluation in Recommender Systems](https://arxiv.org/abs/2509.00333)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: 提出了一种结合IPS加权训练、IPS加权BPR目标和倾向性正则化的方法，用于解决推荐系统中曝光偏差问题，减少方差并提高稳定性


<details>
  <summary>Details</summary>
Motivation: 解决隐式反馈推荐系统中的曝光偏差问题，传统IPS方法存在高方差和不稳定性，需要更稳健的解决方案

Method: 整合IPS加权训练与IPS加权BPR目标，并加入倾向性正则化(PR)来缓解极端倾向权重带来的方差放大

Result: 在合成数据和MovieLens 100K数据上的实验表明，该方法在无偏曝光下泛化更好，评估方差比传统IPS方法更低

Conclusion: 该方法为现实推荐场景中的反事实学习和评估提供了实用指导，能够有效改善模型稳健性和评估稳定性

Abstract: Learning and evaluating recommender systems from logged implicit feedback is
challenging due to exposure bias. While inverse propensity scoring (IPS)
corrects this bias, it often suffers from high variance and instability. In
this paper, we present a simple and effective pipeline that integrates
IPS-weighted training with an IPS-weighted Bayesian Personalized Ranking (BPR)
objective augmented by a Propensity Regularizer (PR). We compare Direct Method
(DM), IPS, and Self-Normalized IPS (SNIPS) for offline policy evaluation, and
demonstrate how IPS-weighted training improves model robustness under biased
exposure. The proposed PR further mitigates variance amplification from extreme
propensity weights, leading to more stable estimates. Experiments on synthetic
and MovieLens 100K data show that our approach generalizes better under
unbiased exposure while reducing evaluation variance compared to naive and
standard IPS methods, offering practical guidance for counterfactual learning
and evaluation in real-world recommendation settings.

</details>


### [45] [Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching](https://arxiv.org/abs/2509.00336)
*An B. Vuong,Michael T. McCann,Javier E. Santos,Yen Ting Lin*

Main category: cs.LG

TL;DR: 扩散模型实际上学习的是Wasserstein梯度流的速度场而非真实的分数函数，即使神经网络学到的向量场不是保守的，生成效果依然很好


<details>
  <summary>Details</summary>
Motivation: 传统观点认为扩散模型学习的是噪声数据的对数密度梯度（分数函数），但这要求向量场是保守的，而实际使用的神经网络架构并不强制执行这一约束

Method: 通过数值实验证明训练好的扩散网络违反了真实分数函数的积分和微分约束，提出从Wasserstein梯度流的角度重新理解扩散训练

Result: 发现即使学习的向量场不是保守的，模型仍能出色地生成样本，非保守的神经网络近似误差不一定损害密度传输

Conclusion: WGF视角为理解扩散生成模型提供了更原则性、优雅且理论基础坚实的框架，消除了对反向时间SDE理论的依赖

Abstract: Diffusion models are commonly interpreted as learning the score function,
i.e., the gradient of the log-density of noisy data. However, this assumption
implies that the target of learning is a conservative vector field, which is
not enforced by the neural network architectures used in practice. We present
numerical evidence that trained diffusion networks violate both integral and
differential constraints required of true score functions, demonstrating that
the learned vector fields are not conservative. Despite this, the models
perform remarkably well as generative mechanisms. To explain this apparent
paradox, we advocate a new theoretical perspective: diffusion training is
better understood as flow matching to the velocity field of a Wasserstein
Gradient Flow (WGF), rather than as score learning for a reverse-time
stochastic differential equation. Under this view, the "probability flow"
arises naturally from the WGF framework, eliminating the need to invoke
reverse-time SDE theory and clarifying why generative sampling remains
successful even when the neural vector field is not a true score. We further
show that non-conservative errors from neural approximation do not necessarily
harm density transport. Our results advocate for adopting the WGF perspective
as a principled, elegant, and theoretically grounded framework for
understanding diffusion generative models.

</details>


### [46] [Scalable Option Learning in High-Throughput Environments](https://arxiv.org/abs/2509.00338)
*Mikael Henaff,Scott Fujimoto,Michael Rabbat*

Main category: cs.LG

TL;DR: SOL算法通过解决分层强化学习在大规模训练中的关键挑战，实现了25倍的高吞吐量提升，在NetHack等复杂环境中表现出色


<details>
  <summary>Details</summary>
Motivation: 现有分层强化学习方法在大规模训练中尚未充分发挥潜力，需要解决高吞吐量环境下的扩展性问题

Method: 提出可扩展选项学习(SOL)算法，通过优化分层结构实现高效的大规模训练，在NetHack游戏中使用200亿帧经验进行训练

Result: 在NetHack游戏中显著超越平面智能体，在MiniHack和Mujoco环境中也验证了算法的通用性，吞吐量比现有分层方法提高25倍

Conclusion: SOL算法成功解决了分层强化学习的大规模扩展问题，展示了良好的扩展趋势和通用适用性

Abstract: Hierarchical reinforcement learning (RL) has the potential to enable
effective decision-making over long timescales. Existing approaches, while
promising, have yet to realize the benefits of large-scale training. In this
work, we identify and solve several key challenges in scaling hierarchical RL
to high-throughput environments. We propose Scalable Option Learning (SOL), a
highly scalable hierarchical RL algorithm which achieves a 25x higher
throughput compared to existing hierarchical methods. We train our hierarchical
agents using 20 billion frames of experience on the complex game of NetHack,
significantly surpassing flat agents and demonstrating positive scaling trends.
We also validate our algorithm on MiniHack and Mujoco environments, showcasing
its general applicability. Our code is open sourced at
github.com/facebookresearch/sol.

</details>


### [47] [LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning](https://arxiv.org/abs/2509.00347)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出LLMDPD方法，利用任务特定提示和大语言模型增强离线强化学习的泛化能力


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临泛化挑战，由于离线数据限制，智能体难以适应新任务或环境

Method: 结合文本任务描述和轨迹提示，使用LLM处理文本提示，transformer编码轨迹提示，通过上下文感知的策略级扩散模型实现泛化

Result: 在未见任务上优于现有离线RL方法，显著提升泛化能力和适应性

Conclusion: LLMDPD有效解决了离线RL的泛化问题，为多样化场景中的适应性提供了新思路

Abstract: Reinforcement Learning (RL) is known for its strong decision-making
capabilities and has been widely applied in various real-world scenarios.
However, with the increasing availability of offline datasets and the lack of
well-designed online environments from human experts, the challenge of
generalization in offline RL has become more prominent. Due to the limitations
of offline data, RL agents trained solely on collected experiences often
struggle to generalize to new tasks or environments. To address this challenge,
we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances
generalization in offline RL using task-specific prompts. Our method
incorporates both text-based task descriptions and trajectory prompts to guide
policy learning. We leverage a large language model (LLM) to process text-based
prompts, utilizing its natural language understanding and extensive knowledge
base to provide rich task-relevant context. Simultaneously, we encode
trajectory prompts using a transformer model, capturing structured behavioral
patterns within the underlying transition dynamics. These prompts serve as
conditional inputs to a context-aware policy-level diffusion model, enabling
the RL agent to generalize effectively to unseen tasks. Our experimental
results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods
on unseen tasks, highlighting its effectiveness in improving generalization and
adaptability in diverse settings.

</details>


### [48] [Theory Foundation of Physics-Enhanced Residual Learning](https://arxiv.org/abs/2509.00348)
*Shixiao Liang,Wang Chen,Keke Long,Peng Zhang,Xiaopeng Li,Jintao Ke*

Main category: cs.LG

TL;DR: 本文从理论角度解释了物理增强残差学习(PERL)方法的三大优势：减少神经网络参数需求、加快收敛速度、减少训练样本需求，并通过自动驾驶轨迹预测案例验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 虽然数值实验表明PERL方法在整合物理模型和神经网络方面具有参数少、收敛快、样本需求少等优势，但这些结果缺乏理论解释，需要从理论层面阐明其优势机理。

Method: 研究具有Lipschitz连续性特性的一类通用问题，通过分析损失函数边界与残差学习结构之间的关系，严格证明了一组解释PERL三大优势的理论定理。

Result: 理论分析证实了PERL的三大优势，自动驾驶轨迹预测的数值实验表明，即使使用显著更少的训练样本，PERL仍能比纯神经网络获得更高的精度。

Conclusion: PERL方法在提高预测性能的同时减少了数据需求，在真实世界自动驾驶应用中具有重要价值，特别是在难以获取极端案例数据的场景下。

Abstract: Intensive studies have been conducted in recent years to integrate neural
networks with physics models to balance model accuracy and interpretability.
One recently proposed approach, named Physics-Enhanced Residual Learning
(PERL), is to use learning to estimate the residual between the physics model
prediction and the ground truth. Numeral examples suggested that integrating
such residual with physics models in PERL has three advantages: (1) a reduction
in the number of required neural network parameters; (2) faster convergence
rates; and (3) fewer training samples needed for the same computational
precision. However, these numerical results lack theoretical justification and
cannot be adequately explained.
  This paper aims to explain these advantages of PERL from a theoretical
perspective. We investigate a general class of problems with Lipschitz
continuity properties. By examining the relationships between the bounds to the
loss function and residual learning structure, this study rigorously proves a
set of theorems explaining the three advantages of PERL.
  Several numerical examples in the context of automated vehicle trajectory
prediction are conducted to illustrate the proposed theorems. The results
confirm that, even with significantly fewer training samples, PERL consistently
achieves higher accuracy than a pure neural network. These results demonstrate
the practical value of PERL in real world autonomous driving applications where
corner case data are costly or hard to obtain. PERL therefore improves
predictive performance while reducing the amount of data required.

</details>


### [49] [Optimized Weight Initialization on the Stiefel Manifold for Deep ReLU Neural Networks](https://arxiv.org/abs/2509.00362)
*Hyungu Lee,Taehyeong Kim,Hayoung Choi*

Main category: cs.LG

TL;DR: 提出了一种专门针对ReLU网络优化的正交初始化方法，通过Stiefel流形上的优化问题来校准预激活统计量，解决了深度网络中神经元失活和梯度消失问题


<details>
  <summary>Details</summary>
Motivation: 现有的权重初始化方法（如He、Xavier、正交初始化）在深度ReLU网络中无法有效调节预激活均值和控制激活稀疏性，导致神经元永久失活（dying ReLU）和梯度不稳定问题

Method: 在Stiefel流形上求解优化问题，推导出闭式解和高效采样方案，专门为ReLU激活函数优化正交初始化，从初始化阶段就保持尺度并校准预激活统计量

Result: 理论分析表明该方法能防止dying ReLU问题、减缓激活方差衰减、缓解梯度消失，在MNIST、Fashion-MNIST、表格数据集和少样本设置中均优于现有初始化方法

Conclusion: 该方法能稳定深度架构中的信号和梯度流动，实现深度网络的稳定训练，适用于ReLU系列激活函数

Abstract: Stable and efficient training of ReLU networks with large depth is highly
sensitive to weight initialization. Improper initialization can cause permanent
neuron inactivation dying ReLU and exacerbate gradient instability as network
depth increases. Methods such as He, Xavier, and orthogonal initialization
preserve variance or promote approximate isometry. However, they do not
necessarily regulate the pre-activation mean or control activation sparsity,
and their effectiveness often diminishes in very deep architectures. This work
introduces an orthogonal initialization specifically optimized for ReLU by
solving an optimization problem on the Stiefel manifold, thereby preserving
scale and calibrating the pre-activation statistics from the outset. A family
of closed-form solutions and an efficient sampling scheme are derived.
Theoretical analysis at initialization shows that prevention of the dying ReLU
problem, slower decay of activation variance, and mitigation of gradient
vanishing, which together stabilize signal and gradient flow in deep
architectures. Empirically, across MNIST, Fashion-MNIST, multiple tabular
datasets, few-shot settings, and ReLU-family activations, our method
outperforms previous initializations and enables stable training in deep
networks.

</details>


### [50] [Unifying Adversarial Perturbation for Graph Neural Networks](https://arxiv.org/abs/2509.00387)
*Jinluan Yang,Ruihao Zhang,Zhengyu Chen,Fei Wu,Kun Kuang*

Main category: cs.LG

TL;DR: 本文提出PerturbEmbedding方法，通过在GNN的隐藏嵌入层直接进行扰动操作，为图神经网络提供统一的对抗训练框架，显著提升了GNN的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法主要局限于特定数据集和GNN类型，缺乏统一的扰动框架。本文旨在开发一种通用的方法，能够增强GNN对各种攻击的抵抗力。

Method: 提出PerturbEmbedding方法，直接在GNN的每个隐藏嵌入层执行扰动操作，提供统一的扰动策略框架，支持随机扰动和对抗性扰动两种形式。

Result: 在多个数据集和不同骨干模型上的实验表明，PerturbEmbedding显著提升了GNN的鲁棒性和泛化能力，性能优于现有方法，能够有效抵抗随机和对抗性扰动。

Conclusion: PerturbEmbedding提供了一个有效的统一框架，通过隐藏层嵌入扰动增强了GNN的防御能力，为图神经网络的对抗训练提供了新的解决方案。

Abstract: This paper studies the vulnerability of Graph Neural Networks (GNNs) to
adversarial attacks on node features and graph structure. Various methods have
implemented adversarial training to augment graph data, aiming to bolster the
robustness and generalization of GNNs. These methods typically involve applying
perturbations to the node feature, weights, or graph structure and subsequently
minimizing the loss by learning more robust graph model parameters under the
adversarial perturbations. Despite the effectiveness of adversarial training in
enhancing GNNs' robustness and generalization abilities, its application has
been largely confined to specific datasets and GNN types. In this paper, we
propose a novel method, PerturbEmbedding, that integrates adversarial
perturbation and training, enhancing GNNs' resilience to such attacks and
improving their generalization ability. PerturbEmbedding performs perturbation
operations directly on every hidden embedding of GNNs and provides a unified
framework for most existing perturbation strategies/methods. We also offer a
unified perspective on the forms of perturbations, namely random and
adversarial perturbations. Through experiments on various datasets using
different backbone models, we demonstrate that PerturbEmbedding significantly
improves both the robustness and generalization abilities of GNNs,
outperforming existing methods. The rejection of both random (non-targeted) and
adversarial (targeted) perturbations further enhances the backbone model's
performance.

</details>


### [51] [Curriculum Guided Personalized Subgraph Federated Learning](https://arxiv.org/abs/2509.00402)
*Minku Kang,Hogun Park*

Main category: cs.LG

TL;DR: CUFL是一个新颖的个性化子图联邦学习框架，通过课程学习和细粒度结构指标来解决子图数据异构性问题，防止早期过拟合并改善模型聚合效果。


<details>
  <summary>Details</summary>
Motivation: 子图联邦学习面临严重的数据异构性问题，稀疏和偏置的子图容易导致快速过拟合，使得客户端相似性矩阵停滞或崩溃，从而失去聚合效果。

Method: 提出CUFL框架：客户端采用课程学习自适应选择边进行训练；服务器端使用随机参考图上重构的细粒度结构指标来估计客户端相似性，改进加权聚合。

Result: 在六个基准数据集上的广泛实验证实，CUFL相比相关基线方法取得了优越的性能表现。

Conclusion: CUFL通过课程学习调节个性化过程，有效防止早期过拟合，改善了子图联邦学习中的数据异构性问题，实现了更好的知识交换和个性化效果。

Abstract: Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs)
across distributed private subgraphs, but it suffers from severe data
heterogeneity. To mitigate data heterogeneity, weighted model aggregation
personalizes each local GNN by assigning larger weights to parameters from
clients with similar subgraph characteristics inferred from their current model
states. However, the sparse and biased subgraphs often trigger rapid
overfitting, causing the estimated client similarity matrix to stagnate or even
collapse. As a result, aggregation loses effectiveness as clients reinforce
their own biases instead of exploiting diverse knowledge otherwise available.
To this end, we propose a novel personalized subgraph FL framework called
Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the
client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges
for training according to their reconstruction scores, exposing each GNN first
to easier, generic cross-client substructures and only later to harder,
client-specific ones. This paced exposure prevents early overfitting to biased
patterns and enables gradual personalization. By regulating personalization,
the curriculum also reshapes server aggregation from exchanging generic
knowledge to propagating client-specific knowledge. Further, CUFL improves
weighted aggregation by estimating client similarity using fine-grained
structural indicators reconstructed on a random reference graph. Extensive
experiments on six benchmark datasets confirm that CUFL achieves superior
performance compared to relevant baselines. Code is available at
https://github.com/Kang-Min-Ku/CUFL.git.

</details>


### [52] [Metis: Training Large Language Models with Advanced Low-Bit Quantization](https://arxiv.org/abs/2509.00404)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Ruijun Huang,Fang Dong,Jixian Zhou,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Yuan Cheng,Fan Wu,Fan Yang,Tun Lu,Ning Gu,Li Shang*

Main category: cs.LG

TL;DR: Metis是一个训练框架，通过谱分解、自适应学习率和双范围正则化，解决了低比特量化训练LLM时的不稳定问题，使FP8训练超越FP32基准，FP4训练达到接近FP32的精度。


<details>
  <summary>Details</summary>
Motivation: 各向异性参数分布是低比特量化训练大语言模型的主要障碍，少数主导奇异值产生宽数值范围，与块级量化的固有偏差冲突，导致训练不稳定和性能低下。

Method: 结合(i)谱分解与随机嵌入分离主导和长尾成分，(ii)谱域自适应学习率增强被低估方向，(iii)双范围正则化联合约束数值精度和参数范围分布。

Result: 使用Metis，FP8训练超越FP32基线，FP4训练达到与FP32相当的精度，为先进低比特量化下的稳健可扩展LLM训练铺平道路。

Conclusion: Metis框架有效解决了低比特量化训练中的数值范围冲突问题，实现了稳定高效的训练，显著提升了量化模型的性能表现。

Abstract: This work identifies anisotropic parameter distributions as a fundamental
barrier to training large language models (LLMs) with low-bit quantization: a
few dominant singular values create wide numerical ranges that conflict with
the inherent bias of block-wise quantization. This bias disproportionately
preserves high-magnitude values while discarding smaller ones, causing training
instability and low model performance. This work introduces Metis, a training
framework that combines (i) spectral decomposition with random embedding to
efficiently disentangle dominant from long-tail components, compressing broad
distributions into quantization-friendly narrow ranges; (ii) adaptive learning
rates in the spectral domain to amplify underrepresented directions and better
capture diverse features critical for performance; and (iii) a dual-range
regularizer that jointly constrains numerical precision and parameter range
distribution, ensuring stable, unbiased low-bit training. With Metis, FP8
training surpasses FP32 baselines, and FP4 training achieves accuracy
comparable to FP32, paving the way for robust and scalable LLM training under
advanced low-bit quantization. The code implementation for Metis is available
at: https://github.com/typename-yyf/Metis-quantization.

</details>


### [53] [Lagrangian Relaxation for Multi-Action Partially Observable Restless Bandits: Heuristic Policies and Indexability](https://arxiv.org/abs/2509.00415)
*Rahul Meshram,Kesav Kaza*

Main category: cs.LG

TL;DR: 本文研究了多动作部分可观测的躁动多臂老虎机问题，这是经典躁动多臂老虎机问题的推广，每个老虎机有有限状态和动作，状态不可观测但可获得反馈信号。针对预算约束下的长期折扣优化问题，分析了拉格朗日边界方法，提出了基于点值迭代和在线滚动策略的近似计算方法，并讨论了启发式策略和Whittle指数策略。


<details>
  <summary>Details</summary>
Motivation: 部分可观测的躁动多臂老虎机在推荐系统、通信系统、公共卫生干预规划等多个领域有广泛应用。传统模型假设每个老虎机只有两个动作，而实际应用中往往需要更多动作选择，因此需要研究多动作情况下的优化问题。

Method: 1) 建立多动作部分可观测躁动多臂老虎机模型，每个老虎机状态演化遵循马尔可夫过程；2) 使用拉格朗日边界方法分析优化问题；3) 采用点基值迭代(PBVI)和在线滚动策略近似计算拉格朗日边界；4) 研究启发式策略和Whittle指数策略。

Result: 提出了多动作部分可观测躁动多臂老虎机的理论框架，分析了值函数的各种性质，为PBVI和在线滚动策略提供了理论见解，并讨论了不同策略在预算约束下的性能表现。

Conclusion: 多动作部分可观测躁动多臂老虎机问题具有重要应用价值，拉格朗日边界方法结合近似计算技术是有效的解决方案，但Whittle指数策略在该模型中存在局限性，需要进一步研究更高效的策略设计方法。

Abstract: Partially observable restless multi-armed bandits have found numerous
applications including in recommendation systems, communication systems, public
healthcare outreach systems, and in operations research. We study multi-action
partially observable restless multi-armed bandits, it is a generalization of
the classical restless multi-armed bandit problem -- 1) each bandit has finite
states, and the current state is not observable, 2) each bandit has finite
actions. In particular, we assume that more than two actions are available for
each bandit. We motivate our problem with the application of public-health
intervention planning. We describe the model and formulate a long term
discounted optimization problem, where the state of each bandit evolves
according to a Markov process, and this evolution is action dependent. The
state of a bandit is not observable but one of finitely many feedback signals
are observable. Each bandit yields a reward, based on the action taken on that
bandit. The agent is assumed to have a budget constraint. The bandits are
assumed to be independent. However, they are weakly coupled at the agent
through the budget constraint.
  We first analyze the Lagrangian bound method for our partially observable
restless bandits. The computation of optimal value functions for finite-state,
finite-action POMDPs is non-trivial. Hence, the computation of Lagrangian
bounds is also challenging. We describe approximations for the computation of
Lagrangian bounds using point based value iteration (PBVI) and online rollout
policy. We further present various properties of the value functions and
provide theoretical insights on PBVI and online rollout policy. We study
heuristic policies for multi-actions PORMAB. Finally, we discuss present
Whittle index policies and their limitations in our model.

</details>


### [54] [Memory Limitations of Prompt Tuning in Transformers](https://arxiv.org/abs/2509.00421)
*Maxime Meyer,Mario Michelessa,Caroline Chaux,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文证明了Transformer的记忆能力随提示长度线性增长，并首次形式化证明了大型语言模型在扩展上下文中的性能退化现象，揭示了Transformer架构处理长序列的内在局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管提示调优在实践中的成功，但其理论分析仍然有限。现有理论工作主要关注通用逼近特性，本文旨在探索Transformer的记忆能力这一不同理论方面。

Method: 通过理论分析证明两个主要贡献：1）Transformer记忆的信息量不能快于提示长度的线性增长；2）形式化证明大型语言模型中观察到的扩展上下文性能退化现象。

Result: 证明了Transformer具有有限的内存，无论上下文大小如何，其能够保留的信息量都受到限制，这解释了处理长序列时的性能退化问题。

Conclusion: 这一发现提供了对Transformer架构内在局限性的根本理解，特别是它们处理长序列的能力，为理解提示调优的理论基础提供了重要洞见。

Abstract: Despite the empirical success of prompt tuning in adapting pretrained
language models to new tasks, theoretical analyses of its capabilities remain
limited. Existing theoretical work primarily addresses universal approximation
properties, demonstrating results comparable to standard weight tuning. In this
paper, we explore a different aspect of the theory of transformers: the
memorization capability of prompt tuning. We provide two principal theoretical
contributions. First, we prove that the amount of information memorized by a
transformer cannot scale faster than linearly with the prompt length. Second,
and more importantly, we present the first formal proof of a phenomenon
empirically observed in large language models: performance degradation in
transformers with extended contexts. We rigorously demonstrate that
transformers inherently have limited memory, constraining the amount of
information they can retain, regardless of the context size. This finding
offers a fundamental understanding of the intrinsic limitations of transformer
architectures, particularly their ability to handle long sequences.

</details>


### [55] [Universal Properties of Activation Sparsity in Modern Large Language Models](https://arxiv.org/abs/2509.00454)
*Filip Szatkowski,Patryk Będkowski,Alessio Devoto,Jan Dubiński,Pasquale Minervini,Mikołaj Piórczyński,Simone Scardapane,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 本文提出了一个通用框架来评估稀疏性鲁棒性，并系统研究了现代大语言模型（包括扩散LLM）FFN层中的激活稀疏性现象，发现了普遍模式并提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 输入相关的激活稀疏性是深度学习模型的重要特性，但现有针对ReLU模型的方法无法直接迁移到使用其他激活函数的现代大语言模型，当前研究缺乏系统性和共识。

Method: 提出了一个评估稀疏性鲁棒性的通用框架，对现代LLM（包括扩散LLM）的FFN层进行系统性激活稀疏性研究。

Result: 发现了LLM中激活稀疏性的普遍模式，提供了对该现象的深入理解。

Conclusion: 研究结果为模型设计和加速中利用激活稀疏性提供了实用指导，填补了现代LLM激活稀疏性研究的空白。

Abstract: Input-dependent activation sparsity is a notable property of deep learning
models, which has been extensively studied in networks with ReLU activations
and is associated with efficiency, robustness, and interpretability. However,
the approaches developed for ReLU-based models depend on exact zero activations
and do not transfer directly to modern large language models~(LLMs), which have
abandoned ReLU in favor of other activation functions. As a result, current
work on activation sparsity in LLMs is fragmented, model-specific, and lacks
consensus on which components to target. We propose a general framework to
assess sparsity robustness and present a systematic study of the phenomenon in
the FFN layers of modern LLMs, including diffusion LLMs. Our findings reveal
universal patterns of activation sparsity in LLMs, provide insights into this
phenomenon, and offer practical guidelines for exploiting it in model design
and acceleration.

</details>


### [56] [Localizing and Mitigating Memorization in Image Autoregressive Models](https://arxiv.org/abs/2509.00488)
*Aditya Kasliwal,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 本研究分析了图像自回归模型中的记忆化现象，发现不同架构的记忆模式存在差异，并提出通过干预最记忆化的组件来减少数据泄露风险，同时保持生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 图像自回归模型在生成速度和质量方面达到最先进水平，但引发了对其训练数据记忆化及其隐私影响的担忧，需要深入探索记忆化发生的位置和方式。

Method: 通过测量细粒度记忆化来分析不同图像自回归架构中的记忆模式，比较分层逐分辨率架构和标准逐令牌预测架构的记忆化差异，并对最记忆化组件进行干预。

Result: 发现分层架构的记忆化早期出现并随分辨率加深，而标准架构的记忆化集中在后期处理阶段。通过干预关键组件，显著降低了数据提取能力，同时对生成图像质量影响最小。

Conclusion: 研究揭示了图像生成模型的内部行为特征，为减轻隐私风险提供了实用策略，表明可以通过针对性干预来平衡模型性能和隐私保护。

Abstract: Image AutoRegressive (IAR) models have achieved state-of-the-art performance
in speed and quality of generated images. However, they also raise concerns
about memorization of their training data and its implications for privacy.
This work explores where and how such memorization occurs within different
image autoregressive architectures by measuring a fine-grained memorization.
The analysis reveals that memorization patterns differ across various
architectures of IARs. In hierarchical per-resolution architectures, it tends
to emerge early and deepen with resolutions, while in IARs with standard
autoregressive per token prediction, it concentrates in later processing
stages. These localization of memorization patterns are further connected to
IARs' ability to memorize and leak training data. By intervening on their most
memorizing components, we significantly reduce the capacity for data extraction
from IARs with minimal impact on the quality of generated images. These
findings offer new insights into the internal behavior of image generative
models and point toward practical strategies for mitigating privacy risks.

</details>


### [57] [Graph Convolutional Network With Pattern-Spatial Interactive and Regional Awareness for Traffic Forecasting](https://arxiv.org/abs/2509.00515)
*Xinyu Ji,Chengcheng Yan,Jibiao Yuan,Fiefie Zhao*

Main category: cs.LG

TL;DR: 提出PSIRAGCN模型，通过模式-空间交互融合框架和区域感知图卷积网络，解决交通预测中多视角时空关联建模和区域异质性挑战


<details>
  <summary>Details</summary>
Motivation: 现有研究难以有效建模多感知视角的时空相关性，忽视了交通模式与空间关联的交互融合，且受限于空间异质性而未能考虑消息传递中的区域异质性

Method: 提出模式-空间交互融合框架（包含模式和空间模块），从全局到局部视角捕获模式与空间相关性；设计基于消息传递的区域感知图卷积网络，利用区域特征库重构数据驱动的消息传递

Result: 在三个真实交通数据集上的实验表明，PSIRAGCN在平衡计算成本的同时优于最先进的基线方法

Conclusion: PSIRAGCN通过模式-空间交互融合和区域感知机制，有效提升了交通预测性能，解决了现有方法在多视角时空建模和区域异质性处理方面的局限性

Abstract: Traffic forecasting is significant for urban traffic management, intelligent
route planning, and real-time flow monitoring. Recent advances in
spatial-temporal models have markedly improved the modeling of intricate
spatial-temporal correlations for traffic forecasting. Unfortunately, most
previous studies have encountered challenges in effectively modeling
spatial-temporal correlations across various perceptual perspectives, which
have neglected the interactive fusion between traffic patterns and spatial
correlations. Additionally, constrained by spatial heterogeneity, most studies
fail to consider distinct regional heterogeneity during message-passing. To
overcome these limitations, we propose a Pattern-Spatial Interactive and
Regional Awareness Graph Convolutional Network (PSIRAGCN) for traffic
forecasting. Specifically, we propose a pattern-spatial interactive fusion
framework composed of pattern and spatial modules. This framework aims to
capture patterns and spatial correlations by adopting a perception perspective
from the global to the local level and facilitating mutual utilization with
positive feedback. In the spatial module, we designed a graph convolutional
network based on message-passing. The network is designed to leverage a
regional characteristics bank to reconstruct data-driven message-passing with
regional awareness. Reconstructed message passing can reveal the regional
heterogeneity between nodes in the traffic network. Extensive experiments on
three real-world traffic datasets demonstrate that PSIRAGCN outperforms the
State-of-the-art baseline while balancing computational costs.

</details>


### [58] [Biological Pathway Informed Models with Graph Attention Networks (GATs)](https://arxiv.org/abs/2509.00524)
*Gavin Wong,Ping Shu Ho,Ivan Au Yeung,Ka Chun Cheung,Simon See*

Main category: cs.LG

TL;DR: 提出基于图注意力网络（GAT）的基因通路建模框架，相比传统MLP方法在预测通路动态方面MSE降低81%，并能从原始数据中重新发现已知基因互作关系


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型将基因视为无结构标记或把通路当作"基因袋"处理，忽略了通路拓扑结构和基因间相互作用的关键生物学信息

Method: 使用图注意力网络（GAT）在基因级别建模通路，通过边干预编码药物机制，提升模型鲁棒性

Result: 在未见治疗条件下预测通路动态时，GAT比MLP的MSE降低81%；能够从原始时间序列mRNA数据中正确重新发现TP53-MDM2-MDM4反馈环中的所有五个基因互作

Conclusion: GAT框架能够有效利用通路拓扑信息，显著提升预测性能，并具有直接从实验数据生成新生物学假设的潜力

Abstract: Biological pathways map gene-gene interactions that govern all human
processes. Despite their importance, most ML models treat genes as unstructured
tokens, discarding known pathway structure. The latest pathway-informed models
capture pathway-pathway interactions, but still treat each pathway as a "bag of
genes" via MLPs, discarding its topology and gene-gene interactions. We propose
a Graph Attention Network (GAT) framework that models pathways at the gene
level. We show that GATs generalize much better than MLPs, achieving an 81%
reduction in MSE when predicting pathway dynamics under unseen treatment
conditions. We further validate the correctness of our biological prior by
encoding drug mechanisms via edge interventions, boosting model robustness.
Finally, we show that our GAT model is able to correctly rediscover all five
gene-gene interactions in the canonical TP53-MDM2-MDM4 feedback loop from raw
time-series mRNA data, demonstrating potential to generate novel biological
hypotheses directly from experimental data.

</details>


### [59] [FedThief: Harming Others to Benefit Oneself in Self-Centered Federated Learning](https://arxiv.org/abs/2509.00540)
*Xiangyu Zhang,Mang Ye*

Main category: cs.LG

TL;DR: 提出FedThief框架，在联邦学习中实现自中心攻击，既降低全局模型性能又提升攻击者私有模型性能


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习攻击策略仅破坏全局模型性能，但攻击者自身模型也会受损。现实攻击者追求竞争优势，希望自己的模型优于其他参与者

Method: FedThief框架：上传阶段修改内容降低全局模型性能；使用基于分歧感知的集成技术整合全局更新和本地知识来增强私有模型

Result: 实验表明该方法能有效降低全局模型性能，同时攻击者获得的集成模型显著优于全局模型

Conclusion: 提出了自中心联邦学习攻击新范式，攻击者既能破坏系统又能获得竞争优势，揭示了联邦学习的新安全威胁

Abstract: In federated learning, participants' uploaded model updates cannot be
directly verified, leaving the system vulnerable to malicious attacks. Existing
attack strategies have adversaries upload tampered model updates to degrade the
global model's performance. However, attackers also degrade their own private
models, gaining no advantage. In real-world scenarios, attackers are driven by
self-centered motives: their goal is to gain a competitive advantage by
developing a model that outperforms those of other participants, not merely to
cause disruption. In this paper, we study a novel Self-Centered Federated
Learning (SCFL) attack paradigm, in which attackers not only degrade the
performance of the global model through attacks but also enhance their own
models within the federated learning process. We propose a framework named
FedThief, which degrades the performance of the global model by uploading
modified content during the upload stage. At the same time, it enhances the
private model's performance through divergence-aware ensemble techniques, where
"divergence" quantifies the deviation between private and global models, that
integrate global updates and local knowledge. Extensive experiments show that
our method effectively degrades the global model performance while allowing the
attacker to obtain an ensemble model that significantly outperforms the global
model.

</details>


### [60] [Advanced spectral clustering for heterogeneous data in credit risk monitoring systems](https://arxiv.org/abs/2509.00546)
*Lu Han,Mengyan Li,Jiping Qiang,Zhi Su*

Main category: cs.LG

TL;DR: 提出ASC方法，整合财务和文本数据，通过优化权重和特征向量选择，在中小企业信用监测中实现比基线方法高18%的轮廓系数


<details>
  <summary>Details</summary>
Motivation: 异构数据（数值财务变量和文本记录）对信用监测带来重大挑战，需要有效整合不同类型数据进行分析

Method: 先进谱聚类(ASC)方法，通过优化权重参数整合财务和文本相似度，采用新颖的特征值-轮廓优化方法选择特征向量

Result: 在1,428家中小企业数据集上，ASC的轮廓系数比单类型数据基线方法高18%；低风险企业中有51%包含'社会招聘'术语；招聘导向型中小企业违约风险低30%

Conclusion: ASC通过将谱聚类理论与异构数据应用相结合，能够识别有意义的聚类模式，为更有针对性和有效的信用干预提供支持，方法在多种聚类算法中表现出鲁棒性

Abstract: Heterogeneous data, which encompass both numerical financial variables and
textual records, present substantial challenges for credit monitoring. To
address this issue, we propose Advanced Spectral Clustering (ASC), a method
that integrates financial and textual similarities through an optimized weight
parameter and selects eigenvectors using a novel eigenvalue-silhouette
optimization approach. Evaluated on a dataset comprising 1,428 small and
medium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18%
higher than that of a single-type data baseline method. Furthermore, the
resulting clusters offer actionable insights; for instance, 51% of low-risk
firms are found to include the term 'social recruitment' in their textual
records. The robustness of ASC is confirmed across multiple clustering
algorithms, including k-means, k-medians, and k-medoids, with
{\Delta}Intra/Inter < 0.13 and {\Delta}Silhouette Coefficient < 0.02. By
bridging spectral clustering theory with heterogeneous data applications, ASC
enables the identification of meaningful clusters, such as recruitment-focused
SMEs exhibiting a 30% lower default risk, thereby supporting more targeted and
effective credit interventions.

</details>


### [61] [Integrated Multivariate Segmentation Tree for the Analysis of Heterogeneous Credit Data in Small and Medium-Sized Enterprises](https://arxiv.org/abs/2509.00550)
*Lu Han,Xiuying Wang*

Main category: cs.LG

TL;DR: 提出IMST模型，整合财务数据和文本信息，通过矩阵分解、Lasso特征选择和多元分割树构建，在中小企业信用评估中达到88.9%准确率，优于传统决策树和其他基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统决策树模型仅依赖数值变量，难以处理高维数据且无法有效整合文本信息，限制了中小企业信用评估的效果。

Method: 三阶段方法：1) 通过矩阵分解将文本数据转为数值矩阵；2) 使用Lasso回归选择重要财务特征；3) 基于基尼指数或熵构建多元分割树，并应用最弱链接剪枝控制模型复杂度。

Result: 在1,428家中国中小企业数据集上，IMST达到88.9%准确率，超越基准决策树(87.4%)以及逻辑回归、SVM等传统模型，同时具有更好的可解释性和计算效率。

Conclusion: IMST框架成功整合了文本和财务数据，在中小企业信用评估中表现出色，具有更高的准确性、可解释性和风险检测能力，为信用评估提供了更全面的解决方案。

Abstract: Traditional decision tree models, which rely exclusively on numerical
variables, often encounter difficulties in handling high-dimensional data and
fail to effectively incorporate textual information. To address these
limitations, we propose the Integrated Multivariate Segmentation Tree (IMST), a
comprehensive framework designed to enhance credit evaluation for small and
medium-sized enterprises (SMEs) by integrating financial data with textual
sources. The methodology comprises three core stages: (1) transforming textual
data into numerical matrices through matrix factorization; (2) selecting
salient financial features using Lasso regression; and (3) constructing a
multivariate segmentation tree based on the Gini index or Entropy, with
weakest-link pruning applied to regulate model complexity. Experimental results
derived from a dataset of 1,428 Chinese SMEs demonstrate that IMST achieves an
accuracy of 88.9%, surpassing baseline decision trees (87.4%) as well as
conventional models such as logistic regression and support vector machines
(SVM). Furthermore, the proposed model exhibits superior interpretability and
computational efficiency, featuring a more streamlined architecture and
enhanced risk detection capabilities.

</details>


### [62] [TranCIT: Transient Causal Interaction Toolbox](https://arxiv.org/abs/2509.00602)
*Salar Nouri,Kaidi Shao,Shervin Safavi*

Main category: cs.LG

TL;DR: TranCIT是一个开源的Python工具箱，用于从非平稳神经信号中量化瞬态因果交互作用，解决了传统方法对短暂神经事件分析不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理短暂神经事件时往往不足，而先进的特定事件技术缺乏在Python生态系统中的可访问实现，需要开发一个综合的分析工具。

Method: TranCIT实现了包括Granger因果关系、传递熵，以及更稳健的基于结构因果模型的动态因果强度(DCS)和相对动态因果强度(rDCS)在内的综合分析流程。

Result: TranCIT成功捕获了高同步状态下传统方法失效的因果关系，并在真实数据中识别出海马CA3到CA1在尖波涟漪事件期间的已知瞬态信息流。

Conclusion: 该软件包提供了一个用户友好、经过验证的解决方案，用于研究控制复杂系统的瞬态因果动力学。

Abstract: Quantifying transient causal interactions from non-stationary neural signals
is a fundamental challenge in neuroscience. Traditional methods are often
inadequate for brief neural events, and advanced, event-specific techniques
have lacked accessible implementations within the Python ecosystem. Here, we
introduce trancit (Transient Causal Interaction Toolbox), an open-source Python
package designed to bridge this gap. TranCIT implements a comprehensive
analysis pipeline, including Granger Causality, Transfer Entropy, and the more
robust Structural Causal Model-based Dynamic Causal Strength (DCS) and relative
Dynamic Causal Strength (rDCS) for accurately detecting event-driven causal
effects. We demonstrate TranCIT's utility by successfully capturing causality
in high-synchrony regimes where traditional methods fail and by identifying the
known transient information flow from hippocampal CA3 to CA1 during sharp-wave
ripple events in real-world data. The package offers a user-friendly, validated
solution for investigating the transient causal dynamics that govern complex
systems.

</details>


### [63] [RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models](https://arxiv.org/abs/2509.00614)
*Shikun Liu,Deyu Zou,Nima Shoghi,Victor Fung,Kai Liu,Pan Li*

Main category: cs.LG

TL;DR: 该论文系统评估了分子图基础模型(MGFMs)的八种微调方法，提出了结合权重插值和集成微调优势的ROFT-MOL方法，在回归和分类任务中均取得改进性能。


<details>
  <summary>Details</summary>
Motivation: 分子图基础模型面临预训练数据集较小、下游任务数据稀缺以及需要同时处理回归和分类任务的独特挑战，需要开发更鲁棒的微调方法来提升模型泛化能力。

Method: 将八种微调方法分为权重基、表示基和部分微调三类机制进行基准测试，在此基础上设计ROFT-MOL方法，结合后处理权重插值和权重集成微调的优势。

Result: 在监督和自监督预训练模型的下游回归和分类任务中进行了广泛评估，ROFT-MOL方法在两种任务类型上都实现了改进的性能。

Conclusion: ROFT-MOL方法有效结合了简单后处理权重插值和复杂权重集成微调的优势，在保持易用性的同时提升了分子图基础模型的微调性能。

Abstract: In the era of foundation models, fine-tuning pre-trained models for specific
downstream tasks has become crucial. This drives the need for robust
fine-tuning methods to address challenges such as model overfitting and sparse
labeling. Molecular graph foundation models (MGFMs) face unique difficulties
that complicate fine-tuning. These models are limited by smaller pre-training
datasets and more severe data scarcity for downstream tasks, both of which
require enhanced model generalization. Moreover, MGFMs must accommodate diverse
objectives, including both regression and classification tasks. To better
understand and improve fine-tuning techniques under these conditions, we
classify eight fine-tuning methods into three mechanisms: weight-based,
representation-based, and partial fine-tuning. We benchmark these methods on
downstream regression and classification tasks across supervised and
self-supervised pre-trained models in diverse labeling settings. This extensive
evaluation provides valuable insights and informs the design of a refined
robust fine-tuning method, ROFT-MOL. This approach combines the strengths of
simple post-hoc weight interpolation with more complex weight ensemble
fine-tuning methods, delivering improved performance across both task types
while maintaining the ease of use inherent in post-hoc weight interpolation.

</details>


### [64] [TimeCopilot](https://arxiv.org/abs/2509.00616)
*Azul Garza,Reneé Rosillo*

Main category: cs.LG

TL;DR: TimeCopilot是首个开源的时间序列预测代理框架，通过统一API结合多个时间序列基础模型和大型语言模型，实现自动化预测流程并提供自然语言解释


<details>
  <summary>Details</summary>
Motivation: 为了解决时间序列预测中模型选择、特征分析和预测生成等复杂流程的自动化问题，同时提供可解释的预测结果和自然语言查询支持

Method: 开发LLM无关的统一框架，结合多个时间序列基础模型，支持跨不同预测家族的集成方法，自动化特征分析、模型选择、交叉验证和预测生成

Result: 在GIFT-Eval大规模基准测试中实现了最先进的概率预测性能，且成本较低

Conclusion: 该框架为可重现、可解释和易访问的代理预测系统提供了实用基础

Abstract: We introduce TimeCopilot, the first open-source agentic framework for
forecasting that combines multiple Time Series Foundation Models (TSFMs) with
Large Language Models (LLMs) through a single unified API. TimeCopilot
automates the forecasting pipeline: feature analysis, model selection,
cross-validation, and forecast generation, while providing natural language
explanations and supporting direct queries about the future. The framework is
LLM-agnostic, compatible with both commercial and open-source models, and
supports ensembles across diverse forecasting families. Results on the
large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art
probabilistic forecasting performance at low cost. Our framework provides a
practical foundation for reproducible, explainable, and accessible agentic
forecasting systems.

</details>


### [65] [Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers](https://arxiv.org/abs/2509.00631)
*Giacomo Acciarini,Simone Mestici,Halil Kelebek,Linnea Wolniewicz,Michael Vergalla,Madhulika Guhathakurta,Umaa Rebbapragada,Bala Poduval,Atılım Güneş Baydin,Frank Soboczenski*

Main category: cs.LG

TL;DR: 提出基于Temporal Fusion Transformers的机器学习框架，用于电离层总电子含量(TEC)预测，可提供24小时稳健预测，误差低至3.33 TECU，并开源为ionopy工具包。


<details>
  <summary>Details</summary>
Motivation: 电离层对GNSS、卫星通信和LEO操作至关重要，但其变异性预测困难，现有模型在强空间天气条件下精度有限，需要更准确的预测方法。

Method: 使用Temporal Fusion Transformers机器学习框架，整合太阳辐照度、地磁指数和GNSS垂直TEC等多源异构数据，采用预处理和时间对齐策略。

Result: 在2010-2025年数据上测试，模型可实现24小时稳健预测，均方根误差低至3.33 TECU，太阳EUV辐照度提供最强预测信号。

Conclusion: 该框架不仅提供高精度预测，还通过注意力机制提供可解释性，支持业务应用和科学发现，开源工具包ionopy促进可重复性和社区发展。

Abstract: The ionosphere critically influences Global Navigation Satellite Systems
(GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet
accurate prediction of its variability remains challenging due to nonlinear
couplings between solar, geomagnetic, and thermospheric drivers. Total Electron
Content (TEC), a key ionospheric parameter, is derived from GNSS observations,
but its reliable forecasting is limited by the sparse nature of global
measurements and the limited accuracy of empirical models, especially during
strong space weather conditions. In this work, we present a machine learning
framework for ionospheric TEC forecasting that leverages Temporal Fusion
Transformers (TFT) to predict sparse ionosphere data. Our approach accommodates
heterogeneous input sources, including solar irradiance, geomagnetic indices,
and GNSS-derived vertical TEC, and applies preprocessing and temporal alignment
strategies. Experiments spanning 2010-2025 demonstrate that the model achieves
robust predictions up to 24 hours ahead, with root mean square errors as low as
3.33 TECU. Results highlight that solar EUV irradiance provides the strongest
predictive signals. Beyond forecasting accuracy, the framework offers
interpretability through attention-based analysis, supporting both operational
applications and scientific discovery. To encourage reproducibility and
community-driven development, we release the full implementation as the
open-source toolkit \texttt{ionopy}.

</details>


### [66] [Disentangling Slow and Fast Temporal Dynamics in Degradation Inference with Hierarchical Differential Models](https://arxiv.org/abs/2509.00639)
*Mengjie Zhao,Olga Fink*

Main category: cs.LG

TL;DR: 通过层次控制微分方程框架，解决供源系统中运行变化与退化过程难以分离的问题，实现更准确的健康状态评估


<details>
  <summary>Details</summary>
Motivation: 在工程系统中，退化过程难以直接观测并主要反映为低频长期变化，而传感器数据多为短期运行变化，导致退化推断困难且噪声大

Method: 提出层次控制微分方程（H-CDE）框架，包含慢速（退化）和快速（运行）组件，采用多尺度时间积分、可学习路径变换和单调激活函数来实现分离和正则化

Result: 在动态响应系统（如桥梁）和稳态系统（如航空发动机）上的综合评估显示，H-CDE能有效分离退化与运行动态，性能超过余差基线方法，实现更准确、稳健和可解释的推断

Conclusion: H-CDE框架为工程系统健康状态监测提供了一种有效的解决方案，能够充分分离退化与运行变化，为准确的质量评估和决策支持提供了可靠基础

Abstract: Reliable inference of system degradation from sensor data is fundamental to
condition monitoring and prognostics in engineered systems. Since degradation
is rarely observable and measurable, it must be inferred to enable accurate
health assessment and decision-making. This is particularly challenging because
operational variations dominate system behavior, while degradation introduces
only subtle, long-term changes. Consequently, sensor data mainly reflect
short-term operational variability, making it difficult to disentangle the
underlying degradation process. Residual-based methods are widely employed, but
the residuals remain entangled with operational history, often resulting in
noisy and unreliable degradation estimation, particularly in systems with
dynamic responses. Neural Ordinary Equations (NODEs) offer a promising
framework for inferring latent dynamics, but the time-scale separation in
slow-fast systems introduces numerical stiffness and complicates training,
while degradation disentanglement remains difficult. To address these
limitations, we propose a novel Hierarchical Controlled Differential Equation
(H-CDE) framework that incorporates a slow (degradation) and a fast (operation)
CDE component in a unified architecture. It introduces three key innovations: a
multi-scale time integration scheme to mitigate numerical stiffness; a
learnable path transformation that extracts latent degradation drivers to
control degradation evolution; and a novel activation function that enforces
monotonicity on inferred degradation as a regularizer for disentanglement.
Through comprehensive evaluations on both dynamic response (e.g., bridges) and
steady state (e.g., aero-engine) systems, we demonstrate that H-CDE effectively
disentangles degradation from operational dynamics and outperforms
residual-based baselines, yielding more accurate, robust, and interpretable
inference.

</details>


### [67] [AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models](https://arxiv.org/abs/2509.00641)
*Zhipeng Yin,Zichong Wang,Avash Palikhe,Zhen Liu,Jun Liu,Wenbin Zhang*

Main category: cs.LG

TL;DR: AMCR框架通过系统重构风险提示词、注意力相似性分析和自适应风险缓解，有效降低生成模型的版权风险而不影响图像质量


<details>
  <summary>Details</summary>
Motivation: 现有基于提示词的版权风险缓解方法在处理微妙侵权情况时效果有限，需要更全面的解决方案来应对生成模型可能无意复制受版权保护内容的问题

Method: 构建AMCR框架：1)系统重构风险提示词为安全形式；2)通过注意力相似性分析检测部分侵权；3)在生成过程中自适应缓解风险

Result: 大量实验验证了AMCR在揭示和缓解潜在版权风险方面的有效性，为生成模型的安全部署提供了实用见解和基准

Conclusion: AMCR提供了一个全面的版权风险评估和缓解框架，能够有效处理微妙侵权情况，促进生成模型的更安全实际部署

Abstract: Generative models have achieved impressive results in text to image tasks,
significantly advancing visual content creation. However, this progress comes
at a cost, as such models rely heavily on large-scale training data and may
unintentionally replicate copyrighted elements, creating serious legal and
ethical challenges for real-world deployment. To address these concerns,
researchers have proposed various strategies to mitigate copyright risks, most
of which are prompt based methods that filter or rewrite user inputs to prevent
explicit infringement. While effective in handling obvious cases, these
approaches often fall short in more subtle situations, where seemingly benign
prompts can still lead to infringing outputs. To address these limitations,
this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a
comprehensive framework which i) builds upon prompt-based strategies by
systematically restructuring risky prompts into safe and non-sensitive forms,
ii) detects partial infringements through attention-based similarity analysis,
and iii) adaptively mitigates risks during generation to reduce copyright
violations without compromising image quality. Extensive experiments validate
the effectiveness of AMCR in revealing and mitigating latent copyright risks,
offering practical insights and benchmarks for the safer deployment of
generative models.

</details>


### [68] [Context-Action Embedding Learning for Off-Policy Evaluation in Contextual Bandits](https://arxiv.org/abs/2509.00648)
*Kushagra Chandak,Vincent Liu,Haanvid Lee*

Main category: cs.LG

TL;DR: 提出了CAEL-MIPS方法，通过学习上下文-动作嵌入来最小化MIPS估计器的均方误差，解决了传统IPS方法在大动作空间或未充分探索区域方差大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统IPS方法在大动作空间或未充分探索的上下文-动作空间存在显著方差问题，现有的MIPS方法虽然利用动作嵌入缓解了这个问题，但未最小化估计器的均方误差且未考虑上下文信息。

Method: 提出CAEL-MIPS方法，从离线数据中学习上下文-动作嵌入，基于MIPS的偏差和方差理论分析，构建MSE最小化目标函数。

Result: 在合成数据集和真实世界数据集上的实证研究表明，该估计器在均方误差方面优于基线方法。

Conclusion: CAEL-MIPS通过优化上下文-动作嵌入学习，有效降低了MIPS估计器的均方误差，在离策略评估任务中表现出优越性能。

Abstract: We consider off-policy evaluation (OPE) in contextual bandits with finite
action space. Inverse Propensity Score (IPS) weighting is a widely used method
for OPE due to its unbiased, but it suffers from significant variance when the
action space is large or when some parts of the context-action space are
underexplored. Recently introduced Marginalized IPS (MIPS) estimators mitigate
this issue by leveraging action embeddings. However, these embeddings do not
minimize the mean squared error (MSE) of the estimators and do not consider
context information. To address these limitations, we introduce Context-Action
Embedding Learning for MIPS, or CAEL-MIPS, which learns context-action
embeddings from offline data to minimize the MSE of the MIPS estimator.
Building on the theoretical analysis of bias and variance of MIPS, we present
an MSE-minimizing objective for CAEL-MIPS. In the empirical studies on a
synthetic dataset and a real-world dataset, we demonstrate that our estimator
outperforms baselines in terms of MSE.

</details>


### [69] [Missing Data Imputation using Neural Cellular Automata](https://arxiv.org/abs/2509.00651)
*Tin Luu,Binh Nguyen,Man Ngo*

Main category: cs.LG

TL;DR: 基于神经细胞自动机(NCA)的新题数据缺失值插补方法，在插计错误和插补后性能方面超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 虽然生成模型如VAE和GAN已用于缺失值插补，但神经细胞自动机(NCA)这种强大的计算模型在此领域被忽略了，需要探索其在数据插补任务中的潜力。

Method: 提出了一种受NCA启发的新题插补方法，通过适当的调整使NCA基模型能够处理缺失数据插补问题。

Result: 多个实验证明，该模型在插补误差和插补后的任务性能方面都超过了现有的state-of-the-art方法。

Conclusion: NCA基模型是一种有效的数据插补方法，为缺失数据处理领域提供了新的解决方案。

Abstract: When working with tabular data, missingness is always one of the most painful
problems. Throughout many years, researchers have continuously explored better
and better ways to impute missing data. Recently, with the rapid development
evolution in machine learning and deep learning, there is a new trend of
leveraging generative models to solve the imputation task. While the imputing
version of famous models such as Variational Autoencoders or Generative
Adversarial Networks were investigated, prior work has overlooked Neural
Cellular Automata (NCA), a powerful computational model. In this paper, we
propose a novel imputation method that is inspired by NCA. We show that, with
some appropriate adaptations, an NCA-based model is able to address the missing
data imputation problem. We also provide several experiments to evidence that
our model outperforms state-of-the-art methods in terms of imputation error and
post-imputation performance.

</details>


### [70] [IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India](https://arxiv.org/abs/2509.00653)
*Tung Nguyen,Harkanwar Singh,Nilay Naharas,Lucas Bandarkar,Aditya Grover*

Main category: cs.LG

TL;DR: 提出了印度天气基准(IndiaWeatherBench)，这是一个针对印度次区域的数据驱动天气预测综合基准，包含精选数据集、评估指标和多种模型基线


<details>
  <summary>Details</summary>
Motivation: 区域天气预报对气候适应和灾害缓解至关重要，但现有研究缺乏统一的数据集和实验设置，限制了公平比较和可复现性

Method: 基于高分辨率区域再分析产品构建精选数据集，实现包括UNet、Transformer和图网络在内的多种架构模型，采用不同的边界条件和训练目标

Result: 建立了全面的基准系统，提供了原始和预处理数据集、模型实现和评估流程，所有资源均已开源

Conclusion: IndiaWeatherBench为推进区域天气预报研究提供了基础框架，可轻松扩展到其他地理区域

Abstract: Regional weather forecasting is a critical problem for localized climate
adaptation, disaster mitigation, and sustainable development. While machine
learning has shown impressive progress in global weather forecasting, regional
forecasting remains comparatively underexplored. Existing efforts often use
different datasets and experimental setups, limiting fair comparison and
reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for
data-driven regional weather forecasting focused on the Indian subcontinent.
IndiaWeatherBench provides a curated dataset built from high-resolution
regional reanalysis products, along with a suite of deterministic and
probabilistic metrics to facilitate consistent training and evaluation. To
establish strong baselines, we implement and evaluate a range of models across
diverse architectures, including UNets, Transformers, and Graph-based networks,
as well as different boundary conditioning strategies and training objectives.
While focused on India, IndiaWeatherBench is easily extensible to other
geographic regions. We open-source all raw and preprocessed datasets, model
implementations, and evaluation pipelines to promote accessibility and future
development. We hope IndiaWeatherBench will serve as a foundation for advancing
regional weather forecasting research. Code is available at
https://github.com/tung-nd/IndiaWeatherBench.

</details>


### [71] [An Evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Operator Learning Network](https://arxiv.org/abs/2509.00663)
*Binghang Lu,Changhong Mou,Guang Lin*

Main category: cs.LG

TL;DR: 提出了一种基于进化多目标优化的副本交换物理信息算子学习网络，用于高效求解参数偏微分方程，在噪声观测数据下表现出优越性能


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经网络和算子学习方法在平衡算子与物理损失、噪声鲁棒性和不确定性量化方面存在局限

Method: 整合进化多目标优化自适应平衡损失、副本交换随机梯度Langevin动力学改善参数空间探索、内置贝叶斯不确定性量化

Result: 在一维Burgers方程和时间分数混合扩散波方程等测试中，在精度、噪声鲁棒性和不确定性量化能力方面均优于传统算子学习方法

Conclusion: 该框架成功解决了现有方法的局限性，为参数偏微分方程求解提供了更有效的解决方案

Abstract: In this paper, we propose an evolutionary Multi-objective Optimization for
Replica-Exchange-based Physics-informed Operator learning Network, which is a
novel operator learning network to efficiently solve parametric partial
differential equations. In forward and inverse settings, this operator learning
network only admits minimum requirement of noisy observational data. While
physics-informed neural networks and operator learning approaches such as Deep
Operator Networks and Fourier Neural Operators offer promising alternatives to
traditional numerical solvers, they struggle with balancing operator and
physics losses, maintaining robustness under noisy or sparse data, and
providing uncertainty quantification. The proposed framework addresses these
limitations by integrating: (i) evolutionary multi-objective optimization to
adaptively balance operator and physics-based losses in the Pareto front; (ii)
replica exchange stochastic gradient Langevin dynamics to improve global
parameter-space exploration and accelerate convergence; and (iii) built-in
Bayesian uncertainty quantification from stochastic sampling. The proposed
operator learning method is tested numerically on several different problems
including one-dimensional Burgers equation and the time-fractional mixed
diffusion-wave equation. The results indicate that our framework consistently
outperforms the general operator learning methods in accuracy, noise
robustness, and the ability to quantify uncertainty.

</details>


### [72] [Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design](https://arxiv.org/abs/2509.00684)
*Amartya Banerjee,Somnath Kar,Anirban Pal,Debabrata Maiti*

Main category: cs.LG

TL;DR: VECTOR+是一个结合对比学习和生成模型的框架，用于低数据条件下的药物分子设计，在PD-L1抑制剂和激酶抑制剂任务中表现出色，生成的新分子具有更好的对接分数和结合稳定性。


<details>
  <summary>Details</summary>
Motivation: 在低数据条件下，如何有效引导生成模型探索药理学相关的化学空间是药物发现中的主要挑战。需要开发能够高效利用有限数据、生成具有特定性质分子的方法。

Method: VECTOR+结合属性引导的表征学习和可控分子生成，适用于回归和分类任务。使用对比学习来学习分子表示，并通过生成模型产生具有目标性质的新分子。

Result: 在PD-L1抑制剂数据集（296个化合物）上，生成8374个分子中有100个超过对接阈值-15.0 kcal/mol，最佳得分-17.6 kcal/mol优于参考抑制剂（-15.4 kcal/mol）。分子动力学模拟证实结合稳定性（配体RMSD < 2.5埃）。在激酶抑制剂上也表现优异，生成的化合物对接分数优于brigatinib和sorafenib等已上市药物。

Conclusion: VECTOR+是一个强大且可扩展的方法，在低数据条件下实现了属性条件分子设计，通过桥接对比学习和生成建模，为AI加速的药物发现提供了可重复的解决方案。

Abstract: Efficiently steering generative models toward pharmacologically relevant
regions of chemical space remains a major obstacle in molecular drug discovery
under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive
Learning for Targeted Optimization and Resampling, a framework that couples
property-guided representation learning with controllable molecule generation.
VECTOR+ applies to both regression and classification tasks and enables
interpretable, data-efficient exploration of functional chemical space. We
evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with
experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056
molecules by binding mode). Despite limited training data, VECTOR+ generates
novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of
8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with
the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor
($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl
pharmacophore while introducing novel motifs. Molecular dynamics (250 ns)
confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes
to kinase inhibitors, producing compounds with stronger docking scores than
established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE
and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity
highlights the superior performance of our method. These results position our
work as a robust, extensible approach for property-conditioned molecular design
in low-data settings, bridging contrastive learning and generative modeling for
reproducible, AI-accelerated discovery.

</details>


### [73] [DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming](https://arxiv.org/abs/2509.00693)
*Arun Vignesh Malarkkan,Haoyue Bai,Anjali Kaushik,Yanjie Fu*

Main category: cs.LG

TL;DR: PPDR任务旨在在最大化目标属性预测精度的同时最小化敏感属性预测精度。DELTA框架通过两阶段变分解耦生成学习，在8个数据集上实现了预测性能提升9.3%和隐私泄露减少35%的效果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据常包含敏感属性且受严格法规约束，现有特征工程主要关注下游任务性能而忽视隐私泄露风险，需要新的隐私保护数据重编程方法。

Method: 提出DELTA两阶段框架：第一阶段使用策略引导强化学习发现具有下游任务效用的特征变换；第二阶段采用变分LSTM seq2seq编码器-解码器，结合效用-隐私解耦潜在空间设计和对抗-因果解耦正则化来抑制隐私信号。

Result: 在8个数据集上的实验表明，DELTA将预测性能提高了约9.3%，同时将隐私泄露减少了约35%。

Conclusion: DELTA框架能够有效实现隐私保护的数据重编程，在保持高预测性能的同时显著降低隐私泄露风险，为隐私敏感场景提供了实用的解决方案。

Abstract: In real-world applications, domain data often contains identifiable or
sensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and
requires explicit data feature engineering for interpretability and
transparency. Existing feature engineering primarily focuses on advancing
downstream task performance, often risking privacy leakage. We generalize this
learning task under such new requirements as Privacy-Preserving Data
Reprogramming (PPDR): given a dataset, transforming features to maximize target
attribute prediction accuracy while minimizing sensitive attribute prediction
accuracy. PPDR poses challenges for existing systems: 1) generating
high-utility feature transformations without being overwhelmed by a large
search space, and 2) disentangling and eliminating sensitive information from
utility-oriented features to reduce privacy inferability. To tackle these
challenges, we propose DELTA, a two-phase variational disentangled generative
learning framework. Phase I uses policy-guided reinforcement learning to
discover feature transformations with downstream task utility, without any
regard to privacy inferability. Phase II employs a variational LSTM seq2seq
encoder-decoder with a utility-privacy disentangled latent space design and
adversarial-causal disentanglement regularization to suppress privacy signals
during feature generation. Experiments on eight datasets show DELTA improves
predictive performance by ~9.3% and reduces privacy leakage by ~35%,
demonstrating robust, privacy-aware data transformation.

</details>


### [74] [Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition](https://arxiv.org/abs/2509.00703)
*Osama Ahmad,Lukas Wesemann,Fabian Waschkowski,Zubair Khalid*

Main category: cs.LG

TL;DR: MAGN网络通过将变分模态分解转化为可训练模块，大幅提升时空预测效率，在LargeST基准上实现85-95%的误差降低和250倍的速度提升


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在时空预测中存在频谱纠缠问题，而分解集成方法如VMGCN虽然提高精度但计算效率低下且需要手动调参

Method: 提出模式自适应图网络(MAGN)，包含展开式VMD模块替代迭代优化，以及模式特定的可学习带宽约束来自适应空间异质性

Result: 在LargeST基准(6,902个传感器，2.41亿观测值)上，相比VMGCN减少85-95%预测误差，分解时间减少250倍

Conclusion: MAGN成功解决了传统分解方法的计算效率问题，同时通过可学习参数消除了手动调参需求，在时空预测任务中表现出色

Abstract: Accurate spatiotemporal forecasting is critical for numerous complex systems
but remains challenging due to complex volatility patterns and spectral
entanglement in conventional graph neural networks (GNNs). While
decomposition-integrated approaches like variational mode graph convolutional
network (VMGCN) improve accuracy through signal decomposition, they suffer from
computational inefficiency and manual hyperparameter tuning. To address these
limitations, we propose the mode adaptive graph network (MAGN) that transforms
iterative variational mode decomposition (VMD) into a trainable neural module.
Our key innovations include (1) an unfolded VMD (UVMD) module that replaces
iterative optimization with a fixed-depth network to reduce the decomposition
time (by 250x for the LargeST benchmark), and (2) mode-specific learnable
bandwidth constraints ({\alpha}k ) adapt spatial heterogeneity and eliminate
manual tuning while preventing spectral overlap. Evaluated on the LargeST
benchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction
in the prediction error over VMGCN and outperforms state-of-the-art baselines.

</details>


### [75] [Why Pool When You Can Flow? Active Learning with GFlowNets](https://arxiv.org/abs/2509.00704)
*Renfei Zhang,Mohit Pandey,Artem Cherkasov,Martin Ester*

Main category: cs.LG

TL;DR: BALD-GFlowNet是一个生成式主动学习框架，通过生成流网络直接采样BALD奖励比例的对象，解决了传统池式主动学习在大规模未标记数据集上的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统池式主动学习方法（如BALD）在评估大规模未标记数据集时计算成本过高，特别是在包含数十亿样本的药物发现虚拟筛选中，这严重限制了方法的可扩展性。

Method: 提出BALD-GFlowNet框架，利用生成流网络（GFlowNets）直接按照BALD奖励比例生成样本，替代传统的池式获取策略，使可扩展性与未标记池大小无关。

Result: 在虚拟筛选实验中，BALD-GFlowNet实现了与标准BALD基线相当的性能，同时生成了结构更多样化的分子。

Conclusion: 该方法为高效且可扩展的分子发现提供了一个有前景的方向，解决了大规模主动学习的计算瓶颈问题。

Abstract: The scalability of pool-based active learning is limited by the computational
cost of evaluating large unlabeled datasets, a challenge that is particularly
acute in virtual screening for drug discovery. While active learning strategies
such as Bayesian Active Learning by Disagreement (BALD) prioritize informative
samples, it remains computationally intensive when scaled to libraries
containing billions samples. In this work, we introduce BALD-GFlowNet, a
generative active learning framework that circumvents this issue. Our method
leverages Generative Flow Networks (GFlowNets) to directly sample objects in
proportion to the BALD reward. By replacing traditional pool-based acquisition
with generative sampling, BALD-GFlowNet achieves scalability that is
independent of the size of the unlabeled pool. In our virtual screening
experiment, we show that BALD-GFlowNet achieves a performance comparable to
that of standard BALD baseline while generating more structurally diverse
molecules, offering a promising direction for efficient and scalable molecular
discovery.

</details>


### [76] [Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient Approach For Continual Graph Learning](https://arxiv.org/abs/2509.00735)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: 提出了TAAM方法，通过神经突触调制器动态调节冻结GNN骨干网络的计算流，解决了持续图学习中的稳定性-可塑性困境和资源消耗问题


<details>
  <summary>Details</summary>
Motivation: 当前持续图学习方法面临两个主要问题：1）稳定性-可塑性困境的平衡问题，基于重放的方法存储成本高；2）资源密集型预训练依赖，现有无重放方法严重依赖预训练骨干网络

Method: 提出任务感知自适应调制(TAAM)方法，使用神经突触调制器(NSM)来动态调节冻结GNN骨干网络的内部计算流。采用原型引导策略：训练时通过深度复制相似调制器初始化新NSM，推理时选择最相关的冻结NSM

Result: 在六个GCIL基准数据集上的广泛实验表明，TAAM全面优于最先进的方法

Conclusion: TAAM为导航稳定性-可塑性困境开辟了新路径，是一种无重放、资源高效的方法，通过轻量级任务特定模块有效指导GNN的推理过程

Abstract: Continual Graph Learning(CGL)focuses on acquiring new knowledge while
retaining previously learned information, essential for real-world graph
applications. Current methods grapple with two main issues:1) The
Stability-Plasticity Dilemma: Replay-based methods often create an imbalance
between the Dilemma, while incurring significant storage costs.2) The
Resource-Heavy Pre-training: Leading replay-free methods critically depend on
extensively pre-trained backbones, this reliance imposes a substantial resource
burden.In this paper, we argue that the key to overcoming these challenges lies
not in replaying data or fine-tuning the entire network, but in dynamically
modulating the internal computational flow of a frozen backbone. We posit that
lightweight, task-specific modules can effectively steer a GNN's reasoning
process. Motivated by this insight, we propose Task-Aware Adaptive
Modulation(TAAM), a replay-free, resource-efficient approach that charts a new
path for navigating the stability-plasticity dilemma. TAAM's core is its Neural
Synapse Modulators(NSM), which are trained and then frozen for each task to
store expert knowledge. A pivotal prototype-guided strategy governs these
modulators: 1) For training, it initializes a new NSM by deep-copying from a
similar past modulator to boost knowledge transfer. 2) For inference, it
selects the most relevant frozen NSM for each task. These NSMs insert into a
frozen GNN backbone to perform fine-grained, node-attentive modulation of its
internal flow-different from the static perturbations of prior methods.
Extensive experiments show that TAAM comprehensively outperforms
state-of-the-art methods across six GCIL benchmark datasets. The code will be
released upon acceptance of the paper.

</details>


### [77] [Attribute Fusion-based Classifier on Framework of Belief Structure](https://arxiv.org/abs/2509.00754)
*Qiying Hu,Yingying Liang,Qianli Zhou,Witold Pedrycz*

Main category: cs.LG

TL;DR: 本文提出了改进的Dempster-Shafer理论属性融合分类器，通过选择性建模策略和新的BPA转换方法，显著提升了分类准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统DST分类器存在隶属函数建模过于简化以及对BPA带来的置信结构利用不足的问题，限制了在复杂现实场景中的有效性。

Method: 采用选择性建模策略（单高斯和高斯混合模型），并通过交叉验证指导模型选择；提出将可能性分布转换为BPA的新方法；将置信结构BPA生成方法应用于证据K近邻分类器。

Result: 在基准数据集上的实验表明，提出的分类器优于现有最佳证据分类器，平均准确率提升4.84%，且保持低方差。

Conclusion: 该方法通过更灵活的隶属函数建模和更丰富的BPA表示，显著提升了证据分类器在复杂场景中的性能和鲁棒性。

Abstract: Dempster-Shafer Theory (DST) provides a powerful framework for modeling
uncertainty and has been widely applied to multi-attribute classification
tasks. However, traditional DST-based attribute fusion-based classifiers suffer
from oversimplified membership function modeling and limited exploitation of
the belief structure brought by basic probability assignment (BPA), reducing
their effectiveness in complex real-world scenarios. This paper presents an
enhanced attribute fusion-based classifier that addresses these limitations
through two key innovations. First, we adopt a selective modeling strategy that
utilizes both single Gaussian and Gaussian Mixture Models (GMMs) for membership
function construction, with model selection guided by cross-validation and a
tailored evaluation metric. Second, we introduce a novel method to transform
the possibility distribution into a BPA by combining simple BPAs derived from
normalized possibility distributions, enabling a much richer and more flexible
representation of uncertain information. Furthermore, we apply the belief
structure-based BPA generation method to the evidential K-Nearest Neighbors
classifier, enhancing its ability to incorporate uncertainty information into
decision-making. Comprehensive experiments on benchmark datasets are conducted
to evaluate the performance of the proposed attribute fusion-based classifier
and the enhanced evidential K-Nearest Neighbors classifier in comparison with
both evidential classifiers and conventional machine learning classifiers. The
results demonstrate that our proposed classifier outperforms the best existing
evidential classifier, achieving an average accuracy improvement of 4.84%,
while maintaining low variance, thus confirming its superior effectiveness and
robustness.

</details>


### [78] [Flow Matters: Directional and Expressive GNNs for Heterophilic Graphs](https://arxiv.org/abs/2509.00772)
*Arman Gupta,Govind Waghmare,Gaurav Oberoi,Nitish Srivastava*

Main category: cs.LG

TL;DR: 该论文研究了在异质图（heterophilic graphs）中结合边方向性和表达能力强的消息传递对节点分类的影响，提出了两种多项式可表达的GNN架构，在多个基准数据集上取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质图中表现不佳，因为其依赖局部同质邻域。先前研究表明建模边方向性可以提高有效同质性，同时多项式可表达的GNN在捕获高阶特征交互方面显示潜力。

Method: 提出了两种架构：(1) 多项式可表达的GAT基线模型(Poly)，(2) 方向感知变体(Dir-Poly)，分别聚合输入和输出边。两种模型都能学习输入特征的置换等变高次多项式，且保持可扩展性。

Result: 在五个基准异质图数据集上的实验表明，Poly模型始终优于现有基线，Dir-Poly在具有固有方向性的图上（如Roman Empire）提供额外增益，达到最先进结果。但在无向图上引入人工方向性并不总是有帮助。

Conclusion: 研究结果强调了边方向和表达能力强的特征建模在异质图学习中的互补作用，方向性消息传递的益处是上下文相关的。

Abstract: In heterophilic graphs, where neighboring nodes often belong to different
classes, conventional Graph Neural Networks (GNNs) struggle due to their
reliance on local homophilous neighborhoods. Prior studies suggest that
modeling edge directionality in such graphs can increase effective homophily
and improve classification performance. Simultaneously, recent work on
polynomially expressive GNNs shows promise in capturing higher-order
interactions among features. In this work, we study the combined effect of edge
directionality and expressive message passing on node classification in
heterophilic graphs. Specifically, we propose two architectures: (1) a
polynomially expressive GAT baseline (Poly), and (2) a direction-aware variant
(Dir-Poly) that separately aggregates incoming and outgoing edges. Both models
are designed to learn permutation-equivariant high-degree polynomials over
input features, while remaining scalable with no added time complexity.
Experiments on five benchmark heterophilic datasets show that our Poly model
consistently outperforms existing baselines, and that Dir-Poly offers
additional gains on graphs with inherent directionality (e.g., Roman Empire),
achieving state-of-the-art results. Interestingly, on undirected graphs,
introducing artificial directionality does not always help, suggesting that the
benefit of directional message passing is context-dependent. Our findings
highlight the complementary roles of edge direction and expressive feature
modeling in heterophilic graph learning.

</details>


### [79] [ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods](https://arxiv.org/abs/2509.00797)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt*

Main category: cs.LG

TL;DR: 提出了ProCause方法来解决Prescriptive Process Monitoring评估中的问题，通过支持序列和非序列模型集成多种因果推理架构，比现有RealCause方法更可靠。


<details>
  <summary>Details</summary>
Motivation: 现有的RealCause方法在评估Prescriptive Process Monitoring时存在两个主要问题：忽略过程数据中的时间依赖性，以及仅依赖单一的TARNet因果推理模型架构，限制了评估效果。

Method: 开发了ProCause生成方法，支持序列模型（如LSTM）和非序列模型，集成多种因果推理架构（S-Learner、T-Learner、TARNet和集成模型），通过模拟器和真实数据分析进行验证。

Result: 研究发现TARNet并非总是最佳选择，模型集成提供更一致的可靠性，当存在时间依赖性时使用LSTM可以改善评估效果。真实数据分析验证了ProCause的实用性。

Conclusion: ProCause为Prescriptive Process Monitoring方法提供了更可靠的评估框架，通过多架构集成和时间依赖性处理，显著提升了评估质量。

Abstract: Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining
that focuses on optimizing processes through real-time interventions based on
event log data. Evaluating PresPM methods is challenging due to the lack of
ground-truth outcomes for all intervention actions in datasets. A generative
deep learning approach from the field of Causal Inference (CI), RealCause, has
been commonly used to estimate the outcomes for proposed intervention actions
to evaluate a new policy. However, RealCause overlooks the temporal
dependencies in process data, and relies on a single CI model architecture,
TARNet, limiting its effectiveness. To address both shortcomings, we introduce
ProCause, a generative approach that supports both sequential (e.g., LSTMs) and
non-sequential models while integrating multiple CI architectures (S-Learner,
T-Learner, TARNet, and an ensemble). Our research using a simulator with known
ground truths reveals that TARNet is not always the best choice; instead, an
ensemble of models offers more consistent reliability, and leveraging LSTMs
shows potential for improved evaluations when temporal dependencies are
present. We further validate ProCause's practical effectiveness through a
real-world data analysis, ensuring a more reliable evaluation of PresPM
methods.

</details>


### [80] [Fairness in Federated Learning: Trends, Challenges, and Opportunities](https://arxiv.org/abs/2509.00799)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 这篇调查性论文系统论了联邦学习中的公平性问题，分析了偏见来源，评估了现有技术方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但异构性导致的公平性问题会影响系统效能，需要系统性研究和解决方案。

Method: 通过调查研究方法，系统分析了数据偏见、客户偏见、模型偏见等多种偏见来源，评估了现有技术方法的优缺点。

Result: 提供了联邦学习公平性问题的全面概述，包括理论基础、技术方法和评估指标，为该领域研究提供了坚实基础。

Conclusion: 这份调查为建立更公平的联邦学习框架指明了方向，并提出了多个有前景的开放研究问题。

Abstract: At the intersection of the cutting-edge technologies and privacy concerns,
Federated Learning (FL) with its distributed architecture, stands at the
forefront in a bid to facilitate collaborative model training across multiple
clients while preserving data privacy. However, the applicability of FL systems
is hindered by fairness concerns arising from numerous sources of heterogeneity
that can result in biases and undermine a system's effectiveness, with skewed
predictions, reduced accuracy, and inefficient model convergence. This survey
thus explores the diverse sources of bias, including but not limited to, data,
client, and model biases, and thoroughly discusses the strengths and
limitations inherited within the array of the state-of-the-art techniques
utilized in the literature to mitigate such disparities in the FL training
process. We delineate a comprehensive overview of the several notions,
theoretical underpinnings, and technical aspects associated with fairness and
their adoption in FL-based multidisciplinary environments. Furthermore, we
examine salient evaluation metrics leveraged to measure fairness
quantitatively. Finally, we envisage exciting open research directions that
have the potential to drive future advancements in achieving fairer FL
frameworks, in turn, offering a strong foundation for future research in this
pivotal area.

</details>


### [81] [XAI-Driven Machine Learning System for Driving Style Recognition and Personalized Recommendations](https://arxiv.org/abs/2509.00802)
*Feriel Amel Sellal,Ahmed Ayoub Bellachia,Meryem Malak Dif,Enguerrand De Rautlin De La Roy,Mouhamed Amine Bouchiha,Yacine Ghamri-Doudane*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的方法，用于驾驶风格分类，在保持高精度的同时提供可解释性，使用Random Forest和XGBoost达到0.92准确率，并通过SHAP技术提供个性化安全驾驶建议。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在驾驶风格分类中表现优异但缺乏可解释性，限制了实际应用中的信任度。需要一种既能保持高精度又具有透明度的替代方案。

Method: 使用机器学习技术（Random Forest、XGBoost、SVM）结合新构建的CARLA-Drive数据集，并应用SHAP可解释性技术来提供个性化驾驶建议。

Result: 在三分类型任务中，Random Forest和XGBoost分类器均达到0.92的准确率，性能与深度学习模型相当。

Conclusion: 该方法在保持高性能的同时提供了可解释性，具有实际部署的实用性和透明度，适用于智能交通系统的现实应用。

Abstract: Artificial intelligence (AI) is increasingly used in the automotive industry
for applications such as driving style classification, which aims to improve
road safety, efficiency, and personalize user experiences. While deep learning
(DL) models, such as Long Short-Term Memory (LSTM) networks, excel at this
task, their black-box nature limits interpretability and trust. This paper
proposes a machine learning (ML)-based method that balances high accuracy with
interpretability. We introduce a high-quality dataset, CARLA-Drive, and
leverage ML techniques like Random Forest (RF), Gradient Boosting (XGBoost),
and Support Vector Machine (SVM), which are efficient, lightweight, and
interpretable. In addition, we apply the SHAP (Shapley Additive Explanations)
explainability technique to provide personalized recommendations for safer
driving. Achieving an accuracy of 0.92 on a three-class classification task
with both RF and XGBoost classifiers, our approach matches DL models in
performance while offering transparency and practicality for real-world
deployment in intelligent transportation systems.

</details>


### [82] [Crystal Structure Prediction with a Geometric Permutation-Invariant Loss Function](https://arxiv.org/abs/2509.00832)
*Emmanuel Jehanno,Romain Menegaux,Julien Mairal,Sergei Grudinin*

Main category: cs.LG

TL;DR: 提出了一种名为SinkFast的新方法，通过可微分的Sinkhorn算法实现线性分配，显著提升了有机材料晶体结构预测的性能


<details>
  <summary>Details</summary>
Motivation: 解决有机材料晶体结构预测的挑战，现有方法依赖计算昂贵的迭代流匹配方法，需要更高效准确的解决方案

Method: 提出新颖的损失函数，通过基于Sinkhorn算法的可微分线性分配方案来捕获关键几何分子特性，同时保持置换不变性

Result: 在COD-Cluster17基准测试中，即使使用简单的回归方法，SinkFast也显著优于更复杂的流匹配方法

Conclusion: SinkFast方法为分子组装和晶体结构预测提供了更高效准确的解决方案，展示了简单方法在复杂问题上的优越性能

Abstract: Crystalline structure prediction remains an open challenge in materials
design. Despite recent advances in computational materials science, accurately
predicting the three-dimensional crystal structures of organic materials--an
essential first step for designing materials with targeted properties--remains
elusive. In this work, we address the problem of molecular assembly, where a
set $\mathcal{S}$ of identical rigid molecules is packed to form a crystalline
structure. Existing state-of-the-art models typically rely on computationally
expensive, iterative flow-matching approaches. We propose a novel loss function
that correctly captures key geometric molecular properties while maintaining
permutation invariance over $\mathcal{S}$. We achieve this via a differentiable
linear assignment scheme based on the Sinkhorn algorithm. Remarkably, we show
that even a simple regression using our method {\em SinkFast} significantly
outperforms more complex flow-matching approaches on the COD-Cluster17
benchmark, a curated subset of the Crystallography Open Database (COD).

</details>


### [83] [Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery](https://arxiv.org/abs/2509.00846)
*Woon Yee Ng,Li Rong Wang,Siyuan Liu,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出了Causal SHAP框架，将因果关系整合到特征归因中，解决SHAP在特征高度相关时错误归因的问题


<details>
  <summary>Details</summary>
Motivation: SHAP解释方法无法区分因果关系和相关关系，在特征高度相关时经常错误归因特征重要性，这在医疗等高风险领域尤为重要

Method: 结合Peter-Clark算法进行因果发现和IDA算法进行因果强度量化，在保持SHAP优良特性的同时整合因果关系

Result: Causal SHAP降低了仅与目标相关的特征归因分数，在合成和真实数据集上验证了有效性

Conclusion: 为可解释AI领域提供了实用的因果感知模型解释框架，特别适用于医疗等需要理解真实因果关系的领域

Abstract: Explaining machine learning (ML) predictions has become crucial as ML models
are increasingly deployed in high-stakes domains such as healthcare. While
SHapley Additive exPlanations (SHAP) is widely used for model interpretability,
it fails to differentiate between causality and correlation, often
misattributing feature importance when features are highly correlated. We
propose Causal SHAP, a novel framework that integrates causal relationships
into feature attribution while preserving many desirable properties of SHAP. By
combining the Peter-Clark (PC) algorithm for causal discovery and the
Intervention Calculus when the DAG is Absent (IDA) algorithm for causal
strength quantification, our approach addresses the weakness of SHAP.
Specifically, Causal SHAP reduces attribution scores for features that are
merely correlated with the target, as validated through experiments on both
synthetic and real-world datasets. This study contributes to the field of
Explainable AI (XAI) by providing a practical framework for causal-aware model
explanations. Our approach is particularly valuable in domains such as
healthcare, where understanding true causal relationships is critical for
informed decision-making.

</details>


### [84] [Predicting Multi-Type Talented Students in Secondary School Using Semi-Supervised Machine Learning](https://arxiv.org/abs/2509.00863)
*Xinzhe Zheng,Zhen-Qun Yang,Jiannong Cao,Jiabei Cheng*

Main category: cs.LG

TL;DR: TalentPredictor是一个半监督多模态神经网络，结合Transformer、LSTM和ANN架构，用于在中学阶段早期识别7类不同才能，准确率达到90.8%。


<details>
  <summary>Details</summary>
Motivation: 传统人才识别方法依赖人工流程、过于关注学术成绩，且通常在高等教育阶段才进行干预，忽视了非学术才能和早期干预机会。

Method: 使用Transformer、LSTM和ANN构建半监督多模态神经网络，将各类获奖记录聚类为才能类别，并从学生多样化学习行为中提取特征。

Result: 在1,041名中学生数据上实现了0.908的分类准确率和0.908的ROCAUC，显著超越了传统方法。

Conclusion: 机器学习方法能够在学生发展早期有效识别多样化才能，为教育干预提供了新的可能性。

Abstract: Talent identification plays a critical role in promoting student development.
However, traditional approaches often rely on manual processes or focus
narrowly on academic achievement, and typically delaying intervention until the
higher education stage. This oversight overlooks diverse non-academic talents
and misses opportunities for early intervention. To address this gap, this
study introduces TalentPredictor, a novel semi-supervised multi-modal neural
network that combines Transformer, LSTM, and ANN architectures. This model is
designed to predict seven different talent types--academic, sport, art,
leadership, service, technology, and others--in secondary school students
within an offline educational setting. Drawing on existing offline educational
data from 1,041 local secondary students, TalentPredictor overcomes the
limitations of traditional talent identification methods. By clustering various
award records into talent categories and extracting features from students'
diverse learning behaviors, it achieves high prediction accuracy (0.908
classification accuracy, 0.908 ROCAUC). This demonstrates the potential of
machine learning to identify diverse talents early in student development.

</details>


### [85] [Tabular Diffusion Counterfactual Explanations](https://arxiv.org/abs/2509.00876)
*Wei Zhang,Brian Barr,John Paisley*

Main category: cs.LG

TL;DR: 本文提出了一种针对表格数据的反事实解释方法，使用基于Gumbel-softmax分布的引导反向过程，在金融和社会科学领域的表格数据上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实解释方法主要关注计算机视觉问题，而在金融和社会科学领域典型的表格数据上缺乏有效解决方案。

Method: 提出基于Gumbel-softmax分布近似的新型引导反向过程，特别处理分类特征，并研究温度参数τ的影响，推导理论边界。

Result: 在多个大规模信贷借贷和其他表格数据集上的实验表明，该方法在可解释性、多样性、稳定性和有效性等量化指标上优于流行基线方法。

Conclusion: 该方法能够生成鲁棒且现实的反事实解释，为表格数据提供了有效的可解释机器学习工具。

Abstract: Counterfactual explanations methods provide an important tool in the field of
{interpretable machine learning}. Recent advances in this direction have
focused on diffusion models to explain a deep classifier. However, these
techniques have predominantly focused on problems in computer vision. In this
paper, we focus on tabular data typical in finance and the social sciences and
propose a novel guided reverse process for categorical features based on an
approximation to the Gumbel-softmax distribution. Furthermore, we study the
effect of the temperature $\tau$ and derive a theoretical bound between the
Gumbel-softmax distribution and our proposed approximated distribution. We
perform experiments on several large-scale credit lending and other tabular
datasets, assessing their performance in terms of the quantitative measures of
interpretability, diversity, instability, and validity. These results indicate
that our approach outperforms popular baseline methods, producing robust and
realistic counterfactual explanations.

</details>


### [86] [An Explainable Gaussian Process Auto-encoder for Tabular Data](https://arxiv.org/abs/2509.00884)
*Wei Zhang,Brian Barr,John Paisley*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的自动编码器架构，用于生成反事实解释样本，该方法参数更少、过拟合风险更低，并引入了新的密度估计器和正则化率选择算法。


<details>
  <summary>Details</summary>
Motivation: 在需要高可信度的机器学习应用中，反事实解释方法变得越来越重要。现有方法大多使用生成模型如自动编码器，但存在参数过多、容易过拟合的问题。

Method: 使用高斯过程构建自动编码器架构，减少可学习参数数量；提出新的密度估计器用于搜索分布内样本；开发算法选择密度估计器的最优正则化率。

Result: 在多个大规模表格数据集上的实验表明，该方法能够生成多样化且符合数据分布的优质反事实样本，性能优于其他基于自动编码器的方法。

Conclusion: 基于高斯过程的自动编码器架构为反事实解释提供了一种更有效、更稳定的解决方案，在保持解释质量的同时降低了模型复杂度。

Abstract: Explainable machine learning has attracted much interest in the community
where the stakes are high. Counterfactual explanations methods have become an
important tool in explaining a black-box model. The recent advances have
leveraged the power of generative models such as an autoencoder. In this paper,
we propose a novel method using a Gaussian process to construct the
auto-encoder architecture for generating counterfactual samples. The resulting
model requires fewer learnable parameters and thus is less prone to
overfitting. We also introduce a novel density estimator that allows for
searching for in-distribution samples. Furthermore, we introduce an algorithm
for selecting the optimal regularization rate on density estimator while
searching for counterfactuals. We experiment with our method in several
large-scale tabular datasets and compare with other auto-encoder-based methods.
The results show that our method is capable of generating diversified and
in-distribution counterfactual samples.

</details>


### [87] [DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers](https://arxiv.org/abs/2509.00925)
*Aman Sharma,Saeed Najafi,Parsa Farinneya,Benyamin Jamialahmadi,Marzieh S. Tahaei,Yuhe Fan,Mehdi Rezagholizadeh,Boxing Chen,Aref Jafari*

Main category: cs.LG

TL;DR: DTRNet是一种改进的Transformer架构，通过动态令牌路由机制，让大多数令牌跳过二次成本的交叉令牌混合，只保留线性更新，显著降低计算成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在每个层对每个令牌都应用二次自注意力计算，计算成本高昂。需要一种方法在保持性能的同时显著降低计算复杂度。

Method: 提出动态令牌路由网络(DTRNet)，保留MLP模块，将大多数令牌的注意力成本降低到线性，只让约10%的令牌通过完整的注意力机制，同时确保每个令牌都得到显式更新。

Result: DTRNet在匹配FLOPs的情况下，在准确性和内存使用上都优于MoD和D-LLM等基于路由的层跳过方法，路由更少的令牌到完整注意力。效率提升随序列长度扩展，对长上下文输入显著减少FLOPs。

Conclusion: DTRNet通过将令牌更新与注意力混合解耦，大幅减少了二次计算份额，为Transformer提供了简单、高效且可扩展的替代方案。

Abstract: Transformers achieve state-of-the-art results across many tasks, but their
uniform application of quadratic self-attention to every token at every layer
makes them computationally expensive. We introduce DTRNet (Dynamic Token
Routing Network), an improved Transformer architecture that allows tokens to
dynamically skip the quadratic cost of cross-token mixing while still receiving
lightweight linear updates. By preserving the MLP module and reducing the
attention cost for most tokens to linear, DTRNet ensures that every token is
explicitly updated while significantly lowering overall computation. This
design offers an efficient and effective alternative to standard dense
attention. Once trained, DTRNet blocks routes only ~10% of tokens through
attention at each layer while maintaining performance comparable to a full
Transformer. It consistently outperforms routing-based layer skipping methods
such as MoD and D-LLM in both accuracy and memory at matched FLOPs, while
routing fewer tokens to full attention. Its efficiency gains, scales with
sequence length, offering significant reduction in FLOPs for long-context
inputs. By decoupling token updates from attention mixing, DTRNet substantially
reduces the quadratic share of computation, providing a simple, efficient, and
scalable alternative to Transformers.

</details>


### [88] [Superposition in Graph Neural Networks](https://arxiv.org/abs/2509.00928)
*Lukas Pertl,Han Xuanyuan,Pietro Liò*

Main category: cs.LG

TL;DR: 该研究通过分析图神经网络潜在空间中的叠加现象，揭示了不同GNN架构中特征几何结构与设计选择（宽度、池化、激活函数）的关系，为提升GNN可解释性提供了实践方法。


<details>
  <summary>Details</summary>
Motivation: 解释图神经网络很困难，因为消息传递会混合信号，且内部通道很少与人类概念对齐。研究直接分析GNN潜在空间中的叠加现象（多个特征共享方向）。

Method: 使用具有明确图概念的受控实验，在图形级别提取类条件质心特征，在节点级别提取线性探测方向特征，并用简单的基不变诊断分析其几何结构。测试了GCN/GIN/GAT等不同架构。

Result: 发现：增加宽度会产生重叠的相位模式；拓扑将重叠印记到节点级特征中，池化部分重新混合为任务对齐的图形轴；更锐利的池化增加轴对齐并减少通道共享；浅层模型可能陷入亚稳态低秩嵌入。

Conclusion: 这些结果将表示几何与具体设计选择（宽度、池化和最终层激活）联系起来，并为构建更可解释的GNN提出了实用方法。

Abstract: Interpreting graph neural networks (GNNs) is difficult because message
passing mixes signals and internal channels rarely align with human concepts.
We study superposition, the sharing of directions by multiple features,
directly in the latent space of GNNs. Using controlled experiments with
unambiguous graph concepts, we extract features as (i) class-conditional
centroids at the graph level and (ii) linear-probe directions at the node
level, and then analyze their geometry with simple basis-invariant diagnostics.
Across GCN/GIN/GAT we find: increasing width produces a phase pattern in
overlap; topology imprints overlap onto node-level features that pooling
partially remixes into task-aligned graph axes; sharper pooling increases axis
alignment and reduces channel sharing; and shallow models can settle into
metastable low-rank embeddings. These results connect representational geometry
with concrete design choices (width, pooling, and final-layer activations) and
suggest practical approaches for more interpretable GNNs.

</details>


### [89] [SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers](https://arxiv.org/abs/2509.00935)
*Aref Jafari,Yuhe Fan,Benyamin Jamialahmadi,Parsa Farinneya,Boxing Chen,Marzieh S. Tahaei*

Main category: cs.LG

TL;DR: SCOUT是一种混合架构，通过局部压缩token和稀疏注意力机制，在保持Transformer表达能力的同时显著降低计算和内存成本，在长序列任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列建模中二次注意力复杂度的可扩展性问题，同时避免纯线性模型因无法保留远距离token详细信息而导致的性能下降。

Method: 提出SCOUT混合架构：1）使用Mamba或滑动窗口注意力线性混合器在固定大小段内局部压缩token；2）每个token稀疏关注少量压缩的检查点token来总结输入历史；3）仅在这些压缩表示上应用注意力。

Result: 在相同计算预算下，SCOUT在长上下文语言建模和推理任务中优于强基线；在4亿和13亿参数规模上，语言建模和常识推理任务匹配全注意力Transformer；端到端吞吐量高于SOTA模型，在长序列基准测试中提供可比结果。

Conclusion: SCOUT通过局部压缩和稀疏注意力的混合设计，在保持接近全注意力性能的同时实现了亚二次方的内存增长，为长序列建模提供了高效可扩展的解决方案。

Abstract: Transformers have demonstrated strong performance across a wide range of
sequence modeling tasks, but their quadratic attention complexity limits
scalability to long sequences. Linear models such as Mamba and sliding-window
attention (SWA) address this by mixing tokens through recurrent or localized
operations with fixed-size memory, achieving efficient inference. However,
these methods risk degrading performance on long sequences due to their
inability to retain detailed information from distant tokens. We propose SCOUT
(Segment Compression for Optimized Utility in Transformers), a hybrid
architecture that compresses tokens locally within fixed-size segments and
applies attention only over these compressed representations. Each token
embedding is first enriched via a linear local mixer, Mamba or SWA, that
integrates recent context. Then, instead of attending to all previous tokens,
each token sparsely attends to a small number of compressed checkpoint tokens
that summarize the input history. This design retains much of the expressivity
of full attention while substantially reducing the computational and memory
cost. By attending to compressed history rather than all previous tokens, SCOUT
incurs slightly higher memory than purely linear models, but its growth rate
remains sub-quadratic and far more scalable than that of full Transformers. We
analyze SCOUT's computational and memory efficiency and evaluate it empirically
on long-context language modeling and reasoning tasks. SCOUT with both Mamba
and SWA mixers outperforms strong long-sequence baselines under the same
computational budget, matches full-attention Transformers on language modeling
and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT
achieves higher end-to-end throughput than SOTA models, while delivering
comparable results on long sequence benchmarks.

</details>


### [90] [ART: Adaptive Resampling-based Training for Imbalanced Classification](https://arxiv.org/abs/2509.00955)
*Arjun Basandrai,Shourya Jain,K. Ilanthenral*

Main category: cs.LG

TL;DR: ART是一种自适应重采样方法，通过基于类别性能动态调整训练数据分布，在类别不平衡分类任务中显著优于传统静态重采样方法


<details>
  <summary>Details</summary>
Motivation: 传统重采样方法使用固定分布，忽略了类别学习难度的动态变化，限制了模型性能

Method: 基于类别级macro F1分数定期更新训练数据分布，在类别层面进行自适应重采样，避免实例级难度建模的噪声问题

Result: 在多个基准测试中显著优于SMOTE、NearMiss等传统方法，平均提升macro F1分数2.64个百分点，统计显著性p<0.05

Conclusion: ART提供了一种可靠的自适应重采样策略，在各种不平衡分类任务中都能提供最稳定的macro F1性能

Abstract: Traditional resampling methods for handling class imbalance typically uses
fixed distributions, undersampling the majority or oversampling the minority.
These static strategies ignore changes in class-wise learning difficulty, which
can limit the overall performance of the model.
  This paper proposes an Adaptive Resampling-based Training (ART) method that
periodically updates the distribution of the training data based on the
class-wise performance of the model. Specifically, ART uses class-wise macro F1
scores, computed at fixed intervals, to determine the degree of resampling to
be performed.
  Unlike instance-level difficulty modeling, which is noisy and
outlier-sensitive, ART adapts at the class level. This allows the model to
incrementally shift its attention towards underperforming classes in a way that
better aligns with the optimization objective.
  Results on diverse benchmarks, including Pima Indians Diabetes and Yeast
dataset demonstrate that ART consistently outperforms both resampling-based and
algorithm-level methods, including Synthetic Minority Oversampling Technique
(SMOTE), NearMiss Undersampling, and Cost-sensitive Learning on binary as well
as multi-class classification tasks with varying degrees of imbalance.
  In most settings, these improvements are statistically significant. On
tabular datasets, gains are significant under paired t-tests and Wilcoxon tests
(p < 0.05), while results on text and image tasks remain favorable. Compared to
training on the original imbalanced data, ART improves macro F1 by an average
of 2.64 percentage points across all tested tabular datasets. Unlike existing
methods, whose performance varies by task, ART consistently delivers the
strongest macro F1, making it a reliable choice for imbalanced classification.

</details>


### [91] [Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems](https://arxiv.org/abs/2509.00992)
*Olusola Odeyomi,Sofiat Olaosebikan,Ajibuwa Opeyemi,Oluwadoyinsola Ige*

Main category: cs.LG

TL;DR: 提出了一种在线去中心化联邦多任务学习算法，利用网络物理特性为邻居的本地模型分配信任概率，在拜占庭客户端占主导的情况下实现模型个性化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中由于数据异构性导致的模型个性化挑战，特别是在在线去中心化设置下应对拜占庭客户端占主导的现实问题。

Method: 开发在线去中心化联邦多任务学习算法，利用无线系统中的接收信号强度等网络物理特性为接收到的邻居本地模型分配信任概率。

Result: 模拟结果显示，所提算法在拜占庭客户端占主导的情况下性能接近无拜占庭环境。

Conclusion: 该方法通过利用网络物理特性成功实现了在拜占庭客户端占多数情况下的联邦学习鲁棒性和模型个性化。

Abstract: Multi-task learning is an effective way to address the challenge of model
personalization caused by high data heterogeneity in federated learning.
However, extending multi-task learning to the online decentralized federated
learning setting is yet to be explored. The online decentralized federated
learning setting considers many real-world applications of federated learning,
such as autonomous systems, where clients communicate peer-to-peer and the data
distribution of each client is time-varying. A more serious problem in
real-world applications of federated learning is the presence of Byzantine
clients. Byzantine-resilient approaches used in federated learning work only
when the number of Byzantine clients is less than one-half the total number of
clients. Yet, it is difficult to put a limit on the number of Byzantine clients
within a system in reality. However, recent work in robotics shows that it is
possible to exploit cyber-physical properties of a system to predict clients'
behavior and assign a trust probability to received signals. This can help to
achieve resiliency in the presence of a dominating number of Byzantine clients.
Therefore, in this paper, we develop an online decentralized federated
multi-task learning algorithm to provide model personalization and resiliency
when the number of Byzantine clients dominates the number of honest clients.
Our proposed algorithm leverages cyber-physical properties, such as the
received signal strength in wireless systems or side information, to assign a
trust probability to local models received from neighbors in each iteration.
Our simulation results show that the proposed algorithm performs close to a
Byzantine-free setting.

</details>


### [92] [MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper](https://arxiv.org/abs/2509.00996)
*Runjia Zeng,Guangyan Sun,Qifan Wang,Tong Geng,Sohail Dianat,Xiaotian Han,Raghuveer Rao,Xueling Zhang,Cheng Han,Lifu Huang,Dongfang Liu*

Main category: cs.LG

TL;DR: MEPT是一种基于专家混合架构的提示调优方法，通过集成多个提示专家来适应性地学习多样化和非平稳的数据分布，在SuperGLUE基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在参数空间上过于刚性，无法动态激活适当的神经通路来灵活适应多样化和不断变化的数据分布。

Method: 提出混合专家提示调优(MEPT)框架，利用专家混合架构集成多个提示专家，自适应学习多样化的数据分布。

Result: 在SuperGLUE基准上优于多个最先进的参数高效基线方法，平均准确率提升1.94%，同时显著减少激活提示79.25%。

Conclusion: MEPT作为一种有效的流形映射框架，通过理论分析和神经激活通路可视化验证了其有效性，为适应非平稳数据分布提供了新思路。

Abstract: Considering deep neural networks as manifold mappers, the
pretrain-then-fine-tune paradigm can be interpreted as a two-stage process:
pretrain establishes a broad knowledge base, and fine-tune adjusts the model
parameters to activate specific neural pathways to align with the target
manifold. Although prior fine-tuning approaches demonstrate success, their
rigid parameter space limits their ability to dynamically activate appropriate
neural pathways, rendering them ill-equipped to adapt flexibly to the diverse
and evolving data distributions. In light of this view, we propose a novel
approach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient
manifold-mapping framework. MEPT leverages the Mixture of Experts architecture
by integrating multiple prompt experts to adaptively learn diverse and
non-stationary data distributions. Empirical evaluations demonstrate that MEPT
outperforms several state-of-the-art parameter efficient baselines on
SuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while
significantly reducing activated prompts by 79.25%. The effectiveness of MEPT
is further supported by theoretical insights from manifold learning and
validated through neural activation pathway visualization results. Our code is
avaliable at https://github.com/runtsang/MEPT.

</details>


### [93] [Any-Order Flexible Length Masked Diffusion](https://arxiv.org/abs/2509.01025)
*Jaeyeon Kim,Lee Cheuk-Kit,Carles Domingo-Enrich,Yilun Du,Sham Kakade,Timothy Ngotiaoco,Sitan Chen,Michael Albergo*

Main category: cs.LG

TL;DR: FlexMDM是一种灵活的掩码扩散模型，可以处理可变长度序列生成，解决了传统MDM只能生成固定长度序列的限制，在数学和代码填充任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散模型(MDMs)虽然具有并行推理和任意顺序生成的优点，但无法支持token插入，只能生成固定长度的序列，这限制了其在实际应用中的灵活性

Method: 基于随机插值框架扩展，FlexMDM通过插入掩码token并逐步去掩码的方式来生成序列，支持可变长度序列生成，同时保持任意顺序推理的灵活性

Result: FlexMDM在困惑度上与MDM相当，但在长度统计建模上表现更好；在迷宫规划任务中成功率比MDM基线高约60%；将LLaDA-8B微调为FlexMDM后，在数学(GSM8K从58%到67%)和代码填充(从52%到65%)任务上性能显著提升

Conclusion: FlexMDM成功解决了MDM固定长度生成的限制，提供了灵活的序列生成能力，同时保持了并行推理的优势，在多个任务上展现出卓越性能

Abstract: Masked diffusion models (MDMs) have recently emerged as a promising
alternative to autoregressive models over discrete domains. MDMs generate
sequences in an any-order, parallel fashion, enabling fast inference and strong
performance on non-causal tasks. However, a crucial limitation is that they do
not support token insertions and are thus limited to fixed-length generations.
To this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a
discrete diffusion paradigm that simultaneously can model sequences of flexible
length while provably retaining MDMs' flexibility of any-order inference.
Grounded in an extension of the stochastic interpolant framework, FlexMDMs
generate sequences by inserting mask tokens and unmasking them. Empirically, we
show that FlexMDMs match MDMs in perplexity while modeling length statistics
with much higher fidelity. On a synthetic maze planning task, they achieve
$\approx 60 \%$ higher success rate than MDM baselines. Finally, we show
pretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes
only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior
performance on math (GSM8K, $58\% \to 67\%$) and code infilling performance
($52\% \to 65\%$).

</details>


### [94] [Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition](https://arxiv.org/abs/2509.01031)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 提出TPRL-DG框架，使用强化学习进行时序特征提取，解决可穿戴传感器人体活动识别中的跨用户泛化问题，无需目标用户标注即可实现优越性能


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器的人体活动识别在医疗健康和智能环境中很重要，但跨用户变异性（运动模式、传感器位置、生理特征差异）导致传统监督学习方法在未见用户上泛化性能差

Method: 提出时序保持强化学习域泛化框架(TPRL-DG)，将特征提取重新定义为强化学习的序列决策过程，使用基于Transformer的自回归生成器产生捕获用户不变活动动态的时序token，通过多目标奖励函数优化

Result: 在DSADS和PAMAP2数据集上的评估显示，TPRL-DG在跨用户泛化方面超越了最先进方法，无需每用户校准即可实现优越准确率

Conclusion: 通过学习鲁棒的用户不变时序模式，TPRL-DG能够实现可扩展的人体活动识别系统，推动个性化医疗、自适应健身追踪和情境感知环境的发展

Abstract: Human Activity Recognition (HAR) using wearable sensors is crucial for
healthcare, fitness tracking, and smart environments, yet cross-user
variability -- stemming from diverse motion patterns, sensor placements, and
physiological traits -- hampers generalization in real-world settings.
Conventional supervised learning methods often overfit to user-specific
patterns, leading to poor performance on unseen users. Existing domain
generalization approaches, while promising, frequently overlook temporal
dependencies or depend on impractical domain-specific labels. We propose
Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a
novel framework that redefines feature extraction as a sequential
decision-making process driven by reinforcement learning. TPRL-DG leverages a
Transformer-based autoregressive generator to produce temporal tokens that
capture user-invariant activity dynamics, optimized via a multi-objective
reward function balancing class discrimination and cross-user invariance. Key
innovations include: (1) an RL-driven approach for domain generalization, (2)
autoregressive tokenization to preserve temporal coherence, and (3) a
label-free reward design eliminating the need for target user annotations.
Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses
state-of-the-art methods in cross-user generalization, achieving superior
accuracy without per-user calibration. By learning robust, user-invariant
temporal patterns, TPRL-DG enables scalable HAR systems, facilitating
advancements in personalized healthcare, adaptive fitness tracking, and
context-aware environments.

</details>


### [95] [MatPROV: A Provenance Graph Dataset of Material Synthesis Extracted from Scientific Literature](https://arxiv.org/abs/2509.01042)
*Hirofumi Tsuruta,Masaya Kumagai*

Main category: cs.LG

TL;DR: 本文提出了MatPROV数据集，使用PROV-DM标准从科学文献中提取材料合成程序，通过图结构捕捉复杂的因果关系，为自动化合成规划提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用刚性、领域特定的模式或假设合成程序是线性序列，无法捕捉真实世界程序的复杂结构，需要更灵活的表达方式。

Method: 采用PROV-DM国际溯源信息标准，使用大语言模型从科学文献中提取符合标准的合成程序，构建图结构表示。

Result: 创建了MatPROV数据集，通过直观的有向图捕捉材料、操作和条件之间的结构复杂性和因果关系。

Conclusion: 该方法实现了机器可解释的合成知识表示，为自动化合成规划和优化等未来研究开辟了机会。

Abstract: Synthesis procedures play a critical role in materials research, as they
directly affect material properties. With data-driven approaches increasingly
accelerating materials discovery, there is growing interest in extracting
synthesis procedures from scientific literature as structured data. However,
existing studies often rely on rigid, domain-specific schemas with predefined
fields for structuring synthesis procedures or assume that synthesis procedures
are linear sequences of operations, which limits their ability to capture the
structural complexity of real-world procedures. To address these limitations,
we adopt PROV-DM, an international standard for provenance information, which
supports flexible, graph-based modeling of procedures. We present MatPROV, a
dataset of PROV-DM-compliant synthesis procedures extracted from scientific
literature using large language models. MatPROV captures structural
complexities and causal relationships among materials, operations, and
conditions through visually intuitive directed graphs. This representation
enables machine-interpretable synthesis knowledge, opening opportunities for
future research such as automated synthesis planning and optimization.

</details>


### [96] [IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models](https://arxiv.org/abs/2509.01073)
*Yuhong Zhang,Xusheng Zhu,Yuchen Xu,ChiaEn Lu,Hsinyu Shih,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: 提出基于大型脑模型LaBraM的相关性注意力映射方法，利用IMU数据的空间通道关系来识别EEG信号中的运动伪影，相比传统单模态方法显著提升运动场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受运动伪影污染，传统单模态方法如ASR和ICA未充分利用IMU等同时记录的模态数据，限制了脑机接口在真实环境中的部署。

Method: 使用微调的大型脑模型LaBraM，通过相关性注意力映射方法分析IMU数据的空间通道关系来识别EEG运动伪影。模型仅需5.9小时EEG-IMU数据训练，参数量约920万。

Result: 在不同时间尺度和运动活动下，与ASR-ICA基准相比，引入IMU参考信号显著提高了各种运动场景下的鲁棒性。

Conclusion: 多模态方法结合IMU数据能有效提升EEG运动伪影去除性能，为脑机接口在真实运动环境中的应用提供了更可靠的解决方案。

Abstract: Electroencephalography (EEG) is a non-invasive method for measuring brain
activity with high temporal resolution; however, EEG signals often exhibit low
signal-to-noise ratios because of contamination from physiological and
environmental artifacts. One of the major challenges hindering the real-world
deployment of brain-computer interfaces (BCIs) involves the frequent occurrence
of motion-related EEG artifacts. Most prior studies on EEG motion artifact
removal rely on single-modality approaches, such as Artifact Subspace
Reconstruction (ASR) and Independent Component Analysis (ICA), without
incorporating simultaneously recorded modalities like inertial measurement
units (IMUs), which directly capture the extent and dynamics of motion. This
work proposes a fine-tuned large brain model (LaBraM)-based correlation
attention mapping method that leverages spatial channel relationships in IMU
data to identify motion-related artifacts in EEG signals. The fine-tuned model
contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU
recordings for training, just 0.2346\% of the 2500 hours used to train the base
model. We compare our results against the established ASR-ICA benchmark across
varying time scales and motion activities, showing that incorporating IMU
reference signals significantly improves robustness under diverse motion
scenarios.

</details>


### [97] [REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis](https://arxiv.org/abs/2509.01082)
*Madhav Kanda,Shubham Ugare,Sasa Misailovic*

Main category: cs.LG

TL;DR: RefineStat是一个语言模型驱动的框架，通过语义约束和诊断感知的细化来提高小语言模型生成概率程序的质量，确保程序语法正确且统计可靠。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在生成概率程序时经常产生语法和语义错误，特别是推理结构缺陷。受概率程序员领域专业知识和调试策略的启发，需要一种方法来确保生成的程序包含有效分布和良好参数。

Method: 引入RefineStat框架：1) 强制执行语义约束确保程序包含有效分布和良好参数；2) 当可靠性检查失败时，应用诊断感知的细化，重新采样先验或似然组件。

Result: 在多个概率编程代码生成任务上评估显示，RefineStat能够生成语法正确且统计可靠的程序，性能通常匹配甚至超过闭源大语言模型（如OpenAI o3）。

Conclusion: RefineStat通过语义约束和细化策略有效解决了小语言模型生成概率程序时的质量问题，为统计模型发现提供了可靠的自动化解决方案。

Abstract: Probabilistic programming offers a powerful framework for modeling
uncertainty, yet statistical model discovery in this domain entails navigating
an immense search space under strict domain-specific constraints. When small
language models are tasked with generating probabilistic programs, they
frequently produce outputs that suffer from both syntactic and semantic errors,
such as flawed inference constructs. Motivated by probabilistic programmers'
domain expertise and debugging strategies, we introduce RefineStat, a language
model--driven framework that enforces semantic constraints ensuring synthesized
programs contain valid distributions and well-formed parameters, and then
applies diagnostic-aware refinement by resampling prior or likelihood
components whenever reliability checks fail. We evaluate RefineStat on multiple
probabilistic-programming code-generation tasks using smaller language models
(SLMs) and find that it produces programs that are both syntactically sound and
statistically reliable, often matching or surpassing those from closed-source
large language models (e.g., OpenAI o3).

</details>


### [98] [A Class of Random-Kernel Network Models](https://arxiv.org/abs/2509.01090)
*James Tian*

Main category: cs.LG

TL;DR: 随机核网络：通过确定性核组合构建多层结构，仅在最外层引入随机性，证明深层结构比浅层结构能以更少的蒙特卡洛样本近似特定函数


<details>
  <summary>Details</summary>
Motivation: 探索深度结构在随机特征模型中的优势，研究深度对样本复杂度的影响，建立深度分离理论

Method: 构建多层随机核网络，通过确定性核组合创建深度结构，仅在网络最外层引入随机性，分析其函数逼近能力

Result: 证明了深层结构在近似特定函数时比任何浅层结构需要更少的蒙特卡洛样本，确立了样本复杂度方面的深度分离定理

Conclusion: 深度结构在随机特征模型中具有显著优势，能够以更低的样本复杂度实现函数逼近，为深度学习的理论分析提供了重要支撑

Abstract: We introduce random-kernel networks, a multilayer extension of random feature
models where depth is created by deterministic kernel composition and
randomness enters only in the outermost layer. We prove that deeper
constructions can approximate certain functions with fewer Monte Carlo samples
than any shallow counterpart, establishing a depth separation theorem in sample
complexity.

</details>


### [99] [CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection](https://arxiv.org/abs/2509.01098)
*Zhijie Zhong,Zhiwen Yu,Yiu-ming Cheung,Kaixiang Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Time Series Anomaly Detection metrics serve as crucial tools for model
evaluation. However, existing metrics suffer from several limitations:
insufficient discriminative power, strong hyperparameter dependency,
sensitivity to perturbations, and high computational overhead. This paper
introduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric
that simultaneously measures prediction confidence and uncertainty consistency.
By employing Bayesian estimation to quantify the uncertainty of anomaly scores,
we construct both global and event-level confidence and consistency scores for
model predictions, resulting in a concise CCE metric. Theoretically and
experimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz
robustness against score perturbations, and linear time complexity
$\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing
the ranking capabilities of various metrics. RankEval represents the first
standardized and reproducible evaluation pipeline that enables objective
comparison of evaluation metrics. Both CCE and RankEval implementations are
fully open-source.

</details>


### [100] [SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning](https://arxiv.org/abs/2509.01119)
*Senura Hansaja Wanasekara,Van-Dinh Nguyen,Kok-Seng,M. -Duong Nguyen,Symeon Chatzinotas,Octavia A. Dobre*

Main category: cs.LG

TL;DR: 提出SC-GIR框架，通过自监督学习提取任务无关的语义表示，实现目标导向的语义通信，在图像传输中比基线方案提升近10%性能


<details>
  <summary>Details</summary>
Motivation: 解决当前语义通信系统需要联合训练、冗余数据传输和依赖标注数据的问题，提升任务无关的实用性

Method: 使用基于协方差的对比学习技术提取不变表示，该表示封装源数据的关键信息且与下游任务无关，实现高效压缩通信

Result: 在多个数据集上实验显示，SC-GIR比基线方案提升近10%，在不同SNR条件下压缩数据分类准确率超过85%

Conclusion: 所提框架能学习紧凑且信息丰富的潜在表示，为目标导向语义通信提供了有效解决方案

Abstract: Goal-oriented semantic communication (SC) aims to revolutionize communication
systems by transmitting only task-essential information. However, current
approaches face challenges such as joint training at transceivers, leading to
redundant data exchange and reliance on labeled datasets, which limits their
task-agnostic utility. To address these challenges, we propose a novel
framework called Goal-oriented Invariant Representation-based SC (SC-GIR) for
image transmission. Our framework leverages self-supervised learning to extract
an invariant representation that encapsulates crucial information from the
source data, independent of the specific downstream task. This compressed
representation facilitates efficient communication while retaining key features
for successful downstream task execution. Focusing on machine-to-machine tasks,
we utilize covariance-based contrastive learning techniques to obtain a latent
representation that is both meaningful and semantically dense. To evaluate the
effectiveness of the proposed scheme on downstream tasks, we apply it to
various image datasets for lossy compression. The compressed representations
are then used in a goal-oriented AI task. Extensive experiments on several
datasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,
and achieves over 85% classification accuracy for compressed data under
different SNR conditions. These results underscore the effectiveness of the
proposed framework in learning compact and informative latent representations.

</details>


### [101] [MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets](https://arxiv.org/abs/2509.01135)
*Guangli Li,Canbiao Wu,Zhehao Zhou,Na Tian,Zhen Liang*

Main category: cs.LG

TL;DR: 提出MATL-DC框架，通过多域聚合和域-类原型实现未见目标域的EEG情感识别，在SEED系列数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 当前迁移学习模型过度依赖源域和目标域数据，限制了EEG情感识别在实际应用中的推广

Method: 设计特征解耦模块分离域特征和类特征，采用多域聚合机制形成超域，使用成对学习策略将分类问题转化为相似性问题

Result: 在SEED、SEED-IV和SEED-V数据集上的准确率分别达到84.70%、68.11%和61.08%，性能优于依赖目标域的方法

Conclusion: MATL-DC框架能够有效处理未见目标域的EEG情感识别问题，具有重要的实际应用价值

Abstract: Emotion recognition based on electroencephalography (EEG) signals is
increasingly becoming a key research hotspot in affective Brain-Computer
Interfaces (aBCIs). However, the current transfer learning model greatly
depends on the source domain and target domain data, which hinder the practical
application of emotion recognition. Therefore, we propose a Multi-domain
Aggregation Transfer Learning framework for EEG emotion recognition with
Domain-Class prototype under unseen targets (MATL-DC). We design the feature
decoupling module to decouple class-invariant domain features from
domain-invariant class features from shallow features. In the model training
stage, the multi-domain aggregation mechanism aggregates the domain feature
space to form a superdomain, which enhances the characteristics of emotional
EEG signals. In each superdomain, we further extract the class prototype
representation by class features. In addition, we adopt the pairwise learning
strategy to transform the sample classification problem into the similarity
problem between sample pairs, which effectively alleviates the influence of
label noise. It is worth noting that the target domain is completely unseen
during the training process. In the inference stage, we use the trained
domain-class prototypes for inference, and then realize emotion recognition. We
rigorously validate it on the publicly available databases (SEED, SEED-IV and
SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%,
68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better
performance than methods that rely on both source and target domains. The
source code is available at https://github.com/WuCB-BCI/MATL-DC.

</details>


### [102] [Nonlinear Performative Prediction](https://arxiv.org/abs/2509.01139)
*Guangzheng Zhong,Yang Liu,Jiming Liu*

Main category: cs.LG

TL;DR: 本文提出了一种将performative prediction扩展到非线性情况的新方法，通过最大间隔和核方法处理非线性数据分布，建立了performative stability的理论条件并开发了相应算法。


<details>
  <summary>Details</summary>
Motivation: 现有的performative prediction研究主要依赖不可控假设（如梯度有界）且局限于线性情况，而现实世界数据通常具有复杂的非线性特征，需要更通用的方法。

Method: 使用最大间隔方法构建损失函数，通过核方法扩展到非线性空间，采用预测误差差异量化数据分布偏移，推导performative stability条件并开发保证稳定性的算法。

Result: 在合成和真实数据集上的实验验证了方法的有效性，在线性和非线性数据分布下均表现出优于现有基线的性能。

Conclusion: 该方法成功将performative prediction推广到非线性场景，保持了关键理论性质，为实际应用提供了更通用的解决方案。

Abstract: Performative prediction is an emerging paradigm in machine learning that
addresses scenarios where the model's prediction may induce a shift in the
distribution of the data it aims to predict. Current works in this field often
rely on uncontrollable assumptions, such as bounded gradients of performative
loss, and primarily focus on linear cases in their examples and evaluations to
maintain consistency between theoretical guarantees and empirical validations.
However, such linearity rarely holds in real-world applications, where the data
usually exhibit complex nonlinear characteristics. In this paper, we relax
these out-of-control assumptions and present a novel design that generalizes
performative prediction to nonlinear cases while preserving essential
theoretical properties. Specifically, we formulate the loss function of
performative prediction using a maximum margin approach and extend it to
nonlinear spaces through kernel methods. To quantify the data distribution
shift, we employ the discrepancy between prediction errors on these two
distributions as an indicator, which characterizes the impact of the
performative effect on specific learning tasks. By doing so, we can derive, for
both linear and nonlinear cases, the conditions for performative stability, a
critical and desirable property in performative contexts. Building on these
theoretical insights, we develop an algorithm that guarantees the performative
stability of the predictive model. We validate the effectiveness of our method
through experiments on synthetic and real-world datasets with both linear and
nonlinear data distributions, demonstrating superior performance compared to
state-of-the-art baselines.

</details>


### [103] [Multi-Modal Machine Learning Framework for Predicting Early Recurrence of Brain Tumors Using MRI and Clinical Biomarkers](https://arxiv.org/abs/2509.01161)
*Cheng Cheng,Zeping Chen,Rui Xie,Peiyao Zheng,Xavier Wang*

Main category: cs.LG

TL;DR: 开发多模态机器学习框架，整合MRI影像特征和临床生物标志物，用于预测脑肿瘤术后早期复发风险


<details>
  <summary>Details</summary>
Motivation: 准确预测脑肿瘤患者手术切除后的早期复发仍然是一个临床挑战，需要更精准的风险分层工具

Method: 采用四种机器学习算法（GBM、RSF、CoxBoost、XGBoost），整合结构MRI特征和临床生物标志物，使用C-index、时间依赖性AUC、校准曲线和决策曲线分析验证模型性能

Result: 模型表现出良好的预测性能，能够有效预测术后复发风险

Conclusion: 该多模态机器学习框架为脑肿瘤患者的风险分层和个性化随访规划提供了有前景的工具

Abstract: Accurately predicting early recurrence in brain tumor patients following
surgical resection remains a clinical challenge. This study proposes a
multi-modal machine learning framework that integrates structural MRI features
with clinical biomarkers to improve postoperative recurrence prediction. We
employ four machine learning algorithms -- Gradient Boosting Machine (GBM),
Random Survival Forest (RSF), CoxBoost, and XGBoost -- and validate model
performance using concordance index (C-index), time-dependent AUC, calibration
curves, and decision curve analysis. Our model demonstrates promising
performance, offering a potential tool for risk stratification and personalized
follow-up planning.

</details>


### [104] [A Multimodal Deep Learning Framework for Early Diagnosis of Liver Cancer via Optimized BiLSTM-AM-VMD Architecture](https://arxiv.org/abs/2509.01164)
*Cheng Cheng,Zeping Chen,Xavier Wang*

Main category: cs.LG

TL;DR: 提出BiLSTM-AM-VMD多模态深度学习框架，整合双向LSTM、多头注意力和变分模态分解，用于早期肝癌诊断，在真实数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 早期肝癌诊断需要整合多种异构数据（临床特征、生化指标、影像变量），传统方法在准确性和可解释性方面存在不足。

Method: 使用双向LSTM处理序列数据，多头注意力机制捕捉特征间依赖关系，变分模态分解处理信号，整合多模态数据进行诊断。

Result: 在真实数据集上实验表明，该方法在预测准确性和可解释性方面均优于传统机器学习和基线深度学习模型。

Conclusion: BiLSTM-AM-VMD框架为早期肝癌诊断提供了有效的多模态深度学习解决方案，具有优异的性能和临床实用性。

Abstract: This paper proposes a novel multimodal deep learning framework integrating
bidirectional LSTM, multi-head attention mechanism, and variational mode
decomposition (BiLSTM-AM-VMD) for early liver cancer diagnosis. Using
heterogeneous data that include clinical characteristics, biochemical markers,
and imaging-derived variables, our approach improves both prediction accuracy
and interpretability. Experimental results on real-world datasets demonstrate
superior performance over traditional machine learning and baseline deep
learning models.

</details>


### [105] [ADMP-GNN: Adaptive Depth Message Passing GNN](https://arxiv.org/abs/2509.01170)
*Yassine Abbahaddou,Fragkiskos D. Malliaros,Johannes F. Lutzeyer,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: 提出了ADMP-GNN框架，通过动态调整每个节点的消息传递层数来提升图神经网络性能，解决了传统GNN对所有节点使用固定层数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN对所有节点使用固定数量的消息传递层，但实际中不同特征节点需要不同的计算深度，这限制了模型性能。

Method: 提出自适应深度消息传递GNN（ADMP-GNN），为每个节点动态确定最优的消息传递层数，适用于任何遵循消息传递方案的模型。

Result: 在节点分类任务上评估，ADMP-GNN相比基线GNN模型表现出性能提升，实证和合成数据集实验均支持该方法的有效性。

Conclusion: 自适应深度调整是提升GNN性能的有效策略，ADMP-GNN框架为图学习任务提供了更灵活和高效的解决方案。

Abstract: Graph Neural Networks (GNNs) have proven to be highly effective in various
graph learning tasks. A key characteristic of GNNs is their use of a fixed
number of message-passing steps for all nodes in the graph, regardless of each
node's diverse computational needs and characteristics. Through empirical
real-world data analysis, we demonstrate that the optimal number of
message-passing layers varies for nodes with different characteristics. This
finding is further supported by experiments conducted on synthetic datasets. To
address this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novel
framework that dynamically adjusts the number of message passing layers for
each node, resulting in improved performance. This approach applies to any
model that follows the message passing scheme. We evaluate ADMP-GNN on the node
classification task and observe performance improvements over baseline GNN
models.

</details>


### [106] [StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time Series Forecasting](https://arxiv.org/abs/2509.01187)
*Zihao Wang,Yunjie Li,Lingmin Zan,Zheng Gong,Mengtao Zhu*

Main category: cs.LG

TL;DR: StoxLSTM是一种随机扩展长短期记忆网络，通过在xLSTM中引入随机潜在变量，将其改进为状态空间建模框架，显著提升了时间序列建模能力和预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管xLSTM在建模复杂时间依赖性方面表现出色，但在处理具有未知、复杂和层次动态的真实世界数据集时，其表示能力和预测性能仍有提升空间。

Method: 提出StoxLSTM方法，在xLSTM架构中引入随机潜在变量，构建状态空间建模框架，通过专门设计的循环块建模潜在动态演化过程。

Result: 在多个研究领域的公开基准数据集上的广泛实验表明，StoxLSTM始终优于最先进的基线方法，具有更好的鲁棒性和更强的泛化能力。

Conclusion: StoxLSTM通过将随机建模思想融入xLSTM架构，成功提升了时间序列建模的性能，为处理复杂真实世界时间序列数据提供了有效解决方案。

Abstract: The Extended Long Short-Term Memory (xLSTM) network has attracted widespread
research interest due to its enhanced capability to model complex temporal
dependencies in diverse time series applications. Despite its success, there is
still potential to further improve its representational capacity and
forecasting performance, particularly on challenging real-world datasets with
unknown, intricate, and hierarchical dynamics. In this work, we propose a
stochastic xLSTM, termed StoxLSTM, that improves the original architecture into
a state space modeling framework by incorporating stochastic latent variables
within xLSTM. StoxLSTM models the latent dynamic evolution through specially
designed recurrent blocks, enabling it to effectively capture the underlying
temporal patterns and dependencies. Extensive experiments on publicly available
benchmark datasets from multiple research communities demonstrate that StoxLSTM
consistently outperforms state-of-the-art baselines with better robustness and
stronger generalization ability.

</details>


### [107] [Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework](https://arxiv.org/abs/2509.01198)
*Eddi Weinwurm,Alexander Kovalenko*

Main category: cs.LG

TL;DR: 提出一种关系保持损失函数(RPL)，通过最小化高维数据和低维嵌入之间的关系矩阵差异，来保持向量空间的正交性和线性独立性等关键属性


<details>
  <summary>Details</summary>
Motivation: 降维过程会扭曲向量空间的关键属性（如正交性和线性独立性），这些属性对于跨模态检索、聚类和分类等任务至关重要

Method: 使用关系保持损失函数(RPL)，通过比较高维数据和低维嵌入之间的关系矩阵（如Gram矩阵或余弦相似度矩阵）的差异来训练神经网络进行非线性投影

Result: 初步实验表明RPL能够在减少嵌入维度的同时，在很大程度上保持下游任务的性能，这得益于其对关键向量空间属性的保持

Conclusion: RPL不仅适用于降维任务，还可广泛应用于跨域对齐和迁移学习、知识蒸馏、公平性和不变性、去中心化、图和流形学习以及联邦学习等领域

Abstract: Dimensionality reduction can distort vector space properties such as
orthogonality and linear independence, which are critical for tasks including
cross-modal retrieval, clustering, and classification. We propose a
Relationship Preserving Loss (RPL), a loss function that preserves these
properties by minimizing discrepancies between relationship matrices (e.g.,
Gram or cosine) of high-dimensional data and their low-dimensional embeddings.
RPL trains neural networks for non-linear projections and is supported by error
bounds derived from matrix perturbation theory. Initial experiments suggest
that RPL reduces embedding dimensions while largely retaining performance on
downstream tasks, likely due to its preservation of key vector space
properties. While we describe here the use of RPL in dimensionality reduction,
this loss can also be applied more broadly, for example to cross-domain
alignment and transfer learning, knowledge distillation, fairness and
invariance, dehubbing, graph and manifold learning, and federated learning,
where distributed embeddings must remain geometrically consistent.

</details>


### [108] [Geometric origin of adversarial vulnerability in deep learning](https://arxiv.org/abs/2509.01235)
*Yixiong Ren,Wenkang Du,Jianhui Zhou,Haiping Huang*

Main category: cs.LG

TL;DR: 提出几何感知深度学习框架，通过分层局部训练优化深度神经网络内部表示，增强类内紧凑性和类间分离性，提高对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决深度学习训练精度与对抗鲁棒性之间的平衡难题，探索生物智能与人工智能系统对齐的物理学习机制

Method: 采用几何感知深度学习框架，利用分层局部训练方法，通过能量模型和Hebbian耦合机制优化隐藏表示

Result: 实现了特征空间的流形平滑性，显著提升了对白盒和黑盒攻击的对抗鲁棒性，同时减少表示干扰

Conclusion: 该框架为理解学习的物理机制提供了新视角，促进生物与人工智能系统的对齐，能够有效整合新信息到现有知识结构中

Abstract: How to balance training accuracy and adversarial robustness has become a
challenge since the birth of deep learning. Here, we introduce a geometry-aware
deep learning framework that leverages layer-wise local training to sculpt the
internal representations of deep neural networks. This framework promotes
intra-class compactness and inter-class separation in feature space, leading to
manifold smoothness and adversarial robustness against white or black box
attacks. The performance can be explained by an energy model with Hebbian
coupling between elements of the hidden representation. Our results thus shed
light on the physics of learning in the direction of alignment between
biological and artificial intelligence systems. Using the current framework,
the deep network can assimilate new information into existing knowledge
structures while reducing representation interference.

</details>


### [109] [What Expressivity Theory Misses: Message Passing Complexity for GNNs](https://arxiv.org/abs/2509.01254)
*Niklas Kemper,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出Message Passing Complexity (MPC)作为新的GNN分析框架，替代传统的表达能力理论，通过连续度量来量化GNN解决任务的实际难度


<details>
  <summary>Details</summary>
Motivation: 传统表达能力理论存在局限性：1）大多数实际任务不需要超越WL测试的表达力；2）二元分类和理想化假设无法反映GNN的实际能力

Method: 提出Message Passing Complexity (MPC)连续度量方法，量化GNN架构通过消息传递解决给定任务的难度，同时捕捉过压缩等实际限制

Result: 在基础GNN任务上的广泛验证表明，MPC的理论预测与实证性能相关，成功解释了架构的成功与失败

Conclusion: MPC超越了表达能力理论，为理解和改进GNN架构提供了更强大和细致入微的框架，有效缩小了理论与实践之间的差距

Abstract: Expressivity theory, characterizing which graphs a GNN can distinguish, has
become the predominant framework for analyzing GNNs, with new models striving
for higher expressivity. However, we argue that this focus is misguided: First,
higher expressivity is not necessary for most real-world tasks as these tasks
rarely require expressivity beyond the basic WL test. Second, expressivity
theory's binary characterization and idealized assumptions fail to reflect
GNNs' practical capabilities. To overcome these limitations, we propose Message
Passing Complexity (MPC): a continuous measure that quantifies the difficulty
for a GNN architecture to solve a given task through message passing. MPC
captures practical limitations like over-squashing while preserving the
theoretical impossibility results from expressivity theory, effectively
narrowing the gap between theory and practice. Through extensive validation on
fundamental GNN tasks, we show that MPC's theoretical predictions correlate
with empirical performance, successfully explaining architectural successes and
failures. Thereby, MPC advances beyond expressivity theory to provide a more
powerful and nuanced framework for understanding and improving GNN
architectures.

</details>


### [110] [Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks](https://arxiv.org/abs/2509.01257)
*Andrea Fox,Francesco De Pellegrini,Eitan Altman*

Main category: cs.LG

TL;DR: 提出了一种去中心化的多智能体强化学习框架，通过共享约束向量实现隐式协调，解决了边缘计算中资源竞争和通信受限的问题。


<details>
  <summary>Details</summary>
Motivation: 边缘计算系统中，自主智能体需要在有限观测和通信约束下快速做出本地决策并竞争共享资源，现有MARL方法依赖集中式评论家或频繁通信，无法满足这些约束条件。

Method: 每个智能体解决一个约束马尔可夫决策过程(CMDP)，通过共享约束向量进行隐式协调，约束条件防止共享服务器资源过载，约束更新频率低且通信开销小，使用安全强化学习确保策略满足本地和全局目标。

Result: 在理论分析上建立了温和假设下的理论保证，实验验证表明该方法在性能上优于集中式和独立基线方法，特别是在大规模场景中表现更佳。

Conclusion: 该去中心化框架通过轻量级的共享约束机制实现了有效的隐式协调，在保持低通信开销的同时提升了多智能体系统的整体性能，特别适用于边缘计算等通信受限环境。

Abstract: In edge computing systems, autonomous agents must make fast local decisions
while competing for shared resources. Existing MARL methods often resume to
centralized critics or frequent communication, which fail under limited
observability and communication constraints. We propose a decentralized
framework in which each agent solves a constrained Markov decision process
(CMDP), coordinating implicitly through a shared constraint vector. For the
specific case of offloading, e.g., constraints prevent overloading shared
server resources. Coordination constraints are updated infrequently and act as
a lightweight coordination mechanism. They enable agents to align with global
resource usage objectives but require little direct communication. Using safe
reinforcement learning, agents learn policies that meet both local and global
goals. We establish theoretical guarantees under mild assumptions and validate
our approach experimentally, showing improved performance over centralized and
independent baselines, especially in large-scale settings.

</details>


### [111] [Iterative In-Context Learning to Enhance LLMs Abstract Reasoning: The Case-Study of Algebraic Tasks](https://arxiv.org/abs/2509.01267)
*Stefano Fioravanti,Matteo Zavatteri,Roberto Confalonieri,Kamyar Zeinalipour,Paolo Frazzetto,Alessandro Sperduti,Nicolò Navarin*

Main category: cs.LG

TL;DR: 通过迭代示例选择策略提升LLM在代数表达式简化任务中的组合性推理能力，发现简单示例比复杂示例更有效


<details>
  <summary>Details</summary>
Motivation: LLM在系统性汇总和处理需要组合规则的推理任务时遇到重大挑战，特别是对分布外示例的处理能力不足

Method: 提出一种迭代示例选择策略，逐步构建优化的少数示例集合，结合显式推理指令，应用于非标准优先级规则的代数表达式简化任务

Result: LLM在这类数学任务中表现有限，但通过迭代提示策略能够显著提升性能，简单示例比复杂示例更有利于汉化性能

Conclusion: 迭代示例选择策略是提升LLM组合性推理能力的有效方法，简单示例的策略性使用能够获得更好的汉化性能

Abstract: LLMs face significant challenges in systematic generalization, particularly
when dealing with reasoning tasks requiring compositional rules and handling
out-of-distribution examples. To address these challenges, we introduce an
in-context learning methodology that improves the generalization capabilities
of general purpose LLMs. Our approach employs an iterative example selection
strategy, which incrementally constructs a tailored set of few-shot examples
optimized to enhance model's performance on a given task. As a proof of
concept, we apply this methodology to the resolution of algebraic expressions
involving non-standard simplification rules, according to which the priority of
addition and multiplication is changed.
  Our findings indicate that LLMs exhibit limited proficiency in these
mathematical tasks. We further demonstrate that LLMs reasoning benefits from
our iterative shot selection prompting strategy integrated with explicit
reasoning instructions. Crucially, our experiments reveal that some LLMs
achieve better generalization performances when prompted with simpler few-shot
examples rather than complex ones following the test data distribution.

</details>


### [112] [Building surrogate models using trajectories of agents trained by Reinforcement Learning](https://arxiv.org/abs/2509.01285)
*Julen Cestero,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出使用强化学习策略来高效采样确定性仿真环境，构建替代模型的方法，相比传统采样方法在广阔状态空间中表现更优


<details>
  <summary>Details</summary>
Motivation: 当前在计算昂贵的仿真环境中，面对广阔状态空间时，现有采样策略效率不足，需要更有效的替代建模方法

Method: 使用强化学习训练的策略进行采样，包括随机智能体、专家智能体和最大化状态转移分布熵的探索智能体，构建混合数据集

Result: 混合数据集在所有数据集中获得最佳评分，对状态空间表示至关重要，显著优于拉丁超立方采样和主动学习+Kriging方法

Conclusion: 该方法改进了现有技术水平，为在复杂仿真器上应用替代辅助的强化学习策略优化开辟了道路

Abstract: Sample efficiency in the face of computationally expensive simulations is a
common concern in surrogate modeling. Current strategies to minimize the number
of samples needed are not as effective in simulated environments with wide
state spaces. As a response to this challenge, we propose a novel method to
efficiently sample simulated deterministic environments by using policies
trained by Reinforcement Learning. We provide an extensive analysis of these
surrogate-building strategies with respect to Latin-Hypercube sampling or
Active Learning and Kriging, cross-validating performances with all sampled
datasets. The analysis shows that a mixed dataset that includes samples
acquired by random agents, expert agents, and agents trained to explore the
regions of maximum entropy of the state transition distribution provides the
best scores through all datasets, which is crucial for a meaningful state space
representation. We conclude that the proposed method improves the
state-of-the-art and clears the path to enable the application of
surrogate-aided Reinforcement Learning policy optimization strategies on
complex simulators.

</details>


### [113] [Equivariant U-Shaped Neural Operators for the Cahn-Hilliard Phase-Field Model](https://arxiv.org/abs/2509.01293)
*Xiao Xue,M. F. P. ten Eikelder,Tianyue Yang,Yiqing Li,Kan He,Shuo Wang,Peter V. Coveney*

Main category: cs.LG

TL;DR: 提出了一种等变U型神经算子(E-UNO)，用于学习Cahn-Hilliard方程控制的相分离过程，通过结合全局谱卷积和多分辨率架构，在保持物理对称性的同时准确预测相场演化。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高且缺乏灵活性，现有神经算子方法难以捕捉多尺度行为且忽略物理对称性，需要开发更高效且物理一致的替代模型。

Method: 采用等变U型神经算子架构，结合全局谱卷积和多分辨率U型结构，通过调节平移等变性来符合底层物理规律，从短历史动态中学习相场变量的演化。

Result: E-UNO在精细尺度和高频结构上优于标准傅里叶神经算子和U型神经算子基线，具有更好的泛化能力、更少的训练数据需求，并能产生物理一致的动力学行为。

Conclusion: E-UNO作为复杂相场系统的高效替代模型，通过编码对称性和尺度层次结构，实现了准确的空间和时间预测，为材料科学和软物质中的界面动力学研究提供了有力工具。

Abstract: Phase separation in binary mixtures, governed by the Cahn-Hilliard equation,
plays a central role in interfacial dynamics across materials science and soft
matter. While numerical solvers are accurate, they are often computationally
expensive and lack flexibility across varying initial conditions and
geometries. Neural operators provide a data-driven alternative by learning
solution operators between function spaces, but current architectures often
fail to capture multiscale behavior and neglect underlying physical symmetries.
Here we show that an equivariant U-shaped neural operator (E-UNO) can learn the
evolution of the phase-field variable from short histories of past dynamics,
achieving accurate predictions across space and time. The model combines global
spectral convolution with a multi-resolution U-shaped architecture and
regulates translation equivariance to align with the underlying physics. E-UNO
outperforms standard Fourier neural operator and U-shaped neural operator
baselines, particularly on fine-scale and high-frequency structures. By
encoding symmetry and scale hierarchy, the model generalizes better, requires
less training data, and yields physically consistent dynamics. This establishes
E-UNO as an efficient surrogate for complex phase-field systems.

</details>


### [114] [Towards Trustworthy Vital Sign Forecasting: Leveraging Uncertainty for Prediction Intervals](https://arxiv.org/abs/2509.01319)
*Li Rong Wang,Thomas C. Henderson,Yew Soon Ong,Yih Yng Ng,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出了两种基于重构不确定性估计(RUE)的预测区间方法，用于生命体征预测中的不确定性量化，以提高临床决策的可信度


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生命体征预测中应用受限，主要因为缺乏可靠的不确定性量化，临床医生无法判断预测异常是真实警告还是模型噪声

Method: 提出了参数化方法（高斯copula分布）和非参数化方法（K近邻），基于RUE不确定性度量来推导预测区间

Result: 高斯copula方法在低频数据上优于conformal prediction基线，KNN方法在高频数据上表现最佳

Conclusion: RUE衍生的预测区间方法为提供可解释、不确定性感知的生命体征预测展示了临床前景

Abstract: Vital signs, such as heart rate and blood pressure, are critical indicators
of patient health and are widely used in clinical monitoring and
decision-making. While deep learning models have shown promise in forecasting
these signals, their deployment in healthcare remains limited in part because
clinicians must be able to trust and interpret model outputs. Without reliable
uncertainty quantification -- particularly calibrated prediction intervals
(PIs) -- it is unclear whether a forecasted abnormality constitutes a
meaningful warning or merely reflects model noise, hindering clinical
decision-making. To address this, we present two methods for deriving PIs from
the Reconstruction Uncertainty Estimate (RUE), an uncertainty measure
well-suited to vital-sign forecasting due to its sensitivity to data shifts and
support for label-free calibration. Our parametric approach assumes that
prediction errors and uncertainty estimates follow a Gaussian copula
distribution, enabling closed-form PI computation. Our non-parametric approach,
based on k-nearest neighbours (KNN), empirically estimates the conditional
error distribution using similar validation instances. We evaluate these
methods on two large public datasets with minute- and hour-level sampling,
representing high- and low-frequency health signals. Experiments demonstrate
that the Gaussian copula method consistently outperforms conformal prediction
baselines on low-frequency data, while the KNN approach performs best on
high-frequency data. These results underscore the clinical promise of
RUE-derived PIs for delivering interpretable, uncertainty-aware vital sign
forecasts.

</details>


### [115] [Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.01321)
*Xinyu Tang,Zhenduo Zhang,Yurou Liu,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: DEPO是一个数据高效的政策优化管道，通过离线高质量数据筛选和在线可探索性动态过滤，显著减少强化学习验证奖励训练的计算成本和数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推理模型的RLVR方法需要大量计算和数据，导致训练成本高且数据效率低，需要更高效的数据选择策略。

Method: 结合离线阶段基于多样性、影响力和适当难度的高质量数据筛选，以及在线阶段基于样本级可探索性指标的动态过滤，并加入重放机制确保充分训练。

Result: 在五个推理基准测试中表现优于现有方法，仅使用20%训练数据就在AIME24上实现1.85倍加速，在AIME25上实现1.66倍加速。

Conclusion: DEPO通过智能数据选择策略有效降低了RLVR训练的计算成本，提高了数据效率，同时保持了优异的性能表现。

Abstract: Recent advances in large reasoning models have leveraged reinforcement
learning with verifiable rewards (RLVR) to improve reasoning capabilities.
However, scaling these methods typically requires extensive rollout computation
and large datasets, leading to high training costs and low data efficiency. To
mitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization
pipeline that combines optimized strategies for both offline and online data
selection. In the offline phase, we curate a high-quality subset of training
samples based on diversity, influence, and appropriate difficulty. During
online RLVR training, we introduce a sample-level explorability metric to
dynamically filter samples with low exploration potential, thereby reducing
substantial rollout computational costs. Furthermore, we incorporate a replay
mechanism for under-explored samples to ensure adequate training, which
enhances the model's final convergence performance. Experiments across five
reasoning benchmarks show that DEPO consistently outperforms existing methods
in both offline and online data selection scenarios. Notably, using only 20% of
the training data, our approach achieves a 1.85 times speed-up on AIME24 and a
1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset.

</details>


### [116] [Multitask Battery Management with Flexible Pretraining](https://arxiv.org/abs/2509.01323)
*Hong Lu,Jiali Chen,Jingzhao Zhang,Guannan He,Xuebing Han,Minggao Ouyang*

Main category: cs.LG

TL;DR: FMAE是一个灵活的预训练框架，能够处理电池数据缺失通道并捕捉数据片段间的相互关联，为多任务电池管理提供统一表示，显著减少数据和工程需求。


<details>
  <summary>Details</summary>
Motivation: 工业级电池管理涉及多种任务，每种任务使用不同的数据特征，构建任务特定方法需要大量数据和工程努力，限制了智能电池管理的可扩展性。

Method: 提出灵活掩码自编码器（FMAE）框架，能够从异构数据中学习统一电池表示，支持缺失数据通道，捕捉跨数据片段的相互关联。

Result: 在5个电池管理任务和11个电池数据集上，FMAE始终优于所有任务特定方法；在剩余寿命预测任务中，使用50倍少的推理数据仍保持最先进结果；即使真实数据缺失某些信息（如系统电压），FMAE仍能应用且性能影响很小。

Conclusion: FMAE展示了一条实用路径，为动态系统的多任务管理提供了一个灵活、数据高效的模型，简化了真实世界的电池管理应用。

Abstract: Industrial-scale battery management involves various types of tasks, such as
estimation, prediction, and system-level diagnostics. Each task employs
distinct data across temporal scales, sensor resolutions, and data channels.
Building task-specific methods requires a great deal of data and engineering
effort, which limits the scalability of intelligent battery management. Here we
present the Flexible Masked Autoencoder (FMAE), a flexible pretraining
framework that can learn with missing battery data channels and capture
inter-correlations across data snippets. FMAE learns unified battery
representations from heterogeneous data and can be adopted by different tasks
with minimal data and engineering efforts. Experimentally, FMAE consistently
outperforms all task-specific methods across five battery management tasks with
eleven battery datasets. On remaining life prediction tasks, FMAE uses 50 times
less inference data while maintaining state-of-the-art results. Moreover, when
real-world data lack certain information, such as system voltage, FMAE can
still be applied with marginal performance impact, achieving comparable results
with the best hand-crafted features. FMAE demonstrates a practical route to a
flexible, data-efficient model that simplifies real-world multi-task management
of dynamical systems.

</details>


### [117] [Globally aware optimization with resurgence](https://arxiv.org/abs/2509.01329)
*Wei Bu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern optimization faces a fundamental challenge: local gradient-based
methods provide no global information about the objective function $L$
landscape, often leading to suboptimal convergence and sensitivity to
initialization. We introduce a novel optimization framework that leverages
resurgence theory from complex analysis to extract global structural
information from divergent asymptotic series. Our key insight is that the
factorially divergent perturbative expansions of parameter space partition
functions encode precise information about all critical objective function
value in the landscape through their Borel transform singularities.
  The algorithm works by computing the statistical mechanical partition
function $Z(g) = \int e^{-L(\theta)/g} d\theta$ for small coupling $g\ll 1$,
extracting its asymptotic series coefficients, and identifying Borel plane
singularities that correspond one-to-one with critical objective function
values. These target values provide global guidance to local optimizers,
enabling principled learning rate adaptation and escape from suboptimal
regions. Unlike heuristic adaptive methods, targets are theoretically grounded
in the geometry of the optimization landscape.

</details>


### [118] [AT Loss: Advanced Torrential Loss Function for Precipitation Forecasting](https://arxiv.org/abs/2509.01348)
*Jaeho Choi,Hyeri Kim,Kwang-Ho Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 本文提出了一种基于二次无约束二进制优化(QUBO)的先进暴雨(AT)损失函数，用于改进机器学习降水预报模型，解决了传统关键成功指数(CSI)在干旱期失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统降水预报方法依赖的关键成功指数(CSI)在长期干旱期当降水量低于阈值时会失效，无法作为有效的优化标准，需要开发更鲁棒的损失函数。

Method: 引入简单的惩罚表达式并将其重新解释为QUBO公式，通过近似过程将QUBO公式松弛为可微分的先进暴雨(AT)损失函数。

Result: 提出的AT损失函数在Lipschitz常数、预报性能评估、一致性实验和与业务模型的消融研究中都表现出优越性。

Conclusion: AT损失函数有效解决了CSI在干旱期的局限性，为机器学习降水预报提供了更可靠的优化标准。

Abstract: Accurate precipitation forecasting is becoming increasingly important in the
context of climate change. In response, machine learning-based approaches have
recently gained attention as an emerging alternative to traditional methods
such as numerical weather prediction and climate models. Nonetheless, many
recent approaches still rely on off-the-shelf loss functions, and even the more
advanced ones merely involve optimization processes based on the critical
success index (CSI). The problem, however, is that CSI may become ineffective
during extended dry periods when precipitation remains below the threshold,
rendering it less than ideal as a criterion for optimization. To address this
limitation, we introduce a simple penalty expression and reinterpret it as a
quadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the
resulting QUBO formulation is relaxed into a differentiable advanced torrential
(AT) loss function through an approximation process. The proposed AT loss
demonstrates its superiority through the Lipschitz constant, forecast
performance evaluations, consistency experiments, and ablation studies with the
operational model.

</details>


### [119] [Causal Sensitivity Identification using Generative Learning](https://arxiv.org/abs/2509.01352)
*Soma Bandyopadhyay,Sudeshna Sarkar*

Main category: cs.LG

TL;DR: 提出了一种基于生成模型的因果影响识别方法，通过干预和反事实分析来识别因果敏感特征并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法可能受到混杂偏倚的影响，需要识别真正的因果特征来提升预测准确性和可解释性。

Method: 利用条件变分自编码器（CVAE）进行因果影响分析，从干预角度识别因果敏感特征，从反事实角度评估因果关系变化的影响。

Result: 在GeoLife数据集和Asia贝叶斯网络基准测试中验证了方法的有效性，能够识别因果影响并提升预测性能。

Conclusion: 该方法通过生成模型成功识别了因果敏感特征，减少了混杂偏倚，在时空轨迹预测等任务中表现出优越性能。

Abstract: In this work, we propose a novel generative method to identify the causal
impact and apply it to prediction tasks. We conduct causal impact analysis
using interventional and counterfactual perspectives. First, applying
interventions, we identify features that have a causal influence on the
predicted outcome, which we refer to as causally sensitive features, and
second, applying counterfactuals, we evaluate how changes in the cause affect
the effect. Our method exploits the Conditional Variational Autoencoder (CVAE)
to identify the causal impact and serve as a generative predictor. We are able
to reduce confounding bias by identifying causally sensitive features. We
demonstrate the effectiveness of our method by recommending the most likely
locations a user will visit next in their spatiotemporal trajectory influenced
by the causal relationships among various features. Experiments on the
large-scale GeoLife [Zheng et al., 2010] dataset and the benchmark Asia
Bayesian network validate the ability of our method to identify causal impact
and improve predictive performance.

</details>


### [120] [DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment](https://arxiv.org/abs/2509.01354)
*Wei Huang,Anda Cheng,Zhao Zhang,Yinggui Wang*

Main category: cs.LG

TL;DR: 提出了DPF-CM框架，专注于中文医疗大语言模型训练数据处理的完整解决方案，包括训练数据处理和部署隐私保护两大核心模块。


<details>
  <summary>Details</summary>
Motivation: 当前开源的中文医疗语言模型训练流程主要关注训练方法优化，但缺乏对训练数据处理的全面探索，存在指令内容不足和隐私泄露风险。

Method: 包含两个核心模块：(1) 训练数据处理管道，采用链式示例上下文学习策略生成问题导向指令，以及集成式过滤机制进行偏好数据筛选；(2) 部署隐私保护模块，通过隐私保护向量数据库(PPVD)方法进行模型记忆搜索、高风险数据库构建、安全数据库构建和匹配替换四个阶段。

Result: 实验结果显示DPF-CM显著提升模型准确性，使训练的中文医疗LLM在开源模型中达到最先进性能，同时将训练数据隐私泄露减少27%。

Conclusion: DPF-CM框架为中文医疗大语言模型提供了全面的数据处理解决方案，有效解决了指令内容缺乏和隐私保护问题，在性能和安全性方面都取得了显著改进。

Abstract: Current open-source training pipelines for Chinese medical language models
predominantly emphasize optimizing training methodologies to enhance the
performance of large language models (LLMs), yet lack comprehensive exploration
into training data processing. To address this gap, we propose DPF-CM, a
holistic Data Processing Framework for Chinese Medical LLMs training and
deployment. DPF-CM comprises two core modules. The first module is a data
processing pipeline tailored for model training. Beyond standard data
processing operations, we (1) introduce a chained examples context-learning
strategy to generate question-oriented instructions to mitigate the lack of
instruction content, and (2) implement an ensemble-based filtering mechanism
for preference data curation that averages multiple reward models to suppress
noisy samples. The second module focuses on privacy preservation during model
deployment. To prevent privacy risks from the inadvertent exposure of training
data, we propose a Privacy Preserving Vector Database (PPVD) approach, which
involves model memory search, high-risk database construction, secure database
construction, and match-and-replace, four key stages to minimize privacy
leakage during inference collectively. Experimental results show that DPF-CM
significantly improves model accuracy, enabling our trained Chinese medical LLM
to achieve state-of-the-art performance among open-source counterparts.
Moreover, the framework reduces training data privacy leakage by 27%.

</details>


### [121] [CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function](https://arxiv.org/abs/2509.01370)
*Jiarui Cao,Zhiyang Zhang,Heming Wang,Jun Xu,Ling Lan,Ran Gu*

Main category: cs.LG

TL;DR: 提出基于条件潜在扩散模型CbLDM解决纳米结构逆问题，通过条件先验估计后验分布提高采样效率，使用拉普拉斯矩阵降低重构误差，相比现有模型显著提升预测精度


<details>
  <summary>Details</summary>
Motivation: 纳米结构逆问题有助于理解纳米材料结构与性能关系，现有方法在从PDF数据恢复纳米结构方面存在效率低和精度不足的问题

Method: 基于潜在扩散模型，引入条件先验估计条件后验分布p(z|x)，减少扩散模型采样步骤；使用拉普拉斯矩阵替代距离矩阵来恢复纳米结构

Result: CbLDM相比现有模型展现出显著更高的预测精度，证明其有效解决纳米结构逆问题的能力

Conclusion: CbLDM不仅能有效解决纳米结构逆问题，还具有处理其他连续条件生成任务的潜力

Abstract: Nowadays, the nanostructure inverse problem is an attractive problem that
helps researchers to understand the relationship between the properties and the
structure of nanomaterials. This article focuses on the problem of using PDF to
recover the nanostructure, which this article views as a conditional generation
problem. This article propose a deep learning model CbLDM, Condition-based
Latent Diffusion Model. Based on the original latent diffusion model, the
sampling steps of the diffusion model are reduced and the sample generation
efficiency is improved by using the conditional prior to estimate conditional
posterior distribution, which is the approximated distribution of p(z|x). In
addition, this article uses the Laplacian matrix instead of the distance matrix
to recover the nanostructure, which can reduce the reconstruction error.
Finally, this article compares CbLDM with existing models which were used to
solve the nanostructure inverse problem, and find that CbLDM demonstrates
significantly higher prediction accuracy than these models, which reflects the
ability of CbLDM to solve the nanostructure inverse problem and the potential
to cope with other continuous conditional generation tasks.

</details>


### [122] [Learn to Jump: Adaptive Random Walks for Long-Range Propagation through Graph Hierarchies](https://arxiv.org/abs/2509.01381)
*Joël Mathys,Federico Errica*

Main category: cs.LG

TL;DR: 提出了一种利用层次图结构和自适应随机游走的新方法，通过可学习的转移概率在原始图和层次捷径之间选择，有效解决了消息传递架构在长距离依赖建模上的局限性。


<details>
  <summary>Details</summary>
Motivation: 消息传递架构在节点和图预测任务中难以充分建模长距离依赖关系，需要新的方法来突破传统方法在原始拓扑上的理论限制。

Method: 利用层次图结构和自适应随机游走，引入可学习的转移概率来决定游走是偏好原始图还是通过层次捷径传播。

Result: 在合成长距离任务上，该方法能够超越传统方法在原始拓扑上的理论限制，偏好层次结构的游走可以达到与原始图上更长游走相同的性能。

Conclusion: 这些初步发现为高效处理大型图同时有效捕获长距离依赖开辟了有前景的研究方向。

Abstract: Message-passing architectures struggle to sufficiently model long-range
dependencies in node and graph prediction tasks. We propose a novel approach
exploiting hierarchical graph structures and adaptive random walks to address
this challenge. Our method introduces learnable transition probabilities that
decide whether the walk should prefer the original graph or travel across
hierarchical shortcuts. On a synthetic long-range task, we demonstrate that our
approach can exceed the theoretical bound that constrains traditional
approaches operating solely on the original topology. Specifically, walks that
prefer the hierarchy achieve the same performance as longer walks on the
original graph. These preliminary findings open a promising direction for
efficiently processing large graphs while effectively capturing long-range
dependencies.

</details>


### [123] [Distillation of a tractable model from the VQ-VAE](https://arxiv.org/abs/2509.01400)
*Armin Hadžić,Milan Papez,Tomáš Pevný*

Main category: cs.LG

TL;DR: 本文提出了一种将VQ-VAE蒸馏为可处理概率模型的方法，通过选择高概率潜变量子集，使其在保持表达能力的同时实现可处理的概率推理。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE等离散潜空间深度生成模型具有优秀的数据生成能力，但由于潜空间过大，其概率推理被认为不可处理。本文旨在解决这一局限性。

Method: 通过选择高概率潜变量子集，将VQ-VAE蒸馏为概率电路模型，利用VQ-VAE经常未充分利用潜空间的特性来提高效率。

Result: 实验表明该方法在密度估计和条件生成任务中具有竞争力，挑战了VQ-VAE作为固有不可处理模型的传统观点。

Conclusion: VQ-VAE可以通过简单的蒸馏策略转化为可处理模型，在保持表达力的同时提供可处理的概率推理能力。

Abstract: Deep generative models with discrete latent space, such as the
Vector-Quantized Variational Autoencoder (VQ-VAE), offer excellent data
generation capabilities, but, due to the large size of their latent space,
their probabilistic inference is deemed intractable. We demonstrate that the
VQ-VAE can be distilled into a tractable model by selecting a subset of latent
variables with high probabilities. This simple strategy is particularly
efficient, especially if the VQ-VAE underutilizes its latent space, which is,
indeed, very often the case. We frame the distilled model as a probabilistic
circuit, and show that it preserves expressiveness of the VQ-VAE while
providing tractable probabilistic inference. Experiments illustrate competitive
performance in density estimation and conditional generation tasks, challenging
the view of the VQ-VAE as an inherently intractable model.

</details>


### [124] [Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring](https://arxiv.org/abs/2509.01409)
*Matteo Ballegeer,Matthias Bogaert,Dries F. Benoit*

Main category: cs.LG

TL;DR: 本研究评估了实例依赖成本敏感分类器在信用评分中的解释稳定性，发现虽然能提高成本效率，但SHAP和LIME解释的稳定性显著降低，特别是在类别不平衡加剧时。


<details>
  <summary>Details</summary>
Motivation: 实例依赖成本敏感分类器在信用评分中能提高成本效率，但其对模型解释稳定性的影响尚未被研究，而监管对透明度的要求日益增加。

Method: 使用四个公开信用评分数据集，评估IDCS分类器的判别能力和成本效率，引入新指标增强跨数据集可比性，并通过控制重采样研究SHAP和LIME特征重要性排名在不同类别不平衡程度下的稳定性。

Result: IDCS分类器提高了成本效率，但与传统模型相比产生了显著不稳定的解释，特别是随着类别不平衡加剧，解释稳定性问题更加突出。

Conclusion: 在监管对可解释性要求日益严格的背景下，需要解决IDCS分类器的稳定性问题，以确保其成本优势不会被不稳定或不可信的解释所削弱。

Abstract: Instance-dependent cost-sensitive (IDCS) classifiers offer a promising
approach to improving cost-efficiency in credit scoring by tailoring loss
functions to instance-specific costs. However, the impact of such loss
functions on the stability of model explanations remains unexplored in
literature, despite increasing regulatory demands for transparency. This study
addresses this gap by evaluating the stability of Local Interpretable
Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP)
when applied to IDCS models. Using four publicly available credit scoring
datasets, we first assess the discriminatory power and cost-efficiency of IDCS
classifiers, introducing a novel metric to enhance cross-dataset comparability.
We then investigate the stability of SHAP and LIME feature importance rankings
under varying degrees of class imbalance through controlled resampling. Our
results reveal that while IDCS classifiers improve cost-efficiency, they
produce significantly less stable explanations compared to traditional models,
particularly as class imbalance increases, highlighting a critical trade-off
between cost optimization and interpretability in credit scoring. Amid
increasing regulatory scrutiny on explainability, this research underscores the
pressing need to address stability issues in IDCS classifiers to ensure that
their cost advantages are not undermined by unstable or untrustworthy
explanations.

</details>


### [125] [Accelerating PDE Solvers with Equation-Recast Neural Operator Preconditioning](https://arxiv.org/abs/2509.01416)
*Qiyun Cheng,Md Hossain Sahadath,Huihua Yang,Shaowu Pan,Wei Ji*

Main category: cs.LG

TL;DR: 提出MD-PNOP框架，通过将参数偏差残差重新表述为附加源项，使神经算子能够在离线状态下精炼解，从而加速参数PDE求解器并保持物理约束。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器的计算开销在大规模参数研究和设计优化中仍然是关键瓶颈，需要一种既能加速求解又能严格保持物理约束的新方法。

Method: 将参数偏差残差重新表述为附加源项，利用训练好的神经算子离线精炼解，并将神经算子预测嵌入迭代PDE求解器作为改进的初始猜测。

Result: 在单组常数参数上训练的神经算子成功加速了具有异质、正弦和不连续参数分布的求解，计算时间减少约50%，同时保持全阶保真度。

Conclusion: MD-PNOP框架有效解决了神经算子的外推限制，实现了无需重新训练的外推泛化，在保持物理约束的同时显著加速了参数PDE求解。

Abstract: The computational overhead of traditional numerical solvers for partial
differential equations (PDEs) remains a critical bottleneck for large-scale
parametric studies and design optimization. We introduce a Minimal-Data
Parametric Neural Operator Preconditioning (MD-PNOP) framework, which
establishes a new paradigm for accelerating parametric PDE solvers while
strictly preserving physical constraints. The key idea is to recast the
residual from parameter deviation as additional source term, where any trained
neural operator can be used to refine the solution in an offline fashion. This
directly addresses the fundamental extrapolation limitation of neural
operators, enabling extrapolative generalization of any neural operator trained
at a single parameter setting across a wide range of configurations without any
retraining. The neural operator predictions are then embedded into iterative
PDE solvers as improved initial guesses, thereby reducing convergence
iterations without sacrificing accuracy. Unlike purely data-driven approaches,
MD-PNOP guarantees that the governing equations remain fully enforced,
eliminating concerns about loss of physics or interpretability. The framework
is architecture-agnostic and is demonstrated using both Deep Operator Networks
(DeepONet) and Fourier Neural Operators (FNO) for Boltzmann transport equation
solvers in neutron transport applications. We demonstrated that neural
operators trained on a single set of constant parameters successfully
accelerate solutions with heterogeneous, sinusoidal, and discontinuous
parameter distributions. Besides, MD-PNOP consistently achieves ~50% reduction
in computational time while maintaining full order fidelity for fixed-source,
single-group eigenvalue, and multigroup coupled eigenvalue problems.

</details>


### [126] [The Geometry of Nonlinear Reinforcement Learning](https://arxiv.org/abs/2509.01432)
*Nikola Milosevic,Nico Scherf*

Main category: cs.LG

TL;DR: 本文提出了一个统一的几何框架，将强化学习中的奖励最大化、安全探索和内在动机视为环境可达成长期行为的单一优化问题实例。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法通常将奖励最大化、安全探索和内在动机作为独立目标进行研究，缺乏统一的数学框架来整合这些不同目标。

Method: 构建了一个几何框架，将各种目标视为在环境可达成长期行为空间上的单一优化问题，并推广了策略镜像下降、自然策略梯度和信任域算法等经典方法，使其适用于非线性效用和凸约束。

Result: 该框架能够统一处理鲁棒性、安全性、探索性和多样性等目标，为深度强化学习中的几何方法提供了理论基础。

Conclusion: 该研究提供了一个统一的数学视角来整合强化学习中的多种目标，但在几何方法与深度强化学习的结合方面仍存在开放挑战。

Abstract: Reward maximization, safe exploration, and intrinsic motivation are often
studied as separate objectives in reinforcement learning (RL). We present a
unified geometric framework, that views these goals as instances of a single
optimization problem on the space of achievable long-term behavior in an
environment. Within this framework, classical methods such as policy mirror
descent, natural policy gradient, and trust-region algorithms naturally
generalize to nonlinear utilities and convex constraints. We illustrate how
this perspective captures robustness, safety, exploration, and diversity
objectives, and outline open challenges at the interface of geometry and deep
RL.

</details>


### [127] [Benchmarking Optimizers for Large Language Model Pretraining](https://arxiv.org/abs/2509.01440)
*Andrei Semenov,Matteo Pagliardini,Martin Jaggi*

Main category: cs.LG

TL;DR: 本文对近期大语言模型优化技术进行了标准化评估，通过系统变化模型大小、批次大小和训练时长，为实践者提供优化器选择指南，并为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型快速发展带来了众多优化方法，但不同实验协议使得方法间直接比较困难，需要标准化评估来指导实践和研究。

Method: 在标准化LLM预训练场景下全面评估近期优化技术，系统变化模型大小、批次大小和训练时长，仔细调优每种方法。

Result: 为不同场景提供了最佳优化器选择指南，识别了有前景的未来优化研究方向。

Conclusion: 通过发布代码和确保实验完全可复现，本研究有助于未来方法的开发和严格基准测试，为优化研究社区提供了重要参考。

Abstract: The recent development of Large Language Models (LLMs) has been accompanied
by an effervescence of novel ideas and methods to better optimize the loss of
deep learning models. Claims from those methods are myriad: from faster
convergence to removing reliance on certain hyperparameters. However, the
diverse experimental protocols used to validate these claims make direct
comparisons between methods challenging. This study presents a comprehensive
evaluation of recent optimization techniques across standardized LLM
pretraining scenarios, systematically varying model size, batch size, and
training duration. Through careful tuning of each method, we provide guidance
to practitioners on which optimizer is best suited for each scenario. For
researchers, our work highlights promising directions for future optimization
research. Finally, by releasing our code and making all experiments fully
reproducible, we hope our efforts can help the development and rigorous
benchmarking of future methods.

</details>


### [128] [Hierarchical Motion Captioning Utilizing External Text Data Source](https://arxiv.org/abs/2509.01471)
*Clayton Leite,Yu Xiao*

Main category: cs.LG

TL;DR: 这篇论文提出了一种两步层次方法来改善现有的运动描述方法，通过利用大语言模型生成详细的低级别描述，并结合外部文本数据来提高运动标注的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的运动标注方法需要高级别描述的运动数据，但现有数据集缺乏这种高级别标注和低级别详细描述，限制了运动标注的性能。

Method: 采用两步层次方法：1）利用大语言模型为高级别标注生成详细的低级别描述，并用于重新训练模型；2）引入检索基机制，将详细描述与外部文本数据源中的高级别标注对齐，结合运动特征生成准确的高级别标注。

Result: 在三个运动-文本数据集（HumanML3D、KIT和BOTH57M）上的实验表明，该方法在BLEU-1、BLEU-4、CIDEr和ROUGE-L指标上的平均性能比最新的M2T-Interpretable方法提高6%-50%。

Conclusion: 该方法能够利用外部文本数据源的知识来显著提高运动标注的准确性，尤其是对于现有数据集未覆盖的运动。

Abstract: This paper introduces a novel approach to enhance existing motion captioning
methods, which directly map representations of movement to high-level
descriptive captions (e.g., ``a person doing jumping jacks"). The existing
methods require motion data annotated with high-level descriptions (e.g.,
``jumping jacks"). However, such data is rarely available in existing
motion-text datasets, which additionally do not include low-level motion
descriptions. To address this, we propose a two-step hierarchical approach.
First, we employ large language models to create detailed descriptions
corresponding to each high-level caption that appears in the motion-text
datasets (e.g., ``jumping while synchronizing arm extensions with the opening
and closing of legs" for ``jumping jacks"). These refined annotations are used
to retrain motion-to-text models to produce captions with low-level details.
Second, we introduce a pioneering retrieval-based mechanism. It aligns the
detailed low-level captions with candidate high-level captions from additional
text data sources, and combine them with motion features to fabricate precise
high-level captions. Our methodology is distinctive in its ability to harness
knowledge from external text sources to greatly increase motion captioning
accuracy, especially for movements not covered in existing motion-text
datasets. Experiments on three distinct motion-text datasets (HumanML3D, KIT,
and BOTH57M) demonstrate that our method achieves an improvement in average
performance (across BLEU-1, BLEU-4, CIDEr, and ROUGE-L) ranging from 6% to 50%
compared to the state-of-the-art M2T-Interpretable.

</details>


### [129] [Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number](https://arxiv.org/abs/2509.01486)
*Jingyuan Zhou,Hao Qian,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: PAFlow是一个基于结构药物设计的新模型，通过先验交互指导和可学习原子数预测器，解决了现有方法概率动态不稳定和分子大小与蛋白质口袋不匹配的问题，在结合亲和力方面达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的药物设计生成模型存在概率动态不稳定、生成分子大小与蛋白质口袋几何不匹配的问题，导致质量不一致和脱靶效应。

Method: 采用高效的流匹配框架建模生成过程，构建离散原子类型的条件流匹配形式，整合蛋白质-配体交互预测器指导向量场向高亲和力区域，设计基于蛋白质口袋信息的原子数预测器来对齐分子大小。

Result: 在CrossDocked2020基准测试中，PAFlow在结合亲和力方面达到新SOTA（最高-8.31平均Vina分数），同时保持良好分子性质。

Conclusion: PAFlow通过先验交互指导和原子数预测，有效解决了分子生成中的稳定性和尺寸匹配问题，在药物设计领域展现出优异性能。

Abstract: Structure-based drug design (SBDD), aiming to generate 3D molecules with high
binding affinity toward target proteins, is a vital approach in novel drug
discovery. Although recent generative models have shown great potential, they
suffer from unstable probability dynamics and mismatch between generated
molecule size and the protein pockets geometry, resulting in inconsistent
quality and off-target effects. We propose PAFlow, a novel target-aware
molecular generation model featuring prior interaction guidance and a learnable
atom number predictor. PAFlow adopts the efficient flow matching framework to
model the generation process and constructs a new form of conditional flow
matching for discrete atom types. A protein-ligand interaction predictor is
incorporated to guide the vector field toward higher-affinity regions during
generation, while an atom number predictor based on protein pocket information
is designed to better align generated molecule size with target geometry.
Extensive experiments on the CrossDocked2020 benchmark show that PAFlow
achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina
Score), simultaneously maintains favorable molecular properties.

</details>


### [130] [Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal](https://arxiv.org/abs/2509.01512)
*Zhangyue Shi,Zekai Wang,Yuxuan Li*

Main category: cs.LG

TL;DR: 提出了一种基于伪重放的半监督持续学习框架，用于解决ECG信号检测中的类别不平衡和数据存储问题，通过GAN生成伪数据来维持对历史模式的检测能力。


<details>
  <summary>Details</summary>
Motivation: ECG自动分析在临床实践中面临类别不平衡问题和历史数据存储负担，需要一种既能检测新异常模式又能保持对现有信号检测性能的方法。

Method: 采用无监督GAN框架检测新模式，并提出伪重放学习策略，利用生成器学习每个任务的数据分布，当新任务出现时生成代表先前学习类别的伪数据。

Result: 在四个公共ECG数据集上的实验验证了该框架的有效性，能够有效识别新异常同时保持良好的现有信号检测性能。

Conclusion: 所提出的伪重放半监督持续学习框架是解决ECG异常检测中类别不平衡和存储限制问题的有前景方法。

Abstract: In clinical practice, automatic analysis of electrocardiogram (ECG) is widely
applied to identify irregular heart rhythms and other electrical anomalies of
the heart, enabling timely intervention and potentially improving clinical
outcomes. However, due to the limited samples in certain types of ECG signals,
the class imbalance issues pose a challenge for ECG-based detection. In
addition, as the volume of patient data grows, long-term storage of all
historical data becomes increasingly burdensome as training samples to
recognize new patterns and classify existing ECG signals accurately. Therefore,
to enhance the performance of anomaly detection while addressing storage
limitations, we propose a pseudo-replay based semi-supervised continual
learning framework, which consists of two components: unsupervised
identification and replay-based detection. For unsupervised identification, an
unsupervised generative adversarial network (GAN)-based framework is integrated
to detect novel patterns. Besides, instead of directly storing all historical
data, a pseudo replay-based learning strategy is proposed which utilizes a
generator to learn the data distribution for each individual task. When a new
task arises, the generator synthesizes pseudo data representative of previous
learnt classes, enabling the model to detect both the existed patterns and the
newly presented anomalies. The effectiveness of the proposed framework is
validated in four public ECG datasets, which leverages supervised
classification problems for anomaly detection. The experimental results show
that the developed approach is very promising in identifying novel anomalies
while maintaining good performance on detecting existing ECG signals.

</details>


### [131] [Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health](https://arxiv.org/abs/2509.01526)
*Mingzhi Dai,Weiwei Cai,Xiang Feng,Huiqun Yu,Weibin Guo,Miao Guo*

Main category: cs.LG

TL;DR: 使用DE-BP神经网络预测活性污泥微生物组成，DPNG-EPMC聚类算法分析污水处理厂特征，SiTime-GAN生成合成数据，以理解活性污泥群落影响因素。


<details>
  <summary>Details</summary>
Motivation: 微生物组工程面临重大障碍，需要改进微生物组控制方法，特别是在污水处理等工程生态系统中。

Method: 采用差分进化优化的反向传播神经网络(DE-BP)预测微生物组成，提出DPNG-EPMC聚类算法分析特征属性，使用SiTime-GAN生成合成数据。

Result: DE-BP模型能提供优越的微生物组成预测，DPNG-EPMC可应用于不同特征属性下的污水处理厂分析，SiTime-GAN能生成有价值的合成数据。

Conclusion: 通过预测微生物群落和分析污水处理厂特征属性，开发了对活性污泥群落影响因素的理解。

Abstract: Microbiomes not only underpin Earth's biogeochemical cycles but also play
crucial roles in both engineered and natural ecosystems, such as the soil,
wastewater treatment, and the human gut. However, microbiome engineering faces
significant obstacles to surmount to deliver the desired improvements in
microbiome control. Here, we use the backpropagation neural network (BPNN),
optimized through differential evolution (DE-BP), to predict the microbial
composition of activated sludge (AS) systems collected from wastewater
treatment plants (WWTPs) located worldwide. Furthermore, we introduce a novel
clustering algorithm termed Directional Position Nonlinear Emotional Preference
Migration Behavior Clustering (DPNG-EPMC). This method is applied to conduct a
clustering analysis of WWTPs across various feature attributes. Finally, we
employ the Similar Time Generative Adversarial Networks (SiTime-GAN), to
synthesize novel microbial compositions and feature attributes data. As a
result, we demonstrate that the DE-BP model can provide superior predictions of
the microbial composition. Additionally, we show that the DPNG-EPMC can be
applied to the analysis of WWTPs under various feature attributes. Finally, we
demonstrate that the SiTime-GAN model can generate valuable incremental
synthetic data. Our results, obtained through predicting the microbial
community and conducting analysis of WWTPs under various feature attributes,
develop an understanding of the factors influencing AS communities.

</details>


### [132] [Forward-Only Continual Learning](https://arxiv.org/abs/2509.01533)
*Jiao Chen,Jiayi He,Fangfang Chen,Zuohong Lv,Jianhua Tang*

Main category: cs.LG

TL;DR: FoRo是一种无需梯度计算的前向持续学习方法，通过轻量级提示调优和知识编码机制，在预训练模型上实现高效持续学习，显著减少遗忘并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖反向传播和梯度优化，计算成本高且不适合资源受限环境。需要开发更高效的前向学习方法来解决灾难性遗忘问题。

Method: 结合CMA-ES进化策略优化提示嵌入，使用非线性随机投影和递归最小二乘法构建知识编码矩阵，实现无需修改预训练模型的增量学习。

Result: FoRo显著降低了平均遗忘率，提高了准确率，同时减少了内存使用和运行时间，在长任务序列中保持高知识保留率。

Conclusion: FoRo为预训练模型的持续学习提供了有前景的方向，特别适用于对效率和效果都有要求的实际多媒体应用场景。

Abstract: Catastrophic forgetting remains a central challenge in continual learning
(CL) with pre-trained models. While existing approaches typically freeze the
backbone and fine-tune a small number of parameters to mitigate forgetting,
they still rely on iterative error backpropagation and gradient-based
optimization, which can be computationally intensive and less suitable for
resource-constrained environments. To address this, we propose FoRo, a
forward-only, gradient-free continual learning method. FoRo consists of a
lightweight prompt tuning strategy and a novel knowledge encoding mechanism,
both designed without modifying the pre-trained model. Specifically, prompt
embeddings are inserted at the input layer and optimized using the Covariance
Matrix Adaptation Evolution Strategy (CMA-ES), which mitigates distribution
shifts and extracts high-quality task representations. Subsequently,
task-specific knowledge is encoded into a knowledge encoding matrix via
nonlinear random projection and recursive least squares, enabling incremental
updates to the classifier without revisiting prior data. Experiments show that
FoRo significantly reduces average forgetting and improves accuracy. Thanks to
forward-only learning, FoRo reduces memory usage and run time while maintaining
high knowledge retention across long task sequences. These results suggest that
FoRo could serve as a promising direction for exploring continual learning with
pre-trained models, especially in real-world multimedia applications where both
efficiency and effectiveness are critical.

</details>


### [133] [Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size](https://arxiv.org/abs/2509.01541)
*Smayan Khanna,Doruk Efe Gökmen,Risi Kondor,Vincenzo Vitelli*

Main category: cs.LG

TL;DR: 研究发现图对比学习(GCL)的优势严重依赖数据集大小和任务难度，在标准数据集上，未经训练的GNN、简单MLP甚至手工统计特征都能与GCL媲美或超越它。


<details>
  <summary>Details</summary>
Motivation: 质疑GCL是否真的优于未经训练的基线方法，探索GCL性能与数据集规模和任务复杂度的关系。

Method: 通过在标准数据集、大型分子数据集和合成数据集上比较GCL与未经训练的GNN、MLP和手工统计特征的性能表现。

Result: 在标准数据集上GCL无明显优势；在ogbg-molhiv分子数据集上，GCL在小规模时落后，超过数千个图后开始领先但最终达到性能平台；在合成数据集上GCL准确率随图数量对数增长，性能差距随任务复杂度变化。

Conclusion: 需要重视数据集规模在基准测试中的作用，并设计能够避免性能平台的GCL算法。

Abstract: Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-
supervised learning on graphs, with strong performance reported on standardized
datasets and growing applications ranging from genomics to drug discovery. We
ask a basic question: does GCL actually outperform untrained baselines? We find
that GCL's advantage depends strongly on dataset size and task difficulty. On
standard datasets, untrained Graph Neural Networks (GNNs), simple multilayer
perceptrons, and even handcrafted statistics can rival or exceed GCL. On the
large molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at small
scales but pulls ahead beyond a few thousand graphs, though this gain
eventually plateaus. On synthetic datasets, GCL accuracy approximately scales
with the logarithm of the number of graphs and its performance gap (compared
with untrained GNNs) varies with respect to task complexity. Moving forward, it
is crucial to identify the role of dataset size in benchmarks and applications,
as well as to design GCL algorithms that avoid performance plateaus.

</details>


### [134] [Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior](https://arxiv.org/abs/2509.01543)
*Konstantin Mark,Leonard Galustian,Maximilian P. -P. Kovar,Esther Heid*

Main category: cs.LG

TL;DR: 本文首次将Feynman-Kac引导方法扩展到条件流匹配(CFM)中，用于精确控制生成样本的特性，特别是在化学反应过渡态生成中解决手性正确性的挑战。


<details>
  <summary>Details</summary>
Motivation: 条件流匹配(CFM)虽然是一种快速高质量的生成建模方法，但在许多应用中需要精确控制生成样本的特性。虽然扩散模型已有多种引导方法，但这些方法尚未扩展到流匹配方法中。

Method: 将精确控制要求表述为用能量势倾斜输出，首次推导出CFM的Feynman-Kac引导方法。该方法在高维空间中生成倾斜分布，并应用于化学反应过渡态的手性正确生成。

Result: 在合成任务上评估了该方法，包括高维空间中倾斜分布的生成这一对引导方法特别具有挑战性的案例。成功解决了化学反应过渡态生成中手性正确性的先前未解决问题。

Conclusion: Feynman-Kac引导的CFM方法能够有效控制生成样本的精确特性，在化学反应过渡态生成等复杂任务中表现出色，为流匹配方法的精确控制提供了新途径。

Abstract: Conditional Flow Matching(CFM) represents a fast and high-quality approach to
generative modelling, but in many applications it is of interest to steer the
generated samples towards precise requirements. While steering approaches like
gradient-based guidance, sequential Monte Carlo steering or Feynman-Kac
steering are well established for diffusion models, they have not been extended
to flow matching approaches yet. In this work, we formulate this requirement as
tilting the output with an energy potential. We derive, for the first time,
Feynman-Kac steering for CFM. We evaluate our approach on a set of synthetic
tasks, including the generation of tilted distributions in a high-dimensional
space, which is a particularly challenging case for steering approaches. We
then demonstrate the impact of Feynman-Kac steered CFM on the previously
unsolved challenge of generated transition states of chemical reactions with
the correct chirality, where the reactants or products can have a different
handedness, leading to geometric constraints of the viable reaction pathways
connecting reactants and products. Code to reproduce this study is avaiable
open-source at https://github.com/heid-lab/fkflow.

</details>


### [135] [Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing](https://arxiv.org/abs/2509.01548)
*Zihao Wang,Enneng Yang,Lu Yin,Shiwei Liu,Li Shen*

Main category: cs.LG

TL;DR: MergeLock是一种主动保护机制，通过破坏模型参数使其无法被合并，防止未经授权的模型合并。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的微调模型公开可用，模型合并的安全性受到关注。未经授权的合并可能侵犯开发者权利并泄露敏感个人信息。现有方法主要检测合并模型是否源自特定源模型，但无法有效防止非法合并。

Method: 利用Transformer模型中注意力机制的固有对称性，随机采样两对可逆矩阵并应用于Query-Key和Value-Output分支，保持模型输出不变的同时使其远离其他微调模型的共享参数空间。

Result: 在视觉和语言任务的广泛实验中，MergeLock能够在大多数情况下使涉及受保护模型的合并模型性能下降超过95%。

Conclusion: MergeLock能有效防止未经授权的模型合并，且受保护的合并模型无法通过低成本恢复方法有效恢复，增强了对抗非法合并的鲁棒性。

Abstract: Model merging leverages multiple finetuned expert models to construct a
multi-task model with low cost, and is gaining increasing attention. However,
as a growing number of finetuned models become publicly available, concerns
about the safety of model merging have emerged. Unauthorized merging may
infringe on developers' rights and risk leaking sensitive personal information.
Most existing methods focus on detecting whether a merged model originates from
a specific source model, but fail to effectively prevent illegal merging. In
this paper, we propose MergeLock, an active protection mechanism that disrupts
model parameters to render them unmergeable, thereby directly preventing
unauthorized model merging. Specifically, leveraging the inherent symmetry of
the attention mechanism in Transformer-based models, we randomly sample two
pairs of invertible matrices and apply them to the Query-Key (QK) and
Value-Output (VO) branches. This transformation keeps the model's output
unchanged while pushing it away from the shared parameter space of other
finetuned models. Extensive experiments across both vision and language tasks
demonstrate that MergeLock can degrade the performance of merged models by over
95% when a protected model is involved in most cases, demonstrating its
effectiveness. Moreover, we further demonstrate that merged models protected by
MergeLock cannot be effectively recovered using low-cost restoration methods,
further enhancing robustness against unauthorized merging. The code is
available at https://github.com/hetailang/Merge-Lock.

</details>


### [136] [Direct Profit Estimation Using Uplift Modeling under Clustered Network Interference](https://arxiv.org/abs/2509.01558)
*Bram van den Akker*

Main category: cs.LG

TL;DR: 该论文提出了一个实用的方法，将干扰感知的AddIPW估计器作为可微分学习目标，用于推荐系统中的提升建模，直接优化经济收益指标，在存在干扰效应时显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的提升建模方法通常忽略干扰效应（一个物品的处理会影响其他物品的结果），这违反了SUTVA假设，导致在真实市场环境中制定次优策略。现有的干扰感知估计器尚未应用于提升建模文献，且基于这些估计器的策略优化方法尚未成熟。

Method: 使用AddIPW估计器作为可微分学习目标，适合基于梯度的优化。将该框架与成熟的响应转换技术结合，直接优化经济结果（如增量利润）。通过模拟实验验证方法有效性。

Result: 模拟实验表明，该方法在干扰效应增强时显著优于忽略干扰的方法。采用利润导向的提升策略在该框架下能够识别出最高影响力的干预措施。

Conclusion: 该方法为更有利可图的激励个性化提供了一条实用路径，通过干扰感知的提升建模直接优化经济指标，在存在干扰的真实推荐场景中表现优越。

Abstract: Uplift modeling is a key technique for promotion optimization in recommender
systems, but standard methods typically fail to account for interference, where
treating one item affects the outcomes of others. This violation of the Stable
Unit Treatment Value Assumption (SUTVA) leads to suboptimal policies in
real-world marketplaces. Recent developments in interference-aware estimators
such as Additive Inverse Propensity Weighting (AddIPW) have not found their way
into the uplift modeling literature yet, and optimising policies using these
estimators is not well-established. This paper proposes a practical methodology
to bridge this gap. We use the AddIPW estimator as a differentiable learning
objective suitable for gradient-based optimization. We demonstrate how this
framework can be integrated with proven response transformation techniques to
directly optimize for economic outcomes like incremental profit. Through
simulations, we show that our approach significantly outperforms
interference-naive methods, especially as interference effects grow.
Furthermore, we find that adapting profit-centric uplift strategies within our
framework can yield superior performance in identifying the highest-impact
interventions, offering a practical path toward more profitable incentive
personalization.

</details>


### [137] [Learning Longitudinal Stress Dynamics from Irregular Self-Reports via Time Embeddings](https://arxiv.org/abs/2509.01569)
*Louis Simon,Mohamed Chetouani*

Main category: cs.LG

TL;DR: 提出Ema2Vec时间嵌入方法，用于处理生态瞬时评估(EMA)中的不规则时间间隔数据，在纵向压力预测任务中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 移动和可穿戴传感技术结合生态自评问卷为人类行为纵向建模提供了机会，但不规则时间间隔和缺失数据给预测带来了挑战

Method: 引入Ema2Vec时间嵌入方法，专门设计用于处理不规则间隔的自评报告，捕捉时间依赖性

Result: Ema2Vec在纵向压力预测任务中表现优于依赖固定大小日窗口的标准基线方法，以及没有时间感知表示的纵向序列模型

Conclusion: 研究强调了在处理不规则采样纵向数据时，纳入时间嵌入表示的重要性

Abstract: The widespread adoption of mobile and wearable sensing technologies has
enabled continuous and personalized monitoring of affect, mood disorders, and
stress. When combined with ecological self-report questionnaires, these systems
offer a powerful opportunity to explore longitudinal modeling of human
behaviors. However, challenges arise from missing data and the irregular timing
of self-reports, which make challenging the prediction of human states and
behaviors. In this study, we investigate the use of time embeddings to capture
time dependencies within sequences of Ecological Momentary Assessments (EMA).
We introduce a novel time embedding method, Ema2Vec, designed to effectively
handle irregularly spaced self-reports, and evaluate it on a new task of
longitudinal stress prediction. Our method outperforms standard stress
prediction baselines that rely on fixed-size daily windows, as well as models
trained directly on longitudinal sequences without time-aware representations.
These findings emphasize the importance of incorporating time embeddings when
modeling irregularly sampled longitudinal data.

</details>


### [138] [One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption](https://arxiv.org/abs/2509.01587)
*Maciej Krzysztof Zuziak,Roberto Pellungrini,Salvatore Rinzivillo*

Main category: cs.LG

TL;DR: 本文提出OCFL算法，一种自动检测最佳聚类时机的聚类无关方法，通过计算客户端梯度余弦距离和温度测量来实现自动化聚类联邦学习。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端群体聚类问题，传统方法需要手动调整超参数且时机选择困难，需要自动化解决方案来提升个性化模型交付效率。

Method: 基于客户端梯度余弦距离和收敛温度测量的One-Shot聚类联邦学习算法，使用密度聚类方法区分不同数据分布训练的神经网络损失曲面。

Result: 在5个基准数据集上测试40多个任务，显示该方法性能良好，无需调整超参数即可自动执行CFL，密度聚类方法在梯度分析中表现出高效率。

Conclusion: OCFL算法有效解决了联邦学习中的自动化聚类时机检测问题，为个性化模型训练提供了实用解决方案，同时通过GradCAM局部解释性分析揭示了个性化与可解释性之间的关系。

Abstract: Federated Learning (FL) is a widespread and well-adopted paradigm of
decentralised learning that allows training one model from multiple sources
without the need to transfer data between participating clients directly. Since
its inception in 2015, it has been divided into numerous subfields that deal
with application-specific issues, such as data heterogeneity or resource
allocation. One such sub-field, Clustered Federated Learning (CFL), deals with
the problem of clustering the population of clients into separate cohorts to
deliver personalised models. Although a few remarkable works have been
published in this domain, the problem remains largely unexplored, as its basic
assumptions and settings differ slightly from those of standard FL. In this
work, we present One-Shot Clustered Federated Learning (OCFL), a
clustering-agnostic algorithm that can automatically detect the earliest
suitable moment for clustering. Our algorithm is based on computing the cosine
distance between the gradients of the clients and a temperature measure that
detects when the federated model starts to converge. We empirically evaluate
our methodology by testing various one-shot clustering algorithms for over
forty different tasks on five benchmark datasets. Our experiments showcase the
good performance of our approach when used to perform CFL in an automated
manner without the need to adjust hyperparameters. We also revisit the
practical feasibility of CFL algorithms based on the gradients of the clients,
providing firm evidence of the high efficiency of density-based clustering
methods when used to differentiate between the loss surfaces of neural networks
trained on different distributions. Moreover, by inspecting the feasibility of
local explanations generated with the help of GradCAM, we can provide more
insights into the relationship between personalisation and the explainability
of local predictions.

</details>


### [139] [Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction](https://arxiv.org/abs/2509.01613)
*Tianye Fang,Xuanshu Luo,Martin Werner*

Main category: cs.LG

TL;DR: 这篇论文提出了一种统一训练框架，通过熵质驱动课程学习和多任务学习来提高人类移动预测的准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在训练人类移动数据时效率低下，且仅预测下一个位置忽略了距离和方向等隐式因素，导致预测结果不佳。

Method: 采用熵质驱动课程学习策略（基于Lempel-Ziv压缩算法评估轨迹可预测性）和多任务训练框架（同时优化位置预测、运动距离和方向估计）。

Result: 在HuMob挑战赛中达到最高水平表现：GEO-BLEU指标0.354，DTW指标26.15，收敛速度提升2.92倍。

Conclusion: 该统一训练框架能够有效提升人类移动预测的准确性和效率，通过结合课程学习和多任务学习来学习更现实的移动模式。

Abstract: The increasing availability of big mobility data from ubiquitous portable
devices enables human mobility prediction through deep learning approaches.
However, the diverse complexity of human mobility data impedes model training,
leading to inefficient gradient updates and potential underfitting. Meanwhile,
exclusively predicting next locations neglects implicit determinants, including
distances and directions, thereby yielding suboptimal prediction results. This
paper presents a unified training framework that integrates entropy-driven
curriculum and multi-task learning to address these challenges. The proposed
entropy-driven curriculum learning strategy quantifies trajectory
predictability based on Lempel-Ziv compression and organizes training from
simple to complex for faster convergence and enhanced performance. The
multi-task training simultaneously optimizes the primary location prediction
alongside auxiliary estimation of movement distance and direction for learning
realistic mobility patterns, and improve prediction accuracy through
complementary supervision signals. Extensive experiments conducted in
accordance with the HuMob Challenge demonstrate that our approach achieves
state-of-the-art performance on GEO-BLEU (0.354) and DTW (26.15) metrics with
up to 2.92-fold convergence speed compared to training without curriculum
learning.

</details>


### [140] [Effects of Distributional Biases on Gradient-Based Causal Discovery in the Bivariate Categorical Case](https://arxiv.org/abs/2509.01621)
*Tim Schwabe,Moritz Lange,Laurenz Wiskott,Maribel Acosta*

Main category: cs.LG

TL;DR: 基于梯度的因果发现方法在数据存在分布偏差时容易受到影响，本文识别了两种偏差类型并提出了控制方法


<details>
  <summary>Details</summary>
Motivation: 梯度因果发现方法虽然高效可扩展，但对数据分布偏差敏感，需要研究这些偏差的影响和解决方案

Method: 使用狄利克雷先验的双变量分类设置，构建学习边际或条件分布的简单模型来检验偏差影响，并通过消除因果分解竞争来增强鲁棒性

Result: 证明了梯度方法对边际分布不对称性和边际分布偏移不对称性两种偏差敏感，但通过控制竞争可以使模型对这些偏差具有鲁棒性

Conclusion: 梯度因果发现方法需要关注数据分布偏差的影响，通过适当的设计可以增强其对分布偏差的鲁棒性

Abstract: Gradient-based causal discovery shows great potential for deducing causal
structure from data in an efficient and scalable way. Those approaches however
can be susceptible to distributional biases in the data they are trained on. We
identify two such biases: Marginal Distribution Asymmetry, where differences in
entropy skew causal learning toward certain factorizations, and Marginal
Distribution Shift Asymmetry, where repeated interventions cause faster shifts
in some variables than in others. For the bivariate categorical setup with
Dirichlet priors, we illustrate how these biases can occur even in controlled
synthetic data. To examine their impact on gradient-based methods, we employ
two simple models that derive causal factorizations by learning marginal or
conditional data distributions - a common strategy in gradient-based causal
discovery. We demonstrate how these models can be susceptible to both biases.
We additionally show how the biases can be controlled. An empirical evaluation
of two related, existing approaches indicates that eliminating competition
between possible causal factorizations can make models robust to the presented
biases.

</details>


### [141] [Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP](https://arxiv.org/abs/2509.01630)
*Bingheng Wang,Yichao Gao,Tianchen Sun,Lin Zhao*

Main category: cs.LG

TL;DR: L2C是一个通过元学习自适应调整ADMM-DDP超参数的框架，用于多智能体系统的分布式轨迹优化，能够适应不同任务和配置，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式轨迹优化方法ADMM-DDP需要大量调参来平衡局部任务性能和全局协调，参数耦合紧密且调优困难。

Method: 提出L2C框架，使用轻量级神经网络元学习超参数，通过分布式方式端到端微分ADMM-DDP流程，重用DDP组件加速元梯度计算，并截断迭代加速训练。

Result: 在合作空中运输任务中，L2C能够生成动态可行轨迹，安全重构四旋翼编队，适应不同团队规模和任务条件，梯度计算速度比现有方法快88%。

Conclusion: L2C提供了一个通用的元学习框架，能够自适应调整分布式优化超参数，显著提升多智能体系统的协调性能和计算效率。

Abstract: Distributed trajectory optimization via ADMM-DDP is a powerful approach for
coordinating multi-agent systems, but it requires extensive tuning of tightly
coupled hyperparameters that jointly govern local task performance and global
coordination. In this paper, we propose Learning to Coordinate (L2C), a general
framework that meta-learns these hyperparameters, modeled by lightweight
agent-wise neural networks, to adapt across diverse tasks and agent
configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in
a distributed manner. It also enables efficient meta-gradient computation by
reusing DDP components such as Riccati recursions and feedback gains. These
gradients correspond to the optimal solutions of distributed matrix-valued LQR
problems, coordinated across agents via an auxiliary ADMM framework that
becomes convex under mild assumptions. Training is further accelerated by
truncating iterations and meta-learning ADMM penalty parameters optimized for
rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a
challenging cooperative aerial transport task, L2C generates dynamically
feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures
quadrotor formations for safe 6-DoF load manipulation in tight spaces, and
adapts robustly to varying team sizes and task conditions, while achieving up
to $88\%$ faster gradient computation than state-of-the-art methods.

</details>


### [142] [Relative Trajectory Balance is equivalent to Trust-PCL](https://arxiv.org/abs/2509.01632)
*Tristan Deleu,Padideh Nouri,Yoshua Bengio,Doina Precup*

Main category: cs.LG

TL;DR: 本文建立了Relative Trajectory Balance (RTB) 与Trust-PCL之间的等价关系，将RTB置于KL正则化强化学习的理论框架中，并表明KL正则化RL方法可以达到与RTB相当的性能。


<details>
  <summary>Details</summary>
Motivation: 近期生成建模进展显示强化学习在微调中的重要性，特别是KL正则化方法对自回归和扩散模型都很有效。RTB作为GFlowNets中的目标函数被提出用于改进序列生成模型的微调，需要明确其与现有方法的关系。

Method: 基于先前连接GFlowNets和最大熵强化学习的工作，建立RTB与Trust-PCL（一种具有KL正则化的离策略RL方法）之间的等价关系，并通过重新分析RTB论文中的示例进行验证。

Result: 证明了RTB与Trust-PCL的等价性，将RTB置于KL正则化RL的更广泛理论框架中，并显示KL正则化RL方法能够达到与RTB相当的性能表现。

Conclusion: 该研究澄清了RTB与早期方法的关系，为序列生成模型的微调提供了替代视角，表明KL正则化强化学习方法可以作为RTB的有效替代方案。

Abstract: Recent progress in generative modeling has highlighted the importance of
Reinforcement Learning (RL) for fine-tuning, with KL-regularized methods in
particular proving to be highly effective for both autoregressive and diffusion
models. Complementing this line of work, the Relative Trajectory Balance (RTB)
objective was recently introduced in the context of Generative Flow Networks
(GFlowNets) to serve the same role of improving fine-tuning in sequential
generative models. Building on prior work linking GFlowNets and maximum-entropy
RL, we establish in this paper an equivalence between RTB and Trust-PCL, an
off-policy RL method with KL regularization. This equivalence situates RTB
within the broader theoretical landscape of KL-regularized RL, and clarifies
its relationship to earlier methods. Leveraging this insight, we revisit an
illustrative example from the RTB paper and show that KL-regularized RL methods
achieve comparable performance, offering an alternative perspective to what was
previously reported.

</details>


### [143] [REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization](https://arxiv.org/abs/2509.01642)
*Maximilian P. Oppelt,Andreas Foltyn,Nadine R. Lang-Richter,Bjoern M. Eskofier*

Main category: cs.LG

TL;DR: 该论文提出了一个新的多模态认知负荷检测数据集，通过结合n-back测试和真实游戏应用来评估模型在跨域场景中的泛化能力。研究发现多模态方法优于单模态，但跨域迁移性能仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 当前认知负荷检测模型在真实场景中缺乏泛化性，现有研究局限于单一任务和有限模态，需要评估模型在真实世界中的鲁棒性和迁移能力。

Method: 构建新的多模态数据集，扩展认知负荷检测基准，包含客观性能指标、NASA-TLX主观评分和任务设计。使用xLSTM、ConvNeXt和Transformer等先进模型在多模态和应用域上进行系统训练评估。

Result: 多模态方法 consistently优于单模态基线，不同模态和模型架构对应用子集的影响各异。模型在一个领域训练后迁移到新应用时性能下降，表明通用认知负荷估计仍面临挑战。

Conclusion: 研究为开发更通用的认知负荷检测系统提供了可靠基线和实用见解，推动了人机交互和自适应系统领域的研究和实践应用。

Abstract: Task load detection is essential for optimizing human performance across
diverse applications, yet current models often lack generalizability beyond
narrow experimental domains. While prior research has focused on individual
tasks and limited modalities, there remains a gap in evaluating model
robustness and transferability in real-world scenarios. This paper addresses
these limitations by introducing a new multimodal dataset that extends
established cognitive load detection benchmarks with a real-world gaming
application, using the $n$-back test as a scientific foundation. Task load
annotations are derived from objective performance, subjective NASA-TLX
ratings, and task-level design, enabling a comprehensive evaluation framework.
State-of-the-art end-to-end model, including xLSTM, ConvNeXt, and Transformer
architectures are systematically trained and evaluated on multiple modalities
and application domains to assess their predictive performance and cross-domain
generalization. Results demonstrate that multimodal approaches consistently
outperform unimodal baselines, with specific modalities and model architectures
showing varying impact depending on the application subset. Importantly, models
trained on one domain exhibit reduced performance when transferred to novel
applications, underscoring remaining challenges for universal cognitive load
estimation. These findings provide robust baselines and actionable insights for
developing more generalizable cognitive load detection systems, advancing both
research and practical implementation in human-computer interaction and
adaptive systems.

</details>


### [144] [Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling](https://arxiv.org/abs/2509.01649)
*Sachin Goyal,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 蒸馏预训练提升LLM测试时扩展性但损害上下文学习能力，特别是在归纳头方面，通过二元模型沙箱分析揭示了这一权衡背后的机制


<details>
  <summary>Details</summary>
Motivation: 探索蒸馏在大型语言模型预训练中对测试时扩展和上下文学习等现代LLM关键能力的影响

Method: 研究蒸馏预训练模型的表现，使用二元模型沙箱进行机制分析，对比蒸馏模型在测试时扩展和上下文学习方面的差异

Result: 蒸馏预训练显著改善测试时扩展性，但损害上下文学习能力（特别是归纳头功能），通过沙箱分析揭示了这一权衡的机制

Conclusion: 蒸馏预训练存在测试时扩展性与上下文学习的权衡，研究为实践者提供了预训练设计选择的重要见解

Abstract: In the past year, distillation has seen a renewed prominence in large
language model (LLM) pretraining, exemplified by the Llama-3.2 and Gemma model
families. While distillation has historically been shown to improve statistical
modeling, its effects on new paradigms that are key to modern LLMs, such as
test-time scaling and in-context learning, remain underexplored. In this work,
we make three main contributions. First, we show that pretraining with
distillation yields models that exhibit remarkably better test-time scaling.
Second, we observe that this benefit comes with a trade-off: distillation
impairs in-context learning capabilities, particularly the one modeled via
induction heads. Third, to demystify these findings, we study distilled
pretraining in a sandbox of a bigram model, which helps us isolate the common
principal factor behind our observations. Finally, using these insights, we
shed light on various design choices for pretraining that should help
practitioners going forward.

</details>


### [145] [Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2509.01679)
*Zhi-Feng Wei,Wenqian Chen,Panos Stinis*

Main category: cs.LG

TL;DR: 本文提出了一系列受Transformer启发的DeepONet变体，通过在分支咄干网络之间引入双向交叉条件化机制，在保持简单性咄效率的同时提高了偏微分方程求解的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决原有DeepONet框架中"香草"版本与修改版本的准确性-效率之间的争议，旨在在保持简单性咄效率的前提下实现更高的准确性。

Method: 受Transformer机制启发，在DeepONet的分支网络咄干网络之间引入双向交叉条件化：向分支网络注入查询点信息，向干网络注入输入函数信息，以实现动态依赖关系。

Result: 在四个PDE基准测试中（平流、扩散-反应、Burgers、KdV方程），每个情况都有一个变体能够达到或超越修改DeepONet的准确性，同时提高训练效率。最佳变体与方程的本质特性自然匹配。

Conclusion: 交叉条件化的有效性依赖于偏微分方程的特性咄物理机制，通过统计分析验证了方法的稳健性。

Abstract: Operator learning has emerged as a promising tool for accelerating the
solution of partial differential equations (PDEs). The Deep Operator Networks
(DeepONets) represent a pioneering framework in this area: the "vanilla"
DeepONet is valued for its simplicity and efficiency, while the modified
DeepONet achieves higher accuracy at the cost of increased training time. In
this work, we propose a series of Transformer-inspired DeepONet variants that
introduce bidirectional cross-conditioning between the branch and trunk
networks in DeepONet. Query-point information is injected into the branch
network and input-function information into the trunk network, enabling dynamic
dependencies while preserving the simplicity and efficiency of the "vanilla"
DeepONet in a non-intrusive manner. Experiments on four PDE benchmarks --
advection, diffusion-reaction, Burgers', and Korteweg-de Vries equations --
show that for each case, there exists a variant that matches or surpasses the
accuracy of the modified DeepONet while offering improved training efficiency.
Moreover, the best-performing variant for each equation aligns naturally with
the equation's underlying characteristics, suggesting that the effectiveness of
cross-conditioning depends on the characteristics of the equation and its
underlying physics. To ensure robustness, we validate the effectiveness of our
variants through a range of rigorous statistical analyses, among them the
Wilcoxon Two One-Sided Test, Glass's Delta, and Spearman's rank correlation.

</details>


### [146] [Reinforcement Learning for Machine Learning Engineering Agents](https://arxiv.org/abs/2509.01684)
*Sherry Yang,Joy He-Yueya,Percy Liang*

Main category: cs.LG

TL;DR: 通过强化学习训练较小模型，解决ML工程任务的动力系统超越了使用更大但静态模型的提示方式，性能提升22%


<details>
  <summary>Details</summary>
Motivation: 现有以大模型为基础的动力系统无法通过经验改进，而通过强化学习训练较小模型可以实现更好的性能

Method: 采用分布异步强化学习框架，提出时长感知梯度更新来处理变长度动作，以及环境仪器技术来提供部分奖励信号

Result: 在MLEBench上的12个Kaggle任务中，使用Qwen2.5-3B模型经RL训练后的系统比使用Claude-3.5-Sonnet的提示方式平均性能提升22%

Conclusion: 通过RL训练较小模型可以超越静态的大模型，关键在于解决变长度动作和部分奖励问题

Abstract: Existing agents for solving tasks such as ML engineering rely on prompting
powerful language models. As a result, these agents do not improve with more
experience. In this paper, we show that agents backed by weaker models that
improve via reinforcement learning (RL) can outperform agents backed by much
larger, but static models. We identify two major challenges with RL in this
setting. First, actions can take a variable amount of time (e.g., executing
code for different solutions), which leads to asynchronous policy gradient
updates that favor faster but suboptimal solutions. To tackle variable-duration
actions, we propose duration- aware gradient updates in a distributed
asynchronous RL framework to amplify high-cost but high-reward actions. Second,
using only test split performance as a reward provides limited feedback. A
program that is nearly correct is treated the same as one that fails entirely.
To address this, we propose environment instrumentation to offer partial
credit, distinguishing almost-correct programs from those that fail early
(e.g., during data loading). Environment instrumentation uses a separate static
language model to insert print statement to an existing program to log the
agent's experimental progress, from which partial credit can be extracted as
reward signals for learning. Our experimental results on MLEBench suggest that
performing gradient updates on a much smaller model (Qwen2.5-3B) trained with
RL outperforms prompting a much larger model (Claude-3.5-Sonnet) with agent
scaffolds, by an average of 22% across 12 Kaggle tasks.

</details>


### [147] [Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection](https://arxiv.org/abs/2509.01719)
*Sara Khan,Mehmed Yüksel,Frank Kirchner*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于异常检测的多模态结构，用于共享汽车系统中的磨损检测，通过集成IMU和麦克风供应实时损坏检测，达到了92%的ROC-AUC性能。


<details>
  <summary>Details</summary>
Motivation: 现有手动检测方法劳动密集且容易出错，图像基于方法在实时性能和底盘损坏检测方面效果差强，需要更有效的解决方案。

Method: 设计了多个多模态自动编码器基础的结构变体，将IMU和麦克风传感器集成到小型设备中并安装在风挡玻璃上，通过集成池化多模态模型进行异常检测。

Result: 集成池化多模态模型达到最高性能，ROC-AUC为92%，显著超越了单模态和现有最佳方法，证明了其在实际应用中的有效性。

Conclusion: 该方法能够实时检测汽车磨损，避免來自高资源密集传感器的需求，并可扩展到汽车安全和自动驾驶车碰撞检测等其他应用领域。

Abstract: Wear and tear detection in fleet and shared vehicle systems is a critical
challenge, particularly in rental and car-sharing services, where minor damage,
such as dents, scratches, and underbody impacts, often goes unnoticed or is
detected too late. Currently, manual inspection methods are the default
approach but are labour intensive and prone to human error. In contrast,
state-of-the-art image-based methods struggle with real-time performance and
are less effective at detecting underbody damage due to limited visual access
and poor spatial coverage. This work introduces a novel multi-modal
architecture based on anomaly detection to address these issues. Sensors such
as IMUs and microphones are integrated into a compact device mounted on the
vehicle's windshield. This approach supports real-time damage detection while
avoiding the need for highly resource-intensive sensors. We developed multiple
variants of multi-modal autoencoder-based architectures and evaluated them
against unimodal and state-of-the-art methods. Our ensemble pooling multi-modal
model achieved the highest performance, with a Receiver Operating
Characteristic-Area Under Curve (ROC-AUC) of 92%, demonstrating its
effectiveness in real-world applications. This approach can also be extended to
other applications, such as improving automotive safety - where it can
integrate with airbag systems for efficient deployment - and helping autonomous
vehicles by complementing other sensors in collision detection.

</details>


### [148] [Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control](https://arxiv.org/abs/2509.01720)
*Georgios Papoudakis,Thomas Coste,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.LG

TL;DR: SoLS是一种新颖的离策略强化学习算法，通过区分正负样本采用不同的更新策略，结合成功转换重放机制，在移动应用控制任务中显著提升了样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 基于基础模型的强化学习在多轮任务中存在稀疏奖励和策略梯度更新的限制，特别是负样本更新可能损害模型性能。

Method: 提出SoLS算法：对正样本采用直接策略更新，对负样本采用保守正则化更新；结合成功转换重放(STR)机制优先学习成功交互。

Result: 在AndroidWorld基准测试中显著优于现有方法（相对提升至少17%），计算资源需求大幅减少，推理速度比GPT-4o方法快5-60倍。

Conclusion: SoLS通过智能区分正负样本更新策略，有效解决了基础模型强化学习中的关键挑战，在移动应用导航任务中实现了高效性能提升。

Abstract: Reinforcement learning (RL) using foundation models for policy approximations
in multi-turn tasks remains challenging. We identify two main limitations
related to sparse reward settings and policy gradient updates, based on which
we formulate a key insight: updates from positive samples with high returns
typically do not require policy regularisation, whereas updates from negative
samples, reflecting undesirable behaviour, can harm model performance. This
paper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RL
algorithm evaluated on mobile app control tasks. SoLS improves sample
efficiency when fine-tuning foundation models for user interface navigation via
a modified off-policy actor-critic approach, applying direct policy updates for
positive samples and conservative, regularised updates for negative ones to
prevent model degradation. We augment SoLS with Successful Transition Replay
(STR), which prioritises learning from successful interactions, further
improving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark,
where it significantly outperforms existing methods (at least 17% relative
increase), including prompt-engineering and RL approaches, while requiring
substantially fewer computational resources than GPT-4o-based methods with
5-60x faster inference.

</details>


### [149] [Convolutional Monge Mapping between EEG Datasets to Support Independent Component Labeling](https://arxiv.org/abs/2509.01721)
*Austin Meek,Carlos H. Mendoza-Cardenas,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 提出CMMN方法的扩展版本，通过两种新方法计算源参考频谱，生成时空可分离滤波器，显著提升脑与非脑独立成分的分类性能


<details>
  <summary>Details</summary>
Motivation: EEG记录存在设备差异、噪声和伪影等问题，传统方法在处理不同通道数的数据集时存在限制，需要改进频谱归一化方法来提升机器学习模型的性能

Method: 扩展CMMN方法，提出两种计算源参考频谱的新方法：(1)通道平均和L1归一化的重心法；(2)寻找与目标对象频谱最接近的源对象的主体间映射法

Result: 生成的时空可分离滤波器可用于不同通道数的数据集映射，在独立成分分类任务中显著提高了脑与非脑成分的识别准确率

Conclusion: 该方法通过改进的频谱归一化技术有效减少了EEG记录中的设备差异影响，提升了自动化伪影去除的可靠性，对癫痫和精神病等神经病理学的诊断监测具有临床意义

Abstract: EEG recordings contain rich information about neural activity but are subject
to artifacts, noise, and superficial differences due to sensors, amplifiers,
and filtering. Independent component analysis and automatic labeling of
independent components (ICs) enable artifact removal in EEG pipelines.
Convolutional Monge Mapping Normalization (CMMN) is a recent tool used to
achieve spectral conformity of EEG signals, which was shown to improve deep
neural network approaches for sleep staging. Here we propose a novel extension
of the CMMN method with two alternative approaches to computing the source
reference spectrum the target signals are mapped to: (1) channel-averaged and
$l_1$-normalized barycenter, and (2) a subject-to-subject mapping that finds
the source subject with the closest spectrum to the target subject. Notably,
our extension yields space-time separable filters that can be used to map
between datasets with different numbers of EEG channels. We apply these filters
in an IC classification task, and show significant improvement in recognizing
brain versus non-brain ICs.
  Clinical relevance - EEG recordings are used in the diagnosis and monitoring
of multiple neuropathologies, including epilepsy and psychosis. While EEG
analysis can benefit from automating artifact removal through independent
component analysis and labeling, differences in recording equipment and context
(the presence of noise from electrical wiring and other devices) may impact the
performance of machine learning models, but these differences can be minimized
by appropriate spectral normalization through filtering.

</details>


### [150] [BM-CL: Bias Mitigation through the lens of Continual Learning](https://arxiv.org/abs/2509.01730)
*Lucas Mansilla,Rodrigo Echeveste,Camila Gonzalez,Diego H. Milone,Enzo Ferrante*

Main category: cs.LG

TL;DR: 提出了基于持续学习的偏差缓解框架BM-CL，通过将偏差缓解重新定义为持续学习问题，在改善弱势群体结果的同时保持对优势群体的性能。


<details>
  <summary>Details</summary>
Motivation: 传统偏差缓解技术存在"降级效应"，即在改善弱势群体结果时会损害优势群体的性能，需要找到平衡公平性和性能的方法。

Method: 借鉴Learning without Forgetting和Elastic Weight Consolidation等持续学习技术，将偏差缓解视为领域增量持续学习问题，模型需要适应变化的公平条件。

Result: 在合成和真实图像数据集上的实验表明，该框架能够有效缓解偏差，同时最小化原始知识的损失。

Conclusion: 该研究将公平性和持续学习领域相结合，为开发既公平又有效的机器学习系统提供了有前景的途径。

Abstract: Biases in machine learning pose significant challenges, particularly when
models amplify disparities that affect disadvantaged groups. Traditional bias
mitigation techniques often lead to a {\itshape leveling-down effect}, whereby
improving outcomes of disadvantaged groups comes at the expense of reduced
performance for advantaged groups. This study introduces Bias Mitigation
through Continual Learning (BM-CL), a novel framework that leverages the
principles of continual learning to address this trade-off. We postulate that
mitigating bias is conceptually similar to domain-incremental continual
learning, where the model must adjust to changing fairness conditions,
improving outcomes for disadvantaged groups without forgetting the knowledge
that benefits advantaged groups. Drawing inspiration from techniques such as
Learning without Forgetting and Elastic Weight Consolidation, we reinterpret
bias mitigation as a continual learning problem. This perspective allows models
to incrementally balance fairness objectives, enhancing outcomes for
disadvantaged groups while preserving performance for advantaged groups.
Experiments on synthetic and real-world image datasets, characterized by
diverse sources of bias, demonstrate that the proposed framework mitigates
biases while minimizing the loss of original knowledge. Our approach bridges
the fields of fairness and continual learning, offering a promising pathway for
developing machine learning systems that are both equitable and effective.

</details>


### [151] [Communication-Aware Knowledge Distillation for Federated LLM Fine-Tuning over Wireless Networks](https://arxiv.org/abs/2509.01750)
*Xinlu Zhang,Na Yan,Yang Su,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种通信效率更高的联邦大语言模型蓄粉方案，通过适配性Top-k选择、动态聚合和LoRA投影技术，在保持性能的同时减少了约50%的通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中传输大语言模型的导出值(logits)需要高带宽，对带宽有限的客户端极具挑战。需要一种更高效的通信方案来降低通信开销同时保持蓄粉效果。

Method: 首先提出适配性Top-k导出值选择机制，根据实时通信条件动态稀疏化导出值。然后设计了适配性导出值聚合方案，避免传统零填充方法引入的无意义输入。最后在蓄粉损失中混入了通过LoRA调整的隐藏层投影，提供更丰富的表征并进一步降低通信开销。

Result: 实验结果显示，该方案在基准方法中表现优异，同时有效减少了约50%的通信开销。

Conclusion: 该研究提出的通信效率优化方案成功解决了联邦大语言模型蓄粉中的高通信开销问题，为带宽有限环境下的联邦学习提供了可行的解决方案。

Abstract: Federated learning (FL) for large language models (LLMs) offers a
privacy-preserving scheme, enabling clients to collaboratively fine-tune
locally deployed LLMs or smaller language models (SLMs) without exchanging raw
data. While parameter-sharing methods in traditional FL models solves number of
technical challenges, they still incur high communication overhead and struggle
with adapting to heterogeneous model architectures. Federated distillation, a
framework for mutual knowledge transfer via shared logits, typically offers
lower communication overhead than parameter-sharing methods. However,
transmitting logits from LLMs remains challenging for bandwidth-limited clients
due to their high dimensionality. In this work, we focus on a federated LLM
distillation with efficient communication overhead. To achieve this, we first
propose an adaptive Top-k logit selection mechanism, dynamically sparsifying
logits according to real-time communication conditions. Then to tackle the
dimensional inconsistency introduced by the adaptive sparsification, we design
an adaptive logits aggregation scheme, effectively alleviating the artificial
and uninformative inputs introduced by conventional zero-padding methods.
Finally, to enhance the distillation effect, we incorporate LoRA-adapted
hidden-layer projection from LLM into the distillation loss, reducing the
communication overhead further while providing richer representation.
Experimental results demonstrate that our scheme achieves superior performance
compared to baseline methods while effectively reducing communication overhead
by approximately 50%.

</details>


### [152] [Toward a Unified Benchmark and Taxonomy of Stochastic Environments](https://arxiv.org/abs/2509.01793)
*Aryan Amit Barsainyan,Jing Yu Lim,Dianbo Liu*

Main category: cs.LG

TL;DR: STORI基准测试：引入多样随机效应和随机性分类法，用于在不确定性条件下系统评估强化学习方法


<details>
  <summary>Details</summary>
Motivation: 当前强化学习基准测试主要关注确定性或过度简化设置，缺乏对真实随机性和部分可观测性的系统评估，阻碍了RL方法在现实世界条件下的鲁棒性发展

Method: 提出STORI（STOchastic-ataRI）基准测试，包含多样随机效应；建立RL环境中随机性的分类法，提供统一的分析框架

Result: 创建了一个能够捕获真实随机性和部分可观测性挑战的基准测试平台，为系统评估RL方法在不同形式不确定性下的表现提供了工具

Conclusion: STORI基准测试和随机性分类法填补了现有研究的空白，为在更真实的不确定性条件下开发和评估强化学习方法提供了重要基础

Abstract: Reinforcement Learning (RL) agents have achieved strong results on benchmarks
such as Atari100k, yet they remain limited in robustness to real-world
conditions. Model-Based RL approaches that rely on learned World Models often
struggle in environments with true stochasticity and partial observability,
despite their theoretical grounding in POMDPs. Current benchmarks rarely
capture these challenges, focusing instead on deterministic or overly
simplified settings, and the lack of a clear taxonomy of stochasticity further
hampers systematic evaluation. To address this gap, we introduce STORI
(STOchastic-ataRI), a benchmark that incorporates diverse stochastic effects
and enables rigorous assessment of RL methods under varied forms of
uncertainty. In addition, we propose a taxonomy of stochasticity in RL
environments, providing a unified framework for analyzing and comparing
approaches.

</details>


### [153] [A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics](https://arxiv.org/abs/2509.01794)
*Trusting Inekwe,Emmanuel Agu,Winnie Mkandawire,Andres Colubri*

Main category: cs.LG

TL;DR: 提出了MBT-CB模型，一种基于BERT的多目标贝叶斯Transformer，用于从电子健康记录中联合预测心血管疾病生物标志物，在COVID-19疫情期间表现出色。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行严重影响了心血管疾病患者的医疗护理，导致关键生物标志物变化。现有方法未能同时处理多目标预测、生物标志物相互依赖关系、时间模式和预测不确定性。

Method: 使用多目标贝叶斯Transformer（MBT）框架，结合预训练的BERT模型，通过贝叶斯变分推断估计不确定性，嵌入机制捕捉时间关系，DeepMTR模型捕捉生物标志物间相互关系。

Result: 在3,390条心血管患者记录上评估，MBT-CB在MAE（0.00887）、RMSE（0.0135）和MSE（0.00027）指标上均优于其他基线模型，有效捕捉了数据不确定性、模型不确定性和时间动态。

Conclusion: MBT-CB模型在心血管生物标志物预测方面表现出卓越性能，具有在大流行期间改善临床决策支持的潜力。

Abstract: The COVID-19 pandemic disrupted healthcare systems worldwide,
disproportionately impacting individuals with chronic conditions such as
cardiovascular disease (CVD). These disruptions -- through delayed care and
behavioral changes, affected key CVD biomarkers, including LDL cholesterol
(LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of
these changes is crucial for predicting disease progression and guiding
preventive care. However, prior work has not addressed multi-target prediction
of CVD biomarker from Electronic Health Records (EHRs) using machine learning
(ML), while jointly capturing biomarker interdependencies, temporal patterns,
and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target
Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to
jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The
model leverages Bayesian Variational Inference to estimate uncertainties,
embeddings to capture temporal relationships and a DeepMTR model to capture
biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data
from 3,390 CVD patient records (304 unique patients) in Central Massachusetts
during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of
baselines including other BERT-based ML models, achieving an MAE of 0.00887,
RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model
uncertainty, patient biomarker inter-relationships, and temporal dynamics via
its attention and embedding mechanisms. MBT-CB's superior performance
highlights its potential to improve CVD biomarker prediction and support
clinical decision-making during pandemics.

</details>


### [154] [When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference](https://arxiv.org/abs/2509.01822)
*Wen Ye,Jinbo Liu,Defu Cao,Wei Yang,Yan Liu*

Main category: cs.LG

TL;DR: TSAIA Benchmark是首个评估LLMs作为时间序列AI助手的基准数据集，包含33个真实世界任务，涵盖约束感知预测、异常检测等复杂推理任务，评估发现当前模型在构建复杂时间序列分析工作流方面存在局限


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列分析中的应用潜力巨大，但它们在真实应用领域中处理时序数据的复杂推理能力尚未得到充分探索，需要建立严谨的基准数据集进行评估

Method: 通过调研20多篇学术论文确定了33个真实世界任务，设计了动态可扩展的问题生成器，采用任务特定的成功标准和定制化的推理质量指标，在统一评估协议下评估了8个最先进的LLMs

Result: 分析揭示了当前模型在组装复杂时间序列分析工作流方面的局限性，强调了领域特定适配专业化方法的必要性

Conclusion: TSAIA Benchmark为评估LLMs在时间序列分析中的能力提供了首个综合性基准，指出了当前模型的不足和未来发展方向，支持持续扩展新的数据集和任务类型

Abstract: The rapid advancement of Large Language Models (LLMs) has sparked growing
interest in their application to time series analysis tasks. However, their
ability to perform complex reasoning over temporal data in real-world
application domains remains underexplored. To move toward this goal, a first
step is to establish a rigorous benchmark dataset for evaluation. In this work,
we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as
time-series AI assistants. To ensure both scientific rigor and practical
relevance, we surveyed over 20 academic publications and identified 33
real-world task formulations. The benchmark encompasses a broad spectrum of
challenges, ranging from constraint-aware forecasting to anomaly detection with
threshold calibration: tasks that require compositional reasoning and
multi-step time series analysis. The question generator is designed to be
dynamic and extensible, supporting continuous expansion as new datasets or task
types are introduced. Given the heterogeneous nature of the tasks, we adopt
task-specific success criteria and tailored inference-quality metrics to ensure
meaningful evaluation for each task. We apply this benchmark to assess eight
state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals
limitations in current models' ability to assemble complex time series analysis
workflows, underscoring the need for specialized methodologies for
domain-specific adaptation. Our benchmark is available at
https://huggingface.co/datasets/Melady/TSAIA, and the code is available at
https://github.com/USC-Melady/TSAIA.

</details>


### [155] [Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation](https://arxiv.org/abs/2509.01838)
*Vaishnav Vaidheeswaran,Dilith Jayakody,Samruddhi Mulay,Anand Lo,Md Mahbub Alam,Gabriel Spadon*

Main category: cs.LG

TL;DR: 一种基于强化学习的船舶路由方案，通过多种奖励因素平衡燃料效率咄时间，适用于不同起终点对咄网格分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有船舶路由研究无法在多个起终点对之间推广，且没有充分利用大规模数据驱动的交通图。

Method: 使用近端策略优化(PPO)算法，结合循环网络、无效动作掩码咄探索策略，在连续观测环境中学习选择方向咄速度。

Result: 动作掩码技术显著提升了策略性能，并通过正向奖励补充获得进一步收益。

Conclusion: 该方法在圣勒拉斯湾成功实现了多目标优化的船舶路由，为大规模海上数据驱动的航行规划提供了有效解决方案。

Abstract: Routing vessels through narrow and dynamic waterways is challenging due to
changing environmental conditions and operational constraints. Existing
vessel-routing studies typically fail to generalize across multiple
origin-destination pairs and do not exploit large-scale, data-driven traffic
graphs. In this paper, we propose a reinforcement learning solution for big
maritime data that can learn to find a route across multiple origin-destination
pairs while adapting to different hexagonal grid resolutions. Agents learn to
select direction and speed under continuous observations in a multi-discrete
action space. A reward function balances fuel efficiency, travel time, wind
resistance, and route diversity, using an Automatic Identification System
(AIS)-derived traffic graph with ERA5 wind fields. The approach is demonstrated
in the Gulf of St. Lawrence, one of the largest estuaries in the world. We
evaluate configurations that combine Proximal Policy Optimization with
recurrent networks, invalid-action masking, and exploration strategies. Our
experiments demonstrate that action masking yields a clear improvement in
policy performance and that supplementing penalty-only feedback with positive
shaping rewards produces additional gains.

</details>


### [156] [Optimizing In-Context Learning for Efficient Full Conformal Prediction](https://arxiv.org/abs/2509.01840)
*Weicao Deng,Sangwoo Park,Min Li,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出E-ICL+FCP框架，通过基于Transformer的ICL模型模拟FCP的多重重新训练，在保持覆盖率的同时显著提高数据效率和计算效率


<details>
  <summary>Details</summary>
Motivation: 解决传统Conformal Prediction方法的数据效率低下和计算复杂度高的问题，Split CP需要数据分割导致效率低，Full CP计算成本过高

Method: 使用基于排列不变性Transformer的in-context learning模型，采用CP感知的损失函数进行训练，模拟FCP的多重重新训练过程

Result: 在合成和真实任务上实验表明，E-ICL+FCP相比现有SCP和FCP基线方法，获得了更优的效率-覆盖率权衡

Conclusion: E-ICL+FCP框架有效解决了传统CP方法的局限性，在保持统计保证的同时显著提升了数据利用效率和计算效率

Abstract: Reliable uncertainty quantification is critical for trustworthy AI. Conformal
Prediction (CP) provides prediction sets with distribution-free coverage
guarantees, but its two main variants face complementary limitations. Split CP
(SCP) suffers from data inefficiency due to dataset partitioning, while full CP
(FCP) improves data efficiency at the cost of prohibitive retraining
complexity. Recent approaches based on meta-learning or in-context learning
(ICL) partially mitigate these drawbacks. However, they rely on training
procedures not specifically tailored to CP, which may yield large prediction
sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP
(E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model
trained with a CP-aware loss. By simulating the multiple retrained models
required by FCP without actual retraining, E-ICL+FCP preserves coverage while
markedly reducing both inefficiency and computational overhead. Experiments on
synthetic and real tasks demonstrate that E-ICL+FCP attains superior
efficiency-coverage trade-offs compared to existing SCP and FCP baselines.

</details>


### [157] [GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping](https://arxiv.org/abs/2509.01842)
*Qifu Wen,Xi Zeng,Zihan Zhou,Shuaijun Liu,Mehdi Hosseinzadeh,Reza Rawassizadeh*

Main category: cs.LG

TL;DR: GradES是一种基于梯度的早期停止方法，通过监控Transformer组件中投影矩阵的梯度大小，在梯度低于阈值时单独冻结该矩阵的参数更新，避免了传统全局验证的昂贵计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统早期停止方法需要监控全局验证损失并同时停止所有参数更新，对于大型Transformer模型来说验证推理时间过长，计算成本高昂。

Method: 在微调过程中跟踪Transformer组件（注意力投影和前馈层矩阵）的反向传播梯度大小，当某个投影矩阵的梯度低于收敛阈值τ时，单独排除该矩阵的进一步更新，允许收敛慢的矩阵继续学习。

Result: GradES将训练时间加速1.57-7.22倍，同时通过早期防止过拟合提高了泛化能力，平均准确率提高1.2%。

Conclusion: GradES通过组件级别的梯度监控实现了高效的早期停止，在显著减少计算成本的同时提升了模型性能。

Abstract: Early stopping monitors global validation loss and halts all parameter
updates simultaneously, which is computationally costly for large transformers
due to the extended time required for validation inference. We propose GradES,
a novel gradient-based early stopping approach that operates within transformer
components (attention projections and Feed-Forward layer matrices). We found
that different components converge at varying rates during fine-tuning. GradES
tracks the magnitude of gradients in backpropagation for these matrices during
training. When a projection matrix's gradients fall below a convergence
threshold $\tau$, we exclude that projection matrix from further updates
individually, eliminating costly validation passes while allowing slow
converging matrices to continue learning. By strategically freezing parameters
when their gradients converge, GradES speeds up training time by
1.57--7.22$\times$ while simultaneously enhancing generalization through early
prevention of overfitting, resulting in 1.2% higher average accuracy.

</details>


### [158] [Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function](https://arxiv.org/abs/2509.01874)
*Jason Abohwo,Thomas Mosen*

Main category: cs.LG

TL;DR: 提出了Signed Quadratic Shrink (SQS)激活函数，使Gated Linear Units (GLUs)能够在不牺牲性能的情况下学习可解释的特征


<details>
  <summary>Details</summary>
Motivation: 现有基于权重的模型特征分析技术存在性能下降和数据效率低的问题，需要一种既能保持性能又能实现权重可解释性的方法

Method: 设计SQS激活函数，专门用于GLU结构，通过权重直接学习可解释特征

Result: SQS在性能上与最先进的激活函数相当，同时实现了基于权重的可解释性

Conclusion: SQS激活函数成功解决了现有技术的局限性，为神经网络提供了计算效率高且具有可解释保证的特征学习方法

Abstract: Understanding the inner workings of machine learning models is critical for
ensuring their reliability and robustness. Whilst many techniques in
mechanistic interpretability focus on activation driven analyses, being able to
derive meaningful features directly from the weights of a neural network would
provide greater guarantees and more computational efficiency. Existing
techniques for analyzing model features through weights suffer from drawbacks
such as reduced performance and data inefficiency. In this paper, we introduce
Signed Quadratic Shrink (SQS), an activation function designed to allow Gated
Linear Units (GLUs) to learn interpretable features without these drawbacks.
Our experimental results show that SQS achieves performance competitive with
state-of-the-art activation functions whilst enabling weight-based
interpretability

</details>


### [159] [Semi-on-Demand Transit Feeders with Shared Autonomous Vehicles and Reinforcement-Learning-Based Zonal Dispatching Control](https://arxiv.org/abs/2509.01883)
*Max T. M. Ng,Roman Engelhardt,Florian Dandl,Hani S. Mahmassani,Klaus Bogenberger*

Main category: cs.LG

TL;DR: 开发基于强化学习的半按需公交接驳服务，使用共享自动驾驶车辆和分区调度控制，结合固定路线和需求响应式运输的优势


<details>
  <summary>Details</summary>
Motivation: 解决低密度地区公共交通可达性问题，结合固定路线公交的成本效益和需求响应式运输的适应性

Method: 使用深度强化学习模型（PPO算法）动态分配车辆到灵活路线区域，基于实时需求波动和运营情况进行分区调度控制

Result: RL控制的服务比传统固定路线服务多服务16%乘客，成本高13%；RL控制带来2.4%额外乘客和1.4%额外成本

Conclusion: 展示了将SAV接驳车和机器学习技术整合到公共交通中的潜力，为解决多模式交通系统中的首末英里问题奠定了基础

Abstract: This paper develops a semi-on-demand transit feeder service using shared
autonomous vehicles (SAVs) and zonal dispatching control based on reinforcement
learning (RL). This service combines the cost-effectiveness of fixed-route
transit with the adaptability of demand-responsive transport to improve
accessibility in lower-density areas. Departing from the terminus, SAVs first
make scheduled fixed stops, then offer on-demand pick-ups and drop-offs in a
pre-determined flexible-route area. Our deep RL model dynamically assigns
vehicles to subdivided flexible-route zones in response to real-time demand
fluctuations and operations, using a policy gradient algorithm - Proximal
Policy Optimization. The methodology is demonstrated through agent-based
simulations on a real-world bus route in Munich, Germany. Results show that
after efficient training of the RL model, the semi-on-demand service with
dynamic zonal control serves 16% more passengers at 13% higher generalized
costs on average compared to traditional fixed-route service. The efficiency
gain brought by RL control brings 2.4% more passengers at 1.4% higher costs.
This study not only showcases the potential of integrating SAV feeders and
machine learning techniques into public transit, but also sets the groundwork
for further innovations in addressing first-mile-last-mile problems in
multimodal transit systems.

</details>


### [160] [Deep Reinforcement Learning for Real-Time Drone Routing in Post-Disaster Road Assessment Without Domain Knowledge](https://arxiv.org/abs/2509.01886)
*Huatian Gong,Jiuh-Biing Sheu,Zheng Wang,Xiaoguang Yang,Ran Yan*

Main category: cs.LG

TL;DR: 提出基于注意力的编码器-解码器模型（AEDM），用于灾后道路损坏评估中的实时无人机路径规划，结合深度强化学习和多任务学习，在求解质量和计算效率上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在灾后道路评估中存在计算时间长、需要领域知识设计算法的问题，无法满足时间敏感的灾害应急响应需求。

Method: 开发网络转换方法将基于链路的路径问题转化为等效节点形式，使用合成路网生成技术解决大规模训练数据稀缺问题，采用带多任务学习能力的POMO策略优化进行模型训练。

Result: AEDM在求解质量上比商业求解器提升16-69%，推理时间仅需1-2秒（传统方法需要100-2000秒），在不同问题规模、无人机数量和时间约束下都表现出强泛化能力。

Conclusion: 该方法有效平衡了计算效率和求解质量，特别适用于时间紧迫的灾害响应应用，为快速决策拯救生命提供了有效工具。

Abstract: Rapid post-disaster road damage assessment is critical for effective
emergency response, yet traditional optimization methods suffer from excessive
computational time and require domain knowledge for algorithm design, making
them unsuitable for time-sensitive disaster scenarios. This study proposes an
attention-based encoder-decoder model (AEDM) for real-time drone routing
decision in post-disaster road damage assessment. The method employs deep
reinforcement learning to determine high-quality drone assessment routes
without requiring algorithmic design knowledge. A network transformation method
is developed to convert link-based routing problems into equivalent node-based
formulations, while a synthetic road network generation technique addresses the
scarcity of large-scale training datasets. The model is trained using policy
optimization with multiple optima (POMO) with multi-task learning capabilities
to handle diverse parameter combinations. Experimental results demonstrate two
key strengths of AEDM: it outperforms commercial solvers by 16--69\% in
solution quality and achieves real-time inference (1--2 seconds) versus
100--2,000 seconds for traditional methods. The model exhibits strong
generalization across varying problem scales, drone numbers, and time
constraints, consistently outperforming baseline methods on unseen parameter
distributions and real-world road networks. The proposed method effectively
balances computational efficiency with solution quality, making it particularly
suitable for time-critical disaster response applications where rapid
decision-making is essential for saving lives.

</details>


### [161] [Predicting NCAP Safety Ratings: An Analysis of Vehicle Characteristics and ADAS Features Using Machine Learning](https://arxiv.org/abs/2509.01897)
*Raunak Kunwar,Aera Kim LeBoulluec*

Main category: cs.LG

TL;DR: 本研究使用机器学习分析NCAP安全评级数据，发现车辆基本特征（车重和车型年份）对预测5星安全评级最重要，但ADAS功能也有显著预测价值，随机森林模型达到89.18%准确率。


<details>
  <summary>Details</summary>
Motivation: 随着NCAP安全评级从传统被动安全扩展到包含ADAS主动安全技术，需要实证研究这些系统如何相互作用，以及它们与车辆基本属性共同预测最高安全评级的能力。

Method: 使用包含5,128辆2011-2025年车型的NCAP数据集，比较逻辑回归、随机森林、梯度提升和SVC四种机器学习模型，采用5折分层交叉验证，并对最佳模型进行超参数优化。

Result: 车辆基本特征（车重和车型年份）贡献了随机森林模型55%以上的特征重要性，但ADAS功能也有有意义的预测贡献。优化后的随机森林模型在测试集上达到89.18%准确率和0.9586 ROC AUC。

Conclusion: 机器学习能有效分析大规模NCAP数据，车辆基本参数和现代ADAS功能的组合对获得顶级安全评级具有重要预测价值，其中传统车辆特征仍占主导地位但ADAS技术也发挥重要作用。

Abstract: Vehicle safety assessment is crucial for consumer information and regulatory
oversight. The New Car Assessment Program (NCAP) assigns standardized safety
ratings, which traditionally emphasize passive safety measures but now include
active safety technologies such as Advanced Driver-Assistance Systems (ADAS).
It is crucial to understand how these various systems interact empirically.
This study explores whether particular ADAS features like Forward Collision
Warning, Lane Departure Warning, Crash Imminent Braking, and Blind Spot
Detection, together with established vehicle attributes (e.g., Curb Weight,
Model Year, Vehicle Type, Drive Train), can reliably predict a vehicle's
likelihood of earning the highest (5-star) overall NCAP rating. Using a
publicly available dataset derived from NCAP reports that contain approximately
5,128 vehicle variants spanning model years 2011-2025, we compared four
different machine learning models: logistic regression, random forest, gradient
boosting, and support vector classifier (SVC) using a 5-fold stratified
cross-validation approach. The two best-performing algorithms (random forest
and gradient boost) were hyperparameter optimized using RandomizedSearchCV.
Analysis of feature importance showed that basic vehicle characteristics,
specifically curb weight and model year, dominated predictive capability,
contributing more than 55% of the feature relevance of the Random Forest model.
However, the inclusion of ADAS features also provided meaningful predictive
contributions. The optimized Random Forest model achieved robust results on a
held-out test set, with an accuracy of 89.18% and a ROC AUC of 0.9586. This
research reveals the use of machine learning to analyze large-scale NCAP data
and highlights the combined predictive importance of both established vehicle
parameters and modern ADAS features to achieve top safety ratings.

</details>


### [162] [VISP: Volatility Informed Stochastic Projection for Adaptive Regularization](https://arxiv.org/abs/2509.01903)
*Tanvir Islam*

Main category: cs.LG

TL;DR: VISP是一种基于梯度波动性的自适应正则化方法，通过动态计算梯度统计量来调整随机噪声注入，选择性地正则化高波动性的输入和隐藏节点，从而提升深度神经网络的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统的正则化方法如均匀噪声注入或固定dropout率无法适应网络不同部分的动态特性，需要一种能够根据梯度波动性自适应调整正则化强度的机制。

Method: VISP通过计算梯度统计量的波动性，动态生成并缩放随机投影矩阵，选择性地对梯度波动性较高的输入和隐藏节点施加更强的正则化，同时保持稳定表示。

Result: 在MNIST、CIFAR-10和SVHN数据集上的实验表明，VISP相比基线模型和固定噪声方法能够持续提升泛化性能，同时稳定网络内部动态并促进更鲁棒的特征表示。

Conclusion: VISP通过利用梯度波动性信息进行自适应正则化，有效缓解过拟合问题，提高深度神经网络的泛化能力和鲁棒性。

Abstract: We propose VISP: Volatility Informed Stochastic Projection, an adaptive
regularization method that leverages gradient volatility to guide stochastic
noise injection in deep neural networks. Unlike conventional techniques that
apply uniform noise or fixed dropout rates, VISP dynamically computes
volatility from gradient statistics and uses it to scale a stochastic
projection matrix. This mechanism selectively regularizes inputs and hidden
nodes that exhibit higher gradient volatility while preserving stable
representations, thereby mitigating overfitting. Extensive experiments on
MNIST, CIFAR-10, and SVHN demonstrate that VISP consistently improves
generalization performance over baseline models and fixed-noise alternatives.
In addition, detailed analyses of the evolution of volatility, the spectral
properties of the projection matrix, and activation distributions reveal that
VISP not only stabilizes the internal dynamics of the network but also fosters
a more robust feature representation.

</details>


### [163] [Causal representation learning from network data](https://arxiv.org/abs/2509.01916)
*Jifan Zhang,Michelle M. Li,Elena Zheleva*

Main category: cs.LG

TL;DR: GraCE-VAE是一个用于非i.i.d.环境下因果解缠的框架，通过整合变分自编码器和图神经网络，利用网络数据中的结构化上下文来恢复潜在因果图和干预效应。


<details>
  <summary>Details</summary>
Motivation: 现有因果解缠研究主要针对i.i.d.数据，但在现实世界中存在大量非i.i.d.的结构化数据（如网络数据），需要开发能够利用这种结构化上下文的因果发现方法。

Method: GraCE-VAE结合了基于差异的变分自编码器(VAE)和图神经网络(GNN)，在存在网络形式结构化上下文的非i.i.d.设置下，联合恢复真实的潜在因果图和干预效应。

Result: 理论证明i.i.d.数据的可识别性结果在该框架中仍然成立，在三个基因扰动数据集上的实验表明，利用结构化上下文能显著提升因果解缠性能，优于现有最先进基线方法。

Conclusion: GraCE-VAE成功将因果解缠扩展到非i.i.d.设置，证明了利用结构化上下文信息的重要性，为处理现实世界中的复杂因果发现问题提供了有效解决方案。

Abstract: Causal disentanglement from soft interventions is identifiable under the
assumptions of linear interventional faithfulness and availability of both
observational and interventional data. Previous research has looked into this
problem from the perspective of i.i.d. data. Here, we develop a framework,
GraCE-VAE, for non-i.i.d. settings, in which structured context in the form of
network data is available. GraCE-VAE integrates discrepancy-based variational
autoencoders with graph neural networks to jointly recover the true latent
causal graph and intervention effects. We show that the theoretical results of
identifiability from i.i.d. data hold in our setup. We also empirically
evaluate GraCE-VAE against state-of-the-art baselines on three genetic
perturbation datasets to demonstrate the impact of leveraging structured
context for causal disentanglement.

</details>


### [164] [A Continuous Encoding-Based Representation for Efficient Multi-Fidelity Multi-Objective Neural Architecture Search](https://arxiv.org/abs/2509.01943)
*Zhao Wei,Chin Chun Ooi,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出了一种自适应协同克里金辅助的多保真度多目标神经架构搜索算法，通过聚类局部采样策略和连续编码方法，在有限计算预算下显著降低NAS成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索(NAS)虽然能自动优化架构设计，但计算成本高昂，特别是在优化多个冲突目标时。需要开发更高效的NAS方法来降低计算负担。

Method: 采用自适应协同克里金辅助的多保真度多目标NAS算法，结合聚类局部多保真度采样策略和新型连续编码方法，减少搜索维度并加速收敛。

Result: 在三个数值基准测试、2D Darcy流回归问题和CHASE_DB1生物医学图像分割问题上，该方法在有限计算预算下优于现有最先进方法。成功应用于城市建模中的风速回归模型，实现了良好预测且计算复杂度更低。

Conclusion: 该方法有效降低了NAS的计算成本，同时发现了U-Net架构设计的重要原则（如允许每个单元整合先前单元信息），证明了其在多目标优化中的高效性和实用性。

Abstract: Neural architecture search (NAS) is an attractive approach to automate the
design of optimized architectures but is constrained by high computational
budget, especially when optimizing for multiple, important conflicting
objectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity
multi-objective NAS algorithm is proposed to further reduce the computational
cost of NAS by incorporating a clustering-based local multi-fidelity infill
sampling strategy, enabling efficient exploration of the search space for
faster convergence. This algorithm is further accelerated by the use of a novel
continuous encoding method to represent the connections of nodes in each cell
within a generalized cell-based U-Net backbone, thereby decreasing the search
dimension (number of variables). Results indicate that the proposed NAS
algorithm outperforms previously published state-of-the-art methods under
limited computational budget on three numerical benchmarks, a 2D Darcy flow
regression problem and a CHASE_DB1 biomedical image segmentation problem. The
proposed method is subsequently used to create a wind velocity regression model
with application in urban modelling, with the found model able to achieve good
prediction with less computational complexity. Further analysis revealed that
the NAS algorithm independently identified principles undergirding superior
U-Net architectures in other literature, such as the importance of allowing
each cell to incorporate information from prior cells.

</details>


### [165] [Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems](https://arxiv.org/abs/2509.01972)
*Long Jiang,Yang Yang,Ting Fong May Chui,Morgan Thornwell,Hoshin Vijai Gupta*

Main category: cs.LG

TL;DR: 提出三阶段统一框架，将基于过程的模型与机器学习结合，通过知识蒸馏逐步嵌入人工智能，实现生态水文过程的智能建模


<details>
  <summary>Details</summary>
Motivation: 基于过程的模型具有物理真实性但计算成本高、结构僵化，机器学习方法高效灵活但缺乏可解释性和可迁移性，需要结合两者优势

Method: 三阶段框架：I阶段行为蒸馏-通过代理学习和模型简化增强过程模型；II阶段结构蒸馏-将过程方程重构为图神经网络模块；III阶段认知蒸馏-使用Eyes-Brain-Hands-Mouth架构嵌入专家推理

Result: 在Samish流域的演示表明，该框架能够重现基于过程的模型输出，提高预测准确性，并支持基于情景的决策制定

Conclusion: 该框架为下一代智能生态水文建模系统提供了可扩展和可迁移的路径，并具有扩展到其他基于过程领域的潜力

Abstract: Simulating ecohydrological processes is essential for understanding complex
environmental systems and guiding sustainable management amid accelerating
climate change and human pressures. Process-based models provide physical
realism but can suffer from structural rigidity, high computational costs, and
complex calibration, while machine learning (ML) methods are efficient and
flexible yet often lack interpretability and transferability. We propose a
unified three-phase framework that integrates process-based models with ML and
progressively embeds them into artificial intelligence (AI) through knowledge
distillation. Phase I, behavioral distillation, enhances process models via
surrogate learning and model simplification to capture key dynamics at lower
computational cost. Phase II, structural distillation, reformulates process
equations as modular components within a graph neural network (GNN), enabling
multiscale representation and seamless integration with ML models. Phase III,
cognitive distillation, embeds expert reasoning and adaptive decision-making
into intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture.
Demonstrations for the Samish watershed highlight the framework's applicability
to ecohydrological modeling, showing that it can reproduce process-based model
outputs, improve predictive accuracy, and support scenario-based
decision-making. The framework offers a scalable and transferable pathway
toward next-generation intelligent ecohydrological modeling systems, with the
potential extension to other process-based domains.

</details>


### [166] [Semantic and episodic memories in a predictive coding model of the neocortex](https://arxiv.org/abs/2509.01987)
*Lucie Fontaine,Frédéric Alexandre*

Main category: cs.LG

TL;DR: 预测编码模型显示新皮层在少量样本训练时具有情景记忆能力，但会过拟合且泛化能力差；大量样本训练时失去回忆能力，支持互补学习系统理论需要海马体的稀疏表征。


<details>
  <summary>Details</summary>
Motivation: 探讨新皮层的情景记忆能力及其与语义记忆的关系，挑战互补学习系统理论中关于情景记忆仅存在于海马体的观点。

Method: 使用生物可信的预测编码神经网络模型，测试其在少量和大量训练样本下的自联想记忆任务表现。

Result: 模型在少量样本训练时可以回忆具体示例但会过拟合，大量样本训练时失去回忆能力但泛化更好。

Conclusion: 新皮层可以通过密集重叠表征逐步编码有限数量的个体示例，但仍需要海马体的稀疏模式分离表征来处理大量情景记忆。

Abstract: Complementary Learning Systems theory holds that intelligent agents need two
learning systems. Semantic memory is encoded in the neocortex with dense,
overlapping representations and acquires structured knowledge. Episodic memory
is encoded in the hippocampus with sparse, pattern-separated representations
and quickly learns the specifics of individual experiences. Recently, this
duality between semantic and episodic memories has been challenged by
predictive coding, a biologically plausible neural network model of the
neocortex which was shown to have hippocampus-like abilities on
auto-associative memory tasks. These results raise the question of the episodic
capabilities of the neocortex and their relation to semantic memory. In this
paper, we present such a predictive coding model of the neocortex and explore
its episodic capabilities. We show that this kind of model can indeed recall
the specifics of individual examples but only if it is trained on a small
number of examples. The model is overfitted to these exemples and does not
generalize well, suggesting that episodic memory can arise from semantic
learning. Indeed, a model trained with many more examples loses its recall
capabilities. This work suggests that individual examples can be encoded
gradually in the neocortex using dense, overlapping representations but only in
a limited number, motivating the need for sparse, pattern-separated
representations as found in the hippocampus.

</details>


### [167] [ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting](https://arxiv.org/abs/2509.01997)
*Jiacheng Shi,Haibin Wei,Jiang Wang,Xiaowei Xu,Longzhi Du,Taixu Jiang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，在物流供需预测中较传统时空长序列方法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有时空分析方法在预测在线餐饮平台的未来订单分布时效果不佳，因为该问题存在强随机性且对时间序列不敏感。需要更有效的方法来捐捕这种信息并保持效率。

Method: 提出了一种创新的图学习网络框架（ACA-Net），使用适配性未来图学习和创新的交叉注意力机制。该方法仅需要两个图（进行中图和全局图）来学习未来订单分布信息。

Result: 在真实生产环境中验证了方法的有效性，与传统时空长序列方法相比获得了更优异的预测结果，显著提高了物流供需压力预测的性能。

Conclusion: 通过仅使用两个图的简洁方法，该模型能够有效捐捕具有强随机性的未来订单分布信息，为在线餐饮平台的物流调度决策提供了更准确的供需预测指标。

Abstract: Logistical demand-supply forecasting that evaluates the alignment between
projected supply and anticipated demand, is essential for the efficiency and
quality of on-demand food delivery platforms and serves as a key indicator for
scheduling decisions. Future order distribution information, which reflects the
distribution of orders in on-demand food delivery, is crucial for the
performance of logistical demand-supply forecasting. Current studies utilize
spatial-temporal analysis methods to model future order distribution
information from serious time slices. However, learning future order
distribution in online delivery platform is a time-series-insensitive problem
with strong randomness. These approaches often struggle to effectively capture
this information while remaining efficient. This paper proposes an innovative
spatiotemporal learning model that utilizes only two graphs (ongoing and
global) to learn future order distribution information, achieving superior
performance compared to traditional spatial-temporal long-series methods. The
main contributions are as follows: (1) The introduction of ongoing and global
graphs in logistical demand-supply pressure forecasting compared to traditional
long time series significantly enhances forecasting performance. (2) An
innovative graph learning network framework using adaptive future graph
learning and innovative cross attention mechanism (ACA-Net) is proposed to
extract future order distribution information, effectively learning a robust
future graph that substantially improves logistical demand-supply pressure
forecasting outcomes. (3) The effectiveness of the proposed method is validated
in real-world production environments.

</details>


### [168] [Bouncy particle sampler with infinite exchanging parallel tempering](https://arxiv.org/abs/2509.02003)
*Yohei Saito,Shun Kimura,Koujin Takeda*

Main category: cs.LG

TL;DR: 本文提出了一种结合Bouncy Particle Sampler (BPS)和Parallel Tempering (PT)的新采样算法，用于加速后验分布的收敛，特别针对多模态分布问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断需要近似后验分布，传统采样方法如HMC和MCMC在参数设置和收敛速度方面存在挑战。BPS虽然参数设置更简单，但在多模态分布中收敛较慢。

Method: 将并行回火(PT)技术引入BPS采样器，并提出当逆温度交换率设为无穷大时的算法变体，以加速后验分布的收敛。

Result: 通过数值模拟验证了该方法在多模态分布上的有效性，证明了其加速收敛的优势。

Conclusion: 提出的BPS-PT混合算法为贝叶斯推断提供了一种参数设置更简单、收敛更快的采样方法，特别适用于复杂的多模态后验分布。

Abstract: Bayesian inference is useful to obtain a predictive distribution with a small
generalization error. However, since posterior distributions are rarely
evaluated analytically, we employ the variational Bayesian inference or
sampling method to approximate posterior distributions. When we obtain samples
from a posterior distribution, Hamiltonian Monte Carlo (HMC) has been widely
used for the continuous variable part and Markov chain Monte Carlo (MCMC) for
the discrete variable part. Another sampling method, the bouncy particle
sampler (BPS), has been proposed, which combines uniform linear motion and
stochastic reflection to perform sampling. BPS was reported to have the
advantage of being easier to set simulation parameters than HMC. To accelerate
the convergence to a posterior distribution, we introduced parallel tempering
(PT) to BPS, and then proposed an algorithm when the inverse temperature
exchange rate is set to infinity. We performed numerical simulations and
demonstrated its effectiveness for multimodal distribution.

</details>


### [169] [Second-Order Tensorial Partial Differential Equations on Graphs](https://arxiv.org/abs/2509.02015)
*Aref Einizade,Fragkiskos D. Malliaros,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 提出了二阶张量图偏微分方程(So-TPDEGs)框架，解决了现有方法局限于一阶导数导致高频信号衰减和信息传播慢的问题，为连续乘积图神经网络提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有多图数据处理方法主要限于离散图滤波，连续方法仅限于一阶导数，这会抑制高频信号、减慢信息传播，难以捕捉复杂的多尺度和异配结构。

Method: 利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时自然保留高频信息，提出了二阶TPDEGs框架。

Result: 提供了图扰动下稳定性和谱特性方面过平滑行为的严格理论分析，建立了连续图学习的稳健理论基础。

Conclusion: 二阶TPDEGs框架为多领域连续图学习提供了理论支撑，能够更好地处理复杂多尺度和异配结构数据。

Abstract: Processing data that lies on multiple interacting (product) graphs is
increasingly important in practical applications, yet existing methods are
mostly restricted to discrete graph filtering. Tensorial partial differential
equations on graphs (TPDEGs) offer a principled framework for modeling such
multidomain data in a continuous setting. However, current continuous
approaches are limited to first-order derivatives, which tend to dampen
high-frequency signals and slow down information propagation. This makes these
TPDEGs-based approaches less effective for capturing complex, multi-scale, and
heterophilic structures. In this paper, we introduce second-order TPDEGs
(So-TPDEGs) and propose the first theoretically grounded framework for
second-order continuous product graph neural networks. Our approach leverages
the separability of cosine kernels in Cartesian product graphs to implement
efficient spectral decomposition, while naturally preserving high-frequency
information. We provide rigorous theoretical analyses of stability under graph
perturbations and over-smoothing behavior regarding spectral properties. Our
theoretical results establish a robust foundation for advancing continuous
graph learning across multiple practical domains.

</details>


### [170] [Genetic Programming with Model Driven Dimension Repair for Learning Interpretable Appointment Scheduling Rules](https://arxiv.org/abs/2509.02034)
*Huan Zhang,Yang Wang,Ya-Hui Jia,Yi Mei*

Main category: cs.LG

TL;DR: 提出了一种维度感知的遗传编程算法，通过维度修复程序来演化具有维度一致性的医疗预约规则，在保持结构的同时提升规则性能和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统遗传编程演化的医疗预约规则缺乏维度一致性，导致规则难以被医疗从业者理解和信任，需要一种能够保持维度一致性的演化方法

Method: 开发了维度感知的遗传编程算法，核心创新是维度修复程序，通过混合整数线性规划模型优化表达式树的维度一致性，最小化结构变化

Result: 在模拟诊所环境中，该方法演化的预约规则在客观值和维度一致性方面显著优于人工设计的规则和现有最先进的维度感知遗传编程方法

Conclusion: 该方法能够演化出高质量、高维度一致性的医疗预约规则，为设计更有效和可解释的预约规则提供了新的见解

Abstract: Appointment scheduling is a great challenge in healthcare operations
management. Appointment rules (AR) provide medical practitioners with a simple
yet effective tool to determine patient appointment times. Genetic programming
(GP) can be used to evolve ARs. However, directly applying GP to design ARs may
lead to rules that are difficult for end-users to interpret and trust. A key
reason is that GP is unaware of the dimensional consistency, which ensures that
the evolved rules align with users' domain knowledge and intuitive
understanding. In this paper, we develop a new dimensionally aware GP algorithm
with dimension repair to evolve ARs with dimensional consistency and high
performance. A key innovation of our method is the dimension repair procedure,
which optimizes the dimensional consistency of an expression tree while
minimizing structural changes and ensuring that its output dimension meets the
problem's requirements. We formulate the task as a mixed-integer linear
programming model that can be efficiently solved using common mathematical
programming methods. With the support of the dimension repair procedure, our
method can explore a wider range of AR structures by temporarily breaking the
dimensional consistency of individuals, and then restoring it without altering
their overall structure, thereby identifying individuals with greater potential
advantages. We evaluated the proposed method in a comprehensive set of
simulated clinics. The experimental results demonstrate that our approach
managed to evolve high-quality ARs that significantly outperform not only the
manually designed ARs but also existing state-of-the-art dimensionally aware GP
methods in terms of both objective values and dimensional consistency. In
addition, we analyzed the semantics of the evolved ARs, providing insight into
the design of more effective and interpretable ARs.

</details>


### [171] [Fantastic Pretraining Optimizers and Where to Find Them](https://arxiv.org/abs/2509.02046)
*Kaiyue Wen,David Hall,Tengyu Ma,Percy Liang*

Main category: cs.LG

TL;DR: 这篇论文通过系统性研究发现，迈向矩阵预条件器的优化器在小模型上有显著速度提升，但随模型规模增大速度优势减小，在1.2B参数模型上仅有1.1倍提升。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型预训练中AdamW优化器占主导地位，虽然有许多研究声称其他优化器能提供1.4-2倍速度提升，但实际采用度低。论文认为两个方法论问题导致了不公平比较：(i)不均等的超参数调整和(ii)有限或误导性的评估方案。

Method: 进行系统性研究，在四个模型规模（0.1B-1.2B参数）和四种数据-模型比例（1-8倍Chinchilla最优值）上测试10种深度学习优化器。重点关注严格的超参数调整和训练结束后的评估。

Result: 发现优化器之间的超参数不能盲目转移，否则会导致不公平比较。实际速度提升比声称的低，且随模型规模增大而减小，在1.2B模型上仅有1.1倍提升。训练过程中的中间检查点比较可能会误导，因为优化器排名可能在训练过程中发生变化。

Conclusion: 所有最快的优化器（如Muon和Soap）都使用矩阵作为预条件器，但矩阵基优化器的速度提升与模型规模成反比，从0.1B模型的1.4倍降到1.2B模型的1.1倍。这解释了为什么AdamW在大模型预训练中仍然占主导地位。

Abstract: AdamW has long been the dominant optimizer in language model pretraining,
despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We
posit that two methodological shortcomings have obscured fair comparisons and
hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited
or misleading evaluation setups. To address these two issues, we conduct a
systematic study of ten deep learning optimizers across four model scales
(0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum).
We find that fair and informative comparisons require rigorous hyperparameter
tuning and evaluations across a range of model scales and data-to-model ratios,
performed at the end of training. First, optimal hyperparameters for one
optimizer may be suboptimal for another, making blind hyperparameter transfer
unfair. Second, the actual speedup of many proposed optimizers over well-tuned
baselines is lower than claimed and decreases with model size to only 1.1x for
1.2B parameter models. Thirdly, comparing intermediate checkpoints before
reaching the target training budgets can be misleading, as rankings between two
optimizers can flip during training due to learning rate decay. Through our
thorough investigation, we find that all the fastest optimizers such as Muon
and Soap, use matrices as preconditioners -- multiplying gradients with
matrices rather than entry-wise scalars. However, the speedup of matrix-based
optimizers is inversely proportional to model scale, decreasing from 1.4x over
AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.

</details>


### [172] [Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation](https://arxiv.org/abs/2509.02048)
*Yi Yin,Guangquan Zhang,Hua Zuo,Jie Lu*

Main category: cs.LG

TL;DR: 提出了一种新颖的双层优化框架，通过上层任务关注数据效用、下层任务关注数据隐私，在数据扰动中实现隐私保护与数据效用的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型训练需要数据集，但直接共享原始数据存在成员推理攻击等隐私风险。现有隐私保护技术如数据扰动、泛化和合成数据生成往往会降低数据准确性、特异性和多样性，影响下游任务性能。

Method: 采用双层优化框架：上层任务通过判别器指导生成过程，确保扰动后的潜在变量映射到高质量样本；下层任务利用数据流形上的局部外在曲率作为个体对MIA脆弱性的量化指标，通过向低曲率区域扰动样本来抑制易受攻击的独特特征组合。

Result: 广泛的实验评估表明，该方法不仅增强了下游任务对MIA的抵抗能力，而且在样本质量和多样性方面超越了现有方法。

Conclusion: 通过交替优化两个目标，该方法实现了隐私保护和数据效用的协同平衡，为解决隐私保护与数据效用之间的权衡挑战提供了有效解决方案。

Abstract: Machine learning models require datasets for effective training, but directly
sharing raw data poses significant privacy risk such as membership inference
attacks (MIA). To mitigate the risk, privacy-preserving techniques such as data
perturbation, generalization, and synthetic data generation are commonly
utilized. However, these methods often degrade data accuracy, specificity, and
diversity, limiting the performance of downstream tasks and thus reducing data
utility. Therefore, striking an optimal balance between privacy preservation
and data utility remains a critical challenge.
  To address this issue, we introduce a novel bilevel optimization framework
for the publication of private datasets, where the upper-level task focuses on
data utility and the lower-level task focuses on data privacy. In the
upper-level task, a discriminator guides the generation process to ensure that
perturbed latent variables are mapped to high-quality samples, maintaining
fidelity for downstream tasks. In the lower-level task, our framework employs
local extrinsic curvature on the data manifold as a quantitative measure of
individual vulnerability to MIA, providing a geometric foundation for targeted
privacy protection. By perturbing samples toward low-curvature regions, our
method effectively suppresses distinctive feature combinations that are
vulnerable to MIA. Through alternating optimization of both objectives, we
achieve a synergistic balance between privacy and utility. Extensive
experimental evaluations demonstrate that our method not only enhances
resistance to MIA in downstream tasks but also surpasses existing methods in
terms of sample quality and diversity.

</details>


### [173] [LUCIE-3D: A three-dimensional climate emulator for forced responses](https://arxiv.org/abs/2509.02061)
*Haiwen Guan,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: LUCIE-3D是一个轻量级三维气候模拟器，基于SFNO架构，能够捕捉大气垂直结构，响应气候变化强迫，并在计算效率和长期稳定性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了开发一个能够模拟大气三维垂直结构、响应气候变化强迫、同时保持计算效率和长期稳定性的气候模拟工具，以支持快速实验和耦合气候动力学研究。

Method: 基于原始LUCIE-2D框架，采用球形傅里叶神经算子(SFNO)主干网络，使用30年ERA5再分析数据进行训练，涵盖8个垂直σ层，将大气CO2作为强迫变量，并可选择整合海表温度(SST)来模拟海气耦合动力学。

Result: LUCIE-3D成功再现了气候平均值、变异性和长期气候变化信号，包括CO2浓度增加下的地表变暖和平流层冷却。模型还捕捉了赤道开尔文波、马登-朱利安振荡和环状模等关键动力学过程，在极端事件统计方面表现出可信行为。

Conclusion: LUCIE-3D结合了稳定性、物理一致性和可访问性，是进行快速实验、消融研究和耦合气候动力学探索的宝贵工具，在古气候研究和未来地球系统模拟方面具有潜在应用价值。

Abstract: We introduce LUCIE-3D, a lightweight three-dimensional climate emulator
designed to capture the vertical structure of the atmosphere, respond to
climate change forcings, and maintain computational efficiency with long-term
stability. Building on the original LUCIE-2D framework, LUCIE-3D employs a
Spherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of
ERA5 reanalysis data spanning eight vertical {\sigma}-levels. The model
incorporates atmospheric CO2 as a forcing variable and optionally integrates
prescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere
dynamics. Results demonstrate that LUCIE-3D successfully reproduces
climatological means, variability, and long-term climate change signals,
including surface warming and stratospheric cooling under increasing CO2
concentrations. The model further captures key dynamical processes such as
equatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,
while showing credible behavior in the statistics of extreme events. Despite
requiring longer training than its 2D predecessor, LUCIE-3D remains efficient,
training in under five hours on four GPUs. Its combination of stability,
physical consistency, and accessibility makes it a valuable tool for rapid
experimentation, ablation studies, and the exploration of coupled climate
dynamics, with potential applications extending to paleoclimate research and
future Earth system emulation.

</details>


### [174] [Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling](https://arxiv.org/abs/2509.02069)
*Srinivas Anumasa,Barath Chandran. C,Tingting Chen,Dianbo Liu*

Main category: cs.LG

TL;DR: 提出了数据依赖平滑Walk-Jump框架，通过核密度估计为每个数据点估计噪声尺度σ，在蛋白质等稀疏高维数据生成中取得一致改进


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了数据依赖的变异性，蛋白质数据分布稀疏且不均匀，小部分样本位于密集簇中，大部分样本占据不同稀疏度的区域

Method: 使用核密度估计(KDE)作为预处理步骤为每个数据点估计噪声尺度σ，然后用这些数据依赖的σ值训练分数模型，将局部数据几何融入去噪过程

Result: 经验评估表明该方法在多个指标上产生一致的改进

Conclusion: 数据感知的sigma预测对于稀疏高维设置中的生成建模具有重要意义

Abstract: Diffusion models have emerged as a powerful class of generative models by
learning to iteratively reverse the noising process. Their ability to generate
high-quality samples has extended beyond high-dimensional image data to other
complex domains such as proteins, where data distributions are typically sparse
and unevenly spread. Importantly, the sparsity itself is uneven. Empirically,
we observed that while a small fraction of samples lie in dense clusters, the
majority occupy regions of varying sparsity across the data space. Existing
approaches largely ignore this data-dependent variability. In this work, we
introduce a Data-Dependent Smoothing Walk-Jump framework that employs kernel
density estimation (KDE) as a preprocessing step to estimate the noise scale
$\sigma$ for each data point, followed by training a score model with these
data-dependent $\sigma$ values. By incorporating local data geometry into the
denoising process, our method accounts for the heterogeneous distribution of
protein data. Empirical evaluations demonstrate that our approach yields
consistent improvements across multiple metrics, highlighting the importance of
data-aware sigma prediction for generative modeling in sparse, high-dimensional
settings.

</details>


### [175] [Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports](https://arxiv.org/abs/2509.02072)
*Jian Chen,Jinbao Tian,Yunqi Xu,Zhou Li*

Main category: cs.LG

TL;DR: ABEX-RAT是一个结合生成式数据增强和鲁棒对抗训练的新框架，用于解决职业事故报告分类中的类别不平衡问题，在OSHA数据集上达到了90.32%的macro-F1分数。


<details>
  <summary>Details</summary>
Motivation: 职业事故报告自动分类对提升工作场所安全和大规模风险分析至关重要，但现实数据集中严重的类别不平衡问题会损害分析模型性能，特别是对罕见但严重的事故类型。

Method: 提出ABEX-RAT框架：1）ABEX两步流程：使用大语言模型提取核心事故语义，再用生成模型为少数类创建高质量合成样本；2）RAT对抗训练：在增强数据上训练轻量级分类器，通过随机扰动提升模型泛化能力和鲁棒性。

Result: 在公开OSHA数据集上实现了新的最先进性能，macro-F1分数达到90.32%，显著优于之前的SOTA和微调大模型基线。

Conclusion: 这种协同策略是专门化、不平衡分类任务中强力微调的高效替代方案，验证了该方法的有效性和效率。

Abstract: The automatic classification of occupational accident reports is a critical
research area for enhancing workplace safety and enabling large-scale risk
analysis. However, the severe class imbalance inherent in these real-world
datasets often compromises the performance of analytical models, particularly
for rare but severe incident types, hindering the development of reliable
automated systems. To address this challenge, we propose ABEX-RAT, a novel and
efficient framework that synergizes generative data augmentation with robust
adversarial training. Our approach first employs a twostep
abstractive-expansive (ABEX) pipeline, which leverages a large language model
to distill core incident semantics and then uses a generative model to create
diverse, highquality synthetic samples for underrepresented classes.
Subsequently, a lightweight classifier is trained on the augmented data using a
computationally efficient random adversarial training (RAT) protocol, which
stochastically applies perturbations to enhance model generalization and
robustness without significant overhead. Experimental results on the public
OSHA dataset demonstrate that our method achieves new state-of-the-art
performance, reaching a macro-F1 score of 90.32% and significantly
outperforming previous SOTA and fine-tuned large model baselines. Our work
validates that this synergistic strategy is a highly effective and efficient
alternative to brute-force fine-tuning for specialized, imbalanced
classification tasks. The code is publicly available
at:https://github.com/nxcc-lab/ABEX-RAT.

</details>


### [176] [Towards Comprehensive Information-theoretic Multi-view Learning](https://arxiv.org/abs/2509.02084)
*Long Shi,Yunshan Ye,Wenjie Wang,Tao Lei,Yu Zhao,Gang Kou,Badong Chen*

Main category: cs.LG

TL;DR: CIML是一个基于信息理论的多视图学习框架，摒弃了传统的多视图冗余假设，同时利用共同信息和独特信息进行预测任务。


<details>
  <summary>Details</summary>
Motivation: 传统多视图学习方法过度依赖多视图冗余假设，认为视图间的共同信息对预测任务足够且必要，但忽略了每个视图独特信息的预测潜力。

Method: 提出CIML框架：1）共同表示学习通过最大化Gacs-Korner共同信息提取共享特征，并用信息瓶颈压缩；2）独特表示学习使用信息瓶颈压缩每个视图的独特信息，同时最小化独特与共同表示之间以及不同独特表示之间的互信息。

Result: 理论证明学习到的联合表示对下游任务具有预测充分性，大量实验结果显示模型优于多个最先进方法。

Conclusion: CIML框架有效利用了共同和独特信息，突破了传统多视图冗余假设的限制，在多视图学习中取得了优异性能。

Abstract: Information theory has inspired numerous advancements in multi-view learning.
Most multi-view methods incorporating information-theoretic principles rely an
assumption called multi-view redundancy which states that common information
between views is necessary and sufficient for down-stream tasks. This
assumption emphasizes the importance of common information for prediction, but
inherently ignores the potential of unique information in each view that could
be predictive to the task. In this paper, we propose a comprehensive
information-theoretic multi-view learning framework named CIML, which discards
the assumption of multi-view redundancy. Specifically, CIML considers the
potential predictive capabilities of both common and unique information based
on information theory. First, the common representation learning maximizes
Gacs-Korner common information to extract shared features and then compresses
this information to learn task-relevant representations based on the
Information Bottleneck (IB). For unique representation learning, IB is employed
to achieve the most compressed unique representation for each view while
simultaneously minimizing the mutual information between unique and common
representations, as well as among different unique representations.
Importantly, we theoretically prove that the learned joint representation is
predictively sufficient for the downstream task. Extensive experimental results
have demonstrated the superiority of our model over several state-of-art
methods. The code is released on CIML.

</details>


### [177] [DivMerge: A divergence-based model merging method for multi-tasking](https://arxiv.org/abs/2509.02108)
*Touayouch Brahim,Fosse Loïc,Damnati Géraldine,Lecorvé Gwénolé*

Main category: cs.LG

TL;DR: 提出了一种基于Jensen-Shannon散度的多任务模型融合方法，能够有效解决任务干扰问题，在无需额外标注数据的情况下保持多任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着微调模型的增多，传统多任务学习通过数据集合并的方法面临任务干扰问题，特别是任务数量增加时性能下降严重。

Method: 利用Jensen-Shannon散度指导模型融合过程，自动平衡任务重要性，无需额外标注数据。

Result: 方法在任务数量增加时保持鲁棒性， consistently outperforms prior work。

Conclusion: 该方法为多任务模型融合提供了有效的解决方案，能够处理大规模任务融合场景。

Abstract: Multi-task learning (MTL) is often achieved by merging datasets before
fine-tuning, but the growing availability of fine-tuned models has led to new
approaches such as model merging via task arithmetic. A major challenge in this
setting is task interference, which worsens as the number of tasks increases.
We propose a method that merges models trained on different tasks into a single
model, maintaining strong performance across all tasks. Our approach leverages
Jensen-Shannon divergence to guide the merging process without requiring
additional labelled data, and automatically balances task importance. Unlike
existing methods, our approach remains robust as the number of tasks grows and
consistently outperforms prior work.

</details>


### [178] [Differentiable Expectation-Maximisation and Applications to Gaussian Mixture Model Optimal Transport](https://arxiv.org/abs/2509.02109)
*Samuel Boïté,Eloi Tanguy,Julie Delon,Agnès Desolneux,Rémi Flamary*

Main category: cs.LG

TL;DR: 本文提出了EM算法的可微分版本，使其能够集成到需要端到端梯度传播的现代学习流程中，并应用于高斯混合模型的Wasserstein距离计算。


<details>
  <summary>Details</summary>
Motivation: 虽然EM算法在统计学和机器学习中广泛应用，但通常被视为不可微分的黑盒，无法在现代学习流程中进行端到端梯度传播。

Method: 提出并比较了多种EM算法微分策略，从完全自动微分到近似方法，评估其准确性和计算效率。将可微分EM应用于高斯混合模型的Wasserstein距离计算。

Result: 在重心计算、颜色和风格迁移、图像生成、纹理合成等任务上的数值实验证明了所提方法在不同设置下的多功能性和有效性。

Conclusion: 可微分EM算法为现代机器学习任务提供了重要的工具，特别是在需要端到端优化的应用中，如基于Wasserstein距离的损失函数计算。

Abstract: The Expectation-Maximisation (EM) algorithm is a central tool in statistics
and machine learning, widely used for latent-variable models such as Gaussian
Mixture Models (GMMs). Despite its ubiquity, EM is typically treated as a
non-differentiable black box, preventing its integration into modern learning
pipelines where end-to-end gradient propagation is essential. In this work, we
present and compare several differentiation strategies for EM, from full
automatic differentiation to approximate methods, assessing their accuracy and
computational efficiency. As a key application, we leverage this differentiable
EM in the computation of the Mixture Wasserstein distance $\mathrm{MW}_2$
between GMMs, allowing $\mathrm{MW}_2$ to be used as a differentiable loss in
imaging and machine learning tasks. To complement our practical use of
$\mathrm{MW}_2$, we contribute a novel stability result which provides
theoretical justification for the use of $\mathrm{MW}_2$ with EM, and also
introduce a novel unbalanced variant of $\mathrm{MW}_2$. Numerical experiments
on barycentre computation, colour and style transfer, image generation, and
texture synthesis illustrate the versatility and effectiveness of the proposed
approach in different settings.

</details>


### [179] [HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis](https://arxiv.org/abs/2509.02113)
*Han Chen,Hanchen Wang,Hongmei Chen,Ying Zhang,Lu Qin,Wenjie Zhang*

Main category: cs.LG

TL;DR: 提出了HiGraph数据集，这是最大的公开层次化图数据集，包含595K函数调用图和200M控制流图，用于恶意软件分析


<details>
  <summary>Details</summary>
Motivation: 现有方法将程序简化为单层图，无法建模高层功能交互与底层指令逻辑之间的语义关系，缺乏大规模层次化数据集

Method: 构建两层图表示：函数调用图（FCG）作为高层，控制流图（CFG）作为底层，保留结构语义

Result: 创建了包含595K FCG和200M CFG的大规模数据集，通过大规模分析揭示了良性软件和恶意软件的明显结构特性差异

Conclusion: HiGraph为社区建立了基础基准，数据集和工具已公开，有助于构建对代码混淆和恶意软件演化具有鲁棒性的检测器

Abstract: The advancement of graph-based malware analysis is critically limited by the
absence of large-scale datasets that capture the inherent hierarchical
structure of software. Existing methods often oversimplify programs into single
level graphs, failing to model the crucial semantic relationship between
high-level functional interactions and low-level instruction logic. To bridge
this gap, we introduce \dataset, the largest public hierarchical graph dataset
for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs)
nested within \textbf{595K} Function Call Graphs (FCGs). This two-level
representation preserves structural semantics essential for building robust
detectors resilient to code obfuscation and malware evolution. We demonstrate
HiGraph's utility through a large-scale analysis that reveals distinct
structural properties of benign and malicious software, establishing it as a
foundational benchmark for the community. The dataset and tools are publicly
available at https://higraph.org.

</details>


### [180] [Threshold-Based Optimal Arm Selection in Monotonic Bandits: Regret Lower Bounds and Algorithms](https://arxiv.org/abs/2509.02119)
*Chanakya Varude,Jay Chaudhary,Siddharth Kaushik,Prasanna Chaporkar*

Main category: cs.LG

TL;DR: 本文研究基于阈值的多臂老虎机问题，旨在选择与预设阈值τ相关的臂，而不是传统的最优臂选择。在臂均值单调结构下，分析多种变体（首个超过τ、第k个超过/低于τ、最接近τ的臂），推导出仅依赖于τ相邻臂的渐近遗憾下界，并提出经蒙特卡洛模拟验证的最优算法。


<details>
  <summary>Details</summary>
Motivation: 受通信网络（CQI分配）、临床给药、能源管理、推荐系统等应用场景驱动，需要基于阈值约束而非单纯最大化奖励的决策机制，扩展经典老虎机理论以适应实际需求。

Method: 在臂均值单调结构假设下，研究多种阈值相关臂选择变体，推导渐近遗憾下界（显示仅依赖于τ相邻臂），设计相应算法并通过蒙特卡洛模拟验证其最优性。

Result: 理论分析表明阈值老虎机问题的遗憾下界仅取决于阈值τ附近的臂，与远离阈值的臂无关；提出的算法在模拟中表现出最优性能。

Conclusion: 本文成功扩展了经典老虎机理论，为阈值约束下的决策问题提供了理论框架和实用算法，在多个应用领域具有重要价值。

Abstract: In multi-armed bandit problems, the typical goal is to identify the arm with
the highest reward. This paper explores a threshold-based bandit problem,
aiming to select an arm based on its relation to a prescribed threshold \(\tau
\). We study variants where the optimal arm is the first above \(\tau\), the
\(k^{th}\) arm above or below it, or the closest to it, under a monotonic
structure of arm means. We derive asymptotic regret lower bounds, showing
dependence only on arms adjacent to \(\tau\). Motivated by applications in
communication networks (CQI allocation), clinical dosing, energy management,
recommendation systems, and more. We propose algorithms with optimality
validated through Monte Carlo simulations. Our work extends classical bandit
theory with threshold constraints for efficient decision-making.

</details>


### [181] [Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time](https://arxiv.org/abs/2509.02129)
*Jintao Cheng,Weibin Li,Jiehao Luo,Xiaoyu Tang,Zhijian He,Jin Wu,Yao Zou,Wei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的零样本视觉地点识别框架，采用测试时缩放技术利用多模态大语言模型的视觉-语言对齐能力，实现了跨域性能的显著提升和计算效率的巨大收益。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉地点识别方法存在计算开销高、跨域转移性差等问题，需要一种无需微调、高效的零样本方案来解决这些挑战。

Method: 提出了一种基于测试时缩放(TTS)的零样本框架，利用多模态大语言模型的视觉-语言对齐能力，通过指导基方法进行直接相似性计分。采用结构化提示生成长度可控的JSON输出，消除了两阶段处理。结合不确定性自我一致性(UASC)实现实时适应。

Result: 实验结果显示，该方法在跨域视觉地点识别性能上获得显著提升，计算效率提升达210倍，具有优秀的通用性。

Conclusion: 该研究提出的零样本框架通过测试时缩放技术，有效解决了当前VPR方法在计算效率和跨域适应性方面的挑战，为视觉地点识别领域提供了一种高效、可扩展的新方案。

Abstract: Visual Place Recognition (VPR) has evolved from handcrafted descriptors to
deep learning approaches, yet significant challenges remain. Current
approaches, including Vision Foundation Models (VFMs) and Multimodal Large
Language Models (MLLMs), enhance semantic understanding but suffer from high
computational overhead and limited cross-domain transferability when
fine-tuned. To address these limitations, we propose a novel zero-shot
framework employing Test-Time Scaling (TTS) that leverages MLLMs'
vision-language alignment capabilities through Guidance-based methods for
direct similarity scoring. Our approach eliminates two-stage processing by
employing structured prompts that generate length-controllable JSON outputs.
The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables
real-time adaptation without additional training costs, achieving superior
generalization across diverse environments. Experimental results demonstrate
significant improvements in cross-domain VPR performance with up to 210$\times$
computational efficiency gains.

</details>


### [182] [Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation](https://arxiv.org/abs/2509.02154)
*Aymene Mohammed Bouayed,Samuel Deslauriers-Gauthier,Adrian Iaccovelli,David Naccache*

Main category: cs.LG

TL;DR: Conditional-$t^3$VAE通过为每个类别定义Student's t联合先验分布，解决了传统VAE在类别不平衡数据集上潜在空间分配不均的问题，显著提高了生成公平性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE和$t^3$VAE在类别不平衡数据集上，潜在空间分配与类别频率成正比，导致尾部类别表示不足，生成公平性降低。

Method: 提出Conditional-$t^3$VAE，为每个类别定义Student's t联合先验分布，使用γ-power散度推导的闭式目标函数进行优化，并推导等权重潜在混合Student's t分布用于类别平衡生成。

Result: 在SVHN-LT、CIFAR100-LT和CelebA数据集上，Conditional-$t^3$VAE始终获得比$t^3$VAE和高斯VAE基线更低的FID分数，特别是在严重类别不平衡情况下。在每类F1评估中也优于条件高斯VAE。

Conclusion: Conditional-$t^3$VAE在极端不平衡情况下显著提高了生成公平性和多样性，而高斯模型在轻度不平衡情况下仍具有竞争力。

Abstract: Variational Autoencoders (VAEs) with global priors mirror the training set's
class frequency in latent space, underrepresenting tail classes and reducing
generative fairness on imbalanced datasets. While $t^3$VAE improves robustness
via heavy-tailed Student's t-distribution priors, it still allocates latent
volume proportionally to the class frequency.In this work, we address this
issue by explicitly enforcing equitable latent space allocation across classes.
To this end, we propose Conditional-$t^3$VAE, which defines a per-class
\mbox{Student's t} joint prior over latent and output variables, preventing
dominance by majority classes. Our model is optimized using a closed-form
objective derived from the $\gamma$-power divergence. Moreover, for
class-balanced generation, we derive an equal-weight latent mixture of
Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,
Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE
and Gaussian-based VAE baselines, particularly under severe class imbalance. In
per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional
Gaussian VAE across all highly imbalanced settings. While Gaussian-based models
remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach
substantially improves generative fairness and diversity in more extreme
regimes.

</details>


### [183] [Simulating classification models to evaluate Predict-Then-Optimize methods](https://arxiv.org/abs/2509.02191)
*Pieter Smet*

Main category: cs.LG

TL;DR: 本文研究了预测-优化方法中预测误差对优化解质量的影响，通过模拟多分类器预测来实验分析预测误差与最优解接近程度的关系，发现这种关系并非简单线性。


<details>
  <summary>Details</summary>
Motivation: 预测-优化方法假设更准确的预测会产生更接近实际最优解的方案，但在复杂约束优化问题中验证这一假设具有挑战性，文献中往往被忽视。

Method: 提出了模拟多分类器预测的新算法，补充了现有的二分类模拟算法，通过计算实验评估算法性能，并将其应用于机器调度问题的预测-优化算法评估。

Result: 分类器性能可以以合理精度进行模拟，但存在一定变异性。实验表明预测误差与解接近最优程度的关系是非平凡的。

Conclusion: 研究结果强调了基于机器学习预测的决策系统设计和评估的重要考虑因素，预测准确性并不总是直接转化为更好的优化解质量。

Abstract: Uncertainty in optimization is often represented as stochastic parameters in
the optimization model. In Predict-Then-Optimize approaches, predictions of a
machine learning model are used as values for such parameters, effectively
transforming the stochastic optimization problem into a deterministic one. This
two-stage framework is built on the assumption that more accurate predictions
result in solutions that are closer to the actual optimal solution. However,
providing evidence for this assumption in the context of complex, constrained
optimization problems is challenging and often overlooked in the literature.
Simulating predictions of machine learning models offers a way to
(experimentally) analyze how prediction error impacts solution quality without
the need to train real models. Complementing an algorithm from the literature
for simulating binary classification, we introduce a new algorithm for
simulating predictions of multiclass classifiers. We conduct a computational
study to evaluate the performance of these algorithms, and show that classifier
performance can be simulated with reasonable accuracy, although some
variability is observed. Additionally, we apply these algorithms to assess the
performance of a Predict-Then-Optimize algorithm for a machine scheduling
problem. The experiments demonstrate that the relationship between prediction
error and how close solutions are to the actual optimum is non-trivial,
highlighting important considerations for the design and evaluation of
decision-making systems based on machine learning predictions.

</details>


### [184] [Baichuan-M2: Scaling Medical Capability with Large Verifier System](https://arxiv.org/abs/2509.02208)
*Baichuan-M2 Team,:,Chengfeng Dou,Chong Liu,Fan Yang,Fei Li,Jiyuan Jia,Mingyang Chen,Qiang Ju,Shuai Wang,Shunya Dang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Chenzheng Zhu,Da Pan,Fei Deng,Guangwei Ai,Guosheng Dong,Hongda Zhang,Jinyang Tai,Jixiang Hong,Kai Lu,Linzhuang Sun,Peidong Guo,Qian Ma,Rihui Xin,Shihui Yang,Shusen Zhang,Yichuan Mo,Zheng Liang,Zhishou Zhang,Hengfu Cui,Zuyi Zhu,Xiaochuan Wang*

Main category: cs.LG

TL;DR: 本文提出了一个动态验证框架和Baichuan-M2医疗增强推理模型，通过交互式强化学习系统解决医疗LLM在静态基准测试与实际临床决策之间的性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域的实际应用存在显著差距，静态基准测试无法捕捉医疗咨询的动态交互特性，需要开发更贴近真实临床环境的评估框架。

Method: 建立动态验证框架，包含患者模拟器和临床评分生成器；开发32B参数的Baichuan-M2模型，采用多阶段强化学习策略和改进的GRPO算法进行训练。

Result: 在HealthBench基准测试中，Baichuan-M2超越所有开源模型和大多数闭源模型，在HealthBench Hard上获得32分以上，此前只有GPT-5达到这一水平。

Conclusion: 动态验证系统对于将LLM能力与实际临床应用对齐至关重要，在医疗AI部署的性能-参数权衡中建立了新的帕累托前沿。

Abstract: As large language models (LLMs) advance in conversational and reasoning
capabilities, their practical application in healthcare has become a critical
research focus. However, there is a notable gap between the performance of
medical LLMs on static benchmarks such as USMLE and their utility in real-world
clinical decision-making. This discrepancy arises because traditional exams
fail to capture the dynamic, interactive nature of medical consultations. To
address this challenge, we introduce a novel dynamic verification framework
that moves beyond static answer verifier, establishing a large-scale,
high-fidelity interactive reinforcement learning system. Our framework
comprises two key components: a Patient Simulator that creates realistic
clinical environments using de-identified medical records, and a Clinical
Rubrics Generator that dynamically produces multi-dimensional evaluation
metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter
medical augmented reasoning model trained through a multi-stage reinforcement
learning strategy with an improved Group Relative Policy Optimization (GRPO)
algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other
open-source models and most advanced closed-source counterparts, achieving a
score above 32 on the challenging HealthBench Hard benchmark-previously
exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier
system is essential for aligning LLM capabilities with practical clinical
applications, establishing a new Pareto front in the performance-parameter
trade-off for medical AI deployment.

</details>


### [185] [ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.02217)
*Binqing Wu,Jianlong Huang,Zongjiang Shang,Ling Chen*

Main category: cs.LG

TL;DR: ST-Hyper是一个用于多元时间序列预测的深度学习方法，通过自适应超图建模来捕捉多尺度时空依赖关系，在长短期预测中均取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多变量时间序列预测方法可能无法有效建模跨多个时空尺度（同时考虑空间和时间范围）的依赖关系，需要一种能够捕捉高阶跨尺度依赖的新方法。

Method: 提出ST-Hyper模型，包含时空金字塔建模模块提取多尺度特征，自适应超图建模模块学习稀疏超图来捕捉鲁棒的高阶依赖关系，并通过三相超图传播进行特征交互。

Result: 在6个真实世界多元时间序列数据集上的实验表明，ST-Hyper在长短期预测中均优于现有最佳基线方法，MAE平均分别降低3.8%和6.8%。

Conclusion: ST-Hyper通过自适应超图建模有效捕捉了多尺度时空动态依赖，在多元时间序列预测任务中表现出优越性能，为解决跨尺度依赖建模问题提供了有效解决方案。

Abstract: In multivariate time series (MTS) forecasting, many deep learning based
methods have been proposed for modeling dependencies at multiple spatial
(inter-variate) or temporal (intra-variate) scales. However, existing methods
may fail to model dependencies across multiple spatial-temporal scales
(ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In
this work, we propose ST-Hyper to model the high-order dependencies across
multiple ST-scales through adaptive hypergraph modeling. Specifically, we
introduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features
at multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph
Modeling (AHM) module that learns a sparse hypergraph to capture robust
high-order dependencies among features. In addition, we interact with these
features through tri-phase hypergraph propagation, which can comprehensively
capture multi-scale spatial-temporal dynamics. Experimental results on six
real-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art
performance, outperforming the best baselines with an average MAE reduction of
3.8\% and 6.8\% for long-term and short-term forecasting, respectively.

</details>


### [186] [VariAntNet: Learning Decentralized Control of Multi-Agent Systems](https://arxiv.org/abs/2509.02271)
*Yigal Koifman,Erez Koifman,Eran Iceland,Ariel Barel,Alfred M. Bruckstein*

Main category: cs.LG

TL;DR: VariAntNet是一个基于深度学习的分散式控制模型，用于简单多智能体系统的聚集任务，通过几何特征提取和神经网络架构，在保持群体凝聚力的同时显著提高收敛速度


<details>
  <summary>Details</summary>
Motivation: 解决简单机器人群体（如消防应用中的蚂蚁机器人）在有限传感和无通信条件下的群体凝聚问题，特别是在时间紧急的灾难响应场景中需要快速收敛

Method: 提出VariAntNet深度学习模型，包括从无序局部观测中提取几何特征，使用基于可见性图拉普拉斯矩阵特性的多目标损失函数来训练神经网络

Result: 在基本多智能体聚集任务中，VariAntNet显著优于现有解析方法，收敛率提高两倍以上，同时保持高群体连通性

Conclusion: 在时间关键的应急响应场景中，虽然解析方法能保证凝聚力但速度太慢，VariAntNet在收敛速度和群体凝聚力之间提供了更好的权衡

Abstract: A simple multi-agent system can be effectively utilized in disaster response
applications, such as firefighting. Such a swarm is required to operate in
complex environments with limited local sensing and no reliable inter-agent
communication or centralized control. These simple robotic agents, also known
as Ant Robots, are defined as anonymous agents that possess limited sensing
capabilities, lack a shared coordinate system, and do not communicate
explicitly with one another. A key challenge for simple swarms lies in
maintaining cohesion and avoiding fragmentation despite limited-range sensing.
Recent advances in machine learning offer effective solutions to some of the
classical decentralized control challenges. We propose VariAntNet, a deep
learning-based decentralized control model designed to facilitate agent
swarming and collaborative task execution. VariAntNet includes geometric
features extraction from unordered, variable-sized local observations. It
incorporates a neural network architecture trained with a novel,
differentiable, multi-objective, mathematically justified loss function that
promotes swarm cohesiveness by utilizing the properties of the visibility graph
Laplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent
gathering task, where agents with bearing-only and limited-range sensing must
gather at some location. VariAntNet significantly outperforms an existing
analytical solution, achieving more than double the convergence rate while
maintaining high swarm connectivity across varying swarm sizes. While the
analytical solution guarantees cohesion, it is often too slow in practice. In
time-critical scenarios, such as emergency response operations where lives are
at risk, slower analytical methods are impractical and justify the loss of some
agents within the swarm. This paper presents and analyzes this trade-off in
detail.

</details>


### [187] [Calibration through the Lens of Indistinguishability](https://arxiv.org/abs/2509.02279)
*Parikshit Gopalan,Lunjia Hu*

Main category: cs.LG

TL;DR: 这篇综述论文探讨了校准在概率预测中的基础问题，包括如何定义和测量校准误差，以及这些测量对下游决策者的意义。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中概率预测的普及，需要解决如何解释预测概率以及如何评估连续概率预测器在离散结果观察世界中的表现这一基本问题。

Method: 论文采用综述研究方法，将校准视为预测器假设的世界与真实世界（由自然或贝叶斯最优预测器控制）之间的一种不可区分性形式。

Result: 提出了一个统一观点：各种校准度量量化了特定类别的区分器或统计度量能够区分这两个世界的程度。

Conclusion: 校准作为概率预测评估的重要概念，通过不可区分性框架为理解预测质量提供了理论基础，对实际决策应用具有重要指导意义。

Abstract: Calibration is a classical notion from the forecasting literature which aims
to address the question: how should predicted probabilities be interpreted? In
a world where we only get to observe (discrete) outcomes, how should we
evaluate a predictor that hypothesizes (continuous) probabilities over possible
outcomes? The study of calibration has seen a surge of recent interest, given
the ubiquity of probabilistic predictions in machine learning. This survey
describes recent work on the foundational questions of how to define and
measure calibration error, and what these measures mean for downstream decision
makers who wish to use the predictions to make decisions. A unifying viewpoint
that emerges is that of calibration as a form of indistinguishability, between
the world hypothesized by the predictor and the real world (governed by nature
or the Bayes optimal predictor). In this view, various calibration measures
quantify the extent to which the two worlds can be told apart by certain
classes of distinguishers or statistical measures.

</details>


### [188] [Balanced Multimodal Learning: An Unidirectional Dynamic Interaction Perspective](https://arxiv.org/abs/2509.02281)
*Shijie Wang,Li Zhang,Xinyan Liang,Yuhua Qian,Shen Hu*

Main category: cs.LG

TL;DR: 提出UDI方法解决多模态学习中模态不平衡问题，通过单向动态交互替代传统联合损失，实现更有效的跨模态特征学习


<details>
  <summary>Details</summary>
Motivation: 传统多模态联合学习会导致模态不平衡，强模态压制弱模态，现有方法都是被动检测和修正不平衡，无法从根本上解决联合损失的竞争性问题

Method: UDI采用主动顺序训练策略：先训练锚定模态至收敛，然后用其学习表示通过无监督损失指导其他模态，动态调整模态交互以适应任务需求

Result: 实验结果表明UDI在处理模态不平衡方面优于现有方法，在多模态学习任务中实现了性能提升

Conclusion: UDI通过解耦模态优化和启用定向信息流，避免了单一模态主导，促进了有效的跨模态特征学习，为多模态不平衡学习提供了新思路

Abstract: Multimodal learning typically utilizes multimodal joint loss to integrate
different modalities and enhance model performance. However, this joint
learning strategy can induce modality imbalance, where strong modalities
overwhelm weaker ones and limit exploitation of individual information from
each modality and the inter-modality interaction information.Existing
strategies such as dynamic loss weighting, auxiliary objectives and gradient
modulation mitigate modality imbalance based on joint loss. These methods
remain fundamentally reactive, detecting and correcting imbalance after it
arises, while leaving the competitive nature of the joint loss untouched. This
limitation drives us to explore a new strategy for multimodal imbalance
learning that does not rely on the joint loss, enabling more effective
interactions between modalities and better utilization of information from
individual modalities and their interactions. In this paper, we introduce
Unidirectional Dynamic Interaction (UDI), a novel strategy that abandons the
conventional joint loss in favor of a proactive, sequential training scheme.
UDI first trains the anchor modality to convergence, then uses its learned
representations to guide the other modality via unsupervised loss. Furthermore,
the dynamic adjustment of modality interactions allows the model to adapt to
the task at hand, ensuring that each modality contributes optimally. By
decoupling modality optimization and enabling directed information flow, UDI
prevents domination by any single modality and fosters effective cross-modal
feature learning. Our experimental results demonstrate that UDI outperforms
existing methods in handling modality imbalance, leading to performance
improvement in multimodal learning tasks.

</details>


### [189] [AdaSwitch: An Adaptive Switching Meta-Algorithm for Learning-Augmented Bounded-Influence Problems](https://arxiv.org/abs/2509.02302)
*Xi Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TL;DR: 提出了一个基于序列预测的多周期在线决策框架AdaSwitch，在预测准确时接近离线最优性能，在预测不准确时保持经典竞争比保证


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型生成的序列预测在在线决策中的应用问题，这些预测的准确性无法保证，需要一种既能利用准确预测又能应对不准确预测的鲁棒方法

Method: 引入有界影响框架，提出AdaSwitch元算法，通过限制过去决策对未来最优奖励的影响，实现预测准确时接近离线最优、预测不准确时保持竞争比保证

Result: AdaSwitch算法在预测准确时性能接近离线基准，在预测高度不准确时仍能保持经典的竞争比保证

Conclusion: 该框架和元算法具有广泛适用性，可应用于处理系统中的提前期报价、k-服务器问题和可重用资源在线分配等多种场景，为学习增强的在线决策提供了灵活且广泛适用的方法

Abstract: We study a class of multi-period online decision-making problems with
sequence-based predictions, which may be generated by machine learning models
but whose accuracy is not guaranteed. In each period, the decision-maker
observes the realized request and must take an irrevocable action that yields a
reward or incurs a cost, without knowledge of future arrivals. We introduce a
bounded-influence framework, in which past decisions and requests exert only
limited impact on the future optimal reward. Within this framework, we propose
the AdaSwitch meta-algorithm, which exploits predictions to attain performance
close to the offline benchmark when predictions are accurate, while preserving
classical competitive-ratio guarantees under highly inaccurate predictions. Our
framework and meta-algorithm apply to diverse settings, including lead-time
quotation in processing systems, the $k$-server problem, and online allocation
of reusable resources. These applications illustrate the flexibility and broad
applicability of our approach to learning-augmented online decision-making.

</details>


### [190] [Extrapolated Markov Chain Oversampling Method for Imbalanced Text Classification](https://arxiv.org/abs/2509.02332)
*Aleksi Avela,Pauliina Ilmonen*

Main category: cs.LG

TL;DR: 提出了一种基于马尔可夫链的文本过采样方法，通过结合少数类和多数类的特征来扩展少数类特征空间，在严重不平衡的文本分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决文本分类中数据不平衡问题，特别是文本数据特有的挑战：当样本量增加时，特征空间也会随之扩展，传统过采样方法难以有效处理

Method: 基于马尔可夫链的文本过采样方法，从少数类估计转移概率，同时部分借鉴多数类的特征，允许少数类特征空间在过采样过程中扩展

Result: 在多个真实数据示例中，该方法相比其他主流过采样方法能够产生极具竞争力的结果，特别是在严重不平衡情况下表现突出

Conclusion: 该方法有效解决了文本数据不平衡分类的独特挑战，通过结合两类特征扩展少数类特征空间，在严重不平衡场景下具有显著优势

Abstract: Text classification is the task of automatically assigning text documents
correct labels from a predefined set of categories. In real-life (text)
classification tasks, observations and misclassification costs are often
unevenly distributed between the classes - known as the problem of imbalanced
data. Synthetic oversampling is a popular approach to imbalanced
classification. The idea is to generate synthetic observations in the minority
class to balance the classes in the training set. Many general-purpose
oversampling methods can be applied to text data; however, imbalanced text data
poses a number of distinctive difficulties that stem from the unique nature of
text compared to other domains. One such factor is that when the sample size of
text increases, the sample vocabulary (i.e., feature space) is likely to grow
as well. We introduce a novel Markov chain based text oversampling method. The
transition probabilities are estimated from the minority class but also partly
from the majority class, thus allowing the minority feature space to expand in
oversampling. We evaluate our approach against prominent oversampling methods
and show that our approach is able to produce highly competitive results
against the other methods in several real data examples, especially when the
imbalance is severe.

</details>


### [191] [RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2509.02341)
*Chih-Yu Lai,Yu-Chien Ning,Duane S. Boning*

Main category: cs.LG

TL;DR: RDIT是一个基于点估计和残差条件扩散的概率时间序列预测框架，通过双向Mamba网络实现，在多个数据集上取得了最优的CRPS分数和快速推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法存在分布建模不理想以及训练与评估指标不匹配的问题，研究发现为强点估计器添加匹配训练误差的高斯分布可以达到state-of-the-art性能。

Method: 提出RDIT框架，结合点估计和基于残差的条件扩散，使用双向Mamba网络，理论证明可以通过调整最优标准差来最小化CRPS，并推导实现分布匹配的算法。

Result: 在8个多变量数据集上的评估显示，RDIT相比强基线实现了更低的CRPS、更快的推理速度和更好的覆盖率。

Conclusion: RDIT框架通过创新的点估计与条件扩散结合方法，有效解决了概率时间序列预测中的分布建模和指标匹配问题，取得了优异的性能表现。

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a critical role in domains
requiring accurate and uncertainty-aware predictions for decision-making.
However, existing methods offer suboptimal distribution modeling and suffer
from a mismatch between training and evaluation metrics. Surprisingly, we found
that augmenting a strong point estimator with a zero-mean Gaussian, whose
standard deviation matches its training error, can yield state-of-the-art
performance in PTSF. In this work, we propose RDIT, a plug-and-play framework
that combines point estimation and residual-based conditional diffusion with a
bidirectional Mamba network. We theoretically prove that the Continuous Ranked
Probability Score (CRPS) can be minimized by adjusting to an optimal standard
deviation and then derive algorithms to achieve distribution matching.
Evaluations on eight multivariate datasets across varied forecasting horizons
demonstrate that RDIT achieves lower CRPS, rapid inference, and improved
coverage compared to strong baselines.

</details>


### [192] [Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology](https://arxiv.org/abs/2509.02355)
*Caterina Fuster-Barcelo,Gonzalo R. Rios-Munoz,Arrate Munoz-Barrutia*

Main category: cs.LG

TL;DR: 该研究通过整合数字协作工具和结构化同伴评估机制，在生物医学图像处理课程中提升了学生参与度和评估公平性


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何通过数字协作工具和结构化评估机制来增强STEM教育中的学习效果和评估公平性，特别是在机器学习健康硕士项目中

Method: 重新设计生物医学图像处理课程，整合Google Colab实时编程、Weights & Biases实验跟踪与报告，以及基于量规的同伴评估机制

Result: 相比干预前队列，两个实施年度显示出成绩离散度增加和最终项目分数熵值更高，表明评估的区分度和公平性得到改善；调查结果显示学生对学科和自身学习过程的参与度更高

Conclusion: 工具支持的协作和结构化评估机制整合具有提升STEM教育学习成果和公平性的潜力

Abstract: This study examines the integration of digital collaborative tools and
structured peer evaluation in the Machine Learning for Health master's program,
through the redesign of a Biomedical Image Processing course over two academic
years. The pedagogical framework combines real-time programming with Google
Colab, experiment tracking and reporting via Weights & Biases, and
rubric-guided peer assessment to foster student engagement, transparency, and
fair evaluation. Compared to a pre-intervention cohort, the two implementation
years showed increased grade dispersion and higher entropy in final project
scores, suggesting improved differentiation and fairness in assessment. The
survey results further indicate greater student engagement with the subject and
their own learning process. These findings highlight the potential of
integrating tool-supported collaboration and structured evaluation mechanisms
to enhance both learning outcomes and equity in STEM education.

</details>


### [193] [Gaming and Cooperation in Federated Learning: What Can Happen and How to Monitor It](https://arxiv.org/abs/2509.02391)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 该论文将联邦学习建模为战略系统而非单纯优化问题，提出了分析框架来区分真正提升性能的行为与仅针对指标的行为，并提供了实用工具和算法来降低指标博弈动机。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的成功依赖于参与者不可见的行动，需要从战略系统角度分析规则和激励机制，以识别真正改善性能的行为与仅针对指标的行为之间的差异。

Method: 提出分析框架，引入两个指标分别量化行为激励和集体性能损失，总结阈值、自动切换规则和预警信号清单，提供有限审计资源分配算法和性能保证。

Result: 在不同环境下的模拟验证了框架预测的模式，提供了完整可复现的程序，展示了在现实操作变异性下的鲁棒应用。

Conclusion: 通过结合定期重新校准、随机化和基于连接的警报，提出了降低指标博弈动机同时维持和扩展稳定合作的设计原则和操作指南。

Abstract: The success of Federated Learning depends on the actions that participants
take out of sight. We model Federated Learning not as a mere optimization task
but as a strategic system entangled with rules and incentives. From this
perspective, we present an analytical framework that makes it possible to
clearly identify where behaviors that genuinely improve performance diverge
from those that merely target metrics. We introduce two indices that
respectively quantify behavioral incentives and collective performance loss,
and we use them as the basis for consistently interpreting the impact of
operational choices such as rule design, the level of information disclosure,
evaluation methods, and aggregator switching. We further summarize thresholds,
auto-switch rules, and early warning signals into a checklist that can be
applied directly in practice, and we provide both a practical algorithm for
allocating limited audit resources and a performance guarantee. Simulations
conducted across diverse environments consistently validate the patterns
predicted by our framework, and we release all procedures for full
reproducibility. While our approach operates most strongly under several
assumptions, combining periodic recalibration, randomization, and
connectivity-based alarms enables robust application under the variability of
real-world operations. We present both design principles and operational
guidelines that lower the incentives for metric gaming while sustaining and
expanding stable cooperation.

</details>


### [194] [Evaluating Cumulative Spectral Gradient as a Complexity Measure](https://arxiv.org/abs/2509.02399)
*Haji Gul,Abdul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.LG

TL;DR: 本文对知识图谱链接预测中的CSG复杂度度量指标进行了严格评估，发现其存在敏感性和相关性不足的问题，挑战了原有结论。


<details>
  <summary>Details</summary>
Motivation: 准确评估数据集复杂度对于知识图谱链接预测模型的比较至关重要，需要验证CSG指标在链接预测任务中的有效性和稳定性。

Method: 在标准知识图谱链接预测基准上（FB15k-237、WN18RR等），通过控制关键参数M（蒙特卡洛采样点数）和K（最近邻数量），系统评估CSG指标的行为表现。

Result: CSG对K值选择高度敏感，无法随目标类别数量自然扩展；与MRR等性能指标相关性弱或无相关性；在链接预测场景中原有的稳定性和泛化预测能力失效。

Conclusion: 研究结果表明需要开发更鲁棒、分类器无关的复杂度度量方法来进行知识图谱链接预测评估。

Abstract: Accurate estimation of dataset complexity is crucial for evaluating and
comparing link prediction models for knowledge graphs (KGs). The Cumulative
Spectral Gradient (CSG) metric derived from probabilistic divergence between
classes within a spectral clustering framework was proposed as a dataset
complexity measure that (1) naturally scales with the number of classes and (2)
correlates strongly with downstream classification performance. In this work,
we rigorously assess CSG behavior on standard knowledge graph link prediction
benchmarks a multi class tail prediction task, using two key parameters
governing its computation, M, the number of Monte Carlo sampled points per
class, and K, the number of nearest neighbors in the embedding space. Contrary
to the original claims, we find that (1) CSG is highly sensitive to the choice
of K and therefore does not inherently scale with the number of target classes,
and (2) CSG values exhibit weak or no correlation with established performance
metrics such as mean reciprocal rank (MRR). Through experiments on FB15k 237,
WN18RR, and other standard datasets, we demonstrate that CSG purported
stability and generalization predictive power break down in link prediction
settings. Our results highlight the need for more robust, classifier agnostic
complexity measures in KG link prediction evaluation.

</details>


### [195] [Fisher information flow in artificial neural networks](https://arxiv.org/abs/2509.02407)
*Maximilian Weimar,Lukas M. Rachbauer,Ilya Starshynov,Daniele Faccio,Linara Adilova,Dorian Bouchet,Stefan Rotter*

Main category: cs.LG

TL;DR: 该论文提出了一种监测Fisher信息在人工神经网络中流动的方法，用于参数估计任务，发现最优估计性能对应Fisher信息最大传输，过拟合会导致信息损失，这为网络训练提供了无需验证集的停止准则。


<details>
  <summary>Details</summary>
Motivation: 随着人工神经网络逐渐成为测量系统的组成部分，需要理解它们如何处理和传输参数相关信息。Fisher信息是理解参数估计精度极限的关键工具，但神经网络内部的信息流动机制尚不明确。

Method: 提出了一种监测Fisher信息从输入层到输出层在神经网络中流动的方法，通过跟踪信息传输过程来分析网络的信息处理能力。

Result: 研究表明最优估计性能对应于Fisher信息的最大传输，训练超过此点会导致信息损失（过拟合）。该方法在成像实验数据上得到验证。

Conclusion: 该方法提供了一个模型无关的训练停止准则，无需单独的验证数据集，在真实物理场景中具有实际应用价值。

Abstract: The estimation of continuous parameters from measured data plays a central
role in many fields of physics. A key tool in understanding and improving such
estimation processes is the concept of Fisher information, which quantifies how
information about unknown parameters propagates through a physical system and
determines the ultimate limits of precision. With Artificial Neural Networks
(ANNs) gradually becoming an integral part of many measurement systems, it is
essential to understand how they process and transmit parameter-relevant
information internally. Here, we present a method to monitor the flow of Fisher
information through an ANN performing a parameter estimation task, tracking it
from the input to the output layer. We show that optimal estimation performance
corresponds to the maximal transmission of Fisher information, and that
training beyond this point results in information loss due to overfitting. This
provides a model-free stopping criterion for network training-eliminating the
need for a separate validation dataset. To demonstrate the practical relevance
of our approach, we apply it to a network trained on data from an imaging
experiment, highlighting its effectiveness in a realistic physical setting.

</details>


### [196] [Cache Management for Mixture-of-Experts LLMs -- extended version](https://arxiv.org/abs/2509.02408)
*Spyros Angelopoulos,Loris Marchal,Adrien Obrecht,Bertrand Simon*

Main category: cs.LG

TL;DR: 本文提出了一种针对混合专家模型（MoE）中专家管理的新分页问题，通过理论分析和实验验证，证明了层感知LRU算法在专家缓存管理中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署中的主要挑战是内存管理，特别是MoE架构中需要高效管理有限缓存空间，确保常用专家存储在快速缓存中而非慢速二级存储器。

Method: 提出了新的专家管理分页问题模型，首先给出确定性和随机化算法的竞争比下界，然后设计了针对该问题的层感知LRU扩展算法。

Result: 理论分析表明LRU类策略具有良好的竞争性能，实验证明该算法在合成数据集和实际MoE使用轨迹上都优于标准LRU等经典分页策略。

Conclusion: 所提出的层感知缓存管理算法能够有效优化MoE架构中的专家管理，为大型语言模型的高效部署提供了实用的缓存管理解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks. One of the main challenges towards the successful
deployment of LLMs is memory management, since they typically involve billions
of parameters. To this end, architectures based on Mixture-of-Experts have been
proposed, which aim to reduce the size of the parameters that are activated
when producing a token. This raises the equally critical issue of efficiently
managing the limited cache of the system, in that frequently used experts
should be stored in the fast cache rather than in the slower secondary memory.
  In this work, we introduce and study a new paging problem that models expert
management optimization. Our formulation captures both the layered architecture
of LLMs and the requirement that experts are cached efficiently. We first
present lower bounds on the competitive ratio of both deterministic and
randomized algorithms, which show that under mild assumptions, LRU-like
policies have good theoretical competitive performance. We then propose a
layer-based extension of LRU that is tailored to the problem at hand.
  Extensive simulations on both synthetic datasets and actual traces of MoE
usage show that our algorithm outperforms policies for the classic paging
problem, such as the standard LRU.

</details>


### [197] [Learnable Loss Geometries with Mirror Descent for Scalable and Convergent Meta-Learning](https://arxiv.org/abs/2509.02418)
*Yilang Zhang,Bingcong Li,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文提出了一种非线性镜像映射方法，通过神经网络学习距离生成函数，有效处理复杂损失几何形状，显著提高了元学习的样本效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 元学习面临的主要挑战是如何高效地利用从相关任务中获得的任务不变知识作为先验信息，在有限数据下快速适应新任务。传统的线性预调节方法在处理复杂损失几何形状时效果有限。

Method: 提出学习一个通用的距离生成函数，通过神经网络参数化诱导非线性镜像映射，能够有效捕获和优化各种损失几何形状。该方法提供了理论保证，证明该生成函数是有效的距离度量。

Result: 理论分析表明该方法具有O(ε⁻²)的收敛速率，与标准基于梯度的元学习方法相当。在少样本学习数据集上的数值测试显示，新算法具有优越的实证性能，显著减少了每个任务的适应步骤数。

Conclusion: 所提出的非线性镜像映射方法能够有效处理复杂损失几何，提高元学习的样本效率和收敛速度，特别适合大规模元学习模型的应用。

Abstract: Utilizing task-invariant knowledge acquired from related tasks as prior
information, meta-learning offers a principled approach to learning a new task
with limited data records. Sample-efficient adaptation of this prior
information is a major challenge facing meta-learning, and plays an important
role because it facilitates training the sought task-specific model with just a
few optimization steps. Past works deal with this challenge through
preconditioning that speeds up convergence of the per-task training. Though
effective in representing locally quadratic loss curvatures, simple linear
preconditioning can be hardly potent with complex loss geometries. Instead of
relying on a quadratic distance metric, the present contribution copes with
complex loss metrics by learning a versatile distance-generating function,
which induces a nonlinear mirror map to effectively capture and optimize a wide
range of loss geometries. With suitable parameterization, this generating
function is effected by an expressive neural network that is provably a valid
distance. Analytical results establish convergence of not only the proposed
method, but also all meta-learning approaches based on preconditioning. To
attain gradient norm less than $\epsilon$, the convergence rate of
$\mathcal{O}(\epsilon^{-2})$ is on par with standard gradient-based
meta-learning methods. Numerical tests on few-shot learning datasets
demonstrate the superior empirical performance of the novel algorithm, as well
as its rapid per-task convergence, which markedly reduces the number of
adaptation steps, hence also accommodating large-scale meta-learning models.

</details>


### [198] [VASSO: Variance Suppression for Sharpness-Aware Minimization](https://arxiv.org/abs/2509.02433)
*Bingcong Li,Yilang Zhang,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: VASSO方法通过抑制方差来稳定SAM中的对抗扰动，避免过度友好的对抗样本，从而提升深度神经网络的泛化性能。


<details>
  <summary>Details</summary>
Motivation: SAM方法虽然通过寻找平坦最小值来提升泛化能力，但在实践中存在对抗扰动过于友好的问题，限制了最终的泛化效果。

Method: 提出VASSO（方差抑制）方法来稳定对抗扰动，通过抑制方差来避免过度友好的对抗样本，可与SAM及其计算高效变体结合使用。

Result: 在广泛的视觉和语言任务上验证了VASSO+SAM组合的改进泛化能力，并在计算效率与泛化性能之间取得了良好平衡。

Conclusion: VASSO提供了一种通用的方法来稳定对抗扰动，显著提升了SAM的泛化性能，同时保持了计算效率。

Abstract: Sharpness-aware minimization (SAM) has well-documented merits in enhancing
generalization of deep neural network models. Accounting for sharpness in the
loss function geometry, where neighborhoods of `flat minima' heighten
generalization ability, SAM seeks `flat valleys' by minimizing the maximum loss
provoked by an adversarial perturbation within the neighborhood. Although
critical to account for sharpness of the loss function, in practice SAM suffers
from `over-friendly adversaries,' which can curtail the outmost level of
generalization. To avoid such `friendliness,' the present contribution fosters
stabilization of adversaries through variance suppression (VASSO). VASSO offers
a general approach to provably stabilize adversaries. In particular, when
integrating VASSO with SAM, improved generalizability is numerically validated
on extensive vision and language tasks. Once applied on top of a
computationally efficient SAM variant, VASSO offers a desirable
generalization-computation tradeoff.

</details>


### [199] [Generative Sequential Notification Optimization via Multi-Objective Decision Transformers](https://arxiv.org/abs/2509.02458)
*Borja Ocejo,Ruofan Wang,Ke Liu,Rohit K. Patra,Haotian Shen,David Liu,Yiwen Yuan,Gokulraj Mohanasundaram,Fedor Borisyuk,Prakruthi Prabhakar*

Main category: cs.LG

TL;DR: 通知推荐系统中采用决策Transformer框架，通过返回条件监督学习提升了稳定性和可解释性，在LinkedIn实际部署中实现了更高的会话活跃度和更低的用户疲劳


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习方法在通知推荐系统中遇到稳定性、分布偏移敏感性、可复现性和可解释性等挑战，需要更稳健可扩展的方案

Method: 提出基于决策Transformer的框架，将策略学习重构为返回条件监督学习，包括多奖励设计、分位数回归返回条件处理和环形缓冲区序列处理机制

Result: 在LinkedIn通知系统中进行广泛离线和在线实验，与多目标CQL基础组相比，DT方法实现了+0.72%的会话增长，提升了通知相关性和用户活动度

Conclusion: 决策Transformer框架在通知推荐系统中表现优异，具有更好的稳健性、可扩展性和模型灵活性，能够同时优化消息效用和用户体验

Abstract: Notifications are an important communication channel for delivering timely
and relevant information. Optimizing their delivery involves addressing complex
sequential decision-making challenges under constraints such as message utility
and user fatigue. Offline reinforcement learning (RL) methods, such as
Conservative Q-Learning (CQL), have been applied to this problem but face
practical challenges at scale, including instability, sensitivity to
distribution shifts, limited reproducibility, and difficulties with
explainability in high-dimensional recommendation settings. We present a
Decision Transformer (DT) based framework that reframes policy learning as
return-conditioned supervised learning, improving robustness, scalability, and
modeling flexibility. Our contributions include a real-world comparison with
CQL, a multi-reward design suitable for non-episodic tasks, a quantile
regression approach to return-to-go conditioning, and a production-ready system
with circular buffer-based sequence processing for near-real-time inference.
Extensive offline and online experiments in a deployed notification system show
that our approach improves notification utility and overall session activity
while minimizing user fatigue. Compared to a multi-objective CQL-based agent,
the DT-based approach achieved a +0.72% increase in sessions for notification
decision-making at LinkedIn by making notification recommendation more
relevant.

</details>


### [200] [Exploring Variational Graph Autoencoders for Distribution Grid Data Generation](https://arxiv.org/abs/2509.02469)
*Syed Zain Abbas,Ehimare Okoyomon*

Main category: cs.LG

TL;DR: 使用变分图自编码器(VGAE)生成合成配电网，评估了四种解码器变体，发现简单解码器效果不佳，GCN方法在简单数据集表现良好但在复杂数据集存在缺陷


<details>
  <summary>Details</summary>
Motivation: 解决能源网络机器学习研究中公开电力系统数据缺乏的问题

Method: 使用变分图自编码器(VGAE)和两种开源数据集(ENGAGE和DINGO)，评估四种解码器变体，通过结构和频谱指标比较生成网络与原始电网

Result: 简单解码器无法捕捉真实拓扑，基于GCN的方法在ENGAGE数据集上表现良好但在更复杂的DINGO数据集上产生断开组件和重复模式等伪影

Conclusion: VGAE在电网合成方面既有前景也有局限性，需要更具表现力的生成模型和更稳健的评估方法

Abstract: To address the lack of public power system data for machine learning research
in energy networks, we investigate the use of variational graph autoencoders
(VGAEs) for synthetic distribution grid generation. Using two open-source
datasets, ENGAGE and DINGO, we evaluate four decoder variants and compare
generated networks against the original grids using structural and spectral
metrics. Results indicate that simple decoders fail to capture realistic
topologies, while GCN-based approaches achieve strong fidelity on ENGAGE but
struggle on the more complex DINGO dataset, producing artifacts such as
disconnected components and repeated motifs. These findings highlight both the
promise and limitations of VGAEs for grid synthesis, underscoring the need for
more expressive generative models and robust evaluation. We release our models
and analysis as open source to support benchmarking and accelerate progress in
ML-driven power system research.

</details>


### [201] [SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning](https://arxiv.org/abs/2509.02479)
*Zhenghai Xue,Longtao Zheng,Qian Liu,Yingru Li,Xiaosen Zheng,Zejun Ma,Bo An*

Main category: cs.LG

TL;DR: SimpleTIR算法通过过滤无效轨迹来解决多轮工具集成推理中的训练不稳定问题，显著提升数学推理性能


<details>
  <summary>Details</summary>
Motivation: 多轮工具集成推理(TIR)在强化学习训练中存在不稳定性和性能崩溃问题，主要原因是外部工具反馈导致的分布漂移和低概率token生成

Method: 提出SimpleTIR算法，核心策略是识别并过滤掉包含无效轮次(既不生成代码块也不产生最终答案)的轨迹，从而阻止有害的高幅度梯度

Result: 在数学推理基准测试中达到最先进性能，将AIME24分数从22.1提升到50.5，并促进模型发现多样化的推理模式

Conclusion: SimpleTIR是一种即插即用的稳定训练算法，有效解决了多轮TIR训练中的不稳定问题，避免了监督微调的限制

Abstract: Large Language Models (LLMs) can significantly improve their reasoning
capabilities by interacting with external tools, a paradigm known as
Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios
using Reinforcement Learning (RL) is often hindered by training instability and
performance collapse. We identify that such instability is primarily caused by
a distributional drift from external tool feedback, leading to the generation
of low-probability tokens. This issue compounds over successive turns, causing
catastrophic gradient norm explosions that derail the training process. To
address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that
stabilizes multi-turn TIR training. Its core strategy is to identify and filter
out trajectories containing void turns, i.e., turns that yield neither a code
block nor a final answer. By removing these problematic trajectories from the
policy update, SimpleTIR effectively blocks the harmful, high-magnitude
gradients, thus stabilizing the learning dynamics. Extensive experiments show
that SimpleTIR achieves state-of-the-art performance on challenging math
reasoning benchmarks, notably elevating the AIME24 score from a text-only
baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.
Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR
encourages the model to discover diverse and sophisticated reasoning patterns,
such as self-correction and cross-validation.

</details>


### [202] [RNN Generalization to Omega-Regular Languages](https://arxiv.org/abs/2509.02491)
*Charles Pert,Dalal Alrajeh,Alessandra Russo*

Main category: cs.LG

TL;DR: 该研究首次探讨了循环神经网络(RNN)能否泛化到从LTL公式导出的ω-正则语言，通过在周期性ω词序列上训练RNN来复制目标Büchi自动机行为，并在分布外序列上评估泛化能力。


<details>
  <summary>Details</summary>
Motivation: Büchi自动机在验证反应式系统时面临可扩展性挑战，而神经网络在模型检查等领域被越来越多地用于解决这些挑战，因此需要研究其泛化能力。

Method: 在周期性ω词序列上训练RNN来复制目标Büchi自动机行为，评估其在分布外序列上的泛化能力，实验涵盖3到100多个状态的确定性自动机。

Result: RNN在比训练示例长8倍的序列上实现了高精度，92.6%的任务实现了完美或接近完美的泛化，表明神经网络方法学习复杂ω-正则语言的可行性。

Conclusion: 这些结果确立了神经网络方法学习复杂ω-正则语言的可行性，表明它们作为神经符号验证方法组件的潜力。

Abstract: B\"uchi automata (BAs) recognize $\omega$-regular languages defined by formal
specifications like linear temporal logic (LTL) and are commonly used in the
verification of reactive systems. However, BAs face scalability challenges when
handling and manipulating complex system behaviors. As neural networks are
increasingly used to address these scalability challenges in areas like model
checking, investigating their ability to generalize beyond training data
becomes necessary. This work presents the first study investigating whether
recurrent neural networks (RNNs) can generalize to $\omega$-regular languages
derived from LTL formulas. We train RNNs on ultimately periodic $\omega$-word
sequences to replicate target BA behavior and evaluate how well they generalize
to out-of-distribution sequences. Through experiments on LTL formulas
corresponding to deterministic automata of varying structural complexity, from
3 to over 100 states, we show that RNNs achieve high accuracy on their target
$\omega$-regular languages when evaluated on sequences up to $8 \times$ longer
than training examples, with $92.6\%$ of tasks achieving perfect or
near-perfect generalization. These results establish the feasibility of neural
approaches for learning complex $\omega$-regular languages, suggesting their
potential as components in neurosymbolic verification methods.

</details>


### [203] [MoPEQ: Mixture of Mixed Precision Quantized Experts](https://arxiv.org/abs/2509.02512)
*Krishna Teja Chitty-Venkata,Jie Ye,Murali Emani*

Main category: cs.LG

TL;DR: 提出MoPEQ算法，为混合专家模型中的每个专家分配最优比特宽度，通过Hessian迹近似分析专家敏感性，在保持精度的同时显著减少内存占用


<details>
  <summary>Details</summary>
Motivation: 混合专家架构的大语言和视觉模型部署时面临巨大计算和内存需求挑战，需要高效的量化方法来平衡精度和模型大小

Method: 使用后训练量化算法MoPEQ，基于Hessian迹近似分析每个专家的敏感性（而非激活频率），对相似专家进行聚类分配最优比特宽度

Result: 在VLMEvalKit基准测试中，Deepseek-VL2和MolmoE模型的混合精度量化MoE在保持竞争力的准确率同时，内存占用相比均匀精度基线方法有显著改善

Conclusion: 该方法为VLM-MoE混合精度量化提供了全面理解，证明了基于专家敏感性的量化策略在减少内存需求方面的有效性

Abstract: Large Language and Vision Models using a Mixture-of-Experts (MoE)
architecture pose significant challenges for deployment due to their
computational and memory demands. Mixed Precision Quantization assigns
different precisions to different layers of an LLM/VLM based on layer
sensitivity and importance within the model. In this work, we propose a Post
Training Quantization algorithm, MoPEQ, that assigns optimal bit width to each
expert. Our method balances accuracy and model size by analyzing each expert's
sensitivity using Hessian trace approximation instead of relying on the
activation frequency of the expert. This per-expert granularity approach
clusters similar experts to maintain model performance while reducing memory
requirements. The experimental results on VLMEvalKit benchmark datasets using
State-of-the-art VLMs Deepseek-VL2 -tiny, -small, -base, and MolmoE models
demonstrate that our mixed precision quantized MoEs achieve competitive
accuracy with substantial improvements in memory footprint compared to
uniform-precision baseline methods. We perform a comprehensive study to analyze
the impact of expert activation frequency and sensitivity using Hessian trace
approximation at both layer-wise and model-wide expert precision allocation of
2, 3, and 4 bits to provide a thorough understanding of mixed precision
quantization of VLM-MoEs.

</details>


### [204] [Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models](https://arxiv.org/abs/2509.02528)
*Wenlong Mou*

Main category: cs.LG

TL;DR: 本文提出了一种基于值函数逼近的扩散过程微调控制策略学习算法，通过求解HJB方程的变分不等式问题，获得了比通用强化学习方法更快的统计速率保证。


<details>
  <summary>Details</summary>
Motivation: 研究如何学习最优控制策略来微调给定的扩散过程，利用值函数逼近方法解决这一控制问题。

Method: 开发了基于Hamilton-Jacobi-Bellman (HJB)方程变分不等式问题求解的新算法类别，使用监督回归方法实现微调。

Result: 证明了学习到的值函数和控制策略具有尖锐的统计速率，该速率取决于函数类的复杂性和逼近误差。

Conclusion: 与通用强化学习问题相比，该方法表明微调可以通过监督回归实现，并获得更快的统计速率保证。

Abstract: We study the problem of learning the optimal control policy for fine-tuning a
given diffusion process, using general value function approximation. We develop
a new class of algorithms by solving a variational inequality problem based on
the Hamilton-Jacobi-Bellman (HJB) equations. We prove sharp statistical rates
for the learned value function and control policy, depending on the complexity
and approximation errors of the function class. In contrast to generic
reinforcement learning problems, our approach shows that fine-tuning can be
achieved via supervised regression, with faster statistical rate guarantees.

</details>


### [205] [Federated learning over physical channels: adaptive algorithms with near-optimal guarantees](https://arxiv.org/abs/2509.02538)
*Rui Zhang,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出了一种新的自适应联邦随机梯度下降算法，通过物理信道传输信息来降低联邦学习的通信成本，考虑了信道噪声和硬件约束，并提供了理论收敛保证和实际验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信成本较高，通过物理信道传输信息可以显著降低通信开销，但需要解决信道噪声和硬件约束带来的挑战。

Method: 提出自适应联邦随机梯度下降算法，在物理信道中实现信息传输，算法能够适应随机梯度噪声水平。

Result: 建立了算法的理论收敛保证，证明了收敛速率对随机梯度噪声水平的自适应性，并通过深度学习模型的仿真研究验证了算法的实际有效性。

Conclusion: 该方法成功解决了联邦学习在物理信道传输中的噪声和硬件约束问题，提供了理论支撑和实际可行性，显著降低了通信成本。

Abstract: In federated learning, communication cost can be significantly reduced by
transmitting the information over the air through physical channels. In this
paper, we propose a new class of adaptive federated stochastic gradient descent
(SGD) algorithms that can be implemented over physical channels, taking into
account both channel noise and hardware constraints. We establish theoretical
guarantees for the proposed algorithms, demonstrating convergence rates that
are adaptive to the stochastic gradient noise level. We also demonstrate the
practical effectiveness of our algorithms through simulation studies with deep
learning models.

</details>


### [206] [Surrogate Benchmarks for Model Merging Optimization](https://arxiv.org/abs/2509.02555)
*Rio Akizuki,Yuya Kudo,Nozomu Yoshinari,Yoichi Hirose,Toshiyuki Nishimoto,Kento Uchida,Shinichi Shirakawa*

Main category: cs.LG

TL;DR: 开发了用于模型合并超参数优化的代理基准，以低成本实现算法开发和性能比较


<details>
  <summary>Details</summary>
Motivation: 模型合并技术需要超参数调优来提升性能，但优化过程计算成本高昂，特别是在合并大语言模型时

Method: 定义两个搜索空间并收集数据样本构建代理模型，用于从超参数预测合并模型的性能

Result: 基准能够很好地预测合并模型性能并模拟优化算法行为

Conclusion: 提出的代理基准为模型合并超参数优化提供了低成本高效的解决方案

Abstract: Model merging techniques aim to integrate the abilities of multiple models
into a single model. Most model merging techniques have hyperparameters, and
their setting affects the performance of the merged model. Because several
existing works show that tuning hyperparameters in model merging can enhance
the merging outcome, developing hyperparameter optimization algorithms for
model merging is a promising direction. However, its optimization process is
computationally expensive, particularly in merging LLMs. In this work, we
develop surrogate benchmarks for optimization of the merging hyperparameters to
realize algorithm development and performance comparison at low cost. We define
two search spaces and collect data samples to construct surrogate models to
predict the performance of a merged model from a hyperparameter. We demonstrate
that our benchmarks can predict the performance of merged models well and
simulate optimization algorithm behaviors.

</details>


### [207] [DynaGuard: A Dynamic Guardrail Model With User-Defined Policies](https://arxiv.org/abs/2509.02563)
*Monte Hoover,Vatsal Baherwani,Neel Jain,Khalid Saifullah,Joseph Vincent,Chirag Jain,Melissa Kazemi Rad,C. Bayan Bruss,Ashwinee Panda,Tom Goldstein*

Main category: cs.LG

TL;DR: 动态监护模型支持用户自定义政策，能够高效检测各种策略违规，性能与前沿推理模型相当但更快速。


<details>
  <summary>Details</summary>
Motivation: 标准监护模型如LlamaGuard只能检测预定义的静态害害类别，无法满足不同应用域的灵活政策需求。

Method: 提出动态监护模型，支持用户自定义政策评估文本，可通过快速检测或链式思绪推理方式输出判断理由。

Result: 动态监护模型在静态害害类别检测上与静态模型准确性相当，在自由形式政策违规检测上达到前沿推理模型的准确度，但耗时更短。

Conclusion: 动态监护模型提供了更灵活、高效的内容审核方案，能够满足多样化应用场景的政策执行需求。

Abstract: Guardian models are used to supervise and moderate the outputs of user-facing
chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian
models like LlamaGuard detect predefined, static categories of harms. We
propose dynamic guardian models that evaluate text based on user-defined
policies, making them useful for different application domains that are not
addressed by standard guardian models. Our dynamic guardian models can be used
for fast detection of policy violations or with chain-of-thought reasoning that
articulates and justifies the model outputs. Our dynamic guardian models match
static models in detection accuracy for static harm categories while
identifying violations of free-form policies with accuracy comparable to
frontier reasoning models in a fraction of the time.

</details>


### [208] [Understanding sparse autoencoder scaling in the presence of feature manifolds](https://arxiv.org/abs/2509.02565)
*Eric J. Michaud,Liv Gorton,Tom McGrath*

Main category: cs.LG

TL;DR: 本文研究了稀疏自编码器(SAE)的缩放规律，发现特征流形可能导致SAE学习的特征数量远少于其潜在变量数量，存在病理性的缩放机制。


<details>
  <summary>Details</summary>
Motivation: 理解稀疏自编码器的缩放行为，特别是特征流形（多维特征）如何影响SAE的缩放规律，以及探索SAE在现实应用中是否处于病理性机制中。

Method: 采用神经网络缩放文献中的容量分配模型(Brill, 2024)，将其适配到SAE缩放分析中，研究特征流形对缩放行为的影响。

Result: 模型重现了不同的缩放机制，发现在某些机制中特征流形会导致SAE学习的特征数量远少于潜在变量数量，存在病理性的缩放效应。

Conclusion: 特征流形对SAE缩放行为有重要影响，可能导致病理性机制，需要进一步研究SAE在现实应用中的实际缩放行为。

Abstract: Sparse autoencoders (SAEs) model the activations of a neural network as
linear combinations of sparsely occurring directions of variation (latents).
The ability of SAEs to reconstruct activations follows scaling laws w.r.t. the
number of latents. In this work, we adapt a capacity-allocation model from the
neural scaling literature (Brill, 2024) to understand SAE scaling, and in
particular, to understand how "feature manifolds" (multi-dimensional features)
influence scaling behavior. Consistent with prior work, the model recovers
distinct scaling regimes. Notably, in one regime, feature manifolds have the
pathological effect of causing SAEs to learn far fewer features in data than
there are latents in the SAE. We provide some preliminary discussion on whether
or not SAEs are in this pathological regime in the wild.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [209] [Improving Nonpreemptive Multiserver Job Scheduling with Quickswap](https://arxiv.org/abs/2509.01893)
*Zhongrui Chen,Adityo Anggraito,Diletta Olliaro,Andrea Marin,Marco Ajmone Marsan,Benjamin Berg,Isaac Grosof*

Main category: cs.PF

TL;DR: 提出MSFQ调度策略来解决多服务器作业调度问题，通过周期性调整优先级来减少等待时间变异性，在保持高资源利用率的同时显著优于传统MSF和FCFS策略


<details>
  <summary>Details</summary>
Motivation: 现代数据中心的多服务器作业是有状态的，抢占会带来高昂开销，因此需要非抢占式调度策略。传统MSF策略虽然利用率高但等待时间变异性大，FCFS可能导致资源闲置和系统不稳定

Method: 提出MSFQ（MSF-Quick Swap）策略，在MSF基础上周期性给予其他作业优先级，减少等待时间变异性。提供稳定性分析和平均响应时间分析，并在复杂场景下进行仿真评估

Result: MSFQ在单核或全核请求场景下显著优于MSF，经过优化的MSFQ变体在真实多服务器作业负载上大幅超越MSF和FCFS

Conclusion: MSFQ策略通过平衡资源利用率和等待时间公平性，为非抢占式多服务器作业调度提供了有效的解决方案，在实际工作负载中表现优异

Abstract: Modern data center workloads are composed of multiserver jobs, computational
jobs that require multiple CPU cores in order to run. A data center server can
run many multiserver jobs in parallel, as long as it has sufficient resources
to meet their demands. However, multiserver jobs are generally stateful,
meaning that job preemptions incur significant overhead from saving and
reloading the state associated with running jobs. Hence, most systems try to
avoid these costly job preemptions altogether. Given these constraints, a
scheduling policy must determine what set of jobs to run in parallel at each
moment in time to minimize the mean response time across a stream of arriving
jobs. Unfortunately, simple non-preemptive policies such as FCFS may leave many
cores idle, resulting in high mean response times or even system instability.
Our goal is to design and analyze non-preemptive scheduling policies for
multiserver jobs that maintain high system utilization to achieve low mean
response time.
  One well-known non-preemptive policy, Most Servers First (MSF), prioritizes
jobs with higher core requirements and achieves high resource utilization.
However, MSF causes extreme variability in job waiting times, and can perform
significantly worse than FCFS in practice. To address this, we propose and
analyze a class of scheduling policies called MSF-Quick Swap (MSFQ) that
performs well. MSFQ reduces the variability of job waiting times by
periodically granting priority to other jobs in the system. We provide both
stability results and an analysis of mean response time under MSFQ to prove
that our policy dramatically outperforms MSF in the case where jobs request one
core or all the cores. In more complex cases, we evaluate MSFQ in simulation.
We show that, with some additional optimization, variants of the MSFQ policy
can greatly outperform MSF and FCFS on real-world multiserver job workloads.

</details>


### [210] [Non-Asymptotic Performance Analysis of DOA Estimation Based on Real-Valued Root-MUSIC](https://arxiv.org/abs/2509.01999)
*Junyang Liu,Weicheng Zhao,Qingping Wang,Xiangtian Meng,Maria Greco,Fulvio Gini*

Main category: cs.PF

TL;DR: 对RV-root-MUSIC算法在非渐近条件下的理论性能分析，重点研究噪声子空间扰动、真实根扰动和镜像根扰动特性，建立了统计模型并验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: RV-root-MUSIC算法存在镜像根估计模糊问题，通常需要传统波束形成技术来滤除镜像根，因此需要系统性的理论分析来优化实际应用中的参数选择。

Method: 基于共轭扩展方法的等效子空间和真实根与镜像根扰动的等价性，建立了统计模型并推导了扰动的广义表达式。

Result: 仿真结果验证了推导的统计特性的正确性和有效性，为DOA估计的参数选择优化提供了理论基础。

Conclusion: 该研究为雷达系统、通信网络和智能传感技术等实际应用中的DOA估计参数优化提供了坚实的理论支撑。

Abstract: This paper presents a systematic theoretical performance analysis of the
Real-Valued root-MUSIC (RV-root-MUSIC) algorithm under non-asymptotic
conditions. However, RV-root-MUSIC suffers from the problem of estimation
ambiguity for the mirror roots, therefore the conventional beamforming (CBF)
technique is typically employed to filter out the mirror roots. Through the
equivalent subspace based on the conjugate extension method and the equivalence
of perturbations for both true roots and mirror roots , this paper provides a
comprehensive investigation of three critical aspects: noise subspace
perturbation, true root perturbation, and mirror root perturbation
characteristics in the RV-root-MUSIC algorithm. The statistical model is
established and the generalized expression of perturbation is further
developed. The simulation results show the correctness and validity of the
derived statistical characteristics. The results provide a solid theoretical
foundation for optimizing the parameter selection of DOA estimation in
practical applications, particularly in radar systems, communication networks,
and intelligent sensing technologies.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [211] [KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache](https://arxiv.org/abs/2509.00579)
*Bo Jiang,Taolue Yang,Youyuan Liu,Chengming Zhang,Xubin He,Sian Jin*

Main category: cs.DC

TL;DR: KVComp是一个针对长文本生成的KV缓存管理框架，通过创新的有损压缩技术显著减少内存使用，同时保持计算效率和模型精度。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批量大小的增加，Transformer大语言模型中的KV缓存内存需求可达数GB，成为长上下文推理的主要挑战。

Method: 采用专门为KV缓存数据特征设计的损失压缩技术，结合压缩算法和系统架构的协同设计，保持与KV缓存增长特性的兼容性。

Result: 平均实现47%的内存减少率，最高可达83%，几乎没有模型精度损失，执行吞吐量极高，在某些情况下甚至加速矩阵向量乘法操作。

Conclusion: KVComp提供了一个通用高效的KV缓存管理解决方案，在长文本生成场景中显著优于现有方法，具有良好的实用价值。

Abstract: Transformer-based large language models (LLMs) demonstrate impressive
potential in various practical applications. However, long context inference
poses a significant challenge due to the enormous memory requirements of the
key-value (KV) cache, which can scale to multiple gigabytes as sequence length
and batch size increase. In this paper, we present KVComp, a generic and
efficient KV cache management framework optimized for long-text generation that
synergistically works with both latency-critical and throughput-critical
inference systems. KVComp employs novel lossy compression techniques
specifically designed for KV cache data characteristics, featuring careful
co-design of compression algorithms and system architecture. Our approach
maintains compatibility with the growing nature of KV cache while preserving
high computational efficiency. Experimental results show that KVComp achieves
on average 47\% and up to 83\% higher memory reduction rate compared to
existing methods with little/no model accuracy degradation. Furthermore, KVComp
achieves extremely high execution throughput, effectively reducing
decompression overhead and, in some cases, even accelerating the matrix-vector
multiplication operation and outperform cuBLAS-based attention kernels with
less data movement.

</details>


### [212] [HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation](https://arxiv.org/abs/2509.00642)
*Qizheng Yang,Tung-I Chen,Siyu Zhao,Ramesh K. Sitaraman,Hui Guan*

Main category: cs.DC

TL;DR: HADIS是一个混合自适应扩散模型服务系统，通过智能查询路由和资源配置优化，在提升响应质量35%的同时大幅降低延迟违规率


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型计算成本高，固定级联配置系统对所有查询都经过轻量级阶段，浪费复杂查询资源

Method: 使用基于规则的提示路由器直接处理困难查询，离线分析生成帕累托最优级联配置表，运行时根据延迟和工作负载约束选择最佳配置

Result: 在真实世界轨迹评估中，响应质量提升高达35%，延迟违规率降低2.7-45倍

Conclusion: HADIS系统通过自适应路由和资源配置优化，有效解决了扩散模型服务的高成本和延迟问题

Abstract: Text-to-image diffusion models have achieved remarkable visual quality but
incur high computational costs, making real-time, scalable deployment
challenging. Existing query-aware serving systems mitigate the cost by
cascading lightweight and heavyweight models, but most rely on a fixed cascade
configuration and route all prompts through an initial lightweight stage,
wasting resources on complex queries. We present HADIS, a hybrid adaptive
diffusion model serving system that jointly optimizes cascade model selection,
query routing, and resource allocation. HADIS employs a rule-based prompt
router to send clearly hard queries directly to heavyweight models, bypassing
the overhead of the lightweight stage. To reduce the complexity of resource
management, HADIS uses an offline profiling phase to produce a Pareto-optimal
cascade configuration table. At runtime, HADIS selects the best cascade
configuration and GPU allocation given latency and workload constraints.
Empirical evaluations on real-world traces demonstrate that HADIS improves
response quality by up to 35% while reducing latency violation rates by
2.7-45$\times$ compared to state-of-the-art model serving systems.

</details>


### [213] [Accelerating Latency-Critical Applications with AI-Powered Semi-Automatic Fine-Grained Parallelization on SMT Processors](https://arxiv.org/abs/2509.00883)
*Denis Los,Igor Petushkov*

Main category: cs.DC

TL;DR: Aira是一个AI驱动的并行化顾问系统，利用SMT技术对延迟关键应用进行细粒度并行化，通过扩展Cursor IDE的AI编码代理，结合热点检测、动态依赖收集和性能模拟等工具，实现了17%的平均性能提升。


<details>
  <summary>Details</summary>
Motivation: 延迟关键应用在高性能超标量处理器中由于频繁缓存缺失和预测错误导致功能单元利用率低，而SMT技术很少用于这类重线程应用，因此需要探索利用SMT技术支持细粒度并行化。

Method: 开发Aira AI并行化顾问，扩展Cursor IDE的AI编码代理，通过模型上下文协议连接额外工具，包括LLM引导的热点检测、动态二进制插桩收集动态依赖、SMT感知性能模拟，并结合Relic并行框架实现SMT核上的细粒度任务并行。

Result: 在代表工业实际应用的延迟关键基准测试中，使用Aira和Relic框架进行并行化，实现了17%的几何平均性能增益。

Conclusion: Aira系统成功证明了AI驱动的并行化方法能够有效利用SMT技术提升延迟关键应用的性能，为这类应用的优化提供了新的解决方案。

Abstract: Latency-critical applications tend to show low utilization of functional
units due to frequent cache misses and mispredictions during speculative
execution in high-performance superscalar processors. However, due to
significant impact on single-thread performance, Simultaneous Multithreading
(SMT) technology is rarely used with heavy threads of latency-critical
applications. In this paper, we explore utilization of SMT technology to
support fine-grained parallelization of latency-critical applications.
Following the advancements in the development of Large Language Models (LLMs),
we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we
extend AI Coding Agent in Cursor IDE with additional tools connected through
Model Context Protocol, enabling end-to-end AI Agent for parallelization.
Additional connected tools enable LLM-guided hotspot detection, collection of
dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance
simulation to estimate performance gains. We apply Aira with Relic parallel
framework for fine-grained task parallelism on SMT cores to parallelize
latency-critical benchmarks representing real-world applications used in
industry. We show 17% geomean performance gain from parallelization of
latency-critical benchmarks using Aira with Relic framework.

</details>


### [214] [Optimal Parallel Scheduling under Concave Speedup Functions](https://arxiv.org/abs/2509.01811)
*Chengzhang Li,Peizhong Ju,Atilla Eryilmaz,Ness Shroff*

Main category: cs.DC

TL;DR: 这篇论文解决了并行作业调度的一个长期未解问题，提出了一种可以处理任意凹速度函数的最优调度算法SmartFill，超越了以往只能处理特定函数形式的方法。


<details>
  <summary>Details</summary>
Motivation: 现代云计算/边缘计算系统中多个AI应用的并行计算资源调度是核心问题。以往的heSRPT方法只能处理特定的指数形式速度函数，而实际工作负载更需要处理任意凹函数，这个普遍情况一直是个未解问题。

Method: 首先发现了最优并行调度的基本规则（CDR规则）：活跃作业速度函数导数比例在时间上保持常数。提出了普通水滩填充方法（GWF）来高效计算满足CDR规则的最优分配。组合这些见解设计了SmartFill算法，能够选择性地决定哪些作业应获得资源及分配量。

Result: 对于广泛的正规速度函数，SmartFill能够得出闭式最优解；对于非正规函数，也能高效地计算最优解。数值评估显示，SmartFill在广泛的凹速度函数范围内都显著超过了heSRPT方法。

Conclusion: 该研究成功解决了并行计算资源调度的普遍问题，提出的CDR规则和SmartFill算法为现代计算系统提供了更为高效和普遍适用的调度方案，对于云计算和边缘计算领域具有重要意义。

Abstract: Efficient scheduling of parallel computation resources across multiple jobs
is a fundamental problem in modern cloud/edge computing systems for many
AI-based applications. Allocating more resources to a job accelerates its
completion, but with diminishing returns. Prior work (heSRPT) solved this
problem only for some specific speedup functions with an exponential form,
providing a closed-form solution. However, the general case with arbitrary
concave speedup functions -- which more accurately capture real-world workloads
-- has remained open.
  In this paper, we solve this open problem by developing optimal scheduling
algorithms for parallel jobs under general concave speedup functions. We first
discover a fundamental and broadly-applicable rule for optimal parallel
scheduling, namely the Consistent Derivative Ratio (CDR) Rule, which states
that the ratio of the derivatives of the speedup functions across active jobs
remains constant over time. To efficiently compute the optimal allocations that
satisfy the CDR Rule, we propose the General Water-Filling (GWF) method, a more
general version of classical water-filling in wireless communications.
Combining these insights, we design the SmartFill Algorithm to solve the
general scheduling problem. Unlike heSRPT, which always allocates resources to
all active jobs, SmartFill selectively determines which jobs should receive
resources and how much they should be allocated. For a broad class of so-called
\emph{regular} speedup functions, SmartFill yields closed-form optimal
solutions, while for non-regular functions it efficiently computes the optimum
with low complexity. Numerical evaluations show that SmartFill can
substantially outperform heSRPT across a wide range of concave speedup
functions.

</details>


### [215] [Parallelizing Drug Discovery: HPC Pipelines for Alzheimer's Molecular Docking and Simulation](https://arxiv.org/abs/2509.00937)
*Paul Ruiz Alliata,Diana Rubaga,Daniel Kumlin,Alberto Puliga*

Main category: cs.DC

TL;DR: 本文探讨了高性能计算在阿尔茨海默病药物发现中的应用，通过并行化工作流程和分子对接原型，显著提升了虚拟筛选、分子对接和分子动力学模拟的效率。


<details>
  <summary>Details</summary>
Motivation: 利用高性能计算技术解决阿尔茨海默病药物发现中的计算瓶颈，通过大规模并行计算加速分子模拟过程，提高药物研发效率。

Method: 采用GROMACS工具结合混合MPI-OpenMP策略实现并行化工作流程，包括能量最小化、平衡和生产阶段；使用Python多进程库开发并行对接原型。

Result: 在能量最小化、平衡和生产阶段展示了良好的扩展性能；对接原型从顺序执行转向基于进程的并行化后获得显著运行时增益；原酰胺衍生物和黄芩素案例研究证实了工作流程的生物学相关性。

Conclusion: 尽管在数据管理、计算成本和扩展效率方面仍存在限制，但研究结果强调了高性能计算在加速神经退行性疾病药物发现方面的巨大潜力。

Abstract: High-performance computing (HPC) is reshaping computational drug discovery by
enabling large-scale, time-efficient molecular simulations. In this work, we
explore HPC-driven pipelines for Alzheimer's disease drug discovery, focusing
on virtual screening, molecular docking, and molecular dynamics simulations. We
implemented a parallelised workflow using GROMACS with hybrid MPI-OpenMP
strategies, benchmarking scaling performance across energy minimisation,
equilibration, and production stages. Additionally, we developed a docking
prototype that demonstrates significant runtime gains when moving from
sequential execution to process-based parallelism using Python's
multiprocessing library. Case studies on prolinamide derivatives and baicalein
highlight the biological relevance of these workflows in targeting amyloid-beta
and tau proteins. While limitations remain in data management, computational
costs, and scaling efficiency, our results underline the potential of HPC to
accelerate neurodegenerative drug discovery.

</details>


### [216] [Energy-Efficient Split Learning for Resource-Constrained Environments: A Smart Farming Solution](https://arxiv.org/abs/2509.02549)
*Keiwan Soltani,Vishesh Kumar Tanwar,Ashish Gupta,Sajal K. Das*

Main category: cs.DC

TL;DR: eEnergy-Split是一个基于分割学习的高效能框架，通过在边缘设备和中央服务器之间分配模型训练，显著降低能耗（比联邦学习节省86%），同时保护数据隐私并提高分类精度。


<details>
  <summary>Details</summary>
Motivation: 解决智能农业系统面临的资源有限、数据隐私需求和农村地区连接性差等挑战，需要一种既能保护隐私又能高效利用能源的协作学习方案。

Method: 采用分割学习(SL)技术，将模型分布在边缘设备和中央服务器之间；提出最优边缘部署算法和基于TSP精确求解的无人机轨迹规划策略，以最小化飞行成本并最大化通信轮次。

Result: 相比联邦学习，能耗降低86%，分类精度提升6.2%（ResNet-18）；在农业害虫数据集上总体精度提升17%，无人机能耗显著降低。轻量级模型如MobileNet节能效果更显著。

Conclusion: 分割学习与能源感知设计相结合，可为资源受限的智能农业环境提供可扩展、隐私保护的解决方案，但能效提升具有模型依赖性。

Abstract: Smart farming systems encounter significant challenges, including limited
resources, the need for data privacy, and poor connectivity in rural areas. To
address these issues, we present eEnergy-Split, an energy-efficient framework
that utilizes split learning (SL) to enable collaborative model training
without direct data sharing or heavy computation on edge devices. By
distributing the model between edge devices and a central server, eEnergy-Split
reduces on-device energy usage by up to 86 percent compared to federated
learning (FL) while safeguarding data privacy. Moreover, SL improves
classification accuracy by up to 6.2 percent over FL on ResNet-18 and by more
modest amounts on GoogleNet and MobileNetV2. We propose an optimal edge
deployment algorithm and a UAV trajectory planning strategy that solves the
Traveling Salesman Problem (TSP) exactly to minimize flight cost and extend and
maximize communication rounds. Comprehensive evaluations on agricultural pest
datasets reveal that eEnergy-Split lowers UAV energy consumption compared to
baseline methods and boosts overall accuracy by up to 17 percent. Notably, the
energy efficiency of SL is shown to be model-dependent-yielding substantial
savings in lightweight models like MobileNet, while communication and memory
overheads may reduce efficiency gains in deeper networks. These results
highlight the potential of combining SL with energy-aware design to deliver a
scalable, privacy-preserving solution for resource-constrained smart farming
environments.

</details>


### [217] [DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving](https://arxiv.org/abs/2509.01083)
*Mingyu Yang,Jae-Young Choi,Kihyo Moon,Minsung Jang,Eunjoo Joen*

Main category: cs.DC

TL;DR: 提出DSDE框架，使用KLD方差作为预测信号动态调整推测解码长度，在批量服务环境中实现更优的延迟和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统推测解码使用固定推测长度，在批量服务环境中面对多样化请求时表现不佳，需要动态适应机制

Method: DSDE框架包含两个核心组件：基于KLD方差的预测信号诊断生成稳定性，以及自适应推测长度上限解决序列解码中的滞后问题

Result: 实验显示基于KLD稳定性信号的算法在端到端延迟上与领先基线竞争，在多样化工作负载下表现出优越的鲁棒性，特别是在低接受率场景中

Conclusion: 后验信号是构建更鲁棒智能LLM推理系统的重要组件，为动态推测长度适应提供了有前景的研究方向

Abstract: Speculative decoding accelerates large language model inference, but its
reliance on a fixed speculation length is suboptimal in large-batch serving
environments with diverse requests. This paper explores a new direction for
dynamic adaptation by investigating a novel class of post-hoc, diagnostic
signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free
framework built on two primary components: (1) a predictive signal based on the
variance of the Kullback-Leibler (KLD) divergence, which diagnoses the
generation's regional stability, and (2) an adaptive speculation length cap to
mitigate the straggler problem in per-sequence decoding. Experiments
demonstrate the potential of using KLD-based stability signals for dynamic
adaptation. An algorithm guided by these signals achieves end-to-end latency
competitive with leading baselines and exhibits superior robustness across
diverse workloads. This robustness is particularly valuable in challenging
low-acceptance-rate regimes, where the proposed signal maintains its diagnostic
utility. Collectively, these findings validate post-hoc signals as a valuable
component for building more robust and intelligent LLM inference systems, and
highlight a promising direction for future research on dynamic speculation
length adaptation.

</details>


### [218] [Safe Memory Reclamation Techniques](https://arxiv.org/abs/2509.02457)
*Ajay Singh*

Main category: cs.DC

TL;DR: 这篇论文研究了非垃圾回收编程语言中乐观和无锁并发数据结构的安全内存回收技术，提出了跨层面的解决方案来应对高性能、易用性和内存占用等挑战。


<details>
  <summary>Details</summary>
Motivation: 在非垃圾回收编程语言中，安全内存回收对于乐观和无锁并发数据结构的内存安全至关重要。但设计理想的安全内存回收算法面临着多重挑战，包括实现高速度和可扩展性、程序员易用性、广泛适用性、管理大内存占用以及避免不对称性开销等。

Method: 研究了多种设计安全内存回收算法的方法，通过融合硬件-软件栈中的思想和工具。这些解决方案跨越传统界限，利用了不同层面曒露的特性。

Result: 论文提出了一系列跨层面的安全内存回收解决方案，能够在硬件-软件栈的不同层面中寻找最优解。

Conclusion: 通过跨层面的方法设计安全内存回收算法，可以更有效地应对并发数据结构中的内存安全挑战，实现高性能、可扩展性和易用性的绝合。

Abstract: Safe memory reclamation is crucial to memory safety for optimistic and
lock-free concurrent data structures in non garbage collected programming
languages. However, several challenges arise in designing an ideal safe memory
reclamation algorithm, including achieving high speed and scalability, easy of
use for programmers, applicability to wide class of data structures, managing
the large memory footprint caused by delayed freeing of memory for safety and
performance, and avoiding asymmetric overhead on data structure operations.
Several approaches to designing safe memory reclamation algorithms are studied
by blending ideas and tools from across the hardware-software stack. These
solutions cross traditional boundaries and exploit features exposed at
different layers.

</details>


### [219] [Ocior: Ultra-Fast Asynchronous Leaderless Consensus with Two-Round Finality, Linear Overhead, and Adaptive Security](https://arxiv.org/abs/2509.01118)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: Ocior是一个实用的异步拜占庭容错共识协议，在容错性、通信、计算和轮次复杂度方面达到最优性能。它是一个无领导者的共识协议，通过并行实例处理交易，并引入了新型非交互式阈值签名方案OciorBLSts。


<details>
  <summary>Details</summary>
Motivation: 传统BFT共识协议依赖指定领导者提出交易，存在性能瓶颈。需要开发一个无领导者的共识协议，在异步环境下实现最优性能，同时保证稳定的活跃性。

Method: 采用并行共识实例处理交易的无领导者架构，引入新型非交互式阈值签名方案OciorBLSts，支持快速签名聚合和实时聚合功能。

Result: 实现了：1) 最优容错性：容忍t个故障节点(n≥3t+1)；2) 最优通信复杂度：每交易O(n)；3) 最优计算复杂度：最好情况O(n)，最坏情况O(n log²n)；4) 最优轮次复杂度：两轮异步回合完成交易最终确认。

Conclusion: Ocior协议在异步BFT共识领域实现了多项最优性能指标，通过创新的无领导者架构和高效的阈值签名方案，为分布式系统提供了高性能的共识解决方案。

Abstract: In this work, we propose Ocior, a practical asynchronous Byzantine
fault-tolerant (BFT) consensus protocol that achieves the optimal performance
in resilience, communication, computation, and round complexity. Unlike
traditional BFT consensus protocols, Ocior processes incoming transactions
individually and concurrently using parallel instances of consensus. While
leader-based consensus protocols rely on a designated leader to propose
transactions, Ocior is a leaderless consensus protocol that guarantees stable
liveness. Ocior achieves: 1) Optimal resilience: Ocior tolerates up to $t$
faulty nodes controlled by an adaptive adversary, for $n\geq 3t+1$. 2) Optimal
communication complexity: The total expected communication per transaction is
$O(n)$. 3) Optimal (or near-optimal) computation complexity: The total
computation per transaction is $O(n)$ in the best case, or $O(n \log^2 n)$ in
the worst case. 4) Optimal round complexity: A legitimate two-party transaction
can be finalized with a good-case latency of two asynchronous rounds, for any
$n\geq 3t+1$. The good case in terms of latency refers to the scenario where
the transaction is proposed by any (not necessarily designated) honest node. A
two-party transaction involves the transfer of digital assets from one user (or
group of users) to one or more recipients. To support efficient consensus, we
introduce a novel non-interactive threshold signature (TS) scheme called
OciorBLSts. It offers fast signature aggregation, and is adaptively secure.
OciorBLSts achieves a signature aggregation computation cost of only $O(n)$ for
the best case. Moreover, OciorBLSts supports the property of Instantaneous TS
Aggregation. This enables real-time aggregation of partial signatures as they
arrive, reducing waiting time and improving responsiveness.

</details>


### [220] [Detecting Rug Pulls in Decentralized Exchanges: Machine Learning Evidence from the TON Blockchain](https://arxiv.org/abs/2509.01168)
*Dmitry Yaremus,Jianghai Li,Alisa Kalacheva,Igor Vodolazov,Yury Yanovich*

Main category: cs.DC

TL;DR: 提出了一个机器学习框架，用于在TON区块链上去中心化交易所早期检测rug pull诈骗，比较了两种rug pull定义方法，在交易前5分钟内实现有效检测。


<details>
  <summary>Details</summary>
Motivation: TON区块链独特的异步架构和Telegram带来的海量web2用户基础，为欺诈分析提供了新颖且关键的环境，需要保护投资者并增强TON DeFi生态安全。

Method: 在TON两大DEX平台Ston.Fi和DeDust上融合数据训练模型，比较TVL-based（流动性撤出）和idle-based（交易活动停止）两种rug pull定义方法，使用梯度提升模型进行检测。

Result: 梯度提升模型能在交易前5分钟内有效识别rug pull，TVL-based方法AUC最高达0.891，idle-based方法在召回率方面表现更优。不同交易所特征分布差异显著。

Conclusion: 该研究为投资者提供了关键的早期预警机制，增强了快速发展的TON DeFi生态系统的安全基础设施，证明了机器学习在区块链欺诈检测中的有效性。

Abstract: This paper presents a machine learning framework for the early detection of
rug pull scams on decentralized exchanges (DEXs) within The Open Network (TON)
blockchain. TON's unique architecture, characterized by asynchronous execution
and a massive web2 user base from Telegram, presents a novel and critical
environment for fraud analysis. We conduct a comprehensive study on the two
largest TON DEXs, Ston.Fi and DeDust, fusing data from both platforms to train
our models. A key contribution is the implementation and comparative analysis
of two distinct rug pull definitions--TVL-based (a catastrophic liquidity
withdrawal) and idle-based (a sudden cessation of all trading activity)--within
a single, unified study. We demonstrate that Gradient Boosting models can
effectively identify rug pulls within the first five minutes of trading, with
the TVL-based method achieving superior AUC (up to 0.891) while the idle-based
method excels at recall. Our analysis reveals that while feature sets are
consistent across exchanges, their underlying distributions differ
significantly, challenging straightforward data fusion and highlighting the
need for robust, platform-aware models. This work provides a crucial
early-warning mechanism for investors and enhances the security infrastructure
of the rapidly growing TON DeFi ecosystem.

</details>


### [221] [LobRA: Multi-tenant Fine-tuning over Heterogeneous Data](https://arxiv.org/abs/2509.01193)
*Sheng Lin,Fangcheng Fu,Haoyang Li,Hao Ge,Xuanyu Wang,Jiawen Niu,Yaofeng Tu,Bin Cui*

Main category: cs.DC

TL;DR: LobRA是一个针对多任务LoRA微调优化的框架，通过处理序列长度变化和偏斜的异构性问题，显著提升联合微调效率


<details>
  <summary>Details</summary>
Motivation: 随着Transformer预训练模型的突破，微调需求激增，需要降低处理多个微调任务的成本。LoRA技术虽然支持共享基础模型联合训练多个适配器，但存在序列长度变化和偏斜的异构性问题影响效率

Method: 1) 部署具有异构资源使用和并行配置的微调副本，匹配序列长度变化带来的不同工作负载；2) 在每个训练步骤中，考虑序列长度偏斜，在异构微调副本间分发训练数据以实现工作负载平衡

Result: 实验验证LobRA显著减少了联合微调所需的GPU时间，降低了45.03%-60.67%

Conclusion: LobRA通过创新的异构资源部署和智能数据分发策略，有效解决了多任务LoRA微调中的效率瓶颈问题

Abstract: With the breakthrough of Transformer-based pre-trained models, the demand for
fine-tuning (FT) to adapt the base pre-trained models to downstream
applications continues to grow, so it is essential for service providers to
reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely
used FT technique that only trains small-scale adapters and keeps the base
model unaltered, conveying the possibility of processing multiple FT tasks by
jointly training different LoRA adapters with a shared base model.
  Nevertheless, through in-depth analysis, we reveal the efficiency of joint FT
is dampened by two heterogeneity issues in the training data -- the sequence
length variation and skewness. To tackle these issues, we develop LobRA, a
brand new framework that supports processing multiple FT tasks by jointly
training LoRA adapters. Two innovative designs are introduced. Firstly, LobRA
deploys the FT replicas (i.e., model replicas for FT) with heterogeneous
resource usages and parallel configurations, matching the diverse workloads
caused by the sequence length variation. Secondly, for each training step,
LobRA takes account of the sequence length skewness and dispatches the training
data among the heterogeneous FT replicas to achieve workload balance. We
conduct experiments to assess the performance of LobRA, validating that it
significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.

</details>


### [222] [LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM Serving](https://arxiv.org/abs/2509.01229)
*Huanqi Hu,Bowen Xiao,Shixuan Sun,Jianian Yin,Zhexi Zhang,Xiang Luo,Chengquan Jiang,Weiqi Xu,Xiaoying Jia,Xin Liu,Minyi Guo*

Main category: cs.DC

TL;DR: LiquidGEMM是一个硬件高效的W4A8 GEMM内核，通过LiquidQuant量化方法和隐式细粒度流水线技术，实现了比现有方案最高2.9倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的W4A8 GEMM内核在实践中效率不足，主要因为CUDA核心上的反量化操作无法跟上Tensor核心的高吞吐量，限制了LLM推理的加速效果。

Method: 提出了LiquidQuant量化方法，每4个元素仅需两条算术指令即可实现快速、溢出安全的反量化；设计了隐式细粒度流水线，在warp组间完全重叠权重加载、反量化和矩阵乘法运算，无需软件同步或冗余内存访问。

Result: 实验结果显示，LiquidGEMM相比最先进的W4A8内核实现了最高2.90倍的加速，端到端系统级加速最高达4.94倍；相比NVIDIA TensorRT-LLM中的各种量化GEMM内核，性能提升1.12-1.63倍，系统级加速最高1.63倍。

Conclusion: LiquidGEMM通过硬件优化的量化方法和高效的流水线设计，显著提升了W4A8量化在LLM推理中的实际性能，为高效LLM服务提供了有效的解决方案。

Abstract: Quantization is a critical technique for accelerating LLM inference by
reducing memory footprint and improving computational efficiency. Among various
schemes, 4-bit weight and 8-bit activation quantization (W4A8) offers a strong
balance between accuracy and performance. However, existing W4A8 GEMM kernels
fall short in practice due to inefficient dequantization on CUDA Cores, which
cannot keep pace with the high throughput of Tensor Cores. In this paper, we
present LiquidGEMM, a hardware-efficient W4A8 GEMM kernel for efficient LLM
serving. LiquidGEMM designs two key techniques: LiquidQuant, a
hardware-efficient quantization method that enables fast, overflow-safe
dequantization using just two arithmetic instructions per four elements; and an
implicit fine-grained pipeline that fully overlaps weight loading,
dequantization, and MMA across warp groups without software synchronization or
redundant memory traffic. Experimental results show that LiquidGEMM achieves up
to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x end-to-end
system-level speedup. Compared to various quantized GEMM kernels in NVIDIA
TensorRT-LLM, LiquidGEMM delivers 1.12-1.63x performance gains, and achieves up
to 1.63x system-level speedup.

</details>


### [223] [HiCR, an Abstract Model for Distributed Heterogeneous Programming](https://arxiv.org/abs/2509.01425)
*Sergio Miguel Martin,Luca Terracciano,Kiril Dichev,Noah Baumann,Jiashu Lin,Albert-Jan Yzelman*

Main category: cs.DC

TL;DR: HiCR是一个用于表示分布式异构应用程序和运行时系统语义的模型，提供硬件拓扑发现、内核执行、内存管理、通信和实例管理等抽象操作，位于分布式异构系统和运行时系统之间的抽象层。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式异构系统在不同硬件平台上需要大量重构的问题，提供一个统一的抽象模型来支持当前和未来系统的执行，同时服务于各种并行编程范式。

Method: 采用基于插件的方法实现模型组件和操作，处理设备特定的实现细节，定义最小化的抽象操作集而不规定具体实现决策。

Result: 开发了HiCR模型并实现了基于该模型的应用程序，这些应用能够在多样化的平台上同等运行。

Conclusion: HiCR作为运行时支持层抽象，成功实现了跨平台兼容性，为分布式异构应用提供了统一的语义表示框架。

Abstract: We present HiCR, a model to represent the semantics of distributed
heterogeneous applications and runtime systems. The model describes a minimal
set of abstract operations to enable hardware topology discovery, kernel
execution, memory management, communication, and instance management, without
prescribing any implementation decisions. The goal of the model is to enable
execution in current and future systems without the need for significant
refactoring, while also being able to serve any governing parallel programming
paradigm. In terms of software abstraction, HiCR is naturally located between
distributed heterogeneous systems and runtime systems. We coin the phrase
\emph{Runtime Support Layer} for this level of abstraction. We explain how the
model's components and operations are realized by a plugin-based approach that
takes care of device-specific implementation details, and present examples of
HiCR-based applications that operate equally on a diversity of platforms.

</details>


### [224] [STZ: A High Quality and High Speed Streaming Lossy Compression Framework for Scientific Data](https://arxiv.org/abs/2509.01626)
*Daoce Wang,Pascal Grosset,Jesus Pulido,Jiannan Tian,Tushar M. Athawale,Jinda Jia,Baixi Sun,Boyuan Zhang,Sian Jin,Kai Zhao,James Ahrens,Fengguang Song*

Main category: cs.DC

TL;DR: 提出了一种支持渐进式解压缩和随机访问解压缩的新型流式压缩框架，在保持高压缩质量和速度的同时解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学数据量庞大，误差有界有损压缩是减少数据量的有效方案。渐进式解压缩和随机访问解压缩对按需数据访问和灵活分析工作流至关重要，但这些特性会严重降低压缩质量和速度。

Method: 设计了首个同时支持渐进式解压缩和随机访问解压缩的压缩框架，采用分层分区策略实现流式特性，并通过分层预测机制减轻分区影响，达到高压缩质量。

Result: 压缩质量可与最先进的非流式压缩器SZ3相媲美，压缩和解压缩速度比SZ3快达6.7倍。

Conclusion: 该框架成功解决了流式压缩中质量与速度的权衡问题，为科学数据压缩提供了高效解决方案。

Abstract: Error-bounded lossy compression is one of the most efficient solutions to
reduce the volume of scientific data. For lossy compression, progressive
decompression and random-access decompression are critical features that enable
on-demand data access and flexible analysis workflows. However, these features
can severely degrade compression quality and speed. To address these
limitations, we propose a novel streaming compression framework that supports
both progressive decompression and random-access decompression while
maintaining high compression quality and speed. Our contributions are
three-fold: (1) we design the first compression framework that simultaneously
enables both progressive decompression and random-access decompression; (2) we
introduce a hierarchical partitioning strategy to enable both streaming
features, along with a hierarchical prediction mechanism that mitigates the
impact of partitioning and achieves high compression quality -- even comparable
to state-of-the-art (SOTA) non-streaming compressor SZ3; and (3) our framework
delivers high compression and decompression speed, up to 6.7$\times$ faster
than SZ3.

</details>


### [225] [A Continuous Energy Ising Machine Leveraging Difference-of-Convex Programming](https://arxiv.org/abs/2509.01928)
*Debraj Banerjee,Santanu Mahapatra,Kunal Narayan Chaudhury*

Main category: cs.DC

TL;DR: 提出一种基于连续松弛和吸引子势能函数的Ising模型求解器，通过凸差分解实现高效迭代算法，在GPU平台上从边缘设备到高性能集群均优于现有求解器


<details>
  <summary>Details</summary>
Motivation: 现有Ising求解器主要基于模拟退火，虽然可扩展但缺乏收敛保证且对冷却调度敏感，需要更可靠高效的求解方法

Method: 将二元自旋松弛为连续变量，引入吸引子势能函数引导解趋向二元配置，将哈密顿量表达为凸差函数，设计高效迭代算法（每迭代仅需一次矩阵向量乘法）

Result: 在多种GPU平台（从边缘设备到高性能集群）上实现，在10^3到10^8自旋的问题规模上 consistently 优于现有求解器

Conclusion: 该方法提供了具有收敛保证的高效Ising求解方案，解决了传统退火方法的局限性，在大规模组合优化问题中表现出色

Abstract: Many combinatorial optimization problems can be reformulated as the task of
finding the ground state of a physical system, such as the Ising model. Most
existing Ising solvers are inspired by simulated annealing. Although annealing
techniques offer scalability, they lack convergence guarantees and are
sensitive to the cooling schedule. We propose to solve the Ising problem by
relaxing the binary spins to continuous variables and introducing a potential
function (attractor) that steers the solution toward binary spin
configurations. The resulting Hamiltonian can be expressed as a difference of
convex functions, enabling the design of efficient iterative algorithms that
require a single matrix-vector multiplication per iteration and are backed by
convergence guarantees. We implement our Ising solver across a range of GPU
platforms: from edge devices to high-performance computing clusters and
demonstrate that it consistently outperforms existing solvers across problem
sizes ranging from small ($10^3$ spins) to ultra-large ($10^8$ spins).

</details>


### [226] [Fault-Tolerant Decentralized Distributed Asynchronous Federated Learning with Adaptive Termination Detection](https://arxiv.org/abs/2509.02186)
*Phani Sahasra Akkinepally,Manaswini Piduguralla,Sushant Joshi,Sathya Peri,Sandeep Kulkarni*

Main category: cs.DC

TL;DR: 这篇论文提出了一种异步分布式联邦学习方法，通过两阶段设计解决了同步方案的延迟问题和客户端故障处理难题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖中央服务器，存在性能瓶颈和单点故障风险。同步FL需要所有客户端等待互相，在异质性环境中容易出现延迟。

Method: 分为两个步骤：第一阶段开发异步FL框架，允许客户端独立学习和更新；第二阶段添加故障容锐机制处理客户端故障和消息丢失。提出客户端自信收敛和客户端响应终止新技术。

Result: 该方法能够在异步通信和故障条件下保证可靠的模型收敛，所有活跃客户端都能有效地完成训练。

Conclusion: 异步分布式FL框架提供了更好的可扩展性和响应能力，同时通过故障容锐机制确保了系统在不可预测条件下的稳健性能。

Abstract: Federated Learning (FL) facilitates collaborative model training across
distributed clients while ensuring data privacy. Traditionally, FL relies on a
centralized server to coordinate learning, which creates bottlenecks and a
single point of failure. Decentralized FL architectures eliminate the need for
a central server and can operate in either synchronous or asynchronous modes.
Synchronous FL requires all clients to compute updates and wait for one another
before aggregation, guaranteeing consistency but often suffering from delays
due to slower participants. Asynchronous FL addresses this by allowing clients
to update independently, offering better scalability and responsiveness in
heterogeneous environments.
  Our research develops an asynchronous decentralized FL approach in two
progressive phases. (a) In Phase 1, we develop an asynchronous FL framework
that enables clients to learn and update independently, removing the need for
strict synchronization. (b) In Phase 2, we extend this framework with fault
tolerance mechanisms to handle client failures and message drops, ensuring
robust performance even under unpredictable conditions. As a central
contribution, we propose Client-Confident Convergence and Client-Responsive
Termination novel techniques that provide each client with the ability to
autonomously determine appropriate termination points. These methods ensure
that all active clients conclude meaningfully and efficiently, maintaining
reliable convergence despite the challenges of asynchronous communication and
faults.

</details>


### [227] [Near-Optimal Stability for Distributed Transaction Processing in Blockchain Sharding](https://arxiv.org/abs/2509.02421)
*Ramesh Adhikari,Costas Busch,Dariusz R. Kowalski*

Main category: cs.DC

TL;DR: 这篇论文研究了区块链分片中的系统稳定性问题，提出了两种调度器算法，在最坏情况和DoS攻击下保证事务队列大小和延迟有界。


<details>
  <summary>Details</summary>
Motivation: 解决区块链分片系统在高载和攻击情况下的稳定性问题，确保事务处理的可靠性和性能。

Method: 提出了两种调度器：单领导者调度器和分布式多领导者调度器，通过理论分析确定了系统稳定的注入速率上界。

Result: 单领导者调度器在注入速率≤max{1/(16k), 1/(16⎚√s⎛)}时保证稳定；分布式调度器在注入速率≤1/(16c1·logD·logs)·max{1/k, 1/⎚√s⎛}时保证稳定，这个结果远超过之前的最佳结果。

Conclusion: 该研究在区块链分片系统稳定性方面取得了重要进展，提供的调度器算法能够在近优注入速率下保证系统稳定，对于建立可靠的区块链分片系统具有重要意义。

Abstract: In blockchain sharding, $n$ processing nodes are divided into $s$ shards, and
each shard processes transactions in parallel. A key challenge in such a system
is to ensure system stability for any ``tractable'' pattern of generated
transactions; this is modeled by an adversary generating transactions with a
certain rate of at most $\rho$ and burstiness $b$. This model captures
worst-case scenarios and even some attacks on transactions' processing, e.g.,
DoS. A stable system ensures bounded transaction queue sizes and bounded
transaction latency. It is known that the absolute upper bound on the maximum
injection rate for which any scheduler could guarantee bounded queues and
latency of transactions is $\max\left\{ \frac{2}{k+1}, \frac{2}{
\left\lfloor\sqrt{2s}\right\rfloor}\right\}$, where $k$ is the maximum number
of shards that each transaction accesses. Here, we first provide a single
leader scheduler that guarantees stability under injection rate $\rho \leq
\max\left\{ \frac{1}{16k}, \frac{1}{16\lceil \sqrt{s} \rceil}\right\}$.
Moreover, we also give a distributed scheduler with multiple leaders that
guarantees stability under injection rate $\rho \leq \frac{1}{16c_1 \log D \log
s}\max\left\{ \frac{1}{k}, \frac{1}{\lceil \sqrt{s} \rceil} \right\}$, where
$c_1$ is some positive constant and $D$ is the diameter of shard graph $G_s$.
This bound is within a poly-log factor from the optimal injection rate, and
significantly improves the best previous known result for the distributed
setting by Adhikari et al., SPAA 2024.

</details>


### [228] [Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster](https://arxiv.org/abs/2509.02440)
*Marie Reinbigler,Rishi Sharma,Rafael Pires,Elisabeth Brunet,Anne-Marie Kermarrec,Catalin Fetita*

Main category: cs.DC

TL;DR: PyramidAI是一种分析千兆像素图像的技术，通过渐进式分辨率分析和自适应区域选择，显著降低计算成本，在保持精度的同时减少2.65倍数据处理量，使分析时间从数小时缩短到几分钟。


<details>
  <summary>Details</summary>
Motivation: 千兆像素图像分析计算需求巨大，需要开发能够降低计算成本的技术，使普通计算机也能进行高效的大规模图像分析。

Method: 采用渐进式分析方法，从低分辨率开始，逐步聚焦到感兴趣区域进行高分辨率详细检查，研究两种自适应分辨率选择策略来平衡精度与计算性能。

Result: 在Camelyon16生物医学图像数据集上验证，PyramidAI减少2.65倍数据处理量，同时保持相关区域识别精度；使用12个普通工作节点可将分析时间从超过1小时缩短到几分钟。

Conclusion: PyramidAI为千兆像素图像分析提供了实用的高效解决方案，实现了计算成本的显著降低和并行化潜力，使大规模图像分析在主流计算机上变得可行。

Abstract: Analyzing gigapixel images is recognized as computationally demanding. In
this paper, we introduce PyramidAI, a technique for analyzing gigapixel images
with reduced computational cost. The proposed approach adopts a gradual
analysis of the image, beginning with lower resolutions and progressively
concentrating on regions of interest for detailed examination at higher
resolutions. We investigated two strategies for tuning the accuracy-computation
performance trade-off when implementing the adaptive resolution selection,
validated against the Camelyon16 dataset of biomedical images. Our results
demonstrate that PyramidAI substantially decreases the amount of processed data
required for analysis by up to 2.65x, while preserving the accuracy in
identifying relevant sections on a single computer. To ensure democratization
of gigapixel image analysis, we evaluated the potential to use mainstream
computers to perform the computation by exploiting the parallelism potential of
the approach. Using a simulator, we estimated the best data distribution and
load balancing algorithm according to the number of workers. The selected
algorithms were implemented and highlighted the same conclusions in a
real-world setting. Analysis time is reduced from more than an hour to a few
minutes using 12 modest workers, offering a practical solution for efficient
large-scale image analysis.

</details>


### [229] [An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447)
*Xinrui Zhong,Xinze Feng,Jingwei Zuo,Fanjiang Ye,Yi Mu,Junfeng Guo,Heng Huang,Myungjin Lee,Yuke Wang*

Main category: cs.DC

TL;DR: QRMark是一种高效的图像水印检测方法，结合QR码纠错机制和分块技术，在保持检测准确性的同时显著提升检测效率


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法主要关注准确性和鲁棒性，但忽视了大规模图像集合中水印检测的效率挑战

Method: 采用Reed-Solomon纠错机制缓解分块带来的精度下降，实现资源感知的流分配策略和基于分块的工作负载交错策略

Result: 相比顺序基线方法，QRMark实现了平均2.43倍的推理加速

Conclusion: QRMark通过算法和系统层面的优化，有效解决了大规模图像水印检测的效率问题，为生成模型的负责任部署提供了实用解决方案

Abstract: Efficient and reliable detection of generated images is critical for the
responsible deployment of generative models. Existing approaches primarily
focus on improving detection accuracy and robustness under various image
transformations and adversarial manipulations, yet they largely overlook the
efficiency challenges of watermark detection across large-scale image
collections. To address this gap, we propose QRMark, an efficient and adaptive
end-to-end method for detecting embedded image watermarks. The core idea of
QRMark is to combine QR Code inspired error correction with tailored tiling
techniques to improve detection efficiency while preserving accuracy and
robustness. At the algorithmic level, QRMark employs a Reed-Solomon error
correction mechanism to mitigate the accuracy degradation introduced by tiling.
At the system level, QRMark implements a resource-aware stream allocation
policy that adaptively assigns more streams to GPU-intensive stages of the
detection pipeline. It further employs a tile-based workload interleaving
strategy to overlap data-loading overhead with computation and schedules
kernels across stages to maximize efficiency. End-to-end evaluations show that
QRMark achieves an average 2.43x inference speedup over the sequential
baseline.

</details>


### [230] [KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End Kubernetes Management](https://arxiv.org/abs/2509.02449)
*Mohsen Seyedkazemi Ardebili,Andrea Bartolini*

Main category: cs.DC

TL;DR: KubeIntellect是一个基于大语言模型的智能Kubernetes控制系统，通过自然语言交互实现端到端的Kubernetes API操作管理，解决了Kubernetes管理的复杂性和碎片化问题。


<details>
  <summary>Details</summary>
Motivation: Kubernetes作为现代云原生基础设施的基础，其管理仍然复杂且碎片化，管理员需要处理大量API、管理异构工作负载，并协调跨断开工具的任务，通常需要精确的命令、YAML配置和上下文专业知识。

Method: 系统采用模块化代理架构，按功能域（如日志、指标、RBAC）对齐，由监督器协调用户查询解释、工作流内存维护、可重用工具调用，或通过安全的代码生成代理合成新工具。集成内存检查点、人工介入澄清和动态任务排序到结构化编排框架中。

Result: 评估结果显示93%的工具合成成功率和200个自然语言查询中100%的可靠性，证明系统在不同工作负载下高效运行的能力。

Conclusion: 这项工作引入了一类新的可解释、可扩展且由LLM驱动的系统，用于管理复杂基础设施。

Abstract: Kubernetes has become the foundation of modern cloud-native infrastructure,
yet its management remains complex and fragmented. Administrators must navigate
a vast API surface, manage heterogeneous workloads, and coordinate tasks across
disconnected tools - often requiring precise commands, YAML configuration, and
contextual expertise.
  This paper presents KubeIntellect, a Large Language Model (LLM)-powered
system for intelligent, end-to-end Kubernetes control. Unlike existing tools
that focus on observability or static automation, KubeIntellect supports
natural language interaction across the full spectrum of Kubernetes API
operations, including read, write, delete, exec, access control, lifecycle, and
advanced verbs. The system uses modular agents aligned with functional domains
(e.g., logs, metrics, RBAC), orchestrated by a supervisor that interprets user
queries, maintains workflow memory, invokes reusable tools, or synthesizes new
ones via a secure Code Generator Agent.
  KubeIntellect integrates memory checkpoints, human-in-the-loop clarification,
and dynamic task sequencing into a structured orchestration framework.
Evaluation results show a 93% tool synthesis success rate and 100% reliability
across 200 natural language queries, demonstrating the system's ability to
operate efficiently under diverse workloads. An automated demo environment is
provided on Azure, with additional support for local testing via kind. This
work introduces a new class of interpretable, extensible, and LLM-driven
systems for managing complex infrastructure.

</details>


### [231] [MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to Break the GPU Memory Wall](https://arxiv.org/abs/2509.02480)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: MLP-Offload是一个针对资源受限环境下LLM训练优化的多级多路径卸载引擎，通过缓解I/O瓶颈实现比现有技术快2.5倍的训练迭代速度


<details>
  <summary>Details</summary>
Motivation: 随着LLM模型规模增长速度超过GPU内存容量，需要多级主机内存或磁盘卸载技术，但现有方法在关键训练路径中产生显著的I/O开销，导致迭代速度变慢

Method: 设计并实现MLP-Offload，通过缓存高效和并发控制的方式在多级存储层之间卸载优化器状态，缓解反向传播和更新阶段的I/O瓶颈

Result: 在高达280B参数的模型评估中，MLP-Offload相比最先进的LLM训练运行时实现了2.5倍的迭代加速

Conclusion: MLP-Offload通过多级多路径卸载策略有效解决了LLM训练中的I/O瓶颈问题，显著提升了训练效率

Abstract: Training LLMs larger than the aggregated memory of multiple GPUs is
increasingly necessary due to the faster growth of LLM sizes compared to GPU
memory. To this end, multi-tier host memory or disk offloading techniques are
proposed by state of art. Despite advanced asynchronous multi-tier read/write
strategies, such offloading strategies result in significant I/O overheads in
the critical path of training, resulting in slower iterations. To this end, we
propose MLP-Offload, a novel multi-level, multi-path offloading engine
specifically designed for optimizing LLM training on resource-constrained
setups by mitigating I/O bottlenecks. We make several key observations that
drive the design of MLP-Offload, such as I/O overheads during the update
dominate the iteration time; I/O bandwidth of the third-level remote storage
tier remains unutilized; and, contention due to concurrent offloading amplifies
I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload
to offload the optimizer states across multiple tiers in a cache-efficient and
concurrency-controlled fashion to mitigate I/O bottlenecks during the backward
and update phases. Evaluations on models up to 280B parameters shows that
MLP-Offload achieves 2.5$\times$ faster iterations compared to the
state-of-the-art LLM training runtimes.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [232] [IoT-based Noise Monitoring using Mobile Nodes for Smart Cities](https://arxiv.org/abs/2509.00979)
*Bhima Sankar Manthina,Shreyash Gujar,Sachin Chaudhari,Kavita Vemuri1,Shivam Chhirolya*

Main category: cs.ET

TL;DR: 这篇论文提出了一种基于物联网的移动器子器网络方案，通过机器学习模型实现高精度的实时城市噪声监测，并在实际部署中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现有城市噪声监测基础设施覆盖范围有限且适应性不足，无法有效管理噪声污染这一公共健康威胁。

Method: 使用低成本声音传感器与GPS模块集成移动节点，通过多种机器学习算法（线性回归、支持向量回归、随机森林等）在移动环境中进行校准，并考虑车辆速度影响。

Result: 随机森林回归模型表现最佳（R2=0.937，RMSE=1.09），在印度郑德拉布部署27天收集436,420个数据点，发现了周内、周末和迪伊丽节期间的显著噪声变化。

Conclusion: 该系统为智慧城市广泛部署物联网噪声感知网络提供了可行性，能够支持有效的噪声污染管理和城市规划。

Abstract: Urban noise pollution poses a significant threat to public health, yet
existing monitoring infrastructures offer limited spatial coverage and
adaptability. This paper presents a scalable, low-cost, IoT-based, real-time
environmental noise monitoring solution using mobile nodes (sensor nodes on a
moving vehicle). The system utilizes a low-cost sound sensor integrated with
GPS-enabled modules to collect geotagged noise data at one-second intervals.
The sound nodes are calibrated against a reference sound level meter in a
laboratory setting to ensure accuracy using various machine learning (ML)
algorithms, such as Simple Linear Regression (SLR), Multiple Linear Regression
(MLR), Polynomial Regression (PR), Segmented Regression (SR), Support Vector
Regression (SVR), Decision Tree (DT), and Random Forest Regression (RFR). While
laboratory calibration demonstrates high accuracy, it is shown that the
performance of the nodes degrades during data collection in a moving vehicle.
To address this, it is demonstrated that the calibration must be performed on
the IoT-based node based on the data collected in a moving environment along
with the reference device. Among the employed ML models, RFR achieved the best
performance with an R2 of 0.937 and RMSE of 1.09 for mobile calibration. The
system was deployed in Hyderabad, India, through three measurement campaigns
across 27 days, capturing 436,420 data points. Results highlight temporal and
spatial noise variations across weekdays, weekends, and during Diwali.
Incorporating vehicular velocity into the calibration significantly improves
accuracy. The proposed system demonstrates the potential for widespread
deployment of IoT-based noise sensing networks in smart cities, enabling
effective noise pollution management and urban planning.

</details>


### [233] [5-axis Multi-material Desktop Additive Manufacturing of Conformal Antennas](https://arxiv.org/abs/2509.01448)
*Ivan Revenga Riesco,Borut Lampret,Connor Myant,David Boyle*

Main category: cs.ET

TL;DR: 使用低成本5轴多材料3D打印技术制造复杂共形天线，相比平面打印天线具有更好的阻抗匹配、更短的制造时间和成本优势


<details>
  <summary>Details</summary>
Motivation: 探索利用低成本5轴多材料增材制造技术来制造功能性的复杂共形天线，以克服传统平面天线的局限性

Method: 使用定制的开源5轴桌面打印机和导电细丝，制造了S波段贴片天线和超宽带天线，并与平面打印版本和电磁仿真进行对比

Result: 结果显示该方法在阻抗匹配方面表现更优，同时减少了制造时间和成本

Conclusion: 多轴多材料原型制造技术适用于制造具有复杂几何形状的天线，具有广阔的应用前景

Abstract: This paper describes the novel use of low-cost, 5-axis, multi-material
additive manufacturing to fabricate functional, complex conformal antennas.
Using a customised open source 5-axis desktop printer incorporating conductive
filaments, conformal S-band patch and Ultra-Wide Band antennas were fabricated
and compared against planar-printed counterparts and electromagnetic
simulations. Results show the potential of the approach for superior impedance
matching, reduced fabrication time, and cost savings; highlighting the
applicability of multi-axis multi-material prototyping of antennas with complex
geometries.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [234] [AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection](https://arxiv.org/abs/2509.00433)
*Houshu He,Naifeng Jing,Li Jiang,Xiaoyao Liang,Zhuoran Song*

Main category: cs.AR

TL;DR: AGS是一个算法-硬件协同设计框架，通过利用SLAM系统中相邻帧的高相似性来加速3D高斯溅射SLAM系统，实现了显著的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM系统由于每帧需要多次训练迭代和大量高斯计算，导致吞吐量不足，需要更高效的解决方案

Method: 软件层面：提出粗粒度到细粒度的位姿跟踪方法，跨帧共享高斯贡献信息避免冗余计算；硬件层面：设计帧共视检测引擎、位姿跟踪引擎和建图引擎，并配备工作负载调度器

Result: AGS相比移动和高性能GPU以及最先进的3DGS加速器GSCore，分别实现了17.12倍、6.71倍和5.41倍的加速比

Conclusion: AGS框架通过算法-硬件协同设计有效解决了3DGS-SLAM系统的效率瓶颈，为实时SLAM应用提供了高效的解决方案

Abstract: Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.

</details>


### [235] [Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator](https://arxiv.org/abs/2509.00500)
*Yizhi Chen,Jingwei Li,Wenyao Zhu,Zhonghai Lu*

Main category: cs.AR

TL;DR: 提出基于'1'比特计数的排序方法，通过减少比特翻转来降低NoC中DNN加速器的链路功耗


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络(DNN)变得日益重要，基于片上网络(NoC)的DNN加速器越来越流行。为了节省NoC中的链路功耗，需要减少比特翻转(BT)

Method: 提出两种数据排序方法：关联排序和分离排序，分别用于联合或单独处理权重和输入数据。提供数学证明验证方法的有效性

Result: 无NoC情况下：浮点32数据最多减少20.38% BT，定点8数据最多减少55.71% BT；有NoC情况下：浮点32数据最多减少32.01% BT，定点8数据最多减少40.85% BT

Conclusion: 提出的排序方法能有效降低DNN加速器中的比特翻转，从而显著减少链路功耗，适用于不同DNN模型、NoC配置和数据精度

Abstract: As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip
(NoC)-based DNN accelerators gained increasing popularity. To save link power
in NoC, many researchers focus on reducing the Bit Transition (BT). We propose
'1'-bit count-based ordering method to reduce BT for DNN workloads. We provide
a mathematical proof of the efficacy of proposed ordering. We evaluate our
method through experiments without NoC and with NoC. Without NoC, our proposed
ordering method achieves up to 20.38% BT reduction for floating-point-32 data
and 55.71% for fixed-point-8 data, respectively. We propose two data ordering
methods, affiliated-ordering and separated-ordering to process weight and input
jointly or individually and apply them to run full DNNs in NoC-based DNN
accelerator. We evaluate our approaches under various configurations, including
different DNN models such as LeNet and DarkNet, various NoC sizes with
different numbers of memory controllers, random weights and trained weights,
and different data precision. Our approach efficiently reduces the link power
by achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT
reduction for fixed-point-8 data.

</details>


### [236] [GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs](https://arxiv.org/abs/2509.01020)
*Elena Espinosa,Rubén Rodríguez Álvarez,José Miranda,Rafael Larrosa,Miguel Peón-Quirós,Oscar Plata,David Atienza*

Main category: cs.AR

TL;DR: GeneTEK是一个基于FPGA的基因组序列比对加速器，使用Myers算法和高层次合成技术，在速度和能效方面优于CPU和GPU解决方案，同时解决了FPGA方法的可扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 下一代测序技术产生了海量基因组数据，序列比对作为生物信息学流程中耗时耗能的关键步骤，需要高效的加速解决方案。虽然FPGA在能效方面表现出色，但现有方法存在可扩展性限制。

Method: 提出了一个可扩展的FPGA加速器模板，采用高层次合成和基于工作者的架构实现Myers算法。GeneTEK是该模板在Xilinx Zynq UltraScale+ FPGA上的具体实现。

Result: GeneTEK相比最先进的CPU和GPU实现，执行速度至少提高19.4%，能耗降低高达62倍，同时比之前的FPGA解决方案支持大72%的比对矩阵。

Conclusion: FPGA作为可扩展基因组工作负载的能效平台具有巨大潜力，GeneTEK成功克服了现有FPGA方法的可扩展性限制，在性能和能效方面均表现出色。

Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic
research by enabling high-throughput data generation through parallel
sequencing of a diverse range of organisms at significantly reduced costs. This
breakthrough has unleashed a "Cambrian explosion" in genomic data volume and
diversity. This volume of workloads places genomics among the top four big data
challenges anticipated for this decade. In this context, pairwise sequence
alignment represents a very time- and energy-consuming step in common
bioinformatics pipelines. Speeding up this step requires the implementation of
heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated
significant performance gains, recent field programmable gate array (FPGA)
implementations have shown improved energy efficiency. However, the latter
often suffer from limited scalability due to constraints on hardware resources
when aligning longer sequences. In this work, we present a scalable and
flexible FPGA-based accelerator template that implements Myers's algorithm
using high-level synthesis and a worker-based architecture. GeneTEK, an
instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,
outperforms state-of-the-art CPU and GPU implementations in both speed and
energy efficiency, while overcoming scalability limitations of current FPGA
approaches. Specifically, GeneTEK achieves at least a 19.4% increase in
execution speed and up to 62x reduction in energy consumption compared to
leading CPU and GPU solutions, while fitting comparison matrices up to 72%
larger compared to previous FPGA solutions. These results reaffirm the
potential of FPGAs as an energy-efficient platform for scalable genomic
workloads.

</details>


### [237] [Real-Time Piano Note Frequency Detection Using FPGA and FFT Core](https://arxiv.org/abs/2509.00589)
*Shafayet M. Anik,D. G. Perera*

Main category: cs.AR

TL;DR: 使用FPGA实现钢琴音频信号的实时FFT频率分析，相比传统DSP方法具有更低延迟和更高计算效率


<details>
  <summary>Details</summary>
Motivation: 传统软件DSP方法在实时频率分析中存在延迟高、计算资源需求大的问题，而FPGA的并行处理能力可以提供更快、更确定性的解决方案

Method: 采用基于FPGA的实时快速傅里叶变换(FFT)系统来分析数字钢琴的模拟音频信号

Result: FPGA平台能够实现高速、低延迟的音频频率分析，适用于实时应用场景

Conclusion: FPGA硬件平台在实时音频频率分析方面相比传统软件方法具有显著优势，特别适合钢琴等乐器的高性能实时处理需求

Abstract: Real-time frequency analysis of musical instruments, such as the piano, is an
essential feature in areas like electronic tuners, music visualizers, and live
sound monitoring. Traditional methods often rely on software-based digital
signal processing (DSP), which may introduce latency and require significant
computational power. In contrast, hardware platforms such as FPGAs (Field
Programmable Gate Arrays) offer the ability to perform such analyses with
greater speed and determinism due to their parallel processing capabilities.
The primary objective of this project was to analyze analog audio signals from
a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT)
system.

</details>


### [238] [COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives](https://arxiv.org/abs/2509.00599)
*Shubham Negi,Manik Singhal,Aayush Ankit,Sudeep Bhoja,Kaushik Roy*

Main category: cs.AR

TL;DR: COMET是一个针对机器学习加速器中复合操作的数据流建模和优化框架，能够显式建模跨空间集群的集体通信，提升现代DNN模型的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习加速器虽然能高效执行DNN，但新兴的大语言模型、状态空间模型等依赖复合操作，现有数据流优化框架要么只关注单一操作，要么缺乏集体通信成本建模，无法满足现代工作负载需求。

Method: 提出COMET框架，引入新颖的表征方法显式建模跨空间集群的集体通信，建立包含GEMM和非GEMM操作级依赖关系的延迟和能耗成本模型。

Result: 优化的数据流在GEMM-Softmax上实现1.42倍加速，GEMM-LayerNorm实现3.46倍加速，自注意力机制实现1.82倍加速，相比未融合的基线。

Conclusion: COMET通过集体感知建模能够探索更广泛的映射空间，显著提升复合操作的性能和能效，适用于边缘和云端加速器配置。

Abstract: Modern machine learning accelerators are designed to efficiently execute deep
neural networks (DNNs) by optimizing data movement, memory hierarchy, and
compute throughput. However, emerging DNN models such as large language models,
state space models increasingly rely on compound operations-structured
compositions of multiple basic operations-which introduce new challenges for
dataflow optimization and minimizing off-chip memory traffic. Moreover, as
model size continues to grow, deployment across spatially distributed compute
clusters becomes essential, requiring frequent and complex collective
communication. Existing dataflow optimization frameworks and performance models
either focus on single operations or lack explicit modeling of collective
communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and
optimizing dataflow for compound operations on machine learning accelerators.
COMET introduces a novel representation that explicitly models collective
communication across spatial clusters, along with latency and energy cost
models that account for both GEMM and non-GEMM operation level dependencies
within compound operations. We demonstrate COMET's capabilities to analyze and
optimize dataflows for compound operations such as GEMM--Softmax,
GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator
configurations. Our collective-aware modeling enables exploration of a broader
mapping space, leading to improved performance and energy efficiency.
Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for
GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for
self-attention compared to unfused baselines.

</details>


### [239] [On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures](https://arxiv.org/abs/2509.00633)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed A. Badawy,Ahmad Patooghy*

Main category: cs.AR

TL;DR: 本文提出了一种针对3D堆叠高带宽内存(HBM)架构的热性能降级攻击，攻击者利用内存bank的垂直和横向邻近性注入热脉冲，形成汇聚热波来延迟受害应用程序的内存访问，且攻击难以检测。


<details>
  <summary>Details</summary>
Motivation: 3D堆叠HBM架构虽然解决了内存墙性能问题，但由于制造过程中的垂直邻近性，容易受到热漏洞攻击。攻击者可能利用这种邻近性设计热攻击来降级受害者应用的性能。

Method: 攻击者通过从垂直和/或横向相邻的内存bank注入短暂而强烈的热脉冲，创建汇聚热波，最大化对受害者应用的影响，延迟其数据/指令访问。攻击模仿合法工作负载，不访问越界内存位置。

Result: 这种攻击能够绕过设计时安全测试和操作系统内存管理策略，因为攻击行为看起来像正常的工作负载，使得检测变得困难。

Conclusion: 3D堆叠HBM架构存在严重的热安全漏洞，攻击者可以利用内存bank的物理邻近性实施难以检测的性能降级攻击，需要新的安全机制来防护这类热基攻击。

Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance
memory interactions to address the well-known performance challenge, namely the
memory wall. However, these architectures are susceptible to thermal
vulnerabilities due to the inherent vertical adjacency that occurs during the
manufacturing process of HBM architectures. We anticipate that adversaries may
exploit the intense vertical and lateral adjacency to design and develop
thermal performance degradation attacks on the memory banks that host
data/instructions from victim applications. In such attacks, the adversary
manages to inject short and intense heat pulses from vertically and/or
laterally adjacent memory banks, creating a convergent thermal wave that
maximizes impact and delays the victim application from accessing its
data/instructions. As the attacking application does not access any
out-of-range memory locations, it can bypass both design-time security tests
and the operating system's memory management policies. In other words, since
the attack mimics legitimate workloads, it will be challenging to detect.

</details>


### [240] [Low Power Approximate Multiplier Architecture for Deep Neural Networks](https://arxiv.org/abs/2509.00764)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出一种用于深度神经网络的低功耗近似乘法器架构，通过设计仅产生单一组合误差的4:2压缩器，在保持低错误率的同时显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 深度神经网络应用需要大量乘法运算，传统精确乘法器能耗高，需要开发在保持计算精度的同时显著降低能耗的近似乘法器架构

Method: 设计仅产生单一组合误差的4:2压缩器，并将其集成到8x8无符号乘法器中，减少精确压缩器的使用，构建自定义卷积层并在神经网络任务中进行评估

Result: 硬件评估显示能耗节省达30.24%，图像去噪任务中PSNR和SSIM指标优于其他近似设计，手写数字识别保持高分类准确率

Conclusion: 该架构在能效和计算精度之间实现了良好平衡，适用于低功耗AI硬件实现

Abstract: This paper proposes an low power approximate multiplier architecture for deep
neural network (DNN) applications. A 4:2 compressor, introducing only a single
combination error, is designed and integrated into an 8x8 unsigned multiplier.
This integration significantly reduces the usage of exact compressors while
preserving low error rates. The proposed multiplier is employed within a custom
convolution layer and evaluated on neural network tasks, including image
recognition and denoising. Hardware evaluation demonstrates that the proposed
design achieves up to 30.24% energy savings compared to the best among existing
multipliers. In image denoising, the custom approximate convolution layer
achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity
Index Measure (SSIM) compared to other approximate designs. Additionally, when
applied to handwritten digit recognition, the model maintains high
classification accuracy. These results demonstrate that the proposed
architecture offers a favorable balance between energy efficiency and
computational precision, making it suitable for low-power AI hardware
implementations.

</details>


### [241] [Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication](https://arxiv.org/abs/2509.00778)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 本文提出了一种包含新型精确和近似处理单元(PE)的脉动阵列架构，在8x8阵列中分别实现22%和32%的能耗节省，在DCT和边缘检测应用中保持良好输出质量


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要高效的矩阵乘法引擎，传统设计能耗较高，需要开发更节能的架构

Method: 使用能量高效的PPC和NPPC单元设计8位精确和近似处理单元，并集成到8x8脉动阵列中

Result: 精确PE节能22%，近似PE节能32%；DCT计算PSNR达38.21dB，边缘检测PSNR达30.45dB

Conclusion: 所提设计在保持竞争力的输出质量同时显著提升能效，适用于容错图像和视觉处理应用

Abstract: Deep Neural Networks (DNNs) require highly efficient matrix multiplication
engines for complex computations. This paper presents a systolic array
architecture incorporating novel exact and approximate processing elements
(PEs), designed using energy-efficient positive partial product and negative
partial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit
exact and approximate PE designs are employed in a 8x8 systolic array, which
achieves a energy savings of 22% and 32%, respectively, compared to the
existing design. To demonstrate their effectiveness, the proposed PEs are
integrated into a systolic array (SA) for Discrete Cosine Transform (DCT)
computation, achieving high output quality with a PSNR of 38.21,dB.
Furthermore, in an edge detection application using convolution, the
approximate PE achieves a PSNR of 30.45,dB. These results highlight the
potential of the proposed design to deliver significant energy efficiency while
maintaining competitive output quality, making it well-suited for
error-resilient image and vision processing applications.

</details>


### [242] [GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency](https://arxiv.org/abs/2509.00911)
*Joongho Jo,Jongsun Park*

Main category: cs.AR

TL;DR: GS-TG是一种基于瓦片分组的3D高斯泼溅加速器，通过减少冗余排序操作并保持光栅化效率，实现了1.54倍的平均加速效果。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然比NeRF速度快，但仍无法满足实时应用的帧率需求，需要在减少排序冗余和保持光栅化效率之间找到平衡。

Method: 提出瓦片分组方法：在排序阶段将小瓦片分组形成大瓦片来共享排序操作，在光栅化阶段使用位掩码标识相关小瓦片，实现排序结果的高效共享。

Result: 实验结果显示GS-TG相比最先进的3D-GS加速器实现了平均1.54倍的加速效果。

Conclusion: GS-TG是一种无损的加速方法，无需重新训练或微调，可与现有优化技术无缝集成，有效解决了3D-GS渲染中的关键权衡问题。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to
neural radiance fields (NeRF) as it offers high speed as well as high image
quality in novel view synthesis. Despite these advancements, 3D-GS still
struggles to meet the frames per second (FPS) demands of real-time
applications. In this paper, we introduce GS-TG, a tile-grouping-based
accelerator that enhances 3D-GS rendering speed by reducing redundant sorting
operations and preserving rasterization efficiency. GS-TG addresses a critical
trade-off issue in 3D-GS rendering: increasing the tile size effectively
reduces redundant sorting operations, but it concurrently increases unnecessary
rasterization computations. So, during sorting of the proposed approach, GS-TG
groups small tiles (for making large tiles) to share sorting operations across
tiles within each group, significantly reducing redundant computations. During
rasterization, a bitmask assigned to each Gaussian identifies relevant small
tiles, to enable efficient sharing of sorting results. Consequently, GS-TG
enables sorting to be performed as if a large tile size is used by grouping
tiles during the sorting stage, while allowing rasterization to proceed with
the original small tiles by using bitmasks in the rasterization stage. GS-TG is
a lossless method requiring no retraining or fine-tuning and it can be
seamlessly integrated with previous 3D-GS optimization techniques. Experimental
results show that GS-TG achieves an average speed-up of 1.54 times over
state-of-the-art 3D-GS accelerators.

</details>


### [243] [LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems](https://arxiv.org/abs/2509.01339)
*Bochen Ye,Gustavo Naspolini,Kimmo Salo,Manil Dev Gomony*

Main category: cs.AR

TL;DR: LinkBo是一种创新的单线通信协议，相比现有的1-wire和UNI/O协议，在延迟、吞吐量和鲁棒性方面有显著提升，支持最长15米线缆和最高7.5 Mbps数据传输速率。


<details>
  <summary>Details</summary>
Motivation: 当前单线通信协议存在延迟高、吞吐量受限和鲁棒性不足的问题，而嵌入式系统需要更高效的单线通信解决方案来减少引脚数量。

Method: 提出了LinkBo协议，包括硬件架构设计和协议层保证机制，支持硬件中断和错误检测功能，并在两个FPGA组成的硬件平台上进行性能评估。

Result: LinkBo协议能够在50.4μs内完成高优先级消息传输，比1-wire快20倍，比UNI/O快6.3倍；支持最长15米线缆（300 kbps）和最短11厘米线缆（7.5 Mbps）。

Conclusion: LinkBo协议为可变距离的芯片间通信提供了低延迟、高吞吐量和高鲁棒性的单线通信解决方案，性能显著优于现有商业协议。

Abstract: Cost-effective embedded systems necessitate utilizing the single-wire
communication protocol for inter-chip communication, thanks to its reduced pin
count in comparison to the multi-wire I2C or SPI protocols. However, current
single-wire protocols suffer from increased latency, restricted throughput, and
lack of robustness. This paper presents LinkBo, an innovative single-wire
protocol that offers reduced latency, enhanced throughput, and greater
robustness with hardware-interrupt for variable-distance inter-chip
communication. The LinkBo protocol-level guarantees that high-priority messages
are delivered with an error detection feature in just 50.4 $\mu$s, surpassing
current commercial options, 1-wire and UNI/O by at least 20X and 6.3X,
respectively. In addition, we present the hardware architecture for this new
protocol and its performance evaluation on a hardware platform consisting of
two FPGAs. Our findings demonstrate that the protocol reliably supports wire
lengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum
data rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for
varying inter-chip communication distances.

</details>


### [244] [Guidance and Control Neural Network Acceleration using Memristors](https://arxiv.org/abs/2509.02369)
*Zacharia A. Rudge,Dario Izzo,Moritz Fieback,Anteneh Gebregiorgis,Said Hamdioui,Dominik Dold*

Main category: cs.AR

TL;DR: 研究探索使用相变存储器和电阻式随机存取存储器在太空应用中实现内存计算AI加速，通过模拟指导控制神经网络验证性能，发现虽然噪声影响精度但重新训练可恢复性能。


<details>
  <summary>Details</summary>
Motivation: 小型卫星和立方体卫星的有限能源预算以及现代芯片的辐射问题限制了AI在太空应用中的发展，需要研究能满足这些要求同时满足计算性能需求的神经网络加速器。

Method: 使用相变存储器(PCM)和电阻式随机存取存储器(RRAM)忆阻器进行内存计算AI加速，模拟指导控制神经网络(G&CNET)在各种场景下的性能，考虑噪声和电导漂移等设备非理想特性。

Result: 忆阻加速器能够学习专家动作，但噪声对精度的影响仍是挑战。在性能退化后重新训练能够将性能恢复到正常水平。

Conclusion: 这项研究为未来基于忆阻器的太空AI加速器研究奠定了基础，突出了其潜力以及需要进一步调查的需求。

Abstract: In recent years, the space community has been exploring the possibilities of
Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),
for a variety of on board applications. However, this development is limited by
the restricted energy budget of smallsats and cubesats as well as radiation
concerns plaguing modern chips. This necessitates research into neural network
accelerators capable of meeting these requirements whilst satisfying the
compute and performance needs of the application. This paper explores the use
of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)
memristors for on-board in-memory computing AI acceleration in space
applications. A guidance and control neural network (G\&CNET) accelerated using
memristors is simulated in a variety of scenarios and with both device types to
evaluate the performance of memristor-based accelerators, considering device
non-idealities such as noise and conductance drift. We show that the memristive
accelerator is able to learn the expert actions, though challenges remain with
the impact of noise on accuracy. We also show that re-training after
degradation is able to restore performance to nominal levels. This study
provides a foundation for future research into memristor-based AI accelerators
for space, highlighting their potential and the need for further investigation.

</details>
