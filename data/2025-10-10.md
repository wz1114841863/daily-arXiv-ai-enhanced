<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 119]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: 该研究关注自闭症谱系障碍儿童的行为模式和情绪识别，提出通过纵向监测建立基线理解，并开发针对性技术辅助框架，以改善学习和发展轨迹。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍严重影响个体的沟通、学习、行为和社交能力，但目前在技能发展前对细微行为模式和情绪识别的理解存在关键空白，特别是在信息技术领域机会有限。

Method: 采用纵向方法监测情绪和行为，分析随时间变化的行为趋势，建立基线理解，并提出针对性的应用和技术辅助开发框架。

Result: 研究强调了对每个儿童行为和情绪景观深度理解的重要性，作为有效技能发展的基础，并提出了基于证据的干预方法。

Conclusion: 通过将重点转向早期行为模式识别，旨在营造更具包容性和支持性的学习环境，显著改善自闭症儿童的教育和发展轨迹。

Abstract: Autism Spectrum Disorder significantly influences the communication
abilities, learning processes, behavior, and social interactions of
individuals. Although early intervention and customized educational strategies
are critical to improving outcomes, there is a pivotal gap in understanding and
addressing nuanced behavioral patterns and emotional identification in autistic
children prior to skill development. This extended research delves into the
foundational step of recognizing and mapping these patterns as a prerequisite
to improving learning and soft skills. Using a longitudinal approach to monitor
emotions and behaviors, this study aims to establish a baseline understanding
of the unique needs and challenges faced by autistic students, particularly in
the Information Technology domain, where opportunities are markedly limited.
Through a detailed analysis of behavioral trends over time, we propose a
targeted framework for developing applications and technical aids designed to
meet these identified needs. Our research underscores the importance of a
sequential and evidence-based intervention approach that prioritizes a deep
understanding of each child's behavioral and emotional landscape as the basis
for effective skill development. By shifting the focus toward early
identification of behavioral patterns, we aim to foster a more inclusive and
supportive learning environment that can significantly improve the educational
and developmental trajectory of children with ASD.

</details>


### [2] [A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search](https://arxiv.org/abs/2510.07325)
*Sixuan Wang,Jiao Yin,Jinli Cao,Mingjian Tang,Yong-Feng Ge*

Main category: cs.LG

TL;DR: 提出MACC-MGNAS方法，通过模态感知协同进化算法搜索多模态图神经网络架构，有效解决软件漏洞共利用攻击预测问题，在减少计算成本的同时显著提升性能


<details>
  <summary>Details</summary>
Motivation: 软件漏洞共利用攻击对企业构成严重威胁，需要分析异构多模态漏洞数据来缓解。现有单模态图神经架构搜索方法无法处理模态异质性，需要开发专门的多模态架构搜索方法

Method: 提出模态感知协同进化框架(MACC)，将全局染色体划分为模态特定基因组分别进化；引入模态感知双轨代理方法(MADTS)降低评估成本；设计基于相似性的种群多样性指标(SPDI)平衡探索与利用

Result: 在标准漏洞共利用数据集上达到81.67%的F1分数，仅需3GPU小时，比现有最优方法提升8.7% F1分数，同时减少27%计算成本

Conclusion: MACC-MGNAS方法能有效处理多模态异质性，显著提升漏洞共利用攻击预测性能，同时大幅降低计算开销，为多模态图神经网络架构搜索提供了有效解决方案

Abstract: Co-exploitation attacks on software vulnerabilities pose severe risks to
enterprises, a threat that can be mitigated by analyzing heterogeneous and
multimodal vulnerability data. Multimodal graph neural networks (MGNNs) are
well-suited to integrate complementary signals across modalities, thereby
improving attack-prediction accuracy. However, designing an effective MGNN
architecture is challenging because it requires coordinating modality-specific
components at each layer, which is infeasible through manual tuning. Genetic
algorithm (GA)-based graph neural architecture search (GNAS) provides a natural
solution, yet existing methods are confined to single modalities and overlook
modality heterogeneity. To address this limitation, we propose a modality-aware
cooperative co-evolutionary algorithm for multimodal graph neural architecture
search, termed MACC-MGNAS. First, we develop a modality-aware cooperative
co-evolution (MACC) framework under a divide-and-conquer paradigm: a
coordinator partitions a global chromosome population into modality-specific
gene groups, local workers evolve them independently, and the coordinator
reassembles chromosomes for joint evaluation. This framework effectively
captures modality heterogeneity ignored by single-modality GNAS. Second, we
introduce a modality-aware dual-track surrogate (MADTS) method to reduce
evaluation cost and accelerate local gene evolution. Third, we design a
similarity-based population diversity indicator (SPDI) strategy to adaptively
balance exploration and exploitation, thereby accelerating convergence and
avoiding local optima. On a standard vulnerabilities co-exploitation (VulCE)
dataset, MACC-MGNAS achieves an F1-score of 81.67% within only 3 GPU-hours,
outperforming the state-of-the-art competitor by 8.7% F1 while reducing
computation cost by 27%.

</details>


### [3] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: MultiFair是一种用于多模态医学分类的新方法，通过双层级梯度调制过程解决模态学习和群体公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习模型存在两个关键问题：不同数据模态学习不均衡导致模型偏向某些模态，以及模型可能对某些人口群体表现不公平。这两个方面相互影响，导致多模态学习既不平衡又不公平。

Method: 提出MultiFair方法，采用双层级梯度调制过程，在数据模态和群体层级上动态调节训练梯度的优化方向和幅度。

Result: 在两个多模态医学数据集上的实验表明，MultiFair在性能上优于最先进的多模态学习和公平性学习方法。

Conclusion: MultiFair通过双层级梯度调制有效解决了多模态医学分类中的模态偏差和群体公平性问题，取得了优于现有方法的性能。

Abstract: Medical decision systems increasingly rely on data from multiple sources to
ensure reliable and unbiased diagnosis. However, existing multimodal learning
models fail to achieve this goal because they often ignore two critical
challenges. First, various data modalities may learn unevenly, thereby
converging to a model biased towards certain modalities. Second, the model may
emphasize learning on certain demographic groups causing unfair performances.
The two aspects can influence each other, as different data modalities may
favor respective groups during optimization, leading to both imbalanced and
unfair multimodal learning. This paper proposes a novel approach called
MultiFair for multimodal medical classification, which addresses these
challenges with a dual-level gradient modulation process. MultiFair dynamically
modulates training gradients regarding the optimization direction and magnitude
at both data modality and group levels. We conduct extensive experiments on two
multimodal medical datasets with different demographic groups. The results show
that MultiFair outperforms state-of-the-art multimodal learning and fairness
learning methods.

</details>


### [4] [Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data](https://arxiv.org/abs/2510.07350)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: 本文评估了两种深度学习模型(GNN-RNN和MMST-ViT)在作物产量预测中的跨区域泛化能力，发现GNN-RNN在异地条件下表现更优，且训练速度快135倍，揭示了时空对齐对稳健泛化的关键作用。


<details>
  <summary>Details</summary>
Motivation: 气候变化日益破坏农业系统，准确的作物产量预测对粮食安全至关重要。虽然深度学习模型在使用卫星和天气数据进行产量预测方面显示出潜力，但它们在跨地理区域和年份的泛化能力（对实际部署至关重要）尚未得到充分测试。

Method: 使用覆盖2017-2022年1200多个美国县的大规模CropNet数据集，在现实的异地分布条件下对GNN-RNN和MMST-ViT两种最先进模型进行基准测试，采用跨七个USDA农场资源区域的留一聚类交叉验证和年度预测场景。

Result: GNN-RNN在异地条件下表现出优越的泛化能力，具有正相关性，而MMST-ViT在域内表现良好但在异地条件下急剧退化。Heartland和Northern Great Plains等区域显示出稳定的转移动态（大豆RMSE小于10蒲式耳/英亩），而Prairie Gateway在所有模型和作物中持续表现不佳（RMSE大于20蒲式耳/英亩）。GNN-RNN训练速度比MMST-ViT快135倍（14分钟 vs. 31.5小时）。

Conclusion: 时空对齐而不仅仅是模型复杂性或数据规模是稳健泛化的关键，需要透明的异地评估协议来确保公平可靠的气候感知农业预测。

Abstract: Climate change is increasingly disrupting agricultural systems, making
accurate crop yield forecasting essential for food security. While deep
learning models have shown promise in yield prediction using satellite and
weather data, their ability to generalize across geographic regions and years -
critical for real-world deployment - remains largely untested. We benchmark two
state-of-the-art models, GNN-RNN and MMST-ViT, under realistic
out-of-distribution (OOD) conditions using the large-scale CropNet dataset
spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out
cross-validation across seven USDA Farm Resource Regions and year-ahead
prediction scenarios, we identify substantial variability in cross-region
transferability. GNN-RNN demonstrates superior generalization with positive
correlations under geographic shifts, while MMST-ViT performs well in-domain
but degrades sharply under OOD conditions. Regions like Heartland and Northern
Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for
soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE
greater than 20 bu/acre) across both models and crops, revealing structural
dissimilarities likely driven by semi-arid climate, irrigation patterns, and
incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves
135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more
viable for sustainable deployment. Our findings underscore that
spatial-temporal alignment - not merely model complexity or data scale - is key
to robust generalization, and highlight the need for transparent OOD evaluation
protocols to ensure equitable and reliable climate-aware agricultural
forecasting.

</details>


### [5] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: 该论文提出了ConCuR数据集和KernelCoder模型，通过生成和筛选带有推理轨迹的高质量CUDA内核来解决内核生成任务中高质量数据稀缺的问题，在KernelBench上显著优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 解决GPU内核生成任务中高质量数据稀缺的挑战，因为大多数高质量内核是专有的且不开源，这阻碍了使用监督微调来对齐LLMs到内核生成任务。

Method: 开发了一个生成和筛选带有推理轨迹的高质量CUDA内核的流水线，构建ConCuR数据集，并训练KernelCoder模型，这是第一个在包含PyTorch、推理和CUDA内核对的数据集上训练的模型。

Result: 在KernelBench设置中，模型显著优于现有最佳模型QwQ-32B，超越了所有为内核生成微调的开源模型以及前沿模型如DeepSeek-V3.1-Think和Claude-4-sonnet。

Conclusion: 平均推理长度可以作为评估内核生成任务难度的指标，观察结果、指标以及数据收集和筛选流水线有助于未来在内核生成任务中获得更好的数据。

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [6] [Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts](https://arxiv.org/abs/2510.07358)
*Yeskendir Koishekenov,Aldo Lipani,Nicola Cancedda*

Main category: cs.LG

TL;DR: ETD方法通过在推理时对关键推理层进行迭代，显著提升小规模语言模型的推理能力，无需改变模型架构或增加参数。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过扩大模型规模或增加推理计算来提升推理能力，但研究表明关键推理计算集中在少数层中。

Method: 在训练阶段训练模型在推理相关层上进行迭代，推理时对这些层进行多次计算，并探索自适应深度策略。

Result: 在17个推理基准测试中取得显著提升，OLMo-2 1B模型在GSM8K上相对准确率提升28.4%，在MATH上提升36%。

Conclusion: 递归潜在推理为增强LLM推理能力提供了一条简单有效的路径。

Abstract: Most efforts to improve the reasoning capabilities of large language models
(LLMs) involve either scaling the number of parameters and the size of training
data, or scaling inference computation by letting models generate complex
chains of thought. Motivated by interpretability studies showing that the
crucial computation required for reasoning tasks is concentrated in a limited
range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances
the reasoning capabilities of a base model by training it to iterate over a
small subset of reasoning-relevant layers during the mid-training stage. ETD
amplifies latent reasoning while preserving the original architecture,
parameter count, hyperparameters, and training data composition. When iterating
on the selected layers at inference time, ETD models yield substantial gains on
17 reasoning benchmarks, including +28.4% relative accuracy improvement on
GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an
adaptive depth strategy that adjusts the computation per input token. Our
results show that recursive latent reasoning offers a simple and effective path
to stronger LLM reasoning.

</details>


### [7] [FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2510.07664)
*Yunbo Li,Jiaping Gui,Zhihang Deng,Fanchao Meng,Yue Wu*

Main category: cs.LG

TL;DR: FedQS是首个理论分析并解决半异步联邦学习中梯度聚合与模型聚合策略差异的框架，通过客户分类和自适应优化，在准确率、收敛速度和稳定性方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 半异步联邦学习在梯度聚合和模型聚合策略之间存在权衡：梯度聚合收敛快但波动大，模型聚合稳定但收敛慢且精度低。需要统一解决方案来平衡这些特性。

Method: FedQS采用分治策略，根据数据分布特征和计算资源将客户端分为四种类型，并自适应优化其本地训练过程。

Result: 在计算机视觉、自然语言处理和真实任务上的实验表明，FedQS实现了最高准确率、最低损失，并且收敛速度名列前茅，优于现有最优基准方法。

Conclusion: FedQS填补了半异步联邦学习中聚合策略之间的差距，为稳定、准确和高效的联邦学习提供了统一解决方案。

Abstract: Federated learning (FL) enables collaborative model training across multiple
parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as
a balanced approach between synchronous and asynchronous FL. However, SAFL
faces significant challenges in optimizing both gradient-based (e.g., FedSGD)
and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct
trade-offs in accuracy, convergence speed, and stability. While gradient
aggregation achieves faster convergence and higher accuracy, it suffers from
pronounced fluctuations, whereas model aggregation offers greater stability but
slower convergence and suboptimal accuracy. This paper presents FedQS, the
first framework to theoretically analyze and address these disparities in SAFL.
FedQS introduces a divide-and-conquer strategy to handle client heterogeneity
by classifying clients into four distinct types and adaptively optimizing their
local training based on data distribution characteristics and available
computational resources. Extensive experiments on computer vision, natural
language processing, and real-world tasks demonstrate that FedQS achieves the
highest accuracy, attains the lowest loss, and ranks among the fastest in
convergence speed, outperforming state-of-the-art baselines. Our work bridges
the gap between aggregation strategies in SAFL, offering a unified solution for
stable, accurate, and efficient federated learning. The code and datasets are
available at https://anonymous.4open.science/r/FedQS-EDD6.

</details>


### [8] [Best-of-Both Worlds for linear contextual bandits with paid observations](https://arxiv.org/abs/2510.07424)
*Nathan Boyer,Dorian Baudry,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 提出了一种用于带付费观测的线性上下文赌博机问题的最佳两全算法，在对抗环境中达到最小最大最优遗憾，在随机环境中实现多对数遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究带付费观测的线性上下文赌博机问题，其中学习者在每轮选择动作以最小化给定上下文中的损失，并可决定支付固定成本来观测任何臂的损失。

Method: 基于带矩阵几何重采样的有效估计器的跟随正则化领导者框架，引入了计算高效的最佳两全算法。

Result: 在对抗环境中实现了最小最大最优遗憾Θ(T^{2/3})，在（被破坏的）随机环境中保证了多对数遗憾。

Conclusion: 该方法为"困难问题"设计最佳两全算法提供了框架，并针对所考虑的设置采用了专门的分析技术。

Abstract: We study the problem of linear contextual bandits with paid observations,
where at each round the learner selects an action in order to minimize its loss
in a given context, and can then decide to pay a fixed cost to observe the loss
of any arm. Building on the Follow-the-Regularized-Leader framework with
efficient estimators via Matrix Geometric Resampling, we introduce a
computationally efficient Best-of-Both-Worlds (BOBW) algorithm for this
problem. We show that it achieves the minimax-optimal regret of
$\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic
regret in (corrupted) stochastic regimes. Our approach builds on the framework
from \cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',
using analysis techniques tailored for the setting that we consider.

</details>


### [9] [SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening](https://arxiv.org/abs/2510.07922)
*Murtaza Rangwala,Farag Azzedin,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.LG

TL;DR: SketchGuard是一个基于草图压缩的去中心化联邦学习框架，通过Count Sketch将高维模型压缩为低维草图进行相似性比较，显著降低了通信和计算开销，同时保持对拜占庭攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的拜占庭鲁棒DFL防御方法需要客户端在每轮训练中与所有邻居交换和比较完整的高维模型向量，产生了过高的通信和计算成本，阻碍了在web规模上的部署。

Method: 使用Count Sketch将d维模型压缩为k维草图（k << d）进行相似性比较，然后仅从接受的邻居处选择性获取完整模型，将每轮通信复杂度从O(d|N_i|)降低到O(k|N_i| + d|S_i|)。

Result: 在多个数据集、网络拓扑和攻击场景下的实验表明，SketchGuard保持了与最先进方法相同的鲁棒性，同时将计算时间减少高达82%，通信开销减少50-70%，且收益随模型维度和网络连接性呈乘性增长。

Conclusion: 草图压缩作为基本使能技术，确立了在web规模上实现鲁棒去中心化联邦学习的可行性。

Abstract: Decentralized Federated Learning (DFL) enables privacy-preserving
collaborative training without centralized servers, but remains vulnerable to
Byzantine attacks where malicious clients submit corrupted model updates.
Existing Byzantine-robust DFL defenses rely on similarity-based neighbor
screening that requires every client to exchange and compare complete
high-dimensional model vectors with all neighbors in each training round,
creating prohibitive communication and computational costs that prevent
deployment at web scale. We propose SketchGuard, a general framework that
decouples Byzantine filtering from model aggregation through sketch-based
neighbor screening. SketchGuard compresses $d$-dimensional models to
$k$-dimensional sketches ($k \ll d$) using Count Sketch for similarity
comparisons, then selectively fetches full models only from accepted neighbors,
reducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| +
d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \le |N_i|$ is the
accepted neighbor count. We establish rigorous convergence guarantees in both
strongly convex and non-convex settings, proving that Count Sketch compression
preserves Byzantine resilience with controlled degradation bounds where
approximation errors introduce only a $(1+O(\epsilon))$ factor in the effective
threshold parameter. Comprehensive experiments across multiple datasets,
network topologies, and attack scenarios demonstrate that SketchGuard maintains
identical robustness to state-of-the-art methods while reducing computation
time by up to 82% and communication overhead by 50-70% depending on filtering
effectiveness, with benefits scaling multiplicatively with model dimensionality
and network connectivity. These results establish the viability of sketch-based
compression as a fundamental enabler of robust DFL at web scale.

</details>


### [10] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: BaRP是一种基于偏好调谐的在线路由方法，使用上下文多臂老虎机框架，在训练时模拟在线反馈环境，支持在推理时动态调整性能/成本权衡而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决LLM部署中的模型选择问题：现有路由器大多依赖离线训练，需要所有候选模型的标签，这在部署环境中不现实，因为只能观察到所选模型的结果。

Method: 将LLM路由建模为上下文多臂老虎机问题，基于提示特征和用户偏好向量，在训练时模拟在线反馈设置，自适应地为每个新提示做出路由决策。

Result: 实验表明，该方法比强离线路由器性能提升至少12.46%，比最大LLM性能提升至少2.45%，并且对未见任务具有鲁棒泛化能力。

Conclusion: BaRP方法在部分反馈限制下有效训练路由器，支持偏好可调推理，显著优于离线方法，为LLM部署提供了更实用的解决方案。

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [11] [From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill](https://arxiv.org/abs/2510.08055)
*Gunjun Lee,Jiwon Kim,Jaiyoung Park,Younjoo Lee,Jung Ho Ahn*

Main category: cs.LG

TL;DR: 提出分层预填充调度方法，通过将Transformer层组作为主要调度单元，在保持无停顿解码的同时消除分块导致的MoE权重重载，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现有分块预填充调度方法虽然能稳定TBT，但在MoE模型中会导致大量专家权重重载，增加内存流量和能耗。

Method: 将模型垂直划分为连续层组，在层组间交错进行预填充和解码，以层而非token为调度轴。

Result: 减少片外带宽需求，TTFT降低70%，端到端延迟降低41%，每token能耗降低22%，改善TTFT-TBT帕累托边界。

Conclusion: 从token到层的调度轴转换开启了高效节能LLM服务的新操作模式。

Abstract: Large Language Model (LLM) inference in production must meet stringent
service-level objectives for both time-to-first-token (TTFT) and
time-between-token (TBT) while maximizing throughput under fixed compute,
memory, and interconnect budgets. Modern serving systems adopt stall-free
scheduling techniques such as chunked prefill, which splits long prompt
processing along the token dimension and interleaves prefill with ongoing
decode iterations. While effective at stabilizing TBT, chunked prefill incurs
substantial overhead in Mixture-of-Experts (MoE) models: redundant expert
weight loads increase memory traffic by up to 39% and inflate energy
consumption. We propose layered prefill, a new scheduling paradigm that treats
transformer layer groups as the primary scheduling unit. By vertically
partitioning the model into contiguous layer groups and interleaving prefill
and decode across the groups, layered prefill sustains stall-free decoding
while eliminating chunk-induced MoE weight reloads. It reduces off-chip
bandwidth demand, lowering TTFT by up to 70%, End-to-End latency by 41% and
per-token energy by up to 22%. Evaluations show that layered prefill
consistently improves the TTFT--TBT Pareto frontier over chunked prefill,
reducing expert-load traffic and energy cost while maintaining stall-free
decoding. Overall, shifting the scheduling axis from tokens to layers unlocks a
new operating regime for high-efficiency, energy-aware LLM serving in
co-located environments.

</details>


### [12] [Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments](https://arxiv.org/abs/2510.07436)
*Ankur Naskar,Gugan Thoppe,Utsav Negi,Vijay Gupta*

Main category: cs.LG

TL;DR: 提出了一个参数无关的联邦时间差分学习算法，在马尔可夫数据下实现了最优的收敛速率，适用于异构环境的联邦学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有联邦强化学习在马尔可夫数据下需要依赖未知问题参数才能达到最优收敛速率，这限制了实际应用。

Method: 采用双时间尺度的联邦时间差分学习结合Polyak-Ruppert平均方法，构建参数无关的算法框架。

Result: 算法在平均奖励和折扣设置下都证明了能达到最优的O(1/NT)收敛速率，其中N是智能体数量，T是迭代次数。

Conclusion: 该方法填补了联邦强化学习在马尔可夫数据下的理论空白，为异构环境下的联邦学习提供了实用的解决方案。

Abstract: Federated learning (FL) can dramatically speed up reinforcement learning by
distributing exploration and training across multiple agents. It can guarantee
an optimal convergence rate that scales linearly in the number of agents, i.e.,
a rate of $\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the
number of agents. However, when the training samples arise from a Markov chain,
existing results on TD learning achieving this rate require the algorithm to
depend on unknown problem parameters. We close this gap by proposing a
two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert
averaging. Our method provably attains the optimal $\tilde{O}(1/NT)$ rate in
both average-reward and discounted settings--offering a parameter-free FTD
approach for Markovian data. Although our results are novel even in the
single-agent setting, they apply to the more realistic and challenging scenario
of FL with heterogeneous environments.

</details>


### [13] [DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems](https://arxiv.org/abs/2510.08522)
*Yuanjun Dai,Keqiang He,An Wang*

Main category: cs.LG

TL;DR: DYNAMIX是一个基于强化学习的批量大小优化框架，使用PPO算法将批量大小选择建模为序列决策问题，在异构动态计算环境中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式机器学习批量大小选择方法依赖静态分配或简单启发式，无法适应异构动态计算环境，导致训练效率低下。

Method: 采用强化学习框架，使用PPO算法，构建包含网络指标、系统资源利用率和训练统计效率的多维状态表示，无需显式系统建模即可与现有分布式训练框架集成。

Result: 在各种工作负载、硬件配置和网络条件下，DYNAMIX实现了最终模型精度提升6.3%，总训练时间减少46%。在扩展到32个节点时保持最佳性能，且学习到的策略能有效迁移到相关模型架构。

Conclusion: DYNAMIX通过强化学习方法有效解决了分布式机器学习中批量大小自适应优化问题，显著提升了训练效率和模型性能，具有良好的可扩展性和迁移能力。

Abstract: Existing batch size selection approaches in distributed machine learning rely
on static allocation or simplistic heuristics that fail to adapt to
heterogeneous, dynamic computing environments. We present DYNAMIX, a
reinforcement learning framework that formulates batch size optimization as a
sequential decision-making problem using Proximal Policy Optimization (PPO).
Our approach employs a multi-dimensional state representation encompassing
network-level metrics, system-level resource utilization, and training
statistical efficiency indicators to enable informed decision-making across
diverse computational resources. Our approach eliminates the need for explicit
system modeling while integrating seamlessly with existing distributed training
frameworks. Through evaluations across diverse workloads, hardware
configurations, and network conditions, DYNAMIX achieves up to 6.3% improvement
in the final model accuracy and 46% reduction in the total training time. Our
scalability experiments demonstrate that DYNAMIX maintains the best performance
as cluster size increases to 32 nodes, while policy transfer experiments show
that learned policies generalize effectively across related model
architectures.

</details>


### [14] [MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting](https://arxiv.org/abs/2510.07459)
*Yoli Shavit,Jacob Goldberger*

Main category: cs.LG

TL;DR: MoGU是一种用于时间序列预测的混合专家框架，通过高斯分布建模专家输出，并使用基于不确定性的门控机制来量化预测及其不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统混合专家方法只提供点估计，无法量化预测的不确定性。MoGU旨在解决这一问题，为回归任务提供更可靠的预测和不确定性估计。

Method: 将每个专家的输出建模为高斯分布，使用基于不确定性的门控机制（而非传统的基于输入的门控网络），通过专家估计的方差来确定其对最终预测的贡献。

Result: 在多个时间序列预测基准测试中，MoGU始终优于单专家模型和传统混合专家设置，并提供与预测误差直接相关的量化不确定性。

Conclusion: MoGU框架通过结合高斯分布建模和不确定性门控，显著提升了时间序列预测的准确性和可靠性，为回归任务提供了有效的解决方案。

Abstract: We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf

</details>


### [15] [metabeta -- A fast neural model for Bayesian mixed-effects regression](https://arxiv.org/abs/2510.07473)
*Alex Kipnis,Marcel Binz,Eric Schulz*

Main category: cs.LG

TL;DR: 提出了metabeta，一种基于Transformer的神经网络模型，用于贝叶斯混合效应回归，能够以比MCMC方法更少的时间达到可比较的性能


<details>
  <summary>Details</summary>
Motivation: 分层数据在实证科学中普遍存在，通常使用混合效应回归分析。贝叶斯推断能估计不确定性但解析上难以处理，需要昂贵的MCMC近似方法

Method: 使用基于Transformer的神经网络模型进行神经后验估计，将大部分计算从推理时间转移到预训练时间，通过模拟数据集进行摊销

Result: 在模拟和真实数据上，metabeta达到了与MCMC参数估计相当且稳定的性能，所需时间仅为通常所需时间的一小部分

Conclusion: metabeta提供了一种高效的贝叶斯混合效应回归方法，显著减少了计算时间同时保持了性能

Abstract: Hierarchical data with multiple observations per group is ubiquitous in
empirical sciences and is often analyzed using mixed-effects regression. In
such models, Bayesian inference gives an estimate of uncertainty but is
analytically intractable and requires costly approximation using Markov Chain
Monte Carlo (MCMC) methods. Neural posterior estimation shifts the bulk of
computation from inference time to pre-training time, amortizing over simulated
datasets with known ground truth targets. We propose metabeta, a
transformer-based neural network model for Bayesian mixed-effects regression.
Using simulated and real data, we show that it reaches stable and comparable
performance to MCMC-based parameter estimation at a fraction of the usually
required time.

</details>


### [16] [Surrogate Modeling for the Design of Optimal Lattice Structures using Tensor Completion](https://arxiv.org/abs/2510.07474)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出使用张量补全作为替代模型来加速材料设计，特别是在训练数据来自设计空间非均匀随机采样的场景下，相比传统机器学习方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 材料设计过程中，随着设计变量增加，搜索空间呈指数级增长，使得合成和验证每个材料的性能变得不切实际且耗时。特别是在训练数据来自非均匀随机采样时，传统机器学习方法表现不足。

Method: 使用张量补全作为替代模型来加速材料设计，特别针对设计空间存在偏差采样的场景。

Result: 实验表明，在存在偏差采样的情况下，张量补全优于高斯过程和XGBoost等经典机器学习方法，R²提高了约5%。在均匀随机采样下，张量补全仍能提供相当的性能。

Conclusion: 张量补全在材料设计领域，特别是在训练数据存在采样偏差的情况下，是一种有效的替代建模方法。

Abstract: When designing new materials, it is often necessary to design a material with
specific desired properties. Unfortunately, as new design variables are added,
the search space grows exponentially, which makes synthesizing and validating
the properties of each material very impractical and time-consuming. In this
work, we focus on the design of optimal lattice structures with regard to
mechanical performance. Computational approaches, including the use of machine
learning (ML) methods, have shown improved success in accelerating materials
design. However, these ML methods are still lacking in scenarios when training
data (i.e. experimentally validated materials) come from a non-uniformly random
sampling across the design space. For example, an experimentalist might
synthesize and validate certain materials more frequently because of
convenience. For this reason, we suggest the use of tensor completion as a
surrogate model to accelerate the design of materials in these atypical
supervised learning scenarios. In our experiments, we show that tensor
completion is superior to classic ML methods such as Gaussian Process and
XGBoost with biased sampling of the search space, with around 5\% increased
$R^2$. Furthermore, tensor completion still gives comparable performance with a
uniformly random sampling of the entire search space.

</details>


### [17] [HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data](https://arxiv.org/abs/2510.07477)
*Maria Mahbub,Robert J. Klein,Myvizhi Esai Selvan,Rowena Yip,Claudia Henschke,Providencia Morales,Ian Goethert,Olivera Kotevska,Mayanka Chandra Shekar,Sean R. Wilkinson,Eileen McAllister,Samuel M. Aguayo,Zeynep H. Gümüş,Ioana Danciu,VA Million Veteran Program*

Main category: cs.LG

TL;DR: HEMERA是一个基于可解释Transformer的深度学习框架，直接处理GWAS原始基因型数据预测肺癌风险，无需临床协变量，在27000多名参与者数据上达到>99% AUC，并能通过事后可解释性模块定位关键SNP。


<details>
  <summary>Details</summary>
Motivation: 肺癌是美国第三常见癌症和主要癌症死因，虽然吸烟是主要风险因素，但不吸烟者和家族聚集研究显示遗传因素的重要性。GWAS识别的遗传生物标志物有望用于肺癌风险评估。

Method: HEMERA框架应用可解释Transformer深度学习处理GWAS SNP数据，引入加性位置编码、神经基因型嵌入和精炼变体筛选，直接处理原始基因型数据而无需临床协变量。

Result: 在27,254名百万退伍军人计划参与者数据上训练，HEMERA达到>99% AUC分数。事后可解释性模块基于层间集成梯度，能将模型预测归因于特定SNP，与已知肺癌风险位点高度一致。

Conclusion: 研究支持使用透明、可生成假设的模型进行个性化肺癌风险评估和早期干预。

Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.

</details>


### [18] [Reinforcement Learning-based Task Offloading in the Internet of Wearable Things](https://arxiv.org/abs/2510.07487)
*Waleed Bin Qaim,Aleksandr Ometov,Claudia Campolo,Antonella Molinaro,Elena Simona Lohan,Jari Nurmi*

Main category: cs.LG

TL;DR: 提出基于强化学习的任务卸载框架，解决可穿戴设备在能量消耗和任务完成时间之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备面临电池容量有限和计算资源不足的挑战，而新兴应用对计算强度和延迟要求越来越高，需要任务卸载来提升用户体验。

Method: 将任务卸载问题建模为马尔可夫决策过程，使用Q-learning技术使可穿戴设备能够在无先验知识的情况下做出最优任务卸载决策。

Result: 通过ns-3网络模拟器进行广泛仿真，评估了不同应用和系统配置下的性能，并展示了Q-learning主要参数变化对任务完成时间、能量消耗和任务卸载比例的影响。

Conclusion: 提出的强化学习框架能够有效解决可穿戴设备任务卸载问题，在能量消耗和任务完成时间之间实现良好平衡。

Abstract: Over the years, significant contributions have been made by the research and
industrial sectors to improve wearable devices towards the Internet of Wearable
Things (IoWT) paradigm. However, wearables are still facing several challenges.
Many stem from the limited battery power and insufficient computation resources
available on wearable devices. On the other hand, with the popularity of smart
wearables, there is a consistent increase in the development of new
computationally intensive and latency-critical applications. In such a context,
task offloading allows wearables to leverage the resources available on nearby
edge devices to enhance the overall user experience. This paper proposes a
framework for Reinforcement Learning (RL)-based task offloading in the IoWT. We
formulate the task offloading process considering the tradeoff between energy
consumption and task accomplishment time. Moreover, we model the task
offloading problem as a Markov Decision Process (MDP) and utilize the
Q-learning technique to enable the wearable device to make optimal task
offloading decisions without prior knowledge. We evaluate the performance of
the proposed framework through extensive simulations for various applications
and system configurations conducted in the ns-3 network simulator. We also show
how varying the main system parameters of the Q-learning algorithm affects the
overall performance in terms of average task accomplishment time, average
energy consumption, and percentage of tasks offloaded.

</details>


### [19] [Black-box Detection of LLM-generated Text Using Generalized Jensen-Shannon Divergence](https://arxiv.org/abs/2510.07500)
*Shuangyi Chen,Ashish Khisti*

Main category: cs.LG

TL;DR: 提出SurpMark方法，通过量化token surprisal的动态变化来检测机器生成文本，使用广义Jensen-Shannon差距比较测试文本与人类/机器参考文本的状态转移矩阵。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中机器生成文本检测的挑战：评分模型可能与未知源模型不匹配，且逐输入对比生成成本高昂。

Method: 将surprisal量化为可解释状态，估计测试文本的状态转移矩阵，通过广义Jensen-Shannon差距与预先构建的人类和机器参考进行比较。

Result: 在多个数据集、源模型和场景下，SurpMark始终匹配或超越基线方法，实验验证了统计量的渐近正态性和离散化方法的有效性。

Conclusion: SurpMark是一种有效的基于参考的检测器，能够在实际约束下可靠地检测机器生成文本。

Abstract: We study black-box detection of machine-generated text under practical
constraints: the scoring model (proxy LM) may mismatch the unknown source
model, and per-input contrastive generation is costly. We propose SurpMark, a
reference-based detector that summarizes a passage by the dynamics of its token
surprisals. SurpMark quantizes surprisals into interpretable states, estimates
a state-transition matrix for the test text, and scores it via a generalized
Jensen-Shannon (GJS) gap between the test transitions and two fixed references
(human vs. machine) built once from historical corpora. We prove a principled
discretization criterion and establish the asymptotic normality of the decision
statistic. Empirically, across multiple datasets, source models, and scenarios,
SurpMark consistently matches or surpasses baselines; our experiments
corroborate the statistic's asymptotic normality, and ablations validate the
effectiveness of the proposed discretization.

</details>


### [20] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: PEAR基准测试系统评估规划者-执行者多智能体系统的效用和脆弱性，发现规划者比执行者更关键，攻击规划者更有效，存在性能与鲁棒性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常检查孤立的攻击面或特定场景，缺乏对多智能体系统漏洞的整体理解。

Method: 引入PEAR基准测试，专注于规划者-执行者结构，通过广泛实验评估系统性能。

Result: 发现弱规划者比弱执行者更严重影响性能；规划者需要记忆模块而执行者不需要；存在性能与鲁棒性权衡；攻击规划者特别有效。

Conclusion: 为增强多智能体系统鲁棒性提供可行见解，为多智能体环境中的原则性防御奠定基础。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [21] [Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift](https://arxiv.org/abs/2510.07509)
*Tianyu Bell Pan,Damon L. Woodard*

Main category: cs.LG

TL;DR: 本文提出了一种多模态协同训练框架，通过利用未标记数据和促进不同模态分类器之间的一致性来增强模型在有限标记数据和分布偏移情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决在标记数据有限且存在分布偏移的情况下，如何有效提升模型泛化能力的问题。

Method: 采用多模态协同训练框架，利用未标记数据，促进不同模态分类器之间的一致性，并保持条件视图独立性。

Result: 建立了新的泛化边界，首次在多模态协同训练背景下分解并量化了利用未标记多模态数据、促进视图间一致性和保持条件视图独立性带来的优势。

Conclusion: 多模态协同训练作为一种结构化方法，能够有效开发数据高效且鲁棒的AI系统，在动态真实环境中实现良好泛化。

Abstract: This paper explores a multimodal co-training framework designed to enhance
model generalization in situations where labeled data is limited and
distribution shifts occur. We thoroughly examine the theoretical foundations of
this framework, deriving conditions under which the use of unlabeled data and
the promotion of agreement between classifiers for different modalities lead to
significant improvements in generalization. We also present a convergence
analysis that confirms the effectiveness of iterative co-training in reducing
classification errors. In addition, we establish a novel generalization bound
that, for the first time in a multimodal co-training context, decomposes and
quantifies the distinct advantages gained from leveraging unlabeled multimodal
data, promoting inter-view agreement, and maintaining conditional view
independence. Our findings highlight the practical benefits of multimodal
co-training as a structured approach to developing data-efficient and robust AI
systems that can effectively generalize in dynamic, real-world environments.
The theoretical foundations are examined in dialogue with, and in advance of,
established co-training principles.

</details>


### [22] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: MLLM4TS是一个新颖的多模态时间序列分析框架，通过将时间序列数据转换为可视化图像，利用多模态大语言模型进行综合分析，在分类、异常检测和预测等任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析面临复杂时间依赖性和跨通道交互的挑战，受人类视觉检查时间序列模式的启发，探索将视觉表示融入自动化时间序列分析的可能性。

Method: 提出MLLM4TS框架，将每个时间序列通道渲染为水平堆叠的彩色线图，采用时间感知视觉补丁对齐策略，将视觉补丁与对应时间片段对齐，融合数值数据的细粒度时间细节和视觉表示的全局上下文信息。

Result: 在标准基准测试上的广泛实验表明，MLLM4TS在预测任务（如分类）和生成任务（如异常检测和预测）上都表现出有效性。

Conclusion: 将视觉模态与预训练语言模型结合，为实现稳健且可泛化的时间序列分析提供了潜力。

Abstract: Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.

</details>


### [23] [EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning](https://arxiv.org/abs/2510.07524)
*Mehdi Zekriyapanah Gashti,Ghasem Farjamnia*

Main category: cs.LG

TL;DR: 提出基于小波变换的自动睡眠分期框架，使用连续小波变换生成时频图，结合集成学习实现88.37%的准确率和73.15的宏平均F1分数，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 准确的睡眠分期对睡眠障碍诊断和管理至关重要，传统方法依赖人工标注或时频域特征提取，需要更自动化和准确的方法。

Method: 使用连续小波变换生成时频图捕捉睡眠分期的瞬态和振荡模式，结合集成学习方法进行分类。

Result: 在Sleep-EDF扩展数据库上达到88.37%的总体准确率和73.15的宏平均F1分数，优于传统机器学习方法，与深度学习方法相当或更优。

Conclusion: 小波分析为稳健、可解释且临床适用的睡眠分期分类提供了潜力。

Abstract: Accurate classification of sleep stages is crucial for the diagnosis and
management of sleep disorders. Conventional approaches for sleep scoring rely
on manual annotation or features extracted from EEG signals in the time or
frequency domain. This study proposes a novel framework for automated sleep
stage scoring using time-frequency analysis based on the wavelet transform. The
Sleep-EDF Expanded Database (sleep-cassette recordings) was used for
evaluation. The continuous wavelet transform (CWT) generated time-frequency
maps that capture both transient and oscillatory patterns across frequency
bands relevant to sleep staging. Experimental results demonstrate that the
proposed wavelet-based representation, combined with ensemble learning,
achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of
73.15, outperforming conventional machine learning methods and exhibiting
comparable or superior performance to recent deep learning approaches. These
findings highlight the potential of wavelet analysis for robust, interpretable,
and clinically applicable sleep stage classification.

</details>


### [24] [Estimating Fair Graphs from Graph-Stationary Data](https://arxiv.org/abs/2510.07536)
*Madeline Navarro,Andrei Buciulea,Samuel Rey,Antonio G. Marques,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出FairSpecTemp方法从图平稳节点观测中估计公平图，确保连接不偏向敏感属性，包含两种变体：一种直接约束偏差，另一种通过限制图谱偏差隐式鼓励公平估计。


<details>
  <summary>Details</summary>
Motivation: 现实世界图中的边往往表现出对某些群体对的连接偏好，这种有偏连接会加剧甚至导致下游基于图的任务的不公平处理。

Method: 提出FairSpecTemp优化方法，包含两种变体：一种利用图平稳性的交换性质直接约束偏差，另一种通过限制图谱中的偏差隐式鼓励公平估计。

Result: 方法具有高概率性能界限，在公平性和准确性之间产生条件权衡，分析表明恢复公平图不必牺牲准确性。在合成和真实数据集上验证了有效性。

Conclusion: FairSpecTemp能有效估计公平图，两种变体各有优势，证明了在保持准确性的同时实现图公平性的可行性。

Abstract: We estimate fair graphs from graph-stationary nodal observations such that
connections are not biased with respect to sensitive attributes. Edges in
real-world graphs often exhibit preferences for connecting certain pairs of
groups. Biased connections can not only exacerbate but even induce unfair
treatment for downstream graph-based tasks. We therefore consider group and
individual fairness for graphs corresponding to group- and node-level
definitions, respectively. To evaluate the fairness of a given graph, we
provide multiple bias metrics, including novel measurements in the spectral
domain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an
optimization-based method with two variants for estimating fair graphs from
stationary graph signals, a general model for graph data subsuming many
existing ones. One variant of FairSpecTemp exploits commutativity properties of
graph stationarity while directly constraining bias, while the other implicitly
encourages fair estimates by restricting bias in the graph spectrum and is thus
more flexible. Our methods enjoy high probability performance bounds, yielding
a conditional tradeoff between fairness and accuracy. In particular, our
analysis reveals that accuracy need not be sacrificed to recover fair graphs.
We evaluate FairSpecTemp on synthetic and real-world data sets to illustrate
its effectiveness and highlight the advantages of both variants of
FairSpecTemp.

</details>


### [25] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: 提出了一种构建目标数字孪生(tDT)的数值框架，通过基于记忆的流映射学习(FML)直接建模感兴趣量(QoIs)的动态，无需完整数字孪生仿真即可高效预测长期动态。


<details>
  <summary>Details</summary>
Motivation: 为了在数字孪生中直接建模感兴趣量的动态，避免完整系统仿真的计算开销，实现计算效率的提升。

Method: 使用基于记忆的流映射学习(FML)方法，利用完整数字孪生生成的短轨迹数据构建数据驱动模型，整个过程为离线计算。

Result: 在二维不可压缩圆柱绕流CFD示例中，tDT能够准确预测长期流体动力，完全绕过完整流场仿真。

Conclusion: 该方法成功构建了紧凑的动态系统，能够高效准确地预测感兴趣量的长期动态，显著节省计算资源。

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [26] [Phase Diagram of Dropout for Two-Layer Neural Networks in the Mean-Field Regime](https://arxiv.org/abs/2510.07554)
*Lénaïc Chizat,Pierre Marion,Yerkin Yesbay*

Main category: cs.LG

TL;DR: 该论文研究了在两层神经网络中使用dropout训练的大宽度渐近行为，发现了五个不同的非退化相，揭示了dropout的"惩罚"效应只在极小的学习率下存在，而在较大学习率下等价于"随机几何"技术。


<details>
  <summary>Details</summary>
Motivation: 作为理解dropout在大型神经网络中作用的第一步，研究在均值场初始化尺度下，带有dropout的梯度下降在两层神经网络中的大宽度渐近行为。

Method: 使用均值场粒子系统和随机过程的工具，分析梯度下降与dropout在两层神经网络中的大宽度渐近行为，获得丰富的渐近相图。

Result: 发现了五个不同的非退化相，取决于dropout率、学习率和宽度的相对大小。dropout的惩罚效应只在O(1/宽度)量级的极小学习率下存在，在较大学习率下等价于随机几何技术。

Conclusion: 研究结果为重新理论理解大规模神经网络中的dropout奠定了基础，揭示了dropout在不同学习率下的不同作用机制。

Abstract: Dropout is a standard training technique for neural networks that consists of
randomly deactivating units at each step of their gradient-based training. It
is known to improve performance in many settings, including in the large-scale
training of language or vision models. As a first step towards understanding
the role of dropout in large neural networks, we study the large-width
asymptotics of gradient descent with dropout on two-layer neural networks with
the mean-field initialization scale. We obtain a rich asymptotic phase diagram
that exhibits five distinct nondegenerate phases depending on the relative
magnitudes of the dropout rate, the learning rate, and the width. Notably, we
find that the well-studied "penalty" effect of dropout only persists in the
limit with impractically small learning rates of order $O(1/\text{width})$. For
larger learning rates, this effect disappears and in the limit, dropout is
equivalent to a "random geometry" technique, where the gradients are thinned
randomly after the forward and backward pass have been computed. In this
asymptotic regime, the limit is described by a mean-field jump process where
the neurons' update times follow independent Poisson or Bernoulli clocks
(depending on whether the learning rate vanishes or not). For some of the
phases, we obtain a description of the limit dynamics both in path-space and in
distribution-space. The convergence proofs involve a mix of tools from
mean-field particle systems and stochastic processes. Together, our results lay
the groundwork for a renewed theoretical understanding of dropout in
large-scale neural networks.

</details>


### [27] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: 使用BERTopic主题建模技术分析lmsys-chat-1m多语言对话数据集，揭示对话主题模式及其与用户偏好的关系，发现特定LLM在某些主题中表现更优。


<details>
  <summary>Details</summary>
Motivation: 分析多语言对话数据集中用户对不同LLM响应的偏好模式，探索特定主题下哪些LLM更受用户青睐，为领域特定优化提供依据。

Method: 设计多语言预处理流程，应用BERTopic提取29个连贯主题，使用主题间距离图、概率分布和模型-主题矩阵等可视化技术分析关系。

Result: 成功识别出人工智能、编程、伦理、云基础设施等主题，发现模型偏好与特定主题存在关联性，某些LLM在特定主题中表现更优。

Conclusion: 研究结果为领域特定微调和优化策略提供信息，有助于提升LLM在实际应用中的性能和用户满意度。

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [28] [EBGAN-MDN: An Energy-Based Adversarial Framework for Multi-Modal Behavior Cloning](https://arxiv.org/abs/2510.07562)
*Yixiao Li,Julia Barth,Thomas Kiefer,Ahmad Fraij*

Main category: cs.LG

TL;DR: EBGAN-MDN框架通过结合能量模型、混合密度网络和对抗训练，有效解决了多模态行为克隆中的模式平均和模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 多模态行为克隆面临模式平均和模式崩溃的挑战，传统模型无法捕捉多样化的输入-输出映射，这在机器人等应用中至关重要，因为建模多个有效动作能确保性能和安全。

Method: 提出EBGAN-MDN框架，整合能量模型、混合密度网络和对抗训练，利用改进的InfoNCE损失和能量强化的MDN损失来应对挑战。

Result: 在合成和机器人基准测试上的实验表明，EBGAN-MDN表现出优越的性能。

Conclusion: EBGAN-MDN是多模态学习任务中有效且高效的解决方案。

Abstract: Multi-modal behavior cloning faces significant challenges due to mode
averaging and mode collapse, where traditional models fail to capture diverse
input-output mappings. This problem is critical in applications like robotics,
where modeling multiple valid actions ensures both performance and safety. We
propose EBGAN-MDN, a framework that integrates energy-based models, Mixture
Density Networks (MDNs), and adversarial training. By leveraging a modified
InfoNCE loss and an energy-enforced MDN loss, EBGAN-MDN effectively addresses
these challenges. Experiments on synthetic and robotic benchmarks demonstrate
superior performance, establishing EBGAN-MDN as a effective and efficient
solution for multi-modal learning tasks.

</details>


### [29] [Automated Machine Learning for Unsupervised Tabular Tasks](https://arxiv.org/abs/2510.07569)
*Prabhant Singh,Pieter Gijsbers,Elif Ceren Gok Yildirim,Murat Onur Yildirim,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: LOTUS是一个用于无监督机器学习任务模型选择的简单有效方法，通过最优传输距离衡量数据集分布相似性来推荐机器学习流程


<details>
  <summary>Details</summary>
Motivation: 机器学习流程在新数据集上表现良好的前提是它在具有相似底层数据分布的数据集上表现良好

Method: 使用最优传输距离计算未标记表格数据集之间的相似性，为异常检测和聚类两个无监督任务推荐机器学习流程

Result: 实验表明LOTUS相比强基线方法表现优异，是无监督ML任务模型选择的有前景的第一步

Conclusion: LOTUS为多个无监督机器学习任务提供了一种统一有效的模型选择方法

Abstract: In this work, we present LOTUS (Learning to Learn with Optimal Transport for
Unsupervised Scenarios), a simple yet effective method to perform model
selection for multiple unsupervised machine learning(ML) tasks such as outlier
detection and clustering. Our intuition behind this work is that a machine
learning pipeline will perform well in a new dataset if it previously worked
well on datasets with a similar underlying data distribution. We use Optimal
Transport distances to find this similarity between unlabeled tabular datasets
and recommend machine learning pipelines with one unified single method on two
downstream unsupervised tasks: outlier detection and clustering. We present the
effectiveness of our approach with experiments against strong baselines and
show that LOTUS is a very promising first step toward model selection for
multiple unsupervised ML tasks.

</details>


### [30] [Symbolic-Diffusion: Deep Learning Based Symbolic Regression with D3PM Discrete Token Diffusion](https://arxiv.org/abs/2510.07570)
*Ryan T. Tymkow,Benjamin D. Schnapp,Mojtaba Valipour,Ali Ghodshi*

Main category: cs.LG

TL;DR: 提出了一种基于离散扩散模型的符号回归方法Symbolic Diffusion，能够同时生成所有方程标记，相比自回归方法在某些指标上表现更好。


<details>
  <summary>Details</summary>
Motivation: 自回归生成方法只能从左到右生成标记，且未来生成的标记仅依赖于先前标记。为了同时生成所有标记以产生更好的闭式方程，提出了基于扩散的生成方法。

Method: 使用D3PM基础的离散状态空间扩散模型，通过离散标记扩散同时生成方程的所有标记。

Result: 在SymbolicGPT的双变量数据集上，扩散生成方法在使用相似编码器和变换器架构时，相比自回归模型具有可比性且在部分指标上表现更优。

Conclusion: 扩散生成方法为基于神经网络的符号回归开辟了新的研究机会，能够提供与自回归生成相当甚至更好的性能。

Abstract: Symbolic regression refers to the task of finding a closed-form mathematical
expression to fit a set of data points. Genetic programming based techniques
are the most common algorithms used to tackle this problem, but recently,
neural-network based approaches have gained popularity. Most of the leading
neural-network based models used for symbolic regression utilize
transformer-based autoregressive models to generate an equation conditioned on
encoded input points. However, autoregressive generation is limited to
generating tokens left-to-right, and future generated tokens are conditioned
only on previously generated tokens. Motivated by the desire to generate all
tokens simultaneously to produce improved closed-form equations, we propose
Symbolic Diffusion, a D3PM based discrete state-space diffusion model which
simultaneously generates all tokens of the equation at once using discrete
token diffusion. Using the bivariate dataset developed for SymbolicGPT, we
compared our diffusion-based generation approach to an autoregressive model
based on SymbolicGPT, using equivalent encoder and transformer architectures.
We demonstrate that our novel approach of using diffusion-based generation for
symbolic regression can offer comparable and, by some metrics, improved
performance over autoregressive generation in models using similar underlying
architectures, opening new research opportunities in neural-network based
symbolic regression.

</details>


### [31] [Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks](https://arxiv.org/abs/2510.07578)
*Shilong Zong,Alex Bierly,Almuatazbellah Boker,Hoda Eldardiry*

Main category: cs.LG

TL;DR: 对液态神经网络(LNNs)与传统循环神经网络(RNNs)及其变体(LSTMs、GRUs)在模型精度、内存效率和泛化能力方面的比较分析。LNN作为新兴的生物启发连续时间动态神经网络，在处理噪声、非平稳数据和实现分布外泛化方面表现出潜力，但在可扩展性方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 比较新兴的液态神经网络与传统循环神经网络在序列数据处理方面的性能差异，探索不同神经网络架构在处理时序数据时的优势和局限性。

Method: 通过系统回顾现有研究，分析LNNs和RNNs的基本原理、数学模型、关键特征和固有挑战，进行多维度比较分析。

Result: LNN在处理噪声和非平稳数据、实现分布外泛化方面表现优异，某些LNN变体在参数效率和计算速度上优于传统RNN。但RNN因其成熟的生态系统和广泛成功应用，仍是序列建模的基石。

Conclusion: LNNs和RNNs各有优势，LNN在特定场景下表现更好，但需要改进可扩展性以应用于更广泛复杂的场景。未来研究应关注提升LNN的可扩展性。

Abstract: This review aims to conduct a comparative analysis of liquid neural networks
(LNNs) and traditional recurrent neural networks (RNNs) and their variants,
such as long short-term memory networks (LSTMs) and gated recurrent units
(GRUs). The core dimensions of the analysis include model accuracy, memory
efficiency, and generalization ability. By systematically reviewing existing
research, this paper explores the basic principles, mathematical models, key
characteristics, and inherent challenges of these neural network architectures
in processing sequential data. Research findings reveal that LNN, as an
emerging, biologically inspired, continuous-time dynamic neural network,
demonstrates significant potential in handling noisy, non-stationary data, and
achieving out-of-distribution (OOD) generalization. Additionally, some LNN
variants outperform traditional RNN in terms of parameter efficiency and
computational speed. However, RNN remains a cornerstone in sequence modeling
due to its mature ecosystem and successful applications across various tasks.
This review identifies the commonalities and differences between LNNs and RNNs,
summarizes their respective shortcomings and challenges, and points out
valuable directions for future research, particularly emphasizing the
importance of improving the scalability of LNNs to promote their application in
broader and more complex scenarios.

</details>


### [32] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 该论文提出了Expanded Action空间(ExpA)，将环境交互从语言中分离，允许LLM在语言环境和外部环境之间自由切换，并引入ExpA强化学习(EARL)来优化策略。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的环境交互必须通过预定义格式的文本表达，这既增加了语言负担又需要外部解析器。作者希望将环境交互内部化，让LLM能直接与环境互动。

Method: 提出ExpA框架，允许LLM在语言环境和外部环境之间路由切换；引入EARL强化学习方法，使用反事实策略优化来探索扩展的动作空间。

Result: 在需要多轮交互和条件规划的任务上，EARL优于词汇约束动作的基线方法；在计算器多任务学习中表现稳健；在部分观察排序问题中实现了100%的Sort-4准确率，并自发现了与经典设计相竞争的高效算法。

Conclusion: ExpA框架成功地将环境交互从语言中分离，EARL方法能够有效探索扩展动作空间，在复杂交互任务中表现出色，为LLM与环境的高效交互提供了新思路。

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


### [33] [TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs](https://arxiv.org/abs/2510.07586)
*Jacob Chmura,Shenyang Huang,Tran Gia Bao Ngo,Ali Parviz,Farimah Poursafaei,Jure Leskovec,Michael Bronstein,Guillaume Rabusseau,Matthias Fey,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: TGM是一个研究导向的时序图机器学习库，首次统一了连续时间和离散时间动态图方法，提供动态节点特征支持、时间粒度转换等功能，相比现有库实现7.8倍到175倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的时序图机器学习库通常针对特定架构设计，缺乏统一的基础设施，且连续时间和离散时间动态图方法之间存在隔阂，限制了不同模型之间的直接比较和思想交流。

Method: 开发了Temporal Graph Modelling (TGM)库，统一处理连续时间和离散时间动态图方法，支持动态节点特征、时间粒度转换，以及链接级、节点级和图级任务的本地处理。

Result: TGM在多个模型、数据集和任务上相比广泛使用的DyGLib实现了平均7.8倍的加速，在图离散化方面相比现有实现实现了平均175倍的加速。

Conclusion: TGM不仅提高了效率，还通过支持动态图属性预测和时间驱动训练范式，开启了全新的研究可能性，为之前难以研究的问题提供了解决方案。

Abstract: Well-designed open-source software drives progress in Machine Learning (ML)
research. While static graph ML enjoys mature frameworks like PyTorch Geometric
and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks
comparable infrastructure. Existing TG libraries are often tailored to specific
architectures, hindering support for diverse models in this rapidly evolving
field. Additionally, the divide between continuous- and discrete-time dynamic
graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To
address these gaps, we introduce Temporal Graph Modelling (TGM), a
research-oriented library for ML on temporal graphs, the first to unify CTDG
and DTDG approaches. TGM offers first-class support for dynamic node features,
time-granularity conversions, and native handling of link-, node-, and
graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across
multiple models, datasets, and tasks compared to the widely used DyGLib, and an
average 175x speedup on graph discretization relative to available
implementations. Beyond efficiency, we show in our experiments how TGM unlocks
entirely new research possibilities by enabling dynamic graph property
prediction and time-driven training paradigms, opening the door to questions
previously impractical to study. TGM is available at
https://github.com/tgm-team/tgm

</details>


### [34] [Transformer-Based Indirect Structural Health Monitoring of Rail Infrastructure with Attention-Driven Detection and Localization of Transient Defects](https://arxiv.org/abs/2510.07606)
*Sizhe Ma,Katherine A. Flanigan,Mario Bergés,James D. Brooks*

Main category: cs.LG

TL;DR: 该研究提出了基于注意力机制的Transformer模型，用于铁路间接结构健康监测中的断轨检测，通过无监督学习方法解决小尺寸瞬态异常检测难题，并在合成数据基准测试中验证了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于车载传感器的间接结构健康监测方法在检测小尺寸（2-10厘米）瞬态断轨异常时面临挑战，主要由于复杂的车辆动力学、信号噪声以及标记数据稀缺限制了监督学习方法的应用。

Method: 提出注意力聚焦Transformer模型，采用自注意力机制，通过重构训练但主要从学习到的注意力权重偏差中推导异常分数，旨在实现有效性和计算效率的平衡。建立了增量合成数据基准来系统评估模型鲁棒性。

Result: 基准测试结果显示，基于Transformer的模型普遍优于其他模型，但所有测试模型都对高频局部噪声表现出显著脆弱性。提出的模型在保持与最先进解决方案相当准确度的同时，展现出更好的推理速度。

Conclusion: 研究强调了未来iSHM模型需要增强噪声鲁棒性的关键需求，并将提出的基于注意力的高效方法定位为开发实用车载异常检测系统的有前景基础。

Abstract: Indirect structural health monitoring (iSHM) for broken rail detection using
onboard sensors presents a cost-effective paradigm for railway track
assessment, yet reliably detecting small, transient anomalies (2-10 cm) remains
a significant challenge due to complex vehicle dynamics, signal noise, and the
scarcity of labeled data limiting supervised approaches. This study addresses
these issues through unsupervised deep learning. We introduce an incremental
synthetic data benchmark designed to systematically evaluate model robustness
against progressively complex challenges like speed variations, multi-channel
inputs, and realistic noise patterns encountered in iSHM. Using this benchmark,
we evaluate several established unsupervised models alongside our proposed
Attention-Focused Transformer. Our model employs a self-attention mechanism,
trained via reconstruction but innovatively deriving anomaly scores primarily
from deviations in learned attention weights, aiming for both effectiveness and
computational efficiency. Benchmarking results reveal that while
transformer-based models generally outperform others, all tested models exhibit
significant vulnerability to high-frequency localized noise, identifying this
as a critical bottleneck for practical deployment. Notably, our proposed model
achieves accuracy comparable to the state-of-the-art solution while
demonstrating better inference speed. This highlights the crucial need for
enhanced noise robustness in future iSHM models and positions our more
efficient attention-based approach as a promising foundation for developing
practical onboard anomaly detection systems.

</details>


### [35] [DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support](https://arxiv.org/abs/2510.07620)
*Muhammad Usman,Yugyung Lee*

Main category: cs.LG

TL;DR: DGTEN是一个用于动态信任评估的图神经网络框架，通过高斯分布表示节点和边的不确定性，结合混合位置编码和ODE残差学习来捕捉信任演化，并采用鲁棒集成分析抵御对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 大型快速演化的图需要能够捕捉变化关系、表达校准置信度并抵抗对抗操纵的信任评估模型。现有方法在处理动态信任关系、不确定性建模和对抗攻击防御方面存在不足。

Method: 1. 使用高斯分布表示节点和边的不确定性传播；2. 采用HAGH位置编码和KAN-based注意力机制；3. ODE残差学习模块捕捉突变和平滑趋势；4. 鲁棒自适应集成系数分析使用余弦和Jaccard相似度过滤可疑交互。

Result: 在两个比特币信任网络上，DGTEN显著优于基线：在Bitcoin-Alpha单时间槽预测中MCC提升10.77%；冷启动场景下MCC增益16.41%；对抗攻击下MCC提升达11.63%。

Conclusion: DGTEN统一框架在动态信任评估中表现出色，有效解决了不确定性建模、时间演化和对抗攻击防御等关键挑战。

Abstract: Dynamic trust evaluation in large, rapidly evolving graphs requires models
that can capture changing relationships, express calibrated confidence, and
resist adversarial manipulation. DGTEN (Deep Gaussian-based Trust Evaluation
Network) introduces a unified graph framework that achieves all three by
combining uncertainty-aware message passing, expressive temporal modeling, and
built-in defenses against trust-targeted attacks. It represents nodes and edges
as Gaussian distributions so that both semantic signals and epistemic
uncertainty propagate through the graph neural network, enabling risk-aware
trust decisions rather than overconfident guesses. To model how trust evolves,
it employs hybrid Absolute-Gaussian-Hourglass (HAGH) positional encoding with
Kolmogorov-Arnold network-based unbiased multi-head attention, followed by an
ordinary differential equation (ODE)-based residual learning module to jointly
capture abrupt shifts and smooth trends. Robust adaptive ensemble coefficient
analysis prunes or down-weights suspicious interactions using complementary
cosine and Jaccard similarity measures, mitigating reputation laundering,
sabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTEN
delivers significant improvements: in single-timeslot prediction on
Bitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in the
cold-start scenario, it achieves a 16.41% MCC gain - the largest across all
tasks and datasets. Under adversarial on/off attacks, it surpasses the baseline
by up to 11.63% MCC. These results validate the effectiveness of the unified
DGTEN framework.

</details>


### [36] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: 本文对大型语言模型（LLM）的机器遗忘方法进行了系统分类和评估，提出了更全面的评估指标，揭示了当前评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM遗忘研究缺乏统一标准和有效评估方法，现有评估主要依赖选择题准确率，无法全面反映模型的实际生成行为，需要更系统的评估框架。

Method: 提出了12种状态遗忘方法的分类体系，分为三类：差异驱动优化、表示错位和基于拒绝的目标遗忘。引入开放式问答指标来评估生成性能，并对WMDP基准进行重新分析。

Result: 发现当前评估方法过度强调成功，忽略了模型的实际生成行为。开放式问答指标更好地揭示了遗忘效果与效用保持之间的权衡关系，并显示不同攻击场景下的鲁棒性差异。

Conclusion: 为LLM遗忘研究提供了全面的重新审视和可操作的指导，强调需要更精细的评估方法来设计未来的遗忘方法。

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [37] [Property Classification of Vacation Rental Properties during Covid-19](https://arxiv.org/abs/2510.07639)
*Favour Yahdii Aghaebe,Dustin Foley,Eric Atwell,Stephen Clark*

Main category: cs.LG

TL;DR: 使用聚类技术对疫情期间活跃的度假租赁房产进行分类，识别内在模式和行为的模式。


<details>
  <summary>Details</summary>
Motivation: 通过聚类分析度假租赁房产，以识别疫情期间的固有模式和宿主行为，从而更好地理解度假租赁评估的复杂性，并为制定针对性的集群特定政策提供支持。

Method: 利用K-means和K-medoids聚类技术对包含超过一百万房产和宿主的数据集进行分析，识别同质群体及其共同特征。

Result: 研究发现能够识别出同质群体及其共同特征，增强了对度假租赁评估复杂性的理解。

Conclusion: 聚类分析有助于理解度假租赁市场的内在模式，并可能用于制定针对特定集群的政策。

Abstract: This study advocates for employing clustering techniques to classify vacation
rental properties active during the Covid pandemic to identify inherent
patterns and behaviours. The dataset, a collaboration between the ESRC funded
Consumer Data Research Centre (CDRC) and AirDNA, encompasses data for over a
million properties and hosts. Utilising K-means and K-medoids clustering
techniques, we identify homogenous groups and their common characteristics. Our
findings enhance comprehension of the intricacies of vacation rental
evaluations and could potentially be utilised in the creation of targeted,
cluster-specific policies.

</details>


### [38] [Design-Based Bandits Under Network Interference: Trade-Off Between Regret and Statistical Inference](https://arxiv.org/abs/2510.07646)
*Zichen Wang,Haoyang Hong,Chuanhao Li,Haoxuan Li,Zhiheng Zhang,Huazheng Wang*

Main category: cs.LG

TL;DR: 本文首次在对抗性网络干扰多臂老虎机中建立了遗憾最小化与推断精度之间的帕累托前沿理论，并提出了EXP3-N-CS算法来平衡这一权衡。


<details>
  <summary>Details</summary>
Motivation: 现有网络干扰多臂老虎机研究主要关注遗憾最小化，但过度强调最优臂会损害次优臂的推断准确性。虽然在单单元场景中已有初步尝试，但在网络干扰环境下这一挑战更加显著。

Method: 建立了对抗性网络干扰多臂老虎机中遗憾最小化与推断精度之间的理论帕累托前沿，并设计了EXP3-N-CS算法，该算法包含任意时间有效的渐近置信序列。

Result: 首次在对抗性网络干扰多臂老虎机中形式化了遗憾最小化与推断精度之间的权衡关系，并提供了相应的算法解决方案。

Conclusion: 该研究为网络干扰多臂老虎机中的探索-利用权衡提供了新的理论框架和实用算法，平衡了遗憾最小化和推断精度两个关键目标。

Abstract: In multi-armed bandits with network interference (MABNI), the action taken by
one node can influence the rewards of others, creating complex interdependence.
While existing research on MABNI largely concentrates on minimizing regret, it
often overlooks the crucial concern that an excessive emphasis on the optimal
arm can undermine the inference accuracy for sub-optimal arms. Although initial
efforts have been made to address this trade-off in single-unit scenarios,
these challenges have become more pronounced in the context of MABNI. In this
paper, we establish, for the first time, a theoretical Pareto frontier
characterizing the trade-off between regret minimization and inference accuracy
in adversarial (design-based) MABNI. We further introduce an anytime-valid
asymptotic confidence sequence along with a corresponding algorithm,
$\texttt{EXP3-N-CS}$, specifically designed to balance the trade-off between
regret minimization and inference accuracy in this setting.

</details>


### [39] [Continual Learning for Adaptive AI Systems](https://arxiv.org/abs/2510.07648)
*Md Hasibul Amin,Tamzid Tanvi Alam*

Main category: cs.LG

TL;DR: 提出了一种基于类间分离(ICS)的新型正则化方法，通过在损失函数中惩罚远离先前任务数据聚类中心的输出来缓解持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习是神经网络在不丢失先前获取知识的情况下学习多个顺序任务的能力，这是开发真正自适应人工智能的重要障碍。深度学习中过拟合是常见问题，需要正则化技术来防止灾难性遗忘。

Method: 在损失函数中引入基于类间分离(ICS)的正则化项，惩罚模型产生远离先前任务数据聚类中心的输出。同时进行超参数调优以找到最佳正则化权重，确保神经网络内部表示中任务间更清晰的分离。

Result: 在标准的5任务Split CIFAR-10基准测试和ResNet-18架构上，ICS在保持初始任务性能方面表现出有效性，但在长期知识保留方面存在局限性，特别是当任务数量增加时。

Conclusion: 该方法凸显了持续学习中的复杂性和权衡，指出了进一步研究的方向，特别是在长期知识保留方面的改进空间。

Abstract: Continual learning the ability of a neural network to learn multiple
sequential tasks without losing previously acquired knowledge remains a
significant obstacle to developing truly adaptive artificial intelligence. Deep
learning models have achieved remarkable results in various applications, but
overfitting remains a common issue. Regularization techniques can help prevent
overfitting by adding constraints to the model's parameters. To prevent
catastrophic forgetting, in this paper we introduce a novel regularization
technique based on inter-cluster separation (ICS) in the loss function, which
penalizes the model for producing outputs that are far away from the centroids
of the clusters formed by the data from previous tasks. We also performed
hyperparameter tuning to find the optimal weighting of the proposed
regularization term. This ensures clearer separation between tasks in the
neural network's internal representation, reducing overlap and mitigating
forgetting. Using the standard 5-task Split CIFAR-10 benchmark and a ResNet-18
architecture, we demonstrate ICS's effectiveness in maintaining strong
performance on initial tasks. However, our results also highlight limitations
in long-term knowledge retention, particularly when the number of tasks
increases. This underscores the complexity and trade-offs inherent in continual
learning and points toward avenues for further research.

</details>


### [40] [Value Flows](https://arxiv.org/abs/2510.07650)
*Perry Dong,Chongyi Zheng,Chelsea Finn,Dorsa Sadigh,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 该论文提出Value Flows方法，使用基于流的模型来估计完整的未来回报分布，并通过流导数ODE识别高回报方差的状态，在37个状态基准和25个图像基准任务上实现了平均1.3倍的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 传统的分布强化学习方法通常将回报分布简化为离散的分类分布或有限分位数，无法揭示回报分布的细粒度结构，也难以识别具有高回报不确定性的状态用于决策制定。

Method: 提出新的流匹配目标，生成满足分布贝尔曼方程的概率密度路径；使用基于流的模型估计完整回报分布；通过流导数ODE估计状态回报不确定性；利用不确定性信息优先学习特定转换的更准确回报估计。

Result: 在37个状态基准和25个图像基准任务上的实验表明，Value Flows方法相比先前方法实现了平均1.3倍的成功率提升。

Conclusion: 使用基于流的模型能够更准确地估计完整回报分布，识别高不确定性状态，从而提供更强的学习信号，在分布强化学习中取得了显著性能提升。

Abstract: While most reinforcement learning methods today flatten the distribution of
future returns to a single scalar value, distributional RL methods exploit the
return distribution to provide stronger learning signals and to enable
applications in exploration and safe RL. While the predominant method for
estimating the return distribution is by modeling it as a categorical
distribution over discrete bins or estimating a finite number of quantiles,
such approaches leave unanswered questions about the fine-grained structure of
the return distribution and about how to distinguish states with high return
uncertainty for decision-making. The key idea in this paper is to use modern,
flexible flow-based models to estimate the full future return distributions and
identify those states with high return variance. We do so by formulating a new
flow-matching objective that generates probability density paths satisfying the
distributional Bellman equation. Building upon the learned flow models, we
estimate the return uncertainty of distinct states using a new flow derivative
ODE. We additionally use this uncertainty information to prioritize learning a
more accurate return estimation on certain transitions. We compare our method
(Value Flows) with prior methods in the offline and online-to-online settings.
Experiments on $37$ state-based and $25$ image-based benchmark tasks
demonstrate that Value Flows achieves a $1.3\times$ improvement on average in
success rates. Website: https://pd-perry.github.io/value-flows Code:
https://github.com/chongyi-zheng/value-flows

</details>


### [41] [Incremental Hybrid Ensemble with Graph Attention and Frequency-Domain Features for Stable Long-Term Credit Risk Modeling](https://arxiv.org/abs/2510.07663)
*Jiajing Wang*

Main category: cs.LG

TL;DR: HYDRA-EI是一个混合集成增量学习框架，用于预测长期贷款违约，通过多阶段特征处理和模型组合来应对数据分布随时间变化的问题。


<details>
  <summary>Details</summary>
Motivation: 长期贷款违约预测困难，因为借款人行为经常变化，数据分布会随时间发生偏移，需要能够适应这种动态变化的解决方案。

Method: 构建关系型、交叉和频域特征，使用图注意力、自动交叉特征创建和频域变换，通过周度数据更新和基于性能的权重调整实现增量学习。

Result: HYDRA-EI提高了模型稳定性和泛化能力，无需频繁手动调整或固定重训练。

Conclusion: 该框架适用于长期信用风险任务，能够有效应对数据分布随时间变化带来的挑战。

Abstract: Predicting long-term loan defaults is hard because borrower behavior often
changes and data distributions shift over time. This paper presents HYDRA-EI, a
hybrid ensemble incremental learning framework. It uses several stages of
feature processing and combines multiple models. The framework builds
relational, cross, and frequency-based features. It uses graph attention,
automatic cross-feature creation, and transformations from the frequency
domain. HYDRA-EI updates weekly using new data and adjusts the model weights
with a simple performance-based method. It works without frequent manual
changes or fixed retraining. HYDRA-EI improves model stability and
generalization, which makes it useful for long-term credit risk tasks.

</details>


### [42] [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](https://arxiv.org/abs/2510.07685)
*Yuhan Sun,Zhiwei Huang,Wanqing Cui,Shaopan Xiong,Yazhi Guo,Meiguang Jin,Junfeng Ma*

Main category: cs.LG

TL;DR: LiveThinking是一个两阶段优化框架，通过知识蒸馏和强化学习压缩推理路径，在淘宝直播中实现亚秒级延迟，提升响应准确性和商业表现


<details>
  <summary>Details</summary>
Motivation: AI电商直播中的数字分身需要实时响应来提升用户参与度，但高延迟的大型推理模型不适合此任务

Method: 第一阶段：使用拒绝采样微调将670B教师模型蒸馏为30B MoE模型（3B激活）；第二阶段：使用GRPO强化学习压缩推理路径，通过多目标奖励函数平衡正确性、帮助性和简洁性

Result: 计算成本降低30倍，实现亚秒级延迟；在淘宝直播中响应正确性提升3.3%，帮助性提升21.8%；显著提升商品交易总额

Conclusion: LiveThinking框架有效解决了电商直播中的实时响应问题，在提升用户体验和商业表现方面具有实际应用价值

Abstract: In AI-powered e-commerce livestreaming, digital avatars require real-time
responses to drive engagement, a task for which high-latency Large Reasoning
Models (LRMs) are ill-suited. We introduce LiveThinking, a practical two-stage
optimization framework to bridge this gap. First, we address computational cost
by distilling a 670B teacher LRM into a lightweight 30B Mixture-of-Experts
(MoE) model (3B active) using Rejection Sampling Fine-Tuning (RFT). This
reduces deployment overhead but preserves the teacher's verbose reasoning,
causing latency. To solve this, our second stage employs reinforcement learning
with Group Relative Policy Optimization (GRPO) to compress the model's
reasoning path, guided by a multi-objective reward function balancing
correctness, helpfulness, and brevity. LiveThinking achieves a 30-fold
reduction in computational cost, enabling sub-second latency. In real-world
application on Taobao Live, it improved response correctness by 3.3% and
helpfulness by 21.8%. Tested by hundreds of thousands of viewers, our system
led to a statistically significant increase in Gross Merchandise Volume (GMV),
demonstrating its effectiveness in enhancing user experience and commercial
performance in live, interactive settings.

</details>


### [43] [Computationally-efficient Graph Modeling with Refined Graph Random Features](https://arxiv.org/abs/2510.07716)
*Krzysztof Choromanski,Avinava Dubey,Arijit Sehanobish,Isaac Reid*

Main category: cs.LG

TL;DR: 提出了GRFs++，一种改进的图随机特征方法，通过游走拼接技术和更灵活的游走长度分布策略，解决了传统GRFs在建模远距离节点关系和采样效率方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统GRFs存在两个主要问题：难以建模远距离节点关系，以及依赖采样长随机游走导致效率低下。GRFs++旨在解决这些长期存在的限制。

Method: 采用游走拼接技术，将多个短游走拼接而不破坏无偏性；扩展了游走终止机制，使用更一般的游走长度分布策略；用并行计算短游走和矩阵乘法替代顺序采样长游走。

Result: GRFs++继承了长游走的近似质量但效率更高，在不增加计算成本的情况下提高了图核的近似精度。

Conclusion: GRFs++通过创新的游走拼接和灵活的终止策略，显著提升了图随机特征方法的效率和准确性，为图核计算提供了更优的解决方案。

Abstract: We propose refined GRFs (GRFs++), a new class of Graph Random Features (GRFs)
for efficient and accurate computations involving kernels defined on the nodes
of a graph. GRFs++ resolve some of the long-standing limitations of regular
GRFs, including difficulty modeling relationships between more distant nodes.
They reduce dependence on sampling long graph random walks via a novel
walk-stitching technique, concatenating several shorter walks without breaking
unbiasedness. By applying these techniques, GRFs++ inherit the approximation
quality provided by longer walks but with greater efficiency, trading
sequential, inefficient sampling of a long walk for parallel computation of
short walks and matrix-matrix multiplication. Furthermore, GRFs++ extend the
simplistic GRFs walk termination mechanism (Bernoulli schemes with fixed
halting probabilities) to a broader class of strategies, applying general
distributions on the walks' lengths. This improves the approximation accuracy
of graph kernels, without incurring extra computational cost. We provide
empirical evaluations to showcase all our claims and complement our results
with theoretical analysis.

</details>


### [44] [DEAS: DEtached value learning with Action Sequence for Scalable Offline RL](https://arxiv.org/abs/2510.07730)
*Changyeon Kim,Haeone Lee,Younggyo Seo,Kimin Lee,Yuke Zhu*

Main category: cs.LG

TL;DR: DEAS是一个简单有效的离线强化学习框架，通过利用动作序列进行价值学习，在复杂长视野任务中显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然避免了昂贵的在线交互，但在复杂长视野顺序决策任务中仍存在困难，需要更好的方法来处理长视野规划问题

Method: 提出DEAS框架，使用动作序列进行价值学习，通过半马尔可夫决策过程Q学习减少有效规划视野，并采用分离价值学习来缓解价值高估问题

Result: 在OGBench的复杂长视野任务中持续优于基线方法，并能显著提升大规模视觉-语言-动作模型在RoboCasa Kitchen仿真和真实世界操作任务中的性能

Conclusion: DEAS通过动作序列和分离价值学习有效解决了离线强化学习中的长视野规划挑战，为复杂顺序决策任务提供了实用的解决方案

Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for
training intelligent agents without expensive online interactions. However,
current approaches still struggle with complex, long-horizon sequential
decision making. In this work, we introduce DEtached value learning with Action
Sequence (DEAS), a simple yet effective offline RL framework that leverages
action sequences for value learning. These temporally extended actions provide
richer information than single-step actions and can be interpreted through the
options framework via semi-Markov decision process Q-learning, enabling
reduction of the effective planning horizon by considering longer sequences at
once. However, directly adopting such sequences in actor-critic algorithms
introduces excessive value overestimation, which we address through detached
value learning that steers value estimates toward in-distribution actions that
achieve high return in the offline dataset. We demonstrate that DEAS
consistently outperforms baselines on complex, long-horizon tasks from OGBench
and can be applied to enhance the performance of large-scale
Vision-Language-Action models that predict action sequences, significantly
boosting performance in both RoboCasa Kitchen simulation tasks and real-world
manipulation tasks.

</details>


### [45] [GeoGen: A Two-stage Coarse-to-Fine Framework for Fine-grained Synthetic Location-based Social Network Trajectory Generation](https://arxiv.org/abs/2510.07735)
*Rongchao Xu,Kunlin Cai,Lin Jiang,Dahai Yu,Zhiqing Hong,Yuan Tian,Guang Wang*

Main category: cs.LG

TL;DR: GeoGen是一个两阶段粗到细的框架，用于生成大规模LBSN签到轨迹数据，解决空间离散、时间不规则和复杂时空模式的挑战。


<details>
  <summary>Details</summary>
Motivation: LBSN签到轨迹数据收集成本高且存在隐私问题，而现有方法难以处理其空间离散、时间不规则的特点以及稀疏活动和不确定人类移动性带来的复杂时空模式。

Method: 提出两阶段框架：第一阶段重构空间连续、时间规则的潜在移动序列，使用稀疏感知时空扩散模型学习行为模式；第二阶段使用基于Transformer的Coarse2FineNet，通过动态上下文融合和多任务混合头解码器生成细粒度轨迹。

Result: 在四个真实数据集上的实验表明，GeoGen在保真度和实用性评估上优于最先进模型，在FS-TKY数据集上距离和半径指标分别提升超过69%和55%。

Conclusion: GeoGen能够有效生成高质量的LBSN签到轨迹数据，在保护隐私的同时保持数据特性，为POI推荐、广告和疫情干预等应用提供支持。

Abstract: Location-Based Social Network (LBSN) check-in trajectory data are important
for many practical applications, like POI recommendation, advertising, and
pandemic intervention. However, the high collection costs and ever-increasing
privacy concerns prevent us from accessing large-scale LBSN trajectory data.
The recent advances in synthetic data generation provide us with a new
opportunity to achieve this, which utilizes generative AI to generate synthetic
data that preserves the characteristics of real data while ensuring privacy
protection. However, generating synthetic LBSN check-in trajectories remains
challenging due to their spatially discrete, temporally irregular nature and
the complex spatio-temporal patterns caused by sparse activities and uncertain
human mobility. To address this challenge, we propose GeoGen, a two-stage
coarse-to-fine framework for large-scale LBSN check-in trajectory generation.
In the first stage, we reconstruct spatially continuous, temporally regular
latent movement sequences from the original LBSN check-in trajectories and then
design a Sparsity-aware Spatio-temporal Diffusion model (S$^2$TDiff) with an
efficient denosing network to learn their underlying behavioral patterns. In
the second stage, we design Coarse2FineNet, a Transformer-based Seq2Seq
architecture equipped with a dynamic context fusion mechanism in the encoder
and a multi-task hybrid-head decoder, which generates fine-grained LBSN
trajectories based on coarse-grained latent movement sequences by modeling
semantic relevance and behavioral uncertainty. Extensive experiments on four
real-world datasets show that GeoGen excels state-of-the-art models for both
fidelity and utility evaluation, e.g., it increases over 69% and 55% in
distance and radius metrics on the FS-TKY dataset.

</details>


### [46] [MeSH: Memory-as-State-Highways for Recursive Transformers](https://arxiv.org/abs/2510.07739)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,Jiaang Li,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 提出MeSH方案解决递归transformer的性能瓶颈，通过外部内存缓冲区和轻量级路由器动态多样化迭代计算，在1.4B规模上超越非递归模型，准确率提升1.06%且参数减少33%。


<details>
  <summary>Details</summary>
Motivation: 递归transformer重用参数并多次迭代隐藏状态，但在相同计算量下，参数较少的递归模型往往落后于非递归模型。研究发现性能差距源于两个瓶颈：无差别计算和信息过载。

Method: 引入Memory-as-State-Highways (MeSH)方案，将状态管理外部化到显式内存缓冲区，使用轻量级路由器动态多样化各迭代的计算。

Result: 在Pythia套件(160M-1.4B)上，MeSH增强的递归transformer持续优于递归基线，在1.4B规模上超越其更大的非递归对应模型，平均下游准确率提升+1.06%，非嵌入参数减少33%。

Conclusion: MeSH是构建更强递归模型的可扩展且有原则的架构，通过诱导迭代间的功能专业化成功解决了递归模型的病理问题。

Abstract: Recursive transformers reuse parameters and iterate over hidden states
multiple times, decoupling compute depth from parameter depth. However, under
matched compute, recursive models with fewer parameters often lag behind
non-recursive counterparts. By probing hidden states, we trace this performance
gap to two primary bottlenecks: undifferentiated computation, where the core is
forced to adopt a similar computational pattern at every iteration, and
information overload, where long-lived and transient information must coexist
in a single hidden state. To address the issues, we introduce a
Memory-as-State-Highways (MeSH) scheme, which externalizes state management
into an explicit memory buffer and employs lightweight routers to dynamically
diversify computation across iterations. Probing visualizations confirm that
MeSH successfully resolves the pathologies by inducing functional
specialization across iterations. On the Pythia suite (160M-1.4B),
MeSH-enhanced recursive transformers consistently improve over recursive
baselines and outperforms its larger non-recursive counterpart at the 1.4B
scale, improving average downstream accuracy by +1.06% with 33% fewer
non-embedding parameters. Our analysis establishes MeSH as a scalable and
principled architecture for building stronger recursive models.

</details>


### [47] [t-SNE Exaggerates Clusters, Provably](https://arxiv.org/abs/2510.07746)
*Noah Bergam,Szymon Snoeck,Nakul Verma*

Main category: cs.LG

TL;DR: t-SNE可视化结果不能可靠反映输入数据的聚类强度和异常点极端程度


<details>
  <summary>Details</summary>
Motivation: 揭示t-SNE可视化结果与输入数据结构之间的不一致性，挑战人们对t-SNE能准确反映数据结构的普遍信念

Method: 通过理论证明和实践案例展示t-SNE的两个主要失效模式

Result: 证明(1)输入聚类强度无法从t-SNE输出可靠推断，(2)异常点极端程度也无法可靠推断

Conclusion: t-SNE可视化结果不能可靠反映输入数据的聚类结构和异常点特征，使用时需要谨慎解读

Abstract: Central to the widespread use of t-distributed stochastic neighbor embedding
(t-SNE) is the conviction that it produces visualizations whose structure
roughly matches that of the input. To the contrary, we prove that (1) the
strength of the input clustering, and (2) the extremity of outlier points,
cannot be reliably inferred from the t-SNE output. We demonstrate the
prevalence of these failure modes in practice as well.

</details>


### [48] [FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling](https://arxiv.org/abs/2510.07755)
*Zhengyu Wu,Yinlin Zhu,Xunkai Li,Ziang Qiu,Rong-Hua Li,Guoren Wang,Chenghu Zhou*

Main category: cs.LG

TL;DR: FedBook是一个联邦图基础模型代码本，通过两阶段聚合策略解决多领域图数据隐私约束下的模型训练问题，在8个基准测试中优于21个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型需要集中访问多领域图数据，但在隐私和机构约束下不可行。联邦图基础模型需要构建既能实现域内语义一致性又能保持域间多样性的全局代码本。

Method: 采用两阶段过程：1) 域内协作 - 通过参考更可靠的跨客户端高频令牌来优化低频令牌；2) 域间集成 - 根据客户端代码本语义独特性加权聚合，保持跨域多样性。

Result: 在多个领域和任务的8个基准测试中，FedBook持续优于21个基线方法，包括孤立监督学习、FL/FGL、集中式GFM的联邦适应和FedGFM技术。

Conclusion: FedBook通过系统化的客户端本地代码本聚合策略，成功构建了鲁棒的联邦图基础模型代码本，在保护隐私的同时实现了优异的跨域性能。

Abstract: Foundation models have shown remarkable cross-domain generalization in
language and vision, inspiring the development of graph foundation models
(GFMs). However, existing GFMs typically assume centralized access to
multi-domain graphs, which is often infeasible due to privacy and institutional
constraints. Federated Graph Foundation Models (FedGFMs) address this
limitation, but their effectiveness fundamentally hinges on constructing a
robust global codebook that achieves intra-domain coherence by consolidating
mutually reinforcing semantics within each domain, while also maintaining
inter-domain diversity by retaining heterogeneous knowledge across domains. To
this end, we propose FedBook, a unified federated graph foundation codebook
that systematically aggregates clients' local codebooks during server-side
federated pre-training. FedBook follows a two-phase process: (1) Intra-domain
Collaboration, where low-frequency tokens are refined by referencing more
semantically reliable high-frequency tokens across clients to enhance
domain-specific coherence; and (2) Inter-domain Integration, where client
contributions are weighted by the semantic distinctiveness of their codebooks
during the aggregation of the global GFM, thereby preserving cross-domain
diversity. Extensive experiments on 8 benchmarks across multiple domains and
tasks demonstrate that FedBook consistently outperforms 21 baselines, including
isolated supervised learning, FL/FGL, federated adaptations of centralized
GFMs, and FedGFM techniques.

</details>


### [49] [Rényi Sharpness: A Novel Sharpness that Strongly Correlates with Generalization](https://arxiv.org/abs/2510.07758)
*Qiaozhe Zhang,Jun Sun,Ruijie Zhang,Yingzhuang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的锐度度量方法——Rényi锐度，通过Rényi熵来表征损失Hessian矩阵特征值的分布情况，并证明了其与泛化能力的强相关性。同时提出了Rényi锐度感知最小化(RSAM)训练方法，在多个实验中优于现有的锐度感知方法。


<details>
  <summary>Details</summary>
Motivation: 现有的锐度度量与泛化能力之间的相关性通常不强，有时甚至很弱。为了弥合直觉与现实之间的差距，需要一种更有效的锐度度量方法。

Method: 1) 提出Rényi锐度，定义为损失Hessian矩阵的负Rényi熵；2) 利用Rényi熵简洁地表征损失Hessian特征值的分布程度；3) 提出Rényi锐度感知最小化(RSAM)作为正则化训练方法。

Result: 实验验证了Rényi锐度与泛化能力之间存在强相关性（特别是Kendall秩相关）。RSAM方法相比经典的SAM方法，测试准确率提升可达近2.5%。

Conclusion: Rényi锐度是一种有效的泛化能力度量指标，基于此提出的RSAM训练方法显著优于现有的锐度感知最小化方法。

Abstract: Sharpness (of the loss minima) is a common measure to investigate the
generalization of neural networks. Intuitively speaking, the flatter the
landscape near the minima is, the better generalization might be.
Unfortunately, the correlation between many existing sharpness measures and the
generalization is usually not strong, sometimes even weak. To close the gap
between the intuition and the reality, we propose a novel sharpness measure,
i.e., \textit{R\'enyi sharpness}, which is defined as the negative R\'enyi
entropy (a generalization of the classical Shannon entropy) of the loss
Hessian. The main ideas are as follows: 1) we realize that \textit{uniform}
(identical) eigenvalues of the loss Hessian is most desirable (while keeping
the sum constant) to achieve good generalization; 2) we employ the
\textit{R\'enyi entropy} to concisely characterize the extent of the spread of
the eigenvalues of loss Hessian. Normally, the larger the spread, the smaller
the (R\'enyi) entropy. To rigorously establish the relationship between
generalization and (R\'enyi) sharpness, we provide several generalization
bounds in terms of R\'enyi sharpness, by taking advantage of the
reparametrization invariance property of R\'enyi sharpness, as well as the
trick of translating the data discrepancy to the weight perturbation.
Furthermore, extensive experiments are conducted to verify the strong
correlation (in specific, Kendall rank correlation) between the R\'enyi
sharpness and generalization. Moreover, we propose to use a variant of R\'enyi
Sharpness as regularizer during training, i.e., R\'enyi Sharpness Aware
Minimization (RSAM), which turns out to outperform all existing sharpness-aware
minimization methods. It is worthy noting that the test accuracy gain of our
proposed RSAM method could be as high as nearly 2.5\%, compared against the
classical SAM method.

</details>


### [50] [A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization](https://arxiv.org/abs/2510.07760)
*Yiqin Lv,Zhiyu Mou,Miao Xu,Jinghao Chen,Qi Wang,Yixiu Mao,Yun Qu,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了VAMO方法，通过验证集梯度对齐来优化多任务学习，解决在线广告中独立优化竞价任务导致的效率低下问题，并在理论和实验上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在线广告中，广告主的异构需求产生了大量定制化竞价任务，这些任务通常独立优化，导致计算量大且数据效率低。多任务学习虽然提供了联合训练框架，但现有方法基于训练动态，在波动的竞价环境中泛化能力差。

Method: 提出了验证对齐多任务优化(VAMO)，根据每个任务的训练梯度与验证集梯度的对齐度自适应分配任务权重，引导更新朝验证集改进方向。还配备了周期性感知时间模块，并与生成式自动竞价骨干网络结合，增强季节性结构的跨任务迁移。

Result: 在模拟和真实大规模广告系统上的广泛实验表明，该方法相比典型基线有显著改进，验证了方法的有效性。

Conclusion: VAMO方法通过验证集梯度对齐机制，有效解决了多任务竞价优化中的泛化问题，理论分析和实验验证都证明了其优越性能。

Abstract: In online advertising, heterogeneous advertiser requirements give rise to
numerous customized bidding tasks that are typically optimized independently,
resulting in extensive computation and limited data efficiency. Multi-task
learning offers a principled framework to train these tasks jointly through
shared representations. However, existing multi-task optimization strategies
are primarily guided by training dynamics and often generalize poorly in
volatile bidding environments. To this end, we present Validation-Aligned
Multi-task Optimization (VAMO), which adaptively assigns task weights based on
the alignment between per-task training gradients and a held-out validation
gradient, thereby steering updates toward validation improvement and better
matching deployment objectives. We further equip the framework with a
periodicity-aware temporal module and couple it with an advanced generative
auto-bidding backbone to enhance cross-task transfer of seasonal structure and
strengthen bidding performance. Meanwhile, we provide theoretical insights into
the proposed method, e.g., convergence guarantee and alignment analysis.
Extensive experiments on both simulated and large-scale real-world advertising
systems consistently demonstrate significant improvements over typical
baselines, illuminating the effectiveness of the proposed approach.

</details>


### [51] [FedLAM: Low-latency Wireless Federated Learning via Layer-wise Adaptive Modulation](https://arxiv.org/abs/2510.07766)
*Linping Qu,Shenghui Song,Chi-Ying Tsui*

Main category: cs.LG

TL;DR: 提出了一种分层自适应调制方案来减少无线联邦学习中的通信延迟，通过为不同DNN层分配最优调制级别，相比现有方案可节省高达73.9%的通信延迟。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中，客户端需要通过带宽受限的通道传输高维DNN参数，这导致了通信延迟问题。现有工作为所有DNN层分配相同的调制级别，没有考虑各层的重要性差异。

Method: 提出分层自适应调制方案，考虑DNN各层的重要性差异，自动为不同层决定最优调制级别，提供更多自由度来节省延迟。

Result: 实验结果显示，相比现有方案，所提方案可节省高达73.9%的通信延迟。

Conclusion: 分层自适应调制方案能有效解决无线联邦学习中的通信延迟问题，通过考虑DNN层的重要性差异来优化调制级别分配。

Abstract: In wireless federated learning (FL), the clients need to transmit the
high-dimensional deep neural network (DNN) parameters through bandwidth-limited
channels, which causes the communication latency issue. In this paper, we
propose a layer-wise adaptive modulation scheme to save the communication
latency. Unlike existing works which assign the same modulation level for all
DNN layers, we consider the layers' importance which provides more freedom to
save the latency. The proposed scheme can automatically decide the optimal
modulation levels for different DNN layers. Experimental results show that the
proposed scheme can save up to 73.9% of communication latency compared with the
existing schemes.

</details>


### [52] [Weak Form Learning for Mean-Field Partial Differential Equations: an Application to Insect Movement](https://arxiv.org/abs/2510.07786)
*Seth Minor,Bret D. Elderd,Benjamin Van Allen,David M. Bortz,Vanja Dukic*

Main category: cs.LG

TL;DR: 本文扩展了弱形式方程学习方法，结合核密度估计，从稀疏的实验数据中学习鳞翅目幼虫种群运动的有效模型。


<details>
  <summary>Details</summary>
Motivation: 理解作物和林业害虫的扩散动态可以更好地预测爆发强度和位置，从而实现更好的害虫管理。

Method: 使用弱形式稀疏识别非线性动力学（WSINDy）算法，结合核密度估计，从稀疏的位置测量数据中学习Fokker-Planck方程。

Result: 该方法在模拟农业条件下，针对不同植物资源和感染状态的秋粘虫位置测量数据上证明了其有效性。

Conclusion: 弱形式方程学习技术能够从高度稀疏的实验数据中学习昆虫种群运动的有效模型，有助于害虫管理预测。

Abstract: Insect species subject to infection, predation, and anisotropic environmental
conditions may exhibit preferential movement patterns. Given the innate
stochasticity of exogenous factors driving these patterns over short
timescales, individual insect trajectories typically obey overdamped stochastic
dynamics. In practice, data-driven modeling approaches designed to learn the
underlying Fokker-Planck equations from observed insect distributions serve as
ideal tools for understanding and predicting such behavior. Understanding
dispersal dynamics of crop and silvicultural pests can lead to a better
forecasting of outbreak intensity and location, which can result in better pest
management. In this work, we extend weak-form equation learning techniques,
coupled with kernel density estimation, to learn effective models for
lepidopteran larval population movement from highly sparse experimental data.
Galerkin methods such as the Weak form Sparse Identification of Nonlinear
Dynamics (WSINDy) algorithm have recently proven useful for learning governing
equations in several scientific contexts. We demonstrate the utility of the
method on a sparse dataset of position measurements of fall armyworms
(Spodoptera frugiperda) obtained in simulated agricultural conditions with
varied plant resources and infection status.

</details>


### [53] [HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs](https://arxiv.org/abs/2510.07796)
*Majid Jaberi-Douraki,Hossein Sholehrasa,Xuan Xu,Remya Ampadi Ramachandran*

Main category: cs.LG

TL;DR: 提出了HySim-LLM框架，通过嵌入加权微调和流形感知去噪来增强LLMs在生物医学结构化数据中的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 科学文献中药物动力学信息的提取和标准化存在挑战，限制了药物开发中数据驱动模型的可靠性。LLMs在处理结构化生物医学数据时受到异质性、噪声和领域偏移的限制。

Method: 提出HySim-LLM统一数学计算框架，整合嵌入加权微调和流形感知去噪。建立了两个理论结果：相似性加权泛化边界和基于流形的去噪保证。

Result: 为结构化生物医学环境中微调LLMs提供了理论基础，量化了嵌入差异下的适应性能，并限制了噪声样本的损失贡献。

Conclusion: 该框架为生物医学和数据密集型科学领域的可靠且可解释的LLM适应提供了数学基础路径。

Abstract: The extraction and standardization of pharmacokinetic (PK) information from
scientific literature remain significant challenges in computational
pharmacology, which limits the reliability of data-driven models in drug
development. Large language models (LLMs) have achieved remarkable progress in
text understanding and reasoning, yet their adaptation to structured biomedical
data, such as PK tables, remains constrained by heterogeneity, noise, and
domain shift. To address these limitations, we propose HySim-LLM, a unified
mathematical and computational framework that integrates embedding-weighted
fine-tuning and manifold-aware denoising to enhance the robustness and
interpretability of LLMs. We establish two theoretical results: (1) a
similarity-weighted generalization bound that quantifies adaptation performance
under embedding divergence, and (2) a manifold-based denoising guarantee that
bounds loss contributions from noisy or off-manifold samples. These theorems
provide a principled foundation for fine-tuning LLMs in structured biomedical
settings. The framework offers a mathematically grounded pathway toward
reliable and interpretable LLM adaptation for biomedical and data-intensive
scientific domains.

</details>


### [54] [SIMU: Selective Influence Machine Unlearning](https://arxiv.org/abs/2510.07822)
*Anu Agarwal,Mihir Pamnani,Dilek Hakkani-Tur*

Main category: cs.LG

TL;DR: 提出了选择性影响机器遗忘(SIMU)框架，通过选择性更新关键神经元来增强二阶优化器遗忘方法，在保持遗忘效果的同时更好地保留模型原有知识。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在让大语言模型遗忘敏感信息时，往往会损害模型的原始能力和知识保留，需要一种既能有效遗忘又能保持模型效用的方法。

Method: 两步框架：首先识别编码遗忘集的关键神经元，然后仅对这些目标神经元进行选择性更新，约束更新范围。

Result: SIMU实现了可比的遗忘效果，同时在保留模型原有知识方面显著优于现有方法。

Conclusion: 选择性更新关键神经元的策略能够有效平衡遗忘效果和知识保留，为机器遗忘提供了更优的解决方案。

Abstract: The undesired memorization of sensitive information by Large Language Models
(LLMs) has emphasized the need for safety mechanisms that can regulate model
behavior. This has led to the development of machine unlearning techniques that
enable models to precisely forget sensitive and unwanted information. For
machine unlearning, first-order and second-order optimizer-based methods have
shown significant progress in enabling LLMs to forget targeted information.
However, in doing so, these approaches often compromise the model's original
capabilities, resulting in unlearned models that struggle to retain their prior
knowledge and overall utility. To address this, we propose Selective Influence
Machine Unlearning (SIMU), a two-step framework that enhances second-order
optimizer-based unlearning by selectively updating only the critical neurons
responsible for encoding the forget-set. By constraining updates to these
targeted neurons, SIMU achieves comparable unlearning efficacy while
substantially outperforming current methods in retaining the model's original
knowledge.

</details>


### [55] [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
*Weisen Jiang,Sinno Jialin Pan*

Main category: cs.LG

TL;DR: MetaDefense是一个防御LLM微调越狱攻击的新框架，通过两阶段防御机制在生成前和生成中检测有害内容，显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制无法泛化到未见攻击模板伪装的有害查询，尽管LLM能够在嵌入空间中区分这些查询。

Method: 提出两阶段防御：预生成防御在响应生成前检测有害查询，中生成防御在生成过程中监控部分响应以防止输出更多有害内容。通过专门提示训练LLM预测查询和部分响应的危害性。

Result: 在多个LLM架构上的实验表明，MetaDefense显著优于现有防御机制，对可见和未见攻击模板的有害查询都能提供鲁棒防御，同时在良性任务上保持竞争力。

Conclusion: MetaDefense框架通过两阶段防御机制有效防御微调越狱攻击，具有良好的泛化能力和实用性。

Abstract: This paper introduces MetaDefense, a novel framework for defending against
finetuning-based jailbreak attacks in large language models (LLMs). We observe
that existing defense mechanisms fail to generalize to harmful queries
disguised by unseen attack templates, despite LLMs being capable of
distinguishing disguised harmful queries in the embedding space. Based on these
insights, we propose a two-stage defense approach: (i) pre-generation defense
that detects harmful queries before response generation begins, and (ii)
mid-generation defense that monitors partial responses during generation to
prevent outputting more harmful content. Our MetaDefense trains the LLM to
predict the harmfulness of both queries and partial responses using specialized
prompts, enabling early termination of potentially harmful interactions.
Extensive experiments across multiple LLM architectures (LLaMA-2-7B,
Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense
significantly outperforms existing defense mechanisms, achieving robust defense
against harmful queries with seen and unseen attack templates while maintaining
competitive performance on benign tasks. Code is available at
https://github.com/ws-jiang/MetaDefense.

</details>


### [56] [Self-Improving LLM Agents at Test-Time](https://arxiv.org/abs/2510.07841)
*Emre Can Acikgoz,Cheng Qian,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: 提出了一种测试时自改进方法，通过识别模型不确定样本、生成类似示例并进行测试时微调，显著提升语言模型性能，使用样本量减少68倍。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型微调需要大量训练数据，效率低且成本高，且无法保证模型能处理复杂场景或更好泛化。现有技术很少评估训练样本是否提供新信息，导致不必要的成本。

Method: 提出三步算法：1）识别模型不确定样本（自我意识）；2）从不确定样本生成类似示例（自数据增强）；3）在测试时使用新生成样本进行微调（自改进）。研究两种变体：TT-SI（同模型自生成）和TT-D（强模型生成）。

Result: 在多个智能体基准测试中，TT-SI平均绝对准确率提升5.48%，超越其他标准学习方法，同时训练样本使用量减少68倍。

Conclusion: 测试时自改进算法展示了构建更强大智能体实现自我演进的新范式潜力。

Abstract: One paradigm of language model (LM) fine-tuning relies on creating large
training datasets, under the assumption that high quantity and diversity will
enable models to generalize to novel tasks after post-training. In practice,
gathering large sets of data is inefficient, and training on them is
prohibitively expensive; worse, there is no guarantee that the resulting model
will handle complex scenarios or generalize better. Moreover, existing
techniques rarely assess whether a training sample provides novel information
or is redundant with the knowledge already acquired by the model, resulting in
unnecessary costs. In this work, we explore a new test-time self-improvement
method to create more effective and generalizable agentic LMs on-the-fly. The
proposed algorithm can be summarized in three steps: (i) first it identifies
the samples that model struggles with (self-awareness), (ii) then generates
similar examples from detected uncertain samples (self-data augmentation), and
(iii) uses these newly generated samples at test-time fine-tuning
(self-improvement). We study two variants of this approach: Test-Time
Self-Improvement (TT-SI), where the same model generates additional training
examples from its own uncertain cases and then learns from them, and contrast
this approach with Test-Time Distillation (TT-D), where a stronger model
generates similar examples for uncertain cases, enabling student to adapt using
distilled supervision. Empirical evaluations across different agent benchmarks
demonstrate that TT-SI improves the performance with +5.48% absolute accuracy
gain on average across all benchmarks and surpasses other standard learning
methods, yet using 68x less training samples. Our findings highlight the
promise of TT-SI, demonstrating the potential of self-improvement algorithms at
test-time as a new paradigm for building more capable agents toward
self-evolution.

</details>


### [57] [Meta-Learning Based Few-Shot Graph-Level Anomaly Detection](https://arxiv.org/abs/2510.07847)
*Liting Li,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的图级异常检测框架MA-GAD，通过图压缩模块减少噪声干扰，利用元学习从相似网络中提取元异常信息，在少样本条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络方法严重依赖大量标注数据，而现实场景中标注数据稀缺；少样本异常检测方法易受噪声干扰，导致嵌入质量差和模型鲁棒性降低。

Method: 1. 图压缩模块减小图规模，减轻噪声干扰同时保留关键节点信息；2. 元学习从相似网络提取元异常信息，学习可快速适应新任务的初始化模型；3. 使用偏置网络增强异常节点与正常节点的区分度。

Result: 在四个真实世界生化数据集上的实验表明，MA-GAD在少样本条件下的图级异常检测性能优于现有最先进方法，在图异常和子图异常检测任务上均验证了有效性。

Conclusion: MA-GAD框架通过结合图压缩和元学习，有效解决了少样本图级异常检测中的噪声干扰和标注数据稀缺问题，在真实数据集上表现出优越性能。

Abstract: Graph-level anomaly detection aims to identify anomalous graphs or subgraphs
within graph datasets, playing a vital role in various fields such as fraud
detection, review classification, and biochemistry. While Graph Neural Networks
(GNNs) have made significant progress in this domain, existing methods rely
heavily on large amounts of labeled data, which is often unavailable in
real-world scenarios. Additionally, few-shot anomaly detection methods based on
GNNs are prone to noise interference, resulting in poor embedding quality and
reduced model robustness. To address these challenges, we propose a novel
meta-learning-based graph-level anomaly detection framework (MA-GAD),
incorporating a graph compression module that reduces the graph size,
mitigating noise interference while retaining essential node information. We
also leverage meta-learning to extract meta-anomaly information from similar
networks, enabling the learning of an initialization model that can rapidly
adapt to new tasks with limited samples. This improves the anomaly detection
performance on target graphs, and a bias network is used to enhance the
distinction between anomalous and normal nodes. Our experimental results, based
on four real-world biochemical datasets, demonstrate that MA-GAD outperforms
existing state-of-the-art methods in graph-level anomaly detection under
few-shot conditions. Experiments on both graph anomaly and subgraph anomaly
detection tasks validate the framework's effectiveness on real-world datasets.

</details>


### [58] [Signal-to-Noise Ratio in Scanning Electron Microscopy: A Comprehensive Review](https://arxiv.org/abs/2510.07886)
*K. S. Sim,I. Bukhori,D. C. Y. Ong,K. B. Gan*

Main category: cs.LG

TL;DR: 这篇综述论文探讨了扫描电子显微镜(SEM)中的信噪比(SNR)问题，涵盖了SEM工作原理、噪声来源、SNR测量方法以及硬件和软件层面的SNR优化技术。


<details>
  <summary>Details</summary>
Motivation: SEM在纳米技术、材料科学和生物成像中至关重要，但噪声会降低图像质量，影响图像的可解释性。因此需要全面理解SNR优化方法。

Method: 采用综述方法，分析SEM成像过程的多个方面，包括传统和新兴技术，重点讨论其应用、优势和局限性。

Result: 论文提供了SEM中SNR优化的全面理解，涵盖了从基础原理到先进技术的各个方面。

Conclusion: 该综述旨在为研究人员和实践者提供SEM中SNR优化的综合知识，并鼓励该领域的进一步研究。

Abstract: Scanning Electron Microscopy (SEM) is critical in nanotechnology, materials
science, and biological imaging due to its high spatial resolution and depth of
focus. Signal-to-noise ratio (SNR) is an essential parameter in SEM because it
directly impacts the quality and interpretability of the images. SEM is widely
used in various scientific disciplines, but its utility can be compromised by
noise, which degrades image clarity. This review explores multiple aspects of
the SEM imaging process, from the principal operation of SEM, sources of noise
in SEM, methods for SNR measurement and estimations, to various aspects that
affect the SNR measurement and approaches to enhance SNR, both from a hardware
and software standpoint. We review traditional and emerging techniques,
focusing on their applications, advantages, and limitations. The paper aims to
provide a comprehensive understanding of SNR optimization in SEM for
researchers and practitioners and to encourage further research in the field.

</details>


### [59] [Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression Filtering Method for SEM Images](https://arxiv.org/abs/2510.07895)
*D. Chee Yong Ong,I. Bukhori,K. S. Sim,K. Beng Gan*

Main category: cs.LG

TL;DR: 提出了一种基于噪声方差估计的SEM图像去噪方法，结合LSR SNR估计和优化GPR噪声方差估计，构建AO-GPRLLSR滤波管道，通过NV引导Wiener滤波器提升图像质量。


<details>
  <summary>Details</summary>
Motivation: SEM图像常受噪声污染影响图像质量和后续分析，需要准确的SNR和噪声方差估计来指导有效的去噪处理。

Method: 比较五种SNR估计方法，选择最优的LSR方法；测试SVM和GPR噪声方差估计，确定优化GPR模型；结合两者构建AO-GPRLLSR滤波管道，使用NV引导Wiener滤波器去噪。

Result: LSR方法在SNR估计中表现最佳，优化GPR模型在噪声方差估计中精度最高；提出的AO-GPRLLSR方法在滤波后获得更低的均方误差。

Conclusion: AO-GPRLLSR方法能有效估计SEM图像的SNR和噪声方差，通过NV引导Wiener滤波器显著提升图像质量，为SEM图像处理提供了鲁棒准确的解决方案。

Abstract: Scanning Electron Microscopy (SEM) images often suffer from noise
contamination, which degrades image quality and affects further analysis. This
research presents a complete approach to estimate their Signal-to-Noise Ratio
(SNR) and noise variance (NV), and enhance image quality using NV-guided Wiener
filter. The main idea of this study is to use a good SNR estimation technique
and infuse a machine learning model to estimate NV of the SEM image, which then
guides the wiener filter to remove the noise, providing a more robust and
accurate SEM image filtering pipeline. First, we investigate five different SNR
estimation techniques, namely Nearest Neighbourhood (NN) method, First-Order
Linear Interpolation (FOL) method, Nearest Neighbourhood with First-Order
Linear Interpolation (NN+FOL) method, Non-Linear Least Squares Regression
(NLLSR) method, and Linear Least Squares Regression (LSR) method. It is shown
that LSR method to perform better than the rest. Then, Support Vector Machines
(SVM) and Gaussian Process Regression (GPR) are tested by pairing it with LSR.
In this test, the Optimizable GPR model shows the highest accuracy and it
stands as the most effective solution for NV estimation. Combining these
results lead to the proposed Adaptive Optimizable Gaussian Process Regression
Linear Least Squares Regression (AO-GPRLLSR) Filtering pipeline. The AO-GPRLLSR
method generated an estimated noise variance which served as input to NV-guided
Wiener filter for improving the quality of SEM images. The proposed method is
shown to achieve notable success in estimating SNR and NV of SEM images and
leads to lower Mean Squared Error (MSE) after the filtering process.

</details>


### [60] [MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation](https://arxiv.org/abs/2510.07910)
*Chongmyung Kwon,Yujin Kim,Seoeun Park,Yunji Lee,Charmgil Hong*

Main category: cs.LG

TL;DR: 提出MMM框架，通过整合三维量子化学信息（电子定位函数ELF）来改进药物推荐中的药物相互作用预测，相比传统GNN方法在多个指标上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的方法使用简化的离散形式表示药物结构，无法充分捕捉分子结合亲和力和反应性，药物相互作用风险仍是临床决策支持系统的重要挑战。

Method: MMM框架结合三维电子密度图（使用ELF生成）和二分图编码器，整合全局电子特性和局部亚结构相互作用，学习药物分子的互补特征。

Result: 在MIMIC-III数据集上与多个基线模型比较，相比GNN-based SafeDrug模型，F1分数、Jaccard和DDI率均获得统计显著提升（p值分别为0.0387、0.0112、0.0386）。

Conclusion: 基于ELF的三维表示有潜力提高预测准确性，支持临床实践中更安全的组合药物处方。

Abstract: Drug recommendation is an essential task in machine learning-based clinical
decision support systems. However, the risk of drug-drug interactions (DDI)
between co-prescribed medications remains a significant challenge. Previous
studies have used graph neural networks (GNNs) to represent drug structures.
Regardless, their simplified discrete forms cannot fully capture the molecular
binding affinity and reactivity. Therefore, we propose Multimodal DDI
Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a
novel framework that integrates three-dimensional (3D) quantum-chemical
information into drug representation learning. It generates 3D electron density
maps using the ELF. To capture both therapeutic relevance and interaction
risks, MMM combines ELF-derived features that encode global electronic
properties with a bipartite graph encoder that models local substructure
interactions. This design enables learning complementary characteristics of
drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442
substructures), comparing it with several baseline models. In particular, a
comparison with the GNN-based SafeDrug model demonstrates statistically
significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),
and the DDI rate (p = 0.0386). These results demonstrate the potential of
ELF-based 3D representations to enhance prediction accuracy and support safer
combinatorial drug prescribing in clinical practice.

</details>


### [61] [GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploratio](https://arxiv.org/abs/2510.07919)
*Tingfeng Hong,Pingye Ren,Xinlong Xiao,Chao Wang,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.LG

TL;DR: 提出了一个个性化多目标排序系统，包含特征中心、多任务学习模型和GRADE多任务融合框架，通过个性化权重计算最终得分并生成混合排序结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决多目标排序中不同用户对各项指标的偏好差异，需要开发能够个性化融合多个任务预测结果的排序系统。

Method: 系统架构包括特征中心和预排序模型、多任务学习模型预测用户反馈信号、GRADE多任务融合模块学习个性化权重，最终通过混合排序模型生成结果。

Result: 构建了一个完整的个性化多目标排序系统框架，能够根据用户个性化偏好融合多个任务预测结果。

Conclusion: GRADE框架通过个性化权重学习有效解决了多目标排序中的个性化融合问题，提升了排序系统的性能。

Abstract: Overall architecture of the personalized multi-objective ranking system. It
comprises: (1) a Feature Center and Prerank Model for initial feature
processing and candidate generation; (2) a Multi-Task Learning (MTL) model
predicting various user feedback signals; (3) a Multi-Task Fusion (MTF) module
(our proposed GRADE framework) that learns personalized weights ($w_1, \dots,
w_n$); these weights are then applied to calculate final scores and sorted to
generate a blended ranking by the Blended Ranking Model, which ultimately
delivers results to users.

</details>


### [62] [Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers](https://arxiv.org/abs/2510.07924)
*Yongqi Ding,Lin Zuo,Mengmeng Jing,Kunshan Yang,Pei He,Tonglan Xie*

Main category: cs.LG

TL;DR: 提出了一种基于SNN时间特性的自蒸馏方法，将SNN解构为多个子模型，通过强子模型指导弱子模型或弱子模型反向蒸馏强子模型来提升性能。


<details>
  <summary>Details</summary>
Motivation: SNN作为低功耗的ANN替代方案存在性能差距，现有知识蒸馏方法依赖大教师模型或引入额外训练开销。

Method: 将SNN的每个时间步实例视为子模型，评估其输出置信度，提出Strong2Weak和Weak2Strong两种自蒸馏方案，支持集成、同步和级联蒸馏实现。

Result: 实验表明该方法有效提升了SNN的判别能力和整体性能，同时增强了对抗鲁棒性。

Conclusion: 巧妙利用SNN的时间特性，为高效训练高性能SNN提供了新思路。

Abstract: Brain-inspired spiking neural networks (SNNs) promise to be a low-power
alternative to computationally intensive artificial neural networks (ANNs),
although performance gaps persist. Recent studies have improved the performance
of SNNs through knowledge distillation, but rely on large teacher models or
introduce additional training overhead. In this paper, we show that SNNs can be
naturally deconstructed into multiple submodels for efficient
self-distillation. We treat each timestep instance of the SNN as a submodel and
evaluate its output confidence, thus efficiently identifying the strong and the
weak. Based on this strong and weak relationship, we propose two efficient
self-distillation schemes: (1) \textbf{Strong2Weak}: During training, the
stronger "teacher" guides the weaker "student", effectively improving overall
performance. (2) \textbf{Weak2Strong}: The weak serve as the "teacher",
distilling the strong in reverse with underlying dark knowledge, again yielding
significant performance gains. For both distillation schemes, we offer flexible
implementations such as ensemble, simultaneous, and cascade distillation.
Experiments show that our method effectively improves the discriminability and
overall performance of the SNN, while its adversarial robustness is also
enhanced, benefiting from the stability brought by self-distillation. This
ingeniously exploits the temporal properties of SNNs and provides insight into
how to efficiently train high-performance SNNs.

</details>


### [63] [Some theoretical improvements on the tightness of PAC-Bayes risk certificates for neural networks](https://arxiv.org/abs/2510.07935)
*Diego García-Pérez,Emilio Parrado-Hernández,John Shawe-Taylor*

Main category: cs.LG

TL;DR: 本文提出了四项理论贡献，改进了基于PAC-Bayes边界的神经网络风险证书的可用性，包括KL散度边界、基于隐式微分的高效方法、非可微目标优化方法，并在MNIST和CIFAR-10上实现了首个非平凡泛化边界。


<details>
  <summary>Details</summary>
Motivation: 改进神经网络风险证书的实用性，特别是针对PAC-Bayes边界在神经网络中的应用，解决现有方法在紧密度和优化效率方面的限制。

Method: 1) 推导伯努利分布KL散度的两个边界；2) 基于隐式微分的优化方法；3) 非可微目标（如0-1损失）的边界优化方法；4) 在MNIST和CIFAR-10数据集上的实证评估。

Result: 实现了最紧的显式分类器真实风险边界，提出了首个在CIFAR-10上对神经网络的非平凡泛化边界，证明了方法的有效性。

Conclusion: 提出的理论贡献显著提升了PAC-Bayes风险证书在神经网络中的实用性和紧密度，为深度学习模型的可信性验证提供了新的工具。

Abstract: This paper presents four theoretical contributions that improve the usability
of risk certificates for neural networks based on PAC-Bayes bounds. First, two
bounds on the KL divergence between Bernoulli distributions enable the
derivation of the tightest explicit bounds on the true risk of classifiers
across different ranges of empirical risk. The paper next focuses on the
formalization of an efficient methodology based on implicit differentiation
that enables the introduction of the optimization of PAC-Bayesian risk
certificates inside the loss/objective function used to fit the network/model.
The last contribution is a method to optimize bounds on non-differentiable
objectives such as the 0-1 loss. These theoretical contributions are
complemented with an empirical evaluation on the MNIST and CIFAR-10 datasets.
In fact, this paper presents the first non-vacuous generalization bounds on
CIFAR-10 for neural networks.

</details>


### [64] [DISCO: Diversifying Sample Condensation for Efficient Model Evaluation](https://arxiv.org/abs/2510.07959)
*Alexander Rubinstein,Benjamin Raible,Martin Gubri,Seong Joon Oh*

Main category: cs.LG

TL;DR: DISCO提出了一种新的模型评估方法，通过选择模型响应差异最大的样本来替代传统基于聚类的锚点子集选择，显著降低了评估成本。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型评估成本过高，传统方法依赖复杂的聚类算法且对设计选择敏感，需要更简单有效的评估方法。

Method: DISCO方法选择top-k个模型响应差异最大的样本，使用贪心的样本级统计而非全局聚类，理论证明模型间分歧提供了信息论最优的选择规则。

Result: 在MMLU、Hellaswag、Winogrande和ARC等基准测试中，DISCO实现了最先进的性能预测结果。

Conclusion: 模型响应多样性比样本多样性更重要，DISCO提供了一种概念更简单、效果更好的模型评估方法。

Abstract: Evaluating modern machine learning models has become prohibitively expensive.
Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.
Costly evaluation reduces inclusivity, slows the cycle of innovation, and
worsens environmental impact. The typical approach follows two steps. First,
select an anchor subset of data. Second, train a mapping from the accuracy on
this subset to the final test result. The drawback is that anchor selection
depends on clustering, which can be complex and sensitive to design choices. We
argue that promoting diversity among samples is not essential; what matters is
to select samples that $\textit{maximise diversity in model responses}$. Our
method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k
samples with the greatest model disagreements. This uses greedy, sample-wise
statistics rather than global clustering. The approach is conceptually simpler.
From a theoretical view, inter-model disagreement provides an
information-theoretically optimal rule for such greedy selection.
$\textbf{DISCO}$ shows empirical gains over prior methods, achieving
state-of-the-art results in performance prediction across MMLU, Hellaswag,
Winogrande, and ARC. Code is available here:
https://github.com/arubique/disco-public.

</details>


### [65] [PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation](https://arxiv.org/abs/2510.07964)
*Jiabei Cheng,Changxi Chi,Jingbo Zhou,Hongyi Xin,Jun Xia*

Main category: cs.LG

TL;DR: 提出了PRESCRIBE框架，用于单细胞扰动预测中联合估计模型不确定性和数据不确定性，通过置信度评分提高预测可靠性


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动预测需要评估预测可靠性，因为基因扰动是随机生化过程，预测准确性取决于目标基因与训练数据的相似度（模型不确定性）和训练数据质量（数据不确定性）

Method: PRESCRIBE是一个多变量深度证据回归框架，能够联合测量两种不确定性来源，为每个预测提供置信度评分

Result: PRESCRIBE的置信度评分与经验准确性高度相关，能够过滤不可靠结果，在实验中相比基线方法实现了超过3%的稳定准确率提升

Conclusion: PRESCRIBE通过联合估计模型和数据不确定性，显著提高了单细胞扰动预测的可靠性，为筛选可信预测提供了有效工具

Abstract: In single-cell perturbation prediction, a central task is to forecast the
effects of perturbing a gene unseen in the training data. The efficacy of such
predictions depends on two factors: (1) the similarity of the target gene to
those covered in the training data, which informs model (epistemic)
uncertainty, and (2) the quality of the corresponding training data, which
reflects data (aleatoric) uncertainty. Both factors are critical for
determining the reliability of a prediction, particularly as gene perturbation
is an inherently stochastic biochemical process. In this paper, we propose
PRESCRIBE (PREdicting Single-Cell Response wIth Bayesian Estimation), a
multivariate deep evidential regression framework designed to measure both
sources of uncertainty jointly. Our analysis demonstrates that PRESCRIBE
effectively estimates a confidence score for each prediction, which strongly
correlates with its empirical accuracy. This capability enables the filtering
of untrustworthy results, and in our experiments, it achieves steady accuracy
improvements of over 3% compared to comparable baselines.

</details>


### [66] [Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A Case Study with CICERO-SCM](https://arxiv.org/abs/2510.07971)
*Oskar Bohn Lassen,Serio Angelo Maria Agriesti,Filipe Rodrigues,Francisco Camara Pereira*

Main category: cs.LG

TL;DR: 提出一个多智能体强化学习框架，集成高效气候代理模型，解决多温室气体气候政策研究中的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 气候政策研究需要模拟多种温室气体对全球温度的综合影响，但现有模型计算成本高，难以嵌入强化学习环境。

Method: 使用预训练的循环神经网络架构作为CICERO-SCM气候模型的代理模型，在多智能体强化学习框架中直接集成该高效气候代理。

Result: 代理模型达到接近模拟器的精度（全球平均温度RMSE≈0.0004K），推理速度提升约1000倍，端到端训练加速超过100倍，且与模拟器收敛到相同最优策略。

Conclusion: 该方法可在不牺牲策略保真度的前提下绕过核心计算瓶颈，支持大规模多智能体实验，研究多气体动态和高保真气候响应的替代气候政策机制。

Abstract: Climate policy studies require models that capture the combined effects of
multiple greenhouse gases on global temperature, but these models are
computationally expensive and difficult to embed in reinforcement learning. We
present a multi-agent reinforcement learning (MARL) framework that integrates a
high-fidelity, highly efficient climate surrogate directly in the environment
loop, enabling regional agents to learn climate policies under multi-gas
dynamics. As a proof of concept, we introduce a recurrent neural network
architecture pretrained on ($20{,}000$) multi-gas emission pathways to
surrogate the climate model CICERO-SCM. The surrogate model attains
near-simulator accuracy with global-mean temperature RMSE $\approx 0.0004
\mathrm{K}$ and approximately $1000\times$ faster one-step inference. When
substituted for the original simulator in a climate-policy MARL setting, it
accelerates end-to-end training by $>\!100\times$. We show that the surrogate
and simulator converge to the same optimal policies and propose a methodology
to assess this property in cases where using the simulator is intractable. Our
work allows to bypass the core computational bottleneck without sacrificing
policy fidelity, enabling large-scale multi-agent experiments across
alternative climate-policy regimes with multi-gas dynamics and high-fidelity
climate response.

</details>


### [67] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 该论文通过稳定性分析推导了多八卦步骤(MGS)的泛化误差和超额误差上界，系统回答了MGS为何有效以及能否完全消除去中心化与中心化训练之间性能差距这两个关键问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化训练虽然通信效率高，但性能往往低于中心化训练。MGS作为连接两者的桥梁能显著减少性能差距，但其有效性的理论原因以及该差距能否被MGS完全消除仍是未解问题。

Method: 使用稳定性分析推导MGS的泛化误差和超额误差上界，在非凸设置下分析学习率、数据异构性、节点数量、节点样本量和通信拓扑等因素对MGS泛化的影响。

Result: 1) MGS以指数速率减少优化误差界，从而指数收紧泛化误差界；2) 即使MGS趋于无穷，去中心化训练与中心化小批量SGD之间仍存在不可忽略的泛化误差差距；3) 在CIFAR数据集上的实验验证了理论发现。

Conclusion: MGS能显著改善去中心化训练性能，但无法完全消除与中心化训练的性能差距，为去中心化训练提供了重要的理论指导。

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


### [68] [Fewer Weights, More Problems: A Practical Attack on LLM Pruning](https://arxiv.org/abs/2510.07985)
*Kazuki Egashira,Robin Staab,Thibaud Gloaguen,Mark Vero,Martin Vechev*

Main category: cs.LG

TL;DR: 该论文首次揭示了现代LLM剪枝方法可能被恶意利用的安全漏洞，攻击者可以构造看似良性的模型，但在剪枝后表现出恶意行为。


<details>
  <summary>Details</summary>
Motivation: 模型剪枝已成为减少大型语言模型内存占用的重要方法，但其安全影响尚未得到充分探索。

Method: 攻击者计算每个参数被剪枝的概率，将恶意行为注入到不太可能被剪枝的参数中，然后使用可能被剪枝的参数修复模型，从而在未剪枝模型中抵消注入的行为。

Result: 在五个模型上的广泛评估显示，经过vLLM中的任意剪枝方法处理后，模型在多种攻击场景中持续表现出强烈的恶意行为（越狱成功率高达95.7%，良性指令拒绝率98.7%，目标内容注入成功率99.5%）。

Conclusion: 研究结果揭示了关键部署时安全漏洞，并强调了在模型压缩中加强安全意识的紧迫性。

Abstract: Model pruning, i.e., removing a subset of model weights, has become a
prominent approach to reducing the memory footprint of large language models
(LLMs) during inference. Notably, popular inference engines, such as vLLM,
enable users to conveniently prune downloaded models before they are deployed.
While the utility and efficiency of pruning methods have improved
significantly, the security implications of pruning remain underexplored. In
this work, for the first time, we show that modern LLM pruning methods can be
maliciously exploited. In particular, an adversary can construct a model that
appears benign yet, once pruned, exhibits malicious behaviors. Our method is
based on the idea that the adversary can compute a proxy metric that estimates
how likely each parameter is to be pruned. With this information, the adversary
can first inject a malicious behavior into those parameters that are unlikely
to be pruned. Then, they can repair the model by using parameters that are
likely to be pruned, effectively canceling out the injected behavior in the
unpruned model. We demonstrate the severity of our attack through extensive
evaluation on five models; after any of the pruning in vLLM are applied
(Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious
behaviors in a diverse set of attack scenarios (success rates of up to $95.7\%$
for jailbreak, $98.7\%$ for benign instruction refusal, and $99.5\%$ for
targeted content injection). Our results reveal a critical deployment-time
security gap and underscore the urgent need for stronger security awareness in
model compression.

</details>


### [69] [DemandCast: Global hourly electricity demand forecasting](https://arxiv.org/abs/2510.08000)
*Kevin Steijn,Vamsi Priya Goli,Enrico Antonini*

Main category: cs.LG

TL;DR: 提出了一个基于XGBoost的机器学习框架，用于跨地域的电力需求预测，整合了历史需求、天气和社会经济变量。


<details>
  <summary>Details</summary>
Motivation: 为能源系统规划者和政策制定者在全球能源转型过程中提供准确、可扩展的电力需求预测。

Method: 使用梯度提升算法XGBoost，结合历史电力需求、天气和社会经济变量，采用时间数据分割策略进行模型训练和评估。

Result: 开发了跨多年多国的大规模数据集，实现了准确且可扩展的需求预测。

Conclusion: 该框架能够为能源转型挑战提供有价值的见解，支持稳健的电力需求预测。

Abstract: This paper presents a machine learning framework for electricity demand
forecasting across diverse geographical regions using the gradient boosting
algorithm XGBoost. The model integrates historical electricity demand and
comprehensive weather and socioeconomic variables to predict normalized
electricity demand profiles. To enable robust training and evaluation, we
developed a large-scale dataset spanning multiple years and countries, applying
a temporal data-splitting strategy that ensures benchmarking of out-of-sample
performance. Our approach delivers accurate and scalable demand forecasts,
providing valuable insights for energy system planners and policymakers as they
navigate the challenges of the global energy transition.

</details>


### [70] [Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training](https://arxiv.org/abs/2510.08008)
*Ruizhe Wang,Yucheng Ding,Xiao Liu,Yaoxiang Wang,Peng Cheng,Baining Guo,Zhengjun Zha,Yeyun Gong*

Main category: cs.LG

TL;DR: 提出通过扩展参数数量并继续训练来回收预训练检查点的方法，针对混合专家模型设计了正交增长方法，包括深度增长和宽度增长，在相同额外计算预算下比从头训练获得10.66%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型的计算成本迅速增加，许多已训练好的检查点由于工程限制或模型容量有限而未被充分利用，需要更高效地重用这些"沉没"成本。

Method: 提出正交增长方法：深度增长采用层间复制，宽度增长采用专家复制并注入噪声；通过全面缩放实验确定检查点序列中的最佳增长时机。

Result: 扩展到700亿参数和超过1万亿训练token的模型，在相同额外计算预算下比从头训练获得10.66%的准确率提升；最终准确率与沉没成本量呈强正相关。

Conclusion: 检查点回收方法为大语言模型预训练建立了经济高效的基础，表明更大的前期投资能带来更好的性能。

Abstract: The rapidly increasing computational cost of pretraining Large Language
Models necessitates more efficient approaches. Numerous computational costs
have been invested in existing well-trained checkpoints, but many of them
remain underutilized due to engineering constraints or limited model capacity.
To efficiently reuse this "sunk" cost, we propose to recycle pretrained
checkpoints by expanding their parameter counts and continuing training. We
propose orthogonal growth method well-suited for converged Mixture-of-Experts
model: interpositional layer copying for depth growth and expert duplication
with injected noise for width growth. To determine the optimal timing for such
growth across checkpoints sequences, we perform comprehensive scaling
experiments revealing that the final accuracy has a strong positive correlation
with the amount of sunk cost, indicating that greater prior investment leads to
better performance. We scale our approach to models with 70B parameters and
over 1T training tokens, achieving 10.66% accuracy gain over training from
scratch under the same additional compute budget. Our checkpoint recycling
approach establishes a foundation for economically efficient large language
model pretraining.

</details>


### [71] [Accelerated Evolving Set Processes for Local PageRank Computation](https://arxiv.org/abs/2510.08010)
*Binbin Huang,Luo Luo,Yanghua Xiao,Deqing Yang,Baojian Zhou*

Main category: cs.LG

TL;DR: 提出基于嵌套演化集过程的新框架来加速个性化PageRank计算，通过局部化不精确近端点迭代求解简化线性系统，时间复杂度和图大小无关。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化PageRank计算方法在大图上效率低的问题，特别是当精度要求高时传统方法复杂度与图规模相关。

Method: 使用嵌套演化集过程框架，在每个阶段采用局部化不精确近端点迭代求解简化线性系统，仅需求解约$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$个线性系统。

Result: 算法时间复杂度为$\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2), \tilde{\mathcal{O}}(m)\}$，当$1/\epsilon^2\ll m$时，总体复杂度为$\tilde{\mathcal{O}}(R^2/(\sqrt{\alpha}\epsilon^2))$，与图大小无关。实验验证了方法的效率。

Conclusion: 该框架成功解决了文献中的开放猜想，提供了与图规模无关的高效个性化PageRank计算方法，在真实图上表现出早期快速收敛特性。

Abstract: This work proposes a novel framework based on nested evolving set processes
to accelerate Personalized PageRank (PPR) computation. At each stage of the
process, we employ a localized inexact proximal point iteration to solve a
simplified linear system. We show that the time complexity of such localized
methods is upper bounded by $\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2),
\tilde{\mathcal{O}}(m)\}$ to obtain an $\epsilon$-approximation of the PPR
vector, where $m$ denotes the number of edges in the graph and $R$ is a
constant defined via nested evolving set processes. Furthermore, the algorithms
induced by our framework require solving only
$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$ such linear systems, where $\alpha$ is
the damping factor. When $1/\epsilon^2\ll m$, this implies the existence of an
algorithm that computes an $\ epsilon $-approximation of the PPR vector with an
overall time complexity of $\tilde{\mathcal{O}}\left(R^2 /
(\sqrt{\alpha}\epsilon^2)\right)$, independent of the underlying graph size.
Our result resolves an open conjecture from existing literature. Experimental
results on real-world graphs validate the efficiency of our methods,
demonstrating significant convergence in the early stages.

</details>


### [72] [Unsupervised Radio Map Construction in Mixed LoS/NLoS Indoor Environments](https://arxiv.org/abs/2510.08015)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 提出了一种基于隐马尔可夫模型的无位置校准无线电地图构建方法，直接从信道传播序列恢复数据采集轨迹，在室内LOS/NLOS环境下实现0.65米平均定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图构建方法需要昂贵的校准过程来收集位置标记的CSI数据集，本文旨在消除位置校准需求，直接从信道传播序列恢复数据采集轨迹。

Method: 采用HMM框架联合建模条件传播模型和用户轨迹演化，分别对MIMO网络中的功率、延迟和角度进行建模，使用高斯-马尔可夫模型建模用户轨迹，同时优化信道传播、移动模型和LOS/NLOS分类参数。

Result: 在模拟MIMO-OFDM网络实验中，该方法在室内LOS/NLOS环境下实现0.65米平均定位精度，构建的无线电地图相比传统监督方法（KNN、SVM、DNN）具有更低的定位误差。

Conclusion: 提出的HMM框架成功实现了无需位置校准的无线电地图构建，在复杂室内环境下表现出优越的定位性能，为无线通信和定位系统提供了更高效的解决方案。

Abstract: Radio maps are essential for enhancing wireless communications and
localization. However, existing methods for constructing radio maps typically
require costly calibration processes to collect location-labeled channel state
information (CSI) datasets. This paper aims to recover the data collection
trajectory directly from the channel propagation sequence, eliminating the need
for location calibration. The key idea is to employ a hidden Markov model
(HMM)-based framework to conditionally model the channel propagation matrix,
while simultaneously modeling the location correlation in the trajectory. The
primary challenges involve modeling the complex relationship between channel
propagation in multiple-input multiple-output (MIMO) networks and geographical
locations, and addressing both line-of-sight (LOS) and non-line-of-sight (NLOS)
indoor conditions. In this paper, we propose an HMM-based framework that
jointly characterizes the conditional propagation model and the evolution of
the user trajectory. Specifically, the channel propagation in MIMO networks is
modeled separately in terms of power, delay, and angle, with distinct models
for LOS and NLOS conditions. The user trajectory is modeled using a
Gaussian-Markov model. The parameters for channel propagation, the mobility
model, and LOS/NLOS classification are optimized simultaneously. Experimental
validation using simulated MIMO-Orthogonal Frequency-Division Multiplexing
(OFDM) networks with a multi-antenna uniform linear arrays (ULA) configuration
demonstrates that the proposed method achieves an average localization accuracy
of 0.65 meters in an indoor environment, covering both LOS and NLOS regions.
Moreover, the constructed radio map enables localization with a reduced error
compared to conventional supervised methods, such as k-nearest neighbors (KNN),
support vector machine (SVM), and deep neural network (DNN).

</details>


### [73] [Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses](https://arxiv.org/abs/2510.08016)
*Stanisław Pawlak,Jan Dubiński,Daniel Marczak,Bartłomiej Twardowski*

Main category: cs.LG

TL;DR: 本文提出了一种将后门攻击视为任务向量的新框架，揭示了模型合并中的安全漏洞，并开发了增强攻击效果的SBV方法和防御后门的IBVS方法。


<details>
  <summary>Details</summary>
Motivation: 模型合并虽然有效，但存在严重的安全风险，特别是容易受到后门攻击。现有研究显示攻击者可以通过在单个微调模型中植入隐藏触发器来控制最终合并模型的输出。

Method: 提出后门向量(BV)概念，计算后门模型与干净模型权重差异；开发稀疏后门向量(SBV)增强攻击效果；提出注入BV减法(IBVS)作为轻量级防御方法。

Result: SBV方法超越了现有攻击方法，是首个利用合并来提升后门有效性的技术；IBVS提供了轻量级通用防御，即使在完全未知后门威胁时仍保持有效。

Conclusion: 通过将后门攻击框架化为任务向量，不仅加深了对攻击机制的理解，还开发出了更有效的攻击和防御方法，揭示了模型合并中的核心安全漏洞。

Abstract: Model merging (MM) recently emerged as an effective method for combining
large deep learning models. However, it poses significant security risks.
Recent research shows that it is highly susceptible to backdoor attacks, which
introduce a hidden trigger into a single fine-tuned model instance that allows
the adversary to control the output of the final merged model at inference
time. In this work, we propose a simple framework for understanding backdoor
attacks by treating the attack itself as a task vector. $Backdoor\ Vector\
(BV)$ is calculated as the difference between the weights of a fine-tuned
backdoored model and fine-tuned clean model. BVs reveal new insights into
attacks understanding and a more effective framework to measure their
similarity and transferability. Furthermore, we propose a novel method that
enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\
(SBV)$ that combines multiple attacks into a single one. We identify the core
vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit
adversarial weaknesses in the base model. To counter this, we propose
$Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against
backdoors in MM. Our results show that SBVs surpass prior attacks and is the
first method to leverage merging to improve backdoor effectiveness. At the same
time, IBVS provides a lightweight, general defense that remains effective even
when the backdoor threat is entirely unknown.

</details>


### [74] [Do We Really Need Permutations? Impact of Width Expansion on Linear Mode Connectivity](https://arxiv.org/abs/2510.08023)
*Akira Ito,Masanori Yamada,Daiki Chijiwa,Atsutoshi Kumagai*

Main category: cs.LG

TL;DR: 研究表明，仅通过增加模型宽度（无需参数置换）并结合合适的softmax温度校准，即可实现线性模式连接（LMC）。作者引入层间指数加权连接性（LEWC）来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为实现LMC需要参数置换搜索和足够宽的模型，但本文旨在证明仅通过模型宽度扩展（无需置换）配合温度校准就能实现LMC。

Method: 使用softmax温度校准技术，分析中间层输出，提出层间指数加权连接性（LEWC）概念，将合并模型的每层输出表示为原始模型对应层输出的指数加权和。

Result: 实证表明，仅通过增加模型宽度并应用合适的温度校准，无需参数置换即可实现线性模式连接。LEWC分析显示合并模型输出等同于原始模型的集成。

Conclusion: 模型宽度扩展不仅促进非线性模式连接，也显著提高实现线性模式连接的可能性，这是首次证明仅通过宽度扩展（无需置换）就能实现LMC的研究。

Abstract: Recently, Ainsworth et al. empirically demonstrated that, given two
independently trained models, applying a parameter permutation that preserves
the input-output behavior allows the two models to be connected by a low-loss
linear path. When such a path exists, the models are said to achieve linear
mode connectivity (LMC). Prior studies, including Ainsworth et al., have
reported that achieving LMC requires not only an appropriate permutation search
but also sufficiently wide models (e.g., a 32 $\times$ width multiplier for
ResNet-20). This is broadly believed to be because increasing the model width
ensures a large enough space of candidate permutations, increasing the chance
of finding one that yields LMC. In this work, we empirically demonstrate that,
even without any permutations, simply widening the models is sufficient for
achieving LMC when using a suitable softmax temperature calibration. We further
explain why this phenomenon arises by analyzing intermediate layer outputs.
Specifically, we introduce layerwise exponentially weighted connectivity
(LEWC), which states that the output of each layer of the merged model can be
represented as an exponentially weighted sum of the outputs of the
corresponding layers of the original models. Consequently the merged model's
output matches that of an ensemble of the original models, which facilitates
LMC. To the best of our knowledge, this work is the first to show that widening
the model not only facilitates nonlinear mode connectivity, as suggested in
prior research, but also significantly increases the possibility of achieving
linear mode connectivity.

</details>


### [75] [Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters](https://arxiv.org/abs/2510.08059)
*Timon Klein,Piotr Minakowski,Sebastian Sager*

Main category: cs.LG

TL;DR: 提出Subject-Conditioned Layer来解决EEG解码中的主体特异性分布偏移问题，通过分解权重为共享组件和低秩修正来实现个性化适应。


<details>
  <summary>Details</summary>
Motivation: 主体特异性分布偏移是EEG解码基础模型开发的重要障碍，需要解决不同主体间的变异性问题。

Method: 设计自适应层作为标准线性或卷积层的替代，将权重分解为共享的主体不变组件和轻量级低秩修正，分离通用知识与个性化适应。

Result: 配备该层的模型在性能上优于仅使用共享权重的模型和单独训练的主体特定模型的平均值。

Conclusion: Subject-Conditioned Layer为构建有效的跨主体EEG基础模型提供了实用且可扩展的路径。

Abstract: Subject-specific distribution shifts represent an important obstacle to the
development of foundation models for EEG decoding. To address this, we propose
Subject-Conditioned Layer,, an adaptive layer designed as a drop-in replacement
for standard linear or convolutional layers in any neural network architecture.
Our layer captures subject-specific variability by decomposing its weights into
a shared, subject-invariant component and a lightweight, low-rank correction
unique to each subject. This explicit separation of general knowledge from
personalized adaptation allows existing models to become robust to subject
shifts. Empirically, models equipped with our layer outperform both a
shared-weight-only model (subject-agnostic model) and the average of
individually trained subject-specific models. Consequently, the
Subject-Conditioned Layer, offers a practical and scalable path towards
building effective cross-subject foundation models for EEG.

</details>


### [76] [Bayesian Decision Making around Experts](https://arxiv.org/abs/2510.08113)
*Daniel Jarne Ornia,Joel Dyer,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge*

Main category: cs.LG

TL;DR: 该论文研究了学习智能体如何最优地整合专家数据，包括离线设置和同时设置，提出了基于信息论的算法来量化专家数据的价值，并指导智能体何时向他人学习。


<details>
  <summary>Details</summary>
Motivation: 随着复杂学习智能体越来越多地与现有专家（如人类操作员或先前训练的智能体）一起部署，需要明确学习者如何最优地整合与自身经验结构不同的专家数据。

Method: 在贝叶斯多臂老虎机背景下研究：（1）离线设置：学习者在交互前接收专家最优策略的结果数据集；（2）同时设置：学习者每一步选择基于自身经验或专家同时获得的结果更新信念。提出了信息导向规则和信任推断策略。

Result: 证明了在专家结果上预训练可以通过专家数据与最优动作之间的互信息来收紧信息论遗憾界限。提出了最大化关于最优动作的单步信息增益的数据源处理规则。

Conclusion: 通过量化专家数据的价值，该框架为智能体智能地决定何时向他人学习提供了实用的信息论算法，包括在专家无效或被破坏时的安全保护机制。

Abstract: Complex learning agents are increasingly deployed alongside existing experts,
such as human operators or previously trained agents. However, it remains
unclear how should learners optimally incorporate certain forms of expert data,
which may differ in structure from the learner's own action-outcome
experiences. We study this problem in the context of Bayesian multi-armed
bandits, considering: (i) offline settings, where the learner receives a
dataset of outcomes from the expert's optimal policy before interaction, and
(ii) simultaneous settings, where the learner must choose at each step whether
to update its beliefs based on its own experience, or based on the outcome
simultaneously achieved by an expert. We formalize how expert data influences
the learner's posterior, and prove that pretraining on expert outcomes tightens
information-theoretic regret bounds by the mutual information between the
expert data and the optimal action. For the simultaneous setting, we propose an
information-directed rule where the learner processes the data source that
maximizes their one-step information gain about the optimal action. Finally, we
propose strategies for how the learner can infer when to trust the expert and
when not to, safeguarding the learner for the cases where the expert is
ineffective or compromised. By quantifying the value of expert data, our
framework provides practical, information-theoretic algorithms for agents to
intelligently decide when to learn from others.

</details>


### [77] [Approximate Domain Unlearning for Vision-Language Models](https://arxiv.org/abs/2510.08132)
*Kodai Kawamura,Yuta Goto,Rintaro Yanagi,Hirokatsu Kataoka,Go Irie*

Main category: cs.LG

TL;DR: 提出了近似领域遗忘(ADU)的新问题设置，旨在从预训练视觉语言模型中移除特定领域知识，同时保持其他领域的识别性能，解决了现有类别遗忘方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型虽然泛化能力强，但会保留下游任务不需要的冗余信息，带来计算效率和信息泄露问题。现有类别遗忘方法在实际应用中不足，比如自动驾驶系统需要区分真实汽车和广告中的插图汽车。

Method: 提出显式解耦领域分布和自适应捕获实例特定领域信息的方法，解决领域分布在特征空间中高度纠缠的问题。

Result: 大量实验表明，该方法在基于VLM调优技术构建的基线方法上表现更优。

Conclusion: 为视觉语言模型中的实用和细粒度遗忘开辟了新途径。

Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization
capabilities, enabling them to recognize a wide range of objects across diverse
domains without additional training. However, they often retain irrelevant
information beyond the requirements of specific downstream tasks, raising
concerns about computational efficiency and potential information leakage. This
has motivated growing interest in approximate unlearning, which aims to
selectively remove unnecessary knowledge while preserving overall model
performance. Existing approaches to approximate unlearning have primarily
focused on class unlearning, where a VLM is retrained to fail to recognize
specified object classes while maintaining accuracy for others. However, merely
forgetting object classes is often insufficient in practical applications. For
instance, an autonomous driving system should accurately recognize real cars
while avoiding misrecognition of illustrated cars depicted in roadside
advertisements as real cars, which could be hazardous. In this paper, we
introduce Approximate Domain Unlearning (ADU), a novel problem setting that
requires reducing recognition accuracy for images from specified domains (e.g.,
illustration) while preserving accuracy for other domains (e.g., real). ADU
presents new technical challenges: due to the strong domain generalization
capability of pre-trained VLMs, domain distributions are highly entangled in
the feature space, making naive approaches based on penalizing target domains
ineffective. To tackle this limitation, we propose a novel approach that
explicitly disentangles domain distributions and adaptively captures
instance-specific domain information. Extensive experiments show that our
approach outperforms baselines built upon VLM tuning techniques, paving the way
for practical and fine-grained unlearning in VLMs. Code:
https://kodaikawamura.github.io/Domain_Unlearning/.

</details>


### [78] [Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning](https://arxiv.org/abs/2510.08141)
*Chen Wang,Zhaochun Li,Jionghao Bai,Yuzhi Zhang,Shisheng Cui,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 提出了AEPO方法解决强化学习微调中的熵崩溃问题，通过温度调节实现精确的熵控制，揭示了熵与性能的非单调关系，并提供了更广泛的RFT范式。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法存在熵崩溃问题，导致探索消失和策略过早收敛，而现有的熵正则化方法只能部分缓解问题并引入偏差和不稳定性。

Method: 提出AEPO方法，用温度调整分布上的REINFORCE策略梯度替代熵奖励，通过温度调节稳定熵，包含策略梯度正则化、分布正则化和REINFORCE正则化三个关键设计。

Result: AEPO能在任意目标水平稳定熵，有效消除GRPO中的熵崩溃；揭示了熵与性能的非单调关系；提供了更广泛的RFT范式。

Conclusion: AEPO解决了熵控制问题，阐明了熵、探索和推理之间的联系，并为RFT提供了更通用的框架。

Abstract: Reinforcement finetuning (RFT) is essential for enhancing the reasoning
capabilities of large language models (LLM), yet the widely adopted Group
Relative Policy Optimization (GRPO) suffers from entropy collapse, where
entropy monotonically decreases, exploration vanishes, and policies converge
prematurely. Existing entropy-regularized methods only partially alleviate this
issue while introducing bias and instability, leaving entropy control
unresolved and the connection between entropy, exploration, and performance
unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which
eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy
gradient on temperature-adjusted distributions and stabilizing entropy through
temperature regulation. AEPO integrates three key designs: policy gradient as
regularization, distribution as regularization, and REINFORCE as
regularization, enabling precise entropy control without distorting
optimization. Experiments demonstrate three major contributions: AEPO (1)
stabilizes entropy at arbitrary target levels, effectively removing collapse in
GRPO; (2) reveals a non-monotonic relation where performance first improves
then declines with increasing entropy, clarifying the link between entropy,
exploration, and reasoning; and (3) generalizes beyond entropy, providing a
broader RFT paradigm where superior target distributions can serve as REINFORCE
regularizers.

</details>


### [79] [Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning](https://arxiv.org/abs/2510.08146)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 提出基于香农熵的框架，利用token级logprobs作为置信信号实现早停，在保持任务准确性的同时节省25-50%计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代推理模型具有基于熵的置信度校准这一涌现特性，可用于提高token效率，而标准指令调优和预训练模型缺乏此特性。

Method: 使用token级logprobs计算香农熵作为置信信号，通过可计算的熵阈值实现早停推理。

Result: 在推理优化模型系列中实现25-50%计算成本降低，同时保持准确性，表明置信机制是现代后训练推理系统的显著特征。

Conclusion: 高级推理模型能够早期识别正确答案，这种涌现的置信意识可被利用来节省token并减少延迟。

Abstract: We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they've gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.

</details>


### [80] [Unsupervised Multi-Source Federated Domain Adaptation under Domain Diversity through Group-Wise Discrepancy Minimization](https://arxiv.org/abs/2510.08150)
*Larissa Reichart,Cem Ata Baykara,Ali Burak Ünal,Mete Akgün,Harlin Lee*

Main category: cs.LG

TL;DR: GALA是一个可扩展的联邦无监督多源域自适应框架，通过组间差异最小化和温度控制加权策略，解决了多源域自适应中的计算复杂性和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式无监督多源域自适应方法假设源域数量较少，无法有效扩展到大量异构域，导致计算开销大或性能不稳定。

Method: 提出两个关键组件：1）组间差异最小化目标，无需二次计算即可近似全对域对齐；2）温度控制的基于质心的加权策略，根据与目标的匹配度动态优先处理源域。

Result: 在标准基准测试中取得竞争性或最先进的结果，在多样性多源设置中显著优于先前方法，特别是在其他方法无法收敛的情况下。

Conclusion: GALA框架能够在大规模异构源域设置下实现稳定且可并行化的训练，为解决多源域自适应中的可扩展性和鲁棒性问题提供了有效方案。

Abstract: Unsupervised multi-source domain adaptation (UMDA) aims to learn models that
generalize to an unlabeled target domain by leveraging labeled data from
multiple, diverse source domains. While distributed UMDA methods address
privacy constraints by avoiding raw data sharing, existing approaches typically
assume a small number of sources and fail to scale effectively. Increasing the
number of heterogeneous domains often makes existing methods impractical,
leading to high computational overhead or unstable performance. We propose
GALA, a scalable and robust federated UMDA framework that introduces two key
components: (1) a novel inter-group discrepancy minimization objective that
efficiently approximates full pairwise domain alignment without quadratic
computation; and (2) a temperature-controlled, centroid-based weighting
strategy that dynamically prioritizes source domains based on alignment with
the target. Together, these components enable stable and parallelizable
training across large numbers of heterogeneous sources. To evaluate performance
in high-diversity scenarios, we introduce Digit-18, a new benchmark comprising
18 digit datasets with varied synthetic and real-world domain shifts. Extensive
experiments show that GALA consistently achieves competitive or
state-of-the-art results on standard benchmarks and significantly outperforms
prior methods in diverse multi-source settings where others fail to converge.

</details>


### [81] [Beyond Sub-6 GHz: Leveraging mmWave Wi-Fi for Gait-Based Person Identification](https://arxiv.org/abs/2510.08160)
*Nabeel Nisar Bhat,Maksim Karnaukh,Jakob Struye,Rafael Berkvens,Jeroen Famaey*

Main category: cs.LG

TL;DR: 本文首次比较了sub-6 GHz和毫米波Wi-Fi信号在人员识别方面的性能，发现毫米波Wi-Fi即使在10Hz低采样率下也能达到91.2%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 随着毫米波技术的出现，其在人员识别方面的优势尚未得到充分探索，本文旨在填补这一空白。

Method: 使用商用Wi-Fi设备收集同步的sub-6 GHz和毫米波测量数据，采用相同的训练流程和模型配置，利用端到端深度学习结合有效的背景减除技术。

Result: 毫米波Wi-Fi信号在20人识别任务中达到91.2%的准确率，显著优于sub-6 GHz频段。

Conclusion: 毫米波Wi-Fi凭借其更精细的空间分辨率，在人员识别方面具有明显优势，特别是在低采样率条件下仍能保持高性能。

Abstract: Person identification plays a vital role in enabling intelligent,
personalized, and secure human-computer interaction. Recent research has
demonstrated the feasibility of leveraging Wi-Fi signals for passive person
identification using a person's unique gait pattern. Although most existing
work focuses on sub-6 GHz frequencies, the emergence of mmWave offers new
opportunities through its finer spatial resolution, though its comparative
advantages for person identification remain unexplored. This work presents the
first comparative study between sub-6 GHz and mmWave Wi-Fi signals for person
identification with commercial off-the-shelf (COTS) Wi-Fi, using a novel
dataset of synchronized measurements from the two frequency bands in an indoor
environment. To ensure a fair comparison, we apply identical training pipelines
and model configurations across both frequency bands. Leveraging end-to-end
deep learning, we show that even at low sampling rates (10 Hz), mmWave Wi-Fi
signals can achieve high identification accuracy (91.2% on 20 individuals) when
combined with effective background subtraction.

</details>


### [82] [Bidirectional Representations Augmented Autoregressive Biological Sequence Generation:Application in De Novo Peptide Sequencing](https://arxiv.org/abs/2510.08169)
*Xiang Zhang,Jiaqi Wei,Zijie Qiu,Sheng Xu,Zhi Jin,ZhiQiang Gao,Nanqing Dong,Siqi Sun*

Main category: cs.LG

TL;DR: 提出了一种混合框架，通过动态整合非自回归机制的丰富上下文信息来增强自回归生成，在九物种肽从头测序基准测试中显著超越AR和NAR基线。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在生物序列生成任务中受限于单向性，无法捕捉关键的全局双向标记依赖关系；非自回归模型虽然提供双向表示，但在生成连贯性和可扩展性方面存在挑战。

Method: 采用共享输入编码器和两个解码器的混合框架：非自回归解码器学习潜在双向生物特征，自回归解码器通过新颖的跨解码器注意力模块迭代查询和整合这些双向特征来合成生物序列。

Result: 在九物种肽从头测序基准测试中，该模型显著超越了自回归和非自回归基线模型，成功协调了自回归的稳定性与非自回归的上下文感知能力。

Conclusion: 该研究推进了生物序列建模技术，为增强自回归模型的双向理解能力提供了一种新颖的架构范式，在复杂序列生成任务中表现出色。

Abstract: Autoregressive (AR) models, common in sequence generation, are limited in
many biological tasks such as de novo peptide sequencing and protein modeling
by their unidirectional nature, failing to capture crucial global bidirectional
token dependencies. Non-Autoregressive (NAR) models offer holistic,
bidirectional representations but face challenges with generative coherence and
scalability. To transcend this, we propose a hybrid framework enhancing AR
generation by dynamically integrating rich contextual information from
non-autoregressive mechanisms. Our approach couples a shared input encoder with
two decoders: a non-autoregressive one learning latent bidirectional biological
features, and an AR decoder synthesizing the biological sequence by leveraging
these bidirectional features. A novel cross-decoder attention module enables
the AR decoder to iteratively query and integrate these bidirectional features,
enriching its predictions. This synergy is cultivated via a tailored training
strategy with importance annealing for balanced objectives and cross-decoder
gradient blocking for stable, focused learning. Evaluations on a demanding
nine-species benchmark of de novo peptide sequencing show that our model
substantially surpasses AR and NAR baselines. It uniquely harmonizes AR
stability with NAR contextual awareness, delivering robust, superior
performance on diverse downstream data. This research advances biological
sequence modeling techniques and contributes a novel architectural paradigm for
augmenting AR models with enhanced bidirectional understanding for complex
sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.

</details>


### [83] [Long-tailed Recognition with Model Rebalancing](https://arxiv.org/abs/2510.08177)
*Jiaan Luo,Feng Hong,Qiang Hu,Xiaofeng Cao,Feng Liu,Jiangchao Yao*

Main category: cs.LG

TL;DR: 提出了MORE框架，通过直接重新平衡模型参数空间来缓解长尾识别问题，使用低秩参数组件和正弦重加权调度，不增加模型复杂度或推理成本。


<details>
  <summary>Details</summary>
Motivation: 长尾识别在深度学习和基础模型微调中普遍存在且具有挑战性，偏斜的类别分布阻碍了模型对尾部类别的泛化能力。现有方法在如多标签长尾识别等广泛场景中难以取得一致改进。

Method: MORE框架引入低秩参数组件来调节参数空间分配，通过定制的损失函数和正弦重加权调度进行指导，但不增加整体模型复杂度或推理成本。

Result: 在多样化的长尾基准测试中，涵盖多类和多标签任务，MORE显著改善了泛化能力，特别是对尾部类别，并能有效补充现有的不平衡缓解方法。

Conclusion: MORE作为长尾设置中的稳健即插即用模块具有巨大潜力，能够在不增加计算成本的情况下有效提升模型性能。

Abstract: Long-tailed recognition is ubiquitous and challenging in deep learning and
even in the downstream finetuning of foundation models, since the skew class
distribution generally prevents the model generalization to the tail classes.
Despite the promise of previous methods from the perspectives of data
augmentation, loss rebalancing and decoupled training etc., consistent
improvement in the broad scenarios like multi-label long-tailed recognition is
difficult. In this study, we dive into the essential model capacity impact
under long-tailed context, and propose a novel framework, Model Rebalancing
(MORE), which mitigates imbalance by directly rebalancing the model's parameter
space. Specifically, MORE introduces a low-rank parameter component to mediate
the parameter space allocation guided by a tailored loss and sinusoidal
reweighting schedule, but without increasing the overall model complexity or
inference costs. Extensive experiments on diverse long-tailed benchmarks,
spanning multi-class and multi-label tasks, demonstrate that MORE significantly
improves generalization, particularly for tail classes, and effectively
complements existing imbalance mitigation methods. These results highlight
MORE's potential as a robust plug-and-play module in long-tailed settings.

</details>


### [84] [Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data](https://arxiv.org/abs/2510.08179)
*Feng Hong,Yu Huang,Zihua Zhao,Zhihan Zhou,Jiangchao Yao,Dongsheng Li,Ya Zhang,Yanfeng Wang*

Main category: cs.LG

TL;DR: 提出了D-SINK框架，通过蒸馏和整合专门处理类别不平衡或标签噪声的'弱'辅助模型的互补见解，来增强深度学习模型对长尾噪声数据的双重鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集经常同时面临类别不平衡和标签噪声的挑战，现有方法难以有效结合，因为区分真正的尾部样本和噪声数据很困难，往往导致冲突的优化策略。

Method: 提出了双粒度Sinkhorn蒸馏(D-SINK)框架，使用最优传输优化的代理标签分配，将目标模型的样本级预测与噪声鲁棒辅助模型对齐，将类别分布与不平衡鲁棒辅助模型对齐。

Result: 在基准数据集上的广泛实验表明，D-SINK显著提高了鲁棒性，并在学习长尾噪声数据方面实现了强大的实证性能。

Conclusion: 通过协同利用专门处理单一问题的'弱'辅助模型，可以有效解决类别不平衡和标签噪声的双重挑战，D-SINK框架为此提供了一种新颖且有效的解决方案。

Abstract: Real-world datasets for deep learning frequently suffer from the co-occurring
challenges of class imbalance and label noise, hindering model performance.
While methods exist for each issue, effectively combining them is non-trivial,
as distinguishing genuine tail samples from noisy data proves difficult, often
leading to conflicting optimization strategies. This paper presents a novel
perspective: instead of primarily developing new complex techniques from
scratch, we explore synergistically leveraging well-established, individually
'weak' auxiliary models - specialized for tackling either class imbalance or
label noise but not both. This view is motivated by the insight that class
imbalance (a distributional-level concern) and label noise (a sample-level
concern) operate at different granularities, suggesting that robustness
mechanisms for each can in principle offer complementary strengths without
conflict. We propose Dual-granularity Sinkhorn Distillation (D-SINK), a novel
framework that enhances dual robustness by distilling and integrating
complementary insights from such 'weak', single-purpose auxiliary models.
Specifically, D-SINK uses an optimal transport-optimized surrogate label
allocation to align the target model's sample-level predictions with a
noise-robust auxiliary and its class distributions with an imbalance-robust
one. Extensive experiments on benchmark datasets demonstrate that D-SINK
significantly improves robustness and achieves strong empirical performance in
learning from long-tailed noisy data.

</details>


### [85] [FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption](https://arxiv.org/abs/2510.08217)
*Justus Viga,Penelope Mueck,Alexander Löser,Torben Weis*

Main category: cs.LG

TL;DR: 本文介绍了船舶燃料消耗预测的新数据集和标准化基准，并首次在该领域应用TabPFN基础模型进行上下文学习，结果显示包含环境条件和时间上下文的模型性能更优。


<details>
  <summary>Details</summary>
Motivation: 航运业中燃料消耗和排放对经济效率和环境可持续性至关重要，但异质方法和有限的高质量数据集阻碍了建模方法的直接比较。

Method: 引入并发布包含三艘船舶运行和环境数据的新数据集；定义涵盖表格回归和时间序列回归的标准化基准；研究使用TabPFN基础模型进行船舶消耗建模的上下文学习方法。

Result: 所有评估模型均表现出强劲性能，支持船上数据驱动燃料预测的可行性。包含环境条件的模型始终优于仅依赖船速的简单多项式基线。TabPFN略微优于其他技术，包含时间上下文可提高准确性。

Conclusion: 基础模型具有上下文学习能力在表格预测中具有潜力，包含环境条件和时间上下文的模型能显著提高船舶燃料消耗预测的准确性。

Abstract: In the shipping industry, fuel consumption and emissions are critical factors
due to their significant impact on economic efficiency and environmental
sustainability. Accurate prediction of ship fuel consumption is essential for
further optimization of maritime operations. However, heterogeneous
methodologies and limited high-quality datasets hinder direct comparison of
modeling approaches. This paper makes three key contributions: (1) we introduce
and release a new dataset
(https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational
and environmental data from three ships; (2) we define a standardized benchmark
covering tabular regression and time-series regression (3) we investigate the
application of in-context learning for ship consumption modeling using the
TabPFN foundation model - a first in this domain to our knowledge. Our results
demonstrate strong performance across all evaluated models, supporting the
feasibility of onboard, data-driven fuel prediction. Models incorporating
environmental conditions consistently outperform simple polynomial baselines
relying solely on vessel speed. TabPFN slightly outperforms other techniques,
highlighting the potential of foundation models with in-context learning
capabilities for tabular prediction. Furthermore, including temporal context
improves accuracy.

</details>


### [86] [Expressive Value Learning for Scalable Offline Reinforcement Learning](https://arxiv.org/abs/2510.08218)
*Nicolas Espinosa-Dice,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: EVOR是一种可扩展的离线强化学习方法，通过流匹配学习正则化Q函数，在推理时通过拒绝采样进行策略提取，避免了蒸馏和反向传播时间的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临可扩展性挑战，现有方法依赖计算昂贵的反向传播时间或存在复合误差的策略蒸馏。需要开发不依赖这两种方法的可扩展离线RL方法。

Method: EVOR结合表达性策略和表达性价值函数，通过流匹配训练学习最优正则化Q函数，在推理时使用拒绝采样进行策略提取。

Result: 实验表明EVOR在多样化离线RL任务上优于基线方法，验证了表达性价值学习在离线RL中的优势。

Conclusion: EVOR通过整合表达性价值学习，为离线强化学习提供了一种可扩展且高效的解决方案，避免了传统方法的计算和误差问题。

Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make
sequences of decisions. However, RL has yet to be fully leveraged in robotics,
principally due to its lack of scalability. Offline RL offers a promising
avenue by training agents on large, diverse datasets, avoiding the costly
real-world interactions of online RL. Scaling offline RL to increasingly
complex datasets requires expressive generative models such as diffusion and
flow matching. However, existing methods typically depend on either
backpropagation through time (BPTT), which is computationally prohibitive, or
policy distillation, which introduces compounding errors and limits scalability
to larger base policies. In this paper, we consider the question of how to
develop a scalable offline RL approach without relying on distillation or
backpropagation through time. We introduce Expressive Value Learning for
Offline Reinforcement Learning (EVOR): a scalable offline RL approach that
integrates both expressive policies and expressive value functions. EVOR learns
an optimal, regularized Q-function via flow matching during training. At
inference-time, EVOR performs inference-time policy extraction via rejection
sampling against the expressive value function, enabling efficient
optimization, regularization, and compute-scalable search without retraining.
Empirically, we show that EVOR outperforms baselines on a diverse set of
offline RL tasks, demonstrating the benefit of integrating expressive value
learning into offline RL.

</details>


### [87] [Post-hoc Stochastic Concept Bottleneck Models](https://arxiv.org/abs/2510.08219)
*Wiktor Jan Hoffmann,Sonia Laguna,Moritz Vandenhirtz,Emanuele Palumbo,Julia E. Vogt*

Main category: cs.LG

TL;DR: PSCBMs是一种轻量级方法，通过添加小型协方差预测模块，为预训练的概念瓶颈模型添加概念间的多元正态分布，无需重新训练主干模型，显著提升了干预性能。


<details>
  <summary>Details</summary>
Motivation: 现有建模概念依赖性的方法通常需要重新训练整个模型，这在原始数据或计算资源有限时不可行。

Method: 在预训练CBM基础上添加小型协方差预测模块，构建概念间的多元正态分布，提出两种训练策略。

Result: PSCBMs在测试时持续匹配或提升概念和目标准确率，在干预下表现远优于标准CBMs，且比从头训练类似随机模型更高效。

Conclusion: PSCBMs通过轻量级方式有效建模概念依赖关系，显著提升干预性能，同时保持高效性。

Abstract: Concept Bottleneck Models (CBMs) are interpretable models that predict the
target variable through high-level human-understandable concepts, allowing
users to intervene on mispredicted concepts to adjust the final output. While
recent work has shown that modeling dependencies between concepts can improve
CBM performance, especially under interventions, such approaches typically
require retraining the entire model, which may be infeasible when access to the
original data or compute is limited. In this paper, we introduce Post-hoc
Stochastic Concept Bottleneck Models (PSCBMs), a lightweight method that
augments any pre-trained CBM with a multivariate normal distribution over
concepts by adding only a small covariance-prediction module, without
retraining the backbone model. We propose two training strategies and show on
real-world data that PSCBMs consistently match or improve both concept and
target accuracy over standard CBMs at test time. Furthermore, we show that due
to the modeling of concept dependencies, PSCBMs perform much better than CBMs
under interventions, while remaining far more efficient than retraining a
similar stochastic model from scratch.

</details>


### [88] [Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning](https://arxiv.org/abs/2510.08226)
*Michal Koren,Or Peretz,Tai Dinh,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了不确定性感知马尔可夫决策过程（UAMDP），一个结合贝叶斯预测、后验采样强化学习和条件风险价值约束的统一框架，用于在波动性高风险环境中进行更安全的顺序决策。


<details>
  <summary>Details</summary>
Motivation: 在波动性高风险的顺序决策环境中，仅最大化期望收益是不够的，需要系统性的不确定性管理方法。

Method: UAMDP框架将贝叶斯预测、后验采样强化学习和条件风险价值约束相结合，在闭环中更新潜在动态信念、通过Thompson采样生成合理未来情景，并在预设风险容忍度下优化策略。

Result: 在高频股票交易和零售库存控制两个领域的评估显示，UAMDP显著改善了长期预测准确性（RMSE降低25%，sMAPE降低32%），并转化为更好的经济表现：交易夏普比率从1.54提升至1.74，最大回撤减半。

Conclusion: 整合校准概率建模、与后验不确定性对齐的探索以及风险感知控制，能够产生稳健、可泛化的方法，实现更安全、更有利的顺序决策。

Abstract: Sequential decisions in volatile, high-stakes settings require more than
maximizing expected return; they require principled uncertainty management.
This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a
unified framework that couples Bayesian forecasting, posterior-sampling
reinforcement learning, and planning under a conditional value-at-risk (CVaR)
constraint. In a closed loop, the agent updates its beliefs over latent
dynamics, samples plausible futures via Thompson sampling, and optimizes
policies subject to preset risk tolerances. We establish regret bounds that
converge to the Bayes-optimal benchmark under standard regularity conditions.
We evaluate UAMDP in two domains-high-frequency equity trading and retail
inventory control-both marked by structural uncertainty and economic
volatility. Relative to strong deep learning baselines, UAMDP improves
long-horizon forecasting accuracy (RMSE decreases by up to 25\% and sMAPE by
32\%), and these gains translate into economic performance: the trading Sharpe
ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These
results show that integrating calibrated probabilistic modeling, exploration
aligned with posterior uncertainty, and risk-aware control yields a robust,
generalizable approach to safer and more profitable sequential decision-making.

</details>


### [89] [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Petr Molodyk,Bo Yuan,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 本文提出了DMPO方法，一种专门为扩散大语言模型设计的强化学习微调方法，通过分布匹配框架显著提升推理能力，在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归大语言模型的替代方案具有更高推理吞吐量的潜力，但需要专门的强化学习算法来达到可比的性能，特别是在推理任务上。

Method: 提出了分布匹配策略优化方法，通过交叉熵优化将dLLM策略分布与最优奖励倾斜分布进行匹配，并针对小批量训练挑战提出了权重基线减法技术。

Result: 在多个推理基准测试中，DMPO无需监督微调即可实现显著性能提升，相比之前SOTA基线准确率提升高达42.9%，相比基础模型提升55.8%。

Conclusion: 分布匹配框架对于提升扩散大语言模型的推理能力非常有效，DMPO方法为dLLMs的强化学习微调提供了理论基础和实践解决方案。

Abstract: Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.

</details>


### [90] [The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models](https://arxiv.org/abs/2510.08236)
*Konrad Löhr,Shuzhou Yuan,Michael Färber*

Main category: cs.LG

TL;DR: 该研究使用政治罗盘测试评估了8个主流大语言模型的政治偏见和刻板印象传播，发现所有模型都呈现左倾政治倾向，且通过语言变化引发的隐性刻板印象比显性刻板印象更明显。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在社会信息传播和决策过程中的作用日益重要，理解其政治偏见对于防止对公共舆论和民主进程产生不当影响至关重要。

Method: 使用二维政治罗盘测试评估模型固有政治倾向，通过角色提示探索显性刻板印象，使用多语言版本PCT揭示隐性刻板印象。

Result: 所有模型都显示一致的左倾政治倾向；不同模型的刻板印象性质和程度差异很大；隐性刻板印象比显性更明显；大多数模型的隐性和显性刻板印象呈现显著一致性。

Conclusion: 研究揭示了大语言模型中政治偏见和刻板印象的复杂相互作用，表明模型对其固有偏见具有一定程度的透明度或"意识"。

Abstract: Large Language Models (LLMs) are increasingly integral to information
dissemination and decision-making processes. Given their growing societal
influence, understanding potential biases, particularly within the political
domain, is crucial to prevent undue influence on public opinion and democratic
processes. This work investigates political bias and stereotype propagation
across eight prominent LLMs using the two-dimensional Political Compass Test
(PCT). Initially, the PCT is employed to assess the inherent political leanings
of these models. Subsequently, persona prompting with the PCT is used to
explore explicit stereotypes across various social dimensions. In a final step,
implicit stereotypes are uncovered by evaluating models with multilingual
versions of the PCT. Key findings reveal a consistent left-leaning political
alignment across all investigated models. Furthermore, while the nature and
extent of stereotypes vary considerably between models, implicit stereotypes
elicited through language variation are more pronounced than those identified
via explicit persona prompting. Interestingly, for most models, implicit and
explicit stereotypes show a notable alignment, suggesting a degree of
transparency or "awareness" regarding their inherent biases. This study
underscores the complex interplay of political bias and stereotypes in LLMs.

</details>


### [91] [Opponent Shaping in LLM Agents](https://arxiv.org/abs/2510.08255)
*Marta Emili Garcia Segura,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: 本文首次研究了基于LLM的智能体中的对手塑造问题，提出了ShapeLLM方法，证明LLM智能体能够在竞争性和合作性游戏中影响对手的学习动态。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为自主智能体在现实环境中部署，多智能体交互不可避免。需要理解LLM智能体是否能够像强化学习智能体一样，通过交互塑造对手的学习动态和行为。

Method: 提出ShapeLLM方法，这是针对基于transformer的智能体量身定制的模型无关对手塑造方法，克服了现有方法需要高阶导数、可扩展性限制或transformer架构缺失组件的问题。

Result: 在竞争性游戏（迭代囚徒困境、匹配硬币和胆小鬼游戏）中，LLM智能体成功引导对手走向可被利用的均衡；在合作性游戏（迭代猎鹿游戏和合作版囚徒困境）中，促进协调并提高集体福利。

Conclusion: LLM智能体能够通过交互塑造对手并被对手塑造，确立了对手塑造作为多智能体LLM研究的关键维度。

Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous
agents in real-world environments. As these deployments scale, multi-agent
interactions become inevitable, making it essential to understand strategic
behavior in such systems. A central open question is whether LLM agents, like
reinforcement learning agents, can shape the learning dynamics and influence
the behavior of others through interaction alone. In this paper, we present the
first investigation of opponent shaping (OS) with LLM-based agents. Existing OS
algorithms cannot be directly applied to LLMs, as they require higher-order
derivatives, face scalability constraints, or depend on architectural
components that are absent in transformers. To address this gap, we introduce
ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based
agents. Using ShapeLLM, we examine whether LLM agents can influence co-players'
learning dynamics across diverse game-theoretic environments. We demonstrate
that LLM agents can successfully guide opponents toward exploitable equilibria
in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and
Chicken) and promote coordination and improve collective welfare in cooperative
games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).
Our findings show that LLM agents can both shape and be shaped through
interaction, establishing opponent shaping as a key dimension of multi-agent
LLM research.

</details>


### [92] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: 提出了Mix-和MoE-DPO框架，通过软混合模型和专家混合架构扩展DPO，使用随机变分推理方法学习专门化的专家策略，解决了标准DPO在异构偏好分布和多任务设置中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法依赖单一模型，限制了其在多任务环境中的表达能力以及对异构或多样化偏好分布的适应性。

Method: 引入基于专家分配的潜变量模型，优化变分证据下界，支持共享基础架构与专家特定策略头或完全独立的专家模型。

Result: 在多种模型规模和多偏好数据集上验证了方法的有效性，展示了Mix-和MoE-DPO作为偏好对齐的强大可扩展方法。

Conclusion: Mix-和MoE-DPO提供了比标准DPO更好的泛化能力、奖励和策略专门化以及上下文对齐，是偏好对齐的有效扩展框架。

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [93] [Counterfactual Identifiability via Dynamic Optimal Transport](https://arxiv.org/abs/2510.08294)
*Fabio De Sousa Ribeiro,Ainkaran Santhirasekaram,Ben Glocker*

Main category: cs.LG

TL;DR: 该论文解决了从观测数据中识别高维多元结果反事实的开放性问题，建立了基于连续时间流的多元反事实识别基础，确保因果推断的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实推断方法缺乏识别性，削弱了其因果估计的有效性。Pearl认为反事实必须可识别才能证明因果主张的合理性。

Method: 使用连续时间流匹配方法，包括非马尔可夫设置下的标准准则，利用动态最优传输工具构建唯一、单调且保持排序的反事实传输映射。

Result: 在具有反事实真实值的受控场景中验证了理论，并在真实图像上展示了在公理化反事实合理性方面的改进。

Conclusion: 建立了多元反事实识别的理论基础，确保反事实推断的一致性和因果有效性。

Abstract: We address the open question of counterfactual identification for
high-dimensional multivariate outcomes from observational data. Pearl (2000)
argues that counterfactuals must be identifiable (i.e., recoverable from the
observed data distribution) to justify causal claims. A recent line of work on
counterfactual inference shows promising results but lacks identification,
undermining the causal validity of its estimates. To address this, we establish
a foundation for multivariate counterfactual identification using
continuous-time flows, including non-Markovian settings under standard
criteria. We characterise the conditions under which flow matching yields a
unique, monotone and rank-preserving counterfactual transport map with tools
from dynamic optimal transport, ensuring consistent inference. Building on
this, we validate the theory in controlled scenarios with counterfactual
ground-truth and demonstrate improvements in axiomatic counterfactual soundness
on real images.

</details>


### [94] [Bridging the Physics-Data Gap with FNO-Guided Conditional Flow Matching: Designing Inductive Bias through Hierarchical Physical Constraints](https://arxiv.org/abs/2510.08295)
*Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种将物理定律层次结构嵌入深度生成模型的分层框架，通过傅里叶神经算子和条件流匹配实现物理约束的时间序列生成。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列生成方法往往忽略领域特定的物理约束，限制了统计和物理一致性，需要将物理定律的层次结构直接整合到生成模型中。

Method: 结合傅里叶神经算子学习物理算子，使用条件流匹配进行概率生成，通过时间依赖的分层约束和FNO引导的修正进行集成。

Result: 在谐波振荡器、人体活动识别和锂离子电池退化实验中，生成质量提高16.3%，物理违规减少46%，预测精度提升18.5%。

Conclusion: 该框架成功将物理信息归纳偏置引入生成模型，显著提升了生成质量和物理一致性，为物理约束的时间序列生成提供了新范式。

Abstract: Conventional time-series generation often ignores domain-specific physical
constraints, limiting statistical and physical consistency. We propose a
hierarchical framework that embeds the inherent hierarchy of physical
laws-conservation, dynamics, boundary, and empirical relations-directly into
deep generative models, introducing a new paradigm of physics-informed
inductive bias. Our method combines Fourier Neural Operators (FNOs) for
learning physical operators with Conditional Flow Matching (CFM) for
probabilistic generation, integrated via time-dependent hierarchical
constraints and FNO-guided corrections. Experiments on harmonic oscillators,
human activity recognition, and lithium-ion battery degradation show 16.3%
higher generation quality, 46% fewer physics violations, and 18.5% improved
predictive accuracy over baselines.

</details>


### [95] [Dynamic Features Adaptation in Networking: Toward Flexible training and Explainable inference](https://arxiv.org/abs/2510.08303)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Merim Dzaferagic,John D. Kelleher*

Main category: cs.LG

TL;DR: 本文提出将自适应随机森林(ARFs)和漂移感知特征重要性(DAFI)相结合，为6G网络构建灵活的AI方法框架，以应对动态变化的网络环境。


<details>
  <summary>Details</summary>
Motivation: 随着AI成为6G网络控制的核心组件，AI模型需要适应不断变化的条件，包括多供应商部署、硬件升级和服务需求演进带来的新特征和测量指标。

Method: 采用自适应随机森林(ARFs)进行动态特征适应，并提出漂移感知特征重要性(DAFI)方法，使用分布漂移检测器来决定何时应用计算密集型特征重要性方法。

Result: 迭代训练ARFs能够实现稳定预测，随着特征增加准确率提高；DAFI在3个数据集上的测试显示，运行时间减少最多2倍，同时产生更一致的特征重要性值。

Conclusion: ARFs和DAFI共同提供了一个有前景的框架，用于构建适应6G网络用例的灵活AI方法。

Abstract: As AI becomes a native component of 6G network control, AI models must adapt
to continuously changing conditions, including the introduction of new features
and measurements driven by multi-vendor deployments, hardware upgrades, and
evolving service requirements. To address this growing need for flexible
learning in non-stationary environments, this vision paper highlights Adaptive
Random Forests (ARFs) as a reliable solution for dynamic feature adaptation in
communication network scenarios. We show that iterative training of ARFs can
effectively lead to stable predictions, with accuracy improving over time as
more features are added. In addition, we highlight the importance of
explainability in AI-driven networks, proposing Drift-Aware Feature Importance
(DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a
distributional drift detector to signal when to apply computationally intensive
FI methods instead of lighter alternatives. Our tests on 3 different datasets
indicate that our approach reduces runtime by up to 2 times, while producing
more consistent feature importance values. Together, ARFs and DAFI provide a
promising framework to build flexible AI methods adapted to 6G network
use-cases.

</details>


### [96] [Robust and Efficient Collaborative Learning](https://arxiv.org/abs/2510.08311)
*Abdellah El Mrini,Sadegh Farhadkhan,Rachid Guerraoui*

Main category: cs.LG

TL;DR: RPEL是一种新颖的分布式机器学习方法，通过基于拉取的流行病传播策略实现可扩展的鲁棒学习，在对抗环境中保持性能且通信成本仅为O(n log n)。


<details>
  <summary>Details</summary>
Motivation: 解决分布式机器学习中对抗行为的容忍问题，现有方法要么依赖中央服务器，要么通信成本过高(O(n^2))，需要一种既去中心化又高效的解决方案。

Method: 采用基于拉取的流行病传播通信策略，节点从小型随机节点子集拉取模型参数，显著降低消息数量，同时保持收敛保证。

Result: RPEL在对抗环境中保持鲁棒性，与全连接通信的准确性相当，并在大规模网络中高效扩展。

Conclusion: RPEL提供了一种可扩展的去中心化解决方案，能够在对抗环境中实现鲁棒学习，且通信效率显著优于传统方法。

Abstract: Collaborative machine learning is challenged by training-time adversarial
behaviors. Existing approaches to tolerate such behaviors either rely on a
central server or induce high communication costs. We propose Robust Pull-based
Epidemic Learning (RPEL), a novel, scalable collaborative approach to ensure
robust learning despite adversaries. RPEL does not rely on any central server
and, unlike traditional methods, where communication costs grow in
$\mathcal{O}(n^2)$ with the number of nodes $n$, RPEL employs a pull-based
epidemic-based communication strategy that scales in $\mathcal{O}(n \log n)$.
By pulling model parameters from small random subsets of nodes, RPEL
significantly lowers the number of required messages without compromising
convergence guarantees, which hold with high probability. Empirical results
demonstrate that RPEL maintains robustness in adversarial settings, competes
with all-to-all communication accuracy, and scales efficiently across large
networks.

</details>


### [97] [To Ask or Not to Ask: Learning to Require Human Feedback](https://arxiv.org/abs/2510.08314)
*Andrea Pugnana,Giovanni De Toni,Cesare Barbera,Roberto Pellungrini,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: 提出了Learning to Ask (LtA)框架，解决传统Learning to Defer (LtD)方法中人类和AI模型互斥决策的限制，允许AI模型在需要时主动寻求专家反馈来提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统LtD方法将人类和AI模型视为互斥决策者，限制了专家只能提供预测结果。为了更有效地整合专家输入，需要开发能够同时处理何时以及如何获取专家反馈的协作框架。

Method: LtA采用两部分架构：标准ML模型和经过专家反馈增强的模型，并设计最优策略来选择何时查询增强模型。提供两种实现：顺序训练方法和联合优化方法，后者设计了具有可实现一致性保证的代理损失函数。

Result: 在合成数据和真实专家数据上的实验表明，LtA为有效的人机协作提供了更灵活和强大的基础，相比传统方法能够更好地利用专家知识。

Conclusion: LtA框架通过允许AI模型主动寻求专家反馈，突破了传统人机协作的限制，为实现更有效的人机协同分类任务提供了新途径。

Abstract: Developing decision-support systems that complement human performance in
classification tasks remains an open challenge. A popular approach, Learning to
Defer (LtD), allows a Machine Learning (ML) model to pass difficult cases to a
human expert. However, LtD treats humans and ML models as mutually exclusive
decision-makers, restricting the expert contribution to mere predictions. To
address this limitation, we propose Learning to Ask (LtA), a new framework that
handles both when and how to incorporate expert input in an ML model. LtA is
based on a two-part architecture: a standard ML model and an enriched model
trained with additional expert human feedback, with a formally optimal strategy
for selecting when to query the enriched model. We provide two practical
implementations of LtA: a sequential approach, which trains the models in
stages, and a joint approach, which optimises them simultaneously. For the
latter, we design surrogate losses with realisable-consistency guarantees. Our
experiments with synthetic and real expert data demonstrate that LtA provides a
more flexible and powerful foundation for effective human-AI collaboration.

</details>


### [98] [Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization](https://arxiv.org/abs/2510.08341)
*Pál Zsámboki,Benjamin Levi,David Ansel Josef Smith,Mitansh Kagalwala,Arlington Kell,Samuel Liechty,Cong Wang*

Main category: cs.LG

TL;DR: 该论文研究了Transformer在集合补集任务中的长度泛化能力，证明了单层注意力Transformer的维度边界，并发现平衡logit位移在短序列上的表现可以泛化到长序列，但精度会下降。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在需要棋盘游戏式推理的集合补集任务中的长度泛化能力，该任务要求模型预测输入序列中缺失的token的均匀分布。

Method: 通过理论分析证明单层注意力Transformer的维度边界，提出如果模型在长度1和2上实现平衡logit位移，就能泛化到更长序列。同时提出dropout和EMA分别应对softmax压缩和训练噪声问题。

Result: 理论证明和实验验证表明，平衡logit位移在短序列上的表现可以泛化到长序列，但精度会随序列增长而下降。随机超参数搜索证实dropout和EMA能改善泛化能力，在更复杂的OthelloGPT设置中EMA也有效。

Conclusion: Transformer在集合补集任务中的长度泛化存在理论限制，但通过dropout和EMA等技术可以缓解softmax压缩和训练噪声问题，提升泛化性能。

Abstract: We study length generalization in transformers through the set complement
task, where a model must predict a uniform distribution over tokens absent from
an input sequence -- an ability central to board-game style reasoning. Our main
theoretical result establishes two statements. First, we prove tight bounds on
embedding and value dimensions for single-layer attention-only transformers.
Second, we show that if such a model achieves balanced logit displacement at
lengths 1 and 2, then it must generalize to longer sequences, though with
reduced precision. A mechanistic reading of the proof explains this limitation:
as more tokens are attended to, softmax compresses logit displacements, eroding
separation between valid and invalid outputs. Training dynamics also suggest a
second obstacle: when many next tokens are possible, updates become noisy. We
hypothesize that dropout can counteract the first effect and Exponential Moving
Average (EMA) the second. We validate these hypotheses through random
hyperparameter search on the set complement task, which confirms both
mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random
Othello moves, and find that EMA again improves length generalization in this
more complex setting.

</details>


### [99] [DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning](https://arxiv.org/abs/2510.08350)
*Daniel Jason Tan,Jiayang Chen,Dilruk Perera,Kay Choong See,Mengling Feng*

Main category: cs.LG

TL;DR: DeepEN是一个基于深度强化学习的个性化肠内营养推荐框架，针对ICU重症患者，通过离线训练生成4小时一次的热量、蛋白质和液体摄入建议。


<details>
  <summary>Details</summary>
Motivation: 传统基于指南或启发式方法的肠内营养治疗存在局限性，需要更个性化的方法来改善重症患者的营养治疗结果。

Method: 使用来自MIMIC-IV数据库的11,000多名ICU患者数据进行离线训练，采用dueling double deep Q-network和conservative Q-learning正则化，结合临床信息的状态空间和定制奖励函数。

Result: DeepEN在各项指标上优于临床医生和指南策略，估计死亡率降低3.7个百分点（18.8% vs 22.5%），关键营养生物标志物得到改善。

Conclusion: 数据驱动的个性化肠内营养治疗有潜力超越传统方法，改善患者预后。

Abstract: We introduce DeepEN, a deep reinforcement learning (RL) framework for
personalized enteral nutrition (EN) in critically ill patients. Trained offline
on over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates
4-hourly recommendations for caloric, protein, and fluid intake tailored to
each patient's evolving physiology. The model integrates a curated, clinically
informed state space with a custom reward function that balances short-term
physiological and nutrition-related goals with long-term survival outcomes.
Using a dueling double deep Q-network with conservative Q-learning
regularization, DeepEN learns clinically realistic policies that align with
high-value clinician actions while discouraging unsafe deviations. Across
various qualitative and quantitative metrics, DeepEN outperforms
clinician-derived and guideline-based policies, achieving a 3.7 $\pm$ 0.17
percentage-point reduction in estimated mortality (18.8% vs 22.5%) and
improvements in key nutritional biomarkers. These findings highlight the
potential of safe, data-driven personalization of EN therapy to improve
outcomes beyond traditional guideline- or heuristic-based approaches.

</details>


### [100] [Guided Star-Shaped Masked Diffusion](https://arxiv.org/abs/2510.08369)
*Viacheslav Meshchaninov,Egor Shibaev,Artem Makoian,Ivan Klimov,Danil Sheshenya,Andrei Malinin,Nikita Balagansky,Daniil Gavrilov,Aibek Alanov,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 提出了一种新的采样算法，通过轻量级微调单个层并采用星形生成范式，显著提升预训练掩码扩散模型的采样质量和效率，特别是在低步数生成场景下。


<details>
  <summary>Details</summary>
Motivation: 预训练掩码扩散模型的性能受限于其采样过程，该过程决策不可逆且在低步数生成场景下表现不佳。

Method: 引入星形生成范式，结合可学习的重掩码调度器，智能识别和修正潜在错误，实现有效的错误纠正。

Result: 在文本和代码生成任务上的综合实验表明，该采样算法优于或匹配现有方法，尤其在少量采样步数下带来显著质量提升。

Conclusion: 该方法通过轻量级微调和智能错误修正机制，有效解决了掩码扩散模型在低步数采样时的性能限制问题。

Abstract: The performance of pre-trained masked diffusion models is often constrained
by their sampling procedure, which makes decisions irreversible and struggles
in low-step generation regimes. We introduce a novel sampling algorithm that
works with pre-trained models and, after a lightweight fine-tuning of a single
layer, significantly improves sample quality and efficiency. Our method
reformulates the generation process using a star-shaped paradigm, which
inherently allows for error correction. To make this process effective, we
augment it with a learnable re-masking scheduler that intelligently identifies
and revises likely errors. This approach yields a substantial quality boost,
particularly when using a small number of sampling steps. We extensively ablate
key components of our approach and show its usability in different scenarios.
In comprehensive experiments on text, and code generation, our sampling
algorithm outperforms or matches existing methods.

</details>


### [101] [Contrastive Self-Supervised Learning at the Edge: An Energy Perspective](https://arxiv.org/abs/2510.08374)
*Fernanda Famá,Roberto Pereira,Charalampos Kalalas,Paolo Dini,Lorena Qendro,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: 评估四种对比学习框架在边缘/雾计算环境中的部署可行性，发现SimCLR在能耗方面表现最佳，并分析了轻量级神经网络架构与对比学习的结合效果。


<details>
  <summary>Details</summary>
Motivation: 对比学习在自监督表示学习中显示潜力，但在资源受限设备上的部署研究不足，面临能耗、数据可用性和内存使用等挑战。

Method: 系统评估SimCLR、MoCo、SimSiam和Barlow Twins四种对比学习框架，采用包含能耗分析和减少训练数据条件的基准测试策略，并评估轻量级神经网络架构与对比学习的配对效果。

Result: SimCLR尽管被认为计算成本高，但在各种数据条件下表现出最低的能耗。

Conclusion: 研究为在有限处理能力的边缘/雾环境中部署对比学习提供了资源影响洞察，并为其未来优化开辟了多个研究方向。

Abstract: While contrastive learning (CL) shows considerable promise in self-supervised
representation learning, its deployment on resource-constrained devices remains
largely underexplored. The substantial computational demands required for
training conventional CL frameworks pose a set of challenges, particularly in
terms of energy consumption, data availability, and memory usage. We conduct an
evaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and Barlow
Twins. We focus on the practical feasibility of these CL frameworks for edge
and fog deployment, and introduce a systematic benchmarking strategy that
includes energy profiling and reduced training data conditions. Our findings
reveal that SimCLR, contrary to its perceived computational cost, demonstrates
the lowest energy consumption across various data regimes. Finally, we also
extend our analysis by evaluating lightweight neural architectures when paired
with CL frameworks. Our study aims to provide insights into the resource
implications of deploying CL in edge/fog environments with limited processing
capabilities and opens several research directions for its future optimization.

</details>


### [102] [Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions](https://arxiv.org/abs/2510.08382)
*Jacob Trauger,Tyson Trauger,Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文提出了广义Natarajan维度来表征有限标签多类设置中可原谅0-1损失函数的学习性，并建立了与集合值反馈学习的联系。


<details>
  <summary>Details</summary>
Motivation: 研究有限标签多类设置中可原谅0-1损失函数的学习性特征，扩展Natarajan维度的概念来解决这一学习问题。

Method: 创建基于Natarajan维度的新组合维度——广义Natarajan维度，并证明假设类在该设置下可学习当且仅当该维度有限。

Result: 证明了假设类在可原谅0-1损失函数设置下可学习当且仅当广义Natarajan维度有限，并建立了集合学习问题可学习性与Natarajan维度的联系。

Conclusion: 广义Natarajan维度成功地表征了有限标签多类设置中可原谅0-1损失函数的学习性，且集合学习问题的可学习性由Natarajan维度决定。

Abstract: In this paper we will give a characterization of the learnability of
forgiving 0-1 loss functions in the finite label multiclass setting. To do
this, we create a new combinatorial dimension that is based off of the
Natarajan Dimension \citep{natarajan1989learning} and we show that a hypothesis
class is learnable in our setting if and only if this Generalized Natarajan
Dimension is finite. We also show a connection to learning with set-valued
feedback. Through our results we show that the learnability of a set learning
problem is characterized by the Natarajan Dimension.

</details>


### [103] [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
*Heming Zou,Yunliang Zang,Wutong Xu,Yao Zhu,Xiangyang Ji*

Main category: cs.LG

TL;DR: FlyLoRA是一种基于果蝇嗅觉回路的隐式MoE-LoRA变体，通过秩级专家激活和隐式路由器解决LoRA的参数干扰问题，在多个领域实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法存在参数干扰问题，导致性能不佳。MoE-based LoRA变体虽然能缓解单任务内的相关性，但引入了额外路由器参数，在多任务模型合并中仍然存在任务间干扰。

Method: 提出FlyLoRA方法：(1)在up-projection矩阵中引入秩级专家激活；(2)使用隐式路由器统一专家路由和down-projection，用冻结的稀疏随机投影矩阵替代传统的密集可训练版本。

Result: 在四个领域（通用知识理解、科学问答、数学推理、代码生成）的广泛实验表明，相比现有方法实现了持续的性能提升。

Conclusion: FlyLoRA通过消除显式路由器的需求解决了任务内去相关性和计算效率之间的权衡，同时由于随机矩阵的正交性特性，固有地缓解了任务间干扰。该方法展示了生物结构如何启发AI技术创新。

Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method for foundation models, but it suffers from parameter interference,
resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based
LoRA variants show promise in mitigating intra-task correlations in single-task
instruction tuning, they introduce additional router parameters and remain
ineffective in multi-task model merging where inter-task interference arises.
Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit
MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the
up-projection matrix, and (2) an implicit router that unifies expert routing
and down-projection, where a frozen sparse random projection matrix replaces
the traditional dense trainable version. This design resolves the trade-off
between intra-task decorrelation and computational efficiency by eliminating
the need for an explicit router, while inherently mitigating inter-task
interference due to the orthogonality property of random matrices. Extensive
experiments across four domains -- general knowledge understanding, scientific
question answering, mathematical reasoning, and code generation -- demonstrate
consistent performance improvements over existing methods. Beyond empirical
gains, FlyLoRA highlights how biological structures can inspire innovations in
AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.

</details>


### [104] [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
*Lauren Anderson,Lucas Chatelain,Nicolas Tremblay,Kathryn Grandfield,David Rousseau,Aurélien Gourrier*

Main category: cs.LG

TL;DR: 本研究测试了多种深度学习超分辨率模型，用于提高牙齿孔隙网络成像的效率和质量，发现传统图像质量评估指标与视觉感知不一致，提出了基于生物学结构特征的新评估方法。


<details>
  <summary>Details</summary>
Motivation: 牙齿机械感觉系统依赖于牙本质孔隙网络中的流体流动，但现有共聚焦显微镜的高分辨率成像视野有限，需要更高效的成像方法。

Method: 使用四种深度学习超分辨率模型（RCAN、pix2pix、FSRCNN、CycleGAN）处理配对的高低分辨率共聚焦图像，通过图像分割、连通性分析和图分析评估模型性能。

Result: 传统图像质量评估指标与视觉感知不一致，基于生物学结构的评估方法能更好解释模型性能差异，揭示了模型对弱强度特征的敏感性和图像生成非线性的影响。

Conclusion: 针对特定生物结构特征设计的评估方法比通用图像质量指标更适合评估超分辨率模型在牙齿孔隙成像中的表现。

Abstract: The mechanosensory system of teeth is currently believed to partly rely on
Odontoblast cells stimulation by fluid flow through a porosity network
extending through dentin. Visualizing the smallest sub-microscopic porosity
vessels therefore requires the highest achievable resolution from confocal
fluorescence microscopy, the current gold standard. This considerably limits
the extent of the field of view to very small sample regions. To overcome this
limitation, we tested different deep learning (DL) super-resolution (SR) models
to allow faster experimental acquisitions of lower resolution images and
restore optimal image quality by post-processing. Three supervised 2D SR models
(RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a
unique set of experimentally paired high- and low-resolution confocal images
acquired with different sampling schemes, resulting in a pixel size increase of
x2, x4, x8. Model performance was quantified using a broad set of similarity
and distribution-based image quality assessment (IQA) metrics, which yielded
inconsistent results that mostly contradicted our visual perception. This
raises the question of the relevance of such generic metrics to efficiently
target the specific structure of dental porosity. To resolve this conflicting
information, the generated SR images were segmented taking into account the
specific scales and morphology of the porosity network and analysed by
comparing connected components. Additionally, the capacity of the SR models to
preserve 3D porosity connectivity throughout the confocal image stacks was
evaluated using graph analysis. This biology-driven assessment allowed a far
better mechanistic interpretation of SR performance, highlighting differences
in model sensitivity to weak intensity features and the impact of non-linearity
in image generation, which explains the failure of standard IQA metrics.

</details>


### [105] [Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors](https://arxiv.org/abs/2510.08413)
*David Madras,Joshua Safyan,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: 本文通过引入基于困惑度的正则化来改进提示优化的泛化边界，在数据稀缺情况下提供非空泛的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法在实践中很成功，但理论分析主要适用于数据丰富场景，缺乏对数据稀缺情况下成功机制的解释。

Method: 提出基于困惑度的数据依赖先验，推导新的泛化边界，并通过困惑度正则化限制探索空间来收紧边界。

Result: 理论分析显示困惑度正则化能有效收紧泛化边界，实证研究验证了困惑度正则化在提升提示泛化性能方面的实际效益。

Conclusion: 困惑度作为有效先验能更好地解释提示工程的成功，特别是在数据稀缺情况下，困惑度正则化是提升提示泛化性能的关键机制。

Abstract: Many prompt engineering techniques have been successful in practice, even
when optimizing over a large prompt space with with a small amount of
task-specific data. Recent work has partially explained this success by showing
generalization bounds which apply PAC-Bayes theory to the discrete prompt
space, but they are non-vacuous only in data-rich scenarios. We argue that such
widespread success can be more fully explained through more carefully
considering data- or distribution-dependent perplexity, which acts as an
effective prior and steers the optimization towards prompts that are more
``natural'' for the task at hand. We derive novel generalization bounds that
are non-vacuous for data-scarce prompt optimization via more useful priors,
formally analyzing how perplexity regularization tightens these bounds by
limiting exploration. Empirically, we explore both the bounds' effectiveness
and the practical benefits of perplexity regularization in improving prompt
generalization.

</details>


### [106] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 提出了DGPO算法，直接利用组级偏好信息进行强化学习，无需依赖低效的随机策略，可使用高效的确定性ODE采样器，训练速度提升20倍且性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO等方法在扩散模型中应用困难，因为需要随机策略而高效采样器是确定性的，使用SDE采样器会导致收敛缓慢。

Method: 提出DGPO在线RL算法，完全摆脱策略梯度框架，直接从组级偏好学习，利用组内样本的相对信息，无需随机策略。

Result: DGPO训练速度比现有最优方法快约20倍，在领域内和领域外奖励指标上均表现更优。

Conclusion: DGPO通过直接组级偏好优化解决了扩散模型RL中的效率瓶颈，实现了快速训练和优越性能。

Abstract: While reinforcement learning methods such as Group Relative Preference
Optimization (GRPO) have significantly enhanced Large Language Models, adapting
them to diffusion models remains challenging. In particular, GRPO demands a
stochastic policy, yet the most cost-effective diffusion samplers are based on
deterministic ODEs. Recent work addresses this issue by using inefficient
SDE-based samplers to induce stochasticity, but this reliance on model-agnostic
Gaussian noise leads to slow convergence. To resolve this conflict, we propose
Direct Group Preference Optimization (DGPO), a new online RL algorithm that
dispenses with the policy-gradient framework entirely. DGPO learns directly
from group-level preferences, which utilize relative information of samples
within groups. This design eliminates the need for inefficient stochastic
policies, unlocking the use of efficient deterministic ODE samplers and faster
training. Extensive results show that DGPO trains around 20 times faster than
existing state-of-the-art methods and achieves superior performance on both
in-domain and out-of-domain reward metrics. Code is available at
https://github.com/Luo-Yihong/DGPO.

</details>


### [107] [ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing](https://arxiv.org/abs/2510.08429)
*Stella C. Dong,James R. Finlay*

Main category: cs.LG

TL;DR: ClauseLens是一个基于条款的强化学习框架，用于生成透明、合规且风险感知的再保险条约报价，显著减少违规并提高尾部风险管理。


<details>
  <summary>Details</summary>
Motivation: 当前再保险条约定价实践不透明且难以审计，需要满足严格的监管标准。

Method: 将报价任务建模为风险感知约束马尔可夫决策过程(RA-CMDP)，从法律和承保语料库中检索法规和政策条款，嵌入到智能体观察中，用于约束可行行动并生成基于条款的自然语言解释。

Result: 在多代理条约模拟器中评估，ClauseLens将偿付能力违规减少51%，尾部风险表现提高27.9%(CVaR_0.10)，基于条款的解释准确率达88.2%，检索精度87.4%，召回率91.1%。

Conclusion: 将法律背景嵌入决策和解释路径可以产生可解释、可审计且符合监管要求的报价行为，与Solvency II、NAIC RBC和欧盟AI法案保持一致。

Abstract: Reinsurance treaty pricing must satisfy stringent regulatory standards, yet
current quoting practices remain opaque and difficult to audit. We introduce
ClauseLens, a clause-grounded reinforcement learning framework that produces
transparent, regulation-compliant, and risk-aware treaty quotes.
  ClauseLens models the quoting task as a Risk-Aware Constrained Markov
Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from
legal and underwriting corpora, embedded into the agent's observations, and
used both to constrain feasible actions and to generate clause-grounded natural
language justifications.
  Evaluated in a multi-agent treaty simulator calibrated to industry data,
ClauseLens reduces solvency violations by 51%, improves tail-risk performance
by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded
explanations with retrieval precision of 87.4% and recall of 91.1%.
  These findings demonstrate that embedding legal context into both decision
and explanation pathways yields interpretable, auditable, and
regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and
the EU AI Act.

</details>


### [108] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter是一个基于工具调用的路由系统，通过强化学习训练智能路由器，在成本与性能之间实现最佳平衡，无需手动设计路由规则。


<details>
  <summary>Details</summary>
Motivation: 现代LLM部署面临成本与性能的权衡：优质模型性能强但昂贵，轻量级模型经济但处理复杂任务能力弱。静态升级规则和关键词启发式方法无法充分利用这一频谱且难以跨任务类型适应。

Method: 使用强化学习端到端训练路由器，采用明确的成本感知奖励函数编码成本-性能权衡，路由器可以直接回答或调用外部模型。

Result: 在多样化基准测试中，xRouter实现了强大的成本-性能权衡（如以可比较的任务完成率显著降低成本），并提供了关于学习路由可靠帮助因素的实证见解。

Conclusion: xRouter的发现和开源实现为推进学习型、成本感知的LLM编排提供了实用基础。

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


### [109] [Synthetic Series-Symbol Data Generation for Time Series Foundation Models](https://arxiv.org/abs/2510.08445)
*Wenxuan Wang,Kai Wu,Yujian Betterest Li,Dan Wang,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 提出了SymTime基础模型，通过系列-符号数据生成机制解决时间序列分析中的数据稀缺问题，在五大任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 时间序列分析基础模型面临训练数据稀缺和不平衡的挑战，需要新的数据生成方法来克服这些限制

Method: 基于复杂动态系统理论设计系列-符号数据生成机制，开发SymTime预训练基础模型，利用符号信息增强时间序列表示

Result: SymTime在五大时间序列分析任务中表现出色，性能可与基于真实数据集预训练的基础模型相媲美

Conclusion: 系列-符号数据生成和预训练机制具有克服数据稀缺和提升任务性能的潜力

Abstract: Foundation models for time series analysis (TSA) have attracted significant
attention. However, challenges such as training data scarcity and imbalance
continue to hinder their development. Inspired by complex dynamic system
theories, we design a series-symbol data generation mechanism, enabling the
unrestricted creation of high-quality time series data paired with
corresponding symbolic expressions. To leverage series-symbol data pairs with
strong correlations, we develop \texttt{SymTime}, a pre-trained foundation
model for enhancing time series representation using symbolic information.
\texttt{SymTime} demonstrates competitive performance across five major TSA
tasks when fine-tunes with downstream tasks, rivaling foundation models
pre-trained on real-world datasets. This approach underscores the potential of
series-symbol data generation and pretraining mechanisms in overcoming data
scarcity and enhancing task performance. The code is available at
https://github.com/wwhenxuan/SymTime.

</details>


### [110] [gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity](https://arxiv.org/abs/2510.08450)
*Hugh Blayney,Álvaro Arroyo,Xiaowen Dong,Michael M. Bronstein*

Main category: cs.LG

TL;DR: 本文重新审视了图神经网络中的过度压缩问题，从模型存储和检索容量的角度分析，并提出了新的架构来改善信息瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在消息传递机制中容易出现过度压缩问题，即大量节点表示信息被压缩到固定大小的向量中，形成信息瓶颈。

Method: 通过分析现有测量方法的局限性，引入新的合成任务来验证容量饱和现象，并借鉴序列建模中的关联记忆、快速权重编程和xLSTM思想，开发了具有改进容量的新型GNN架构。

Result: 新架构在容量合成任务和多个真实世界图基准测试中表现出强大的性能。

Conclusion: 从存储和检索容量的角度理解过度压缩问题，并提出有效解决方案，能够显著提升GNN的性能表现。

Abstract: Graph Neural Networks (GNNs) leverage the graph structure to transmit
information between nodes, typically through the message-passing mechanism.
While these models have found a wide variety of applications, they are known to
suffer from over-squashing, where information from a large receptive field of
node representations is collapsed into a single fixed sized vector, resulting
in an information bottleneck. In this paper, we re-examine the over-squashing
phenomenon through the lens of model storage and retrieval capacity, which we
define as the amount of information that can be stored in a node's
representation for later use. We study some of the limitations of existing
tasks used to measure over-squashing and introduce a new synthetic task to
demonstrate that an information bottleneck can saturate this capacity.
Furthermore, we adapt ideas from the sequence modeling literature on
associative memories, fast weight programmers, and the xLSTM model to develop a
novel GNN architecture with improved capacity. We demonstrate strong
performance of this architecture both on our capacity synthetic task, as well
as a range of real-world graph benchmarks.

</details>


### [111] [Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning](https://arxiv.org/abs/2510.08456)
*Ankur Mali,Lawrence Hall,Jake Williams,Gordon Richards*

Main category: cs.LG

TL;DR: 提出了一个九维积分签名框架来严格分类激活函数，结合高斯传播统计、渐近斜率和正则性度量，为激活函数选择提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有的激活函数比较大多基于启发式方法，缺乏严格的分类框架来指导神经网络的设计和稳定性分析。

Method: 构建九维积分签名S_sigma(phi)，包含高斯传播统计(m1,g1,g2,m2,eta)、渐近斜率(alpha_plus,alpha_minus)和正则性度量(TV(phi'),C(phi))，建立重参数化定律和闭包性质。

Result: 对八种标准激活函数进行了分类，证明了饱和、线性增长和平滑激活函数之间的明显区别，数值验证支持理论预测。

Conclusion: 该框架将激活函数选择从试错转向可证明的稳定性和核条件，为神经网络设计提供原则性指导。

Abstract: Activation functions govern the expressivity and stability of neural
networks, yet existing comparisons remain largely heuristic. We propose a
rigorous framework for their classification via a nine-dimensional integral
signature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2,
m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures
(TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine
reparameterization laws with bias, and closure under bounded slope variation.
Dynamical analysis yields Lyapunov theorems with explicit descent constants and
identifies variance stability regions through (m2', g2). From a kernel
perspective, we derive dimension-free Hessian bounds and connect smoothness to
bounded variation of phi'. Applying the framework, we classify eight standard
activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving
sharp distinctions between saturating, linear-growth, and smooth families.
Numerical Gauss-Hermite and Monte Carlo validation confirms theoretical
predictions. Our framework provides principled design guidance, moving
activation choice from trial-and-error to provable stability and kernel
conditioning.

</details>


### [112] [SummDiff: Generative Modeling of Video Summarization with Diffusion](https://arxiv.org/abs/2510.08458)
*Kwanseok Kim,Jaehoon Hahm,Sumin Kim,Jinhwan Sul,Byunghak Kim,Joonseok Lee*

Main category: cs.LG

TL;DR: 提出SummDiff方法，将视频摘要重新定义为条件生成任务，使用扩散模型生成多个可能的摘要，更好地反映人类主观性。


<details>
  <summary>Details</summary>
Motivation: 传统视频摘要方法忽略任务的主观性，仅回归到多个标注者的平均帧分数，无法反映不同人类视角的多样性。

Method: 采用扩散模型，将视频摘要作为条件生成任务，模型学习良好摘要的分布并生成多个合理摘要，动态适应视觉上下文。

Result: 在多个基准测试中达到最先进性能，生成的摘要更符合个体标注者偏好，并通过背包问题分析提供新的评估指标。

Conclusion: SummDiff通过扩散模型成功捕捉视频摘要的主观性，生成多样化且高质量的摘要，为评估提供了新视角。

Abstract: Video summarization is a task of shortening a video by choosing a subset of
frames while preserving its essential moments. Despite the innate subjectivity
of the task, previous works have deterministically regressed to an averaged
frame score over multiple raters, ignoring the inherent subjectivity of what
constitutes a good summary. We propose a novel problem formulation by framing
video summarization as a conditional generation task, allowing a model to learn
the distribution of good summaries and to generate multiple plausible summaries
that better reflect varying human perspectives. Adopting diffusion models for
the first time in video summarization, our proposed method, SummDiff,
dynamically adapts to visual contexts and generates multiple candidate
summaries conditioned on the input video. Extensive experiments demonstrate
that SummDiff not only achieves the state-of-the-art performance on various
benchmarks but also produces summaries that closely align with individual
annotator preferences. Moreover, we provide a deeper insight with novel metrics
from an analysis of the knapsack, which is an important last step of generating
summaries but has been overlooked in evaluation.

</details>


### [113] [In-Context Clustering with Large Language Models](https://arxiv.org/abs/2510.08466)
*Ying Wang,Mengye Ren,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出了In-Context Clustering (ICC)，一种基于LLM的灵活聚类方法，能够处理来自不同分布的数据，通过注意力机制捕获复杂关系，并在文本编码的数字数据和图像数据上展示了强大的聚类能力。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法受限于预定义的相似性度量，无法灵活捕获输入之间的复杂关系。本文旨在利用LLM的注意力机制和上下文学习能力，扩展聚类方法的灵活性。

Method: 使用预训练LLM的注意力矩阵进行谱聚类，并通过Next Token Prediction (NTP)损失对数字和图像数据进行微调，支持文本条件图像聚类。

Result: LLM在文本编码数字数据上表现出令人印象深刻的零样本聚类能力，注意力矩阵显示显著的聚类模式。基于注意力矩阵的谱聚类性能具有竞争力。

Conclusion: 该工作将上下文学习扩展到无监督设置，展示了LLM在聚类任务中的有效性和灵活性，为传统聚类方法提供了新的替代方案。

Abstract: We propose In-Context Clustering (ICC), a flexible LLM-based procedure for
clustering data from diverse distributions. Unlike traditional clustering
algorithms constrained by predefined similarity measures, ICC flexibly captures
complex relationships among inputs through an attention mechanism. We show that
pretrained LLMs exhibit impressive zero-shot clustering capabilities on
text-encoded numeric data, with attention matrices showing salient cluster
patterns. Spectral clustering using attention matrices offers surprisingly
competitive performance. We further enhance the clustering capabilities of LLMs
on numeric and image data through fine-tuning using the Next Token Prediction
(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned
image clustering, a capability that classical clustering methods lack. Our work
extends in-context learning to an unsupervised setting, showcasing the
effectiveness and flexibility of LLMs for clustering. Our code is available at
https://agenticlearning.ai/icc.

</details>


### [114] [Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models](https://arxiv.org/abs/2510.08492)
*Sharut Gupta,Shobhita Sundaram,Chenyu Wang,Stefanie Jegelka,Phillip Isola*

Main category: cs.LG

TL;DR: UML：一种模态无关的训练范式，通过交替处理不同模态的输入并共享参数，利用未配对的辅助多模态数据增强目标模态的表征学习


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习依赖配对数据集，但能否利用未配对的辅助多模态数据直接增强目标模态的表征学习是一个被忽视但具有潜力的问题

Method: 提出UML（Unpaired Multimodal Learner），采用模态无关的训练范式，单一模型交替处理不同模态输入并跨模态共享参数，利用不同模态是共享底层现实投影的假设

Result: 理论上在线性数据生成假设下，未配对辅助数据能产生比单模态训练更丰富的信息表征；实证表明使用文本、音频或图像等未配对辅助模态数据能持续提升图像和音频等单模态目标的下游性能

Conclusion: UML范式证明未配对多模态数据可以有效增强单模态表征学习，为利用丰富但未配对的跨模态数据提供了新途径

Abstract: Traditional multimodal learners find unified representations for tasks like
visual question answering, but rely heavily on paired datasets. However, an
overlooked yet potentially powerful question is: can one leverage auxiliary
unpaired multimodal data to directly enhance representation learning in a
target modality? We introduce UML: Unpaired Multimodal Learner, a
modality-agnostic training paradigm in which a single model alternately
processes inputs from different modalities while sharing parameters across
them. This design exploits the assumption that different modalities are
projections of a shared underlying reality, allowing the model to benefit from
cross-modal structure without requiring explicit pairs. Theoretically, under
linear data-generating assumptions, we show that unpaired auxiliary data can
yield representations strictly more informative about the data-generating
process than unimodal training. Empirically, we show that using unpaired data
from auxiliary modalities -- such as text, audio, or images -- consistently
improves downstream performance across diverse unimodal targets such as image
and audio. Our project page: https://unpaired-multimodal.github.io/

</details>


### [115] [Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning](https://arxiv.org/abs/2510.08526)
*Yash Jhaveri,Harley Wiltzer,Patrick Shafto,Marc G. Bellemare,David Meger*

Main category: cs.LG

TL;DR: 提出了一个理论框架，通过熵正则化和温度解耦策略，保证收敛到特定的最优策略，实现可解释且保持多样性的最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法只关注期望回报，难以描述学习到的策略特性。需要一种能保证收敛到特定最优策略并保持策略多样性的方法。

Method: 使用熵正则化和温度解耦策略，在正则化温度趋近于零时实现可解释且保持多样性的最优策略。

Result: 该方法能保证收敛到特定最优策略，实现可解释且保持多样性的策略，并能准确估计相关回报分布。

Conclusion: 提出的框架为强化学习提供了理论保证，能收敛到可解释且保持多样性的最优策略，解决了传统方法难以描述策略特性的问题。

Abstract: In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.

</details>


### [116] [On the optimization dynamics of RLVR: Gradient gap and step size thresholds](https://arxiv.org/abs/2510.08539)
*Joe Suk,Yaqi Duan*

Main category: cs.LG

TL;DR: 本文为RLVR（带可验证奖励的强化学习）建立了理论框架，分析了其训练过程，提出了梯度间隙概念，并推导了收敛的关键条件和步长阈值。


<details>
  <summary>Details</summary>
Motivation: RLVR使用简单的二元反馈来训练大语言模型，在实证中表现出色，但缺乏理论理解。本文旨在从原理上解释RLVR为何有效。

Method: 在完整响应和token级别分析RLVR训练过程，引入梯度间隙概念，通过理论推导和实验验证（包括带控制的赌博机模拟和Qwen2.5-7B训练）。

Result: 证明了收敛取决于更新方向与梯度间隙的对齐，推导了基于梯度间隙大小的尖锐步长阈值，预测了关键步长如何随响应长度和成功率缩放。

Conclusion: 理论解释了为什么长度归一化等实用启发式方法能提高稳定性，并表明固定学习率下成功率可能停滞在100%以下。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple
binary feedback to post-train large language models, has shown significant
empirical success. However, a principled understanding of why it works has been
lacking. This paper builds a theoretical foundation for RLVR by analyzing its
training process at both the full-response (trajectory) and token levels.
Central to our analysis is a quantity called the Gradient Gap, which formalizes
the direction of improvement from low-reward to high-reward regions of the
response space. We prove that convergence critically depends on aligning the
update direction with this Gradient Gap. Moreover, we derive a sharp step-size
threshold based on the magnitude of the Gradient Gap: below it, learning
converges, whereas above it, performance collapses. Our theory further predicts
how the critical step size must scale with response length and the success
rate, thereby explaining why practical heuristics such as length normalization
improve stability and showing that, with a fixed learning rate, the success
rate can stagnate strictly below $100\%$. We validate these predictions through
controlled bandit simulations and LLM experiments, including training
Qwen2.5-7B with GRPO.

</details>


### [117] [Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints](https://arxiv.org/abs/2510.08549)
*Zilin Kang,Chonghua Liao,Tingqiang Xu,Huazhe Xu*

Main category: cs.LG

TL;DR: ERA是一种新范式，通过应用特殊设计的激活函数来约束模型输出的采样熵，在LLM、强化学习和图像分类等多个领域显著提升性能，计算开销低于7%。


<details>
  <summary>Details</summary>
Motivation: 探索通过控制模型输出熵来提升各种机器学习任务性能的新方法，验证输出激活作为熵控制工具的有效性。

Method: 设计特殊激活函数应用于模型输出，约束采样熵不低于给定阈值，实现熵控制。

Result: 1) Qwen2.5-Math-7B在AIME 2025上提升37.4%；2) 强化学习在HumanoidBench上比SAC提升30%以上；3) ResNet-50在ImageNet上提升0.69% top-1准确率。

Conclusion: 输出激活是强大的熵控制工具，为设计更简单鲁棒的算法开辟了新方向。

Abstract: We propose ERA, a new paradigm that constrains the sampling entropy above
given thresholds by applying specially designed activations to the outputs of
models. Our approach demonstrates broad effectiveness across different domains:
1) for large language models(LLMs), boosting the AIME 2025 score for
Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning
agents, improving performance by more than 30% over strong baselines such as
SAC on the challenging HumanoidBench; 3) for image classification, enhancing
ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a
computational overhead of less than 7%. Our work validates output activation as
a powerful tool for entropy control, opening a new direction for designing
simpler and more robust algorithms.

</details>


### [118] [Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization](https://arxiv.org/abs/2510.08554)
*Kevin Rojas,Jiahe Lin,Kashif Rasul,Anderson Schneider,Yuriy Nevmyvaka,Molei Tao,Wei Deng*

Main category: cs.LG

TL;DR: 提出了GDPO算法，通过半确定性蒙特卡洛方案降低ELBO估计器的方差，为扩散语言模型提供有效的强化学习微调方法。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有并行生成和迭代优化的优势，但现有RL微调方法存在偏差或计算成本高的问题，需要更有效的优化方案。

Method: 基于ELBO分解分析方差来源，提出GDPO算法，使用半确定性蒙特卡洛方案减少ELBO估计器的方差，实现高效的RL微调。

Result: GDPO在数学、推理和编程基准测试中优于预训练模型和现有最佳基线diffu-GRPO，表现出更好的性能。

Conclusion: GDPO为扩散语言模型的RL微调提供了有效的低方差估计器，在多个任务上取得了显著改进。

Abstract: Diffusion language models (DLMs) enable parallel, order-agnostic generation
with iterative refinement, offering a flexible alternative to autoregressive
large language models (LLMs). However, adapting reinforcement learning (RL)
fine-tuning to DLMs remains an open challenge because of the intractable
likelihood. Pioneering work such as diffu-GRPO estimated token-level
likelihoods via one-step unmasking. While computationally efficient, this
approach is severely biased. A more principled foundation lies in
sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a
surrogate. Yet, despite this clean mathematical connection, ELBO-based methods
have seen limited adoption due to the prohibitive cost of likelihood
evaluation. In this work, we revisit ELBO estimation and disentangle its
sources of variance. This decomposition motivates reducing variance through
fast, deterministic integral approximations along a few pivotal dimensions.
Building on this insight, we introduce \textbf{Group Diffusion Policy
Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages
simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the
variance explosion of ELBO estimators under vanilla double Monte Carlo
sampling, yielding a provably lower-variance estimator under tight evaluation
budgets. Empirically, GDPO achieves consistent gains over pretrained
checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,
on the majority of math, reasoning, and coding benchmarks.

</details>


### [119] [Who Said Neural Networks Aren't Linear?](https://arxiv.org/abs/2510.08570)
*Nimrod Berman,Assaf Hallak,Assaf Shocher*

Main category: cs.LG

TL;DR: 该论文提出了一种称为Linearizer的架构，通过将线性算子夹在两个可逆神经网络之间，使得非线性函数在特定向量空间中表现为线性，从而将线性代数工具应用于非线性映射。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的非线性特性，探索是否可以通过构建特殊的向量空间，使传统非线性函数在其中表现为线性，从而应用线性代数的强大工具。

Method: 提出Linearizer架构：f(x)=g_y^{-1}(A g_x(x))，其中A是线性算子，g_x和g_y是可逆神经网络。通过这种方式诱导出新的向量空间，其中非线性映射变为线性。

Result: 成功将SVD、伪逆、正交投影等线性代数工具应用于非线性映射；证明了共享神经网络的Linearizer组合仍为Linearizer；在扩散模型中实现数百步采样压缩为单步；构建了全局投影生成模型和模块化风格迁移。

Conclusion: Linearizer框架为非线性映射提供了线性代数工具的应用途径，在扩散模型采样、投影生成模型和风格迁移等任务中展现出显著优势。

Abstract: Neural networks are famously nonlinear. However, linearity is defined
relative to a pair of vector spaces, $f$$:$$X$$\to$$Y$. Is it possible to
identify a pair of non-standard vector spaces for which a conventionally
nonlinear function is, in fact, linear? This paper introduces a method that
makes such vector spaces explicit by construction. We find that if we sandwich
a linear operator $A$ between two invertible neural networks, $f(x)=g_y^{-1}(A
g_x(x))$, then the corresponding vector spaces $X$ and $Y$ are induced by newly
defined addition and scaling actions derived from $g_x$ and $g_y$. We term this
kind of architecture a Linearizer. This framework makes the entire arsenal of
linear algebra, including SVD, pseudo-inverse, orthogonal projection and more,
applicable to nonlinear mappings. Furthermore, we show that the composition of
two Linearizers that share a neural network is also a Linearizer. We leverage
this property and demonstrate that training diffusion models using our
architecture makes the hundreds of sampling steps collapse into a single step.
We further utilize our framework to enforce idempotency (i.e. $f(f(x))=f(x)$)
on networks leading to a globally projective generative model and to
demonstrate modular style transfer.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [120] [SEPhIA: <1 laser/neuron Spiking Electro-Photonic Integrated Multi-Tiled Architecture for Scalable Optical Neuromorphic Computing](https://arxiv.org/abs/2510.07427)
*Matěj Hejda,Aishwarya Natarajan,Chaerin Hong,Mehmet Berkay On,Sébastien d'Herbais de Thun,Raymond G. Beausoleil,Thomas Van Vaerenbergh*

Main category: cs.ET

TL;DR: SEPhIA是一种光子-电子混合的多瓦片SNN架构，通过微环谐振调制器和多波长光源实现每个脉冲神经元低于一个激光器的效率，在四类脉冲编码数据集上达到超过90%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有光学SNN研究主要关注脉冲器件、可激发激光器网络或大型架构的数值模拟，但往往忽略了光功率限制、串扰和尺寸等关键约束条件。

Method: 利用微环谐振调制器和多波长光源，通过时域协同模拟可激发CMOS-MRR耦合电路，并开发物理感知的可训练光电SNN模型，使用实验获得的器件参数。

Result: 多层光电SNN在四类脉冲编码数据集上实现超过90%的分类准确率，与软件模型性能相当；设计空间研究量化了光子器件参数在受限信噪比条件下对SNN性能的影响。

Conclusion: SEPhIA为神经形态光子计算提供了一个可扩展、表达能力强且物理基础扎实的解决方案，能够处理脉冲编码任务。

Abstract: Research into optical spiking neural networks (SNNs) has primarily focused on
spiking devices, networks of excitable lasers or numerical modelling of large
architectures, often overlooking key constraints such as limited optical power,
crosstalk and footprint. We introduce SEPhIA, a photonic-electronic,
multi-tiled SNN architecture emphasizing implementation feasibility and
realistic scaling. SEPhIA leverages microring resonator modulators (MRMs) and
multi-wavelength sources to achieve effective sub-one-laser-per-spiking neuron
efficiency. We validate SEPhIA at both device and architecture levels by
time-domain co-simulating excitable CMOS-MRR coupled circuits and by devising a
physics-aware, trainable optoelectronic SNN model, with both approaches
utilizing experimentally derived device parameters. The multi-layer
optoelectronic SNN achieves classification accuracies over 90% on a four-class
spike-encoded dataset, closely comparable to software models. A design space
study further quantifies how photonic device parameters impact SNN performance
under constrained signal-to-noise conditions. SEPhIA offers a scalable,
expressive, physically grounded solution for neuromorphic photonic computing,
capable of addressing spike-encoded tasks.

</details>


### [121] [A Distributed Emulation Environment for In-Memory Computing Systems](https://arxiv.org/abs/2510.08257)
*Eleni Bougioukou,Anastasios Petropoulos,Nikolaos Toulgaridis,Theodoros Chatzimichail,Theodore Antonakopoulos*

Main category: cs.ET

TL;DR: 提出了一个用于内存计算集成电路快速原型设计的分布式可扩展仿真系统，包括架构、软件开发工具和实验结果


<details>
  <summary>Details</summary>
Motivation: 内存计算技术在AI设备中广泛应用，但开发和集成需要大量时间，需要实时仿真环境在芯片可用前分析系统、测试微码和部署应用

Method: 开发分布式可扩展仿真系统架构和软件开发工具，用于内存计算集成电路的快速原型设计

Result: 实验结果证明了所提出仿真器的实用性

Conclusion: 该仿真系统为内存计算集成电路的快速原型设计提供了有效的解决方案

Abstract: In-memory computing technology is used extensively in artificial intelligence
devices due to lower power consumption and fast calculation of matrix-based
functions. The development of such a device and its integration in a system
takes a significant amount of time and requires the use of a real-time
emulation environment, where various system aspects are analyzed, microcode is
tested, and applications are deployed, even before the real chip is available.
In this work, we present the architecture, the software development tools, and
experimental results of a distributed and expandable emulation system for rapid
prototyping of integrated circuits based on in-memory computing technologies.
Presented experimental results demonstrate the usefulness of the proposed
emulator.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [122] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: 提出一个自适应调度器SmartDiff，通过动态调整批处理大小和线程数，在固定CPU和内存预算下最小化p95延迟，相比基准方法降低延迟23-40%，内存使用降低16-32%且无内存溢出。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法在批处理大小和并行度选择上表现不佳，导致高延迟和内存溢出问题，需要一种能自适应调整参数以优化性能的调度器。

Method: 使用轻量级预分析器估计每行字节数和I/O速率，在线成本/内存模型修剪不安全操作，采用带背压和慢任务缓解的防护爬山策略，根据工作集估计选择内存线程或Dask并行执行模式。

Result: 在合成和公开表格基准测试中，相比调优预热启发式方法，p95延迟降低23-28%；相比固定网格基线，延迟降低35-40%，峰值内存降低25-32%，且无内存溢出问题，吞吐量相当。

Conclusion: 该自适应调度器能有效优化延迟和内存使用，通过智能参数调整和模式选择实现性能提升，证明了自适应调度在数据处理系统中的价值。

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [123] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: 本文提出了DT模拟桥接框架，支持数字孪生与模拟环境之间的双向交互，提升工业系统的设计灵活性和运行分析能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生在物联网和工业物联网中能力的增强，需要与模拟平台无缝集成以支持系统设计、验证和实时操作。

Method: 设计了DT模拟桥接软件框架，支持数字孪生与模拟环境之间的双向数据交换和多样化交互模式。

Result: 实验结果表明，该框架增强了设计敏捷性，促进了虚拟调试，并支持在真实条件下的实时行为分析。

Conclusion: DT模拟桥接框架在各种工业场景中表现出有效性，为数字孪生与模拟环境的集成提供了可行的解决方案。

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [124] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: 论文提出重新设计无服务器计算的硬件架构，使用硬件隔离而非软件隔离，每个函数使用独立处理器，以显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在传统服务器硬件上运行数千个函数，需要昂贵的软件隔离机制和高度的过度配置，导致能源效率低下。

Method: 采用硬件隔离方法，为每个函数分配独立处理器，构建仅在实际工作时消耗能源的无服务器硬件堆栈。

Result: 初步评估显示，这种方法可减少90.63%的能耗开销，平均节省70.8MW。

Conclusion: 硬件隔离的无服务器架构能显著提高能源效率，更好地满足无服务器软件的需求。

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [125] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: 提出了一种分布式资源选择机制，用于云边环境中的动态资源分配，通过分布式决策避免集中式协调瓶颈，实现高效、可扩展和弹性的资源管理。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的云边环境中，集中式协调成为瓶颈，需要分布式决策机制来满足复杂分布式应用的需求。

Method: 采用基于共识的机制，利用本地知识和代理间协作，不依赖中央控制器实现分布式编排。

Result: 计算时间是影响分配决策的关键因素，该方法在保持最优性的同时实现快速分配，比集中式启发式方法快达30倍。

Conclusion: 该分布式资源选择机制为分布式编排系统提供了核心组件，能够在云边环境中实现智能应用部署和实时适应。

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [126] [Energy-Efficient Maximal Independent Sets in Radio Networks](https://arxiv.org/abs/2510.08244)
*Dominick Banasik,Varsha Dani,Fabien Dufoulon,Aayush Gupta,Thomas P. Hayes,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 该论文提出了无线网络中更节能的分布式最大独立集算法，在碰撞检测和无碰撞检测两种模型下分别实现了O(log n)和O(log²n log log n)的能量复杂度，其中CD模型的能量复杂度被证明是最优的。


<details>
  <summary>Details</summary>
Motivation: 无线网络（特别是自组织和传感器网络）通常由电池供电，能量是宝贵资源。因此需要设计能量复杂度尽可能低的分布式算法来解决最大独立集这一基础问题。

Method: 使用随机化分布式算法，在两种无线电网络模型（有碰撞检测CD和无碰撞检测no-CD）下分别设计算法，通过优化节点唤醒轮次来最小化能量消耗。

Result: CD模型：能量复杂度O(log n)，轮复杂度O(log² n)，失败概率1/poly(n)，能量复杂度被证明是最优的。no-CD模型：能量复杂度O(log²n log log n)，轮复杂度O(log³ n log Δ)，显著优于现有最佳算法的O(log³ n)复杂度。

Conclusion: 论文在无线网络中实现了更节能的最大独立集算法，特别是在CD模型中达到了能量复杂度的最优界，为能量受限的无线网络提供了高效的分布式解决方案。

Abstract: The maximal independent set (MIS) is one of the most fundamental problems in
distributed computing, and it has been studied intensively for over four
decades. This paper focuses on the MIS problem in the Radio Network model, a
standard model widely used to model wireless networks, particularly ad hoc
wireless and sensor networks. Energy is a premium resource in these networks,
which are typically battery-powered. Hence, designing distributed algorithms
that use as little energy as possible is crucial. We use the well-established
energy model where a node can be sleeping or awake in a round, and only the
awake rounds (when it can send or listen) determine the energy complexity of
the algorithm, which we want to minimize.
  We present new, more energy-efficient MIS algorithms in radio networks with
arbitrary and unknown graph topology. We present algorithms for two popular
variants of the radio model -- with collision detection (CD) and without
collision detection (no-CD). Specifically, we obtain the following results:
  1. CD model: We present a randomized distributed MIS algorithm with energy
complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability
$1 / poly(n)$, where $n$ is the network size. We show that our energy
complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.
  2. no-CD model: In the more challenging no-CD model, we present a randomized
distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$,
round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 /
poly(n)$. The energy complexity of our algorithm is significantly lower than
the round (and energy) complexity of $O(\log^3 n)$ of the best known
distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.

</details>


### [127] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: 提出了一种重新分区策略，通过平衡CPU矩阵组装和GPU线性求解来改进OpenFOAM中的GPU加速性能


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算越来越依赖GPU，但在复杂科学框架如OpenFOAM中集成GPU加速仍面临挑战。现有方法要么完全重构代码库，要么使用基于插件的GPU求解器，在性能和开发工作量之间存在权衡

Method: 提出重新分区策略，包括详细的计算模型、新颖的矩阵重新分区和更新过程，在异构CPU-GPU环境中平衡CPU矩阵组装和GPU线性求解

Result: 该方法显著缓解了过订阅问题，提高了异构CPU-GPU环境中的求解器性能和资源利用率

Conclusion: 所提出的重新分区方法有效改进了OpenFOAM中基于插件的GPU加速性能，为复杂科学框架的GPU集成提供了更好的解决方案

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [128] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 提出基于排队论的模型框架，揭示现代服务器运行延迟关键应用时的空闲机会浪费问题


<details>
  <summary>Details</summary>
Motivation: 现代服务器运行延迟关键应用时存在显著的空闲时间浪费机会，主要由于空闲管理器的准确性和传统深度空闲状态切换延迟问题

Method: 使用三种排队模型（M/M/1、cxM/M/1和M/M/c）在CPU核心和系统级别估计理论空闲时间分布，并与实际服务器空闲情况进行对比

Result: 理论模型与实际服务器空闲情况的对比显示存在显著错失进入深度空闲状态的机会

Conclusion: 该框架为早期设计探索提供了方法，能够洞察不同服务器系统配置和负载下的空闲时间行为和优化机会

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [129] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM是一种新型PIM架构，通过动态检测数据移动开销并主动将数据移动到本地内存的保留区域，使用分布式地址间接硬件查找表重定向流量，从而减少远程内存访问的数据传输和排队延迟。


<details>
  <summary>Details</summary>
Motivation: 现有PIM架构虽然能提高能效和性能，但其优势依赖于数据与处理单元的邻近性。数据移动开销会降低PIM的性能和能效，因为需要在处理单元和远程内存位置之间移动数据。

Method: 提出DL-PIM架构，动态检测数据移动开销，主动将数据移动到请求处理单元的本地内存保留区域。使用分布式地址间接硬件查找表重定向流量，并在HMC和HBM两种3D堆叠内存上实现。采用自适应机制评估间接访问的成本与收益，动态启用或禁用以防止对某些工作负载产生负面影响。

Result: DL-PIM将HMC的平均内存延迟降低54%，HBM降低50%。对于具有大量数据重用的工作负载，HMC性能提升15%，HBM提升5%。所有代表性工作负载中，HMC实现6%加速，HBM实现3%加速。

Conclusion: DL-PIM通过增强数据局部性有效提高了系统整体性能，证明了动态数据移动管理在PIM架构中的重要性。

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [130] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出基于FPGA的可动态配置DNN推理加速器架构，采用脉动阵列、高带宽内存和UltraRAM，支持多种处理单元配置，并通过启发式权重传输调度实现高吞吐效率。


<details>
  <summary>Details</summary>
Motivation: 随着DNN推理对专用硬件计算效率需求的增长，需要设计可适应不同模型和未来FPGA设计的灵活加速器架构。

Method: 使用FPGA构建动态可配置加速器，包含脉动阵列、高带宽内存和UltraRAM，提供两种不同计算能力的处理单元配置，采用多PU实例化和启发式权重传输调度策略。

Result: 该架构在吞吐效率方面优于先前工作，并能扩展模拟模拟内存计算设备，用于研究设备级噪声行为。

Conclusion: 该工作提出了一种适用于各种模型和未来FPGA设计的通用DNN推理加速架构，具有高度适应性和扩展性。

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [131] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache是一个利用可编程交换机在数据平面直接服务文件系统元数据请求的框架，解决了多客户端环境下元数据缓存一致性的问题，显著提升了分布式文件系统的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 分布式文件系统中，跨多个元数据服务器的快速可扩展元数据管理对于处理大量文件和目录至关重要。客户端缓存虽然能减轻服务器负载，但在客户端数量增加时，维护缓存一致性会带来显著开销和复杂性。

Method: 提出FMCache框架，利用可编程交换机在交换机数据平面直接服务多个客户端的文件系统元数据请求，解决了在严格交换机资源约束下的文件系统特定路径依赖问题。

Result: 在Hadoop HDFS上的实现和Tofino交换机测试平台评估显示，FMCache相比原生HDFS实现了高达181.6%的吞吐量提升，与客户端缓存结合时还能带来额外139.6%的吞吐量增益，同时保持低延迟和有限的交换机资源使用。

Conclusion: FMCache通过利用可编程交换机实现高效的元数据缓存，显著提升了分布式文件系统的性能，同时解决了多客户端环境下的缓存一致性问题。

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>


### [132] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: SPAD提出专门针对LLM推理的预填充和解码阶段的专用硬件设计，通过减少资源浪费来降低服务成本


<details>
  <summary>Details</summary>
Motivation: 现有数据中心GPU/TPU采用"越多越好"的设计理念，导致预填充阶段内存带宽利用不足和解码阶段计算资源利用不足，增加了服务成本

Method: 设计专门的预填充芯片（更大脉动阵列+GDDR内存）和解码芯片（高内存带宽+减少计算能力），采用"少即是多"的方法论

Result: 相比H100，预填充芯片性能提升8%，硬件成本降低52%；解码芯片达到97%性能，TDP降低28%。端到端模拟显示硬件成本降低19%-41%，TDP降低2%-17%

Conclusion: SPAD设计能够显著降低LLM推理的硬件成本和功耗，同时保持性能，并且具有很好的适应性和长期性

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>
