<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 145]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Block Sparse Flash Attention](https://arxiv.org/abs/2512.07011)
*Daniel Ohayon,Itay Lamprecht,Itay Hubara,Israel Cohen,Daniel Soudry,Noam Elata*

Main category: cs.LG

TL;DR: BSFA是一种无需训练的注意力加速方法，通过计算精确的查询-键相似度选择最重要的值块，跳过约50%计算和内存传输，在保持99%以上基线准确率的同时实现1.10-1.24倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型需要长上下文进行推理和多文档任务，但注意力的二次复杂度造成了严重的计算瓶颈，需要高效的加速方法。

Method: BSFA通过计算精确的查询-键相似度，为每个查询选择top-k最重要的值块，通过比较每块最大分数与校准阈值，跳过约50%的计算和内存传输。无需训练，仅需在小数据集上进行一次性阈值校准。

Result: 在Llama-3.1-8B上，BSFA在真实世界推理基准上实现1.10倍加速，在"大海捞针"检索任务上实现1.24倍加速，同时保持99%以上基线准确率，某些配置甚至通过关注最相关内容提高了准确性。

Conclusion: BSFA是一种有效的训练无关注意力加速方法，可作为FlashAttention的直接替代品，在保持模型质量的同时显著加速长上下文推理，优于现有稀疏注意力方法。

Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention

</details>


### [2] [A self-driving lab for solution-processed electrochromic thin films](https://arxiv.org/abs/2512.05989)
*Selma Dahms,Luca Torresi,Shahbaz Tareq Bandesha,Jan Hansmann,Holger Röhm,Alexander Colsmann,Marco Schott,Pascal Friederich*

Main category: cs.LG

TL;DR: 利用自驱动实验室结合自动化和机器学习加速电致变色涂层的开发，通过贝叶斯优化高效探索工艺参数


<details>
  <summary>Details</summary>
Motivation: 旋涂电致变色薄膜层的优化复杂性给快速开发带来挑战，需要高效方法来加速电致变色涂层的发展

Method: 结合自动化数据采集、图像处理、光谱分析和贝叶斯优化的自驱动实验室系统，用于高效探索工艺参数

Result: 该方法不仅提高了通量，还能有针对性地搜索最优工艺参数，可应用于各种溶液处理材料

Conclusion: 自驱动实验室在增强材料发现和工艺优化方面具有巨大潜力，为电致变色涂层开发提供了高效途径

Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.

</details>


### [3] [Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure](https://arxiv.org/abs/2512.05990)
*Xin Li*

Main category: cs.LG

TL;DR: 提出MAI框架，用代数拓扑统一学习和记忆，通过同调奇偶性区分内容与上下文，将复杂搜索转化为简单查找


<details>
  <summary>Details</summary>
Motivation: 当前机器学习将参数结构与推理流程分离，缺乏生物认知的样本效率和热力学经济性，需要统一学习和记忆的理论框架

Method: 基于代数拓扑的MAI框架，引入同调奇偶性原则区分稳定内容(H_even)和动态上下文(H_odd)，通过拓扑三一变换实现搜索→闭合→结构的转换

Result: 将高复杂度递归搜索转化为低复杂度查找，通过拓扑循环闭合机制，用拓扑泛化的Wake-Sleep算法协调推理和学习

Conclusion: 为从慢思考到快思考的涌现提供严格解释，为后图灵架构提供基于拓扑共振的计算蓝图

Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.

</details>


### [4] [Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra](https://arxiv.org/abs/2512.06059)
*Andrea Della Valle,Annalisa D'Arco,Tiziana Mancini,Rosanna Mosetti,Maria Chiara Paolozzi,Stefano Lupi,Sebastiano Pilati,Andrea Perali*

Main category: cs.LG

TL;DR: 该论文提出了一种结合实验数据和合成数据的方法，利用条件生成神经网络增强VOC红外光谱数据集，训练出能够可靠识别9种VOC类别并精确预测其浓度的判别神经网络。


<details>
  <summary>Details</summary>
Motivation: 挥发性有机化合物（VOCs）对人类健康构成重大风险，需要准确检测以监测和最小化暴露。红外光谱虽然能实现超灵敏检测，但复杂的光谱限制了实时识别和定量分析的可能性。深度神经网络需要大量训练数据，而实验数据集通常有限。

Method: 1. 创建9种不同类别化合物在不同浓度下的实验VOC数据集（红外吸收光谱）
2. 使用条件生成神经网络生成合成光谱，增加光谱数量和浓度多样性
3. 结合实验和合成数据训练稳健的判别神经网络
4. 实现VOC识别和浓度预测

Result: 成功训练出能够可靠识别9种VOC类别并精确预测其浓度的神经网络。该网络适用于集成到VOC识别和分析的传感设备中。

Conclusion: 通过结合实验数据和条件生成神经网络生成的合成数据，可以有效解决VOC红外光谱识别中训练数据不足的问题，开发出适用于实际传感设备的稳健识别和定量分析模型。

Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.

</details>


### [5] [Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning](https://arxiv.org/abs/2512.06649)
*Camellia Zakaria,Aryan Sadeghi,Weaam Jaafar,Junshi Xu,Alex Mariakakis,Marianne Hatzopoulou*

Main category: cs.LG

TL;DR: 利用交通监控视频和天气数据，通过机器学习模型估计街道级黑碳浓度，为城市污染治理提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 城市黑碳排放主要来自交通，但监测成本高导致数据缺乏，而交通监控系统广泛部署，存在交通状况与环境后果之间的信息鸿沟。

Method: 从交通视频中提取车辆行为和状况的视觉信息，结合天气数据，构建机器学习模型来估计街道级黑碳浓度。

Result: 模型达到R平方值0.72和RMSE为129.42 ng/m³，能够有效估计黑碳浓度。

Conclusion: 利用现有城市基础设施和建模技术生成交通排放相关信息，为污染减排、城市规划、公共卫生和环境正义提供可操作见解。

Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.

</details>


### [6] [When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models](https://arxiv.org/abs/2512.06062)
*S. M. Mustaqim,Anantaa Kotal,Paul H. Yi*

Main category: cs.LG

TL;DR: 提出一种基于聚类和密度分析的成员推理攻击方法，能够从隐私保护的合成数据中推断训练样本的成员信息，即使生成器使用了差分隐私等噪声机制。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型被广泛用于生成保护隐私的合成数据，但作者发现合成数据仍可能通过数据流形的结构重叠泄露训练样本信息。现有研究主要关注样本级别的记忆化，而忽视了分布邻域推理带来的隐私风险。

Method: 提出黑盒成员推理攻击：1) 重复查询生成模型获取大量合成样本；2) 进行无监督聚类识别合成分布中的密集区域；3) 分析聚类中心点和邻域，这些区域对应原始训练数据中的高密度区域；4) 利用这些邻域作为训练样本的代理，推断成员信息或重建近似记录。

Result: 在医疗、金融等敏感领域的实验表明，真实数据与合成数据之间的聚类重叠会导致可测量的成员信息泄露，即使生成器使用了差分隐私或其他噪声机制。

Conclusion: 合成数据生成管道存在未充分探索的攻击面，需要更强的隐私保证机制，不仅要考虑样本级别的记忆化，还要考虑分布邻域推理。这项工作强调了在隐私保护数据发布中需要更全面的隐私评估框架。

Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.

</details>


### [7] [JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning](https://arxiv.org/abs/2512.06102)
*Ufuk Çakır,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: JaxWildfire是一个基于JAX实现的高性能野火模拟器，通过向量化GPU计算实现6-35倍加速，支持强化学习训练和梯度优化。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自然灾害管理中有应用潜力，但现有野火模拟器速度太慢，限制了RL代理的训练效率。

Method: 基于细胞自动机的概率性火势传播模型，使用JAX框架实现，通过vmap实现向量化模拟，支持GPU加速。

Result: 相比现有软件实现6-35倍加速，支持基于梯度的模拟器参数优化，并能用于训练野火扑救策略的RL代理。

Conclusion: JaxWildfire为推进强化学习在自然灾害管理中的应用提供了重要工具，解决了模拟速度瓶颈问题。

Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.

</details>


### [8] [ARC-AGI Without Pretraining](https://arxiv.org/abs/2512.06104)
*Isaac Liao,Albert Gu*

Main category: cs.LG

TL;DR: 76K参数模型CompressARC无需预训练，通过最小描述长度在推理时解决20%的ARC-AGI视觉谜题，展示极端泛化能力


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点：解决ARC-AGI视觉谜题通常需要大规模预训练。研究者希望证明存在替代方法，通过最小描述长度原理在极有限数据条件下实现智能

Method: 提出CompressARC模型（仅76K参数），采用最小描述长度原则，在推理时仅使用目标谜题本身（移除最终解信息）进行训练，完全不使用预提供的训练集

Result: 模型解决了20%的评估谜题，在极端数据限制条件下（仅单样本训练）仍能解决多样化的创造性谜题，展示了传统深度学习难以企及的泛化能力

Conclusion: 最小描述长度是产生智能的可行替代路径，无需依赖传统的大规模预训练方法，为AGI研究提供了新方向

Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.

</details>


### [9] [A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts](https://arxiv.org/abs/2512.06111)
*Arthur Mukwaya,Nancy Kasamala,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi,Denis Ruganuza,Mark Ngotonie*

Main category: cs.LG

TL;DR: 提出机器学习框架，通过选择最优代表性日进行短期交通计数，提高年度平均日交通量预测精度


<details>
  <summary>Details</summary>
Motivation: 美国各州交通部门难以获取准确的年度平均日交通量数据，特别是未监测道路。连续计数站成本高、部署难，迫使机构依赖短期计数，但现有方法缺乏优化选择计数日期的策略。

Method: 提出机器学习框架，使用迭代方法选择对AADT估计最具信息量的最优代表性日。利用德克萨斯州2022-2023年交通量数据，比较"最优日"方法和"无优化日"基线。采用留一法技术生成无偏代表性日交通特征。

Result: 最优日方法在Top 5天内均优于基线，最佳日（第186天）误差更低（RMSE: 7,871.15，MAE: 3,645.09，MAPE: 11.95%），R²更高（0.9756），而基线为RMSE: 11,185.00，MAE: 5,118.57，MAPE: 14.42%，R²: 0.9499。

Conclusion: 该研究为交通部门提供了改进传统短期计数实践的替代方案，提高了AADT估计精度，支持公路性能监测系统合规性，并降低了全州交通数据收集的运营成本。

Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.

</details>


### [10] [Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting](https://arxiv.org/abs/2512.06134)
*Georgi Hrusanov,Duy-Thanh Vu,Duy-Cat Can,Sophie Tascedda,Margaret Ryan,Julien Bodelet,Katarzyna Koscielska,Carsten Magnus,Oliver Y. Chén*

Main category: cs.LG

TL;DR: NKM（神经库普曼机）是一种新型机器学习架构，用于同时预测阿尔茨海默病的多种认知评分，通过融合多模态数据实现个性化、可解释的认知衰退预测。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病个体认知衰退的早期预测对疾病评估和管理至关重要。现有方法难以在保持可解释性的同时整合多模态数据进行纵向个性化预测。

Method: 提出神经库普曼机（NKM），受动力系统和注意力机制启发，整合分析知识（α）和生物学知识（β）来指导特征分组并控制分层注意力机制。通过融合组感知分层注意力在库普曼算子框架内，将复杂非线性轨迹转换为可解释的线性表示。

Result: 在ADNI数据集上的应用表明，NKM在预测认知衰退轨迹方面持续优于传统机器学习和深度学习模型。能够同时预测多种认知评分变化、量化不同生物标志物对预测的贡献、识别与认知恶化最相关的大脑区域。

Conclusion: NKM通过可解释的显式系统，利用过去的多模态数据推进了阿尔茨海默病未来认知衰退的个性化、可解释预测，并揭示了疾病进展的潜在多模态生物学基础。

Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.

</details>


### [11] [gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points](https://arxiv.org/abs/2512.06143)
*Marcus M. Noack,Mark D. Risser,Hengrui Luo,Vardaan Tekriwal,Ronald J. Pandolfi*

Main category: cs.LG

TL;DR: 提出gp2Scale方法，无需诱导点或近似，通过设计紧凑支撑的非平稳核函数，利用协方差矩阵的自然稀疏结构，将精确高斯过程扩展到千万级数据点。


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程扩展方法存在计算速度、预测精度和不确定性量化之间的权衡，大多依赖各种近似降低了精度并限制了核函数和噪声模型设计的灵活性，而当前表达性非平稳核函数在许多领域日益重要。

Method: 提出gp2Scale方法，利用高度灵活、紧凑支撑的非平稳核函数识别协方差矩阵中自然存在的稀疏结构，然后利用这种稀疏性进行线性系统求解和对数行列式计算，避免使用诱导点、核插值或邻域近似。

Result: 方法能够扩展到超过1000万个数据点，在多个真实数据集上展示了功能，与最先进的近似算法相比，在许多情况下显示出优越的近似性能。

Conclusion: 该方法的核心优势在于对任意高斯过程定制（核心核设计、噪声和均值函数）以及输入空间类型的不可知性，使其非常适合现代高斯过程应用。

Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.

</details>


### [12] [Learning Invariant Graph Representations Through Redundant Information](https://arxiv.org/abs/2512.06154)
*Barproda Halder,Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 该论文提出了一种基于部分信息分解(PID)的冗余信息引导不变图学习(RIG)框架，用于提升图表示在分布外(OOD)泛化中的不变性。


<details>
  <summary>Details</summary>
Motivation: 现有基于经典信息论的图表示学习方法难以完全消除虚假成分，导致OOD泛化能力不足。需要更精确地处理虚假子图和不变子图之间关于目标Y的冗余信息。

Method: 提出RIG框架：1)使用PID识别虚假子图Gs和不变子图Gc之间的冗余信息；2)采用多级优化方法，交替估计冗余信息下界并最大化该下界；3)同时隔离虚假和因果子图。

Result: 在合成和真实世界图数据集上的实验表明，RIG框架在多种分布偏移下具有优越的泛化能力。

Conclusion: PID为图表示学习提供了超越经典信息论的新工具，RIG框架通过精确处理冗余信息有效提升了OOD泛化性能。

Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.

</details>


### [13] [PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations](https://arxiv.org/abs/2512.06183)
*Lindong Liu,Zhixiong Jin,Seongjin Choi*

Main category: cs.LG

TL;DR: PMA-Diffusion：一种物理引导的掩码感知扩散框架，用于从稀疏观测中重建高速公路速度场，在5%可见度下仍优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要高分辨率交通状态信息，但传统检测器（环形线圈、探测车）获取的数据通常过于稀疏和嘈杂，难以捕捉交通流的详细动态。

Method: 提出PMA-Diffusion框架，包含两个掩码感知训练策略（单掩码和双掩码），在推理阶段使用物理引导的后验采样器，交替进行反向扩散更新、观测投影和基于自适应各向异性平滑的物理引导投影。

Result: 在I-24 MOTION数据集上测试，即使在仅5%可见度的严重稀疏情况下，PMA-Diffusion在三个重建误差指标上均优于其他基线方法，且使用稀疏观测训练的性能接近基于完整观测训练的基线模型。

Conclusion: 将掩码感知扩散先验与物理引导后验采样器相结合，为现实传感稀疏条件下的交通状态估计提供了可靠且灵活的解决方案。

Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.

</details>


### [14] [How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?](https://arxiv.org/abs/2512.06200)
*Tomohiro Yamashita,Daichi Amagata,Yusuke Matsui*

Main category: cs.LG

TL;DR: 提出一个用于评估ANNS索引数据删除效率的实验框架和综合评估指标，并将框架应用于HNSW分析删除效果，提出动态选择删除方法的Deletion Control方法


<details>
  <summary>Details</summary>
Motivation: 动态数据上的近似最近邻搜索（ANNS）在实际应用中越来越重要（如检索增强生成），但目前缺乏对ANNS数据删除的全面评估方法

Method: 1. 提出实验框架和综合评估指标来评估ANNS索引的数据删除效率；2. 将图基ANNS的数据删除方法分为三类并进行数学形式化；3. 在准确性、查询速度等指标上评估性能；4. 将框架应用于HNSW分析删除效果；5. 提出Deletion Control方法动态选择删除方法

Result: 建立了ANNS数据删除的评估框架，对HNSW进行了删除效果分析，并提出了能够根据所需搜索精度动态选择适当删除方法的Deletion Control方法

Conclusion: 该研究为ANNS数据删除提供了首个全面的评估框架，通过Deletion Control方法实现了在动态数据环境下平衡删除效率和搜索性能的目标

Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.

</details>


### [15] [K2-V2: A 360-Open, Reasoning-Enhanced LLM](https://arxiv.org/abs/2512.06201)
*K2 Team,Zhengzhong Liu,Liping Tang,Linghao Jin,Haonan Li,Nikhil Ranjan,Desai Fan,Shaurya Rohatgi,Richard Fan,Omkar Pangarkar,Huijuan Wang,Zhoujun Cheng,Suqi Sun,Seungwook Han,Bowen Tan,Gurpreet Gosal,Xudong Han,Varad Pimpalkhute,Shibo Hao,Ming Shan Hee,Joel Hestness,Haolong Jia,Liqun Ma,Aaryamonvikram Singh,Daria Soboleva,Natalia Vassilieva,Renxi Wang,Yingquan Wu,Yuekai Sun,Taylor Killian,Alexander Moreno,John Maggs,Hector Ren,Guowei He,Hongyi Wang,Xuezhe Ma,Yuqi Wang,Mikhail Yurochkin,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-V2是一个从头开始构建的360度开放大语言模型，专为推理适应设计，在72B参数规模中表现优异，超越Qwen2.5-72B并接近Qwen3-235B的性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个专门为复杂推理任务优化的开源基础模型，为社区提供一个强大的推理中心化基础，同时支持对话、知识检索等通用功能。

Method: 从头开始训练，在训练过程中主动注入领域知识、推理能力、长上下文和工具使用能力；采用简单的监督微调建立强基线；发布完整的训练历史和数据组成。

Result: K2-V2成为最强的完全开源模型，在其规模类别中与开源领导者竞争，超越Qwen2.5-72B并接近Qwen3-235B的性能；展示了通过简单微调就能获得强大推理能力的潜力。

Conclusion: K2-V2为社区提供了一个强大的推理中心化基础模型，通过完整的开源（包括训练数据）最大化持续训练的效果，为高级对齐留下了显著提升空间。

Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.

</details>


### [16] [A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation](https://arxiv.org/abs/2512.06547)
*Xiaocan Li,Shiliang Wu,Zheng Shen*

Main category: cs.LG

TL;DR: A-3PO通过近似近端策略消除额外前向传播计算瓶颈，在保持性能的同时减少18%训练时间


<details>
  <summary>Details</summary>
Motivation: 解耦损失在异步强化学习中能处理数据陈旧性问题，但近端策略需要在每个训练步骤进行额外前向传播，对大型语言模型造成计算瓶颈

Method: 提出A-3PO方法，通过简单插值近似近端策略，无需显式计算，消除额外前向传播开销

Result: 减少18%训练时间，同时保持与原始方法相当的性能表现

Conclusion: A-3PO有效解决了近端策略优化的计算瓶颈问题，为大规模语言模型的强化学习训练提供了高效解决方案

Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md

</details>


### [17] [Quantifying Memory Use in Reinforcement Learning with Temporal Range](https://arxiv.org/abs/2512.06204)
*Rodney Lafuente-Mercado,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 提出Temporal Range指标，用于量化RL策略对历史观测的依赖程度，通过计算输出对输入序列的时间敏感性来测量平均滞后


<details>
  <summary>Details</summary>
Motivation: 需要量化训练好的RL策略实际使用过去观测的程度，以理解策略的记忆依赖特性，比较不同智能体和环境，并选择最短的足够上下文

Method: 提出Temporal Range指标，通过反向自动微分计算Jacobian块∂y_s/∂x_t，得到时间影响剖面，然后用幅度加权平均滞后进行总结；在线性设置中通过一组自然公理进行特征化

Result: 在诊断和控制任务（POPGym；闪烁/遮挡；Copy-k）及架构（MLP、RNN、SSM）上验证：在完全观测控制中保持较小值；在Copy-k中与任务真实滞后成比例缩放；与实现接近最优回报所需的最小历史窗口一致

Conclusion: Temporal Range提供了实用的序列级记忆依赖读取，可用于比较智能体和环境，并选择最短的足够上下文，为RL策略的记忆使用提供了量化工具

Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.

</details>


### [18] [Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration](https://arxiv.org/abs/2512.06218)
*Huizhen Yu,Yi Wan,Richard S. Sutton*

Main category: cs.LG

TL;DR: 将异步随机逼近理论应用于平均奖励半马尔可夫决策过程的强化学习，证明了RVI Q-learning算法的收敛性，并引入了新的单调性条件来估计最优奖励率。


<details>
  <summary>Details</summary>
Motivation: 将作者在Borkar-Meyn框架下的异步随机逼近（SA）最新成果应用于平均奖励半马尔可夫决策过程（SMDPs）的强化学习，扩展了经典Schweitzer相对值迭代算法的异步版本RVI Q-learning的收敛性分析。

Method: 应用Borkar-Meyn框架下的异步随机逼近理论，分析RVI Q-learning算法在有限状态、弱通信SMDPs中的收敛性。引入新的单调性条件来估计最优奖励率，并采用新颖的稳定性与收敛性分析论证。

Result: 证明了RVI Q-learning算法几乎必然收敛到平均奖励最优方程解的紧致连通子集，在额外的步长和异步条件下收敛到唯一的样本路径依赖解。新的单调性条件显著扩展了先前考虑的算法框架。

Conclusion: 成功将异步随机逼近理论应用于平均奖励SMDPs的强化学习，建立了RVI Q-learning的收敛性理论，并通过引入新的单调性条件扩展了算法分析框架，为相关算法提供了更坚实的理论基础。

Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.

</details>


### [19] [Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph](https://arxiv.org/abs/2512.06236)
*Haiyang Yu,Meng-Chieh Lee,Xiang song,Qi Zhu,Christos Faloutsos*

Main category: cs.LG

TL;DR: 提出GraphDeT框架，通过添加边去噪辅助任务来提升图神经网络在域适应节点分类中的泛化能力


<details>
  <summary>Details</summary>
Motivation: 图结构域偏移（如不同时间或区域收集的图数据）导致GNN在目标图上性能下降，需要提升GNN在域适应场景下的泛化能力

Method: 提出GraphDeT框架，在GNN训练中集成边去噪辅助任务，理论分析表明该任务通过约束图泛化边界来提升泛化性能

Result: 实验结果显示在时间和区域域图偏移场景下，GraphDeT相比现有基线方法表现出优越性能

Conclusion: 简单的边去噪辅助任务能有效提升GNN在域适应节点分类中的泛化能力，GraphDeT框架为处理图结构域偏移提供了有效解决方案

Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.

</details>


### [20] [Quantization Blindspots: How Model Compression Breaks Backdoor Defenses](https://arxiv.org/abs/2512.06243)
*Rohan Pandey,Eric Ye*

Main category: cs.LG

TL;DR: 量化（INT8/INT4）使现有后门防御失效，攻击成功率仍高于99%，暴露了防御评估（FP32）与实际部署（量化）的脱节


<details>
  <summary>Details</summary>
Motivation: 现实世界部署中，模型通常会被量化（INT8或更低精度）以减少内存和延迟，但现有后门防御主要在FP32模型上评估，需要研究量化对防御效果的影响

Method: 对5种代表性后门防御进行系统实证研究，在3种精度设置（FP32、INT8动态、INT4模拟）和2个标准视觉基准上，使用经典的BadNet攻击进行评估

Result: INT8量化将所有防御的检测率降至0%，而攻击成功率仍高于99%；INT4量化效果存在数据集依赖性，Neural Cleanse在GTSRB上有效但在CIFAR-10上失效，但后门攻击成功率仍高于90%

Conclusion: 量化鲁棒性应成为未来后门防御评估和设计的必要维度，当前防御评估与实际部署存在严重脱节

Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.

</details>


### [21] [Auto-exploration for online reinforcement learning](https://arxiv.org/abs/2512.06244)
*Caleb Ju,Guanghui Lan*

Main category: cs.LG

TL;DR: 提出具有自动探索功能的强化学习方法，无需先验知识或算法相关参数，在表格和线性函数近似设置下实现O(ε⁻²)样本复杂度


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法需要假设状态和动作空间的充分探索，这导致算法不可实现且性能次优。需要参数无关的自动探索方法来解决探索-利用困境。

Method: 提出两类自动探索方法：表格设置和线性函数近似。采用动态混合时间、折扣状态分布采样、鲁棒梯度估计器和优势差距函数等创新技术。

Result: 在存在探索最优策略的假设下，两种方法都能达到O(ε⁻²)的样本复杂度来求解ε误差问题，且复杂度不包含可能任意大的算法相关参数。

Conclusion: 新方法通过参数无关的自动探索解决了强化学习中的探索-利用困境，易于实现且避免了先前工作中算法相关参数的影响，为高效强化学习提供了新途径。

Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.

</details>


### [22] [Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning](https://arxiv.org/abs/2512.06250)
*Chris Tava*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应策略切换方法，让智能体在迷宫导航中动态切换系统探索和目标导向策略，相比固定阈值方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自主智能体需要多种策略解决复杂任务，但何时切换策略是挑战。固定阈值方法不够灵活，需要手动设计启发式规则，缺乏适应性。

Method: 使用Q-learning学习两个正交导航策略（系统探索和目标导向）之间的切换阈值。将状态空间离散化为覆盖率和距离到目标的桶，基于覆盖百分比和目标距离动态调整切换行为（阈值20-60%），无需迷宫墙壁位置、最优阈值或手工启发式知识。

Result: 在240个测试配置（4种迷宫大小×10个独特迷宫×6种智能体变体）中，自适应阈值学习优于单策略智能体和固定40%阈值基线：完成时间提升23-55%，运行时方差降低83%，最坏情况改进71%。性能增益随问题复杂度增加而增加。

Conclusion: 自适应策略切换方法能有效学习动态切换策略，在复杂环境中显著优于固定阈值方法，且泛化能力强，性能增益随问题复杂度增加而增加。

Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.

</details>


### [23] [Learning Without Time-Based Embodiment Resets in Soft-Actor Critic](https://arxiv.org/abs/2512.06252)
*Homayoon Farrahi,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 本文研究了在无终止和无重置条件下使用SAC算法的挑战，提出了持续SAC版本，并展示了通过调整奖励函数和增加策略熵可以恢复因移除重置而损失的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习任务通常包含终止条件和环境重置等辅助组件，这些设置虽然能加速学习，但会导致不自然的任务设置，并可能阻碍在真实世界中的长期性能。本文旨在探索在没有终止和机器人本体重置情况下的学习挑战。

Method: 提出了持续版本的Soft Actor-Critic算法，通过简单修改现有任务的奖励函数，使持续SAC能达到或优于周期性SAC的性能。在修改的Gym Reacher任务上分析无重置学习的失败原因，并通过增加策略熵来改善探索。

Result: 持续SAC在减少对折扣率γ敏感性的同时，能达到或优于周期性SAC的性能。无本体重置会导致状态空间探索不足和学习失败/显著变慢。增加策略熵是恢复因移除重置而损失性能的有效干预措施。

Conclusion: 本文展示了在无终止和无重置条件下使用SAC算法的可行性，并提供了改善探索的方法。通过调整奖励函数和策略熵，可以在更自然的任务设置下实现有效的强化学习，这对真实世界应用具有重要意义。

Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $γ$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.

</details>


### [24] [Networked Restless Multi-Arm Bandits with Reinforcement Learning](https://arxiv.org/abs/2512.06274)
*Hanmo Zhang,Zenghui Sun,Kai Wang*

Main category: cs.LG

TL;DR: 该论文提出了Networked RMAB框架，将传统多臂老虎机与独立级联模型结合，以捕捉网络环境中个体间的交互作用，并通过子模性分析和爬山算法获得近似保证。


<details>
  <summary>Details</summary>
Motivation: 传统RMAB假设各臂独立，无法处理现实世界中个体间的交互作用。在公共卫生等资源分配场景中，个体间的网络效应可能显著影响干预效果，因此需要能捕捉网络交互的框架。

Method: 提出Networked RMAB框架，结合RMAB和独立级联模型。建立Bellman方程，证明其子模性，应用爬山算法获得1-1/e近似保证。通过改进的收缩分析证明近似Bellman更新的收敛性，并开发了针对网络设置的Q-learning算法。

Result: 理论分析表明Bellman方程具有子模性，爬山算法可获得1-1/e近似保证，近似Bellman更新保证收敛。实验结果显示，在真实图数据上，提出的Q-learning方法优于k步前瞻和忽略网络的方法。

Conclusion: Networked RMAB框架成功捕捉了网络环境中的交互效应，理论分析和实验验证表明该方法能有效处理网络效应，在存在网络交互的场景中具有优势。

Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.

</details>


### [25] [Theoretical Compression Bounds for Wide Multilayer Perceptrons](https://arxiv.org/abs/2512.06288)
*Houssam El Cheairi,David Gamarnik,Rahul Mazumder*

Main category: cs.LG

TL;DR: 该论文提出了一种随机贪心压缩算法，用于后训练剪枝和量化，并严格证明了多层感知机（MLP）和卷积神经网络（CNN）中存在性能具有竞争力的剪枝/量化子网络。


<details>
  <summary>Details</summary>
Motivation: 剪枝和量化技术在实践中能有效减少大型神经网络的参数量，但其经验成功的理论依据不足。本文旨在为这些压缩技术的有效性提供严格的理论分析，弥合理论与应用之间的差距。

Method: 提出了一种随机贪心压缩算法，该算法类似于Optimal Brain Damage（OBD）的后训练随机版本。算法用于多层感知机（MLP）的结构化剪枝和量化，并扩展到卷积神经网络（CNN）。分析不依赖数据假设，展示了可压缩性与网络宽度之间的权衡关系。

Result: 严格证明了在宽神经网络中存在性能具有竞争力的剪枝/量化子网络。为MLP和CNN的结构化剪枝提供了统一的理论分析框架，为压缩技术在宽多层感知机中的经验成功提供了理论依据。

Conclusion: 该研究为剪枝和量化技术的有效性提供了严格的理论证明，展示了压缩算法与网络宽度之间的权衡关系。提出的随机贪心算法为后训练压缩提供了理论支持，弥合了神经网络压缩领域理论与应用之间的差距。

Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.

</details>


### [26] [Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media](https://arxiv.org/abs/2512.06293)
*Fatima Ashraf,Muhammad Ayub Sabir,Jiaxin Deng,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于影响力加权关键词共现图和泊松解卷积分解的社交媒体主题建模方法，用于从稀疏的公共交通服务风险信号中提取可解释的主题。


<details>
  <summary>Details</summary>
Motivation: 城市交通机构越来越多地使用社交媒体监控服务风险（如拥挤、延误、安全事件），但这些关注信号稀疏、简短，容易被日常聊天淹没。需要一种能够从嘈杂社交流中提取可解释风险主题的方法。

Method: 1. 从清理后的帖子构建影响力加权关键词共现图，使有社会影响力的帖子按比例贡献证据；2. 提出泊松解卷积分解（PDF），将图分解为低秩主题结构和主题局部残差交互；3. 使用去相关正则化促进主题区分；4. 轻量级优化程序确保在非负性和归一化约束下的稳定收敛；5. 通过一致性驱动的扫描选择主题数量。

Result: 在大规模社交流数据上，该模型实现了最先进的主题一致性和强大的多样性，相比领先基线表现优异。

Conclusion: 该方法能够从稀疏、嘈杂的社交媒体数据中有效提取可解释的公共交通服务风险主题，为交通机构提供有价值的监控工具。代码和数据集已公开。

Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git

</details>


### [27] [Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks](https://arxiv.org/abs/2512.06297)
*Luca Di Carlo,Chase Goddard,David J. Schwab*

Main category: cs.LG

TL;DR: 神经网络损失景观中的吸引盆通过低损失路径相连，但优化动态通常局限在单个凸盆中，很少探索中间点。研究发现这是由于曲率变化与优化噪声相互作用产生的熵势垒所致。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络优化中的一个矛盾现象：虽然损失景观中的吸引盆通过低损失路径相连，但优化动态通常局限在单个凸盆中，很少探索中间点。研究者希望理解这种连通性与限制性并存的机制。

Method: 通过分析曲率变化与优化噪声的相互作用，识别熵势垒的形成机制。实证研究发现曲率在远离最小值时系统性上升，产生有效力将噪声动态偏转回端点。

Result: 曲率在远离最小值时系统性上升，产生有效力将噪声动态偏转回端点，即使损失保持近乎平坦。这些熵势垒比能量势垒持续更久，塑造了参数空间中解的后期定位。

Conclusion: 曲率诱导的熵力在深度学习景观中既控制连通性又控制限制性，解释了为什么优化动态通常局限在单个吸引盆中，即使存在低损失的连接路径。

Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.

</details>


### [28] [Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics](https://arxiv.org/abs/2512.06301)
*Jihun Ahn,Gabriella Pasya Irianti,Vikram Thapar,Su-Mi Hur*

Main category: cs.LG

TL;DR: CI-LLM框架结合HAPPY分子表示和数值描述符，实现了聚合物性质预测和逆向设计，比SMILES模型更快更准


<details>
  <summary>Details</summary>
Motivation: 机器学习已成功应用于无机化合物和小分子发现，但聚合物领域仍难以应用。虽然数据稀缺常被认为是主要瓶颈，但作者认为通过策略性的分子表示可以克服这一限制

Method: 提出CI-LLM框架，结合HAPPY（分层抽象聚合物重复单元）将化学子结构编码为token，并在transformer架构中加入数值描述符。使用De³BERTa进行性质预测，GPT-based生成器进行逆向设计

Result: De³BERTa比SMILES模型推理速度快3.5倍，R²分数在四个性质上提升0.9-4.1%。逆向设计器能100%保留支架结构，成功优化负相关的多目标性质

Conclusion: 策略性分子表示能显著推进聚合物科学中的机器学习应用，该框架展示了聚合物性质预测和逆向设计的综合能力

Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.

</details>


### [29] [Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization](https://arxiv.org/abs/2512.06303)
*Preksha Girish,Rachana Mysore,Kiran K. N.,Hiranmayee R.,Shipra Prashanth,Shrey Kumar*

Main category: cs.LG

TL;DR: 提出多模态图神经网络框架，整合结构MRI、扩散张量成像和功能MRI，建模大脑网络的时空重组，生成可解释的生物标志物用于预测认知衰退风险。


<details>
  <summary>Details</summary>
Motivation: 理解大脑网络的动态重组对于预测认知衰退、神经进展和临床结果的个体差异至关重要。现有方法需要更全面的多模态整合和数学严谨的建模来捕捉网络动态变化。

Method: 使用多模态图神经网络框架，将大脑区域表示为节点，结构和功能连接表示为边，构建纵向大脑图。通过分数随机微分算子嵌入图循环网络来捕捉时间演化，注意力机制融合多模态信息并生成可解释的生物标志物。

Result: 在纵向神经影像数据集上的实验证明了预测准确性和可解释性。提出的框架能够从现有影像数据中提取临床意义的生物标志物，无需新数据收集。

Conclusion: 数学严谨的多模态图神经网络方法具有从现有影像数据中提取临床意义生物标志物的潜力，为预测网络不稳定性和认知衰退提供了有效工具。

Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.

</details>


### [30] [Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness](https://arxiv.org/abs/2512.06341)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出Interpretive Efficiency（解释效率）指标，量化解释性表示中任务相关信息传输的比例，基于五个公理，与互信息相关，具有理论保证和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性指标很少能有效量化数据对解释性表示的支持程度，需要一种理论严谨且实用的度量方法来评估解释性表示的质量。

Method: 提出Interpretive Efficiency（解释效率）这一标准化、任务感知的函数，基于五个公理：有界性、Blackwell式单调性、数据处理稳定性、容许不变性和渐近一致性。将其与互信息关联，推导局部Fisher几何展开，并使用标准经验过程工具建立渐近和有限样本估计保证。

Result: 在受控图像和信号任务上的实验表明，该指标能恢复理论排序，揭示被准确性掩盖的表示冗余，并与鲁棒性相关，成为表示设计的实用理论支持诊断工具。

Conclusion: Interpretive Efficiency是一个理论严谨、实用的度量指标，能够有效评估解释性表示中任务相关信息的传输效率，为表示设计提供诊断依据。

Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.

</details>


### [31] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

TL;DR: 论文分析了Bradley-Terry损失函数的梯度问题，发现梯度范数受两个因素影响：预测奖励差异和表示距离。表示距离会导致大距离对梯度过强、小距离对梯度过弱的问题，为此提出了NormBT归一化方案来平衡表示驱动效应。


<details>
  <summary>Details</summary>
Motivation: Bradley-Terry损失函数是LLM对齐中奖励模型的核心目标函数，但其梯度存在表示距离导致的偏差问题。大表示距离的样本对会产生过强的梯度更新，而小距离的样本对即使排序错误也会获得很弱的更新，这影响了奖励模型的学习效果。

Method: 提出了NormBT方法，一种自适应成对归一化方案。该方法分析BT损失的逐样本梯度，发现梯度范数包含预测误差和表示距离两个分量。NormBT通过归一化来平衡表示驱动效应，使学习信号聚焦于预测误差，是一种轻量级的即插即用改进方案。

Result: 在各种LLM骨干网络和数据集上，NormBT都能一致提升奖励模型性能。在RewardBench的推理类别上取得了超过5%的显著提升，该类别包含大量小距离样本对。

Conclusion: 这项工作揭示了广泛使用的BT目标函数的关键局限性，并提供了一个简单有效的修正方案。NormBT通过平衡表示距离效应，使奖励模型能够更好地学习细粒度区分，特别是在小距离样本对上的表现得到显著改善。

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [32] [Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry](https://arxiv.org/abs/2512.06347)
*Naoki Yoshida,Isao Ishikawa,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 论文证明在师生框架下，机器学习模型的插值器泛化误差在训练样本超过特定阈值后会变为0


<details>
  <summary>Details</summary>
Motivation: 理解大规模模型（如深度神经网络）的高泛化能力是机器学习理论的核心开放问题。虽然近期理论将这种现象归因于SGD向良好泛化解的隐式偏置，但实证证据表明这主要源于模型本身的特性，特别是随机采样的插值器也能有效泛化。

Method: 在师生框架下，利用代数几何工具数学刻画参数空间中插值器集合的几何结构，证明随机采样插值器的泛化误差特性。

Result: 证明随机采样插值器的泛化误差在训练样本数量超过由参数空间中插值器集合几何结构决定的阈值后会精确变为0。

Conclusion: 模型本身的几何特性（而非优化算法）是决定泛化能力的关键因素，这为理解大规模模型的优异泛化性能提供了新的理论视角。

Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.

</details>


### [33] [LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing](https://arxiv.org/abs/2512.06351)
*Zhiying Yang,Fang Liu,Wei Zhang,Xin Lou,Malcolm Yoke Hean Low,Boon Ping Gan*

Main category: cs.LG

TL;DR: LUCA是一个结合大语言模型和图神经网络的强化学习框架，用于碳感知柔性作业车间调度，在保持相同排放水平下显著降低完工时间。


<details>
  <summary>Details</summary>
Motivation: 解决智能制造系统中动态和可持续调度的挑战，在考虑碳排放的同时优化调度性能。

Method: 通过图神经网络和大语言模型融合，结合精心设计的提示策略，生成捕捉调度状态结构特征和上下文语义的融合嵌入，然后由深度强化学习策略网络生成实时调度决策。

Result: 在合成数据集上，相比最佳对比算法平均降低4.1%的完工时间（最高达12.2%），同时保持相同排放水平；在公共数据集上，完工时间和排放都有额外改善。

Conclusion: LUCA是智能制造中碳感知调度的有效实用解决方案，能够同时优化完工时间和碳排放目标。

Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.

</details>


### [34] [DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction](https://arxiv.org/abs/2512.06356)
*Yifan Song,Fenglin Yu,Yihong Luo,Xingjian Tao,Siya Qiu,Kai Han,Jing Tang*

Main category: cs.LG

TL;DR: DDFI提出了一种结合特征传播和图掩码自编码器的多样化、分布感知缺失特征补全方法，解决了传统特征传播在非全连接图、过平滑和归纳任务分布偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 现实图中节点特征常不完整（如用户属性部分隐私），导致GNN性能下降。传统特征传播方法存在三个问题：1）对非全连接图效果不佳；2）补全特征存在过平滑；3）只适用于直推任务，忽视归纳任务中的特征分布偏移。

Method: DDFI结合特征传播和图掩码自编码器：1）提出共标签链接算法，随机连接训练集中相同标签节点以增强多连通分量图的性能；2）在推理阶段采用两步表示生成过程，先通过特征传播补全特征，再通过完整MAE重构特征以减少分布偏移并增强特征多样性。

Result: 在六个公共数据集和新收集的Sailing数据集（包含自然缺失特征）上实验表明，DDFI在直推和归纳设置下均优于现有方法。

Conclusion: DDFI通过结合特征传播和图掩码自编码器，有效解决了图中缺失特征补全的关键问题，特别是在非全连接图和归纳任务场景下表现优异。

Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.

</details>


### [35] [Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction](https://arxiv.org/abs/2512.06357)
*Tony Sallooma,Okyay Kaynak,Xinbo Yub,Wei He*

Main category: cs.LG

TL;DR: 提出基于PID控制思想的方法来提升神经网络在多步时间序列预测中的准确性，同时保持系统复杂度基本不变


<details>
  <summary>Details</summary>
Motivation: 多步时间序列预测在工业决策中很重要，但神经网络结构的复杂性会影响预测精度，需要一种既能提升性能又不显著增加复杂度的方法

Method: 受PID控制方法启发，在每个时间步对预测值进行PID修正，使其更接近真实值。该方法作为后处理增强器应用于现有神经网络模型

Result: 在水需求预测和能源消耗预测两个案例中，应用PID增强器后，预测精度显著提升，同时系统复杂度基本保持不变

Conclusion: PID增强方法能有效提升神经网络在多步周期性时间序列预测中的准确性，且不显著增加系统复杂度，具有广泛适用性

Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.

</details>


### [36] [Optimizing Optimizers for Fast Gradient-Based Learning](https://arxiv.org/abs/2512.06370)
*Jaerin Lee,Kyoung Mu Lee*

Main category: cs.LG

TL;DR: 提出自动化优化器设计的理论框架，将优化器设计问题转化为最大化瞬时损失下降的凸优化问题，能够自动推导出最优优化器及其超参数。


<details>
  <summary>Details</summary>
Motivation: 当前梯度学习中的优化器设计主要依赖经验和试错，缺乏系统性的理论基础。本文旨在为自动化优化器设计建立理论框架，使优化器设计更加系统化和自动化。

Method: 基于贪心原则，将优化器设计问题形式化为最大化瞬时损失下降。将优化器视为将梯度信号转换为参数运动的函数，将问题简化为在优化器空间上的凸优化问题。在不同约束下求解这些问题。

Result: 该方法不仅能够推导出多种流行优化器的闭式解，还能自动确定这些优化器针对具体问题的最优超参数。实现了根据训练过程中收集的梯度统计信息来系统化设计优化器和调整超参数。

Conclusion: 建立了自动化优化器设计的理论基础，实现了"优化的优化"，能够在训练过程中动态进行优化器设计和超参数调整，为梯度学习提供了系统化的优化器设计方法。

Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.

</details>


### [37] [RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs](https://arxiv.org/abs/2512.06392)
*Runlong Zhou,Lefan Zhang,Shang-Chen Wu,Kelvin Zou,Hanzhi Zhou,Ke Ye,Yihao Feng,Dong Yin,Alex Guillen Garcia,Dmytro Babych,Rohit Chatterjee,Matthew Hopkins,Xiang Kong,Chang Lan,Lezhi Li,Yiping Ma,Daniele Molinari,Senyu Tong,Yanchao Sun,Thomas Voice,Jianyu Wang,Chong Wang,Simon Wang,Floris Weers,Yechen Xu,Guolin Yin,Muyang Yu,Yi Zhang,Zheng Zhou,Danyang Zhuo,Ruoming Pang,Cheng Leong*

Main category: cs.LG

TL;DR: RLAX是一个在TPU上可扩展的强化学习框架，采用参数服务器架构，通过系统优化和新的数据对齐技术，显著提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为提升大语言模型推理能力的主流方法，但需要可扩展且能处理训练中断的框架来支持大规模训练。

Method: 采用参数服务器架构，主训练器定期推送更新的模型权重到参数服务器，推理工作节点拉取最新权重生成新的rollout。引入系统技术实现可扩展和可中断的RL训练，并开发新的数据集整理和对齐技术。

Result: 在1024个v5p TPU上，仅用12小时48分钟就将QwQ-32B模型的pass@8准确率提升了12.8%，同时在训练中断时保持鲁棒性。

Conclusion: RLAX是一个高效、可扩展的强化学习框架，能够显著加速大语言模型的训练收敛并提升模型质量，同时具备处理训练中断的鲁棒性。

Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.

</details>


### [38] [Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator](https://arxiv.org/abs/2512.06417)
*Yifan Sun,Lei Cheng,Jianlong Li,Peter Gerstoft*

Main category: cs.LG

TL;DR: Hankel-FNO：基于傅里叶神经算子的高效水下声学制图方法，结合声传播知识和地形数据，在保持高计算速度的同时实现高精度预测


<details>
  <summary>Details</summary>
Motivation: 传统水下声学制图方法依赖计算昂贵的数值求解器，无法满足大规模或实时应用需求。现有的深度学习替代模型存在固定分辨率限制或依赖显式偏微分方程公式等问题，限制了其适用性和泛化能力。

Method: 提出Hankel-FNO模型，基于傅里叶神经算子框架，结合声传播知识和海底地形数据，实现高效准确的水下声学制图。

Result: Hankel-FNO在速度上优于传统求解器，在精度上超越数据驱动替代方法，尤其在长距离预测中表现突出。模型能够适应不同环境和声源设置，仅需少量微调。

Conclusion: Hankel-FNO为水下声学制图提供了一种高效准确的解决方案，克服了传统方法和现有深度学习模型的局限性，具有良好的泛化能力和实际应用价值。

Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.

</details>


### [39] [A new initialisation to Control Gradients in Sinusoidal Neural network](https://arxiv.org/abs/2512.06427)
*Andrea Combette,Antoine Venaille,Nelly Pustelnik*

Main category: cs.LG

TL;DR: 提出一种针对SIREN等正弦激活函数网络的新初始化方法，通过控制梯度和预激活分布来改善训练稳定性和泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有初始化策略对梯度爆炸/消失问题的理论理解不足，特别是对于SIREN等正弦激活函数网络，需要更精确的初始化方法来控制梯度缩放和训练动态

Method: 通过预激活分布收敛和雅可比矩阵序列方差获得固定点，推导出参数初始化的闭式表达式，控制梯度并实现预激活消失，防止不适当频率的出现

Result: 新初始化方法在函数拟合和图像重建任务中一致优于原始SIREN方案和其他基线方法，包括物理信息神经网络任务

Conclusion: 提出的初始化策略通过精确控制梯度和预激活分布，显著改善了SIREN网络的训练稳定性和泛化性能，并通过NTK框架揭示了初始化对训练动态的强影响

Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.

</details>


### [40] [Neural expressiveness for beyond importance model compression](https://arxiv.org/abs/2512.06440)
*Angelos-Christos Maroudis,Sotirios Xydis*

Main category: cs.LG

TL;DR: 提出名为"表达力"的新模型压缩准则，基于神经元激活重叠来评估其信息重分配能力，与权重重要性不同，该准则与网络初始化状态相关，可实现数据无关的剪枝策略。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法主要依赖权重的重要性，但作者认为神经元或滤波器组的"表达力"——即基于激活重叠有效重分配信息的能力——是更根本的压缩准则，且与学习状态无关，为"何时剪枝"问题提供新基础。

Method: 提出"表达力"准则，衡量神经元或神经元组基于激活重叠的信息重分配能力。该准则可通过任意数据或有限代表性样本近似，支持数据无关策略。还可与重要性剪枝策略混合使用，形成互补。

Result: 在参数压缩比方面比基于权重的方法获得高达10倍的额外增益，平均性能下降仅1%。在YOLOv8上实现46.1%的MACs减少，移除55.4%参数，在COCO数据集上目标检测的mAP50-95提高3%。

Conclusion: 表达力准则为模型压缩提供了新的基础，与网络初始化状态相关且数据无关，可独立使用或与重要性剪枝混合，在压缩效率和性能方面均优于现有方法。

Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.

</details>


### [41] [BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination](https://arxiv.org/abs/2512.06457)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: BitStopper：一种无需稀疏预测器的细粒度算法-架构协同设计，通过位级稀疏推测和异步处理，显著提升Transformer加速器的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的大语言模型存在二次计算成本问题，动态稀疏注意力虽然能缓解但硬件效率受限，需要额外的预测阶段和大量内存访问。需要设计更高效的解决方案。

Method: 1. 位串行使能阶段融合（BESF）：重用并最小化内存访问，逐步终止无关token，将预测阶段合并到执行阶段；2. 轻量自适应token选择（LATS）：与位级稀疏推测协同工作；3. 位级异步处理（BAP）：在位粒度内存获取期间提高计算利用率；4. 精心设计的架构实现理论复杂度到实际性能的转化。

Result: 相比最先进的Transformer加速器，BitStopper在Sanger上实现2.03倍加速，SOFA上实现1.89倍加速，同时在能效上分别提升2.4倍和2.1倍。

Conclusion: BitStopper通过算法-架构协同设计，无需稀疏预测器，有效解决了动态稀疏注意力的硬件效率问题，显著提升了Transformer加速器的性能和能效。

Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.

</details>


### [42] [Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control](https://arxiv.org/abs/2512.06471)
*Nathan P. Lawrence,Ali Mesbah*

Main category: cs.LG

TL;DR: 本文分析了基于最优控制的目标条件强化学习，推导了经典二次目标与目标条件奖励之间的最优性差距，并将状态估计与概率奖励联系起来，验证了目标条件策略在非线性不确定环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习训练智能体最大化到达目标状态的概率，但传统密集奖励（如二次型）在此任务中可能失效。本文旨在从最优控制角度分析目标条件设置，解释为什么经典奖励方法会失败，以及为什么目标条件方法更有效。

Method: 基于最优控制理论，推导了经典二次目标与目标条件奖励之间的最优性差距。在部分可观测马尔可夫决策过程中，将状态估计与概率奖励联系起来，使目标条件奖励适用于双重控制问题。使用强化学习和预测控制技术在非线性不确定环境中验证方法。

Result: 理论分析揭示了目标条件强化学习成功的原因，以及经典密集奖励失败的原因。在部分可观测环境中，状态估计与概率奖励的连接使目标条件奖励特别适合双重控制问题。实验验证了目标条件策略在非线性不确定环境中的优势。

Conclusion: 目标条件强化学习从最优控制角度具有理论优势，特别适合部分可观测环境中的双重控制问题。该方法为理解目标条件RL的成功提供了理论依据，并为实际应用提供了指导。

Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.

</details>


### [43] [Optimizing LLMs Using Quantization for Mobile Execution](https://arxiv.org/abs/2512.06490)
*Agatsya Yadav,Renta Chintala Bhargavi*

Main category: cs.LG

TL;DR: 本文研究了使用4位后训练量化技术压缩大语言模型，使其能在移动设备上运行，成功将Llama 3.2 3B模型大小减少68.66%，并在Android设备上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但其庞大的规模和计算需求阻碍了在资源受限的移动设备上的部署，需要寻找有效的压缩方法。

Method: 使用BitsAndBytes库和Hugging Face Transformers框架对Meta的Llama 3.2 3B模型进行4位后训练量化，然后通过llama.cpp工具转换为GGUF格式进行移动端优化推理。

Result: 4位量化使模型大小减少了68.66%，量化后的模型能够在Android设备上成功执行推理任务，通过Termux环境和Ollama框架验证了可行性。

Conclusion: 4位精度的后训练量化结合GGUF等移动优化格式，为在移动设备上部署能力强大的大语言模型提供了实用途径，平衡了模型大小和性能。

Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.

</details>


### [44] [Diagnosis-based mortality prediction for intensive care unit patients via transfer learning](https://arxiv.org/abs/2512.06511)
*Mengqi Xu,Subha Maity,Joel Dubin*

Main category: cs.LG

TL;DR: 该研究评估了在ICU诊断特异性死亡率预测中使用迁移学习的方法，发现迁移学习优于仅使用诊断特异性数据或APACHE IVa评分的模型，并建议Youden截断值比传统0.5更适合作为决策阈值。


<details>
  <summary>Details</summary>
Motivation: ICU中危重病的潜在病因在不同诊断间差异很大，但现有的预测模型未能系统性地考虑诊断异质性。需要开发能够处理诊断特异性差异的死亡率预测方法。

Method: 使用迁移学习方法进行诊断特异性死亡率预测，应用基于GLM和XGBoost的模型，在eICU协作研究数据库上进行评估。比较了迁移学习模型与仅使用诊断特异性数据、APACHE IVa评分以及合并数据训练的模型。

Result: 迁移学习模型在性能上持续优于仅使用诊断特异性数据的模型和APACHE IVa评分，同时比合并数据训练的模型具有更好的校准性。Youden截断值比传统0.5阈值更适合作为决策标准，且迁移学习在各种截断标准下都保持稳定的高预测性能。

Conclusion: 迁移学习是处理ICU诊断异质性的有效方法，能够提高死亡率预测的准确性和校准性。Youden截断值应作为更合适的决策阈值，迁移学习在各种决策标准下都能保持稳健的性能。

Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.

</details>


### [45] [Hierarchical geometric deep learning enables scalable analysis of molecular dynamics](https://arxiv.org/abs/2512.06520)
*Zihan Pengmei,Spencer C. Guo,Chatipat Lorpaiboon,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络的方法，通过局部信息聚合减少内存和计算需求，实现对大规模生物分子系统（数千残基）的分子动力学模拟分析


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟产生原子级轨迹，但缺乏定量描述符时分析困难。传统图神经网络处理大规模生物分子系统（数百残基以上）时面临内存、计算和长程相互作用捕捉的挑战

Method: 开发局部信息聚合方法，在保持原子细节的同时减少内存和运行时需求，使图神经网络能够处理数千残基的蛋白质-核酸复合物系统

Result: 该方法可在单GPU上数分钟内分析数千残基的系统；对于数百残基的系统，提高了性能和可解释性

Conclusion: 局部信息聚合方法解决了图神经网络分析大规模生物分子动力学模拟的瓶颈，为复杂系统的原子级分析提供了高效工具

Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.

</details>


### [46] [Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning](https://arxiv.org/abs/2512.06533)
*Ming Chen,Sheng Tang,Rong-Xi Tan,Ziniu Li,Jiacheng Chen,Ke Xue,Chao Qian*

Main category: cs.LG

TL;DR: 该论文提出使用强化学习（RL）解决解码式回归中离散token目标与连续数值不对齐的问题，通过序列级奖励提升预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解码式回归将回归任务转化为序列生成任务，但现有方法存在离散token级目标（如交叉熵）与连续数值不对齐的问题，token级约束难以捕捉目标值的全局大小，限制了预测精度和泛化能力。

Method: 将生成过程建模为马尔可夫决策过程，使用序列级奖励来强制全局数值一致性，具体采用ReMax和GRPO强化学习方法。

Result: 在表格回归和代码指标回归任务上的实验表明，该方法（特别是ReMax和GRPO）持续优于最先进的token级基线和传统回归头，展示了引入序列级信号的优势。

Conclusion: 强化学习显著提高了采样效率和预测精度，确立了解码式回归作为通用数值预测的稳健准确范式。

Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.

</details>


### [47] [Deep Manifold Part 2: Neural Network Mathematics](https://arxiv.org/abs/2512.06563)
*Max Y. Ma,Gen-Hua Shi*

Main category: cs.LG

TL;DR: 论文提出从流形几何、不动点理论和边界条件迭代的角度重新理解神经网络，认为神经网络不是从固定点开始，而是通过残差驱动迭代构建不动点区域，从而解释能力涌现现象。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络理论通常基于固定坐标和算子的视角，无法充分解释真实世界数据带来的复杂性（强数据复杂度、近乎无限的规模、小批量碎片化）以及训练动态产生的学习复杂度（节点覆盖变化、曲率积累、可塑性涨落）。这些力量限制了可学习性，需要新的理论框架来理解神经网络的能力涌现机制。

Method: 通过堆叠分段流形、不动点理论和边界条件迭代来建立神经网络的全局方程。移除固定坐标和算子的传统视角，将神经网络视为由流形复杂度、高阶非线性和边界条件塑造的可学习数值计算。

Result: 提出神经网络不是从固定点开始，而是通过残差驱动迭代构建不动点区域，只有当不动点区域稳定时能力才会涌现。这一视角阐明了单一模型在几何和数据诱导可塑性下的局限性。

Conclusion: 基于几何、代数、不动点和真实数据复杂性的理论框架，提出需要将流形复杂度分布在多个弹性模型中的架构和联邦系统，形成连贯的世界建模框架。

Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.

</details>


### [48] [QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling](https://arxiv.org/abs/2512.06582)
*Isaac Kofi Nti*

Main category: cs.LG

TL;DR: QL-LSTM通过参数共享统一门控机制和分层门控递归结构，在减少48%参数的同时保持竞争性准确率，解决了传统RNN参数冗余和长距离信息保留问题。


<details>
  <summary>Details</summary>
Motivation: 传统LSTM和GRU等循环神经网络存在两个核心限制：1) 门控机制中参数冗余，每个门都有独立的参数矩阵；2) 在长距离时间跨度上信息保留能力下降。需要设计更高效的循环架构来解决这些问题。

Method: 提出了QL-LSTM架构，包含两个独立组件：1) 参数共享统一门控机制(PSUG)，用单一共享权重矩阵替代所有门特定的变换，减少约48%参数；2) 分层门控递归与加法跳跃连接(HGR-ASC)，添加无乘法路径，改善长距离信息流并减少遗忘门退化。

Result: 在IMDB数据集上进行情感分类评估，与LSTM、GRU和BiLSTM基准模型比较。QL-LSTM在使用显著更少参数的情况下实现了竞争性准确率。但当前原型受限于循环模型的固有顺序性，尚未实现实际运行时间改进。

Conclusion: QL-LSTM通过创新的参数共享和分层递归设计，有效解决了传统RNN的参数冗余和长距离依赖问题，在保持性能的同时大幅减少参数数量，但需要进一步内核级优化才能实现运行时间改进。

Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.

</details>


### [49] [On fine-tuning Boltz-2 for protein-protein affinity prediction](https://arxiv.org/abs/2512.06592)
*James King,Lewis Cornwall,Andrei Cristian Nica,James Day,Aaron Sim,Neil Dalchau,Lilly Wollman,Joshua Meyers*

Main category: cs.LG

TL;DR: 将蛋白质-配体亲和力预测模型Boltz-2适配用于蛋白质-蛋白质亲和力预测，但结构模型表现不如序列模型，结合两者可互补提升


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白质-蛋白质结合亲和力对于理解分子相互作用和设计治疗方法至关重要。需要评估结构基模型在蛋白质-蛋白质亲和力预测中的表现

Method: 将最先进的结构基蛋白质-配体亲和力预测器Boltz-2适配为蛋白质-蛋白质亲和力回归模型(Boltz-2-PPI)，在TCR3d和PPB-affinity两个数据集上评估，并与序列基模型比较，还尝试结合结构基和序列基嵌入

Result: 尽管结构准确性高，但Boltz-2-PPI在小规模和较大规模数据情况下都表现不如序列基替代方案。结合Boltz-2-PPI嵌入与序列基嵌入可产生互补改进，特别是对于较弱的序列模型，表明序列基和结构基模型学习了不同的信号

Conclusion: 当前结构基表示方法不适合高性能亲和力预测，结果反映了与结构数据训练相关的已知偏差，序列基和结构基模型学习不同信号，结合两者可互补提升

Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.

</details>


### [50] [A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs](https://arxiv.org/abs/2512.06607)
*Humzah Merchant,Bradford Levy*

Main category: cs.LG

TL;DR: 提出一种通过调整大型基础模型logits来消除前瞻性偏差的方法，使用一对小型专用模型指导生成，避免金融预测中的回测问题


<details>
  <summary>Details</summary>
Motivation: 在金融预测中应用LLMs面临前瞻性偏差挑战，因为模型训练使用了长时间序列数据，而重新训练前沿模型以设置特定知识截止点成本过高

Method: 在推理时通过调整大型基础模型的logits来指导生成，使用一对小型专用模型——一个微调于需要遗忘的信息，另一个微调于需要保留的信息

Result: 该方法有效移除了字面和语义知识，纠正了偏差，并且优于先前的方法

Conclusion: 提出了一种快速、有效且低成本的替代方案，解决了金融预测中LLMs的前瞻性偏差问题，使回测成为可能

Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.

</details>


### [51] [Vector Quantization using Gaussian Variational Autoencoder](https://arxiv.org/abs/2512.06609)
*Tongda Xu,Wendi Zheng,Jiajun He,Jose Miguel Hernandez-Lobato,Yan Wang,Ya-Qin Zhang,Jie Tang*

Main category: cs.LG

TL;DR: 提出Gaussian Quant (GQ)方法，将高斯VAE转换为VQ-VAE而无需额外训练，通过理论证明和实验验证其优于现有VQ-VAE方法


<details>
  <summary>Details</summary>
Motivation: VQ-VAE由于离散化过程难以训练，需要一种更简单有效的离散化方法

Method: GQ方法：1) 生成随机高斯噪声作为码本；2) 寻找与后验均值最接近的噪声；3) 提出目标散度约束(TDC)来训练高斯VAE以优化GQ效果

Result: GQ在UNet和ViT架构上均优于VQGAN、FSQ、LFQ、BSQ等现有VQ-VAE方法，TDC也改进了TokenBridge等高斯VAE离散化方法

Conclusion: GQ提供了一种简单有效的方法将高斯VAE转换为VQ-VAE，无需额外训练，在理论和实践上都有良好表现

Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.

</details>


### [52] [Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study](https://arxiv.org/abs/2512.06630)
*Chi-Sheng Chen,Xinyu Zhang,Rong Fu,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: 量子时间卷积神经网络(QTCNN)结合经典时间编码器和参数高效的量子卷积电路，用于股票收益预测，在JPX东京交易所数据集上实现了0.538的夏普比率，比最佳经典基线提升约72%。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型在处理噪声输入、市场机制转换和有限泛化能力方面存在困难。量子机器学习在复杂、噪声、高度动态的金融环境中具有增强股票市场预测的潜力。

Method: 提出量子时间卷积神经网络(QTCNN)：1) 经典时间编码器从序列技术指标中提取多尺度模式；2) 参数高效的量子卷积电路利用量子叠加和纠缠增强特征表示并抑制过拟合。

Result: 在JPX东京交易所数据集上进行综合基准测试，通过构建多空组合并使用样本外夏普比率作为主要性能指标。QTCNN实现0.538的夏普比率，比最佳经典基线提升约72%。

Conclusion: 量子增强的预测模型QTCNN在量化金融中具有实际应用潜力，能够实现稳健的决策制定，显著优于传统方法。

Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.

</details>


### [53] [The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News](https://arxiv.org/abs/2512.06638)
*Isha Karn,David Jensen*

Main category: cs.LG

TL;DR: 现有假新闻检测基准数据集（GossipCop和PolitiFact）的图结构过于简单，无法有效评估GNN模型的结构建模能力，导致GNN与简单的MLP性能相近。


<details>
  <summary>Details</summary>
Motivation: 当前假新闻检测领域广泛使用GNN模型来建模新闻传播结构，但缺乏对基准数据集是否真正需要结构建模能力的验证。研究发现常用数据集可能无法有效评估GNN的结构建模优势。

Method: 系统比较5种GNN架构与结构无关的MLP（使用相同节点特征）；通过特征打乱和边随机化的控制实验分离结构和特征的贡献；分析数据集的图拓扑结构特征。

Result: MLP与GNN性能相当或接近（差距1-2%，置信区间重叠）；特征打乱导致性能崩溃，边随机化性能稳定；超过75%节点距离根节点仅一跳，结构多样性不足；在合成数据集上GNN显著优于MLP。

Conclusion: 常用基准数据集无法有效测试结构建模能力，需要开发具有更丰富、更多样图拓扑的数据集来评估GNN在假新闻检测中的真正价值。

Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.

</details>


### [54] [Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network](https://arxiv.org/abs/2512.06648)
*Xiao Li*

Main category: cs.LG

TL;DR: 提出基于卷积神经网络的中国A股上市公司财务舞弊检测框架，通过将面板数据转换为类图像表示来捕捉跨截面和时间模式，实现提前预测，并在准确性、鲁棒性和预警性能上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 上市公司财务舞弊屡禁不止，传统检测方法存在局限性：统计模型难以处理非线性特征交互，机器学习模型缺乏可解释性，且现有方法多基于当年数据判断当年舞弊，时效性不足。

Method: 设计特征工程方案，将公司年度面板数据转换为类图像表示，使用卷积神经网络捕捉跨截面和时间模式，实现提前预测；采用局部解释技术从实体、特征和时间三个维度分析模型。

Result: CNN在准确性、鲁棒性和预警性能上优于逻辑回归和LightGBM；发现偿债能力、比率结构、治理结构和内部控制是舞弊的通用预测因子，环境指标主要在高污染行业重要；舞弊公司表现出集中在短期窗口的异质模式。

Conclusion: 基于CNN的财务舞弊检测框架能有效提前识别舞弊，通过阈值调整适应高风险场景；可解释性分析揭示了舞弊的关键驱动因素，为监管和审计提供了实用工具。

Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.

</details>


### [55] [Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts](https://arxiv.org/abs/2512.06652)
*Xiaolei Lu,Shamim Nemati*

Main category: cs.LG

TL;DR: 该论文提出了AdaTTT框架，通过自适应测试时训练来改善ICU患者有创机械通气需求预测模型的跨机构泛化能力，利用自监督学习和原型学习来应对电子健康记录系统的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: ICU患者有创机械通气需求预测对及时干预和资源分配至关重要，但由于不同机构在患者群体、临床实践和电子健康记录系统方面的差异，导致预测模型在部署时出现领域偏移和泛化性能下降的问题。

Method: 提出了自适应测试时训练框架AdaTTT：1）推导了测试时预测误差的信息论界限；2）设计了自监督学习框架，包含重构和掩码特征建模两个前置任务，采用动态掩码策略强调对主任务重要的特征；3）结合原型学习和部分最优传输进行灵活的部分特征对齐，同时保持临床意义的患者表征。

Result: 在多中心ICU队列实验表明，该方法在不同测试时适应基准上取得了有竞争力的分类性能。

Conclusion: AdaTTT框架通过自适应测试时训练有效改善了ICU有创机械通气预测模型在跨机构部署时的泛化能力，为电子健康记录系统中的领域适应问题提供了有效解决方案。

Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.

</details>


### [56] [GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655)
*Jehyeok Yeon,Federico Cinus,Yifan Wu,Luca Luceri*

Main category: cs.LG

TL;DR: 提出GSAE方法，通过图正则化稀疏自编码器学习分布式安全表示，实现运行时安全引导，有效拒绝有害内容同时保持良性查询的实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临安全挑战，容易被对抗提示和越狱攻击操纵生成有害内容。现有防御方法通常将安全视为单一潜在特征，但研究表明抽象概念（如拒绝和时序性）分布在多个特征中而非孤立于一个特征。

Method: 引入图正则化稀疏自编码器(GSAEs)，在SAE基础上增加神经元共激活图的拉普拉斯平滑惩罚项，学习平滑的分布式安全表示。采用两阶段门控机制，仅在检测到有害提示或续写时激活干预。

Result: GSAE引导平均达到82%的选择性拒绝率，显著优于标准SAE引导(42%)，同时在TriviaQA(70%)、TruthfulQA(65%)、GSM8K(74%)上保持强任务准确性。在LLaMA-3、Mistral、Qwen、Phi等模型上表现鲁棒，对GCG、AutoDAN等越狱攻击保持≥90%的有害内容拒绝率。

Conclusion: GSAE能够有效学习分布式安全表示，实现自适应安全引导，在拒绝有害内容的同时保持良性查询的实用性，为LLM安全防御提供了新方法。

Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.

</details>


### [57] [Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods](https://arxiv.org/abs/2512.06665)
*Panagiota Kiourti,Anu Singh,Preeti Duraipandian,Weichao Zhou,Wenchao Li*

Main category: cs.LG

TL;DR: 该论文挑战了当前忽视模型输出差异的特征归因鲁棒性概念，提出了新的相似输入定义、鲁棒性指标和基于GAN的生成方法，并进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前特征归因方法的鲁棒性评估主要关注输入扰动下的归因图变化，但忽视了模型输出本身的差异，这可能导致对归因方法鲁棒性的不准确评估。

Method: 提出了新的相似输入定义、新的鲁棒性指标，以及基于生成对抗网络（GAN）生成这些输入的新方法。对现有指标和最先进的归因方法进行了全面评估。

Result: 研究发现需要更客观的指标来揭示归因方法本身的弱点，而不是神经网络的弱点，从而提供更准确的归因方法鲁棒性评估。

Conclusion: 该研究强调了重新思考特征归因鲁棒性评估的必要性，提出了新的评估框架，能够更准确地评估归因方法的鲁棒性，而不是混淆神经网络本身的脆弱性。

Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.

</details>


### [58] [The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification](https://arxiv.org/abs/2512.06666)
*Urav Maniar*

Main category: cs.LG

TL;DR: 该研究探讨时间序列分类中准确率与计算效率的权衡，通过结合两种高效算法(Hydra和Quant)构建六种集成配置，在大型数据集上测试性能，发现当前元学习策略难以有效利用算法互补性。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类面临准确率与计算效率的根本权衡。虽然像HIVE-COTE 2.0这样的综合集成能达到最先进的准确率，但其在UCR基准测试上340小时的训练时间使其在大规模数据集上不实用。研究旨在探索是否通过结合两种来自互补范式的高效算法，能够在保持计算可行性的同时获得集成效益。

Method: 结合Hydra(竞争卷积核)和Quant(分层区间分位数)两种高效算法，构建六种集成配置。在10个大规模MONSTER数据集(7,898到1,168,774个训练实例)上评估性能。分析预测组合集成与特征拼接方法的差异，评估元学习优化差距。

Result: 最强配置将平均准确率从0.829提高到0.836，在10个数据集中成功7个。但预测组合集成仅捕获了11%的理论oracle潜力，显示出显著的元学习优化差距。特征拼接方法通过学习新决策边界超越了oracle界限，预测级互补性与集成增益呈中等相关性。

Conclusion: 核心发现：挑战已从确保算法不同转变为学习如何有效组合它们。当前元学习策略难以利用oracle分析确认存在的互补性。改进的组合策略可能使集成增益在多样化时间序列分类应用中翻倍或三倍。

Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.

</details>


### [59] [GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning](https://arxiv.org/abs/2512.06678)
*Shrihari Sridharan,Deepak Ravikumar,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: GradientSpace：一种在完整梯度空间中直接聚类样本的框架，通过在线SVD算法识别潜在技能，训练专门的LoRA专家和轻量级路由器，在推理时选择最佳专家，优于现有聚类方法和微调技术。


<details>
  <summary>Details</summary>
Motivation: 指令调优是适配大语言模型的关键步骤，但现实世界数据集通常异构，导致梯度干扰问题。现有方法基于语义相似性聚类或随机投影梯度降维，无法准确捕捉数据对模型参数的影响，且依赖专家集成导致推理延迟高。

Method: 提出GradientSpace框架：1）在完整梯度空间中直接聚类样本；2）引入基于LoRA梯度的在线SVD算法识别潜在技能，避免存储所有样本梯度；3）为每个聚类训练专门的LoRA专家；4）训练轻量级路由器在推理时选择最佳专家。

Result: 在数学推理、代码生成、金融和创意写作任务上的实验表明：1）GradientSpace能产生连贯的专家专业化；2）相比现有聚类方法和微调技术获得一致准确率提升；3）单专家路由优于先前工作的专家集成，同时显著降低推理延迟。

Conclusion: GradientSpace通过直接在完整梯度空间聚类、识别潜在技能、训练专门化专家和轻量级路由器，有效解决了梯度干扰问题，在多个任务上优于现有方法，同时降低了推理延迟。

Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.

</details>


### [60] [State Diversity Matters in Offline Behavior Distillation](https://arxiv.org/abs/2512.06692)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文发现离线行为蒸馏（OBD）中存在原始数据集与蒸馏数据集之间的错位问题，提出状态多样性在OBD中比状态质量更重要，并设计了状态密度加权（SDW）OBD算法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线行为蒸馏（OBD）能够将大量离线RL数据压缩成紧凑的合成行为数据集，为高效策略训练提供了有前景的方法。然而，研究发现原始高质量数据集不一定能产生优质的合成数据集，存在原始数据集与蒸馏数据集之间的错位问题，需要深入理解这种错位的本质。

Method: 1. 通过实证分析不同训练损失水平下的策略性能，发现状态多样性在训练损失较大时比状态质量更重要；2. 理论分析将状态质量和多样性分别与减少关键误差和周围误差相关联，证明在关键误差较大时周围误差对策略性能影响更大；3. 提出状态密度加权（SDW）OBD算法，通过使用状态密度倒数的权重来强调状态多样性，将更多样化的状态信息蒸馏到合成数据中。

Result: 在多个D4RL数据集上的广泛实验证实，当原始数据集状态多样性有限时，SDW算法能显著提升OBD性能。理论分析和实验结果表明，在OBD场景中状态多样性比状态质量更为关键。

Conclusion: 本文揭示了离线行为蒸馏中原始数据集与合成数据集之间的错位问题，证明了状态多样性在OBD中的重要性，并提出简单有效的SDW-OBD算法来改善蒸馏效果，特别是在原始数据集状态多样性有限的情况下。

Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.

</details>


### [61] [Mitigating Barren plateaus in quantum denoising diffusion probabilistic models](https://arxiv.org/abs/2512.06695)
*Haipeng Cao,Kaining Zhang,Dacheng Tao,Zhaofeng Su*

Main category: cs.LG

TL;DR: 量子去噪扩散概率模型（QuDDPM）存在贫瘠高原问题，本文提出改进方案通过使用远离Haar分布的输入分布来解决该问题，提高训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 量子生成模型利用量子叠加和纠缠特性提升学习效率，QuDDPM作为量子生成学习框架表现出色，但研究发现其存在贫瘠高原问题，严重影响模型性能，需要解决这一问题以实现可扩展的高效量子生成学习。

Method: 通过理论分析和实验验证确认原始QuDDPM存在贫瘠高原问题，然后提出改进的QuDDPM，使用保持与Haar分布一定距离的分布作为去噪过程的输入，确保更好的可训练性。

Result: 实验结果表明，改进方法有效缓解了贫瘠高原问题，生成了更高质量的样本，为可扩展和高效的量子生成学习铺平了道路。

Conclusion: 原始QuDDPM因使用2-design状态作为输入而存在贫瘠高原问题，改进方案通过调整输入分布有效解决了这一问题，显著提升了量子生成模型的训练效率和生成质量。

Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.

</details>


### [62] [Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models](https://arxiv.org/abs/2512.06702)
*Xiangjun Meng,Zhongjian Wang*

Main category: cs.LG

TL;DR: 该论文为基于流的生成模型提供了可实现的误差分析工具，证明了在Wasserstein度量下，最优采样迭代复杂度与维度呈O(√d)关系，误差由两个独立部分控制。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对基于流的生成模型的误差分析工具，特别是关于采样迭代复杂度与维度依赖关系的理论保证。需要建立可实现的误差估计方法，以理解这些模型在不同维度下的性能表现。

Method: 通过分析流生成模型在Wasserstein度量下的误差，将误差分解为两部分：1）反向流推前映射的Lipschitz性（与维度无关）；2）局部离散化误差（与维度呈O(√d)关系）。这些分析基于Föllmer过程和1-整流流在Gaussian尾假设下的性质。

Result: 证明了基于流的生成模型的最优采样迭代复杂度为O(√d)，误差由两个可控制的部分组成。特别地，采样迭代复杂度与协方差算子迹的平方根呈线性关系，这与前向过程的平稳分布相关。

Conclusion: 该研究为流生成模型提供了实用的误差分析框架，揭示了采样复杂度与维度的平方根关系，为理解和改进这类模型的性能提供了理论指导，特别是在高维场景下的应用。

Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.

</details>


### [63] [A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations](https://arxiv.org/abs/2512.06708)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出多模态RUL框架，结合图像表示和时间频率表示，使用多分支架构和注意力机制，在减少训练数据需求的同时提升性能，并引入可解释性技术增强模型透明度。


<details>
  <summary>Details</summary>
Motivation: 滚动轴承是机械故障的常见原因，现有RUL估计方法存在泛化能力差、鲁棒性不足、数据需求高和可解释性有限等问题，需要更有效的解决方案。

Method: 提出多模态RUL框架：1) 图像表示分支：使用Bresenham线算法将振动信号转换为图像；2) 时间频率表示分支：使用连续小波变换；3) 融合分支：结合特征后输入LSTM建模时序模式，使用多头注意力机制突出重要特征，最后线性层回归RUL。引入多模态层相关传播技术增强可解释性。

Result: 在XJTU-SY和PRONOSTIA基准数据集上验证，性能匹配或超越现有最佳方法，在未见工况下表现良好。训练数据需求减少28%（XJTU-SY）和48%（PRONOSTIA），模型具有强噪声鲁棒性，可视化确认了预测的可解释性和可信度。

Conclusion: 该多模态框架在减少数据需求的同时提升了RUL估计性能，结合可解释性技术增强了模型透明度，非常适合实际工业部署。

Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.

</details>


### [64] [A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting](https://arxiv.org/abs/2512.06714)
*Tony Salloom,Okyay Kaynak,Wei He*

Main category: cs.LG

TL;DR: 提出一种结合数据扩展和GRU-K-means混合模型的短期用水需求预测方法，显著降低极端点误差和模型复杂度


<details>
  <summary>Details</summary>
Motivation: 深度学习在短期用水需求预测中存在两个主要问题：1）模型参数过多导致复杂度高；2）极端点预测误差大。本文首次专门针对极端点误差问题提出解决方案。

Method: 1）数据扩展：在实际数据中插入虚拟数据以缓解极端点周围的非线性；2）提出新型深度学习模型：使用GRU处理时序关系，结合K-means无监督分类创建新特征，在减少参数的同时提高预测精度

Result: 1）模型复杂度降低至文献中方法的六分之一，同时保持相同精度；2）数据扩展方法使误差降低约30%；3）使用中国两个不同水厂的实际数据进行验证，证明了方法的有效性

Conclusion: 提出的数据扩展和GRU-K-means混合模型能有效解决短期用水需求预测中的极端点误差和模型复杂度问题，但数据扩展会增加训练时间

Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.

</details>


### [65] [Decoding Motor Behavior Using Deep Learning and Reservoir Computing](https://arxiv.org/abs/2512.06725)
*Tian Lan*

Main category: cs.LG

TL;DR: 提出ESNNet，结合CNN空间特征提取与ESN时序建模能力，用于EEG运动行为分类，在滑板技巧数据集上表现优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构（如EEGNet、DeepConvNet）能有效捕捉局部空间模式，但在建模长程时序依赖和非线性动态方面表现不足，限制了EEG解码性能。

Method: 将回声状态网络（ESN）集成到解码流程中，ESN构建高维稀疏连接循环储备池，擅长追踪时序动态，与CNN的空间表征能力互补，形成ESNNet模型。

Result: 在经PREP预处理、MNE-Python实现的滑板技巧EEG数据集上，ESNNet获得83.2%的受试者内准确率和51.3%的留一受试者交叉验证准确率，超越广泛使用的CNN基线方法。

Conclusion: ESNNet通过结合CNN的空间特征提取和ESN的时序建模优势，有效提升了EEG运动行为分类性能，为无创脑机接口解码提供了新思路。

Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals

</details>


### [66] [KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models](https://arxiv.org/abs/2512.06727)
*Sourjya Roy,Shrihari Sridharan,Surya Selvam,Anand Raghunathan*

Main category: cs.LG

TL;DR: KV CAR框架通过轻量级自编码器和相似性驱动重用机制，显著减少KV缓存内存占用，实现高达47.85%的压缩率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和上下文长度的增加，KV缓存在自回归解码过程中的内存需求成为主要瓶颈，限制了批处理大小和上下文窗口。

Method: KV CAR结合两种互补技术：1) 轻量级自编码器沿嵌入维度学习KV张量的紧凑表示，在存储前压缩并在检索时恢复；2) 相似性驱动重用机制识别相邻层间特定注意力头KV张量的重用机会。

Result: 在GPT-2和TinyLLaMA模型上的评估显示，KV CAR在Wikitext、C4、PIQA和Winogrande数据集上实现高达47.85%的KV缓存内存减少，对困惑度和零样本准确率影响最小。NVIDIA A40 GPU上的系统级测量显示减少的KV占用直接转化为更长的序列长度和更大的批处理大小。

Conclusion: KV CAR通过减少KV张量的维度和结构冗余，无需改变Transformer架构，有效实现了内存高效的大语言模型推理。

Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.

</details>


### [67] [Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data](https://arxiv.org/abs/2512.06730)
*Lin Yang,Xiang Li,Xin Ma,Xinxin Zhao*

Main category: cs.LG

TL;DR: 提出AR-SSVEP系统结合MACNN-BiLSTM模型，用于增强运动功能障碍患者的康复训练参与度和实时运动意图识别


<details>
  <summary>Details</summary>
Motivation: 运动功能障碍患者康复训练参与度低，传统SSVEP-BCI系统依赖外部视觉刺激设备，实际应用受限，且治疗师工作负担重

Method: 1) 设计基于HoloLens 2的四种EEG类别；2) 在CNN-BiLSTM架构基础上集成多头注意力机制(MACNN-BiLSTM)；3) 提取10个时频EEG特征，用CNN学习高级表征；4) 用BiLSTM建模序列依赖，多头注意力突出运动意图相关模式；5) 使用SHAP方法可视化EEG特征对决策的贡献

Result: 收集了7名健康受试者的EEG数据进行分析，提出的方法增强了模型可解释性

Conclusion: AR-SSVEP系统结合MACNN-BiLSTM模型能增强实时运动意图识别，支持运动障碍患者的康复恢复

Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.

</details>


### [68] [Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics](https://arxiv.org/abs/2512.06737)
*Nikhil Verma,Joonas Linnosmaa,Espinosa-Leal Leonardo,Napat Vajragupta*

Main category: cs.LG

TL;DR: ArcGD优化器在非凸基准函数和真实ML数据集上均优于Adam等先进优化器，表现出更好的泛化能力和抗过拟合特性


<details>
  <summary>Details</summary>
Motivation: 开发一种新的优化器，能够在高维非凸问题和真实机器学习任务中超越现有先进优化器（如Adam、AdamW、Lion、SGD），特别是在泛化能力和抗过拟合方面

Method: 提出ArcGD优化器，首先在具有挑战性的Rosenbrock函数上进行评估（2D到1000D，极端情况50,000D），消除学习率偏差；然后在CIFAR-10图像分类数据集上测试8种不同MLP架构，与Adam、AdamW、Lion、SGD进行比较

Result: 在Rosenbrock函数上，ArcGD在使用自身有效学习率时始终优于Adam；在CIFAR-10上，ArcGD在20,000次迭代时达到最高平均测试准确率50.7%，在8种架构中赢得或平局6种，且持续改进而不需要早停调优

Conclusion: ArcGD在几何压力测试和深度学习基准测试中表现优异，具有广泛适用性，且其变体可解释为Lion优化器的特例，揭示了不同优化方法之间的内在联系

Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.

</details>


### [69] [Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets](https://arxiv.org/abs/2512.06752)
*Chang Liu,Vivian Li,Linus Leong,Vladimir Radenkovic,Pietro Liò,Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 提出几何图U-Nets，通过层次化图粗化和细化学习蛋白质多尺度表征，在蛋白质折叠分类任务上优于现有几何GNN和Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 现有几何GNN和Transformer依赖消息传递，无法捕捉蛋白质功能所需的层次化相互作用（如全局结构域和长程变构调节），因此需要网络架构本身反映生物层次结构。

Method: 引入几何图U-Nets，通过递归粗化和细化蛋白质图来学习多尺度表征，形成层次化设计。

Result: 理论证明该层次化设计比标准几何GNN更具表达力；在蛋白质折叠分类任务上，几何U-Nets显著优于不变和等变基线方法。

Conclusion: 该工作为设计能够学习生物分子多尺度结构的几何深度学习架构提供了理论基础。

Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.

</details>


### [70] [Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship](https://arxiv.org/abs/2512.06758)
*Zilong Wang,Shuai Li*

Main category: cs.LG

TL;DR: 该论文提出了一种多级连续选择算法，在序列独裁假设下的双边匹配市场中实现了与已知下界匹配的遗憾上界O(Nlog(T)/Δ² + Klog(T)/Δ)。


<details>
  <summary>Details</summary>
Motivation: 在线双边匹配市场中，参与者通过多轮交互学习未知偏好。现有研究存在下界Ω(Nlog(T)/Δ² + Klog(T)/Δ)和上界O(Klog(T)/Δ²)之间的差距，范围从N到K。不清楚是下界还是上界需要改进。

Method: 提出多级连续选择算法，在序列独裁假设下工作。该算法通过多级选择过程优化匹配，旨在达到与已知下界匹配的遗憾上界。

Result: 算法实现了O(Nlog(T)/Δ² + Klog(T)/Δ)的遗憾上界，与已知下界完全匹配。这是首次在匹配市场与赌博机问题中提出与下界匹配的算法。

Conclusion: 该研究填补了在线双边匹配市场中遗憾上下界之间的差距，证明了已知下界的紧致性，并为序列独裁假设下的匹配市场提供了最优算法。

Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $Ω\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $Δ$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{Δ^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.

</details>


### [71] [Measuring Over-smoothing beyond Dirichlet energy](https://arxiv.org/abs/2512.06782)
*Weiqi Guan,Zihao Shi*

Main category: cs.LG

TL;DR: 提出基于高阶特征导数的节点相似性度量家族，分析过平滑衰减率与图拉普拉斯谱隙的关系，发现注意力GNN在这些度量下存在过平滑问题


<details>
  <summary>Details</summary>
Motivation: Dirichlet能量作为量化过平滑的主流指标仅能捕捉一阶特征导数，存在局限性，需要更全面的度量方法

Method: 提出基于高阶特征导数能量的广义节点相似性度量家族，通过理论分析建立连续热扩散和离散聚合算子下Dirichlet能量衰减率，揭示过平滑衰减率与图拉普拉斯谱隙的内在联系

Result: 建立了高阶特征导数能量度量的理论框架，证明了过平滑衰减率与谱隙的关系，实验表明注意力GNN在这些新度量下确实存在过平滑问题

Conclusion: 提出的高阶特征导数能量度量能更全面评估过平滑现象，揭示了过平滑与图结构谱特性的深层联系，为GNN设计提供了新视角

Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.

</details>


### [72] [Angular Regularization for Positive-Unlabeled Learning on the Hypersphere](https://arxiv.org/abs/2512.06785)
*Vasileios Sevetlidis,George Pavlidis,Antonios Gasteratos*

Main category: cs.LG

TL;DR: AngularPU：一种基于余弦相似度和角度间隔的PU学习新框架，使用可学习的正类原型向量，无需显式负类建模，通过角度正则化器改善未标记数据的分离效果。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法要么依赖强分布假设，要么在高维设置中容易崩溃。需要一种无需显式负类建模、在高维空间中稳定且具有几何可解释性的PU学习框架。

Method: 在单位超球面上使用余弦相似度和角度间隔，通过可学习的正类原型向量表示正类，分类简化为计算嵌入向量与原型的余弦相似度并设定阈值。引入角度正则化器促使未标记数据在超球面上分散，改善分离效果。

Result: 在基准数据集上的实验表明，AngularPU在正样本稀缺和高维嵌入设置中，相比最先进的PU方法取得了竞争性或更优的性能，同时提供几何可解释性和可扩展性。

Conclusion: AngularPU提供了一种无需显式负类建模的PU学习新范式，通过角度几何方法在高维空间中稳定工作，具有理论保证和实际优势，特别适用于正样本稀缺和高维嵌入场景。

Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.

</details>


### [73] [Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games](https://arxiv.org/abs/2512.06791)
*Vedansh Sharma*

Main category: cs.LG

TL;DR: 提出SGN（Small-Gain Nash）方法，通过自定义块加权几何中的块小增益条件，为梯度学习提供收敛保证，即使伪梯度在欧几里得几何中非单调。


<details>
  <summary>Details</summary>
Motivation: 传统博弈中梯度学习的收敛保证需要伪梯度在欧几里得几何中满足（强）单调性条件，这在具有强跨玩家耦合的简单博弈中经常失败。

Method: 引入SGN（Small-Gain Nash）方法，在自定义块加权几何中建立块小增益条件，将局部曲率和跨玩家Lipschitz耦合边界转化为可处理的收缩证书，构造加权块度量使伪梯度在边界成立的任何区域都强单调。

Result: 连续流在设计的几何中指数收缩，投影欧拉和RK4离散化在显式步长边界下收敛；在二次博弈中，欧几里得单调性分析无法预测收敛时，SGN成功认证收敛；扩展到马尔可夫博弈中的镜像/Fisher几何用于熵正则化策略梯度。

Conclusion: SGN提供离线认证流程，在紧凑区域估计曲率、耦合和Lipschitz参数，优化块权重以扩大SGN边界，返回包含度量、收缩率和安全步长的结构化可计算收敛证书，用于非单调博弈。

Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.

</details>


### [74] [Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation](https://arxiv.org/abs/2512.06813)
*Agung Nugraha,Heungjun Im,Jihwan Lee*

Main category: cs.LG

TL;DR: 提出一种用于高性能混凝土部分逆向设计的协作神经网络框架，通过耦合的插补模型和代理模型，在单次前向传递中生成满足约束且性能一致的配合比设计。


<details>
  <summary>Details</summary>
Motivation: 高性能混凝土需要复杂的配合比设计，涉及许多相互依赖的变量和实际约束。虽然数据驱动方法在正向设计预测建模方面有所进展，但逆向设计（确定实现目标性能的配合比组成）仍然有限，特别是在某些混合变量被约束固定而只有剩余变量需要确定的设计情况下。

Method: 提出协作神经网络框架，结合两个耦合的神经网络模型：一个插补模型用于推断未确定的变量，一个代理模型用于预测抗压强度。通过协作学习，模型在单次前向传递中生成有效且性能一致的配合比设计，同时适应不同的约束组合而无需重新训练。

Result: 在基准数据集上评估，所提模型实现了稳定且较高的R平方值（0.87-0.92），与自编码器基线相比平均减少50%的均方误差，与贝叶斯推理相比平均减少70%的均方误差。

Conclusion: 协作神经网络为混凝土工程中约束感知、数据驱动的配合比设计提供了准确、鲁棒且计算高效的基础。

Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.

</details>


### [75] [Neural Factorization-based Bearing Fault Diagnosis](https://arxiv.org/abs/2512.06837)
*Zhenhao Li,Xu Cheng,Yi Zhou*

Main category: cs.LG

TL;DR: 提出基于神经分解的分类(NFC)框架用于高铁轴承故障诊断，通过多模态潜在特征向量嵌入和神经分解融合来挖掘复杂故障特征，实验证明优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高铁轴承作为列车运行系统的核心部件，其健康状况直接关系到运行安全。传统诊断方法在复杂工况下诊断精度不足，需要更有效的解决方案。

Method: 提出神经分解分类(NFC)框架：1)将振动时间序列嵌入为多模态潜在特征向量以捕捉多样化故障模式；2)利用神经分解原理将这些向量融合为统一的振动表示。基于CP和Tucker融合方案分别实例化为CP-NFC和Tucker-NFC模型。

Result: 实验结果表明，CP-NFC和Tucker-NFC模型相比传统机器学习方法都取得了更优越的诊断性能。比较分析为高铁轴承监测中选择有效诊断策略提供了有价值的实证证据和实践指导。

Conclusion: NFC框架能够从原始时间序列数据中有效挖掘复杂的潜在故障特征，为高铁轴承故障诊断提供了有效的解决方案，具有实际应用价值。

Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.

</details>


### [76] [Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis](https://arxiv.org/abs/2512.06917)
*Clifford F,Devika Jay,Abhishek Sarkar,Satheesh K Perepu,Santhosh G S,Kaushik Dey,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出一个新颖的轨迹级可解释RL框架，通过结合Q值差异和"激进项"的状态重要性度量来排名轨迹，识别最优行为并提供反事实解释。


<details>
  <summary>Details</summary>
Motivation: 当前可解释强化学习(XRL)主要关注局部单步决策，缺乏对智能体长期行为的解释。为了建立可信赖的自主系统，需要能够解释智能体整个轨迹层面的行为。

Method: 引入一个新颖框架，定义并聚合新的状态重要性度量。该度量结合经典Q值差异和捕捉智能体到达目标倾向的"激进项"，从而对轨迹进行排名。通过从关键状态生成反事实rollouts来提供对比解释。

Result: 实验表明，该方法能成功从异构智能体经验中识别最优轨迹。提出的重要性度量在识别最优行为方面比经典方法更有效，并能证明智能体选择的路径比替代方案更优越。

Conclusion: 该框架为强化学习智能体的长期行为提供了有效的轨迹级解释，通过"为什么是这个而不是那个"的解释增强了可信度，是迈向可信赖自主系统的重要一步。

Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.

</details>


### [77] [Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models](https://arxiv.org/abs/2512.06920)
*Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: 提出PGSRM框架，使用父模型参考输出与子模型生成输出的嵌入余弦相似度作为语义奖励，替代传统RLHF中的复杂奖励建模


<details>
  <summary>Details</summary>
Motivation: 传统强化学习语言模型训练依赖二进制正确性信号、人工偏好数据或训练好的奖励模型，这些方法成本高且复杂。需要一种轻量级、无需人工标注的奖励框架

Method: PGSRM框架：使用父模型（参考模型）的输出嵌入与子模型（被训练模型）生成输出的嵌入之间的余弦相似度作为奖励信号。这种方法无需人工标注或额外模型训练，直接提供密集的语义奖励

Result: 在五个语言任务上测试，相比二进制奖励基线，PGSRM产生更平滑的奖励改进和更稳定的PPO动态，表明嵌入语义奖励是RLHF式奖励建模的实用替代方案

Conclusion: 基于嵌入的语义奖励为小型transformer模型的父引导对齐提供了实用替代方案，避免了传统RLHF奖励建模的复杂性

Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.

</details>


### [78] [Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features](https://arxiv.org/abs/2512.06925)
*Aseer Al Faisal*

Main category: cs.LG

TL;DR: 提出QR-DQN方法结合RoBERTa语义嵌入和手工特征进行钓鱼检测，通过分位数回归建模回报分布，在105,000个URL数据集上达到99.86%准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击通过欺诈信息、误导广告和网站劫持窃取个人信息造成经济损失，传统DQN方法使用单一标量Q值估计存在局限性，需要更稳定且能处理不确定性的检测方法。

Method: 提出Quantile Regression Deep Q-Network (QR-DQN)方法，整合RoBERTa语义嵌入和手工词法特征，使用分位数回归建模回报分布而非单一标量Q值，在105,000个URL数据集上采用80/20训练测试划分。

Result: QR-DQN达到测试准确率99.86%、精确率99.75%、召回率99.96%、F1分数99.85%；相比仅使用词法特征的标准DQN，混合方法将泛化差距从1.66%降至0.04%；五折交叉验证平均准确率99.90%±0.04%。

Conclusion: 提出的混合QR-DQN方法能有效识别钓鱼威胁，适应不断演变的攻击策略，在未见数据上泛化性能优异，显著提升了检测的鲁棒性和稳定性。

Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.

</details>


### [79] [Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise](https://arxiv.org/abs/2512.06926)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: 该研究系统分析了BiLSTM时间序列预测中两个关键数据因素（输入序列长度和加性噪声）的影响，发现长序列增加过拟合风险，噪声降低预测精度，两者同时存在时模型稳定性下降最严重。


<details>
  <summary>Details</summary>
Motivation: 尽管BiLSTM在时间序列预测中表现出色，但现有文献对输入数据特性（如序列长度和噪声）如何影响模型鲁棒性和泛化能力的研究不足。本研究旨在填补这一空白，深入理解数据因素对深度学习预测模型的影响。

Method: 开发了模块化、可复现的预测流程，包含标准化预处理、序列生成、模型训练、验证和评估。在三个不同采样频率的真实世界数据集上进行控制实验，系统分析输入序列长度和加性噪声对BiLSTM性能的影响。

Result: 三个关键发现：1) 长输入序列显著增加过拟合和数据泄露风险，尤其在数据受限环境中；2) 加性噪声在不同采样频率下都持续降低预测精度；3) 两个因素同时存在时模型稳定性下降最严重。高采样频率数据集虽然更鲁棒，但在两个挑战同时存在时仍很脆弱。

Conclusion: 研究揭示了当前基于深度学习的预测流程的重要局限性，强调了数据感知设计策略的必要性。为理解深度学习模型在动态时间序列环境中的行为提供了深入见解，并为开发更可靠、可泛化的预测系统提供了实用指导。

Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.

</details>


### [80] [Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding](https://arxiv.org/abs/2512.06929)
*MinCheol Jeon*

Main category: cs.LG

TL;DR: AdaMamba是一个统一的时间序列预测架构，通过自适应归一化、多尺度趋势提取和上下文序列建模来解决非平稳性、多尺度时间模式和分布偏移等挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测面临非平稳性、多尺度时间模式和分布偏移等挑战，这些因素会降低模型的稳定性和准确性。传统方法在处理这些复杂动态时存在局限性。

Method: AdaMamba采用自适应归一化块去除非平稳成分，通过多尺度卷积趋势提取和通道级重新校准实现一致的去趋势和方差稳定。然后通过上下文编码器结合补丁嵌入、位置编码和Mamba增强的Transformer层进行序列处理，最后通过轻量级预测头和去归一化机制生成预测。

Result: 实验评估表明，AdaMamba的自适应归一化和专家增强的上下文建模相结合，在稳定性和准确性方面相比传统基于Transformer的基线方法有持续改进。

Conclusion: AdaMamba通过整合自适应归一化、多尺度趋势提取和上下文序列建模，有效缓解协变量偏移，增强预测可靠性，具有强大的表示能力和模块化可扩展性。

Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.

</details>


### [81] [Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies](https://arxiv.org/abs/2512.06932)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: 该研究探讨了数据泄露对LSTM时间序列预测模型评估的影响，发现验证设计（特别是10折交叉验证）对泄露敏感，而2-way和3-way分割更稳健，输入窗口大小和滞后步长显著影响泄露风险。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（特别是LSTM）在时间序列预测中被广泛使用，但评估完整性常因数据泄露而受损。数据泄露是指输入输出序列在数据集划分前构建，导致未来信息无意中影响训练。本研究旨在探究数据泄露对性能的影响，重点关注验证设计如何调节泄露敏感性。

Method: 研究评估了三种常用验证技术（2-way分割、3-way分割和10折交叉验证）在有泄露（预分割序列生成）和无泄露条件下的表现。无泄露条件通过在数据分割前强制时间分离来降低泄露风险。使用RMSE增益（泄露与无泄露设置之间的百分比差异）评估泄露影响，并分析输入窗口大小和滞后步长对泄露敏感性的影响。

Result: 实证结果显示：10折交叉验证在较长滞后步长下表现出高达20.5%的RMSE增益；而2-way和3-way分割表现出更强的稳健性，通常将RMSE增益维持在5%以下。输入窗口大小和滞后步长显著影响泄露敏感性：较小的窗口和较长的滞后步长会增加泄露风险，而较大的窗口有助于减少泄露。

Conclusion: 研究结果强调了需要配置感知、抗泄露的评估流程，以确保可靠的性能估计。验证设计选择对数据泄露敏感性有重要影响，特别是在使用交叉验证时需要特别注意泄露风险。

Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.

</details>


### [82] [A Unifying Human-Centered AI Fairness Framework](https://arxiv.org/abs/2512.06944)
*Munshi Mahbubur Rahman,Shimei Pan,James R. Foulds*

Main category: cs.LG

TL;DR: 提出一个统一的人类中心公平框架，涵盖八种公平指标，帮助利益相关者根据价值观和情境选择公平干预措施，通过权重调整揭示公平指标间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: AI在关键社会领域应用增加引发公平性担忧，但现有研究在平衡不同公平概念与预测准确性方面存在挑战，阻碍了公平AI系统的实际部署。

Method: 引入统一的人类中心公平框架，系统涵盖八种公平指标（结合个体与群体公平、边际内与交叉假设、结果导向与机会平等视角），采用一致易懂的公式化表达，允许利益相关者为多个公平目标分配权重。

Result: 在四个真实数据集（UCI Adult收入预测、COMPAS累犯预测、German Credit信用评估、MEPS医疗利用）上应用该框架，显示调整权重能揭示不同公平指标间的细微权衡关系。

Conclusion: 通过司法决策和医疗保健案例研究，证明该框架能够指导公平AI系统的实际部署，支持价值敏感的系统设计，促进多利益相关者妥协。

Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.

</details>


### [83] [Comparing BFGS and OGR for Second-Order Optimization](https://arxiv.org/abs/2512.06969)
*Adrian Przybysz,Mikołaj Kołek,Franciszek Sobota,Jarek Duda*

Main category: cs.LG

TL;DR: 比较BFGS的Sherman-Morrison更新与新型在线梯度回归(OGR)方法在Hessian矩阵估计上的表现，OGR在非凸优化中表现更优


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中Hessian矩阵估计面临高维度和计算成本挑战，传统BFGS方法基于凸性假设保持正定Hessian近似，但无法处理非凸结构

Method: 提出在线梯度回归(OGR)方法，使用指数移动平均对梯度与位置进行回归，在线估计二阶导数而无需Hessian求逆，可估计一般(非正定)Hessian矩阵

Result: 在标准测试函数上评估两种方法，OGR实现更快的收敛速度和更好的损失值，特别是在非凸设置中表现更优

Conclusion: OGR方法比传统BFGS更适用于非凸优化问题，能够处理更一般的Hessian结构，在神经网络训练等复杂优化任务中具有优势

Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.

</details>


### [84] [Prediction with Expert Advice under Local Differential Privacy](https://arxiv.org/abs/2512.06971)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.LG

TL;DR: 论文研究了在本地差分隐私约束下的专家建议预测问题，提出了两个改进算法RW-AdaBatch和RW-Meta，通过隐私放大和元专家选择机制，在保持隐私的同时显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 在本地差分隐私约束下进行专家建议预测是一个经典问题。现有方法在隐私保护与预测性能之间存在权衡，特别是在处理实际数据时效果有限。需要开发既能保护隐私又能适应不同数据难度和专家类型的算法。

Method: 首先证明经典算法自然满足LDP，然后提出两个新算法：1) RW-AdaBatch利用LDP诱导的有限切换行为实现隐私放大，这种放大在数据较简单时更强；2) RW-Meta开发了一种在专家之间进行私有选择的方法，这些专家本身是非平凡的学习算法。基于随机游走理论进行分析，并推导了与专家独立性程度成反比的遗憾界。

Result: 在COVID-19疫情期间医院报告数据的实际评估中，RW-Meta在预测每周报告COVID患者密度最高的医院任务上，比经典基线和最先进的中心差分隐私算法性能提升1.5-3倍。RW-AdaBatch的隐私放大几乎不带来效用损失。

Conclusion: 该研究为本地差分隐私下的专家建议预测提供了有效的算法框架，通过隐私放大和元专家选择机制，在保持严格隐私保护的同时显著提升了预测性能，特别适用于医疗数据等敏感应用场景。

Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.

</details>


### [85] [LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding](https://arxiv.org/abs/2512.06982)
*Yu Yu,Qian Xie,Nairen Cao,Li Jin*

Main category: cs.LG

TL;DR: 提出基于LLM的神经架构搜索方法，用于高效设计多源强化学习的状态编码器，在交通控制任务中优于传统NAS和GENIUS框架。


<details>
  <summary>Details</summary>
Motivation: 多源强化学习（包含传感器测量、时序信号、图像观测和文本指令等多种信息源）的状态编码器设计缺乏系统方法，通常需要手动设计，现有NAS方法忽略了模块中间输出的有用信息。

Method: 将问题形式化为复合神经架构搜索，提出LLM驱动的NAS流程，利用语言模型先验和中间输出信号（如表示质量）来指导高效搜索高性能复合状态编码器。

Result: 在混合自主交通控制任务中，该方法比传统NAS基线和LLM-based GENIUS框架发现更高性能的架构，且需要更少的候选评估次数。

Conclusion: LLM驱动的NAS方法能够有效利用中间输出信号和语言模型先验，在多源强化学习场景中实现样本高效的复合状态编码器架构搜索。

Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.

</details>


### [86] [OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction](https://arxiv.org/abs/2512.06987)
*Emily Jin,Andrei Cristian Nica,Mikhail Galkin,Jarrid Rector-Brooks,Kin Long Kelvin Lee,Santiago Miret,Frances H. Arnold,Michael Bronstein,Avishek Joey Bose,Alexander Tong,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: OXtal是一个100M参数的全原子扩散模型，直接从2D化学图预测3D分子晶体结构，在晶体结构预测任务上比现有方法有数量级提升。


<details>
  <summary>Details</summary>
Motivation: 从2D化学图准确预测实验可实现的3D分子晶体结构是计算化学中长期存在的开放挑战（晶体结构预测CSP）。高效解决这个问题对制药和有机半导体等领域有重要意义，因为晶体堆积直接影响有机固体的物理化学性质。

Method: 1. 放弃显式等变架构，采用数据增强策略处理晶体对称性；2. 提出新颖的结晶启发式无晶格训练方案S^4（化学计量随机壳采样），高效捕获长程相互作用，避免显式晶格参数化；3. 使用600K实验验证晶体结构的大规模数据集；4. 构建100M参数的全原子扩散模型，直接学习分子内构象和周期性堆积的条件联合分布。

Result: OXtal在晶体结构预测上比先前的从头算机器学习方法有数量级提升，同时比传统量子化学方法便宜数量级。具体而言，OXtal能够恢复实验结构（构象RMSD1<0.5 Å），并达到超过80%的堆积相似率，展示了其建模分子结晶热力学和动力学规律的能力。

Conclusion: OXtal通过大规模扩散模型和数据驱动方法，在晶体结构预测任务上取得了显著进展，为从2D化学图预测3D晶体结构提供了高效准确的解决方案，对材料科学和药物开发有重要应用价值。

Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ Å and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.

</details>


### [87] [Flash Multi-Head Feed-Forward Network](https://arxiv.org/abs/2512.06989)
*Minshen Zhang,Xiang Hu,Jianguo Li,Wei Wu,Kewei Tu*

Main category: cs.LG

TL;DR: FlashMHF提出多头FFN替代传统FFN，通过融合内核和动态加权并行子网络设计，在提升性能的同时大幅降低内存消耗和加速推理。


<details>
  <summary>Details</summary>
Motivation: 受多头注意力机制启发，探索多头FFN替代传统FFN。但直接应用面临两个挑战：内存消耗随头数线性增长，以及模型扩展时中间维度与头维度比例失衡，影响可扩展性和表达能力。

Method: 提出Flash Multi-Head FFN (FlashMHF)，包含两个关键创新：1) I/O感知融合内核，类似FlashAttention在SRAM中在线计算输出；2) 使用动态加权并行子网络设计，保持中间维度与头维度的平衡比例。

Result: 在128M到1.3B参数规模的模型上验证，FlashMHF相比SwiGLU FFNs持续改善困惑度和下游任务准确率，同时将峰值内存使用降低3-5倍，推理加速最高达1.08倍。

Conclusion: 多头设计是FFN的优越架构原则，FlashMHF作为Transformer中FFN的强大、高效且可扩展的替代方案。

Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.

</details>


### [88] [Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation](https://arxiv.org/abs/2512.06993)
*Ali Ebrahimpour-Boroojeny*

Main category: cs.LG

TL;DR: 本文提出两种新的机器学习遗忘方法：AMUN用于样本遗忘，通过对抗样本微调降低遗忘样本置信度；TRW用于类别遗忘，通过倾斜重加权分布近似重训练模型行为，均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法存在不足，无法有效模拟重训练模型在遗忘样本和剩余样本上的预测行为，导致隐私泄露风险。需要开发更有效的遗忘方法来保护模型隐私。

Method: 1. AMUN：通过生成对抗样本并微调模型来降低遗忘样本的置信度，结合FastClip控制模型平滑度；2. TRW：通过估计类别间相似性，构造倾斜重加权分布作为微调目标，近似重训练模型在剩余类别上的分布。

Result: AMUN在图像分类任务中超越了现有遗忘方法，基于SOTA MIA得分表现最佳；TRW在多个基准测试中匹配或超越了现有类别遗忘方法，有效减少了隐私泄露。

Conclusion: 基于预测相似性的遗忘方法能有效模拟重训练模型行为，AMUN和TRW分别解决了样本遗忘和类别遗忘问题，显著提升了遗忘效果和隐私保护能力。

Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.

</details>


### [89] [Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation](https://arxiv.org/abs/2512.07010)
*Kevin Lee,Pablo Millan Arias*

Main category: cs.LG

TL;DR: DynamicLRP：首个模型无关的LRP框架，在张量操作级别实现可解释性，无需架构特定规则，支持任意计算图


<details>
  <summary>Details</summary>
Motivation: 现有LRP实现基于模块级别，需要架构特定的传播规则和修改，限制了通用性和可持续性，无法适应不断演进的神经网络架构

Method: 提出DynamicLRP框架，在计算图内部分解到单个张量操作级别，引入Promise System延迟激活解析机制，独立于反向传播机制，支持任意计算图

Result: 在31,465个计算图节点（15种不同架构）上达到99.92%覆盖率，性能匹配或超过专用实现（VGG上1.77 vs 1.69 ABPC，ViT相当，RoBERTa-large和Flan-T5-large在SQuADv2上分别达到93.70%和95.06% top-1归因准确率），支持亿级参数模型

Conclusion: DynamicLRP通过操作级分解和Promise System为LRP建立了可持续、可扩展的基础，实现了真正的架构无关性，同时保持LRP的理论保证

Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.

</details>


### [90] [Transferring Clinical Knowledge into ECGs Representation](https://arxiv.org/abs/2512.07021)
*Jose Geraldo Fernandes,Luiz Facury de Souza,Pedro Robles Dutenhefner,Gisele L. Pappa,Wagner Meira*

Main category: cs.LG

TL;DR: 提出三阶段训练范式，将多模态临床数据知识迁移到单模态ECG编码器中，提升ECG分类准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在ECG分类中准确率高，但黑盒特性缺乏可解释性，阻碍临床采用。需要建立更可信赖的ECG分类模型。

Method: 三阶段训练范式：1) 自监督联合嵌入预训练，利用多模态临床数据（实验室检查、生命体征、生物特征）丰富ECG表示；2) 仅需ECG信号进行推理；3) 训练模型从ECG嵌入预测相关实验室异常，作为间接解释机制。

Result: 在MIMIC-IV-ECG数据集上，模型在多标签诊断分类中优于标准单信号基线，显著缩小了与需要所有数据推理的全多模态模型的性能差距。

Conclusion: 通过将抽象预测转化为基于生理学的解释，该方法为AI更安全地集成到临床工作流程提供了有前景的路径，创建了更准确、可信赖的ECG分类模型。

Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.

</details>


### [91] [Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis](https://arxiv.org/abs/2512.07040)
*Sakib Mostafa,Lei Xing,Md. Tauhidul Islam*

Main category: cs.LG

TL;DR: Graph2Image将大型生物网络转换为二维图像，利用CNN进行高效分析，解决了传统方法在可扩展性、长距离依赖和多模态集成方面的限制。


<details>
  <summary>Details</summary>
Motivation: 传统生物网络分析方法（包括深度学习方法）面临可扩展性有限、长距离依赖过平滑、多模态集成困难、表达能力受限和可解释性差等挑战，需要一种能处理大规模复杂生物网络的新方法。

Method: Graph2Image框架通过将代表性网络节点在2D网格上进行空间排列，将大型生物网络转换为二维图像集合。这种转换将节点解耦为图像，使能够使用具有全局感受野和多尺度金字塔的卷积神经网络。

Result: 在多个大规模生物网络数据集上，Graph2Image比现有方法提高了高达67.2%的分类准确率，提供了可解释的可视化，揭示了生物学上一致的模式，并能在个人计算机上分析超过10亿节点的超大型网络。

Conclusion: Graph2Image为生物网络分析提供了一种可扩展、可解释且支持多模态的方法，为疾病诊断和复杂生物系统研究提供了新的机会。

Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.

</details>


### [92] [Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design](https://arxiv.org/abs/2512.07064)
*Jiannan Yang,Veronika Thost,Tengfei Ma*

Main category: cs.LG

TL;DR: 该研究将分子图的自监督预训练-微调流程统一到概率框架中，通过控制实验发现：对于常见的节点级预测任务，复杂的掩码分布相比均匀采样并无优势；预测目标的选择及其与编码器架构的协同作用更为关键。


<details>
  <summary>Details</summary>
Motivation: 当前基于掩码的分子表示学习方法多为启发式设计，缺乏原则性评估，难以确定哪些设计选择真正有效。需要建立统一框架来透明比较和理解不同掩码策略。

Method: 1. 将预训练-微调流程统一到概率框架中；2. 在严格控制条件下，对三个核心设计维度进行控制研究：掩码分布、预测目标和编码器架构；3. 使用信息论度量评估预训练信号的信息量，并与下游性能关联。

Result: 1. 复杂掩码分布在常见节点级预测任务中相比均匀采样无一致优势；2. 预测目标的选择及其与编码器架构的协同作用更为关键；3. 转向语义更丰富的预测目标能带来显著的下游改进，特别是与表达能力强的图Transformer编码器配对时。

Conclusion: 该研究为分子图自监督学习方法提供了实用指导：应更关注预测目标的选择及其与编码器架构的匹配，而非过度优化掩码分布策略。

Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.

</details>


### [93] [Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation](https://arxiv.org/abs/2512.07079)
*Anton Morgunov,Victor S. Batista*

Main category: cs.LG

TL;DR: RetroCast是一个统一的计算机辅助合成规划评估套件，通过标准化模型输出实现公平比较，揭示高"可解性"分数常掩盖化学无效性，并发现搜索方法在复杂合成计划中性能急剧下降


<details>
  <summary>Details</summary>
Motivation: 计算机辅助合成规划领域缺乏标准化评估基础设施，现有指标过于关注拓扑完成度而忽视化学有效性，导致进展难以衡量和比较

Method: 开发RetroCast统一评估套件，将异构模型输出标准化为通用模式；包含可重复的基准测试流程（分层抽样和自举置信区间）和SynthArena交互式平台；使用该基础设施评估主流搜索式和序列式算法

Result: 发现"可解性"（库存终止率）与路线质量存在分歧：高可解性分数常掩盖化学无效性，且与实验真实情况再现不相关；识别出"复杂性悬崖"现象：搜索方法尽管可解性高，但在重建长程合成计划时性能急剧下降，不如序列方法

Conclusion: RetroCast框架支持透明可重复的合成规划研究，揭示了当前评估指标的局限性，并发布了完整框架、基准定义和标准化模型预测数据库以推动领域发展

Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.

</details>


### [94] [TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization](https://arxiv.org/abs/2512.07082)
*Yuan-Ting Zhong,Ting Huang,Xiaolin Xiao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: TRACE是一个可迁移的概念漂移估计器，用于检测流数据中的分布变化，具有跨数据集泛化能力，并能集成到流优化器中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 现有流数据驱动优化方法存在限制性假设（如固定漂移间隔、完全环境可观测性），难以适应多样化的动态环境，需要更灵活的概念漂移检测方法。

Method: TRACE采用原则性标记化策略从数据流中提取统计特征，使用基于注意力的序列学习建模漂移模式，实现跨数据集的概念漂移检测。

Result: 在多样化基准测试中，TRACE表现出优越的泛化能力、鲁棒性和有效性，能够准确检测未见数据集中的概念漂移。

Conclusion: TRACE是一个可迁移的概念漂移估计器，能够有效检测流数据中的分布变化，其即插即用特性使其能够集成到流优化器中，实现未知漂移下的自适应优化。

Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.

</details>


### [95] [The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models](https://arxiv.org/abs/2512.07092)
*Zhixiang Wang*

Main category: cs.LG

TL;DR: Soul Engine框架通过线性表示假设，在不修改基础模型权重的情况下实现人格解耦与注入，解决了LLM个性化中的稳定性-可塑性困境。


<details>
  <summary>Details</summary>
Motivation: 当前个性化大语言模型部署面临稳定性-可塑性困境，传统对齐方法（如监督微调）通过随机权重更新会导致"对齐税"——降低通用推理能力。

Method: 基于线性表示假设（人格特质存在于正交线性子空间），提出Soul Engine框架，使用SoulBench数据集和双头架构，在冻结的Qwen-2.5基础上提取解耦的人格向量而不修改主干权重。

Result: 三个突破：1) 高精度人格分析（MSE=0.011）；2) 几何正交性（人格流形离散连续，实现"零样本人格注入"且保持原始智能）；3) 确定性引导（通过向量运算实现稳健行为控制）。

Conclusion: 挑战了微调对个性化的必要性，从概率提示转向确定性潜在干预，为安全可控的AI个性化提供了数学严谨的基础。

Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.

</details>


### [96] [Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph](https://arxiv.org/abs/2512.07100)
*Hong Wang,Yinglong Zhang,Hanhan Guo,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: DRCL是一个完全无监督的框架，通过双向精炼循环整合图结构和文本语义信息，用于文本属性网络的社区发现，无需标签或类别定义。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型依赖标注数据难以部署到真实世界文本属性网络，而传统社区检测方法忽略文本语义，限制了在内容组织、推荐和风险监控等下游应用中的实用性。

Method: DRCL通过热启动初始化和双向精炼循环整合结构和语义信息：GCN社区检测模块和文本语义建模模块迭代交换伪标签，让语义线索增强结构聚类，结构模式指导文本表示学习。

Result: 在多个文本属性图数据集上，DRCL持续提升发现社区的结构和语义质量；仅使用DRCL社区信号训练的Mamba分类器达到与监督模型相当的准确率。

Conclusion: DRCL展示了在标注数据稀缺或成本高昂的大规模系统中部署的潜力，为无监督文本属性网络分析提供了有效解决方案。

Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.

</details>


### [97] [FOAM: Blocked State Folding for Memory-Efficient LLM Training](https://arxiv.org/abs/2512.07112)
*Ziqing Wen,Jiahuan Wang,Ping Luo,Dongsheng Li,Tao Sun*

Main category: cs.LG

TL;DR: FOAM是一种通过计算块状梯度均值和残差校正来压缩优化器状态的内存高效优化器，可将总训练内存减少约50%，消除高达90%的优化器状态内存开销，同时保持与Adam相当的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于参数规模巨大，在使用Adam等内存密集型优化器时面临显著的内存瓶颈。现有内存高效方法通常依赖SVD、投影或权重冻结等技术，但这些方法会引入计算开销、需要额外内存或降低模型性能。

Method: 提出FOAM方法：通过计算块状梯度均值来压缩优化器状态，并引入残差校正来恢复丢失的信息。该方法与其他内存高效优化器兼容。

Result: 理论上，FOAM在标准非凸优化设置下达到与原始Adam相当的收敛率。实证中，FOAM减少约50%的总训练内存，消除高达90%的优化器状态内存开销，并加速收敛。性能与吞吐量匹配或优于全秩和现有内存高效基线。

Conclusion: FOAM是一种高效的内存优化器压缩方法，能显著减少训练内存需求，同时保持模型性能，为解决大规模语言模型训练中的内存瓶颈问题提供了有效解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.

</details>


### [98] [PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes](https://arxiv.org/abs/2512.07113)
*Kepeng Lin,Qizhe Zhang,Rui Wang,Xuehai Hu,Wei Xu*

Main category: cs.LG

TL;DR: PlantBiMoE：一种轻量级植物基因组语言模型，结合双向Mamba和稀疏专家混合框架，在31个数据集上取得最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有植物基因组模型存在参数过大（如AgroNT）或无法有效建模DNA双链双向特性（如PDLLMs）的问题，需要开发更高效且能捕捉基因组双向依赖关系的模型

Method: 提出PlantBiMoE模型，整合双向Mamba以捕捉DNA正反链的结构依赖，采用稀疏专家混合（SparseMoE）框架减少激活参数数量，提升计算效率而不牺牲建模能力

Result: 在改进的植物基因组基准（MPGB）上测试，包含31个数据集和11个代表性任务，输入序列长度50-6000bp。PlantBiMoE在31个数据集中的20个上取得最佳性能，平均表现优于现有模型

Conclusion: PlantBiMoE能有效表示植物基因组序列，为多样化基因组任务提供强大计算工具，对植物基因组学、基因编辑和合成生物学有实质性贡献

Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE

</details>


### [99] [Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search](https://arxiv.org/abs/2512.07142)
*Tanay Arora,Christof Teuscher*

Main category: cs.LG

TL;DR: CTS算法通过组合优化方法，在初始化阶段高效寻找高性能稀疏子网络，相比传统方法在计算效率和稀疏性能上都有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有彩票假设方法存在计算成本高或精度-稀疏度权衡差的问题，需要更高效的子网络发现方法。

Method: 提出Concrete Ticket Search (CTS)算法，将子网络发现建模为组合优化问题，使用Concrete松弛和GRADBALANCE梯度平衡方案控制稀疏度，并采用KL散度作为剪枝目标。

Result: CTS在ResNet-20/CIFAR10上达到99.3%稀疏度时获得74.0%准确率，计算时间仅7.9分钟，优于LTR的68.3%准确率和95.2分钟计算时间。

Conclusion: CTS能够高效发现高性能稀疏子网络，在保持精度的同时大幅减少计算成本，特别是在高稀疏度区域表现优异。

Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

</details>


### [100] [FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers](https://arxiv.org/abs/2512.07150)
*Jonghyun Park,Jong Chul Ye*

Main category: cs.LG

TL;DR: FlowLPS：一种基于预训练流模型解决逆问题的新框架，通过Langevin近端采样策略，在FFHQ和DIV2K数据集上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于深度生成模型的无训练方法在应用于流模型时存在两个主要问题：1）难以收敛到后验模式；2）在潜在空间中存在流形偏差。需要一种能同时保持重建保真度和感知质量的方法。

Method: 提出FlowLPS框架，结合Langevin动力学进行流形一致探索和近端优化进行精确模式搜索。Langevin部分确保在流形上一致采样，近端部分则专注于寻找精确的后验模式。

Result: 在FFHQ和DIV2K数据集上的多个逆问题任务中，FlowLPS在重建保真度和感知质量之间取得了优越的平衡，超越了现有的先进逆问题求解器。

Conclusion: FlowLPS通过Langevin近端采样策略有效解决了流模型在逆问题中的收敛和流形偏差问题，为基于生成先验的逆问题求解提供了更优的解决方案。

Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.

</details>


### [101] [Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration](https://arxiv.org/abs/2512.07173)
*Jucheng Shen,Gaurav Sarkar,Yeonju Ro,Sharath Nittur Sridhar,Zhangyang Wang,Aditya Akella,Souvik Kundu*

Main category: cs.LG

TL;DR: CadLLM是一种无需训练的方法，通过动态调整生成块大小、步长和阈值来加速基于扩散的大语言模型推理吞吐量，最高可达2.28倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）的推理速度较慢，需要提高其推理吞吐量。现有方法在动态token置信度管理方面存在不足，需要一种轻量级自适应方法来优化推理效率。

Method: 首先分析token unmasking置信度在块和步骤间的动态特性，然后提出轻量级自适应方法：1）基于未mask token的平均置信度动态控制生成块大小、步长和阈值；2）通过动态利用词汇表子集来减少softmax计算开销；3）兼容基于KV缓存的dLLMs，即插即用且模型无关。

Result: 在四个流行任务上的实验表明，CadLLM相比最先进的基线方法，在保持竞争力的准确度下，推理吞吐量最高提升2.28倍。

Conclusion: CadLLM是一种有效的训练免费方法，通过动态自适应策略显著加速扩散式大语言模型的推理速度，同时保持模型性能，具有实际应用价值。

Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.

</details>


### [102] [SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models](https://arxiv.org/abs/2512.07175)
*Yibo Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: SPACE是一种基于噪声对比估计的自博弈微调方法，通过二元分类区分真实样本和合成样本，避免了传统基于奖励差距方法的不稳定问题，在多个任务上显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈微调方法主要关注真实数据和合成数据之间的奖励差距，忽略了它们的绝对值，导致目标函数可能退化，造成不稳定演化。需要一种更稳定、能独立优化各类数据绝对奖励值的方法。

Method: 提出SPACE方法，采用噪声对比估计捕捉真实世界数据分布。将合成样本作为辅助成分，以二元分类方式区分真实样本和合成样本，独立优化每类数据的绝对奖励值，确保目标函数始终有意义。

Result: 理论上证明SPACE的最优解与真实世界数据的基础分布一致，并保证可证明的稳定收敛。实证表明SPACE在多个任务上显著提升LLM性能，优于使用更多真实样本的监督微调，相比基于差距的自博弈方法表现出显著优势和稳定演化。

Conclusion: SPACE通过噪声对比估计解决了传统自博弈微调的不稳定问题，实现了稳定收敛和性能提升，为有限真实数据下的LLM微调提供了更可靠的方法。

Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.

</details>


### [103] [UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.07184)
*Da Zhang,Bingyu Li,Zhuyuan Zhao,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: UniDiff：一个统一的扩散框架，用于多模态时间序列预测，通过跨模态融合和时间戳/文本信息的解耦控制实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据（文本和时间戳）日益增多，但现有扩散模型在时间序列预测中主要局限于单模态数值序列，忽略了复杂异构数据中丰富的跨模态信号。

Method: 1) 将时间序列分块并映射到嵌入空间；2) 核心是统一并行融合模块，使用单一交叉注意力机制自适应加权整合时间戳的结构信息和文本的语义上下文；3) 引入新颖的classifier-free guidance机制，支持多源条件控制，实现文本和时间戳信息的解耦引导强度调节。

Result: 在八个领域的真实世界基准数据集上进行广泛实验，证明UniDiff模型达到了最先进的性能。

Conclusion: UniDiff成功解决了多模态时间序列预测的挑战，通过统一的扩散框架有效融合异构信息，显著提升了预测准确性和模型鲁棒性。

Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.

</details>


### [104] [Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction](https://arxiv.org/abs/2512.07200)
*Zhen Huang,Jiaxin Deng,Jiayu Xu,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: 提出基于强化学习的非均匀道路分段方法用于公交车到达时间预测，通过两阶段解耦（强化学习分段+线性预测）实现高效且性能更优的预测系统。


<details>
  <summary>Details</summary>
Motivation: 传统公交车到达时间预测采用均匀道路分段策略，无法考虑道路条件、交叉口、兴趣点等物理约束的差异，限制了预测效率。需要一种能自适应学习非均匀分段的方法。

Method: 1) 提出强化学习框架，基于影响分数提取非均匀道路分段；2) 对选定分段应用线性预测模型进行预测。将预测过程解耦为分段选择与预测两个阶段。

Result: 实验结果表明该方法在大规模基准测试中不仅提高了效率，还提升了学习性能。线性方法甚至能比更复杂的方法获得更好的性能。

Conclusion: 基于强化学习的非均匀道路分段方法在公交车到达时间预测中优于传统均匀分段策略，实现了效率与性能的双重提升，且线性预测模型足够有效。

Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.

</details>


### [105] [Geometric Prior-Guided Federated Prompt Calibration](https://arxiv.org/abs/2512.07208)
*Fei Luo,Ziwei Zhao,Mingxuan Wang,Duoyang Li,Zhe Qian,Jiayi Tuo,Chenyue Zhou,Yanbiao Ma*

Main category: cs.LG

TL;DR: GGTPC提出几何引导的文本提示校准框架，通过全局几何先验纠正联邦提示学习中的数据异构性导致的局部训练偏差，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦提示学习(FPL)虽然参数高效，但数据异构性导致局部训练的提示产生偏差，现有方法聚焦聚合或正则化，未能解决局部训练偏差的根本原因。

Method: 提出GGTPC框架：1) 服务器端通过协方差矩阵重构全局数据分布的几何形状作为全局几何先验；2) 客户端使用几何先验校准层(GPCL)在训练时将局部特征分布与全局先验对齐；3) 整个过程保持隐私保护。

Result: 在标签倾斜的CIFAR-100数据集上(β=0.1)比SOTA提升2.15%；在极端倾斜(β=0.01)时比基线提升9.17%；在域倾斜的Office-Home数据集上作为即插即用模块，将FedAvg性能提升4.60%。

Conclusion: GGTPC通过纠正局部训练偏差有效缓解数据异构问题，可作为通用模块增强各种联邦学习算法，验证了几何先验在联邦提示学习中的有效性。

Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.

</details>


### [106] [Pay Less Attention to Function Words for Free Robustness of Vision-Language Models](https://arxiv.org/abs/2512.07222)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Chao Shen*

Main category: cs.LG

TL;DR: 提出Function-word De-Attention (FDA)方法，通过消除功能词对跨模态注意力机制的影响，提升视觉语言模型的鲁棒性，在多种攻击下显著降低攻击成功率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒视觉语言模型在鲁棒性和性能之间存在权衡问题。研究发现功能词是导致VLM对跨模态对抗攻击脆弱的关键因素，因此需要一种方法来减轻功能词对模型的影响。

Method: 提出Function-word De-Attention (FDA)方法，类似于差分放大器，计算原始跨注意力和功能词跨注意力，然后将后者从前者的差分中减去，从而获得更对齐和鲁棒的VLM。

Result: 在6种不同攻击、2个下游任务、3个数据集和3个模型上的综合实验显示：在检索任务上，FDA使3个测试模型的攻击成功率平均下降18%/13%/53%，性能仅下降0.2%/0.3%/0.6%；在视觉定位任务上，攻击成功率下降90%，性能提升0.3%。

Conclusion: FDA方法有效提升了视觉语言模型的鲁棒性，同时保持了模型性能，具有良好的可扩展性、泛化能力和零样本性能，为解决VLM的鲁棒性与性能权衡问题提供了有效方案。

Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

</details>


### [107] [PINE: Pipeline for Important Node Exploration in Attributed Networks](https://arxiv.org/abs/2512.07244)
*Elizaveta Kovtun,Maksim Makarenko,Natalia Semenova,Alexey Zaytsev,Semen Budennyy*

Main category: cs.LG

TL;DR: PINE是一个无监督的、属性感知的图节点重要性识别框架，通过注意力机制结合节点语义特征和网络结构来识别关键节点。


<details>
  <summary>Details</summary>
Motivation: 传统节点重要性识别方法（如度中心性、PageRank）只考虑网络结构而忽略节点属性，而现有神经网络方法需要监督学习。需要一种既能利用节点语义特征又无需监督的方法。

Method: 提出PINE（Pipeline for Important Node Exploration）框架，核心是基于注意力的图模型，将节点语义特征融入学习过程以识别图结构特性，利用注意力分布计算节点重要性分数。

Result: PINE在多种同质和异质属性网络上表现出优越性能，并已作为工业系统应用于大规模企业图中关键实体的无监督识别。

Conclusion: PINE填补了无监督且属性感知的节点重要性识别方法的空白，通过注意力机制有效结合节点语义和网络结构，在实际工业应用中具有重要价值。

Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.

</details>


### [108] [IFFair: Influence Function-driven Sample Reweighting for Fair Classification](https://arxiv.org/abs/2512.07249)
*Jingran Yang,Min Zhang,Lingfeng Zhang,Zhaohui Wang,Yonggang Zhang*

Main category: cs.LG

TL;DR: 提出基于影响函数的预处理方法IFFair，通过动态调整训练样本权重来缓解机器学习模型中的偏见问题，无需修改网络结构或数据特征。


<details>
  <summary>Details</summary>
Motivation: 机器学习在提高社会效率的同时，基于数据的模式可能导致算法学习甚至加剧样本中的潜在偏见，对某些弱势群体产生歧视性决策，剥夺其平等对待权利，损害社会福利并阻碍相关应用发展。

Method: 基于影响函数的预处理方法IFFair，利用训练样本对不同群体的影响差异作为指导，在训练过程中动态调整样本权重，不修改网络结构、数据特征和决策边界。

Result: 在多个真实数据集和指标上的实验表明，IFFair在分类设置中缓解了多个公认指标的偏见，包括人口统计均等、均衡机会、机会平等和错误率均等，且在这些指标间没有冲突。相比之前的预处理方法，IFFair在多个效用和公平性指标之间实现了更好的权衡。

Conclusion: IFFair是一种有效的预处理方法，能够在不修改模型结构的情况下，通过动态调整样本权重来缓解机器学习中的偏见问题，在公平性和模型效用之间取得良好平衡。

Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.

</details>


### [109] [SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents](https://arxiv.org/abs/2512.07287)
*Sijia Li,Yuchen Huang,Zifan Liu,Zijian Li,Jingjing fu,Lei Song,Jiang Bian,Jun Zhang,Rui Wang*

Main category: cs.LG

TL;DR: SIT-Graph通过构建状态增强的工具图，结合情景记忆和程序记忆，提升多轮工具使用的效果


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在多轮工具使用场景中面临挑战，要么将整个轨迹或预定义子任务视为不可分割单元，要么仅利用工具间依赖关系，难以适应状态和信息在轮次间的演变

Method: 提出状态集成工具图(SIT-Graph)，从历史轨迹中构建工具图，并在每条边上添加对话和工具历史的紧凑状态摘要，实现情景记忆片段和程序化例程的平衡

Result: 在多个状态化多轮工具使用基准测试中，SIT-Graph持续优于基于记忆和图的基础方法，提供更稳健的工具选择和更有效的经验迁移

Conclusion: SIT-Graph通过结合情景记忆和程序记忆，实现了人类决策式的平衡，显著提升了多轮工具使用的效果和适应性

Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.

</details>


### [110] [Towards a Relationship-Aware Transformer for Tabular Data](https://arxiv.org/abs/2512.07310)
*Andrei V. Konstantinov,Valerii A. Zuev,Lev V. Utkin*

Main category: cs.LG

TL;DR: 提出基于改进注意力机制的模型，用于处理表格数据中的样本间依赖关系，在回归和因果效应估计任务中表现优于梯度提升决策树


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型无法有效处理表格数据中样本间的依赖关系图，而图神经网络仅考虑相邻节点，难以应用于稀疏图。需要一种能利用外部依赖关系的方法，特别是在因果效应估计等任务中。

Method: 提出基于改进注意力机制的解决方案，通过在注意力矩阵中添加项来考虑数据点间的可能关系。开发了多个模型变体，并在合成和真实数据集上进行回归任务评估，同时在IHDP数据集上进行因果效应估计任务测试。

Result: 提出的模型在回归任务中相互比较并与梯度提升决策树对比，在因果效应估计任务中在IHDP数据集上表现出色，证明了改进注意力机制在处理样本间依赖关系方面的有效性。

Conclusion: 基于改进注意力机制的模型能够有效处理表格数据中的样本间依赖关系，在回归和因果效应估计任务中优于传统方法，为利用外部依赖图提供了可行的解决方案。

Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.

</details>


### [111] [Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach](https://arxiv.org/abs/2512.07313)
*Bosun Kang,Hyejun Park,Chenglin Fan*

Main category: cs.LG

TL;DR: 提出基于贝叶斯决策和机器学习预测的滑雪租赁问题新框架，统一传统最坏情况保证和学习增强方法，通过精确后验分布实现不确定性量化和专家先验整合


<details>
  <summary>Details</summary>
Motivation: 传统滑雪租赁算法只关注最坏情况成本，而最近的学习增强方法虽然利用预测但缺乏理论框架。本文旨在统一这两种视角，通过贝叶斯框架实现更灵活、理论更严谨的决策方法

Method: 提出离散贝叶斯框架，维护时间范围的精确后验分布，支持专家先验整合和不确定性量化。算法能优雅地在最坏情况和完全信息设置之间插值，并扩展到多预测、非均匀先验和上下文信息

Result: 算法实现了先验依赖的竞争性保证，在广泛实验评估中表现出优越的实证性能：在准确先验下接近最优结果，同时保持稳健的最坏情况保证

Conclusion: 贝叶斯推理为具有不完美预测的在线决策问题提供了实用优势，框架自然扩展到多预测、非均匀先验和上下文信息，展示了贝叶斯方法在在线决策中的价值

Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.

</details>


### [112] [Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach](https://arxiv.org/abs/2512.07332)
*Zhengquan Luo,Guy Tadmor,Or Amar,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: RicciKGE：通过将知识图谱嵌入损失梯度与局部曲率耦合在扩展的Ricci流中，实现实体嵌入与底层流形几何的共同演化，以自适应地建模知识图谱的异质结构。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法将所有实体放置在单一均匀流形（欧几里得、球面、双曲或其乘积/多曲率变体）上，无法适应真实知识图谱中局部区域表现出的急剧变化的曲率。这种先验强加的几何结构若与知识图谱的局部曲率不匹配，会扭曲实体间距离并损害嵌入的表达能力。

Method: 提出RicciKGE方法，将KGE损失梯度与局部曲率耦合在扩展的Ricci流中，使实体嵌入与底层流形几何共同动态演化，实现相互适应。理论上证明，当耦合系数有界且适当选择时，所有边曲率呈指数衰减（流形趋向欧几里得平坦），且KGE距离严格收敛到全局最优。

Result: 在链接预测和节点分类基准测试中，RicciKGE表现出改进效果，证明了其在适应异质知识图谱结构方面的有效性。

Conclusion: RicciKGE通过耦合嵌入优化与几何演化，能够自适应地建模知识图谱的异质结构，解决了现有方法因使用预定义均匀流形而导致的几何不匹配问题。

Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.

</details>


### [113] [Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning](https://arxiv.org/abs/2512.07374)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: R2F提出了一种基于LoRA适配器重建完整模型梯度方向的高效LLM遗忘框架，通过训练梯度解码器近似完整模型梯度，无需完整模型微调或原始训练数据。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型（如LLMs）的遗忘对于动态知识更新、数据删除权利执行和模型行为修正至关重要。现有遗忘方法通常需要完整模型微调或访问原始训练数据，限制了其可扩展性和实用性。

Method: Recover-to-Forget (R2F)框架：1) 使用多个改写提示计算LoRA参数的梯度；2) 训练梯度解码器近似对应的完整模型梯度；3) 在代理模型上训练解码器并迁移到目标模型；4) 提供跨模型泛化的理论分析。

Result: 实验结果表明R2F实现了有效的遗忘同时保持一般模型性能，为预训练LLMs提供了可扩展且轻量级的遗忘替代方案，无需完整重新训练或访问内部参数。

Conclusion: R2F通过从低秩LoRA适配器更新重建完整模型梯度方向，为大型基础模型提供了一种高效、可扩展的遗忘框架，解决了现有方法在可扩展性和实用性方面的限制。

Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

</details>


### [114] [LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples](https://arxiv.org/abs/2512.07375)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: LUNE：基于LoRA的轻量级大语言模型遗忘框架，通过仅更新低秩适配器实现高效知识移除，计算成本比全微调降低一个数量级


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以移除特定信息，影响隐私保护、偏见缓解和知识修正。传统遗忘方法计算成本高，不适用于实际部署

Method: 基于LoRA的负向遗忘框架，仅更新低秩适配器并冻结主干网络，通过负向示例针对中间表示进行知识抑制或替换

Result: 在多个事实遗忘任务上，LUNE达到与全微调和内存编辑方法相当的效果，同时计算成本降低约一个数量级

Conclusion: LUNE提供了一种轻量级、高效的模型遗忘解决方案，在保持性能的同时大幅降低计算开销，适用于实际部署

Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.

</details>


### [115] [Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood](https://arxiv.org/abs/2512.07390)
*Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang*

Main category: cs.LG

TL;DR: SICL框架通过风格不变性进行鲁棒不确定性估计，在测试时自适应中显著降低校准误差


<details>
  <summary>Details</summary>
Motivation: 测试时自适应（TTA）虽然能高效适应部署模型，但常导致预测不确定性校准不佳，这在自动驾驶、金融和医疗等高风险领域是严重问题。现有校准方法通常假设固定模型或静态分布，在真实世界动态测试条件下性能下降。

Method: 提出SICL框架，利用风格不变性进行鲁棒不确定性估计。通过测量预测在不同风格变换变体间的一致性来估计实例级正确性似然，仅需模型前向传播，无需反向传播，可作为即插即用的校准模块与任何TTA方法兼容。

Result: 在4个基线、5种TTA方法和2个现实场景（使用3种模型架构）的综合评估中，SICL相比传统校准方法平均降低校准误差13个百分点。

Conclusion: SICL提供了一种高效、无需反向传播的不确定性校准方法，能显著提升TTA在动态测试环境下的可靠性，适用于高风险应用领域。

Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

</details>


### [116] [Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects](https://arxiv.org/abs/2512.07393)
*Yann Bourdin,Pierrick Legrand,Fanny Roche*

Main category: cs.LG

TL;DR: 该论文研究了在数字音频效果建模中优化截断时间反向传播（TBPTT）的方法，重点关注动态范围压缩效果。通过调整TBPTT的关键超参数（序列数、批大小和序列长度），提升了模型性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在数字音频效果建模中，特别是动态范围压缩这种复杂效果，需要有效的训练方法。TBPTT是处理长序列数据的常用技术，但其超参数设置对模型性能有重要影响。研究如何优化这些参数可以提高模型准确性、训练稳定性并降低计算成本。

Method: 采用卷积-循环神经网络架构，通过大量实验评估TBPTT的三个关键超参数：序列数、批大小和序列长度。实验在有用户控制条件和无条件的两种数据集上进行，进行客观性能评估和主观听觉测试。

Result: 实验结果表明，精心调整TBPTT参数可以显著提升模型准确性、增强训练稳定性，同时减少计算需求。客观评估显示优化设置改善了性能，主观听觉测试表明修订后的TBPTT配置保持了高感知质量。

Conclusion: TBPTT超参数的优化对于数字音频效果建模至关重要。通过系统调整序列数、批大小和序列长度，可以在保持感知质量的同时提高模型性能和训练效率，为音频效果建模提供了实用的训练策略。

Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.

</details>


### [117] [Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse](https://arxiv.org/abs/2512.07400)
*Giulia Lanzillotta,Damiano Meier,Thomas Hofmann*

Main category: cs.LG

TL;DR: 论文揭示了持续学习中深度特征空间遗忘与浅层分类器遗忘的差异，发现小缓冲区能有效防止深度遗忘但需要大缓冲区缓解浅层遗忘，提出通过修正统计伪影可在最小回放下实现鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的一个悖论：神经网络即使输出预测失败，仍能保留过去任务的线性可分表示。需要理解深度特征空间遗忘与浅层分类器遗忘的区别，并解释为什么经验回放中缓冲区大小对这两种遗忘的影响不同。

Method: 将神经崩溃框架扩展到顺序设置，分析深度遗忘作为几何漂移的特征，证明任何非零回放分数都能保证线性可分性的保留。识别小缓冲区导致的"强崩溃"会产生秩不足协方差和膨胀的类均值，使分类器无法识别真实边界。

Result: 揭示了经验回放中的关键不对称性：小缓冲区能成功锚定特征几何并防止深度遗忘，但缓解浅层遗忘通常需要更大的缓冲区容量。将持续学习与分布外检测统一，挑战了对大缓冲区的依赖。

Conclusion: 通过显式修正统计伪影（秩不足协方差和膨胀类均值），可以在最小回放下实现鲁棒性能，为持续学习提供了新的理论视角和实践方向。

Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.

</details>


### [118] [Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.07417)
*Giray Önür,Azita Dabiri,Bart De Schutter*

Main category: cs.LG

TL;DR: 提出一个多智能体强化学习框架，用于自适应调整状态反馈交通控制器的参数，结合状态反馈控制器的反应性和强化学习的适应性


<details>
  <summary>Details</summary>
Motivation: 传统交通管理策略（如路线引导、匝道控制和交通信号控制）通常依赖状态反馈控制器，虽然简单反应快，但缺乏应对复杂时变交通动态的适应性

Method: 多智能体强化学习框架，每个智能体自适应调整状态反馈交通控制器的参数，以较低频率调整参数而非高频直接确定控制动作，提高训练效率

Result: 在模拟的多类交通网络中评估，结果显示该框架优于无控制和固定参数状态反馈控制，与单智能体RL自适应状态反馈控制性能相当，但对部分故障有更好的鲁棒性

Conclusion: 多智能体强化学习框架成功结合了状态反馈控制器的反应性和强化学习的适应性，提高了交通控制系统的训练效率和鲁棒性

Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.

</details>


### [119] [Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models](https://arxiv.org/abs/2512.07419)
*Haidong Kang,Jun Du,Lihong Lin*

Main category: cs.LG

TL;DR: 提出TAP框架，利用大语言模型自动发现混合精度量化的训练无关代理，无需人工专家参与或训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法要么依赖昂贵的可微分优化（效率低且不灵活），要么需要人工专家设计代理（劳动密集且需要专业知识），因此需要自动化、无需训练的代理发现方法。

Method: 提出LLM驱动的训练无关自动代理发现框架TAP，利用大语言模型为混合精度量化寻找优质代理。采用基于直接策略优化的强化学习优化提示，在LLM和MPQ任务间建立正反馈循环。

Result: 在主流基准测试中，TAP实现了最先进的性能表现。

Conclusion: TAP为混合精度量化社区提供了LLM驱动设计算法的新视角，有望显著推动该领域发展。

Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

</details>


### [120] [MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.07430)
*Yangle Li,Danli Luo,Haifeng Hu*

Main category: cs.LG

TL;DR: 提出MIDG框架，通过混合不变专家模型和跨模态适配器解决多模态情感分析中的领域泛化问题，提升跨模态协同和知识注入效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析的领域泛化方法在提取不变特征时忽视了模态间的协同作用，无法准确捕捉多模态数据的丰富语义信息。同时，现有的知识注入技术存在跨模态知识碎片化问题，忽略了超越单模态界限的特定表示。

Method: 1. 混合不变专家模型：提取领域不变特征，增强模型学习模态间协同关系的能力。2. 跨模态适配器：通过跨模态知识注入增强多模态表示的语义丰富度。

Result: 在三个数据集上进行的广泛领域实验表明，提出的MIDG框架取得了优越的性能。

Conclusion: 提出的MIDG框架通过增强模态间协同和跨模态知识注入，有效解决了多模态情感分析中的领域泛化问题，提升了模型性能。

Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.

</details>


### [121] [Mitigating Bias in Graph Hyperdimensional Computing](https://arxiv.org/abs/2512.07433)
*Yezi Liu,William Youngwoo Chung,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: FairGHDC：一种用于图超维计算（HDC）的公平性感知训练框架，通过引入偏差校正项和公平性因子来减少群体间的不公平，同时保持HDC的计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 图超维计算（HDC）在认知任务中表现出潜力，但其公平性影响尚未被充分探索。数据表示和决策规则中的偏见可能导致对不同群体的不平等对待，需要研究如何减轻这些偏见。

Method: 提出FairGHDC框架，引入基于差距的人口均等正则化器推导出的偏差校正项，将其转换为标量公平性因子，用于缩放真实标签类别超向量的更新。该方法直接在超向量空间中进行去偏，无需修改图编码器或反向传播。

Result: 在六个基准数据集上的实验表明，FairGHDC显著减少了人口均等和机会均等差距，同时保持与标准GNN和公平性感知GNN相当的准确性。在GPU训练时间上比GNN基线快约10倍。

Conclusion: FairGHDC成功解决了图HDC中的公平性问题，在保持HDC计算效率优势的同时，有效减轻了群体偏见，为公平性感知的图超维计算提供了实用框架。

Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.

</details>


### [122] [KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models](https://arxiv.org/abs/2512.07437)
*Chenwei Shi,Xueyu Luan*

Main category: cs.LG

TL;DR: KAN-Dreamer：将KAN和FastKAN架构集成到DreamerV3框架中，替换部分MLP和卷积组件，在保持性能的同时探索参数效率和可解释性


<details>
  <summary>Details</summary>
Motivation: 结合DreamerV3的卓越样本效率与KANs的参数效率和可解释性优势，同时通过FastKAN变体解决KANs的计算开销问题

Method: 1. 将DreamerV3中的特定MLP和卷积组件替换为KAN和FastKAN层；2. 在JAX-based World Model中实现完全向量化版本并简化网格管理；3. 在三个子系统（视觉感知、潜在预测、行为学习）中系统评估

Result: 在DeepMind Control Suite（walker_walk）上的实验表明，使用适配的FastKAN作为奖励和继续预测器的替代方案，在样本效率、训练速度和渐近性能方面与原始MLP架构表现相当

Conclusion: KAN-Dreamer作为初步研究表明，FastKAN可以作为DreamerV3中MLP的有效替代品，为未来基于KAN的世界模型开发奠定了基础

Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.

</details>


### [123] [Forget and Explain: Transparent Verification of GNN Unlearning](https://arxiv.org/abs/2512.07450)
*Imran Ahsan,Hyunwook Yu,Jinsung Kim,Mucheol Kim*

Main category: cs.LG

TL;DR: 提出了一种基于可解释性的GNN遗忘验证器，通过对比删除前后的模型快照，使用归因偏移和局部结构变化作为透明证据来验证遗忘是否真正发生。


<details>
  <summary>Details</summary>
Motivation: 现有GNN遗忘方法主要关注效率和可扩展性，但缺乏透明度，难以验证遗忘是否真正发生，特别是在GDPR等隐私法规要求下。

Method: 提出可解释性驱动的验证器，在删除前后对模型进行快照，使用五种可解释性指标：残差归因、热图偏移、可解释性分数偏差、图编辑距离和诊断图规则偏移。

Result: 评估了两种骨干网络（GCN、GAT）和四种遗忘策略（Retrain、GraphEditor、GNNDelete、IDEA）在五个基准数据集上的表现。结果显示Retrain和GNNDelete实现近乎完全遗忘，GraphEditor提供部分擦除，IDEA留下残差信号。

Conclusion: 可解释性差异提供了主要的人类可读遗忘证据，同时成员推断ROC-AUC作为补充的图级隐私信号，为GNN遗忘提供了透明验证框架。

Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.

</details>


### [124] [Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification](https://arxiv.org/abs/2512.07463)
*Rongmei Liang,Zizheng Liu,Xiaofei Wu,Jingwen Tu*

Main category: cs.LG

TL;DR: 提出基于共识结构的统一优化框架，开发分布式并行ADMM算法处理分布式存储大数据中的组合正则化支持向量机，并引入高斯回代法确保收敛，应用于音乐信息检索。


<details>
  <summary>Details</summary>
Motivation: 在人工智能快速发展的时代，组合正则化支持向量机（CR-SVMs）能有效处理数据特征间的结构信息，但在分布式存储的大数据场景中缺乏高效算法。

Method: 提出基于共识结构的统一优化框架，适用于多种损失函数和组合正则化项（包括非凸正则化）。基于该框架开发分布式并行ADMM算法，引入高斯回代法确保收敛，并提出稀疏组套索支持向量机（SGL-SVM）模型应用于音乐信息检索。

Result: 理论分析表明算法计算复杂度不受不同正则化项和损失函数影响，具有普适性。在合成和自由音乐档案数据集上的实验验证了算法的可靠性、稳定性和效率。

Conclusion: 提出的统一优化框架和分布式并行ADMM算法有效解决了分布式存储大数据中CR-SVMs的计算问题，具有强扩展性和普适性，在音乐信息检索等应用中表现出色。

Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.

</details>


### [125] [Materium: An Autoregressive Approach for Material Generation](https://arxiv.org/abs/2512.07486)
*Niklas Dobberstein,Jan Hamaekers*

Main category: cs.LG

TL;DR: Materium：基于自回归Transformer的晶体结构生成模型，通过序列化3D材料表示实现快速、可扩展的晶体生成


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在晶体结构生成中需要多次去噪步骤、计算成本高的问题，开发更快速、高效的生成方法

Method: 将3D材料表示（元素氧化态、分数坐标、晶格参数）转换为token序列，使用自回归Transformer进行生成，直接预测精确的原子位置

Result: 模型可在单GPU上几小时内完成训练，在GPU和CPU上生成速度远超扩散方法；在单条件和多条件设置下均表现良好，生成的结构与输入条件一致

Conclusion: Materium提供了一种高效、快速的晶体结构生成方法，能够根据多种材料属性条件生成符合要求的候选结构，为材料设计提供了实用工具

Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.

</details>


### [126] [Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent](https://arxiv.org/abs/2512.07490)
*Zhiyu Liu,Zhi Han,Yandong Tang,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出交替预条件梯度下降(APGD)算法，解决低管秩张量估计中过参数化导致的梯度下降收敛缓慢问题，即使在高估秩的情况下也能实现线性收敛。


<details>
  <summary>Details</summary>
Motivation: 低管秩张量估计是信号处理、机器学习和图像科学中的基础问题。传统方法使用张量奇异值分解计算量大，不适用于大规模张量。现有基于因子分解的梯度下降方法需要准确估计张量秩，当秩被高估时，梯度下降收敛显著减慢甚至发散。

Method: 提出交替预条件梯度下降(APGD)算法：在原始梯度上添加预条件项，交替更新两个因子张量。算法在过参数化设置下加速收敛，基于目标函数的几何假设，建立了更通用的低管秩张量估计问题的线性收敛保证。

Result: 理论分析表明APGD即使在过参数化情况下也能实现线性收敛，且收敛率与张量条件数无关。对低管秩张量分解和低管秩张量恢复的具体案例进行了分析。在合成数据上的大量仿真验证了理论断言。

Conclusion: APGD算法有效解决了低管秩张量估计中过参数化导致的收敛问题，为大规模张量处理提供了高效解决方案，具有理论保证和实际验证。

Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.

</details>


### [127] [Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces](https://arxiv.org/abs/2512.07509)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 论文提出使用预定义向量系统（如An根系统）作为潜在空间配置目标，可在无分类层情况下训练分类器，特别适用于超多类别数据集，并能加速训练和收敛。


<details>
  <summary>Details</summary>
Motivation: 神经网络性能与其潜在空间嵌入分布特性密切相关。传统方法在处理极多类别数据集时面临分类层复杂性和训练效率问题，需要一种能确保理想潜在空间结构的新方法。

Method: 使用预定义向量系统（特别是An根系统向量）作为潜在空间配置目标，训练编码器和视觉变换器时无需传统分类层。研究不同向量系统的构建方法，并探索最小潜在空间维度对特定类别数的优化。

Result: 该方法显著加速了ImageNet-1K和50k-600k类别数据集的训练。使用最小潜在空间维度能实现更快收敛，同时减少存储神经网络嵌入的向量数据库大小。

Conclusion: 预定义向量系统为神经网络潜在空间配置提供了有效框架，特别适用于大规模类别分类任务，能提高训练效率、加速收敛，并减少存储需求。

Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.

</details>


### [128] [Machine Learning: Progress and Prospects](https://arxiv.org/abs/2512.07519)
*Alexander Gammerman*

Main category: cs.LG

TL;DR: 这篇1996年的就职讲座回顾了机器学习的历史起源，从亚里士多德、奥卡姆到费舍尔、香农，探讨了该领域的多学科背景和不同研究方向。


<details>
  <summary>Details</summary>
Motivation: 讲座旨在追溯机器学习的起源，展示该领域深厚的历史根基和多学科背景，强调机器学习并非新兴学科，而是有着悠久的哲学和数学渊源。

Method: 采用历史回顾的方法，从不同历史时期和思想流派追溯机器学习的起源，包括哲学（亚里士多德、奥卡姆）、统计学（费舍尔）和计算机科学（香农）等不同领域的贡献。

Result: 展示了机器学习思想的多元起源，从古希腊哲学到中世纪逻辑学，再到20世纪的统计学和计算机科学，形成了包含归纳学习、神经网络、聚类和学习理论等多个研究方向的综合领域。

Conclusion: 机器学习是一个历史悠久且多学科交叉的领域，其思想根源可以追溯到古代哲学，经过多个世纪的发展，形成了今天包含多个子领域的综合性学科。

Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.

</details>


### [129] [Model-Based Reinforcement Learning Under Confounding](https://arxiv.org/abs/2512.07528)
*Nishanth Venkatesh,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出一种在上下文未观测的混淆环境中进行模型强化学习的方法，通过代理变量识别混淆奖励期望，结合行为平均转移模型构建替代MDP


<details>
  <summary>Details</summary>
Motivation: 在上下文未观测的C-MDPs中，传统模型学习方法存在根本不一致性，因为行为策略下的转移和奖励机制与评估状态策略所需的干预量不对应

Method: 采用近端离策略评估方法，利用代理变量在温和可逆条件下识别混淆奖励期望，结合行为平均转移模型构建替代MDP，与最大因果熵模型学习框架集成

Result: 构建的替代MDP具有明确定义且一致的Bellman算子，适用于状态策略，能够在上下文未观测、不可用或不切实际收集的混淆环境中实现原则性模型学习和规划

Conclusion: 该方法解决了混淆环境中模型强化学习的根本挑战，为上下文信息缺失的场景提供了可行的解决方案

Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.

</details>


### [130] [FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.07539)
*Qingyuan Yang,Shizhuo,Dongyue Chen,Da Teng,Zehua Gan*

Main category: cs.LG

TL;DR: FRWKV：结合线性注意力与频域分析的时间序列预测框架，将计算复杂度从O(T²)降低到O(T)，在8个真实数据集上取得最佳平均排名。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长序列时间序列预测中存在两个主要瓶颈：1）二次计算复杂度O(T²)限制了处理长序列的能力；2）未能有效利用频域信息。需要开发更高效且能充分利用频谱信息的模型。

Method: 提出FRWKV框架，结合RWKV的线性注意力机制（O(T)复杂度）与频域分析。模型集成线性注意力机制和频域分析，在注意力路径上实现线性计算复杂度，同时利用频谱信息增强时间特征表示。

Result: 在8个真实世界数据集上，FRWKV取得了第一的平均排名。消融研究证实了线性注意力和频域编码器组件都起到关键作用。

Conclusion: 这项工作展示了线性注意力与频域分析之间的强大协同效应，为可扩展的时间序列建模建立了新范式。代码已开源。

Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.

</details>


### [131] [RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.07542)
*Jad Mounayer,Sebastian Rodriguez,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: RRAEDy是一种新型潜空间动力学模型，通过自动发现合适的潜维度、强制正则化和线性化动力学，解决了现有模型需要预先固定潜维度、依赖复杂损失平衡和缺乏正则化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有潜空间动力学模型存在三个主要限制：需要预先固定潜维度、依赖复杂的损失平衡来近似线性动力学、缺乏对潜变量的正则化。这些限制影响了模型的灵活性和性能。

Method: 基于秩约减自编码器（RRAEs），RRAEDy通过奇异值自动排序和剪枝潜变量，同时学习一个潜动态模态分解（DMD）算子来管理其时间演化。这种无结构但线性约束的公式使模型能够学习稳定且低维的动力学，无需辅助损失或手动调参。

Result: 在经典基准测试（包括Van der Pol振荡器、Burgers方程、2D Navier-Stokes和旋转高斯分布）上的实验表明，RRAEDy实现了准确且鲁棒的预测。理论分析证明了学习算子的稳定性。

Conclusion: RRAEDy通过自动发现潜维度、强制正则化和线性化动力学，提供了一种更灵活、更稳定的潜空间动力学建模方法，并可扩展到处理参数化常微分方程。

Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.

</details>


### [132] [ReLaX: Reasoning with Latent Exploration for Large Reasoning Models](https://arxiv.org/abs/2512.07558)
*Shimin Zhang,Xianwei Chen,Yufan Shen,Ziyuan Ye,Jibin Wu*

Main category: cs.LG

TL;DR: ReLaX通过Koopman算子理论线性化大推理模型的隐状态动力学，提出动态谱分散度指标量化探索程度，有效缓解RLVR中的熵崩溃问题，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然能增强大推理模型的推理能力，但常导致熵崩溃，造成策略过早收敛和性能饱和。现有方法主要操纵token级熵来促进探索，但作者认为token生成背后的隐状态动力学包含更丰富的计算结构，能更好地指导探索-利用权衡。

Method: 1) 利用Koopman算子理论获得模型隐状态动力学的线性化表示；2) 提出动态谱分散度(DSD)指标量化隐状态动力学的异质性，作为策略探索的直接指标；3) 提出ReLaX范式，在策略优化中显式结合隐状态动力学来调节探索和利用。

Result: 在广泛的多模态和纯文本推理基准测试中，ReLaX显著缓解了过早收敛问题，并持续实现了最先进的性能。

Conclusion: 通过分析隐状态动力学并引入DSD指标，ReLaX有效解决了RLVR中的熵崩溃问题，为大推理模型的策略优化提供了更有效的探索-利用平衡机制。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

</details>


### [133] [Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting](https://arxiv.org/abs/2512.07569)
*Joel Ekstrand,Tor Mattsson,Zahra Taghiyarrenani,Slawomir Nowaczyk,Jens Lundström,Mikael Lindén*

Main category: cs.LG

TL;DR: WECA方法通过加权对比适应提升多元时间序列在异常条件下的预测可靠性，在ATM现金物流等应用中显著改善异常数据的预测精度


<details>
  <summary>Details</summary>
Motivation: 现代深度预测模型在正常数据上表现良好，但在分布偏移（如异常条件）下常常失效，而ATM现金物流等应用需要可靠处理突发需求变化

Method: 提出加权对比适应（WECA），使用加权对比目标对齐正常和异常增强表示，保留异常相关信息同时保持良性变化下的一致性

Result: 在全国ATM交易数据集上，WECA将异常影响数据的SMAPE相比正常训练基线提升了6.1个百分点，对正常数据性能影响可忽略

Conclusion: WECA能在不牺牲正常操作性能的情况下，显著提升异常条件下的预测可靠性

Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.

</details>


### [134] [Time Series Foundation Models for Process Model Forecasting](https://arxiv.org/abs/2512.07624)
*Yongbo Yu,Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）在过程模型预测（PMF）中表现出色，零样本使用即可超越传统方法，微调提升有限，展示了跨领域时间结构迁移的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和深度学习方法在过程模型预测中表现有限，主要由于直接跟随关系时间序列的稀疏性和异质性。研究探索时间序列基础模型作为替代方案，评估其在PMF任务中的表现。

Method: 使用真实事件日志生成的直接跟随关系时间序列，比较时间序列基础模型的零样本使用（无需额外训练）与在PMF特定数据上微调的变体。与传统和专门模型进行对比，评估MAE和RMSE等预测误差指标。

Result: 时间序列基础模型通常比传统和专门模型获得更低的预测误差，表明从非过程领域有效迁移了时间结构。微调虽然能进一步提高准确性，但提升幅度较小，在较小或更复杂数据集上可能消失，因此零样本使用仍是强默认选择。

Conclusion: 时间序列基础模型在过程相关时间序列预测中展现出强大的泛化能力和数据效率，为过程模型预测提供了新的有效方法，首次系统评估了时间基础模型在PMF任务中的应用。

Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.

</details>


### [135] [A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance](https://arxiv.org/abs/2512.07647)
*Georgios Tzachristas,Lei Deng,Ioannis Tzachristas,Gong Zhang,Renhai Chen*

Main category: cs.LG

TL;DR: 本文提出了一个统一的数学框架，用于认证Top-k注意力截断，量化了分布和输出层面的近似误差，并推导出确定性误差界限和渐近规则。


<details>
  <summary>Details</summary>
Motivation: 注意力机制中的Top-k截断虽然能提高计算效率，但缺乏对近似误差的严格理论保证。现有方法通常使用通用不等式，无法提供特定于Top-k的精确误差界限。

Method: 建立统一的数学框架，证明总变差距离等于丢弃的softmax尾部质量，并推导出仅使用有序logits的非渐近确定性界限。通过精确的头尾分解，将输出误差分解为TV距离与头尾均值差的乘积。

Result: 在i.i.d.高斯分数模型下推导出闭合形式的尾部质量和渐近规则，实验证明预测的k_ε/n缩放规律，认证Top-k能将评分键平均减少2-4倍，同时满足指定的总变差预算。

Conclusion: 该框架为Top-k注意力截断提供了严格的误差认证，能够显著减少计算成本的同时保证近似质量，为高效注意力机制的设计提供了理论基础。

Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=τ\|μ_{\mathrm{tail}}-μ_{\mathrm{head}}\|_2$ with $τ=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\leτ\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxΦ_c(σ+Φ^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.

</details>


### [136] [Depth-Wise Activation Steering for Honest Language Models](https://arxiv.org/abs/2512.07667)
*Gracjan Góral,Marysia Winkels,Steven Basart*

Main category: cs.LG

TL;DR: 提出一种无需训练的激活引导方法，使用高斯调度在深度上分配引导强度，以提升大语言模型的诚实性而非准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有时会陈述虚假信息，尽管内部知道正确答案，这是诚实性问题而非准确性问题，影响了可审计性和安全性。现有方法主要优化事实正确性或依赖重新训练和脆弱的单层编辑，对真实报告的控制有限。

Method: 提出无需训练的激活引导方法，使用高斯调度在深度上分配引导强度。该方法简单、模型无关、无需微调，通过高斯调度在深度上加权引导强度，而非单层编辑。

Result: 在MASK基准测试中评估了LLaMA、Qwen和Mistral家族的7个模型，高斯调度在6个模型中比无引导和单层基线提升了诚实性。在LLaMA-3.1-8B-Instruct和Qwen-2.5-7B-Instruct上的等预算消融实验显示，高斯调度优于随机、均匀和盒式滤波深度分配。

Conclusion: 该方法提供了一种低成本的控制手段，可以从模型现有能力中引出真实报告。深度上如何分配干预对结果有实质性影响，而不仅仅是总强度。

Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.

</details>


### [137] [A Bootstrap Perspective on Stochastic Gradient Descent](https://arxiv.org/abs/2512.07676)
*Hongjian Lan,Yucong Liu,Florian Schäfer*

Main category: cs.LG

TL;DR: SGD通过梯度变异性作为数据收集过程随机性的代理，隐式正则化梯度协方差矩阵的迹，从而控制算法变异性，提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究SGD比确定性GD具有更好泛化能力的原因，从统计bootstrap角度理解SGD如何利用梯度变异性来近似数据收集过程的随机性。

Method: 通过经验风险最小化的理想化实验，证明SGD倾向于选择在重采样下稳健的参数；理论分析证明SGD隐式正则化梯度协方差矩阵的迹；在神经网络训练中显式加入算法变异性估计作为正则器。

Result: SGD能够避免虚假解，即使这些解位于训练损失更宽更深的极小值中；显式加入算法变异性正则器能够改善测试性能；支持bootstrap估计是SGD泛化优势的基础这一主张。

Conclusion: SGD通过将批次采样的梯度变异性作为数据收集过程随机性的代理，隐式正则化梯度协方差矩阵的迹，从而控制算法变异性，产生对采样噪声不敏感的解决方案，提高泛化能力。

Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.

</details>


### [138] [In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models](https://arxiv.org/abs/2512.07705)
*Saroj Gopali,Bipin Chhetri,Deepika Giri,Sima Siami-Namini,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 比较时间序列预测方法，发现TimesFM表现最佳，LLMs在零样本学习中也表现良好，预训练时间序列基础模型是实时预测的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 随着预训练基础模型（如LLMs和TimesFM）的发展，需要研究这些基础模型是否能在时间序列数据分析预测中超越现有方法（如ARIMA、Transformer、LSTM、TCN）。

Method: 研究LLMs在时间序列预测中的表现，探索上下文学习、零样本学习和少样本学习训练方法，使用OpenAI o4-mini、Gemini 2.5 Flash Lite、TimesFM以及TCN和LSTM网络进行比较。

Result: TimesFM表现最佳，具有最低RMSE值（0.3023）和竞争性推理时间（266秒）。OpenAI的o4-mini在零样本学习中也表现出良好性能。

Conclusion: 预训练时间序列基础模型是实时预测的有前景方向，能够以最少的模型适应实现准确且可扩展的部署。

Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.

</details>


### [139] [Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity](https://arxiv.org/abs/2512.07723)
*Yonggeon Lee,Jibin Hwang,Alfred Malengo Kondoro,Juhyun Song,Youngtae Noh*

Main category: cs.LG

TL;DR: 提出基于Transformer的实时事件预测模型，用于准确预测电动汽车用户的出发时间，以优化充电策略延长电池寿命。


<details>
  <summary>Details</summary>
Motivation: 电动汽车锂电池在高电量状态下会加速退化，可以通过延迟充满电直到出发前缓解，这需要准确预测用户出发时间。

Method: 使用Transformer-based实时事件预测模型，将每天离散化为基于网格的token序列，利用流式上下文信息而非仅依赖历史模式。

Result: 在93名用户的真实世界研究中，该方法有效捕捉了个体日常中的不规则出发模式，性能优于基线模型。

Conclusion: 该方法具有实际部署潜力，有助于可持续交通系统发展。

Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.

</details>


### [140] [A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data](https://arxiv.org/abs/2512.07741)
*Agnes Norbury,George Fairs,Alexandra L. Georgescu,Matthew M. Nour,Emilia Molimpakis,Stefano Goria*

Main category: cs.LG

TL;DR: 该研究提出使用贝叶斯网络模型从语音特征预测抑郁和焦虑症状，在30,135名说话者的大规模数据集上验证了性能，并评估了人口统计学公平性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 临床精神病学评估中，医生需要同时考虑患者的口头报告和非言语信号（语调、语速、流畅性、反应性、肢体语言等）。整合这些不同信息源具有挑战性，需要智能工具支持，但目前临床实践中尚未实现。研究旨在解决采用障碍，特别是通过贝叶斯网络建模方法。

Method: 采用贝叶斯网络建模方法，从语音和言语特征预测抑郁和焦虑症状。研究基于大规模数据集（30,135名独特说话者），评估模型在条件和症状预测方面的性能，同时评估人口统计学公平性，并研究不同输入模态类型之间的整合和冗余性。

Result: 模型表现出良好的预测性能：抑郁和焦虑的ROC-AUC分别为0.842和0.831，预期校准误差分别为0.018和0.015；核心个体症状的ROC-AUC均大于0.74。研究还评估了人口统计学公平性，探索了临床实用性指标和精神健康服务使用者的可接受性。

Conclusion: 当提供足够丰富的大规模多模态数据流，并在症状层面而非疾病层面表示常见精神状况时，贝叶斯网络模型是构建稳健评估支持工具的原则性方法。这种方法能够以透明、可解释的格式提供临床相关输出，并直接适用于专家临床监督。

Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.

</details>


### [141] [Formalized Hopfield Networks and Boltzmann Machines](https://arxiv.org/abs/2512.07766)
*Matteo Cipollina,Michail Karatarakis,Freek Wiedijk*

Main category: cs.LG

TL;DR: 本文在Lean 4中形式化神经网络，涵盖确定性和随机模型，包括Hopfield网络和Boltzmann机，证明了收敛性和学习规则的正确性。


<details>
  <summary>Details</summary>
Motivation: 神经网络应用广泛，但其分析和验证仍然具有挑战性。为了提供严格的数学保证，需要在证明助手中对神经网络进行形式化验证。

Method: 使用Lean 4定理证明器形式化神经网络。首先形式化Hopfield网络，证明其收敛性和Hebbian学习规则的正确性（限于两两正交模式）。然后形式化随机网络，以Boltzmann机为例，证明其遍历性并建立到唯一平稳分布的收敛性，使用了Perron-Frobenius定理的新形式化。

Result: 成功在Lean 4中形式化了确定性和随机神经网络模型。证明了Hopfield网络的收敛性和Hebbian学习规则的正确性。证明了Boltzmann机的遍历性，展示了其收敛到唯一平稳分布。

Conclusion: 该工作展示了在定理证明器中形式化神经网络的可行性，为神经网络分析和验证提供了严格的数学基础。形式化的Perron-Frobenius定理也为随机网络的分析提供了新工具。

Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.

</details>


### [142] [GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory](https://arxiv.org/abs/2512.07782)
*Jiaxu Liu,Yuhe Bai,Christos-Savvas Bouganis*

Main category: cs.LG

TL;DR: 提出GatedFWA：一种内存门控的滑动窗口注意力机制，在保持线性时间效率的同时，通过可学习的收缩门稳定内存更新并控制梯度流，解决了传统滑动窗口注意力训练目标无界和Softmax注意力内存收缩的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的Softmax全注意力存在二次方复杂度问题，而滑动窗口注意力(SWA)虽然实现线性时间编码/解码，但在关联内存解释下其差分式更新导致训练目标无界。同时，Softmax注意力通过归一化更新会导致内存收缩和梯度消失问题。

Method: 提出GatedFWA机制：为每个token/head累积一个门控值，将其作为衰减偏置添加到注意力logits中，作为内存递归中的可学习收缩。实现了融合的单次门预处理和与FlashAttention兼容的内核，在滑动掩码下注入门控，确保I/O效率和数值稳定性。

Result: 在语言建模基准测试中，GatedFWA以可忽略的开销实现了有竞争力的吞吐量，能更好地利用全局上下文，并能与NSA等token压缩/选择方法无缝集成，适用于各种自回归领域。

Conclusion: GatedFWA在保持滑动窗口注意力效率优势的同时，通过门控机制解决了内存更新稳定性和梯度控制问题，为高效的自回归建模提供了新的解决方案。

Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.

</details>


### [143] [Group Representational Position Encoding](https://arxiv.org/abs/2512.07805)
*Yifan Zhang,Zixiang Chen,Yifeng Liu,Zhen Qin,Huizhuo Yuan,Kangping Xu,Yang Yuan,Quanquan Gu,Andrew Chi-Chih Yao*

Main category: cs.LG

TL;DR: GRAPE是一个基于群作用的统一位置编码框架，包含乘法旋转和加法logit偏置两种机制，将RoPE和ALiBi作为特例包含在内。


<details>
  <summary>Details</summary>
Motivation: 为长上下文模型提供一个原则性的位置几何设计空间，统一现有的位置编码方法，特别是将RoPE和ALiBi等机制纳入一个统一的数学框架中。

Method: 基于群作用理论，提出两种位置编码机制：1) 乘法GRAPE：使用SO(d)群中的乘法旋转，通过指数映射生成相对、组合、保范的位置映射；2) 加法GRAPE：使用GL群中的单能作用，产生加法logit偏置。两种方法都能保持相对位置关系和流式缓存能力。

Result: GRAPE框架精确恢复了RoPE（当d/2平面为规范坐标对且具有对数均匀谱时）以及ALiBi和Forgetting Transformer作为特例。通过学习的交换子空间和紧凑非交换混合，以O(d)和O(rd)的成本扩展了几何表达能力。

Conclusion: GRAPE为长上下文模型提供了一个统一的位置编码设计空间，将现有的重要位置编码方法作为特例包含在内，并提供了扩展几何表达能力的灵活机制。

Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,ω\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.

</details>


### [144] [Provable Long-Range Benefits of Next-Token Prediction](https://arxiv.org/abs/2512.07818)
*Xinyuan Cao,Santosh S. Vempala*

Main category: cs.LG

TL;DR: 本文证明：优化RNN的下一词预测任务，可以学习到训练分布的长程结构，使得模型生成的k个连续token与真实文档的k个token在计算上不可区分。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型通过下一词预测训练，却能生成连贯文档并捕捉长程结构。本文旨在从理论角度解释这一现象，证明下一词预测在理论上确实具有学习长程结构的能力。

Method: 使用循环神经网络(RNN)进行下一词预测优化，从计算复杂性角度分析模型能力。证明对于训练分布中的文档，任何有界描述长度的算法都无法区分模型生成的k个连续token与真实文档的k个token。

Result: 证明了下一词预测训练RNN可以近似训练分布：模型生成的k个token与真实文档的k个token在计算上不可区分。给出了实现k-token不可区分性所需模型大小的多项式边界（与文档长度无关）。

Conclusion: 下一词预测在理论上具有学习长程结构的能力，这为实践中观察到的语言模型长程连贯性提供了复杂性理论解释。模型大小只需多项式增长（相对于k）即可实现k-token不可区分性。

Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.

</details>


### [145] [The Adoption and Usage of AI Agents: Early Evidence from Perplexity](https://arxiv.org/abs/2512.07828)
*Jeremy Yang,Noah Yonack,Kate Zyskowski,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 首个大规模AI智能体在开放网络环境中的使用研究，基于Perplexity的Comet浏览器及其智能体助手，分析数百万用户交互数据，揭示用户采用、使用强度和用例模式。


<details>
  <summary>Details</summary>
Motivation: 理解通用AI智能体在开放网络环境中的实际采用情况、使用强度和具体用例，填补大规模实证研究的空白，为研究者、企业、政策制定者和教育工作者提供洞察。

Method: 基于Perplexity的Comet浏览器及其Comet Assistant智能体的数百万匿名用户交互数据，采用分层智能体分类法（主题、子主题、任务三级）系统分析使用模式。

Result: 发现用户采用存在显著异质性：早期采用者、高GDP国家用户、教育程度高者、数字/知识密集型行业从业者更可能采用。生产力与工作流、学习与研究两大主题占57%查询；个人使用占55%，专业和教学分别占30%和16%；短期使用具有粘性，长期用户转向更认知导向的主题。

Conclusion: AI智能体的扩散对各方有重要影响，需要新的研究视角。使用模式显示从简单任务向复杂认知活动的演进，为理解AI能力发展提供了实证基础。

Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [146] [Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices](https://arxiv.org/abs/2512.06443)
*Xiangyu Li,Chengyu Yin,Weijun Wang,Jianyu Wei,Ting Cao,Yunxin Liu*

Main category: cs.DC

TL;DR: 提出向量查找表(Vec-LUT)方法，解决传统标量查找表在并行推理中内存带宽利用率低的问题，在边缘设备上实现高达4.2倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署到边缘设备，量化技术已发展到1.58位。虽然基于查找表的推理让CPU比NPU更快，但标量查找表范式在并行推理（如预填充、测试时扩展等场景）中存在内存带宽利用不足的问题，根源在于每个token都需要重复且非连续的内存访问。

Method: 提出向量查找表(Vec-LUT)新范式：1) 构建跨并行token的统一查找表，每个索引执行一次1→N查找；2) 向量查找表中心张量布局；3) 缓存感知流式查找技术。

Result: 在5个边缘设备和3个大语言模型上的评估显示，Vec-LUT比现有最优方法提升高达4.2倍。实现已集成到llama.cpp中。

Conclusion: 向量查找表解决了标量查找表在并行推理中的内存带宽瓶颈，为边缘设备上的大语言模型部署提供了更高效的推理方案，推动了无处不在的设备端智能发展。

Abstract: Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.
  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.
  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.

</details>


### [147] [Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks](https://arxiv.org/abs/2512.06784)
*Long Shi,Bingyan Ou,Kang Wei,Weihao Zhu,Zhe Wang,Zhiyong Chen*

Main category: cs.DC

TL;DR: 提出基于Lyapunov优化的分布式MoE训练令牌路由框架Stable-MoE，解决边缘网络中异构计算能力和随机令牌到达导致的负载积压、资源低效和性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统分布式MoE训练在资源受限的边缘网络中面临挑战：异构计算能力和随机令牌到达导致工作负载积压、资源利用低效和性能下降，需要新的路由策略来保证系统稳定性和效率。

Method: 提出Lyapunov-based token routing框架Stable-MoE，将长期随机优化问题转化为可处理的每时隙子问题，在线决策令牌路由和计算频率分配，无需未来系统状态信息，同时保证令牌队列和能量队列的长期稳定性。

Result: 在SVHN和CIFAR-100数据集上的实验表明，Stable-MoE相比基线方法在系统吞吐量上至少提升40%，在测试准确率上至少提升5%。

Conclusion: Stable-MoE框架有效解决了资源异构边缘网络中分布式MoE训练的挑战，通过Lyapunov优化实现了系统吞吐量和门控一致性的最大化，同时保证了系统的长期稳定性。

Abstract: The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.

</details>


### [148] [Cloud Revolution: Tracing the Origins and Rise of Cloud Computing](https://arxiv.org/abs/2512.06800)
*Deepa Gurung,S M Zia Ur Rashid,Zain ul Abdeen,Suman Rath*

Main category: cs.DC

TL;DR: 本文重新审视了云计算的历史演变，从虚拟化、分布式系统等技术基础到现代全球联合云生态系统，分析了技术经济驱动力、组织计算习惯改变、当前局限性和新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 重新审视云计算领域的历史演变，分析其从基础技术发展到当前广泛应用的过程，理解云计算如何改变组织计算习惯，并探讨其面临的挑战和未来发展方向。

Method: 采用历史分析方法，重新审视云计算的技术演进历程，包括资源共享和基于效用的计算方法的初始理念，以及超大规模数据中心和现代全球联合云生态系统的发展。

Result: 云计算的发展经历了数十年技术进步，改变了组织计算习惯，降低了数据密集型和计算密集型应用的门槛，但同时也带来了安全配置漏洞、特定法规要求和供应商依赖等局限。

Conclusion: 云计算是一个快速变化的范式，其未来方向需要在可扩展性、开放性和信任之间取得平衡，新兴趋势包括边缘与云基础设施融合、AI优化架构和量子计算服务的初步采用。

Abstract: The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.

</details>


### [149] [Optimizing video analytics inference pipelines: a case study](https://arxiv.org/abs/2512.07009)
*Saeid Ghafouri,Yuming Ding,Katerine Diaz Chito,Jesús Martinez del Rincón,Niamh O'Connell,Hans Vandierendonck*

Main category: cs.DC

TL;DR: 该论文通过系统级优化（多级并行化、GPU加速、向量化聚类、内存高效后处理）将家禽福利监控系统的视频分析速度提升2倍，同时保持模型精度


<details>
  <summary>Details</summary>
Motivation: 商业养殖场的高分辨率视频和近实时监控需求产生大量计算负载，需要成本效益高且可扩展的视频分析解决方案

Method: 采用系统级改进，包括多级并行化、用GPU加速代码替代CPU代码、向量化聚类和内存高效后处理，优化检测、跟踪、聚类和行为分析模块

Result: 在真实农场视频数据上评估，优化使整个处理流水线获得高达2倍的速度提升，且不损害模型准确性

Conclusion: 研究为构建高吞吐量、低延迟的视频推理系统提供了实用策略，可降低农业和智能传感部署以及其他大规模视频分析应用的基础设施需求

Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.

</details>


### [150] [PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval](https://arxiv.org/abs/2512.07189)
*Jiahao Zhang,Minghui Xu,Hechuan Guo,Xiuzhen Cheng*

Main category: cs.DC

TL;DR: PIR-DSN是首个在去中心化存储网络中集成私有信息检索的协议，通过安全映射和文件复制技术，在保证隐私的同时实现可验证性和拜占庭容错。


<details>
  <summary>Details</summary>
Motivation: 去中心化存储网络作为Web 3.0的基础设施，在文件检索过程中存在用户隐私泄露的风险，当前协议未能充分解决这一关键漏洞。

Method: 提出PIR-DSN协议，集成私有信息检索技术，采用创新的安全映射方法将稀疏文件标识符转换为紧凑整数索引，支持单服务器和多服务器设置，并通过文件复制实现拜占庭容错的私有检索。

Result: 实验评估显示，PIR-DSN在文件上传和删除方面具有可比较的开销，虽然私有检索带来更高的延迟，但保持了可比较的吞吐量，证明了其在隐私敏感应用中的实际可行性。

Conclusion: PIR-DSN是首个在去中心化存储网络中实现私有信息检索的实用协议，为隐私敏感应用提供了可行的解决方案，平衡了隐私保护与系统性能。

Abstract: Decentralized Storage Networks (DSNs) are emerging as a foundational infrastructure for Web 3.0, offering global peer-to-peer storage. However, a critical vulnerability persists: user privacy during file retrieval remains largely unaddressed, risking the exposure of sensitive information. To overcome this, we introduce PIR-DSN, the first DSN protocol to integrate Private Information Retrieval (PIR) for both single and multi-server settings. Our key innovations include a novel secure mapping method that transforms sparse file identifiers into compact integer indexes, enabling both public verifiability of file operations and efficient private retrieval. Furthermore, PIR-DSN guarantees Byzantine-robust private retrieval through file replication across multiple miners. We implement and rigorously evaluate PIR-DSN against three prominent industrial DSN systems. Experimental results demonstrate that PIR-DSN achieves comparable overhead for file upload and deletion. While PIR inherently introduces an additional computational cost leading to higher retrieval latency, PIR-DSN maintains comparable throughput. These findings underscore PIR-DSN's practical viability for privacy-sensitive applications within DSN environments.

</details>


### [151] [ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum](https://arxiv.org/abs/2512.07280)
*Hendrik Reiter,Janick Edinger,Martin Kabierski,Agnes Koschmider,Olaf Landsiedel,Arvid Lepsien,Xixi Lu,Andrea Marrella,Estefania Serral,Stefan Schulte,Florian Tschorsch,Matthias Weidlich,Wilhelm Hasselbring*

Main category: cs.DC

TL;DR: 提出ContinuumConductor框架，在边缘-云连续体中实现去中心化流程挖掘，平衡隐私、响应性和资源效率


<details>
  <summary>Details</summary>
Motivation: 传统流程挖掘假设集中式事件数据收集和分析，但现代工业物联网系统在分布式、资源受限的边缘-云基础设施上运行，需要支持去中心化的流程挖掘方法

Method: 提出ContinuumConductor分层决策框架，指导何时在流程挖掘管道（预处理、关联、发现等）中执行集中或分散处理，分析各层去中心化与集中化的权衡并提出决策标准

Result: 在内河港口流程优化实际用例中演示了ContinuumConductor框架，为网络物理系统和工业物联网系统中的计算感知流程挖掘奠定基础

Conclusion: ContinuumConductor框架为在边缘-云连续体中实现去中心化流程挖掘提供了结构化方法，能够平衡隐私保护、响应性和资源效率，适用于现代工业物联网系统

Abstract: Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.

</details>


### [152] [Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding](https://arxiv.org/abs/2512.07344)
*Shengyuan Ye,Bei Ouyang,Tianyi Qian,Liekang Zeng,Mu Yuan,Xiaowen Chu,Weijie Hong,Xu Chen*

Main category: cs.DC

TL;DR: Venus是一个面向在线视频理解的高效边缘-云端分离内存检索系统，通过边缘处理视频流、构建分层内存和智能关键帧选择，实现实时响应并大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在在线视频理解应用中部署时存在系统开销过大的问题，现有研究主要关注提升模型推理能力而忽视了实际部署约束，导致真实世界部署中产生压倒性的系统开销。

Method: 提出Venus边缘-云端分离架构，包含两个阶段：1) 摄取阶段：在边缘连续处理视频流，通过场景分割和聚类选择关键帧，使用多模态嵌入模型构建分层内存；2) 查询阶段：索引查询并采用基于阈值的渐进采样算法进行关键帧选择，平衡系统成本和推理精度。

Result: Venus相比最先进方法实现了15-131倍的总响应延迟加速，能够在几秒内实现实时响应，同时保持相当甚至更优的推理精度。

Conclusion: Venus通过创新的边缘-云端分离架构和智能内存检索机制，有效解决了视觉语言模型在在线视频理解中的部署效率问题，为实际应用提供了可行的解决方案。

Abstract: Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.

</details>


### [153] [Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism](https://arxiv.org/abs/2512.07350)
*Zhiyuan Wu,Shuai Wang,Li Chen,Kaihui Gao,Dan Li,Yanyu Ren,Qiming Zhang,Yong Wang*

Main category: cs.DC

TL;DR: 提出Latent Parallelism (LP)并行策略，针对视频扩散模型3D时空注意力计算的内存立方增长问题，通过动态旋转潜在空间划分维度减少通信开销97%，保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型(VDMs)在3D时空域进行注意力计算，相比LLMs处理1D序列，内存消耗呈立方增长，需要多GPU并行服务。传统并行策略划分计算图，需要频繁的高维激活传输，造成严重通信瓶颈。

Method: 提出Latent Parallelism (LP)策略：1) 利用扩散去噪过程的局部时空依赖性；2) 在扩散时间步中动态旋转划分维度（时间、高度、宽度）；3) 设计patch对齐的重叠划分策略匹配视觉patch边界；4) 位置感知的潜在重建机制实现平滑拼接。

Result: 在三个基准测试上，LP相比基线方法减少通信开销高达97%，同时保持可比的生成质量。作为非侵入式插件范式，LP可与现有并行策略无缝集成。

Conclusion: LP是首个针对VDM服务的并行策略，通过分解全局去噪问题为可并行子问题，显著降低通信开销，实现高效可扩展的视频生成服务。

Abstract: Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.

</details>


### [154] [Otus Supercomputer](https://arxiv.org/abs/2512.07401)
*Sadaf Ehtesabi,Manoar Hossain,Tobias Kenter,Andreas Krawinkel,Holger Nitsche,Lukas Ostermann,Christian Plessl,Heinrich Riebler,Stefan Rohde,Robert Schade,Michael Schwarz,Jens Simon,Nils Winnwa,Alex Wiens,Xin Wu*

Main category: cs.DC

TL;DR: Otus是德国帕德博恩大学PC2中心于2025年推出的高性能计算集群，作为NHR计划的一部分，补充了Noctua 2系统，提供约两倍计算能力，在Top500排名中CPU分区第164位、GPU分区第255位，在Green500能效排名中GPU分区第5位。


<details>
  <summary>Details</summary>
Motivation: 介绍Otus高性能计算集群的系统架构、硬件软件配置、系统集成和能效设计，为使用该系统的科学家和运营HPC集群的其他中心提供独特见解，并持续更新以反映最新系统设置和测量结果。

Method: 提供系统的全面概述，包括硬件配置（三种节点类型：CPU计算节点、高端GPU节点、HPC级FPGA节点）、软件环境、系统集成设计，以及数据中心建筑的整体集成以确保能效运行。

Result: Otus在Top500全球超级计算机排名中，CPU分区排名第164位，GPU分区排名第255位；在Green500全球能效排名中，GPU分区排名第5位，展示了出色的计算性能和能源效率。

Conclusion: Otus作为Noctua 2的补充系统，提供了约两倍的计算能力，同时保持了三种节点类型的架构特点，在计算性能和能源效率方面都取得了显著成就，为科学研究和HPC集群运营提供了有价值的参考。

Abstract: Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).
  This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.

</details>


### [155] [Bandwidth-Aware Network Topology Optimization for Decentralized Learning](https://arxiv.org/abs/2512.07536)
*Yipeng Shen,Zehan Zhu,Yan Huang,Changzhi Yan,Cheng Zhuo,Jinming Xu*

Main category: cs.DC

TL;DR: 提出带宽感知的网络拓扑优化框架，在边数约束下最大化共识速度，通过ADMM方法求解混合整数SDP问题，实验显示在异构带宽场景下训练速度提升1.21倍


<details>
  <summary>Details</summary>
Motivation: 现有网络拓扑设计大多未考虑带宽限制，而带宽对分布式学习中的参数同步效率至关重要。需要在带宽约束下优化拓扑结构以提高共识速度。

Method: 提出带宽感知的拓扑优化框架，针对异构带宽场景引入最大带宽分配策略。将问题重构为混合整数SDP问题，采用ADMM方法求解，在ADMM子步中使用共轭梯度法高效求解大规模线性方程以提高可扩展性。

Result: 实验结果表明，所得网络拓扑在共识速度上优于基准拓扑，在真实数据集上减少去中心化学习任务的训练时间，在均匀带宽和异构带宽设置下分别实现1.11倍和1.21倍的加速。

Conclusion: 带宽感知的拓扑优化能显著提升分布式学习的参数同步效率，所提框架在带宽约束下有效优化网络拓扑，加速共识过程并减少训练时间。

Abstract: Network topology is critical for efficient parameter synchronization in distributed learning over networks. However, most existing studies do not account for bandwidth limitations in network topology design. In this paper, we propose a bandwidth-aware network topology optimization framework to maximize consensus speed under edge cardinality constraints. For heterogeneous bandwidth scenarios, we introduce a maximum bandwidth allocation strategy for the edges to ensure efficient communication among nodes. By reformulating the problem into an equivalent Mixed-Integer SDP problem, we leverage a computationally efficient ADMM-based method to obtain topologies that yield the maximum consensus speed. Within the ADMM substep, we adopt the conjugate gradient method to efficiently solve large-scale linear equations to achieve better scalability. Experimental results demonstrate that the resulting network topologies outperform the benchmark topologies in terms of consensus speed, and reduce the training time required for decentralized learning tasks on real-world datasets to achieve the target test accuracy, exhibiting speedups of more than $1.11\times$ and $1.21\times$ for homogeneous and heterogeneous bandwidth settings, respectively.

</details>


### [156] [A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator](https://arxiv.org/abs/2512.07750)
*Roozbeh Bostandoost,Pooria Namyar,Siva Kesava Reddy Kakarla,Ryan Beckett,Santiago Segarra,Eli Cortez,Ankur Mallick,Kevin Hsieh,Rodrigo Fonseca,Mohammad Hajiesmaili,Behnaz Arzani*

Main category: cs.DC

TL;DR: SANJESH是一个帮助云系统操作员理解多个机器学习模型如何影响端到端系统性能的工具，通过双层优化支持多种性能查询，能发现传统模拟方法检测不到的性能问题。


<details>
  <summary>Details</summary>
Motivation: 云系统通常使用多个机器学习模型来提高效率和性能，但操作员缺乏工具来理解每个模型以及模型之间的交互如何影响整体系统性能。

Method: SANJESH通过双层优化来回答性能相关查询，并发明了新颖的机制来加速优化求解过程，解决了之前工作24小时内无法解决的优化问题。

Result: 在虚拟机放置的生产系统案例中，SANJESH发现这些ML模型在某些场景下会导致比模拟方法检测到的性能差约4倍的问题。

Conclusion: SANJESH是一个有效的工具，能够帮助云系统操作员理解多个ML模型对系统性能的影响，发现传统方法无法检测的性能问题。

Abstract: Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.
  As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\times$ worse performance than what simulation-based approaches detect.

</details>


### [157] [Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing](https://arxiv.org/abs/2512.07792)
*Animesh Dangwal,Yufeng Jiang,Charlie Arnold,Jun Fan,Mohamed Bassem,Aish Rajagopal*

Main category: cs.DC

TL;DR: Meta开发了新的流处理负载均衡系统，通过分层调度器架构处理TB级数据，应对日益增长的应用复杂性


<details>
  <summary>Details</summary>
Motivation: 随着Meta流处理应用复杂性和用户需求的增长，原本负载均衡需求较低的基础设施部分现在需要更鲁棒和主动的负载均衡机制，以支持TB级数据的实时处理

Method: 设计并构建专注于关键计算资源和应用特性的负载均衡系统，将新调度器集成到现有分层调度器架构中，使多个调度器能够在各自基础设施层面协同工作

Result: 开发了能够处理TB级数据（仅需数秒）的高效流处理框架，通过多层基础设施和高效调度器实现跨计算资源的负载分配

Conclusion: 提出的分层负载均衡系统能够有效应对复杂流处理应用的挑战，通过多调度器协同工作提升基础设施的鲁棒性和响应能力

Abstract: Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.

</details>


### [158] [Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective](https://arxiv.org/abs/2512.07799)
*Roozbeh Bostandoost,Adam Lechowicz,Walid A. Hanafy,Prashant Shenoy,Mohammad Hajiesmaili*

Main category: cs.DC

TL;DR: 碳感知调度器通过考虑工作负载内部任务依赖关系，相比传统整体任务调度，平均可降低25%碳排放而不增加最优完成时间


<details>
  <summary>Details</summary>
Motivation: 现有碳感知调度器大多将工作负载视为单一整体任务，忽略了视频编码、离线推理等工作负载由具有特定依赖关系和资源需求的小任务组成，了解这种结构可以带来更高的碳效率

Method: 将问题建模为柔性作业车间调度变体，使用离线求解器计算碳排放和能源节省的上限，量化依赖感知方法的最大效益

Result: 结果显示平均可降低25%碳排放而不增加最优完成时间；在异构服务器设置中，这些调度可能比能源最优调度使用更多能源；允许两倍最优完成时间可使碳节省几乎翻倍

Conclusion: 依赖感知调度能显著降低数据中心碳足迹，但存在碳排放、能源消耗和完成时间之间的权衡关系，工作负载结构和服务器数量是影响碳减排效果的关键因素

Abstract: Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.
  We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [159] [Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization](https://arxiv.org/abs/2512.06699)
*Karthik Prabhakar*

Main category: cs.PF

TL;DR: 使用机器学习预测ML训练中的I/O性能并推荐最优存储配置，XGBoost模型达到R²=0.991，可将配置时间从数天减少到几分钟。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习训练越来越受限于数据I/O而非计算，GPU利用率经常低于50%等待数据，需要解决存储配置优化问题。

Method: 收集141个观测数据，涵盖不同存储后端、数据格式和访问模式，评估7个回归模型和3个分类方法，使用XGBoost进行性能预测。

Result: XGBoost表现最佳，R²=0.991，I/O吞吐量预测平均误差11.8%，特征重要性分析显示吞吐量指标和批大小是主要性能驱动因素。

Conclusion: 数据驱动方法可将配置时间从数天试错减少到几分钟预测推荐，方法可复现并可扩展到其他ML系统资源管理问题。

Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project

</details>


### [160] [AFarePart: Accuracy-aware Fault-resilient Partitioner for DNN Edge Accelerators](https://arxiv.org/abs/2512.07449)
*Mukta Debnath,Krishnendu Guha,Debasri Saha,Amlan Chakrabarti,Susmita Sur-Kolay*

Main category: cs.PF

TL;DR: 提出一个基于NSGA-II的精度感知、容错DNN划分框架，在故障条件下优化精度、能耗和延迟，在硬件加速器上实现最高27.7%的容错性提升。


<details>
  <summary>Details</summary>
Motivation: 随着DNN在分布式和资源受限平台（如SoC加速器和边缘云系统）上的部署，DNN通常被划分到异构处理单元上执行以优化延迟和能耗。然而，这些划分模型在硬件故障和通信错误下的可靠性仍然是一个关键但未被充分探索的话题，特别是在安全关键应用中。

Method: 提出一个精度感知、容错的DNN划分框架，使用NSGA-II进行多目标优化，将故障条件下的精度退化作为核心指标与能耗和延迟一起优化。框架在优化过程中执行运行时故障注入，并利用反馈循环优先考虑容错划分。

Result: 在包括AlexNet、SqueezeNet和ResNet18在内的基准CNN上评估该方法，在硬件加速器上展示了最高27.7%的容错性改进，且性能开销最小。

Conclusion: 结果强调了将弹性纳入DNN划分的重要性，从而为在易出错环境中实现稳健的AI推理铺平了道路。

Abstract: Deep Neural Networks (DNNs) are increasingly deployed across distributed and resource-constrained platforms, such as System-on-Chip (SoC) accelerators and edge-cloud systems. DNNs are often partitioned and executed across heterogeneous processing units to optimize latency and energy. However, the reliability of these partitioned models under hardware faults and communication errors remains a critical yet underexplored topic, especially in safety-critical applications. In this paper, we propose an accuracy-aware, fault-resilient DNN partitioning framework targeting multi-objective optimization using NSGA-II, where accuracy degradation under fault conditions is introduced as a core metric alongside energy and latency. Our framework performs runtime fault injection during optimization and utilizes a feedback loop to prioritize fault-tolerant partitioning. We evaluate our approach on benchmark CNNs including AlexNet, SqueezeNet and ResNet18 on hardware accelerators, and demonstrate up to 27.7% improvement in fault tolerance with minimal increase in performance overhead. Our results highlight the importance of incorporating resilience into DNN partitioning, and thereby paving the way for robust AI inference in error-prone environments.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [161] [Enhancing Urban Sensing Utility with Sensor-enabled Vehicles and Easily Accessible Data](https://arxiv.org/abs/2512.07124)
*Hui Zhong,Qing-Long Lu,Qiming Zhang,Hongliang Lu,Xinhu Zheng*

Main category: cs.ET

TL;DR: 本文提出了一种自适应框架，通过整合异构开源数据和时空加权优化，提升传感器车辆的城市感知效用，减少冗余并降低车队规模。


<details>
  <summary>Details</summary>
Motivation: 城市感知对智慧城市发展至关重要，现代车辆从单纯的交通工具转变为有价值的数据收集传感器，具有灵活性、成本效益和广泛时空覆盖的优势。然而，优化感知策略以平衡时空覆盖、最小化冗余并满足预算约束仍是一个关键挑战。

Method: 提出自适应框架，整合异构开源数据，利用时空加权优化车辆选择和感知覆盖。开发基于熵的车辆选择策略 Improved OptiFleet，最大化感知效用同时最小化冗余。使用广州320辆传感器车辆两个月的真实空气质量数据进行验证。

Result: 该方法优于基线策略，在减少车队规模的情况下提供高达5%的更高感知效用，并突显动态城市数据在优化移动感知策略中的关键作用。

Conclusion: 提出的自适应框架能有效提升传感器车辆的城市感知效用，通过优化车辆选择和减少冗余，为智慧城市管理提供更高效的数据收集解决方案。

Abstract: Urban sensing is essential for the development of smart cities, enabling monitoring, computing, and decision-making for urban management.Thanks to the advent of vehicle technologies, modern vehicles are transforming from solely mobility tools to valuable sensors for urban data collection, and hold the potential of improving traffic congestion, transport sustainability, and infrastructure inspection.Vehicle-based sensing is increasingly recognized as a promising technology due to its flexibility, cost-effectiveness, and extensive spatiotemporal coverage. However, optimizing sensing strategies to balance spatial and temporal coverage, minimize redundancy, and address budget constraints remains a key challenge.This study proposes an adaptive framework for enhancing the sensing utility of sensor-equipped vehicles.By integrating heterogeneous open-source data, the framework leverages spatiotemporal weighting to optimize vehicle selection and sensing coverage across various urban contexts.An entropy-based vehicle selection strategy, \texttt{Improved OptiFleet}, is developed to maximize sensing utility while minimizing redundancy.The framework is validated using real-world air quality data from 320 sensor-equipped vehicles operating in Guangzhou, China, over two months.Key findings show that the proposed method outperforms baseline strategies, providing up to 5\% higher sensing utility with reduced fleet sizes, and also highlights the critical role of dynamic urban data in optimizing mobile sensing strategies.

</details>


### [162] [DBMC-aNOMAly: Asynchronous NOMA with Pilot-Symbol Optimization Protocol for Diffusion-Based Molecular Communication Networks](https://arxiv.org/abs/2512.07317)
*Alexander Wietfeld,Wolfgang Kellerer*

Main category: cs.ET

TL;DR: 本文提出DBMC-aNOMAly协议，用于优化异步分子通信网络中的非正交多址接入参数，降低误码率，并通过化学反应网络实现。


<details>
  <summary>Details</summary>
Motivation: 在未来的扩散式分子通信网络中，需要有效的多址接入方案来实现多节点协作。非正交多址接入是使用单一分子类型实现高效同时多址接入的有前景方案，但需要解决异步网络中的参数优化和误码率降低问题。

Method: 首先分析推导误码率，并与时分多址、分子分多址等其他方案进行对比。提出DBMC-aNOMAly协议，这是一种基于导频符号的异步DBMC-NOMA优化协议，通过简单的比较和加法操作设计，使其可通过化学反应网络实现。

Result: 研究表明异步特性可被利用来提升性能，通过避免最差的偏移配置可实现上限性能。DBMC-aNOMAly协议在不同网络规模、噪声水平、采样抖动以及运行时条件变化下都能提供稳健的误码率降低效果。

Conclusion: DBMC-aNOMAly协议为异步扩散式分子通信网络中的非正交多址接入提供了有效的参数优化方案，其简单设计为通过化学反应网络实现奠定了未来研究基础。

Abstract: Multiple access (MA) schemes can enable cooperation between multiple nodes in future diffusion-based molecular communication (DBMC) networks. Non-orthogonal MA for DBMC networks (DBMC-NOMA) is a promising option for efficient simultaneous MA using a single molecule type. Expanding significantly upon previous work on the topic, this paper addresses the question of parameter optimization and bit error probability (BEP) reduction in an asynchronous network using DBMC-NOMA. First, we analytically derive the associated BEP and use the result for a thorough comparison with other MA schemes like time-division and molecule-division MA. We show that the asynchronous nature of the system can be exploited for performance gain, and the upper-bound performance can be achieved in all circumstances by avoiding a few worst-case offset configurations. Subsequently, we propose DBMC-aNOMAly, a pilot-symbol-based optimization protocol for asynchronous DBMC-NOMA, and extensively evaluate it using Monte-Carlo simulations. DBMC-aNOMAly is shown to provide robust BEP reduction for different network sizes, noise levels, subjected to sampling jitter, as well as for changing conditions during runtime, particularly, compared to protocols in previous work. DBMC-aNOMAly consists of a set of simple operations such as comparisons and additions, deliberately designed to be implementable with chemical reaction networks, setting up future work on the realistic modeling of the protocol.

</details>


### [163] [The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic](https://arxiv.org/abs/2512.07724)
*Zhengzheng Tang*

Main category: cs.ET

TL;DR: 提出原生脉冲微架构，将MOF材料的随机离子通道转化为确定性浮点计算，实现FP8精度100%对齐PyTorch，线性层延迟降低至O(log N)


<details>
  <summary>Details</summary>
Motivation: MOF材料具有天然脉冲动力学特性，但现有神经形态方法无法满足Transformer等AI工作负载的确定性精度要求，需要解决从随机离子到确定性浮点的转换问题

Method: 提出原生脉冲微架构，将噪声神经元视为逻辑原语，引入空间组合流水线和粘性额外校正机制

Result: 在全部16,129个FP8数对验证中实现100%比特精确对齐PyTorch，线性层延迟降低至O(log N)，获得17倍加速，对极端膜泄漏具有鲁棒性

Conclusion: 该架构成功弥合了随机离子通道与确定性AI计算之间的鸿沟，为MOF材料在精确AI计算中的应用提供了可行方案

Abstract: The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap "from stochastic ions to deterministic floats," we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [164] [Análisis de rendimiento y eficiencia energética en el cluster Raspberry Pi Cronos](https://arxiv.org/abs/2512.07622)
*Martha Semken,Mariano Vargas,Ignacio Tula,Giuliana Zorzoli,Andrés Rojas Paredes*

Main category: cs.AR

TL;DR: 评估基于Raspberry Pi4和3b微电脑组成的Cronos集群在计算性能和能耗效率方面的表现，使用HPL基准测试，分析不同节点配置下的可扩展性、稳定性和功耗。


<details>
  <summary>Details</summary>
Motivation: 为教育目的设计低成本ARM集群，需要评估其计算性能和能耗效率，为教育研究环境中的集群设计和使用提供实际参考。

Method: 使用High Performance Linpack (HPL)基准测试，在配置Slurm资源管理和Open MPI并行通信的环境下，对不同节点配置（同构和异构）进行实验测试，测量计算性能和功耗。

Result: 6个Raspberry Pi4节点的同构配置最高达到6.91 GFLOPS性能；异构节点（包含Pi3b）会负面影响稳定性和效率；测量了系统总功耗并计算了性能功耗比(GFLOPS/W)。

Conclusion: 该研究为低成本ARM集群在教育研究环境中的设计、评估和使用提供了具体贡献，展示了Raspberry Pi集群的实际性能和能耗特性。

Abstract: This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.

</details>


### [165] [Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads](https://arxiv.org/abs/2512.06093)
*Boyu Li,Zongwei Zhu,Yi Xiong,Qianyue Cao,Jiawei Geng,Xiaonan Zhang,Xi Li*

Main category: cs.AR

TL;DR: 提出Compass框架，通过计算执行图编码方案和遗传算法搜索，优化LLM推理在多芯片加速器上的映射，实现63.12%的平均EDP降低。


<details>
  <summary>Details</summary>
Motivation: 现有映射空间探索主要针对传统CNN/Transformer工作负载，无法充分支持真实LLM推理服务中混合请求类型和可变序列长度的动态行为。

Method: 1) 提出基于计算执行图的映射编码方案，解耦微批次和层，实现异构芯片上的细粒度执行控制；2) 开发Compass框架，集成评估引擎和基于遗传算法的映射生成引擎。

Result: 与最先进工作相比，平均EDP（能耗延迟积）降低63.12%。

Conclusion: Compass框架通过创新的映射编码和搜索机制，有效解决了LLM推理在多芯片加速器上的动态映射问题，显著提升了能效。

Abstract: Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.

</details>


### [166] [Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures](https://arxiv.org/abs/2512.06113)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AR

TL;DR: MERINDA是一个基于FPGA加速的模型恢复框架，通过数据流流水线重构计算，相比GPU方案解决了迭代依赖、内核启动开销等问题，在典型MR工作负载上比FPGA基准方案减少6.3倍周期。


<details>
  <summary>Details</summary>
Motivation: 模型恢复是物理AI和实时数字孪生的核心原语，但GPU执行效率低下，存在迭代依赖、内核启动开销、内存带宽利用不足和高数据移动延迟等问题。

Method: 将计算重构为流式数据流水线，利用BRAM分块、定点数内核、LUT结构和进位链加法器的并发使用，实现细粒度空间并行，同时最小化片外通信。

Result: 在典型MR工作负载上，MERINDA比基于FPGA的LTC基准方案减少高达6.3倍的周期，为时间关键物理系统提供实时性能。

Conclusion: MERINDA通过硬件感知的公式化方法消除了同步瓶颈，在MR的迭代更新中维持高吞吐量，实现了高效的FPGA加速模型恢复。

Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.

</details>


### [167] [From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators](https://arxiv.org/abs/2512.06177)
*Jiahan Xie,Evan Williams,Adrian Sampson*

Main category: cs.AR

TL;DR: 开发了一个从PyTorch ML模型到可综合SystemVerilog的端到端开源编译器工具链，性能接近闭源商业工具


<details>
  <summary>Details</summary>
Motivation: 为机器学习硬件加速提供开源解决方案，替代闭源的商业工具如Vitis HLS，降低硬件设计门槛

Method: 基于Allo加速器设计语言、Calyx硬件中间表示和LLVM的CIRCT项目构建工具链，实现内存分区编译优化

Result: 能够有效生成优化的FPGA可实现的硬件设计，性能与Vitis HLS等闭源工业级工具相当

Conclusion: 提供了一个完整的开源ML到硬件编译器解决方案，证明开源工具链可以达到商业工具的性能水平

Abstract: We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.

</details>


### [168] [SparsePixels: Efficient Convolution for Sparse Data on FPGAs](https://arxiv.org/abs/2512.06208)
*Ho Fung Tsoi,Dylan Rankin,Vladimir Loncar,Philip Harris*

Main category: cs.AR

TL;DR: SparsePixels框架利用图像空间稀疏性，在FPGA上实现高效卷积，针对稀疏图像数据（如物理实验图像）实现73倍推理加速，仅损失少量性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在FPGA上推理时，由于需要密集卷积所有像素点，导致高延迟和长启动间隔，特别是对于大尺寸图像。然而，某些图像数据（如物理实验图像）具有空间稀疏性，大部分计算浪费在空区域上。

Method: 提出SparsePixels框架，实现特殊类别的CNN，仅选择性地保留和计算活跃像素子集，忽略其余像素。开发支持量化感知训练的稀疏CNN构建库，以及用于FPGA部署的HLS实现。

Result: 在包含约4k像素但非常稀疏的中微子物理数据集上，标准CNN推理延迟为48.665μs，而稀疏CNN仅计算不到1%的输入像素，实现73倍加速至0.665μs，资源利用率在芯片预算内，仅损失少量性能。在其他稀疏图像数据集上也展示了至少一个数量级的加速。

Conclusion: 该工作为现代实验（如CERN大型强子对撞机的触发和数据采集系统）中的快速高效数据读出算法开发提供了有益参考，通过利用图像空间稀疏性显著加速推理，同时保持可接受的性能损失。

Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.

</details>


### [169] [A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS](https://arxiv.org/abs/2512.06362)
*Junyi Yang,Xinyu Luo,Ye Ke,Zheng Wang,Hongyang Shang,Shuai Dong,Zhengnan Fu,Xiaofeng Yang,Hongjie Liu,Arindam Basu*

Main category: cs.AR

TL;DR: 提出一种结合可重构非线性内存ADC的LSTM加速器，直接在模拟域计算非线性激活，显著提升能效和面积效率


<details>
  <summary>Details</summary>
Motivation: 传统模拟内存计算加速器在处理LSTM网络时，大量非线性操作需要数字计算，限制了能效提升

Method: 采用可重构1-5位非线性内存ADC，包含：1) 双9T位单元支持有符号输入和三元权重；2) RUDC技术提升读取动态范围；3) 双电源6T-SRAM阵列减少位单元数量和延迟

Result: 5位NLIM ADC实现平均误差<1 LSB，12类关键词识别任务达到92.0%片上推理准确率，系统级能效提升2.2倍，面积效率提升1.6倍

Conclusion: 提出的LSTM加速器通过模拟域直接计算非线性激活，显著提升了能效和面积效率，为RNN加速器设计提供了新思路

Abstract: The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.

</details>


### [170] [Approximate Multiplier Induced Error Propagation in Deep Neural Networks](https://arxiv.org/abs/2512.06537)
*A. M. H. H. Alahakoon,Hassaan Saadat,Darshana Jayasinghe,Sri Parameswaran*

Main category: cs.AR

TL;DR: 提出分析框架连接近似乘法器误差分布与DNN精度影响，通过误差矩阵Frobenius范数推导闭式解，发现失真主要由乘法器均值误差主导，验证了预测失真与实际精度下降强相关。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络严重依赖密集算术运算，近似乘法器可降低硬件能耗，但缺乏数学理论描述近似乘法器误差分布如何影响DNN精度。

Method: 建立分析框架连接近似乘法器统计误差矩与GEMM诱导失真，使用误差矩阵Frobenius范数推导闭式表达式，在ImageNet规模网络中通过受控误差注入评估模型。

Result: 预测失真与观测精度下降强相关，FPGA上可配置误差近似乘法器案例研究进一步证实分析趋势，框架为行为或硬件级仿真提供轻量级替代方案。

Conclusion: 该分析框架能快速估计近似乘法器对DNN推理质量影响，失真主要由乘法器均值误差主导，为近似乘法器设计提供理论指导。

Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.

</details>


### [171] [ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design](https://arxiv.org/abs/2512.06854)
*Qijun Zhang,Yao Lu,Mengming Li,Shang Liu,Zhiyao Xie*

Main category: cs.AR

TL;DR: ArchPower是首个开源架构级处理器功耗建模数据集，包含200个CPU数据样本，覆盖25种CPU配置和8种工作负载，提供超过100个架构特征和细粒度功耗标签。


<details>
  <summary>Details</summary>
Motivation: 传统CPU功耗评估需要耗时数月的完整IC实现流程，早期架构级功耗模型准确性差。现有ML方法缺乏开源数据集，且私有数据集难以反映真实设计场景。

Method: 通过复杂真实的设计流程收集CPU架构信息作为特征，使用功耗仿真获得真实功耗作为标签。数据集包含硬件参数和事件参数两类特征。

Result: 创建了包含200个CPU数据样本的开源数据集，每个样本有100+架构特征和细粒度功耗标签（总功耗和11个组件功耗，每个组件功耗进一步分解为4种类型）。

Conclusion: ArchPower填补了架构级处理器功耗建模领域开源数据集的空白，为ML功耗模型研究提供了高质量、细粒度的基准数据集。

Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.

</details>


### [172] [DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management](https://arxiv.org/abs/2512.07312)
*Zhongchun Zhou,Chengtao Lai,Yuhang Gu,Wei Zhang*

Main category: cs.AR

TL;DR: 该论文提出了一种面向大语言模型AI加速器的共享系统级缓存架构，通过应用感知的管理策略（包括缓存替换、死块预测和旁路决策）来简化编程并提升性能，相比传统缓存架构实现了最高1.8倍的加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI加速器设计趋向于更强大和专业化。传统方法使用复杂的层次化暂存器内存（SPMs）及其异步管理，增加了软件开发复杂性。本研究探索相反的设计方向：采用共享系统级缓存和应用感知管理策略，保持编程简单性。

Method: 设计多核AI加速器，配备共享系统级缓存，利用软件栈中的数据流信息指导缓存替换（包括死块预测），结合旁路决策和缓解缓存颠簸的机制。通过周期精确模拟器评估，并建立考虑实际重叠行为的分析模型。

Result: 相比传统缓存架构实现了最高1.80倍的性能提升。旁路和颠簸缓解策略能处理有无核间数据共享的场景。RTL实现面积为0.064mm²（15nm工艺），时钟频率可达2GHz。分析模型成功扩展了策略到更大规模工作负载。

Conclusion: 共享缓存设计展示了简化AI加速器编程同时提升性能的潜力，为未来AI加速器系统开发提供了有前景的方向，特别是在大语言模型等计算密集型应用中。

Abstract: The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
  Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

</details>


### [173] [aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \& Software Formal Verification](https://arxiv.org/abs/2512.07520)
*Noé Amiot,Quentin L. Meunier,Karine Heydemann,Emmanuelle Encrenaz*

Main category: cs.AR

TL;DR: aLEAKator是一个开源框架，用于从HDL描述中自动形式化验证掩码加密加速器和CPU上运行的软件，支持多种泄漏模型和信号粒度


<details>
  <summary>Details</summary>
Motivation: 现有验证方法要么局限于小型硬件模块或CPU上的小程序，要么受限于泄漏模型，或需要硬件特定先验知识，难以验证复杂掩码实现的安全性

Method: 引入混合域仿真方法，支持精确建模和验证各种1-探测泄漏模型，支持可变信号粒度，无需目标CPU架构的先验知识，支持查找表存在的情况

Result: 验证了与现有工具和实际测量的一致性，并实现了创新性结果，如在各种CPU上验证完整的一阶掩码AES实现

Conclusion: aLEAKator框架解决了掩码硬件和软件实现安全验证的挑战，支持更广泛的泄漏模型和复杂实现，无需硬件特定知识

Abstract: Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs

</details>
