<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 69]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [First Demonstration of Second-order Training of Deep Neural Networks with In-memory Analog Matrix Computing](https://arxiv.org/abs/2512.05342)
*Saitao Zhang,Yubiao Luo,Shiqing Wang,Pushen Zuo,Yongxiang Li,Lunshuai Pan,Zheng Miao,Zhong Sun*

Main category: cs.ET

TL;DR: 基于RRAM内存模拟矩阵计算的二阶优化器，通过单步矩阵求逆加速神经网络训练，相比SGD和Adam减少训练轮次，提升吞吐量和能效


<details>
  <summary>Details</summary>
Motivation: 二阶优化方法虽然收敛更快更稳定，但在大规模神经网络训练中，二阶信息矩阵求逆的计算成本过高，阻碍了实际应用。需要寻找能高效执行矩阵运算的新硬件方案。

Method: 使用电阻式随机存取存储器(RRAM)构建内存模拟矩阵计算(AMC)系统，实现单步矩阵求逆操作。将该硬件加速器应用于二阶优化器，训练两层卷积神经网络进行手写字母分类。

Result: 相比带动量的SGD和Adam，分别减少26%和61%的训练轮次。在相同二阶方法下，相比最先进的数字处理器，吞吐量提升5.88倍，能效提升6.9倍。

Conclusion: AMC电路为二阶神经网络训练提供了可行有效的硬件加速方案，开辟了通往高能效AI加速的新路径，解决了传统二阶优化器计算瓶颈问题。

Abstract: Second-order optimization methods, which leverage curvature information, offer faster and more stable convergence than first-order methods such as stochastic gradient descent (SGD) and Adam. However, their practical adoption is hindered by the prohibitively high cost of inverting the second-order information matrix, particularly in large-scale neural network training. Here, we present the first demonstration of a second-order optimizer powered by in-memory analog matrix computing (AMC) using resistive random-access memory (RRAM), which performs matrix inversion (INV) in a single step. We validate the optimizer by training a two-layer convolutional neural network (CNN) for handwritten letter classification, achieving 26% and 61% fewer training epochs than SGD with momentum and Adam, respectively. On a larger task using the same second-order method, our system delivers a 5.88x improvement in throughput and a 6.9x gain in energy efficiency compared to state-of-the-art digital processors. These results demonstrate the feasibility and effectiveness of AMC circuits for second-order neural network training, opening a new path toward energy-efficient AI acceleration.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR提出了一种联邦学习框架，通过渐进式模型恢复解决带宽受限客户端在异构环境中的参与问题，实现更快的收敛和更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 在异构联邦学习环境中，带宽受限客户端由于通信容量有限，其小子模型在训练初期学习快，但后期会变得参数不足，导致收敛缓慢和泛化能力下降。

Method: FedGMR采用渐进式模型恢复策略，在训练过程中逐步增加每个客户端的子模型密度；同时开发了针对异步异构联邦学习的掩码感知聚合规则，并提供了收敛性保证。

Result: 在FEMNIST、CIFAR-10和ImageNet-100上的实验表明，FedGMR在高异构性和非独立同分布设置下实现了更快的收敛和更高的准确率。

Conclusion: FedGMR通过渐进式模型恢复使带宽受限客户端在整个训练过程中保持有效贡献，缩小了与完整模型联邦学习的性能差距。

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [3] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 研究基于真实轨迹数据验证公交车载边缘服务器在车联网中的可行性，通过分析上海公交/出租车/电信数据集，发现公交车能覆盖大部分地理区域和需求点，并提出贪心算法选择有限数量公交车以最大化需求点覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统固定边缘服务器（如路边单元或基站）部署后位置和容量固定，难以处理时空动态的用户需求。移动服务器（如公交车）具有为车联网系统增加计算弹性的潜力，但需要验证其实际可行性。

Method: 1. 使用上海公交/出租车/电信数据集分析公交车和基站的覆盖能力；2. 建立数学模型并设计简单贪心启发式算法，在有限预算下选择最优公交车集合以最大化需求点覆盖；3. 进行基于轨迹的仿真验证算法性能。

Result: 公交车载边缘服务器能覆盖大部分地理区域和需求点，具有巨大潜力。提出的贪心算法能有效处理服务器容量和购买数量等现实约束下的动态用户需求。

Conclusion: 城市区域车联网中公交车载边缘服务器是可行、有益且有价值的解决方案，能够为动态用户需求提供弹性计算支持。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出一种针对硬件代码生成的LLM遗忘框架，通过语法保持遗忘策略和细粒度选择性损失，有效移除问题知识而不损害代码生成能力。


<details>
  <summary>Details</summary>
Motivation: LLM在加速数字硬件设计方面潜力巨大，但现有模型存在三个主要问题：1）记忆专有知识产权（IP），2）基准测试数据污染，3）不安全编码模式。这些风险影响了LLM在硬件设计中的可靠性。

Method: 提出新颖的遗忘框架，包含两个核心组件：1）语法保持遗忘策略，在遗忘过程中保护硬件代码的结构完整性；2）细粒度floor-aware选择性损失，实现精确高效的问题知识移除。两者结合确保有效遗忘而不降低LLM代码生成能力。

Result: 实验表明该框架支持最多3倍大的遗忘集，通常只需单个训练周期，同时保持寄存器传输级（RTL）代码的语法正确性和功能完整性。

Conclusion: 该工作为可靠的LLM辅助硬件设计开辟了新途径，通过专门设计的遗忘机制解决了LLM在硬件代码生成中的可靠性问题。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [5] [Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques](https://arxiv.org/abs/2512.05169)
*Abdelmalik Moujahid,Fadi Dornaika*

Main category: cs.LG

TL;DR: 这篇论文是一篇关于多视图聚类(MVC)的综述，系统性地分类了MVC方法，分析了各种方法的优缺点，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术在实现最佳性能时面临诸多挑战，包括计算约束、单视图学习算法的局限性以及处理来自不同领域、来源或视图的大型数据集的复杂性。多视图聚类作为一种无监督多视图学习方法，能够克服这些挑战，提供更丰富的数据表示和有效的解决方案。

Method: 通过系统性地回顾140多篇基础性和近期文献，将多视图聚类方法分类为：协同训练、协同正则化、子空间、深度学习、基于核、基于锚点和基于图的策略。同时比较了早期融合、晚期融合和联合学习等集成策略。

Result: 提供了对多视图聚类方法的系统性分类和深入分析，包括各自的优势、弱点以及实际挑战（如可扩展性和不完整数据）。涵盖了医疗保健、多媒体和社交网络分析等实际应用案例。

Conclusion: 这篇综述填补了多视图聚类研究中的现有空白，为该领域的进步提供了可行的见解，并讨论了新兴趋势、跨学科应用和未来研究方向。

Abstract: Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.

</details>


### [6] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出CV-Masking方法，根据实验室检测指标的波动性自适应调整掩码概率，相比随机掩码在EHR表示学习中表现更好


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法在电子健康记录中使用均匀随机掩码，假设所有特征同等可预测。但实际上实验室检测指标存在显著异质性波动性：有些生物标志物稳定，有些波动剧烈且更难建模。临床上，波动性生物标志物常提示急性病理生理变化，需要更复杂的建模来捕捉其时间模式。

Method: 提出波动性感知预训练策略CV-Masking，根据每个特征的内在变异性自适应调整掩码概率。结合与临床工作流程对齐的仅值掩码目标，相比随机和基于方差的策略有系统性改进。

Result: 在大规模实验室检测面板上的实验表明，CV-Masking增强了重建能力，提高了下游预测性能，加速了收敛，产生了更稳健且临床意义更强的EHR表示。

Conclusion: CV-Masking方法通过考虑实验室检测指标的波动性异质性，改进了MAE在EHR表示学习中的效果，为临床任务提供了更好的基础表示。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [7] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 系统评估临床时间序列建模中的分词策略，发现时间编码无一致显著效益，值特征重要性因任务而异，冻结预训练编码器优于可训练版本且参数更少


<details>
  <summary>Details</summary>
Motivation: 当前对电子健康记录处理中分词策略有效性的公平比较有限，需要系统评估不同分词方法对临床时间序列建模的影响

Method: 使用基于Transformer的架构，在MIMIC-IV数据集上对四个临床预测任务进行受控消融实验，比较不同分词策略（时间编码、值特征、代码序列等）的效果

Result: 1) 显式时间编码对评估的下游任务无一致统计显著效益；2) 值特征重要性任务依赖，影响死亡率预测但不影响再入院预测；3) 冻结预训练代码编码器显著优于可训练版本且参数更少；4) 更大的临床编码器在所有任务上提供一致改进

Conclusion: 更简单、参数高效的方法在许多情况下可实现强大性能，但最佳分词策略仍取决于具体任务；受控评估支持更公平的分词比较

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [8] [Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance](https://arxiv.org/abs/2512.05222)
*Yanhua Xu*

Main category: cs.LG

TL;DR: 结合预训练蛋白质语言模型与半监督学习，可在抗原性标记数据稀缺时保持高预测准确性，解决流感病毒抗原性评估的标记瓶颈。


<details>
  <summary>Details</summary>
Motivation: 流感A病毒抗原性进化快，需要频繁更新疫苗，但传统血凝抑制试验劳动密集且难以规模化，导致基因组数据远超可用表型标记，限制了传统监督模型的有效性。

Method: 评估两种半监督学习策略（自训练和标签传播），与完全监督基线比较，使用四种PLM衍生嵌入（ESM-2、ProtVec、ProtT5、ProtBert）应用于血凝素序列，通过嵌套交叉验证框架模拟低标记情况（25%、50%、75%、100%标记可用性）。

Result: 半监督学习在标记稀缺情况下持续改善性能，自训练与ProtVec组合产生最大相对增益，ESM-2保持高度稳健，仅用25%标记数据即可达到0.82以上的F1分数，H1N1和H9N2预测准确率高，但高变异的H3N2亚型仍具挑战性。

Conclusion: 整合蛋白质语言模型与半监督学习可解决抗原性标记瓶颈，更有效地利用未标记监测序列，支持快速变异优先排序和及时疫苗株选择。

Abstract: Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.

</details>


### [9] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 提出VaRDASS方法，通过分层采样减少无监督域适应中的方差，提高域差异估计精度和性能


<details>
  <summary>Details</summary>
Motivation: 无监督域适应中，域差异估计在随机设置下存在高方差问题，阻碍了方法的理论优势发挥

Method: 提出VaRDASS方法，针对相关性对齐和最大均值差异两种域差异度量，推导分层采样目标，并设计k-means风格优化算法

Result: 在三个域偏移数据集上实验，证明能提高域差异估计精度和目标域性能，且MMD目标在特定假设下理论最优

Conclusion: VaRDASS是首个专门针对UDA的随机方差减少技术，能有效解决域差异估计的高方差问题，提升模型在目标域的表现

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [10] [MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System](https://arxiv.org/abs/2512.05234)
*Felix Mulitze,Herbert Woisetschläger,Hans Arno Jacobsen*

Main category: cs.LG

TL;DR: MAR-FL是一种新型的P2P联邦学习系统，通过迭代分组聚合大幅降低通信开销，同时保持对网络波动的鲁棒性，通信复杂度从O(N²)降低到O(N log N)。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统与分布式机器学习的融合需要高效且鲁棒的联邦学习方法，而现有的P2P FL方法存在通信复杂度过高的问题，限制了实际可扩展性。

Method: 采用迭代分组聚合机制，通过分组方式减少通信开销，同时保持对不可靠客户端和网络波动的鲁棒性，并可集成隐私计算。

Result: MAR-FL将通信复杂度从传统方法的O(N²)降低到O(N log N)，显著提升了系统可扩展性，特别是在聚合轮次中参与节点数量增加时效果更明显。

Conclusion: MAR-FL系统在保持对网络波动和不可靠客户端鲁棒性的同时，大幅降低了通信开销，为无线环境下的P2P联邦学习提供了高效可扩展的解决方案。

Abstract: The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.

</details>


### [11] [Edged Weisfeiler-Lehman Algorithm](https://arxiv.org/abs/2512.05238)
*Xiao Yue,Bo Liu,Feng Zhang,Guangzhi Qu*

Main category: cs.LG

TL;DR: 论文提出E-WL算法扩展1-WL以利用边特征，并基于此构建EGIN模型，在12个边特征图数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统GNN采用传播-聚合方法，类似1-WL算法通过颜色细化测试同构，但1-WL不利用边特征。许多GNN也存在不利用图数据边特征的缺点，限制了在边特征重要领域的性能。

Method: 提出E-WL算法扩展1-WL以纳入边特征，并基于E-WL构建EGIN模型，专门设计用于充分利用边特征进行图学习。

Result: 在12个边特征基准图数据集上评估，EGIN模型在图形分类任务中普遍表现出优于现有最先进基线模型的性能。

Conclusion: E-WL算法成功扩展了1-WL以利用边特征，基于此构建的EGIN模型有效解决了GNN不利用边特征的局限性，在边特征图学习任务中表现出优越性能。

Abstract: As a classical approach on graph learning, the propagation-aggregation methodology is widely exploited by many of Graph Neural Networks (GNNs), wherein the representation of a node is updated by aggregating representations from itself and neighbor nodes recursively. Similar to the propagation-aggregation methodology, the Weisfeiler-Lehman (1-WL) algorithm tests isomorphism through color refinement according to color representations of a node and its neighbor nodes. However, 1-WL does not leverage any edge features (labels), presenting a potential improvement on exploiting edge features in some fields. To address this limitation, we proposed a novel Edged-WL algorithm (E-WL) which extends the original 1-WL algorithm to incorporate edge features. Building upon the E-WL algorithm, we also introduce an Edged Graph Isomorphism Network (EGIN) model for further exploiting edge features, which addresses one key drawback in many GNNs that do not utilize any edge features of graph data. We evaluated the performance of proposed models using 12 edge-featured benchmark graph datasets and compared them with some state-of-the-art baseline models. Experimental results indicate that our proposed EGIN models, in general, demonstrate superior performance in graph learning on graph classification tasks.

</details>


### [12] [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)
*Bruno Jacob,Amanda A. Howard,Panos Stinis*

Main category: cs.LG

TL;DR: 提出多保真度学习框架，用量子求解器生成低保真度解，再用稀疏经典数据训练神经网络校正到高保真度，克服量子硬件限制


<details>
  <summary>Details</summary>
Motivation: 量子PDE求解器受限于近端硬件：量子比特数限制空间分辨率，电路深度限制长时间积分精度。这些硬件瓶颈使量子求解器只能产生低保真度解，尽管理论上有计算加速潜力

Method: 引入多保真度学习框架：1) 用量子求解器生成大量低保真度解作为代理模型；2) 用稀疏经典高保真数据训练多保真度神经网络架构，学习线性与非线性校正映射；3) 在粘性Burgers方程和不可压缩Navier-Stokes流动等非线性PDE上验证

Result: 框架成功校正粗网格量子预测，实现超出经典训练窗口的时间外推，预测精度可与经典方法竞争，显著减少高保真模拟需求

Conclusion: 该工作通过桥接硬件受限的量子模拟与应用需求，为从当前量子设备中提取计算价值建立了路径，推进了量子计算在计算物理中的算法开发和实际部署

Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.

</details>


### [13] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 提出一种高效遗忘框架，通过识别对模型输出影响可忽略的训练数据子集，在遗忘前减少数据集规模，实现显著计算节省（约50%）


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题日益突出，从训练模型中遗忘或移除特定数据点的能力变得越来越重要。现有遗忘方法通常平等对待遗忘集中的所有数据点，但有些数据点对模型学习的影响微乎其微，是否也需要移除？

Method: 通过跨语言和视觉任务的影响函数比较分析，识别对模型输出影响可忽略的训练数据子集。基于这一洞察，提出高效遗忘框架，在遗忘前减少数据集规模

Result: 在真实世界实证案例中实现了显著的计算节省（高达约50%）

Conclusion: 挑战了传统遗忘方法平等对待所有数据点的做法，提出通过识别影响可忽略的数据子集来优化遗忘过程，实现计算效率提升

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [14] [DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models](https://arxiv.org/abs/2512.05287)
*Ziqi Zhang*

Main category: cs.LG

TL;DR: 提出DMAGT模型，基于多层Transformer图神经网络预测药物-miRNA关联，在三个数据集上达到95.24% AUC，实验验证了14/20预测关联。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验在探索药物与miRNA关联时存在效率和成本限制，需要计算模型来加速miRNA靶向药物开发。

Method: 将药物-miRNA关联转化为图结构，使用Word2Vec嵌入药物分子结构和miRNA碱基序列特征，通过图Transformer模型学习嵌入特征和关系结构来预测关联。

Result: 在ncDR、RNAInter和SM2miR三个数据集上达到最高95.24±0.05 AUC，优于同类方法。对5-氟尿嘧啶和奥沙利铂的预测中，20个最可能关联中有14个得到验证。

Conclusion: DMAGT在预测药物-miRNA关联方面表现出色且稳定，为miRNA药物开发提供了新的快捷途径。

Abstract: MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.

</details>


### [15] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，通过RKHS-SHAP状态归因改进Actor-Critic方法，实现高效、稳定且可解释的强化学习


<details>
  <summary>Details</summary>
Motivation: 传统Actor-Critic方法缺乏可解释性，现有可解释RL方法很少利用状态归因辅助训练，且忽视不同状态维度对奖励的异质性影响

Method: 提出RSA2C算法：1) Actor在向量值RKHS中实现，使用马氏加权算子值核；2) Value Critic和Advantage Critic在标量RKHS中；3) 通过RKHS-SHAP计算状态归因，转换为马氏门控权重来调节Actor梯度和Advantage Critic目标；4) 使用稀疏化字典结构

Result: 理论分析：在状态扰动下推导了全局非渐近收敛界，通过扰动误差项证明稳定性，通过收敛误差项证明效率。实验：在三个标准连续控制环境中验证了算法的高效性、稳定性和可解释性

Conclusion: RSA2C算法成功将状态归因整合到Actor-Critic框架中，实现了效率、稳定性和可解释性的统一，为可解释强化学习提供了新思路

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [16] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: CFO是一个连续流算子框架，通过流匹配直接学习PDE的右侧项，避免了自回归预测的误差累积问题，支持任意时间分辨率和反向时间推理。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子代理方法使用自回归预测方案，存在误差累积问题且需要均匀时间离散化。需要一种能够学习连续时间PDE动态的方法，同时避免标准连续方法（如神经ODE）的计算负担。

Method: CFO重新利用流匹配直接学习PDE的右侧项，无需通过ODE求解器反向传播。方法包括：1）对轨迹数据进行时间样条拟合；2）在节点处使用时间导数的有限差分估计构建概率路径；3）通过流匹配训练神经算子预测这些解析速度场。

Result: 在四个基准测试（Lorenz、1D Burgers、2D扩散反应、2D浅水方程）中，CFO表现出优越的长期稳定性和显著的数据效率。仅使用25%不规则子采样时间点训练的CFO优于使用完整数据训练的自回归基线，相对误差减少高达87%。

Conclusion: CFO提供了一种时间分辨率不变的学习框架，支持任意时间网格的训练和任意时间分辨率的推理，同时实现了竞争性的计算效率，仅需基线方法50%的函数评估，并独特支持反向时间推理和任意时间查询。

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


### [17] [Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.05306)
*Y. Sungtaek Ju*

Main category: cs.LG

TL;DR: SVGP KANs：将稀疏变分高斯过程与Kolmogorov-Arnold网络结合，实现可解释且具有不确定性量化的科学机器学习架构


<details>
  <summary>Details</summary>
Motivation: 传统Kolmogorov-Arnold网络缺乏系统的不确定性量化能力，这在科学应用中至关重要。需要一种既能保持可解释性又能进行贝叶斯推断的架构。

Method: 将稀疏变分高斯过程推断与Kolmogorov-Arnold拓扑结构集成，通过解析矩匹配在深层加性结构中传播不确定性，计算复杂度与样本量呈准线性关系。

Result: 通过三个案例研究展示了框架区分偶然不确定性和认知不确定性的能力：流体流动重建中的异方差测量噪声校准、平流-扩散动力学多步预测中的置信度退化量化、卷积自编码器中的分布外检测。

Conclusion: SVGP KANs是科学机器学习中具有不确定性感知能力的有前景架构，结合了可解释性和贝叶斯推断优势。

Abstract: Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.

</details>


### [18] [The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?](https://arxiv.org/abs/2512.05311)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee*

Main category: cs.LG

TL;DR: 研究评估SOTA模型区分人类与LLM生成科学想法的能力，发现连续改写会显著降低检测性能，而加入研究问题作为上下文可提升检测效果


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为研究代理的日益普及，区分LLM与人类生成的想法对于理解LLM研究能力的认知差异变得至关重要。虽然检测LLM生成文本已有广泛研究，但区分人类与LLM生成的科学想法仍是未探索领域

Method: 系统评估SOTA机器学习模型区分人类与LLM生成想法的能力，特别关注连续改写阶段后的检测效果，并研究加入研究问题作为上下文信息的影响

Result: SOTA模型在来源归因方面面临挑战，经过五个连续改写阶段后检测性能平均下降25.4%；加入研究问题作为上下文信息可将检测性能提升最多2.97%；检测算法在想法被改写成简化、非专家风格时表现最差

Conclusion: 区分人类与LLM生成的科学想法具有挑战性，连续改写会显著降低检测性能，而上下文信息有助于提升检测效果，特别是简化改写对LLM特征识别影响最大

Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.

</details>


### [19] [Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay](https://arxiv.org/abs/2512.05320)
*Mehmet Efe Lorasdagi,Dogan Can Cicek,Furkan Burak Mutlu,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出DPER方法，通过为Actor和Critic网络分别采样不同的经验回放批次，提升深度确定性策略梯度算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Actor-Critic架构使用相同的经验回放批次训练，但Actor和Critic的学习目标和更新动态不同，统一使用经验可能不是最优选择。

Method: 提出解耦优先经验回放（DPER），允许为Actor和Critic独立采样不同的转移批次。该方法可集成到任何连续控制领域的离策略深度强化学习算法中，并与TD3算法结合进行评估。

Result: DPER在多个MuJoCo任务中优于传统的经验回放策略（如普通经验回放和优先经验回放）。

Conclusion: 解耦Actor和Critic的经验回放可以改善训练动态和最终策略质量，DPER为广泛的Actor-Critic离策略强化学习算法提供了通用性能提升机制。

Abstract: Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.
  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.
  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.
  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.
  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.

</details>


### [20] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试AI天气预报模型FourCastNetv2对输入噪声的鲁棒性，通过注入高斯噪声到飓风初始条件和完全随机初始条件来评估模型输出稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预报模型对输入噪声和不确定性的鲁棒性对于极端天气事件（如飓风）的预测可靠性至关重要。

Method: 进行两个实验：1）在飓风Florence的ERA5初始条件中注入不同水平的高斯噪声，观察轨迹和强度预测变化；2）使用完全随机初始条件启动模型，观察模型对无意义输入的反应。

Result: FCNv2在低到中等噪声下能准确保持飓风特征；高噪声下仍能维持基本轨迹和结构，但位置精度下降；模型在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预报。

Conclusion: FCNv2对输入噪声表现出良好的鲁棒性，倾向于生成稳定平滑的输出，该方法简单且可移植到其他数据驱动的AI天气预报模型。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [21] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 提出新的联邦优化通信模型，区分不同客户端选择策略的成本，并基于SAGA开发RG-SAGA算法，在非凸优化中达到最优通信和本地计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦优化算法比较指标未能区分不同客户端选择策略的实际通信成本差异，需要建立能够量化通信和本地计算复杂度的统一模型。

Method: 1) 建立量化通信和本地计算复杂度的联邦优化模型；2) 基于不精确复合梯度法设计新算法；3) 为SAGA推导新的方差界；4) 提出递归梯度技术改进梯度估计器，得到RG-SAGA。

Result: 新算法在非凸联邦优化中实现了已知最优的通信和本地计算复杂度，RG-SAGA相比原始SAGA具有改进的误差界。

Conclusion: 通过建立统一的通信成本模型和改进梯度估计技术，提出的方法能更公平地比较不同联邦优化算法，并在实际应用中提供更好的性能。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [22] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER使用蒙特卡洛树搜索生成训练路径轨迹，通过子答案召回和LLM作为裁判验证过滤错误轨迹，并重新表述子查询处理检索失败，从而提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于训练的多跳问答系统仍受限于LLM幻觉和错误推理路径，影响性能表现。

Method: 1) 使用蒙特卡洛树搜索生成训练路径轨迹；2) 通过子答案召回和LLM作为裁判验证过滤错误和冗长轨迹；3) 重新表述子查询处理检索失败情况。

Result: PATHFINDER在公开基准数据集上提升了多跳问答的性能。

Conclusion: 通过改进训练数据质量和处理检索失败，PATHFINDER能有效提升多跳问答系统的性能。

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [23] [Interaction Tensor Shap](https://arxiv.org/abs/2512.05338)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TL;DR: 提出Interaction Tensor SHAP (IT SHAP)，将高阶Shapley交互表示为张量网络收缩，在多项式时间内计算高维模型中的特征交互效应。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型变得更深、维度更高，理解特征如何影响预测变得困难。现有Shapley值方法无法高效计算高阶交互：Shapley Taylor交互指数需要指数级枚举，而边际SHAP张量方法仅限于一阶效应。需要同时保持公理精确性并避免指数计算复杂度的框架。

Method: 提出Interaction Tensor SHAP (IT SHAP)，将Shapley Taylor交互指数重新表述为值张量和权重张量的收缩。假设权重张量具有多项式TT秩的有限状态张量链表示。在张量链结构化的模型和分布张量下，将STII的指数复杂度Θ(4^n)降低到NC2并行时间。

Result: IT SHAP能够在多项式时间和多对数深度下计算高阶Shapley交互，为高维模型中的主效应和高阶交互提供了统一、公理化且计算可行的公式化方法。

Conclusion: IT SHAP为可扩展的交互感知可解释AI奠定了基础，使得之前因组合结构而无法进行交互分析的大型黑盒模型现在可以进行交互分析。

Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.

</details>


### [24] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: Roblox Guard 1.0是基于Llama-3.1-8B-Instruct构建的安全防护LLM，通过指令微调增强LLM系统的输入输出安全审核能力，并发布RobloxGuard-Eval评估基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在训练后进行了安全对齐，但仍可能生成不当输出，存在用户风险，需要跨输入输出的鲁棒安全防护机制。

Method: 基于Llama-3.1-8B-Instruct进行指令微调，使用合成和开源安全数据集混合训练，增强链式思维推理和输入反转技术以提升上下文理解和决策能力。

Result: 模型能够泛化到未见过的安全分类体系，在领域外安全基准测试中表现优异，并发布了可扩展安全分类的RobloxGuard-Eval评估基准。

Conclusion: Roblox Guard 1.0为LLM系统提供了全面的输入输出安全防护框架，通过指令微调和创新训练方法提升了安全审核能力，同时提供了系统化评估工具。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [25] [Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation](https://arxiv.org/abs/2512.05367)
*Mariia Karabin,Isaac Armstrong,Leo Beck,Paulina Apanel,Markus Eisenbach,David B. Mitzi,Hanna Terletska,Hendrik Heinz*

Main category: cs.LG

TL;DR: 提出机器学习框架预测杂化金属卤化物结构维度，通过化学特征工程和类别不平衡处理技术提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 杂化金属卤化物（包括有机-无机钙钛矿）的结构维度（0D、1D、2D、3D）预测对材料设计至关重要，但现有数据集存在严重的类别不平衡问题，影响预测模型的性能

Method: 结合化学信息特征工程和类别不平衡处理技术，开发基于相互作用的描述符，采用多阶段工作流程，包括特征选择、模型堆叠和性能优化，并使用SMOTE技术将数据集从494个扩展到1336个

Result: 显著提高了少数类别的F1分数，在所有维度上都实现了稳健的交叉验证性能，有效解决了类别不平衡带来的预测挑战

Conclusion: 该机器学习框架通过创新的特征工程和类别不平衡处理策略，成功提升了杂化金属卤化物结构维度预测的准确性和鲁棒性，为材料设计提供了有效工具

Abstract: We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.

</details>


### [26] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: CATR框架通过选择稀疏的必要文本特征子集来解决文本因果推断中的正性假设违反问题，提高治疗效果估计的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 文本数据在因果推断中的应用面临挑战：高维文本特征会导致正性假设违反，产生极端倾向得分、不稳定权重和方差膨胀，影响治疗效果估计的准确性。

Method: 提出Confounding-Aware Token Rationalization (CATR)框架，使用残差独立性诊断选择稀疏的必要文本特征子集，保留足够的混杂信息以保持无混杂性，同时丢弃无关文本。

Result: 在合成数据和MIMIC-III数据库的真实世界研究中，CATR相比现有基线方法产生了更准确、稳定和可解释的因果效应估计。

Conclusion: CATR通过选择稀疏的文本特征子集有效解决了文本因果推断中的正性假设违反问题，提高了治疗效果估计的质量和可解释性。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [27] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: 该研究扩展了CorrDiff扩散模型，用于中国区域3公里分辨率天气预报降尺度，相比基准区域模型CMA-MESO在MAE指标上表现更优，并能生成更真实的雷达反射率细节。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报中高效生成高分辨率预报是一个基本挑战。统计降尺度方法通过建立低分辨率与高分辨率历史数据间的统计关系来解决此问题。深度学习特别是扩散模型在超分辨率任务中表现出色，但现有CorrDiff模型仅考虑小区域和地表变量，需要扩展到更大区域和多层大气变量。

Method: 采用基于扩散模型的CorrDiff降尺度框架，扩展到中国区域（面积扩大近20倍），不仅考虑地表变量，还包含6个气压层的高空变量。添加全局残差连接提高精度。将训练好的模型应用于CMA-GFS（25公里全球网格预报）和SFF（基于球面傅里叶神经算子的数据驱动模型），生成3公里分辨率预报。

Result: 实验结果表明，该方法降尺度后的预报在目标变量的平均绝对误差（MAE）上普遍优于CMA-MESO直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成精细尺度细节，相比确定性回归模型产生更真实的预测。

Conclusion: 扩展后的CorrDiff扩散模型能够有效应用于大区域多层大气变量的天气预报降尺度任务，在精度和真实性方面优于传统区域模型，展示了生成模型在气象降尺度中的潜力。

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [28] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 评估蛋白质-配体评分函数在新蛋白质靶点上的泛化能力，发现常用基准测试不能反映真实挑战，探索大规模自监督预训练和简单测试数据利用方法


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中越来越重要，需要确保可学习的蛋白质-配体评分函数在新蛋白质靶点上的可靠性。虽然许多评分函数在标准基准测试中表现良好，但它们在训练数据之外的泛化能力仍然是一个重大挑战。

Method: 1) 在模拟有限已知结构和实验亲和力测量的新靶点的数据集划分上评估最先进评分函数的泛化能力；2) 研究大规模自监督预训练是否能弥合泛化差距；3) 探索利用有限测试靶点数据改进评分函数性能的简单方法。

Result: 分析显示常用基准测试不能反映泛化到新靶点的真实挑战；提供了大规模自监督预训练潜力的初步证据；测试了利用有限测试数据改进性能的方法。

Conclusion: 研究结果强调了需要更严格的评估协议，并为设计具有扩展到新蛋白质靶点预测能力的评分函数提供了实用指导。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [29] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 提出了MineROI-Net，一个基于Transformer的模型，用于预测比特币挖矿硬件购买的盈利性，帮助矿工在波动的市场中做出更好的投资决策。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿硬件购买面临市场波动、技术快速过时和协议驱动收入周期等挑战，但缺乏何时购买ASIC硬件的指导，也没有计算框架解决这一决策问题。

Method: 将硬件购买问题建模为时间序列分类任务，预测购买ASIC机器在一年内是否盈利。提出了MineROI-Net，一个开源的基于Transformer的架构，用于捕捉挖矿盈利性的多尺度时间模式。

Result: 在2015-2024年发布的20个ASIC矿机数据上评估，MineROI-Net优于LSTM和TSLANet基线，达到83.7%准确率和83.1%宏F1分数。模型在经济相关性方面表现强劲，对不盈利时期的检测精度达93.6%，对盈利时期达98.5%。

Conclusion: MineROI-Net为挖矿硬件购买时机提供了实用的数据驱动工具，可能降低资本密集型挖矿操作的财务风险。模型已在GitHub开源。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [30] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD：一种反射式进化编排器，将LLM推理与反馈对齐的架构搜索有效结合，通过多轮多专家共识、自适应反射探索和帕累托进化选择，实现高性能神经架构设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的神经架构设计系统存在局限性：令牌级设计循环是离散且不可微分的，反馈无法平滑指导架构改进，导致模式崩溃为冗余结构或漂移到不可行设计。

Method: 1. 多轮多专家共识：将孤立的设计规则转化为有意义的架构线索；2. 自适应反射探索：根据奖励方差调整探索程度，不确定时探索，稳定时细化；3. 帕累托进化选择：联合优化准确性、效率、延迟、置信度和结构多样性。

Result: 在CIFAR10、CIFAR100、ImageNet16-120、COCO-5K和Cityscape数据集上达到最先进性能，消融和迁移研究验证了RevoNAD在实际可靠和可部署神经架构设计中的有效性。

Conclusion: RevoNAD成功地将LLM推理与反馈对齐的架构搜索相结合，解决了现有方法的离散性和不可微问题，实现了高性能、可靠且可部署的神经架构设计。

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [31] [Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets](https://arxiv.org/abs/2512.05416)
*Bozhi Dan,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: Triplet-GCN：一种基于图卷积网络的脓毒症早期预警模型，通过将电子健康记录表示为患者-特征-值三元组构建二分图，在ICU环境中显著优于传统表格模型


<details>
  <summary>Details</summary>
Motivation: ICU环境中脓毒症是导致患者疾病和死亡的主要原因，但电子健康记录数据的复杂性、稀疏性和异质性阻碍了及时检测。需要一种能有效处理这种复杂数据结构的模型来改善早期预警。

Method: 提出Triplet-GCN模型：1) 将每次就诊表示为患者-特征-值三元组；2) 构建二分EHR图；3) 使用图卷积网络学习患者嵌入；4) 后接轻量级多层感知机进行分类。采用类型特定的预处理策略，保留测量值在边上以保持"谁测量了什么以及测量了多少"的信息。

Result: 在中国三家三级医院的回顾性多中心队列（N=648，70/30训练测试分割）中，Triplet-GCN在区分度和平衡误差指标上一致优于KNN、SVM、XGBoost、随机森林等强基线表格模型，产生了更有利的敏感性-特异性权衡，提高了早期预警的整体效用。

Conclusion: 将EHR编码为三元组并在患者-特征图上传播信息比特征独立模型产生更具信息量的患者表示，为可部署的脓毒症风险分层提供了一个简单、端到端的蓝图。

Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient-feature-value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train-test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity-specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient-feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.

</details>


### [32] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 提出TS-Hint框架，结合时序基础模型与思维链推理，通过注意力机制和显著性数据提供训练提示，在有限数据下实现半导体制造中材料去除率的高效预测。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法从时间序列中提取静态特征来近似半导体制造过程（如化学机械抛光）的材料去除率，但这导致时间动态信息丢失，且需要大量数据进行有效训练。

Method: 提出TS-Hint框架，将时序基础模型与思维链推理相结合，基于注意力机制数据和显著性数据在训练过程中提供注意力提示，支持从多元时间序列特征直接学习。

Result: 实验结果表明，该模型在有限数据设置下通过少样本学习表现出有效性，能够直接从多元时间序列特征中学习。

Conclusion: TS-Hint框架通过结合时序基础模型和思维链推理，解决了现有方法丢失时间动态信息、需要大量训练数据的问题，在半导体制造过程预测中具有实用价值。

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [33] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: 提出IdealTSF框架，利用非理想负样本增强时间序列预测，通过预训练、训练和优化三阶段，在噪声样本或低质量数据场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据中普遍存在缺失值和异常值等问题，传统方法主要关注特征提取或将这些问题数据作为正样本进行知识迁移，但未能充分利用这些非理想数据作为负样本的潜力来增强事件预测。

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据中提取知识；2) 训练阶段将序列数据转换为理想正样本；3) 采用带有对抗扰动的负优化机制。

Result: 实验证明负样本数据在基础注意力架构中释放了显著的时间序列预测潜力，IdealTSF特别适合噪声样本或低质量数据的应用场景。

Conclusion: 通过有效利用非理想负样本，IdealTSF框架显著提升了时间序列预测性能，为处理噪声数据提供了新的解决方案。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [34] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 研究显示，集成模型在非线性结构化数据上能提升5-7%的测试准确率，同时保持低于3%的泛化差距，但在线性数据上优势有限，需正则化处理噪声和不平衡数据。


<details>
  <summary>Details</summary>
Motivation: 集成模型通常比单一学习器获得更高准确率，但其保持小泛化差距的能力尚未被充分理解。本研究旨在探究集成模型如何在准确率和过拟合之间取得平衡。

Method: 使用重复分层交叉验证和统计显著性检验，比较线性模型、单一决策树和9种集成方法在四个表格分类任务（乳腺癌、心脏病、糖尿病、信用卡欺诈）上的表现。

Result: 在近线性和干净数据上，线性模型已能很好泛化，集成模型提供额外收益有限。在具有非线性结构的数据集上，基于树的集成模型能将测试准确率提升5-7个百分点，同时保持差距低于3%。在噪声或高度不平衡数据集上，集成模型仍具竞争力但需要正则化以避免过拟合。

Conclusion: 集成模型通过平均或受控提升减少方差，能在保持高准确率的同时控制过拟合。研究提供了数据集复杂度指标（线性度评分、Fisher比率、噪声估计）来解释集成模型何时能有效控制方差，为实际表格应用中的模型选择提供实用指导。

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [35] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 比较不同几何量子机器学习模型在分子几何结构上的性能，发现图嵌入和置换对称嵌入对几何数据集具有更好的可训练性和泛化性


<details>
  <summary>Details</summary>
Motivation: 研究分子几何层次结构中不同对称性量子机器学习模型的性能差异，为几何学习任务选择合适模型提供标准

Method: 使用LiH（线性分子）和NH3（三角锥形分子）两个分子数据集，比较四种模型：无对称性、旋转和置换等变性、图嵌入置换等变性，以经典等变模型为基准

Result: 图嵌入特征对几何数据集是提高可训练性的有效途径，置换对称嵌入是几何学习中最具泛化性的量子机器学习模型

Conclusion: 分子几何结构与模型性能差异揭示了模型选择标准，置换对称嵌入是几何学习的最佳量子模型选择

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [36] [Turbulence Regression](https://arxiv.org/abs/2512.05483)
*Yingang Fan,Binjie Ding,Baiyi Chen*

Main category: cs.LG

TL;DR: 提出基于离散化数据的NeuTucker分解模型，用于低空湍流预测，通过Tucker神经网络构建低秩分解模型，在风廓线雷达数据缺失观测估计中表现优于传统回归模型。


<details>
  <summary>Details</summary>
Motivation: 低空湍流由多种复杂因素导致，传统方法在仅使用风廓线雷达数据时难以准确预测湍流状态，需要新的建模方法。

Method: 1) 将连续输入数据离散化以适应需要离散数据输入的模型；2) 构建四维Tucker交互张量表示不同高度和三维风速间的时空交互；3) 基于Tucker神经网络构建低秩Tucker分解模型。

Result: 在真实数据集缺失观测估计中，离散化NeuTucF模型相比多种常见回归模型表现出更优越的性能。

Conclusion: 提出的离散化NeuTucker分解模型能有效处理连续稀疏的三维风场数据，捕捉潜在交互，为低空湍流预测提供有效解决方案。

Abstract: Air turbulence refers to the disordered and irregular motion state generated by drastic changes in velocity, pressure, or direction during airflow. Various complex factors lead to intricate low-altitude turbulence outcomes. Under current observational conditions, especially when using only wind profile radar data, traditional methods struggle to accurately predict turbulence states. Therefore, this paper introduces a NeuTucker decomposition model utilizing discretized data. Designed for continuous yet sparse three-dimensional wind field data, it constructs a low-rank Tucker decomposition model based on a Tucker neural network to capture the latent interactions within the three-dimensional wind field data. Therefore, two core ideas are proposed here: 1) Discretizing continuous input data to adapt to models like NeuTucF that require discrete data inputs. 2) Constructing a four-dimensional Tucker interaction tensor to represent all possible spatio-temporal interactions among different elevations and three-dimensional wind speeds. In estimating missing observations in real datasets, this discretized NeuTucF model demonstrates superior performance compared to various common regression models.

</details>


### [37] [GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop](https://arxiv.org/abs/2512.05502)
*Omid Bazgir,Vineeth Manthapuri,Ilia Rattsev,Mohammad Jafarnejad*

Main category: cs.LG

TL;DR: GRASP是一个多智能体图推理框架，通过人机对话界面将QSP模型编码为类型化生物知识图谱，并编译为可执行的MATLAB/SimBiology代码，同时保持单位、质量平衡和生理约束。


<details>
  <summary>Details</summary>
Motivation: 定量系统药理学(QSP)建模对药物开发至关重要，但需要大量时间投入，限制了领域专家的处理能力。现有方法在保持生物医学保真度方面存在挑战。

Method: 采用两阶段工作流程：理解阶段（从遗留代码重建图谱）和行动阶段（约束检查、语言驱动的修改）。通过状态机协调迭代验证，使用广度优先参数对齐为新实体发现依赖量并提出生物合理默认值。

Result: 在LLM作为评判的对比评估中，GRASP在生物合理性、数学正确性、结构保真度和代码质量方面优于SME引导的CoT和ToT基线（约9-10/10 vs. 5-7/10）。BFS对齐在依赖发现、单位和范围方面达到F1=0.95。

Conclusion: 图结构化的智能体工作流程可以使QSP模型开发既易于访问又严谨，使领域专家能够用自然语言指定机制而不牺牲生物医学保真度。

Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.

</details>


### [38] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 提出了CDEC和IDEC两种新的不确定性量化方法，通过信度集和区间证据分布来处理分类任务中的不确定性，能够识别和标记超出阈值的认知和偶然不确定性，并在可接受范围内提供具有概率保证的标签集合。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化在人工智能中至关重要，影响决策制定、风险评估和模型可靠性。现有方法存在不足，需要新的方法来避免过拟合，并系统评估认知不确定性和偶然不确定性。

Method: 提出CDEC（基于信度集）和IDEC（基于区间证据分布）两种方法，使用标准反向传播和基于证据理论的损失函数进行训练。CDEC使用封闭凸概率集，IDEC使用证据预测分布的区间。

Result: 在MNIST、CIFAR-10、CIFAR-100及其自然OoD偏移数据集上的实验表明，CDEC和IDEC实现了竞争力的预测准确性、最先进的OoD检测性能，以及紧致且校准良好的预测区域。CDEC仅需小规模集成就能获得稳定的不确定性估计。

Conclusion: CDEC和IDEC是处理分类任务中不确定性量化的有效新方法，能够克服先前工作的不足，扩展了证据深度学习文献，在预测准确性、OoD检测和不确定性校准方面表现出色。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [39] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: IDK-S是一种用于数据流异常检测的新型增量分布核方法，通过动态核均值嵌入表示，在保持高检测精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测面临两大挑战：需要在分布演化中保持高检测精度，同时确保实时效率。现有方法难以同时满足这两个要求。

Method: IDK-S基于隔离分布核（Isolation Distributional Kernel）框架，采用轻量级增量更新机制，避免完全模型重训练，在核均值嵌入框架中创建动态表示。

Result: 在13个基准测试中，IDK-S实现了优于现有最先进方法的检测精度，同时运行速度显著更快（在许多情况下快一个数量级），且统计上等价于完全重训练模型。

Conclusion: IDK-S成功解决了数据流异常检测中精度与效率的权衡问题，通过创新的增量分布核方法，在保持统计等价性的同时大幅提升计算效率。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [40] [On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability](https://arxiv.org/abs/2512.05534)
*Yiming Tang,Harshvardhan Saini,Yizhen Liao,Dianbo Liu*

Main category: cs.LG

TL;DR: 提出了首个统一的理论框架来分析稀疏字典学习方法，解释了特征吸收、死神经元等经验现象，并设计了验证实验。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，理解其内部表示和处理机制变得重要。稀疏字典学习方法在经验上成功但缺乏理论理解，现有理论仅限于特定约束的稀疏自编码器，需要统一的理论框架。

Method: 将稀疏字典学习视为统一的优化问题，建立理论框架，分析不同方法在该框架下的实例化，对优化景观进行严格分析，并设计控制实验验证理论结果。

Result: 首次为特征吸收、死神经元和神经元重采样等技术提供了理论解释，验证了理论框架的有效性。

Conclusion: 提出的统一理论框架为稀疏字典学习方法提供了理论基础，解释了经验现象，推动了机制可解释性的理论发展。

Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.

</details>


### [41] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: SCoNE是一种新型多视图异常检测方法，通过球面一致邻域集成直接表示多视图实例，无需中间表示，具有数据依赖特性，实现O(N)时间复杂度，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图异常检测方法存在两个关键问题：1) 在不同视图中密度变化区域难以有效捕获一致邻域，导致检测精度低；2) 学习过程计算复杂度高(O(N²))，不适用于大规模数据集。

Method: 提出SCoNE方法，具有两个独特特征：a) 直接使用多视图实例表示一致邻域，无需中间表示；b) 邻域具有数据依赖特性，在稀疏区域形成大邻域，在密集区域形成小邻域，无需学习过程。

Result: 实验评估表明，SCoNE具有优越的检测精度，在大规模数据集上运行速度比现有方法快几个数量级。

Conclusion: SCoNE通过直接表示数据依赖的一致邻域，解决了多视图异常检测中的一致邻域表示和计算效率问题，实现了高精度和线性时间复杂度。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [42] [RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs](https://arxiv.org/abs/2512.05542)
*Jonathan Geuter,Gregor Kornhardt*

Main category: cs.LG

TL;DR: RoBoN是一种多模型推理方法，通过在线路由机制在多个LLM之间选择最佳响应，相比传统单模型best-of-n方法能获得更好的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统best-of-n方法只使用单一模型生成多个响应，但不同LLM在不同任务上具有互补优势。作者希望利用模型多样性来提升推理性能。

Method: RoBoN采用顺序路由机制，在多个模型间逐个路由生成响应。基于奖励模型分数和预测响应的一致性信号进行路由决策，无需额外训练，保持计算量相当。

Result: 在多个推理基准测试（MATH500、OlympiadBench、MinervaMath、GSM8K、MMLU）上，RoBoN在较大n值时始终优于标准单模型best-of-n方法，绝对准确率提升高达3.4%，也优于均匀多模型组合基线。

Conclusion: 模型多样性可以在推理时被利用来提升best-of-n性能，提供了一种简单、无需训练的测试时扩展方法，能够超越任何单一组成模型。

Abstract: Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.

</details>


### [43] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 提出一种基于MARS和N-ball采样的新型可解释方法，相比LIME能更好地捕捉非线性局部边界，提高解释的忠实度


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型在关键领域的应用增加，需要提供可靠的预测解释。LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确。

Method: 使用多元自适应回归样条(MARS)建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从期望分布采样，而不是像LIME那样重新加权样本。

Result: 在三个UCI数据集上评估，相比基线方法，新方法能产生更忠实的解释，平均降低37%的均方根误差，显著提高了局部忠实度。

Conclusion: 提出的方法通过MARS建模非线性边界和N-ball采样技术，有效提高了局部解释的忠实度，为复杂黑盒模型提供了更可靠的可解释性解决方案。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [44] [Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection](https://arxiv.org/abs/2512.05567)
*Antoine Blais,Nicolas Couëllan*

Main category: cs.LG

TL;DR: 提出基于最优传输的半监督学习方法，利用Wasserstein距离作为图像相似性度量，通过标签传播机制在卷积网络中学习稀缺标注数据，应用于GNSS多径干扰检测


<details>
  <summary>Details</summary>
Motivation: 在图像数据标注稀缺的情况下，需要开发有效的半监督学习方法。传统方法可能无法充分利用未标注数据，特别是在需要精确相似性度量的应用场景中

Method: 基于隐式图传导半监督学习框架，使用Wasserstein距离作为图像样本间的相似性度量，将该度量融入标签传播机制，结合深度卷积网络进行学习

Result: 在GNSS多径干扰检测实验中，通过调整半监督程度和度量敏感度的超参数，分类准确率相比全监督训练方法有显著提升

Conclusion: 基于最优传输的半监督学习方法能有效利用稀缺标注数据，在特定超参数设置下显著提升分类性能，特别适用于需要精确相似性度量的实际应用

Abstract: The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.

</details>


### [45] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出熵比剪裁(ERC)机制，通过约束当前与先前策略的熵比来稳定强化学习训练，解决分布偏移问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练依赖强化学习提升能力，但离策略训练引入分布偏移，导致策略超出信任区域，表现为策略熵波动和梯度不稳定。PPO-Clip通过重要性剪裁缓解此问题，但忽略了动作的全局分布偏移。

Method: 提出使用当前与先前策略的熵比作为全局度量，量化策略探索的相对变化。基于此提出熵比剪裁(ERC)机制，对熵比施加双向约束，在全局分布层面稳定策略更新，弥补PPO-clip无法调节未采样动作概率偏移的不足。将ERC集成到DAPO和GPPO算法中。

Result: 在多个基准测试上的实验表明，ERC能持续提升性能。

Conclusion: ERC机制通过全局熵比约束有效稳定强化学习训练，解决了传统方法忽略的全局分布偏移问题，在多种算法中均表现出性能提升。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [46] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 该研究探讨了如何通过超参数迁移来扩展预条件优化器（如Shampoo、SOAP、Muon）的规模，发现遵循μP规则缩放学习率并采用1/宽度缩放权重衰减，能在Llama架构语言模型上实现1.3-1.4倍于AdamW的加速效果。


<details>
  <summary>Details</summary>
Motivation: 近期基于矩阵级预条件的深度学习优化器在小规模实验中显示出优于AdamW的潜力，但大规模验证结果不一。研究旨在理解如何通过超参数迁移策略，使这些优化器在更大规模模型中保持优势。

Method: 研究优化器超参数（学习率、权重衰减）随模型宽度和深度的缩放规律，基于μP理论框架，并考虑分块（blocking）和嫁接（grafting）等常用技术的影响。通过实验验证不同缩放规则在Llama架构语言模型（190M到1.4B参数）上的效果。

Result: 遵循μP规则缩放学习率可改善迁移效果，但仍存在有限宽度偏差；分块和显式谱归一化可缓解此问题。计算最优缩放中，权重衰减按1/宽度缩放近乎最优。应用这些规则后，Muon和Shampoo分别在Llama模型上实现1.4倍和1.3倍于AdamW的加速，而不正确缩放会使加速效果随规模增大迅速消失。

Conclusion: 研究超参数最优迁移策略对于在有限调参预算下可靠比较大规模优化器性能至关重要。正确的缩放规则能使预条件优化器在大规模语言模型训练中保持显著优势。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [47] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 提出一个灵活的GNN社区检测框架，允许用户指定社区数量范围或确切数量，解决传统GNN方法无法可靠控制社区数量的问题。


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法需要预先指定社区数量，而GNN方法即使指定了期望数量也经常无法准确返回。现有方法缺乏灵活控制社区数量的机制。

Method: 提出一个原则性框架，允许用户指定社区数量的合理范围，并在训练过程中强制执行这些边界约束。同时支持指定确切社区数量并可靠返回。

Result: 该框架能够灵活控制GNN发现的社区数量，既支持范围约束也支持精确数量控制，解决了GNN社区检测中数量控制不可靠的问题。

Conclusion: 通过引入灵活的社区数量控制机制，增强了GNN在社区检测任务中的实用性和可靠性，为用户提供了更灵活的控制选项。

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [48] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 提出Modular Jets框架，通过估计模块级局部线性响应图来识别回归和分类流水线的内部分解，区分可识别与幻象（mirage）机制


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅通过预测风险评估模型，无法确定模型内部分解是否由数据和评估设计唯一确定。需要新的评估框架来理解模块化系统的内部结构

Method: 引入Modular Jets框架：给定任务流形、模块分解和模块级表示访问，估计经验jets（局部线性响应图）。提出MoJet算法进行经验jet估计和幻象诊断

Result: 在线性回归流水线中证明了jet可识别性定理：在温和秩假设下，模块级jets能唯一确定内部分解，而仅风险评估则允许大量幻象分解。通过线性和深度回归以及流水线分类验证框架

Conclusion: Modular Jets框架超越了传统风险评估，能够识别模块化系统的内部分解，区分可识别与幻象机制，为理解模型内部结构提供了新工具

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [49] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM（选择性梯度掩码）是一种改进的梯度路由技术，通过在预训练时零掩码选定梯度，将目标知识隔离到专用参数中，从而在存在标签噪声的情况下更有效地移除有害能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型的双重用途风险，特别是数据过滤方法面临的挑战：大规模标注有害数据成本高，且即使少量错误标注也可能导致危险能力。需要一种在标签噪声不可避免的情况下仍能有效移除特定知识的预训练时缓解方法。

Method: 提出SGTM（选择性梯度掩码），改进梯度路由技术，通过零掩码选定梯度，使目标领域示例仅更新其专用参数，从而将目标知识隔离到模型参数的特定子集中，便于后续移除。

Result: 在双语合成数据集移除语言知识和英语维基百科移除生物学知识两个应用中，SGTM在存在标签错误的情况下比数据过滤和先前梯度路由方法提供更好的保留/遗忘权衡。SGTM对对抗性微调表现出强鲁棒性，需要比基于微调的遗忘方法多7倍的微调步骤才能恢复遗忘集性能。

Conclusion: SGTM为现有安全缓解措施提供了一个有前景的预训练时补充，特别是在标签噪声不可避免的场景中，能够有效隔离和移除特定知识，增强模型安全性。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [50] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编码通过自然语言与AI助手交互开发应用，相比传统低代码平台更具灵活性，研究表明非程序员能在合理时间内成功完成基本Web应用开发。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助的终端用户编码是否可行，能否补充甚至替代现有的低代码/无代码平台，为组织数字化转型提供新途径。

Method: 通过案例研究，让非程序员与AI助手交互开发基本Web应用，分析任务完成情况和参与者反馈。

Result: 大多数参与者能在合理时间内成功完成任务，并支持AI辅助的终端用户编码作为可行的终端用户开发方法。

Conclusion: AI辅助的终端用户编码是可行的终端用户开发范式，可能补充甚至替代传统低代码平台，具有实践、研究和教学意义。

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [51] [Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks](https://arxiv.org/abs/2512.05680)
*Alexander Mattick,George Yammine,Georgios Kontes,Setareh Maghsudi,Christopher Mutschler*

Main category: cs.LG

TL;DR: 本文提出了一种基于部分可观测马尔可夫决策过程（POMDP）的波束选择方法，将波束选择问题建模为在线搜索过程，以定位移动的最优波束。


<details>
  <summary>Details</summary>
Motivation: 在5G/6G网络中，大规模天线阵列的模拟波束成形面临挑战：大型码本、反射和遮挡效应使得最优波束选择困难。现有方法使用监督学习基于历史波束预测最优波束，但难以处理新轨迹和环境变化。

Method: 将波束选择问题建模为POMDP，环境建模为码本本身。在每个时间步，基于不可观测最优波束的信念状态和先前探测的波束选择候选波束，将问题框架化为定位移动最优波束的在线搜索过程。

Result: 该方法能够处理新的或不可预见的轨迹和物理环境变化，性能比先前工作高出数个数量级。

Conclusion: 基于POMDP的在线搜索方法比传统的监督学习方法更有效地解决了动态环境下的波束选择问题，具有更好的适应性和性能。

Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.

</details>


### [52] [BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language](https://arxiv.org/abs/2512.05721)
*Nitin Priyadarshini Shankar,Vaibhav Singh,Sheetal Kalyani,Christian Maciocco*

Main category: cs.LG

TL;DR: BERTO是一个基于BERT的框架，用于蜂窝网络的流量预测和能耗优化，通过自然语言提示平衡节能与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络需要智能的RAN部署来平衡节能和性能这两个相互竞争的目标，传统方法难以灵活调整这种权衡。

Method: 基于Transformer架构构建BERTO框架，使用平衡损失函数和基于提示的自定义功能，通过自然语言提示指导模型管理预测不足和过度预测。

Result: 在真实数据集上的实验表明，BERTO相比现有模型将MSE降低了4.13%，能够在1.4kW功率范围和9倍服务质量变化范围内灵活操作。

Conclusion: BERTO通过简单的自然语言输入平衡节能和性能目标，适用于智能RAN部署，为网络运营商提供了灵活的自定义能力。

Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.

</details>


### [53] [Teaching Language Models Mechanistic Explainability Through Arrow-Pushing](https://arxiv.org/abs/2512.05722)
*Théo A. Neukomm,Zlatko Jončev,Philippe Schwaller*

Main category: cs.LG

TL;DR: 该研究开发了一个基于语言模型的化学反应机制预测框架，使用MechSMILES格式编码分子结构和电子流，在多个机制预测任务上取得高准确率，并应用于合成规划验证、原子映射和催化剂识别。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划系统缺乏机制基础，无法提供化学反应机制的关键洞察。需要开发能够预测化学反应机制的计算框架，使合成规划更具可解释性和化学有效性。

Method: 引入箭头推动形式主义的计算框架，开发MechSMILES紧凑文本格式编码分子结构和电子流，使用语言模型在四个复杂度递增的机制预测任务上进行训练，使用mech-USPTO-31k和FlowER等机制反应数据集。

Result: 模型在基本步骤预测上达到超过95%的top-3准确率，在mech-USPTO-31k上超过73%，在FlowER数据集上达到93%的完整反应机制检索准确率。实现了三个关键应用：CASP系统后验证、全原子映射和催化剂感知反应模板提取。

Conclusion: 通过基于物理意义的电子移动进行预测，确保质量和电荷守恒，为更可解释和化学有效的计算合成规划提供了途径，同时提供了机制预测的架构无关基准测试框架。

Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.

</details>


### [54] [Towards agent-based-model informed neural networks](https://arxiv.org/abs/2512.05764)
*Nino Antulov-Fantulin*

Main category: cs.LG

TL;DR: 提出ABM-NNs框架，将基于智能体模型的约束融入神经网络设计，确保学习到的动力学保持物理约束和结构特性，在三个复杂性递增的案例中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程在建模复杂系统时存在局限，无法保证物理不变量（如能量）的保持，但许多系统需要强制约束（如质量守恒、网络局部性、有限理性）。需要设计能够保持基于智能体模型原则的神经网络。

Method: 提出Agent-Based-Model informed Neural Networks(ABM-NNs)，利用受限图神经网络和层次分解来学习可解释、结构保持的动力学。通过约束图神经网络结构和层次分解确保学习到的模型符合智能体模型的底层原则。

Result: 在三个案例中验证：1)广义Lotka-Volterra系统中从短轨迹恢复真实参数；2)图基SIR传染模型中在样本外预测和噪声鲁棒性上优于GCN、GraphSAGE、Graph Transformer等基线；3)十大经济体宏观经济模型中从经验数据学习耦合GDP动态，并实现基于梯度的政策干预反事实分析。

Conclusion: ABM-NNs框架能够有效学习保持智能体模型约束的动力学，在复杂系统建模中优于标准方法，为政策分析和干预提供了可解释的梯度工具。

Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.

</details>


### [55] [Learnability Window in Gated Recurrent Neural Networks](https://arxiv.org/abs/2512.05790)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，解释门控机制如何决定循环神经网络的"可学习性窗口"——梯度信息在时间上保持统计可恢复的最大时间范围。研究发现，可学习性不仅取决于数值稳定性，更由"有效学习率"这一关键量控制，它作为乘性滤波器调节梯度传输。


<details>
  <summary>Details</summary>
Motivation: 传统分析主要关注循环神经网络中雅可比乘积的数值稳定性，但作者认为这不足以解释门控网络如何学习长期依赖关系。需要建立一个理论框架来理解门控机制如何影响梯度信息在时间上的传输和统计可恢复性。

Method: 通过一阶展开分析门控诱导的雅可比乘积，推导出"有效学习率"这一关键量。在重尾（α-稳定）梯度噪声假设下，建立检测滞后依赖所需最小样本量与有效学习率包络之间的数学关系，得出可学习性窗口的显式公式。

Result: 理论表明：1）更宽或更异质的门控谱产生更慢的有效学习率衰减，从而扩大可学习性窗口；2）更重的尾部噪声通过减缓统计集中度来压缩可学习性窗口；3）有效学习率是控制门控循环网络学习长期时间依赖的根本量。

Conclusion: 有效学习率是连接门控诱导的时间尺度结构、梯度噪声和样本复杂性的核心量，它决定了门控循环网络何时以及能够学习多长时间的长期时间依赖关系。

Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $μ_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($α$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-α}$, where $f(\ell)=\|μ_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.

</details>


### [56] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: TopK SAEs可识别抗体语言模型中的生物学特征，但特征相关不等于因果控制；Ordered SAEs能可靠识别可操控特征，但激活模式更复杂、可解释性降低。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器在蛋白质语言模型机制可解释性中的应用，特别是探索如何有效识别和操控抗体语言模型中的潜在特征，以实现生成导向。

Method: 使用TopK和Ordered两种稀疏自编码器分析自回归抗体语言模型p-IgGen，比较它们在特征识别和生成操控方面的表现。

Result: TopK SAEs能揭示具有生物学意义的潜在特征，但高特征概念相关性不能保证对生成的因果控制；Ordered SAEs通过层次结构能可靠识别可操控特征，但激活模式更复杂且可解释性降低。

Conclusion: TopK SAEs适合将潜在特征映射到概念，而Ordered SAEs在需要精确生成操控时更优，这推进了领域特定蛋白质语言模型的机制可解释性研究。

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [57] [Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws](https://arxiv.org/abs/2512.05817)
*Zhengquan Luo,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 论文提出了一个统一的理论框架来分析数据集蒸馏，揭示了性能随蒸馏样本数增长的缩放规律，以及所需样本数与配置多样性之间的线性关系。


<details>
  <summary>Details</summary>
Motivation: 数据集蒸馏方法虽然经验上进展迅速，但缺乏统一的理论基础。现有方法基于不同的代理目标和优化假设，难以分析其共同原理或提供通用保证。同时，不清楚在训练配置（如优化器、架构、数据增强）变化时，蒸馏数据如何保持有效性。

Method: 提出了"配置-动态-误差分析"统一理论框架，将主要数据集蒸馏方法重新表述为通用泛化误差视角。该框架提供了两个主要结果：1）缩放定律（单配置上界）；2）覆盖定律（所需蒸馏样本数与配置多样性呈线性关系）。

Result: 理论分析表明：1）误差随蒸馏样本数增加而减少，解释了常见的性能饱和效应；2）所需蒸馏样本数与配置多样性呈线性比例关系，且上下界匹配；3）各种匹配方法是可互换的代理，都能减少相同的泛化误差。

Conclusion: 该统一框架为数据集蒸馏提供了理论基础，揭示了各种方法的共同原理，解释了为什么它们都能实现数据集蒸馏，并为设计紧凑、配置鲁棒的数据集蒸馏提供了理论指导。实验验证了推导的定律。

Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.

</details>


### [58] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 论文提供了超体积近似算法的完整数学和算法描述，填补了文献中的空白


<details>
  <summary>Details</summary>
Motivation: 超体积贝叶斯优化中，获取函数的优化计算成本高，主要源于超体积改进计算的昂贵。虽然超体积盒分解能处理频繁的精确改进计算，但存在超多项式内存复杂度问题。现有近似算法缺乏严格的算法描述。

Method: 提供Couckuyt等人（2012）提出的超体积近似算法的全面数学和算法细节描述，填补文献中的空白。

Result: 论文提供了该近似算法的完整数学框架和算法实现细节，使该算法在文献中有了严格的描述。

Conclusion: 通过提供超体积近似算法的全面数学和算法描述，填补了多目标决策中贝叶斯优化领域的一个重要空白。

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [59] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: NEAT是一种用于3D分子生成的自回归模型，通过将分子图视为原子集合，学习图边界上可接受标记的顺序无关分布，实现了计算高效且具有原子级置换不变性的分子生成。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在3D分子结构生成中存在局限性：文本具有自然顺序，但分子图中的下一个标记预测应对原子置换保持不变。先前工作通过使用规范顺序或焦点原子来回避这一不匹配问题，作者认为这是不必要的。

Method: NEAT（Neighborhood-guided, Efficient, Autoregressive, Set Transformer）将分子图视为原子集合，使用自回归流模型学习图边界上可接受标记的顺序无关分布，实现置换不变性。

Result: NEAT在3D分子生成中接近最先进性能，具有高计算效率和原子级置换不变性，为可扩展分子设计建立了实用基础。

Conclusion: NEAT通过处理分子图的置换不变性，为自回归3D分子生成提供了更自然和高效的方法，避免了先前工作中不必要的顺序假设。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [60] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出一种简单的后训练方法，使Transformer注意力稀疏化而不牺牲性能，可将注意力连接减少到约0.3%，同时保持原始预训练损失


<details>
  <summary>Details</summary>
Motivation: 探索Transformer注意力中的冗余计算，利用稀疏性作为结构先验，以获得更组织化和可解释的连接模式，同时保持模型能力

Method: 采用简单的后训练方法，在约束损失目标下应用灵活的稀疏正则化，使Transformer注意力稀疏化

Result: 在高达10亿参数的模型上，可将注意力连接减少到约0.3%的边，同时保持原始预训练损失；局部稀疏性级联为全局电路简化，任务特定电路涉及的组件和连接边减少高达100倍

Conclusion: Transformer注意力可以变得数量级更稀疏，表明其大部分计算是冗余的，稀疏性可作为构建更结构化和可解释模型的指导原则

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [61] [Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks](https://arxiv.org/abs/2512.05868)
*Brian Ezinwoke,Oliver Rhodes*

Main category: cs.LG

TL;DR: 该研究将脉冲神经网络（SNN）应用于高频交易中的价格尖峰预测，通过贝叶斯优化和惩罚性尖峰准确率（PSA）目标函数进行超参数调优，在模拟交易中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中存在突然的价格尖峰，既带来风险也创造机会，但传统金融模型难以捕捉所需的精细时间结构。脉冲神经网络具有处理离散事件和保持毫秒级时间精度的天然优势，适合解决这一问题。

Method: 将高频股票数据转换为脉冲序列，评估三种架构：1）基于STDP训练的经典无监督SNN；2）具有显式抑制竞争的新型SNN；3）监督式反向传播网络。使用贝叶斯优化进行超参数调优，驱动优化的目标函数是新颖的惩罚性尖峰准确率（PSA），确保网络预测的价格尖峰率与实际价格事件率一致。

Result: 模拟交易显示，使用PSA优化的模型始终优于使用尖峰准确率（SA）调优的对应模型和基线。具体而言，扩展的SNN模型在简单回测中实现了最高的累计回报（76.8%），显著优于监督式替代方案（42.54%回报）。

Conclusion: 研究验证了脉冲神经网络在通过任务特定目标进行稳健调优后，在高频交易价格尖峰预测中的有效性，展示了SNN在金融时间序列分析中的应用潜力。

Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.

</details>


### [62] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 提出基于机器学习的数据驱动方法预测蒸汽压，用于筛选和发现适合太空环境的液体润滑剂


<details>
  <summary>Details</summary>
Motivation: 太空环境中移动机械组件（MMAs）需要低蒸汽压的液体润滑剂，但现有选项有限且各有局限性，限制了MMA设计

Method: 使用机器学习模型，结合高通量分子动力学模拟和实验数据库数据训练，注重模型可解释性以识别化学结构与蒸汽压的关系

Result: 训练了可解释的ML模型，基于化学结构-蒸汽压关系提出了多个有潜力的候选分子用于未来太空润滑应用

Conclusion: 数据驱动的ML方法能够有效预测蒸汽压，为发现新型太空适用液体润滑剂提供了可行途径，有助于克服当前太空润滑剂选择的限制

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [63] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 提出Neural Coherence方法，仅需少量无标注目标域样本即可选择最佳预训练模型检查点，显著提升跨域泛化性能


<details>
  <summary>Details</summary>
Motivation: 当目标任务数据稀缺、无标注且分布外时，传统基于验证集的方法不可靠。需要一种仅需少量无标注样本就能可靠选择预训练模型检查点的方法

Method: 提出Neural Coherence概念，通过分析模型在源域和目标域的激活统计特征，设计高效的数据选择方法。在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K、iNaturalist等目标域进行实验

Result: 相比现有基线方法，Neural Coherence显著提升了不同目标域的泛化性能，并在元学习设置中表现良好。同时证明该方法在训练数据选择方面也具有有效性

Conclusion: Neural Coherence是一个强大的原则，仅需少量无标注目标域样本就能可靠选择预训练模型，解决了分布外、数据稀缺场景下的模型选择问题

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [64] [DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints](https://arxiv.org/abs/2512.05881)
*Rahul Golder,Bimol Nath Roy,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: DAE-HardNet：一种物理约束神经网络，通过可微分投影层严格满足微分代数方程约束，相比传统PINNs能大幅减少物理损失


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)通常以软约束方式最小化物理约束违反，难以严格满足包含微分算子的微分代数方程(DAEs)约束。需要开发能同时学习函数及其导数并严格满足代数与微分约束的物理约束神经网络。

Method: 提出DAE-HardNet，通过可微分投影层将模型预测投影到约束流形上，同时学习函数及其导数，严格强制执行代数与微分约束。网络包含主干神经网络和投影层两部分。

Result: 在Lotka-Volterra捕食者-猎物系统和瞬态热传导等DAEs控制的问题上，相比MLPs和PINNs，DAE-HardNet实现了物理损失的数量级减少，同时保持预测精度。还能学习导数并用于参数估计问题。

Conclusion: DAE-HardNet能严格满足物理约束，显著优于传统PINNs的软约束方法。投影层设计使得在某些问题中可以绕过投影层进行更快推理，同时学习导数有助于约束主干网络的学习。

Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.

</details>


### [65] [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)
*Neha Gupta,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 基于LSTM的循环神经网络框架用于估计分数泊松过程参数，相比传统矩估计方法减少55.3%的均方误差，在真实高频数据上有效追踪日模式和参数变化。


<details>
  <summary>Details</summary>
Motivation: 分数泊松过程能够建模具有记忆性和长程依赖的事件到达，但传统参数估计方法存在局限性。需要开发更有效的估计方法来处理真实世界中的复杂时间依赖性。

Method: 提出基于LSTM的循环神经网络框架，从到达间隔时间序列中估计分数泊松过程的两个关键参数μ和β。利用LSTM建模时间依赖性，通过合成数据训练网络。

Result: 在合成数据上，相比传统矩估计方法减少约55.3%的均方误差。在真实世界高频数据集（蒙哥马利县紧急呼叫记录和AAPL股票交易数据）上，LSTM能有效追踪日模式和参数变化。

Conclusion: 基于LSTM的神经网络框架为分数泊松过程参数估计提供了有效方法，在合成和真实数据上均表现良好，特别适用于具有复杂时间依赖性的高频事件数据。

Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $μ>0$ and $β\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.

</details>


### [66] [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: 提出基于LMI框架的L-Lipschitz深度残差网络设计方法，通过LDL^T分解扩展至任意非线性架构，在121个UCI数据集上比SLL Layers提升3%-13%准确率。


<details>
  <summary>Details</summary>
Motivation: ResNets在计算机视觉任务中表现出色，但需要控制网络的Lipschitz常数以增强对抗鲁棒性和网络可验证性。现有方法在构建Lipschitz约束网络方面存在局限性。

Method: 将ResNet架构重新表述为循环三对角LMI，推导网络参数的闭式约束以确保L-Lipschitz连续性。采用新的LDL^T分解方法验证LMI可行性，将构造扩展到任意非线性架构。使用Cholesky分解进行高效参数化。

Result: 提出的LDL^T公式是SDP-based网络的紧松弛，保持完全表达能力，在121个UCI数据集上比SLL Layers获得3%-13%的准确率提升。

Conclusion: 该方法为构建Lipschitz约束的残差网络和其他分层架构提供了可证明的参数化方法，适用于对抗鲁棒性、认证训练和控制系统等应用。

Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.

</details>


### [67] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: KQ-SVD：一种直接对注意力矩阵进行最优低秩分解的新方法，通过闭式解实现，相比现有方法能更精确地保持注意力输出


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批处理大小的增长，KV缓存成为LLM推理的主要内存瓶颈。现有压缩方法通常只对键进行低秩分解或联合嵌入查询和键，但都忽略了注意力本质上依赖于它们的内积这一事实

Method: 提出KQ-SVD方法，直接对注意力矩阵进行最优低秩分解，通过闭式解实现。该方法针对冗余的真正来源，在压缩时能更精确地保持注意力输出

Result: 在LLaMA和Mistral模型上的广泛评估表明，KQ-SVD方法在投影质量方面始终优于现有方法

Conclusion: 通过直接对注意力矩阵进行低秩分解，KQ-SVD提供了一种简单且计算高效的方法，能更好地保持注意力输出，解决了KV缓存的内存瓶颈问题

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [68] [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931)
*Neil G. Marchant,Andrew C. Cullen,Feng Liu,Sarah M. Erfani*

Main category: cs.LG

TL;DR: 现有分歧差异代理损失存在贝叶斯不一致性问题，作者提出了一种新的理论框架和可证明一致的代理损失方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在现实世界部署中常因分布偏移而失败，分歧差异是解决这一问题的关键方法。然而，现有优化分歧差异的方法使用不可微的0-1损失，需要依赖代理损失，但这些代理损失存在贝叶斯不一致性问题。

Method: 提出新的理论框架，为代理损失的最优性差距提供上下界。基于此理论，设计了一种新的分歧损失，与交叉熵结合形成可证明一致的代理损失。

Result: 在多个基准测试上的实证评估表明，该方法比现有方法能更准确、更鲁棒地估计分歧差异，特别是在具有挑战性的对抗条件下。

Conclusion: 解决了现有分歧差异代理损失的贝叶斯不一致性问题，提出了理论保证和实用方法，为分布偏移下的模型鲁棒性提供了更可靠的评估工具。

Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.

</details>


### [69] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 该论文提出使用机器学习模型生成合成公共使用微观数据样本(PUMS)，以解决美国人口普查局商业调查数据中的隐私泄露风险，并通过实证研究验证了合成数据的质量。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力提升和大数据可用性增加，传统匿名化数据面临重新识别风险，可能违反调查对象的保密承诺。特别是商业调查数据，由于企业缺乏匿名性和某些行业在特定地理区域易于识别，开发公共使用的企业数据面临独特挑战。

Method: 使用机器学习模型构建基于年度商业调查(ABS)的合成PUMS，并开发了2007年企业主调查的两个合成PUMS版本。通过计量经济学复制在《小企业经济学》上发表的高影响力分析来验证合成数据的真实性。

Result: 合成数据成功保留了经验数据的关键特征，同时不包含任何真实个体或企业的记录。通过复制已发表研究的分析，证明了合成数据与真实数据的高度相似性，验证了合成数据的质量。

Conclusion: 合成PUMS为解决商业调查数据的隐私保护问题提供了可行方案，能够平衡数据可用性和保密性需求。该方法为ABS等商业调查数据的公共使用开辟了新的可能性，同时维护了调查对象的保密承诺。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>


### [70] [Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950)
*Zalish Mahmud,Anantaa Kotal,Aritran Piplai*

Main category: cs.LG

TL;DR: Impugan使用条件生成对抗网络(cGAN)进行缺失值填补和异构数据集整合，相比传统方法能更好地捕捉非线性多模态关系


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常存在缺失值，传统填补方法基于线性独立性假设，难以处理复杂异构数据，导致偏差或过度平滑

Method: 提出Impugan模型，基于cGAN框架，生成器从观测特征重建缺失值，判别器区分真实与填补数据，通过对抗训练学习非线性依赖关系

Result: 在基准数据集和多源整合任务中，Impugan相比基线方法降低82%的地球移动距离(EMD)和70%的互信息偏差(MI)

Conclusion: 对抗训练的生成模型为填补和整合不完整异构数据提供了可扩展且原理性的方法

Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025

</details>


### [71] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: MaxShapley是一种用于生成式搜索引擎的高效公平归因算法，基于可分解的max-sum效用函数，将计算复杂度从指数级降低到线性级，显著减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的生成式搜索引擎正在取代传统搜索，改变了信息提供者的补偿方式。为了维持这个生态系统，需要公平的机制来根据内容提供者对生成答案的贡献进行归因和补偿。

Method: MaxShapley算法是著名Shapley值的特例，利用可分解的max-sum效用函数，在检索增强生成(RAG)管道中计算归因，将计算复杂度从指数级降低到线性级。

Result: 在三个多跳QA数据集(HotPotQA、MuSiQUE、MS MARCO)上的评估显示，MaxShapley在归因质量上与精确Shapley计算相当，同时显著减少资源消耗。例如，在相同归因准确度下，比先前最先进方法减少高达8倍的资源消耗。

Conclusion: MaxShapley为生成式搜索引擎提供了一种高效、公平的内容归因机制，解决了传统Shapley值计算成本过高的问题，有助于建立可持续的生成式搜索生态系统。

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [72] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 本文提出使用α-散度来平衡RL训练LLM时的精度与多样性，避免传统RL导致的模式崩溃问题，在定理证明任务中实现了精度-覆盖率的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在微调LLM时会导致模型多样性显著下降，这是因为RL隐式优化了反向KL散度（模式寻求/零强制），使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（通过过滤错误答案但保留正确答案的相对概率），使用α-散度族来近似该目标分布。α-散度统一了先前方法，通过插值模式寻求和质量覆盖散度来直接控制精度-多样性权衡。

Result: 在Lean定理证明基准测试中，该方法在覆盖-精度帕累托前沿上实现了最先进的性能，在覆盖率轴上优于所有先前方法。

Conclusion: 通过显式使用α-散度来平衡精度与多样性，可以有效解决RL训练LLM时的模式崩溃问题，在需要多样性和覆盖率的任务中取得更好的性能。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>
