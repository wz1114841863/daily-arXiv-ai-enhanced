<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 63]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的性能，发现最佳检测器取决于训练集中的故障样本数量，并提供了工业部署的实用见解。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业系统（如质量控制和预测性维护）中具有应用潜力，但面临极端类别不平衡的挑战，主要由于训练期间故障数据有限。需要评估异常检测算法在真实工程约束下的性能。

Method: 使用问题无关的模拟数据集（2D和10D超球面异常分布），在异常率0.05%-20%、训练规模1,000-10,000（测试集40,000）的条件下，对14种检测器进行基准测试，评估性能和泛化误差。

Result: 最佳检测器高度依赖于训练集中的故障样本总数，额外健康样本在大多数情况下益处有限。故障样本少于20个时，无监督方法（kNN/LOF）占优；30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。半监督方法在10个特征时显示出优势。

Conclusion: 研究揭示了异常检测方法在小数据集上的泛化性能下降，为工业环境中部署异常检测提供了实用见解，强调了根据可用故障数据量选择适当检测器的重要性。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [2] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 该研究将Yahtzee游戏建模为MDP，使用多种策略梯度方法训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优性能的241.78分，但仍存在长期信用分配和探索挑战。


<details>
  <summary>Details</summary>
Motivation: Yahtzee作为具有随机组合结构和延迟奖励的经典骰子游戏，是一个有趣的中等规模强化学习基准。虽然单人Yahtzee的最优策略可以通过动态规划计算，但多人版本难以处理，需要近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程，使用REINFORCE、A2C和PPO等策略梯度方法训练自博弈智能体，采用具有共享主干的多头网络。对特征和动作编码、架构、回报估计器和熵正则化进行了消融研究。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感且未能达到接近最优性能，而A2C在各种设置下都能稳健训练。智能体在10万次评估游戏中获得中位数241.78分，接近最优DP分数254.59的5%以内，上区奖励和Yahtzee达成率分别为24.9%和34.1%。

Conclusion: A2C在Yahtzee游戏中表现最稳健，但所有模型都难以学习上区奖励策略，过度关注四骰组合，突显了长期信用分配和探索的持续挑战。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [3] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文提出一种针对LLM模型组合技术的攻击方法：通过设计单个"破坏令牌"，在捐赠模型中功能无害，但在移植到基础模型后能可靠重构为恶意特征，从而破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM生态系统越来越多地使用模型组合技术（如权重合并、推测解码、词汇表扩展），这些方法在不同模型家族间应用的关键前提是令牌移植。作者发现这一关键互操作性步骤引入了供应链漏洞，需要研究其安全风险。

Method: 通过利用系数重用的几何特性，将攻击形式化为双目标优化问题，使用稀疏求解器实例化攻击。攻击是无需训练的，通过谱模仿来规避异常检测，同时保持对微调和权重合并的结构持久性。

Result: 攻击成功创建了不对称可实现性差距，破坏了基础模型的生成能力，同时捐赠模型的效用与正常行为在统计上无法区分。攻击展示了在模块化AI组合流程中的隐藏风险。

Conclusion: 令牌移植这一关键互操作性步骤引入了供应链漏洞，单个"破坏令牌"就能在模型组合过程中创建持久性攻击，突显了模块化AI组合流程中的安全风险，需要更强的安全措施。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [4] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: IMBWatch是一个时空图神经网络框架，用于大规模检测非法按摩院，通过分析在线广告、商业记录和评论构建动态图，识别人口贩卖和剥削网络。


<details>
  <summary>Details</summary>
Motivation: 非法按摩院（IMBs）以合法按摩服务为掩护，从事人口贩卖、性剥削和强迫劳动。传统检测方法（如社区举报和监管检查）是反应式的，难以揭示更广泛的犯罪网络。由于数字广告编码、人员和地点频繁变更以及共享基础设施（如电话号码和地址）的重复使用，检测IMBs非常困难。

Method: IMBWatch是一个时空图神经网络（ST-GNN）框架，从开源情报（包括在线广告、商业许可证记录和众包评论）构建动态图。节点代表异构实体（如企业、别名、电话号码、位置），边捕获时空和关系模式（包括共址、重复电话使用和同步广告）。该框架结合图卷积操作和时间注意力机制，建模IMB网络在时间和空间上的演化，捕捉跨城市工人流动、一次性电话轮换和协调广告激增等模式。

Result: 在美国多个城市的真实数据集上的实验表明，IMBWatch优于基线模型，实现了更高的准确率和F1分数。除了性能提升外，IMBWatch还提供了更好的可解释性，为主动和有针对性的干预提供可操作的见解。该框架具有可扩展性，可适应其他非法领域，并发布了匿名数据和开源代码以支持可重复研究。

Conclusion: IMBWatch是一个有效的时空图神经网络框架，能够大规模检测非法按摩院网络，克服了传统方法的局限性，为打击人口贩卖和剥削提供了新的技术手段。该框架的可扩展性和可解释性使其适用于更广泛的非法活动检测领域。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [5] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出一种渐近置信度的最佳臂识别框架，通过放宽精确误差控制要求，在长时域下实现更紧的最优性和对非参数分布的更好处理


<details>
  <summary>Details</summary>
Motivation: 现有BAI方法在实际应用中存在局限：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致效率低下。现实场景常涉及弱信号、高显著性要求和实验后推断需求，这些都需要长时域

Method: 引入渐近框架，要求误差控制相对于最小样本量渐近有效；开发新颖的渐近任意时间有效置信序列，并基于此设计新的BAI算法；灵活整合协变量进行方差缩减，确保在完全非参数设置下的近似误差控制

Result: 在温和收敛假设下，提供了样本复杂度的渐近界，并证明最坏情况样本复杂度与已知方差下高斯BAI的最佳情况样本复杂度匹配；实验表明该方法在保持误差控制的同时减少了平均样本复杂度

Conclusion: 提出的渐近BAI框架通过放宽精确误差控制要求，在长时域场景中实现了更优的性能，能够更好地处理非参数分布并利用个体级上下文，为实际应用提供了更实用的解决方案

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [6] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO：通过贝叶斯优化自适应选择指令，解决LLM在方程发现中的指令脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在方程发现中表现出潜力，但输出对提示词高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解。

Method: 将提示工程重构为序列决策问题，维护离散推理策略库，使用贝叶斯优化基于数值反馈在每一步选择最优指令。

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简洁的解决方案。

Conclusion: 自适应指令选择能有效解决LLM在方程发现中的指令脆弱性问题，提升模型性能和解的质量。

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [7] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个几何强化学习框架，用于未知环境中的同时导航与建图，通过局部能量景观和哈密顿优化实现无需全局地图的高质量导航。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图的挑战性问题，传统方法需要构建全局地图或设计复杂的多智能体策略，而GRL-SNAM旨在仅依赖局部感知实现高效导航。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化：将感知输入转换为局部能量景观（编码可达性、障碍物屏障和变形约束），通过更新哈密顿量来演化感知、规划和重构策略。

Result: 在2D导航任务上评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM保持安全距离，泛化到未见过的布局，通过局部能量优化而非广泛全局建图实现高质量导航。

Conclusion: 基于哈密顿量更新的几何强化学习能够通过最小化探索实现高质量导航，证明了局部能量优化相比全局建图的优势，为未知环境导航提供了新思路。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [8] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 研究非马尔可夫状态和成本过程下线性函数近似的强化学习方法，包括策略评估和Q学习的收敛性分析，并应用于部分可观测马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法大多假设马尔可夫过程，但实际应用中状态和成本过程往往是非马尔可夫的。需要研究非马尔可夫环境下线性函数近似的收敛性和性能保证。

Method: 1. 对策略评估方法，在非马尔可夫过程的遍历性条件下证明算法收敛；2. 对Q学习，在基函数基于量化映射的特殊情况下证明收敛；3. 将结果应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示。

Result: 1. 策略评估算法在遍历性条件下收敛到正交投影和辅助马尔可夫决策过程贝尔曼算子联合算子的不动点；2. Q学习在特定基函数选择下可证明收敛；3. 对部分可观测马尔可夫决策过程推导了学习算法极限的显式误差界。

Conclusion: 该研究为非马尔可夫环境下线性函数近似的强化学习提供了理论保证，特别是在遍历性条件下证明了策略评估的收敛性，并识别了Q学习可收敛的特殊情况，为部分可观测系统的实际应用奠定了基础。

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [9] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大模型联邦定制化，分析了主要挑战，回顾了多种定制技术，讨论了它们在联邦学习框架下的实现，并通过实验验证了联邦前缀调优的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索大型模型在联邦学习框架下的定制化问题，解决在分布式、隐私保护环境下如何有效定制大模型的关键挑战。

Method: 回顾了多种大模型定制技术（全微调、高效微调、提示工程、前缀调优、知识蒸馏、检索增强生成），讨论了它们在联邦学习中的实现方式，并重点实验了联邦前缀调优方法。

Result: 联邦前缀调优实验验证了其可行性，性能接近集中式方法；与其他三种联邦定制方法相比，表现出竞争性性能、令人满意的效率和一致的鲁棒性。

Conclusion: 联邦前缀调优是首个在联邦学习环境中应用前缀调优的尝试，实验证明该方法在联邦定制大模型中具有可行性、竞争力和鲁棒性。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [10] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是关键预测因素，但模型对极端严重事故预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，训练XGBoost分类器，通过随机搜索交叉验证优化，采用类别加权处理类别不平衡问题。

Result: 模型整体准确率78%，对主要类别（严重程度2）的精确率和召回率达到87%。特征重要性分析显示时间、地理位置、能见度、温度和风速是最强预测因素，但降水和能见度预测能力有限。

Conclusion: 研究发现时间、地理位置和天气变量是事故严重程度的关键预测因素，但数据集以中等严重程度事故为主限制了极端案例的预测能力，需要改进采样策略、特征工程和外部数据整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [11] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种使用纯强化学习梯度在线微调决策变换器的新方法，解决了传统方法中后见回报重标注与重要性采样算法不兼容的问题，在多个基准测试中取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但在在线设置中使用纯强化学习梯度进行微调的研究较少。现有方法在在线微调时仍严重依赖监督序列建模目标，而后见回报重标注这一标准组件与基于重要性采样的强化学习算法（如GRPO）存在根本性不兼容，导致训练不稳定。

Method: 提出了几种新算法：1）将GRPO适配到决策变换器；2）引入子轨迹优化以改进信用分配；3）使用序列级似然目标增强稳定性和效率；4）采用主动采样鼓励在不确定区域的探索。

Result: 通过大量实验证明，该方法超越了现有的在线决策变换器基线，在多个基准测试中取得了新的最先进性能，展示了纯强化学习梯度在线微调决策变换器的有效性。

Conclusion: 本文成功实现了决策变换器的纯强化学习梯度在线微调，解决了后见回报重标注与重要性采样算法的不兼容问题，为决策变换器在在线强化学习中的应用开辟了新途径。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [12] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: Sequential Reservoir Computing通过将大储层分解为一系列小储层，在保持传统RC简单高效的同时，显著提升了高维时空系统的预测性能，实现了更长的预测时间、更低的误差和更低的训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中存在梯度训练和内存瓶颈问题，而传统Reservoir Computing虽然通过固定循环层和凸读出优化缓解了这些问题，但在输入维度增加时仍然存在扩展性问题。

Method: 提出Sequential Reservoir Computing架构，将大型储层分解为一系列小型互连储层，这种设计减少了内存和计算成本，同时保持了长期时间依赖性。

Result: 在低维混沌系统和高维物理模拟中，Sequential RC相比LSTM和标准RNN基线实现了15-25%更长的有效预测时间、20-30%更低的误差指标，以及高达三个数量级的训练成本降低。

Conclusion: Sequential RC在保持传统RC简单高效的同时，实现了对高维动力系统的卓越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [13] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 开发基于电子健康记录的机器学习模型，用于在肝硬化诊断前1-3年进行预测，性能显著优于传统FIB-4评分


<details>
  <summary>Details</summary>
Motivation: 需要更早、更准确地预测肝硬化风险，以支持预防性干预。传统FIB-4评分在早期预测方面性能有限，而电子健康记录数据提供了丰富的预测信息。

Method: 回顾性队列研究，使用大型学术医疗系统的去标识化电子健康记录数据。识别脂肪肝患者，根据ICD-9/10编码分为肝硬化和非肝硬化队列。构建预测场景，使用观察窗口和预测窗口模拟真实临床使用。从观察窗口汇总人口统计学、诊断、实验室结果、生命体征和共病指数。训练XGBoost模型用于1年、2年和3年预测时间范围，并在保留测试集上评估性能。

Result: XGBoost模型在1年、2年和3年预测中分别获得AUC为0.81、0.73和0.69，显著优于FIB-4的0.71、0.63和0.57。随着预测时间延长，性能优势更加明显，表明模型具有更好的早期风险区分能力。

Conclusion: 基于常规电子健康记录数据的机器学习模型在肝硬化早期预测方面显著优于传统FIB-4评分。这些模型能够实现更早、更准确的风险分层，可以作为自动化决策支持工具整合到临床工作流程中，支持主动的肝硬化预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [14] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下显著提升语义通信质量


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义保持的挑战，传统信道编码（如LDPC、Reed-Solomon）无法实现细粒度语义保护

Method: 采用强化学习框架，通过自适应重复编码实现按维度不等错误保护；使用复合语义失真度量，平衡全局嵌入相似性和实体级保持

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 简单但智能分配的重复编码可实现细粒度语义保护，代码结构需与语义粒度对齐，适用于边缘计算和物联网等带宽稀缺但语义保真度关键的场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [15] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN半监督学习框架，仅需1-3%标注数据即可实现蚊子神经元尖峰信号的高精度分类，用于检测寨卡病毒、登革热病毒感染，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒疾病的主要传播媒介，但手动分类神经元尖峰模式非常耗时耗力。现有深度学习解决方案需要完全标注的数据集和高度预处理的信号，难以在实际场景中大规模应用。

Method: 提出半监督Swin启发式GAN（SSI-GAN），采用基于变换器的生成器和Swin启发的移位窗口判别器，通过多头自注意力机制捕捉稀疏高频尖峰特征。仅使用1-3%标注数据，结合超过1500万个尖峰样本进行训练。

Result: SSI-GAN在感染后第三天仅用3%标注数据达到99.93%分类准确率，在感染各阶段仅需1%监督即保持高准确率。相比标准监督方法，在相同性能水平下减少97-99%人工标注工作量。

Conclusion: 提出的移位窗口变换器设计大幅超越所有基线方法，在基于尖峰的神经元感染分类中创下新纪录，为实际场景中的大规模应用提供了可行解决方案。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [16] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出一种资源高效的数据中心框架，通过特征工程使心律失常数据线性可分，实现超轻量级模型（8.54KB）和实时分类（0.46μs延迟），在MIT-BIH和INCART数据集上达到98.44%准确率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测。现有深度学习方法计算开销过大，不适合资源受限的边缘设备。

Method: 采用资源高效的数据中心框架，优先特征工程而非模型复杂度。整合时频小波分解和图论结构描述符（如PageRank中心性），构建混合特征空间，再通过互信息和递归消除进行特征选择，最终使用可解释的超轻量级线性分类器。

Result: 在MIT-BIH和INCART数据集上获得98.44%诊断准确率，模型大小仅8.54KB。分类推理延迟0.46μs，每搏处理管道52ms，确保实时操作。相比压缩模型KD-Light（25KB，96.32%准确率）实现数量级效率提升。

Conclusion: 该框架通过优化特征工程使复杂心律失常数据线性可分，实现了超轻量级、实时的心律失常检测系统，为无电池心脏传感器提供了可行解决方案。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [17] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 提出一种利用未标注网络数据增强生成模型归因的方法，通过约束优化提升对未见生成器的泛化能力


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真伪检测，准确识别特定生成模型来源。现有方法在未见或新发布生成器上泛化能力不足

Method: 使用CLIP特征和线性分类器建立基线，提出约束优化方法利用未标注网络数据，鼓励网络样本被分类为非目标模型，同时保持标注数据性能

Result: 实验表明，引入网络数据显著提升了在挑战性未见生成器上的归因性能，证明未标注数据能有效增强开放世界场景下的AI生成内容归因

Conclusion: 利用未标注网络数据可以有效解决生成模型归因中的泛化问题，为开放世界场景下的AI生成内容溯源提供了有效解决方案

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [18] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 提出对抗性图提示（AGP）框架，将对抗学习融入图提示中，实现鲁棒的图微调，解决现有参数高效微调方法对图拓扑和节点特征噪声的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在适应预训练GNN模型到下游任务时，对图拓扑和节点属性/特征的各种噪声和攻击表现出显著脆弱性，需要提高鲁棒性。

Method: 提出对抗性图提示（AGP）框架：1）将AGP建模为min-max优化问题，采用交替优化方案；2）内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声；3）外层最小化学习最优节点提示来对抗对抗噪声。

Result: AGP能够理论上处理图拓扑和节点噪声，验证了其多功能性和鲁棒性。实验证明AGP在多个基准任务上相比最先进方法具有更好的鲁棒性和有效性。

Conclusion: AGP是一个通用方法，可与各种预训练GNN模型集成，增强其在下游任务上的鲁棒性，为解决图微调中的噪声脆弱性问题提供了有效解决方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [19] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: GRIT是一种动态、曲率感知的LoRA改进方法，通过K-FAC预条件梯度、定期重投影到Fisher特征方向、自适应调整有效秩，在减少46%可训练参数的同时达到或超越LoRA/QLoRA性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA和QLoRA在固定、随机方向的低秩子空间中优化，主要使用一阶下降，忽略了局部损失曲率，这可能导致有效更新预算膨胀和沿弱约束方向的漂移放大。

Method: GRIT保持LoRA参数化但：1) 使用K-FAC作为自然梯度代理在秩空间中对梯度进行预条件处理；2) 定期将低秩基重投影到主导Fisher特征方向以抑制漂移；3) 根据谱自适应调整有效秩，使容量集中在信号所在位置。

Result: 在LLaMA骨干上的指令跟随、理解和推理基准测试中，GRIT匹配或超越了LoRA和QLoRA，同时平均减少46%的可训练参数（跨任务25-80%），在不同提示风格和数据混合下没有实际质量损失。

Conclusion: GRIT通过曲率感知的动态LoRA优化，显著减少了参数效率微调中的漂移问题，在更新与保留之间实现了更好的平衡，优于现有的PEFT优化器基线。

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [20] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: 宽L2正则化网络中监督学习本质上是压缩性的，其核秩受类别数限制，SGD噪声也是低秩的，这与自监督学习的高秩表示形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究宽L2正则化网络中特征学习的本质特性，揭示监督学习与自监督学习在表示结构上的根本差异。

Method: 提出特征学习理论，推导核ODE预测"水填充"谱演化，证明稳定稳态下核秩受类别数限制，分析SGD噪声的低秩特性。

Result: 监督学习的核秩受类别数C限制，SGD噪声也是O(C)低秩的，动态被限制在任务相关子空间，监督学习是低秩压缩的，而自监督学习是高秩扩张的。

Conclusion: 监督学习本质上是压缩性的，表示结构受任务类别数限制，这与自监督学习的高秩表示形成鲜明对比，统一了确定性和随机性视角下的对齐理论。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [21] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，获得比传统参数平均更准确的全局奖励估计。


<details>
  <summary>Details</summary>
Motivation: 在机器人和多智能体系统中，智能体群在相似但不同的环境中运行，追求共同的高级目标。由于动态差异、隐私约束和通信带宽限制，直接汇集数据学习共享奖励函数通常不切实际。

Method: 每个客户端首先在本地执行轻量级最大熵逆强化学习，遵守其计算和隐私限制。然后通过Wasserstein重心融合得到的奖励函数，考虑其底层几何结构。

Result: 证明这种重心融合比联邦学习中传统的参数平均方法产生更准确的全局奖励估计。提供了一种原则性和通信高效的框架。

Conclusion: 该工作为推导能在异构智能体和环境中泛化的共享奖励函数提供了原则性和通信高效的框架。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [22] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 提出基于国际象棋战术的量子基准测试QKRD，用于评估QAOA在结构化问题上的性能，发现约束保持混合器、预热策略等设计选择优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有QAOA基准测试主要使用随机实例（如MaxCut、TSP、SAT），这些实例缺乏语义结构和人类可解释性，难以反映真实世界问题的性能。需要结构化、可解释的基准来评估量子算法在实际问题上的表现。

Method: 引入量子王环支配（QKRD）基准，基于国际象棋战术位置构建，包含5000个结构化实例，具有one-hot约束、空间局部性和10-40量子比特规模。该基准结合人类可解释的覆盖度指标和针对经典启发式算法的内在验证。

Result: 使用QKRD系统评估QAOA设计选择：约束保持混合器（XY、domain-wall）比标准混合器收敛快约13步；预热策略减少45步收敛时间；CVaR优化表现更差。QAOA优于贪婪启发式算法12.6%，优于随机选择80.1%。

Conclusion: 结构化基准能揭示在随机实例中被掩盖的问题感知QAOA技术的优势。QKRD为可重复的NISQ算法研究提供了代码、数据和实验工件。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [23] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种无需姿态控制系统的纳米卫星电力故障检测方法，使用神经网络模拟正常状态，结合多种机器学习算法进行故障分类


<details>
  <summary>Details</summary>
Motivation: 纳米卫星在LEO轨道上面临多种电力系统故障风险（如光伏子系统线路故障、DC-DC转换器IGBT故障等），而传统需要姿态控制系统的故障检测方法成本高，因此需要开发无需ADCS的故障检测方案

Method: 首先基于神经网络建立无故障状态模型，输入为太阳辐射和太阳能板表面温度，输出为电流和负载；然后使用神经网络分类器结合PCA分类、决策树、KNN等多种机器学习方法，根据故障模式和类型进行故障诊断

Result: 成功开发了无需姿态控制系统的纳米卫星电力故障检测方法，能够有效识别光伏子系统的线路故障、DC-DC转换器故障和地面电池调节器故障等多种故障类型

Conclusion: 该方法为纳米卫星提供了一种成本效益高且可靠的电力故障检测方案，无需依赖复杂的姿态控制系统，具有实际应用价值

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [24] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 该论文提出结合光流和三种深度学习模型（S-CNN、预训练CNN特征提取器、H-ELM）进行无人机视频中人体检测的方法，在UCF-ARG数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识，对光照变化、相机抖动等动态事件敏感。需要更鲁棒、自动化的特征学习方法来解决无人机视频中的人体检测问题。

Method: 结合光流和三种深度学习模型：1) 监督卷积神经网络(S-CNN)配合softmax或SVM分类器；2) 预训练CNN特征提取器；3) 分层极限学习机(H-ELM)。在UCF-ARG无人机数据集上训练测试，涵盖五种人类动作。

Result: 预训练CNN平均准确率98.09%，S-CNN（softmax）95.6%，S-CNN（SVM）91.7%，H-ELM 95.9%。H-ELM在CPU上训练时间445秒，S-CNN在GPU上训练时间770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务中表现成功，预训练CNN效果最佳，H-ELM在计算效率上有优势，为动态环境下的视觉检测提供了有效解决方案。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [25] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出Deep Delta Learning (DDL)架构，通过可学习的几何变换调制恒等捷径连接，替代传统的严格加法残差连接，实现更复杂的状态转换建模。


<details>
  <summary>Details</summary>
Motivation: 深度残差网络虽然通过恒等捷径连接缓解了梯度消失问题，但这种严格的加法归纳偏置限制了网络建模复杂状态转换的能力。

Method: 引入Delta Operator，作为恒等矩阵的秩-1扰动，由反射方向向量k(X)和门控标量β(X)参数化，将残差更新重构为同步秩-1注入。

Result: 门控β(X)能够动态插值恒等映射、正交投影和几何反射，使网络能够显式控制层间转换算子的谱，建模复杂非单调动态。

Conclusion: DDL在保持门控残差架构稳定训练特性的同时，通过可学习的几何变换调制恒等捷径，增强了网络建模复杂状态转换的能力。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [26] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化来增强流匹配模型的人类偏好对齐，通过合并低熵步骤为高熵SDE采样步骤来解决多步去噪中的稀疏奖励信号问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化多步去噪时面临稀疏和模糊的奖励信号问题。观察到高熵步骤能实现更高效有效的探索，而低熵步骤导致无区别的roll-outs。

Method: 提出E-GRPO（熵感知组相对策略优化），合并连续低熵步骤形成单个高熵SDE采样步骤，对其他步骤应用ODE采样。引入多步组归一化优势，在共享相同合并SDE去噪步骤的样本中计算组相对优势。

Result: 在不同奖励设置下的实验结果证明了该方法的有效性。

Conclusion: 通过熵感知的SDE采样步骤合并和组相对优势计算，E-GRPO能有效解决流匹配模型中人类偏好对齐的奖励稀疏性问题。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [27] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 对16种可解释机器学习方法在216个表格数据集上进行大规模比较评估，发现性能具有层次结构且高度依赖数据特征，EBM在回归任务中表现最佳，SR和IGANN在非线性场景中表现突出，GOSDT对类别不平衡敏感。


<details>
  <summary>Details</summary>
Motivation: 机器学习在医疗、金融、法律等高风险领域的广泛应用引发了对模型可解释性和问责制的担忧，但当前对表格数据可解释模型的系统性评估相对缺乏，且主要关注聚合性能指标，需要更深入的研究。

Method: 对16种可解释方法（包括线性模型、决策树、EBM、符号回归、GOSDT等）在216个真实表格数据集上进行大规模比较评估，根据数据维度、样本量、线性度、类别不平衡等特征分层分析性能，并评估训练时间和分布偏移下的鲁棒性。

Result: 结果显示清晰的性能层次结构，特别是在回归任务中EBM始终表现出强大的预测准确性；性能高度依赖上下文：SR和IGANN在非线性场景中表现突出，GOSDT对类别不平衡敏感；不同方法在不同数据特征下表现各异。

Conclusion: 研究结果为寻求可解释性与预测性能平衡的实践者提供了实用指导，加深了对表格数据可解释建模的实证理解，强调了根据具体数据特征选择合适可解释方法的重要性。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [28] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）作为通用骨干网络用于异常检测，通过零样本推理、全模型适应和参数高效微调（PEFT）策略，在多个基准测试中优于任务特定基线，特别是在类别不平衡严重时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法大多需要大量任务特定训练，而时间序列基础模型（TSFMs）在大规模异构数据上预训练后，能否作为异常检测的通用骨干网络，实现更高效和可扩展的检测。

Method: 通过系统实验比较三种策略：1）零样本推理；2）全模型适应（完整微调）；3）参数高效微调（PEFT），包括LoRA、OFT和HRA等方法。在多个基准测试上评估TSFMs的性能。

Result: TSFMs在AUC-PR和VUS-PR指标上显著优于任务特定基线，特别是在类别不平衡严重的情况下表现更佳。PEFT方法不仅降低了计算成本，在大多数情况下还能匹配甚至超越完整微调的性能。

Conclusion: 时间序列基础模型（TSFMs）即使是为预测任务预训练的，也能高效适应异常检测任务，成为可扩展和高效的时间序列异常检测的通用模型，PEFT方法为实现这一目标提供了计算高效的途径。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [29] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型（CCBMs），支持概念-标签级、概念级和数据级三种粒度编辑，无需重新训练即可实现模型动态维护。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型（CBMs）主要关注静态场景，而实际应用中需要持续维护：删除错误/敏感数据（遗忘）、纠正错误标注概念、纳入新样本（增量学习）以适应动态环境。现有方法需要重新训练，在大规模应用中效率低下。

Method: 提出可控概念瓶颈模型（CCBMs），基于影响函数推导出数学上严格的闭式近似解，支持三种粒度编辑：概念-标签级（修正概念与标签关系）、概念级（修正概念本身）、数据级（数据删除和添加）。

Result: 实验结果表明CCBMs在编辑效率和适应性方面表现优异，验证了其在实现动态可信概念瓶颈模型方面的实用价值。

Conclusion: CCBMs通过数学严谨的闭式近似实现了无需重新训练的高效模型编辑，解决了传统CBMs在动态环境中的维护难题，为构建可编辑、可信赖的概念瓶颈模型提供了实用解决方案。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [30] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: TGE：基于轨迹级生成嵌入的离线观察模仿学习，通过扩散模型潜在空间估计专家状态密度，构建平滑的代理奖励函数，有效处理专家数据稀缺且离线数据与专家行为分布差异大的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决离线观察模仿学习中专家演示稀缺且离线次优数据与专家行为分布差异大的问题。现有分布匹配方法因严格的支撑约束和脆弱的单步模型而难以从非完美数据中提取有效信号。

Method: 提出TGE（轨迹级生成嵌入）方法：在离线轨迹数据上训练时序扩散模型，利用其潜在空间估计专家状态密度，构建密集平滑的代理奖励函数。通过扩散嵌入的平滑几何特性捕捉长期时序动态，弥合分布差异。

Result: 在D4RL运动和控制基准测试中，TGE方法一致匹配或优于现有离线观察模仿学习方法。

Conclusion: TGE通过轨迹级生成嵌入有效解决了专家数据稀缺且离线数据分布差异大的离线观察模仿学习问题，利用扩散模型的平滑潜在空间构建鲁棒的学习信号。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [31] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 该论文研究了残差网络中分层SGD如何高效学习层次化模型，这类模型超越了先前可学习模型的深度限制，并探讨了人类教师提供的层次结构如何促进高效学习。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习中层次化模型的可学习性，特别是残差网络如何通过分层SGD高效学习具有未知标签层次结构的复杂模型。作者认为理解这类层次化模型的学习机制可能成为理解深度学习成功的关键基础。

Method: 采用残差网络和分层随机梯度下降（layerwise SGD）算法，学习具有层次标签结构 $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$ 的模型。标签在层次中由简单到复杂：$L_1$ 中的标签是输入的简单函数，而 $L_i$（$i>1$）中的标签是更简单标签的简单函数。

Result: 证明了这类层次化模型可以被残差网络中的分层SGD高效学习，且该模型类达到了高效可学习性的深度极限。相比先前可被深度学习算法学习的模型（可用对数深度电路计算），该模型类包含需要多项式深度才能表达的模型。

Conclusion: 层次化模型的学习可能为理解深度学习提供基础。人类教师通过提供细粒度标签，实质上揭示了大脑内部算法的"提示"或"片段"，这种教学过程中自然出现的层次结构促进了高效学习。在教师部分了解自身内部逻辑的简化模型中，这种层次结构确实会出现并促进高效学习。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [32] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失无法有效提升MoE模型专家多样性，反而增加权重空间重叠，对性能影响不一致且不可靠


<details>
  <summary>Details</summary>
Motivation: 探索几何正则化在MoE模型专家专业化中的作用，特别是正交性损失是否能有效增强专家多样性

Method: 在MoE模型中应用正交性损失来强制专家多样性，在7个正则化强度下分析权重空间和激活空间的重叠情况

Result: 正交性损失在多方面失败：权重空间重叠反而增加114%，激活空间重叠保持高位(~0.6)，性能影响不一致且高度可变，权重与激活正交性无显著相关性(r=-0.293,p=0.523)

Conclusion: 权重空间正则化既未实现其几何目标，也未可靠提升性能，不适合用于MoE多样性增强

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [33] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 该研究开发了一种基于1D UNet的数据增强模型AugUNet1D，用于自动标记脑电图中的棘慢波放电，相比传统方法和现有算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 脑电图记录中手动标记事件（特别是棘慢波放电）耗时费力，尤其是在连续数周至数月的记录中。需要自动标记方法来减少人工工作量。

Method: 比较了14种机器学习分类器在961小时小鼠脑电图数据上的表现，发现1D UNet最佳。通过数据增强改进1D UNet（AugUNet1D），并与时间-频率算法"Twin Peaks"进行比较。

Result: 1D UNet在SWD标记任务中表现最佳，数据增强（特别是缩放）显著提升性能。AugUNet1D优于"Twin Peaks"算法，检测到的事件特征更接近人工标记。

Conclusion: AugUNet1D是自动标记脑电图棘慢波放电的有效工具，性能优于现有方法。模型已公开供其他研究者使用。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [34] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 提出一种结合图同质性和非线性奖励函数的多用户上下文赌博机框架，通过统一的多用户RKHS核函数融合图拉普拉斯与基础臂核，设计出理论有保障的探索算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多用户上下文赌博机时，要么假设线性奖励函数，要么忽略用户间的图结构关系。本文旨在同时解决非线性奖励函数和用户间图同质性问题，为结构化探索提供统一的理论框架。

Method: 提出一个原则性的联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一多用户RKHS中的平方范数，并显式推导其再生核，该核将图拉普拉斯与基础臂核优雅融合。基于此设计LK-GP-UCB和LK-GP-TS算法，利用高斯过程后验进行探索。

Result: 理论分析提供了高概率遗憾界，其缩放依赖于多用户核的有效维度，而非用户数量或环境维度。实验表明，在非线性设置中，该方法优于强线性基线和无图感知基线，即使在真实奖励为线性时也保持竞争力。

Conclusion: 本文提供了一个统一、理论有保障且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，有效处理了多用户非线性奖励函数和图同质性问题。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [35] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 论文比较了基于无自注意力Transformer架构的神经链与数值离散化方法，发现PINN学习和标准数值离散化获得相同动力学知识，但PINN使用随机矩阵导致参数多、训练成本高且缺乏物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习中无自注意力Transformer架构（神经链）与离散化神经积分/偏微分方程之间的类比关系，比较标准数值离散化和PINN学习方法在求解Burgers和Eikonal方程上的差异。

Method: 通过比较分析标准数值离散化（也表示为神经链形式）和PINN学习方法，对粘性和非粘性Burgers方程以及Eikonal方程进行数值求解，对比两种方法的矩阵结构和参数特性。

Result: 标准数值离散化和PINN学习提供了获取系统动力学知识的两种不同途径，但获得的知识本质相同。PINN使用随机矩阵，其可接受解的数量远多于有限差分法的唯一三对角矩阵形式，导致参数更多、训练成本更高且缺乏物理可解释性。

Conclusion: 对于一维动态问题，PINN相比标准数值方法没有优势，参数多、成本高且缺乏可解释性。但研究结果不排除PINN和机器学习在高维问题上可能提供更好的策略。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [36] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: 研究发现小型语言模型（7-9B参数）存在严重的"正确但推理错误"现象，50-69%的正确答案包含根本性推理缺陷，传统准确率指标无法检测。作者提出推理完整性评分（RIS）作为过程评估指标，发现RAG能改善推理完整性，而元认知干预反而损害小模型性能。


<details>
  <summary>Details</summary>
Motivation: 部署小型语言模型作为自主代理需要信任其推理过程，而不仅仅是输出结果。传统准确率指标无法检测模型"正确但推理错误"的现象，这构成了可靠性危机。

Method: 分析了10,734条推理轨迹，引入推理完整性评分（RIS）作为过程评估指标。研究了检索增强生成（RAG）和元认知干预对推理完整性的影响，并通过机制分析理解其作用原理。最后训练了神经网络分类器来验证推理完整性。

Result: 发现50-69%的正确答案包含根本性推理缺陷。RAG显著改善推理完整性（Cohen's d=0.23-0.93），而元认知干预在小模型上反而损害性能（d=-0.14到-0.33）。RAG通过外部证据减少7.6%的错误，元认知则因模型容量不足而增加混淆。验证分类器达到0.86 F1分数和100倍加速。

Conclusion: 仅依赖准确率指标对于可信代理是危险的，因为模型可能基于完全错误的推理得出正确答案。必须采用基于过程的验证方法，如RIS，来确保小型语言模型作为自主代理的可靠性。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [37] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一种用于检测LLM智能体多步行动计划异常的Siamese循环自编码器方法，通过对比学习和重构的混合损失函数，能同时检测任务轨迹对齐和序列结构异常，比现有方法性能更好、速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适合LLM智能体生成的多步行动计划检测：均值池化嵌入会稀释异常步骤，而纯对比学习方法忽略序列结构。标准无监督方法在预训练嵌入上F1分数不超过0.69，无法有效检测"错误计划"和"畸形计划结构"。

Method: 提出Trajectory Guard，一种Siamese循环自编码器，采用混合损失函数：通过对比学习学习任务-轨迹对齐，通过重构学习序列有效性。这种双重目标能统一检测"错误的任务计划"和"畸形计划结构"。

Result: 在合成扰动和真实世界失败（安全审计RAS-Eval和多智能体系统Who&When）的基准测试中，在平衡数据集上F1分数达到0.88-0.94，在不平衡外部基准测试中召回率达到0.86-0.92。推理延迟32ms，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能有效检测LLM智能体行动计划的异常，兼顾任务对齐和序列结构，性能优于现有方法且速度快，适合生产部署中的实时安全验证。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [38] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: 提出SAFN框架，通过稀疏注意力机制融合多模态数据，用于帕金森病分类，在PPMI数据集上达到98%准确率，具有临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病具有异质性表现，需要整合生物和临床标志物。现有多模态模型存在可解释性差、类别不平衡、高维特征融合困难等问题。

Method: 提出SAFN框架：使用模态特定编码器处理MRI皮层厚度、体积测量、临床评估和人口统计学数据；采用对称交叉注意力机制捕捉非线性交互；稀疏约束注意力门控层动态选择信息模态；使用类别平衡焦点损失处理数据不平衡。

Result: 在PPMI数据集703名参与者（570名PD，133名健康对照）上，通过五折交叉验证，SAFN达到0.98±0.02的准确率和1.00±0.00的PR-AUC，优于现有基线方法。可解释性分析显示约60%预测权重分配给临床评估，符合运动障碍学会诊断原则。

Conclusion: SAFN为神经退行性疾病的计算分析提供了一个可重复、透明的多模态建模范式，具有临床可解释性和鲁棒性。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [39] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: LSTM模型压缩研究：通过减少隐藏单元从128到16，发现64单元模型在保持精度的同时显著减小模型大小，实现73%压缩和47%精度提升


<details>
  <summary>Details</summary>
Motivation: 标准LSTM神经网络在零售行业销售预测中准确但计算资源需求大，对中小型零售企业构成挑战，需要探索模型压缩方法

Method: 逐步减少LSTM隐藏单元数量（从128到16），使用Kaggle Store Item Demand Forecasting数据集（91.3万条日销售记录），分析模型大小与预测精度的权衡关系

Result: 64隐藏单元模型在保持相同精度水平的同时，MAPE从128单元的23.6%降至12.4%，模型大小减少73%（280KB到76KB），精度提升47%

Conclusion: 更大的模型并不总是获得更好结果，通过适当的模型压缩可以在显著减小模型大小的同时提高预测精度，为中小型零售企业提供可行的LSTM部署方案

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [40] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 基于扩散模型的云原生架构，可自动生成店铺专用货架图，将设计时间从30小时减少到0.5小时（减少98.3%），同时实现94.4%的约束满足率。


<details>
  <summary>Details</summary>
Motivation: 传统货架图创建过程耗时且复杂，平均每个复杂布局需要30小时。零售业需要更高效的自动化解决方案来优化货架空间布局。

Method: 采用云原生架构，结合AWS云训练和边缘部署实现实时推理。使用扩散模型学习多个零售点成功货架布局，通过改进的损失函数集成零售特定约束。

Result: 系统将货架图设计时间减少98.3%（30小时→0.5小时），约束满足率达94.4%。经济分析显示创建费用减少97.5%，投资回收期4.4个月。架构可线性扩展，支持10,000个并发店铺请求。

Conclusion: 该研究证明了生成式AI在自动化零售空间优化中的可行性，通过云原生扩散模型架构实现了高效、可扩展的货架图自动生成。

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [41] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 基于非平衡随机动力学的熵触发重训练框架，通过Fokker-Planck方程建模数据漂移，利用KL散度的时间导数进行熵平衡分解，实现比传统方法更高效的重训练策略。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在非平稳环境中部署时，由于数据漂移会导致性能下降。现有的漂移检测方法大多缺乏原理性的动力学解释，并且无法在重训练频率和运营成本之间提供有效指导。

Method: 提出基于熵的重训练框架，将部署时的数据漂移建模为受Fokker-Planck方程控制的概率流，使用时变Kullback-Leibler散度量化模型-数据不匹配。该不匹配的时间导数具有熵平衡分解，包含由概率流驱动的非负熵产生项。

Result: 在受控的非平稳分类实验中，熵触发重训练实现了与高频重训练相当的预测性能，同时相对于每日重训练和基于标签的策略，将重训练事件减少了一个数量级。

Conclusion: 熵触发重训练作为一种无标签干预策略，能够响应累积的不匹配而非延迟的性能崩溃，为机器学习模型在非平稳环境中的部署提供了更高效的重训练方法。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [42] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 该论文提出需要区分两种对抗性弱点：利用非鲁棒特征的攻击和不利用这些特征的攻击，并提出了基于集成的方法来测量对抗扰动对非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然被广泛接受，但忽略了那些不直接利用这些特征的对抗样本。作者认为这两种样本代表了不同类型的对抗性弱点，需要在评估对抗鲁棒性时加以区分。

Method: 提出了基于集成的度量方法，用于测量对抗扰动对非鲁棒特征的操纵程度，并用该度量分析攻击者生成的对抗样本的构成。

Result: 通过新度量方法能够分析对抗样本的构成，区分不同类型的对抗性弱点，并为重新审视多个现象提供了新视角。

Conclusion: 对抗性弱点应分为利用非鲁棒特征和不利用非鲁棒特征两种类型，新的度量方法有助于更精确地评估对抗鲁棒性，并为理解相关现象提供了新框架。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [43] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: 提出自定义损失函数(CLF)来减少训练结果对随机因素的敏感性，提高深度学习模型的稳定性和可靠性


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法缺乏确保跨运行一致性和鲁棒性的机制，即使在受控条件下模型准确率也存在显著波动，这影响了模型的可靠性和可复现性

Method: 提出自定义损失函数(CLF)，通过调整其参数来平衡预测准确率和训练稳定性，减少对权重初始化和数据洗牌等随机因素的敏感性

Result: 在图像分类和时间序列预测的多种架构上进行广泛实验，证明CLF能显著提高训练鲁棒性而不牺牲预测性能

Conclusion: CLF是开发更稳定、可靠和可信赖神经网络的有效高效策略

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [44] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE：一种基于MoE的异构联邦学习框架，通过专家重要性评估和自适应选择，为资源受限客户端定制专家子集，实现高效LLM微调


<details>
  <summary>Details</summary>
Motivation: 虽然MoE模型通过稀疏激活专家减少计算负担，但在FL微调中面临三个关键挑战：1）缺乏可靠指标评估专家对本地性能的影响；2）客户端异构计算资源限制；3）客户端特定专家子集和路由偏好破坏全局聚合

Method: 提出HFedMoE框架：1）基于专家对微调性能的贡献评估其重要性；2）从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3）设计稀疏感知模型聚合策略，加权聚合活跃专家和门控参数

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于现有最先进基准方法

Conclusion: HFedMoE有效解决了MoE在联邦学习微调中的挑战，通过定制化专家选择和智能聚合，实现了资源受限环境下LLM的高效微调

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [45] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习预测骑行时长，结合路线拓扑特征和运动员当前体能状态，相比传统物理模型更实用


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时长预测方案需要大量参数（如空气阻力系数、实时风速预报），这对业余骑行者不实用。需要一种更实用的方法来预测骑行时长，用于训练规划和赛事准备。

Method: 采用机器学习方法，使用路线拓扑特征结合运动员当前体能状态（从训练负荷指标推导）。模型从历史数据中学习运动员特定的表现模式，用历史表现代理替代复杂的物理测量。在单人数据集（N=96次骑行）上采用N-of-1研究设计，通过严格的特征工程消除数据泄漏，使用Lasso回归模型。

Result: Lasso回归模型（拓扑+体能特征）达到MAE=6.60分钟和R2=0.922。整合体能指标（CTL, ATL）相比仅使用拓扑特征，误差减少了14%（MAE从7.66分钟降至6.60分钟）。渐进检查点预测支持动态比赛规划。

Conclusion: 机器学习方法能够有效预测骑行时长，结合路线拓扑和运动员体能状态比传统物理模型更实用。生理状态即使在自定节奏的努力中也对表现有显著约束作用。该方法为业余骑行者提供了实用的训练和赛事规划工具。

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [46] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 提出基于图神经网络和强化学习的交通感知出租车热点预测框架，在模拟德里数据集上减少56%乘客等待时间和38%行驶距离


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共事件等动态因素对出租车供需匹配的影响，无法实现实时优化的出租车调度

Method: 将城市道路网络建模为图结构（节点为交叉口，边为路段），利用GNN编码时空依赖关系，结合Q-learning智能体推荐最优出租车热点位置，奖励机制联合优化乘客等待时间、司机行驶距离和拥堵避免

Result: 在基于真实地理边界和历史叫车请求模式生成的模拟德里出租车数据集上，相比随机选择基线，模型减少了约56%的乘客等待时间和38%的行驶距离

Conclusion: 该交通感知的图强化学习框架能有效优化城市出租车调度，可适应多模式交通系统并集成到智慧城市平台中实现实时城市移动性优化

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [47] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一个用于在向下封闭凸体上最大化非负、非单调γ-弱DR-子模函数的近似算法，其保证随γ平滑变化，在γ=1时恢复0.401近似比，在γ<1时性能优雅下降并优于现有结果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，最大化约束下的子模目标函数是一个基本问题。现有研究主要关注单调DR-子模函数，但对于非单调且弱DR-子模函数（γ<1）的情况，缺乏有效的近似算法。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，提出了一种简单而有效的处理非单调性的方法。

Result: 算法在γ=1时达到0.401近似比，在γ<1时性能优雅下降，并且在相同约束下改进了先前报告的γ-弱DR-子模最大化边界。

Conclusion: 该研究为在向下封闭凸体上最大化非单调γ-弱DR-子模函数提供了最先进的保证，通过平滑依赖γ的设计实现了优雅的性能退化。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [48] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个轻量级基准测试，用于量化LLM在简洁理想提示上的过度生成问题，通过测量超出基准答案的额外长度来评估模型的冗余回答倾向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM（如ChatGPT、Claude、Gemini）作为通用助手时，经常对简单请求给出不必要的冗长回答，包含冗余解释、模棱两可的表达或模板化内容，这增加了认知负担和基于token的推理成本。先前研究表明基于偏好的后训练和LLM评估可能导致系统性长度偏差，即使质量相当，更长的回答也更容易获得奖励。

Method: 提出YapBench基准测试，包含300多个英文提示，涵盖三种简洁理想场景：(A) 最小或模糊输入，理想行为是简短澄清；(B) 封闭式事实问题，有简短稳定答案；(C) 单行编码任务，单个命令或代码片段即可。主要指标YapScore测量超出基准答案的字符数，不依赖特定分词器；YapIndex是类别级别中位数YapScore的均匀加权平均值。

Result: 评估76个助手型LLM，发现中位数额外长度存在数量级差异，并观察到特定类别的失败模式：在模糊输入上的"真空填充"行为，以及在单行技术请求上的解释或格式化开销。

Conclusion: YapBench提供了一个量化LLM过度生成问题的标准化方法，揭示了不同模型在简洁回答能力上的显著差异，并发布了基准测试和实时排行榜来跟踪模型冗长行为随时间的变化。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [49] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG度量特征重要性，并引入最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法缺乏结构化领域知识整合，且特征重要性度量存在分布外问题，需要一种能有效结合领域知识并保证模型性能的优化框架。

Method: 提出IGBO双目标优化框架：1) 将特征重要性层次编码为有向无环图；2) 使用时序积分梯度度量特征重要性；3) 引入最优路径预言机学习数据流形感知的积分路径解决OOD问题。

Result: 理论分析证明了收敛性和对mini-batch噪声的鲁棒性；在时间序列数据上的实验表明，IGBO能有效实施DAG约束且精度损失最小，优于标准正则化基线方法。

Conclusion: IGBO成功整合了结构化领域知识到模型训练中，通过双目标优化平衡了可解释性和准确性，为解决可解释AI中的领域知识整合问题提供了有效框架。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [50] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Avatar Forcing框架，通过扩散强迫实现实时交互式头部虚拟人生成，解决实时运动生成和无标签学习表达性反应两大挑战


<details>
  <summary>Details</summary>
Motivation: 当前虚拟人生成模型缺乏真正的交互感，无法实现实时情感互动。主要挑战在于：1) 因果约束下的实时运动生成；2) 无需额外标注数据学习表达性反应

Method: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-虚拟人交互，处理多模态输入（音频和动作）。引入直接偏好优化方法，利用丢弃用户条件构建的合成负样本，实现无标签的表达性交互学习

Result: 框架实现低延迟实时交互（约500ms），相比基线加速6.8倍。生成的虚拟人运动反应性和表达性更强，在80%的对比中优于基线

Conclusion: Avatar Forcing框架成功解决了交互式虚拟人生成的关键挑战，实现了实时、表达性强的虚拟人交互，为虚拟通信和内容创作提供了更自然的交互体验

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [51] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: IRPO提出了一种新的强化学习框架，通过将Bradley-Terry模型融入GRPO，用点式评分替代成对比较，解决了生成奖励模型在RL中的计算瓶颈问题，在保持可解释性的同时显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 生成奖励模型（GRMs）因其可解释性、推理时扩展性和通过强化学习优化的潜力而受到关注，但广泛使用的成对GRMs在与GRPO等RL算法集成时存在计算瓶颈：1）获取相对分数的成对比较需要O(n²)时间复杂度；2）重复采样或额外思维链推理带来的计算开销。

Method: 提出Intergroup Relative Preference Optimization（IRPO）框架，将Bradley-Terry模型融入Group Relative Policy Optimization（GRPO），为每个响应生成点式评分，从而在RL训练中高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRMs中的最先进性能，与当前领先的成对GRMs性能相当。更重要的是，在后训练评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过点式评分方法有效解决了成对GRMs的计算瓶颈问题，在保持可解释性和细粒度奖励信号的同时，显著提升了强化学习训练的效率，为生成奖励模型的实际应用提供了更可行的解决方案。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [52] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR是一个面向电信领域的端到端故障排除系统，通过集成领域特定排序和生成模型，自动化处理工单分类、历史工单检索和故障分析报告生成，显著提升电信故障排除的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信领域的故障排除任务非常耗时，需要专家解读工单内容、查阅文档和搜索历史记录，这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。需要自动化系统来提升故障排除的效果和效率。

Method: 提出TeleDoCTR系统，集成领域特定排序和生成模型，自动化故障排除工作流的关键步骤：1）将工单路由到适当的专家团队（分类任务）；2）检索上下文和语义相似的历史工单（检索任务）；3）生成详细的故障分析报告，包括问题、根本原因和潜在解决方案（生成任务）。

Result: 在真实世界的电信基础设施数据集上评估TeleDoCTR，证明其性能优于现有最先进方法，显著提升了故障排除过程的准确性和效率。

Conclusion: TeleDoCTR是一个有效的电信领域特定故障排除系统，通过自动化关键工作流步骤，能够显著改善电信工单处理的效率和效果。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [53] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE：轻量级框架，通过群体探索层增强策略梯度方法，在非平稳奖励和高维策略任务中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂任务中探索不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上增加紧凑的群体探索层。它将策略动作与粒子驱动建议相结合，每个粒子代表动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应调节探索。

Result: 在简单基准上仅有轻微改进（如CartPole-v1 +0.7%），但在挑战性任务中表现显著：LunarLander-v3 +46%，Hopper-v4 +22%，同时在Walker2d和Ant上保持稳定。在非平稳奖励变化下，ARISE提供明显鲁棒性优势，在CartPole上比PPO高出75分。

Conclusion: ARISE提供了一个简单、架构无关的途径，可以在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的RL智能体。消融研究证实群体组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [54] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出贝叶斯逆博弈框架，通过变分自编码器和可微分纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，相比最大似然估计能更好量化不确定性并提升下游决策安全性。


<details>
  <summary>Details</summary>
Motivation: 多智能体交互通常建模为非合作博弈，但实际部署需要已知所有智能体目标。现有最大似然逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划可能过度自信地采取不安全行动。

Method: 提出近似贝叶斯推理方法：训练结构化变分自编码器，嵌入可微分纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，无需真实目标标签，支持多模态观测数据融合。

Result: 框架成功学习先验和后验分布，相比最大似然估计方法提升推理质量，在保持效率的同时实现更安全的下游决策。当轨迹信息不充分时，多模态推理能进一步降低不确定性。

Conclusion: 贝叶斯逆博弈框架能有效量化智能体目标估计的不确定性，通过多模态观测减少不确定性，为自主决策提供更安全可靠的逆博弈解决方案。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [55] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: BSAT：基于B样条的自适应分词器，用于长时序预测，通过高曲率区域自适应分段，结合混合位置编码L-RoPE，实现高压缩率下的强性能


<details>
  <summary>Details</summary>
Motivation: Transformer在长时序预测中存在自注意力二次复杂度和均匀分块与数据语义结构不匹配的问题，需要更高效的自适应方法

Method: 提出B样条自适应分词器(BSAT)，通过B样条拟合时序数据，在高曲率区域自适应放置token；结合混合位置编码L-RoPE（可学习位置编码+层间可学习基的旋转位置嵌入）

Result: 在多个公开基准测试中表现出竞争力，在高压缩率下保持强性能，特别适合内存受限场景

Conclusion: BSAT提供了一种参数免费的自适应时序分词方法，结合L-RoPE混合编码，在保持性能的同时显著降低内存需求，适用于资源受限的长时序预测任务

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [56] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文多臂老虎机问题动态选择计算步骤的精度配置，平衡精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 科学计算中混合精度方法需要手动调优精度配置，缺乏自动化方法。现有工作未充分利用强化学习进行精度自动调优，特别是在未见数据集上的验证。

Method: 将问题建模为上下文多臂老虎机，使用离散化状态空间和增量动作值估计的Q表方法。特征包括近似条件数和矩阵范数等，通过epsilon-greedy策略优化多目标奖励函数（平衡精度和计算成本）。在线性系统迭代求精中应用该框架。

Result: 实验结果表明，框架能有效选择精度配置，在保持与双精度基线相当精度的同时显著降低计算成本。框架能泛化到未见数据集，验证了RL精度选择的可行性。

Conclusion: 这是首个基于强化学习的精度自动调优工作，并在未见数据集上验证。框架可推广到其他数值算法，推动了科学计算中混合精度方法的发展。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [57] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: STAC算法通过引入分布式critic网络建模时序回报的不确定性，利用时间性随机不确定性而非认知不确定性来缩放悲观偏差，解决了critic网络高估问题，同时通过dropout正则化提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 离策略actor-critic方法虽然样本效率高，但critic网络存在系统性高估问题。现有方法使用集成学习来量化认知不确定性以引入悲观偏差，但这种方法计算效率较低。

Method: 提出STAC算法：1）使用单个分布式critic网络建模时序回报的不确定性（包括随机转移、奖励和策略引起的变异性）；2）基于时间性随机不确定性而非认知不确定性来缩放TD更新中的悲观偏差；3）对critic和actor网络应用dropout进行正则化。

Result: 实验表明：1）仅基于分布式critic的悲观偏差足以缓解高估问题；2）在随机环境中自然产生风险规避行为；3）dropout进一步提高了训练稳定性和性能；4）使用单个分布式critic网络提升了计算效率。

Conclusion: STAC通过利用时间性随机不确定性而非认知不确定性来缩放悲观偏差，有效解决了critic高估问题，同时通过dropout正则化和单个分布式critic设计，在保持性能的同时提升了计算效率。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [58] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文提出Distributional Creative Reasoning (DCR)框架，分析当前LLM推理管道中基于正确性的训练导致推理路径分布崩溃的问题，并提供防止崩溃的理论和实用方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理管道主要依赖自举推理循环，通过采样多样化的思维链并强化得分最高的路径，主要优化正确性。这种设计选择会导致模型在推理路径上的分布崩溃，降低语义熵并削弱创造性问题解决能力。

Method: 引入Distributional Creative Reasoning (DCR)框架，将训练视为通过解决方案轨迹的概率测度的梯度流。该统一变分目标将STaR、GRPO、DPO以及熵奖励等方法都视为其特例。

Result: 框架提供三个核心成果：(1)多样性衰减定理，描述基于正确性的目标如何导致STaR、GRPO和DPO的不同多样性衰减模式；(2)确保收敛到稳定且多样化策略的设计，有效防止崩溃；(3)实现这一目标的简单实用方法。

Conclusion: DCR为LLM提供了第一个既保持正确性又保持创造性的原则性方法，解决了当前推理管道中分布崩溃的问题。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [59] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的角球防守分析方法，通过球员追踪数据推断盯人和区域防守分配，实现无标签的防守表现评估和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调移动，现有反事实方法(如ghosting模型)依赖"平均"行为模拟而缺乏战术上下文，特别是在角球这种高度结构化的场景中。

Method: 针对角球场景开发协变量依赖隐马尔可夫模型(CDHMM)，从球员追踪数据中无监督推断时间分辨的盯人和区域防守分配，并基于此提出防守贡献归因框架和角色条件ghosting反事实分析方法。

Result: 模型能够从追踪数据中准确推断防守分配，提出的框架提供了可解释的防守贡献评估，相比传统方法能更好地结合战术上下文进行反事实分析。

Conclusion: CDHMM方法为角球防守提供了有效的无标签分析框架，通过角色条件反事实分析实现了对无球防守表现的上下文感知评估，为足球防守分析开辟了新途径。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [60] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，结合在线重置机制防止码本崩溃，使用KV-LoRA高效利用压缩记忆，将记忆库大小减少到最强基线的0.3%同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，持续学习需要更新模型而不遗忘旧知识。现有记忆增强方法面临记忆库随数据流不断增长的严重问题，需要更高效的记忆压缩方案。

Method: 提出MBC模型：1) 通过码本优化策略在线压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在注意力层使用Key-Value低秩适配(KV-LoRA)高效利用压缩记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过码本压缩和在线重置机制有效解决了记忆库增长问题，实现了高效的持续学习，代码已开源。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [61] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 提出一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化问题，相比传统方法具有更好的优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统分类变量梯度优化方法存在局限性：基于分数函数的估计器虽然无偏但噪声大，而连续松弛方法虽然能获得路径梯度但优化的是有偏的温度依赖目标。需要一种更好的方法来处理分类分布的优化问题。

Method: 提出扩散基软重参数化方法，利用高斯噪声过程下的去噪器具有闭式解且可高效计算的特性，构建无需训练的扩散采样器，通过该采样器进行反向传播。

Result: 在各种基准测试中，提出的重参数化技巧展现出具有竞争力或改进的优化性能。

Conclusion: 扩散基软重参数化为分类分布提供了一种有效的优化方法，扩展了连续松弛方法家族，在保持路径梯度的同时改善了优化效果。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [62] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: FedHypeVAE：一个基于超网络的差分隐私联邦数据合成框架，通过条件VAE架构和客户端感知解码器解决非IID数据下的嵌入级数据生成问题，同时提供形式化隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦数据共享方法在非IID客户端异构性下表现不佳，且对梯度泄漏的正式保护有限。需要一种能在生成器层面统一个性化、隐私保护和分布对齐的解决方案。

Method: 基于条件VAE架构，用超网络生成的客户端感知解码器和类条件先验替代单一全局解码器；采用差分隐私优化超网络，结合局部MMD对齐和Lipschitz正则化增强稳定性；使用中性元代码实现领域无关合成。

Result: FedHypeVAE在生成器层面实现了个性化、隐私保护和分布对齐的统一，为联邦设置下的隐私保护数据合成建立了原则性基础，支持可控的多领域覆盖。

Conclusion: 该框架为联邦环境中的隐私保护数据合成提供了系统化解决方案，通过超网络驱动的个性化生成和差分隐私保护，有效应对非IID数据挑战，同时保持数据效用。

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


### [63] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过分析注意力矩阵的谱特征来检测大语言模型中的有效数学推理，在多个模型上达到85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏无需训练数据、微调或学习分类器的方法来检测大语言模型中的有效数学推理，需要一种能够直接分析模型内部注意力模式来验证推理有效性的方法。

Method: 将注意力矩阵视为动态图的邻接矩阵，提取四个可解释的谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，通过单阈值分类来区分有效和无效数学证明。

Result: 在七个来自四个不同架构家族的Transformer模型上实验，效应量高达Cohen's d=3.30，分类准确率达到85.0-95.6%，校准阈值在完整数据集上达到93-95%。发现该方法检测的是逻辑一致性而非编译器接受度。

Conclusion: 谱图分析为推理验证提供了原则性框架，具有即时应用于幻觉检测和AI安全监控的潜力，同时揭示了注意力机制设计影响哪些谱特征捕获推理有效性。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [64] [Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation](https://arxiv.org/abs/2601.00450)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: 提出REAP-cache方案，通过消除STT-MRAM缓存中读取干扰错误的累积，显著提升可靠性，同时保持性能基本不变


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为SRAM替代品具有高密度、低功耗等优势，但其可靠性受读取干扰错误威胁。传统ECC方案在并行读取缓存块时，未请求的块未进行ECC检查，导致读取干扰错误累积，严重降低缓存可靠性

Method: 首先形式化描述读取干扰累积现象，然后提出REAP-cache方案，完全消除读取干扰累积而不影响缓存性能

Result: REAP-cache将缓存平均故障时间(MTTF)提升171倍，缓存面积增加小于1%，能耗仅增加2.7%

Conclusion: REAP-cache是一种简单有效的解决方案，能显著提升STT-MRAM缓存的可靠性，同时保持面积和能耗开销极小

Abstract: Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.

</details>


### [65] [ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches](https://arxiv.org/abs/2601.00456)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: ROBIN是一种针对STT-MRAM数据依赖错误模式的高效纠错码配置，相比传统ECC显著提升纠错能力


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为片上缓存存储器有前景，但高错误率是主要限制因素。传统纠错码因数据依赖错误模式而效率低下

Method: 首先对数据依赖错误模式进行全面分析，然后提出名为ROBIN的高效纠错码配置方案

Result: 传统ECC使缓存错误率平均增加151.7%，而ROBIN将此值降低超过28.6倍

Conclusion: ROBIN能有效解决STT-MRAM的数据依赖错误问题，显著提升纠错能力，适用于片上缓存存储器

Abstract: Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [66] [On the Error Floor Evaluation of NOMA-Irregular Repetition Slotted ALOHA](https://arxiv.org/abs/2601.00317)
*Estefanía Recayte*

Main category: cs.ET

TL;DR: 提出了一种针对NOMA-IRSA方案在错误平层区域包丢失率的简单而紧致的解析近似方法


<details>
  <summary>Details</summary>
Motivation: 在物联网场景中，需要分析非正交多址接入(NOMA)与不规则重复时隙ALOHA(IRSA)结合方案的包丢失率，特别是在有限长度区域和错误平层区域

Method: 用户随机选择基于设计的度分布的副本数量和预定功率级别，接收端执行连续干扰消除(SIC)，推导出有限长度区域的包丢失率解析表达式

Result: 推导的包丢失率表达式能够快速评估，通过蒙特卡洛仿真验证了其准确性，在不同信道负载下（包括超出低负载区域）都表现出良好匹配

Conclusion: 该方法为NOMA-IRSA方案提供了准确且易于计算的包丢失率分析工具，特别适用于物联网场景的性能评估

Abstract: In this work, we provide a simple yet tight analytical approximation of the packet loss rate in the error floor region for a non-orthogonal multiple access (NOMA)-based irregular repetition slotted ALOHA (IRSA) scheme. Considering an Internet of Things (IoT) scenario, users randomly select both the number of replicas based on a designed degree distribution and the transmission power from predetermined levels, while successive interference cancellation (SIC) is performed at the receiver. Our derived packet loss rate expression in the finite length regime is promptly evaluated. Its accuracy is validated through Monte-Carlo simulations, demonstrating a strong match across channel loads, including those beyond the low load regime

</details>


### [67] [Two-Step Interference Cancellation for Energy Saving in Irregular Repetition Slotted ALOHA](https://arxiv.org/abs/2601.00343)
*Estefanía Recayte,Leonardo Badia,Andrea Munari*

Main category: cs.ET

TL;DR: 提出一种改进的IRSA协议，通过中间解码和成功节点提前终止传输来减少不必要的传输，从而降低能耗，特别适用于低负载场景。


<details>
  <summary>Details</summary>
Motivation: 现有IRSA文献主要关注高负载渐近情况，而在低负载场景下，许多传输不会发生碰撞因此是冗余的。通过减少这些不必要的传输可以显著降低能耗。

Method: 修改IRSA协议，引入中间解码机制，成功解码的节点提前终止传输。同时建立了有限帧长和低负载下的能耗与成功概率模型。

Result: 分析显示该技术能在保持对标准ALOHA性能优势的同时，显著降低IRSA能耗。例如在10%负载下可实现33%的能耗节省而不影响吞吐量。

Conclusion: 提出的改进IRSA协议通过中间解码和提前终止传输，在低负载场景下能有效降低能耗，同时保持性能优势，为能量受限的无线网络提供了实用解决方案。

Abstract: We evaluate a modification of irregular repetition slotted ALOHA (IRSA) involving intermediate decoding and early transmission termination by some nodes, upon their decoding success. This is meant to avoid unnecessary transmissions, thereby reducing energy consumption. We expect this to be particularly useful at low loads, where most transmissions can be avoided as they do not often result in a collision and are therefore redundant. To validate this proposal, we observe that most of the literature related to IRSA considers an asymptotic heavily loaded regime; thus, we also present a model of energy consumption and success probability for frames of limited length and low offered loads. Thanks to our analysis, also confirmed by simulation, we are able to show that the proposed technique is able to reduce IRSA energy consumption by minimizing transmissions, while preserving performance gains over standard ALOHA. For example, we are able to get a 33% energy saving at offered loads around 10% without affecting throughput.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [68] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

TL;DR: 本文结合Serverless计算和MapReduce模型，通过优化Map和Reduce函数数量来提升词频统计任务的执行效率。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算需求的增长，Serverless计算和MapReduce模型分别因其高并发性和大数据处理能力而受到关注。本文旨在结合两者的优势，优化词频统计任务的执行时间和效率。

Method: 采用基于Serverless计算平台的MapReduce编程模型，针对特定任务探索最优的Map和Reduce函数数量配置。

Result: 实验表明，随着Map和Reduce函数数量的增加，相同工作负载下的执行时间减少，程序整体效率以不同速率提升。

Conclusion: 发现最优的Map和Reduce函数数量配置可以帮助企业和程序员找到最优化解决方案，提升大数据处理效率。

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [69] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

TL;DR: Revati是一个时间扭曲仿真器，通过直接执行真实服务系统代码实现性能建模，无需物理GPU，比真实GPU执行快5-17倍，预测误差小于5%


<details>
  <summary>Details</summary>
Motivation: 部署LLM需要测试数百种服务配置，但在GPU集群上评估每个配置需要数小时和数千美元成本。离散事件仿真器虽然更快更便宜，但需要重新实现服务系统的控制逻辑，随着框架演进负担加重。

Method: Revati是一个时间扭曲仿真器，通过拦截CUDA API调用来虚拟化设备管理，允许服务框架在没有物理GPU的情况下运行。系统不执行GPU内核，而是执行时间跳跃——通过预测的内核持续时间快速推进虚拟时间。提出了一种协调协议，在分布式进程中同步这些时间跳跃，同时保持因果关系。

Result: 在vLLM和SGLang上，Revati在多个模型和并行配置下实现了小于5%的预测误差，同时运行速度比真实GPU执行快5-17倍。

Conclusion: Revati通过直接执行真实服务系统代码实现高性能建模，解决了传统仿真方法需要重新实现控制逻辑的问题，为LLM服务配置测试提供了高效、准确的解决方案。

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [70] [Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure](https://arxiv.org/abs/2601.00530)
*Ravi Teja Pagidoju*

Main category: cs.DC

TL;DR: 该研究对零售POS系统在Google Cloud Platform和Microsoft Azure上的性能进行了系统比较，发现GCP响应时间快23%，而Azure成本效率高71.9%，为小零售商提供了透明的评估方法。


<details>
  <summary>Details</summary>
Motivation: 零售业数字化转型加速了云端POS系统的采用，但缺乏针对零售工作负载的平台特定性能实证研究，特别是对小零售商而言，需要透明、可重复的评估方法。

Method: 使用免费层云资源，通过实时API端点和开源基准测试代码，建立系统化、可重复的POS工作负载部署比较方法，测量响应延迟、吞吐量、可扩展性等性能指标，并根据实际资源使用和当前公有云定价估算运营成本。

Result: GCP在基准负载下实现23.0%更快的响应时间，而Azure在稳态操作中显示71.9%更高的成本效率。所有表格和图表均直接从代码输出生成，确保实验数据与报告结果一致。

Conclusion: 该研究建立了强大的零售云应用开放基准测试方法，提供了首个全面的、代码驱动的POS系统特有工作负载在主要云平台上的比较，为商家考虑云POS实施提供了有用框架。

Abstract: Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.

</details>


### [71] [FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding](https://arxiv.org/abs/2601.00644)
*Yuchen Li,Rui Kong,Zhonghao Lyu,Qiyang Li,Xinran Chen,Hengyi Cai,Lingyong Yan,Shuaiqiang Wang,Jiashu Zhao,Guangxu Zhu,Linghe Kong,Guihai Chen,Haoyi Xiong,Dawei Yin*

Main category: cs.DC

TL;DR: FlexSpec：一种面向边缘-云协同推理的通信高效框架，通过共享骨干架构和信道感知自适应推测机制，解决传统推测解码中模型同步开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 在移动和边缘计算环境中部署大语言模型面临资源限制、无线带宽稀缺和模型频繁更新的挑战。传统边缘-云协同推理中的推测解码方法存在模型紧耦合问题，导致重复的模型同步带来过多通信开销，增加端到端延迟，限制了推测解码在边缘环境中的可扩展性。

Method: 提出FlexSpec框架，核心设计是共享骨干架构，使单个静态的边缘侧草稿模型能够与一系列演化的云侧目标模型保持兼容。此外，开发了信道感知自适应推测机制，根据实时信道状态信息和设备能量预算动态调整推测草稿长度。

Result: 大量实验表明，FlexSpec在推理效率方面相比传统推测解码方法取得了优越的性能，显著减少了通信和维护成本。

Conclusion: FlexSpec通过解耦边缘部署与云侧模型更新，消除了边缘侧重新训练或重复模型下载的需求，为演化中的边缘-云系统提供了一种通信高效的协同推理解决方案。

Abstract: Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.

</details>
