<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 77]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [The influence of the random numbers quality on the results in stochastic simulations and machine learning](https://arxiv.org/abs/2510.25269)
*Benjamin A. Antunes*

Main category: cs.PF

TL;DR: 研究评估了不同统计质量的伪随机数生成器(PRNG)对随机计算任务的影响，发现低质量PRNG会显著影响结果，而达到足够统计鲁棒性后，不同生成器家族对大多数任务影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 尽管PRNG在随机模拟和机器学习中广泛应用，但其统计质量对计算结果的影响尚未充分探索。本研究旨在调查PRNG质量差异是否会影响代表性随机应用的结果。

Method: 评估了7种PRNG，从低质量的线性同余生成器到高质量的Mersenne Twister、PCG和Philox。将这些PRNG应用于四个任务：流行病学基于代理的模型、两个独立的MNIST分类实现和强化学习CartPole环境。每个实验使用固定种子重复30次以确保可重复性。

Result: 统计质量极差的LCG在ABM中产生显著的流行病动态偏差，降低MNIST分类准确率，并严重恶化RL性能。中等和良好质量的LCG尽管在部分测试中失败，但在大多数任务中表现与顶级PRNG相当，RL实验是主要例外。

Conclusion: 一旦生成器达到足够的统计鲁棒性阈值，其家族或设计对大多数工作负载的结果影响可忽略，选择可基于性能和实现考虑。但在敏感随机计算中使用低质量PRNG会引入显著且系统性的错误。

Abstract: Pseudorandom number generators (PRNGs) are ubiquitous in stochastic
simulations and machine learning (ML), where they drive sampling, parameter
initialization, regularization, and data shuffling. While widely used, the
potential impact of PRNG statistical quality on computational results remains
underexplored. In this study, we investigate whether differences in PRNG
quality, as measured by standard statistical test suites, can influence
outcomes in representative stochastic applications. Seven PRNGs were evaluated,
ranging from low-quality linear congruential generators (LCGs) with known
statistical deficiencies to high-quality generators such as Mersenne Twister,
PCG, and Philox. We applied these PRNGs to four distinct tasks: an
epidemiological agent-based model (ABM), two independent from-scratch MNIST
classification implementations (Python/NumPy and C++), and a reinforcement
learning (RL) CartPole environment. Each experiment was repeated 30 times per
generator using fixed seeds to ensure reproducibility, and outputs were
compared using appropriate statistical analyses. Results show that very poor
statistical quality, as in the ''bad'' LCG failing 125 TestU01 Crush tests,
produces significant deviations in ABM epidemic dynamics, reduces MNIST
classification accuracy, and severely degrades RL performance. In contrast,
mid-and good-quality LCGs-despite failing a limited number of Crush or BigCrush
tests-performed comparably to top-tier PRNGs in most tasks, with the RL
experiment being the primary exception where performance scaled with
statistical quality. Our findings indicate that, once a generator meets a
sufficient statistical robustness threshold, its family or design has
negligible impact on outcomes for most workloads, allowing selection to be
guided by performance and implementation considerations. However, the use of
low-quality PRNGs in sensitive stochastic computations can introduce
substantial and systematic errors.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [2] [Cryogenic Characterization of Ferroelectric Non-volatile Capacitors](https://arxiv.org/abs/2510.25040)
*Madhav Vadlamani,Dyutimoy Chakraborty,Jianwei Jia,Halid Mulaosmanovic,Stefan Duenkel,Sven Beyer,Suman Datta,Shimeng Yu*

Main category: cs.ET

TL;DR: 该论文研究了铁电电容交叉阵列在低温下的性能，通过降低温度来减少热噪声，从而提高有效比特数（ENOB）至约5比特。


<details>
  <summary>Details</summary>
Motivation: 铁电电容交叉阵列虽然能解决电阻交叉阵列的漏电路径和高静态功耗问题，但易受热噪声影响，限制了加权和的有效比特数。降低温度是减少热噪声的直接方法。

Method: 首先在28纳米工艺平台上对非易失性电容器（nvCaps）进行低温（低至77K）特性表征，评估存储窗口和ON状态保持能力，然后使用校准后的器件模型在SPICE中模拟电容交叉阵列在低温下的性能。

Result: 在低温下，电容交叉阵列的128x128乘累加（MAC）操作实现了更高的有效比特数（ENOB），达到约5比特。

Conclusion: 通过降低温度可以有效减少铁电电容交叉阵列的热噪声，显著提高其计算精度和有效比特数，为低温内存计算提供了可行性。

Abstract: Ferroelectric-based capacitive crossbar arrays have been proposed for
energy-efficient in-memory computing in the charge domain. They combat the
challenges like sneak paths and high static power faced by resistive crossbar
arrays but are susceptible to thermal noise limiting the effective number of
bits (ENOB) for the weighted sum. A direct way to reduce this thermal noise is
by lowering the temperature as thermal noise is proportional to temperature. In
this work, we first characterize the non-volatile capacitors (nvCaps) on a
foundry 28 nm platform at cryogenic temperatures to evaluate the memory window,
ON state retention as a function of temperature down to 77K, and then use the
calibrated device models to simulate the capacitive crossbar arrays in SPICE at
lower temperatures to demonstrate higher ENOB (~5 bits) for 128x128
multiple-and-accumulate (MAC) operations.

</details>


### [3] [Modulation Schemes for Functionalized Vesicle-based MC Transmitters](https://arxiv.org/abs/2510.25676)
*Teena tom Dieck,Lukas Brand,Sebastian Lotter,Kathrin Castiglione,Robert Schober,Maximilian Schäfer*

Main category: cs.ET

TL;DR: 提出了一种更现实的分子通信发射器模型，包含信号分子释放延迟和生物噪声，并设计了两种调制方案来缓解发射器引起的内存效应。


<details>
  <summary>Details</summary>
Motivation: 现有分子通信研究大多依赖简化的发射器模型，未考虑实际生物硬件的物理和生化限制，需要开发更实用的MC系统模型。

Method: 基于功能化囊泡的发射器模型，包含SM释放延迟和TX噪声，提出了两种专门设计的调制方案来缓解发射器引起的内存效应。

Result: 数值评估表明，所提出的方案在现实生化约束下提高了通信可靠性，为物理可实现的MC系统迈出了重要一步。

Conclusion: 通过更现实的发射器模型和专门设计的调制方案，可以有效缓解TX引起的内存效应，实现低复杂度接收器设计，推动分子通信系统的实际应用。

Abstract: Molecular communication (MC) enables information exchange through the
transmission of signaling molecules (SMs) and holds promise for many innovative
applications. However, most existing MC studies rely on simplified transmitter
(TX) models that do not account for the physical and biochemical limitations of
realistic biological hardware. This work extends previous efforts toward
developing models for practical MC systems by proposing a more realistic TX
model that incorporates the delay in SM release and TX noise introduced by
biological components. Building on this more realistic, functionalized
vesicle-based TX model, we propose two novel modulation schemes specifically
designed for this TX to mitigate TX-induced memory effects that arise from
delayed and imperfectly controllable SM release. The proposed modulation
schemes enable low-complexity receiver designs by mitigating memory effects
directly at the TX. Numerical evaluations demonstrate that the proposed schemes
improve communication reliability under realistic biochemical constraints,
offering an important step toward physically realizable MC systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree是首个将WMO FM-301标准从单次雷达体扫扩展到时间序列、分析就绪存档的数据集级框架，通过开源架构将运营雷达档案转换为符合FAIR原则的云优化数据集。


<details>
  <summary>Details</summary>
Motivation: 天气雷达数据是科学价值最高但结构利用不足的地球观测数据集之一。尽管广泛公开可用，但雷达档案仍然分散、供应商特定，且与FAIR原则不符，阻碍了大规模研究、可重复性和云原生计算。

Method: 基于FM-301/CfRadial 2.1标准，使用xarray DataTree将雷达体扫组织为层次化、元数据丰富的结构，并序列化为Zarr格式进行可扩展分析。结合Icechunk实现ACID兼容存储和版本控制。

Result: 在准垂直剖面(QVP)和降水累积工作流等案例研究中展示了显著的性能提升，所有工具和数据集通过Raw2Zarr仓库公开发布。

Conclusion: 这项工作为雷达数据管理、高性能地球科学和AI就绪天气基础设施提供了可重复和可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [5] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: 提出了一种多分辨率模型融合方法，通过结合低分辨率训练和原始分辨率微调来显著减少神经网络训练时间，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络在科学研究中应用广泛，但训练高维大样本数据时计算成本高昂，需要高效的训练方法来降低计算成本。

Method: 多分辨率模型融合方法：先在降低分辨率的数据上训练模型，然后在原始分辨率数据上进行微调，通过加速每个融合阶段的收敛来减少总训练时间。

Result: 在CosmoFlow和Neuron Inverter两个真实科学应用中，训练时间分别减少了47%和44%，同时模型精度不受影响。

Conclusion: 多分辨率模型融合方法能显著减少端到端训练时间，同时确保最终模型保持高分辨率洞察力，为科学计算中的神经网络训练提供了高效解决方案。

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [6] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 提出了ER-Mapping和NI-Balancer方法，在晶圆级芯片上优化MoE模型的专家并行计算，通过平衡通信压力和隐藏专家迁移开销，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，MoE技术成为主流，但GPU集群中的跨节点通信开销限制了专家并行的应用。晶圆级芯片提供了高性能网络，但其网状拓扑导致通信不平衡，且缺乏片上磁盘导致专家迁移开销高。

Method: 1. ER-Mapping：共同设计注意力层和MoE层的映射策略，平衡通信压力；2. NI-Balancer：将完整的专家迁移拆分为多步，交替利用两个层的冷链路来隐藏迁移开销。

Result: ER-Mapping实现通信减少高达62%，NI-Balancer在MoE计算和通信上分别带来54%和22%的改进。相比NVL72超级节点，晶圆级芯片平台平均提供39%更高的每设备MoE性能。

Conclusion: 通过ER-Mapping和NI-Balancer的协同设计，晶圆级芯片平台能够充分发挥其在托管MoE模型方面的潜力，显著提升专家并行计算的性能和可扩展性。

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [7] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: 提出了一种多阶段的安全AI训练方法，通过模拟临床知识图谱设计模型，在联邦学习框架中训练，并在医院环境中使用真实数据完成训练，仅返回聚合性能指标，实现隐私保护的医疗AI开发。


<details>
  <summary>Details</summary>
Motivation: 临床数据整合对个性化医疗发展至关重要，但受GDPR严格限制，特别是对于罕见疾病的小型队列。需要高质量结构化数据来开发预测性医疗AI，同时保护患者隐私。

Method: 四阶段方法：1) 在模拟临床知识图谱上设计模型；2) 在FeatureCloud联邦学习框架中准备模型；3) 在医院环境中使用真实数据训练；4) 执行验证评估脚本仅返回聚合性能指标。

Result: 在TUM.ai Makeathon 2024挑战中成功验证，50名学生无需访问真实数据即可开发患者分类和诊断模型。

Conclusion: 通过联邦学习框架部署安全算法是实现医疗领域隐私保护AI的实用方法。

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [8] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: 本文对大数据密集型工作负载进行分类，并综述了大规模分布式系统中常用的调度方法，提出了新的调度策略并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大数据爆炸式增长，工作负载变得更加复杂和计算密集，需要有效的调度技术来应对数据密集型应用在并行度、数据局部性、服务质量等方面的挑战。

Method: 提出数据密集型工作负载的分类方法，并综述大规模分布式系统中常用的调度方法，包括文献中提出的新颖策略。

Result: 提供了数据密集型工作负载的全面分类框架，总结了现有调度方法的优缺点，并识别了关键挑战。

Conclusion: 数据密集型工作负载调度面临重大挑战，需要开发更有效的调度策略来满足服务质量要求和资源优化目标，未来研究应关注开放挑战和新方向。

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [9] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: 该论文研究了移动智能体在分布式网络中的聚集问题，特别关注当智能体可能共享相同标签时的确定性聚集能力。论文给出了可聚集团队的完整特征描述，设计了多项式时间算法，并证明了所需共同知识量的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统确定性聚集算法假设智能体具有唯一标签，但实际应用中标签可能重复。论文旨在探索在标签可能重复的情况下，哪些团队能够实现确定性聚集，以及如何高效实现。

Method: 通过理论分析给出可聚集团队的完整特征描述，设计了一种多项式时间算法，该算法仅需少量共同知识，并证明了所需共同知识量的最优性。

Result: 成功刻画了所有可聚集团队的特征，设计了在多项式时间内完成聚集的算法，并证明所需共同知识量几乎是最优的。

Conclusion: 论文解决了标签重复情况下的确定性聚集问题，给出了完整的理论框架和高效算法，为分布式移动系统提供了重要的理论基础和实用解决方案。

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [10] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming是一个支持精确一次处理的流处理系统，通过窗口化无冲突复制数据类型(Windowed CRDTs)实现可扩展的全局聚合计算，采用去中心化协调机制，相比现有系统在延迟和吞吐量方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有流处理系统在全局聚合计算时存在可扩展性瓶颈，要么在单个任务实例中计算，要么使用静态聚合树，这限制了系统性能并成为瓶颈。此外，端到端延迟由树中最慢路径决定，故障和重新配置会导致大的延迟峰值。

Method: 提出确定性编程模型，使用窗口化无冲突复制数据类型(Windowed CRDTs)作为共享复制状态的新抽象。Windowed CRDTs使全局聚合计算具有可扩展性，其确定性、收敛性等特性支持设计高效的去中心化故障恢复算法。

Result: 评估显示，在全局聚合工作负载上，相比现有流处理系统，延迟降低5倍，吞吐量提高2倍，在故障场景下延迟减少11倍。

Conclusion: 论文证明了去中心化协调与确定性的有效性，以及Windowed CRDTs在全局聚合计算中的实用性。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [DIRC-RAG: Accelerating Edge RAG with Robust High-Density and High-Loading-Bandwidth Digital In-ReRAM Computation](https://arxiv.org/abs/2510.25278)
*Kunming Shao,Zhipeng Liao,Jiangnan Yu,Liang Zhao,Qiwei Li,Xijie Huang,Jingyu He,Fengshi Tian,Yi Zou,Xiaomeng Wang,Tim Kwang-Ting Cheng,Chi-Ying Tsui*

Main category: cs.AR

TL;DR: DIRCRAG是一种基于数字In-ReRAM计算的新型边缘RAG加速架构，通过集成高密度多级ReRAM子阵列和SRAM单元，实现超低功耗、单周期数据加载，显著降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上RAG系统面临的高存储、高能耗和高延迟挑战，同时克服现有CIM技术内存密度低或计算精度有限的问题。

Method: 采用数字In-ReRAM计算(DIRC)架构，集成高密度多级ReRAM子阵列与SRAM单元，利用SRAM和差分传感实现稳健的ReRAM读取和数字MAC操作，支持查询静态数据流，并引入误差优化和检测电路。

Result: 在TSMC40nm工艺下实现5.18Mb/mm²的片上非易失性内存密度和131 TOPS的吞吐量，4MB检索延迟为5.6μs/查询，能耗为0.956μJ/查询，同时保持检索精度。

Conclusion: DIRCRAG架构有效解决了边缘RAG系统的性能瓶颈，为边缘AI应用提供了高效、低功耗的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieval but faces challenges on edge devices
due to high storage, energy, and latency demands. Computing-in-Memory (CIM)
offers a promising solution by storing document embeddings in CIM macros and
enabling in-situ parallel retrievals but is constrained by either low memory
density or limited computational accuracy. To address these challenges, we
present DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital
In-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM
subarray with an SRAM cell, utilizing SRAM and differential sensing for robust
ReRAM readout and digital multiply-accumulate (MAC) operations. By storing all
document embeddings within the CIM macro, DIRC achieves ultra-low-power,
single-cycle data loading, substantially reducing both energy consumption and
latency compared to offchip DRAM. A query-stationary (QS) dataflow is supported
for RAG tasks, minimizing on-chip data movement and reducing SRAM buffer
requirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by
extracting the bit-wise spatial error distribution of the ReRAM subarray and
applying targeted bit-wise data remapping. An error detection circuit is also
implemented to enhance readout resilience against deviceand circuit-level
variations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process
achieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput
of 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\mu}s/query and an
energy consumption of 0.956{\mu}J/query, while maintaining the retrieval
precision.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo协议利用群体智能和分布式成对排名共识，在AI推理中实现优于多数投票的性能，在GPQA Diamond上达到85.90%准确率，比多数投票高17.21个百分点。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI达到计算上限和更大训练运行带来的收益递减，需要能够水平和垂直扩展的推理层来满足需求。

Method: 采用群体推理方法：基于同行排名、声誉加权的异构模型共识，使用成对排名和Bradley-Terry式聚合模型，结合链上声誉机制和能力证明来抵抗Sybil攻击。

Result: 在六个基准测试中表现优异，GPQA Diamond准确率85.90%，比多数投票高+17.21个百分点；对对抗性提示具有强韧性，提示注入退化仅0.12%。

Conclusion: 为去中心化AI系统奠定了基础，通过集体智能民主化高质量推理访问，同时保持可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [13] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文分析了从弱线性CNN到强两层ReLU CNN的弱到强泛化现象，揭示了在数据稀缺和数据丰富两种机制下的不同泛化行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多局限于抽象框架或线性/随机特征模型，缺乏对具体CNN架构中弱到强泛化现象的形式化分析。

Method: 使用梯度下降动态分析，考虑包含不同难度标签相关信号和标签无关噪声的结构化数据，在强模型使用预训练弱模型标注的数据进行训练。

Result: 识别了两种机制：数据稀缺机制下通过良性过拟合或有害过拟合实现泛化；数据丰富机制下早期通过标签校正实现泛化，但过度训练会降低性能。

Conclusion: 弱到强泛化的成功取决于数据集的信噪比特性，不同机制下有不同的泛化行为和过渡边界。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [14] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析适应性景观，计算20个生物学相关特征来表征景观地形，帮助解释和比较多种适应性预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏关于底层适应性景观的地形信息，这阻碍了模型性能的解释和比较。

Method: GraphFLA框架从DNA、RNA、蛋白质等不同模式的突变数据构建适应性景观，计算20个特征来表征4个基本地形方面。

Result: 应用于5,300多个景观，展示了其在解释和比较数十种适应性预测模型性能方面的实用性，并发布了155个组合完整的经验适应性景观。

Conclusion: GraphFLA提供了一个强大的工具来表征适应性景观地形，有助于更深入地理解模型性能和不同模型的优势。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [15] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 该论文研究了在ESP32-S3微控制器上使用压缩CNN模型和低功耗广域网进行环境监测，通过设备端推理和仅传输结果，能耗比传输原始图像数据降低高达5倍。


<details>
  <summary>Details</summary>
Motivation: 环境监测需要能够在偏远地区长期运行的节能物联网设备，特别是处理图像数据时，数据传输能耗高，需要寻找降低能耗的解决方案。

Method: 在ESP32-S3微控制器上评估常见低功耗广域网和针对特定领域数据集训练的压缩CNN模型，通过设备端推理减少数据传输量。

Result: 设备端执行CNN推理并仅传输结果，相比传输原始图像数据，整体能耗降低高达5倍；模型量化仅导致精度轻微下降几个百分点。

Conclusion: 嵌入式机器学习能够开发碳足迹更小、能够在环境监测场景中自主运行的物联网应用。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [16] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现OOD泛化基准中ID与OOD准确率的正相关性可能是异质OOD样本聚合的假象，通过OODSelect方法识别出语义一致的子集，显示更高ID准确率反而预测更低OOD准确率。


<details>
  <summary>Details</summary>
Motivation: 挑战当前OOD泛化基准中普遍存在的"准确率在线"现象，认为这种正相关性可能掩盖了实际存在的伪相关问题。

Method: 使用基于梯度的OODSelect方法，从异质OOD样本中识别语义一致的子集，在这些子集上检验ID与OOD准确率的关系。

Result: 在广泛使用的分布偏移基准中，OODSelect发现了占标准OOD集一半以上的子集，其中更高ID准确率预测更低OOD准确率。

Conclusion: 聚合指标可能掩盖OOD鲁棒性的重要失效模式，需要更细粒度的分析方法来揭示真实的泛化问题。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [17] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应多任务EEG分类器，使用GRU-TCN网络和DQN阈值调整，用于卒中快速分诊，在卒中类型分类上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确、床旁可部署的工具，EEG有潜力但在首次接触时使用不足。

Method: 将32通道EEG信号转换为功率谱密度特征，使用GRU-TCN网络预测卒中类型、半球偏侧化和严重程度，并应用DQN实时调整决策阈值。

Result: 基线GRU-TCN在卒中类型分类上达到89.3%准确率，严重程度96.9%，偏侧化96.7%；使用DQN阈值适应后，卒中类型准确率提升至98.0%。

Conclusion: 自适应阈值调整可将操作点转移到临床偏好的敏感度-特异度权衡，同时集成的头皮图和频谱可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [18] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出nnLDA，一种神经增强概率主题模型，通过神经先验机制动态整合辅助信息，在主题一致性、困惑度和下游分类任务上优于传统LDA和Dirichlet-Multinomial回归。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型如LDA难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达能力、个性化和可解释性。

Method: nnLDA通过神经先验机制建模文档主题分布，其中主题比例的先验由基于辅助特征的神经网络生成，使用随机变分EM算法联合优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类任务上持续优于LDA和Dirichlet-Multinomial回归。

Conclusion: 在辅助信息可用的场景下，将神经表示学习与概率主题建模相结合具有显著优势。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [19] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 该论文提出了一种分布式机器学习的计算资源优化方法，通过协同优化数据分配和CPU资源分配，在时变网络中实现高效训练，比现有CPU调度方案提升50%以上的成本最优性差距。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能研究的快速发展，对快速、计算高效且可扩展解决方案的需求日益增长。需要解决分布式机器学习和优化中的计算资源优化问题。

Method: 提出协同优化框架，同时优化数据处理和计算资源分配。算法在所有迭代中都保持可行性，支持信息共享通道上的对数尺度量化，并在时变但权重平衡的网络中确保共识类型收敛。

Result: 与现有CPU调度解决方案相比，所提算法将成本最优性差距提高了50%以上。通过扰动理论、Lyapunov稳定性和特征谱分析证明了向最优情况的收敛性。

Conclusion: 该算法为分布式机器学习提供了一种有效的计算资源优化方法，在保持训练性能的同时显著提升了资源利用效率。

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [20] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一个用于冰盖建模的快速准确模拟器，将Kolmogorov-Arnold网络作为特征校准器置于图卷积网络之前，通过可学习的一维扭曲和线性混合步骤改善特征条件和非线性编码。


<details>
  <summary>Details</summary>
Motivation: 提高冰盖数值模型模拟器的性能，在不增加消息传递深度的情况下改善特征条件和非线性编码能力。

Method: 使用KAN作为前端特征校准器，结合图卷积网络，在36个融化率模拟和3种网格尺寸设置下训练和测试Pine Island Glacier模型。

Result: 在2-5层架构中，KAN-GCN匹配或超越了纯GCN和MLP-GCN基线的准确性，在较粗网格上通过替换边级消息传递层提高了推理吞吐量。

Conclusion: KAN优先设计为大型瞬态场景扫描提供了有利的准确性与效率权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [21] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出WBT-BGRL框架，通过加权三元组损失增强自举学习，用于二分图的归纳链接预测


<details>
  <summary>Details</summary>
Motivation: 二分图链接预测在推荐系统和故障检测中很重要，但现有方法在归纳、加权和二分场景下效果未经测试，对比方法存在负采样效率低和偏差问题

Method: 使用双GCN编码器的二分架构，在自举学习中通过加权三元组损失增强，是非对比学习方法

Result: 在真实数据集（工业和电商）上表现有竞争力，特别是预训练时应用加权机制效果更好

Conclusion: 加权非对比学习对二分图归纳链接预测具有重要价值

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [22] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现LLMs生成的思维链中许多步骤对最终预测没有实际因果影响，存在装饰性思考步骤。提出了True Thinking Score来量化每个推理步骤的因果影响，并发现LLMs在潜在空间中存在TrueThinking方向，通过控制该方向可以强制模型执行或忽略特定推理步骤。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs生成的思维链常被视为模型内部思考过程的忠实反映，用于监控不安全意图。但作者怀疑许多推理步骤实际上并不真正影响模型的最终预测，这影响了LLM推理的效率和思维链的可信度。

Method: 提出了True Thinking Score(TTS)来测量每个推理步骤对模型最终预测的因果影响，并识别了LLMs潜在空间中的TrueThinking方向，通过控制该方向来验证推理步骤的真实作用。

Result: 发现LLMs经常在真实思考步骤和装饰性思考步骤之间交替，只有很小一部分推理步骤具有高TTS值。例如在AIME数据集中，只有平均2.3%的推理步骤TTS≥0.7。通过控制TrueThinking方向可以改变模型对特定推理步骤的处理方式。

Conclusion: LLMs经常口头表达推理步骤但实际上并不在内部执行它们，这既削弱了LLM推理的效率，也损害了思维链的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [23] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型存在文化敏感神经元，这些神经元对特定文化背景的输入表现出选择性敏感，并在解码器层中聚集。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现优异，但在处理文化相关输入时仍存在困难，需要理解模型如何处理文化基础信息。

Method: 使用CVQA基准识别文化选择性神经元，通过消融实验测试不同识别方法标记的神经元，并提出新的基于边际的选择器CAS。

Result: 实验表明存在特定神经元，其消融会显著影响对应文化问题的性能，而对其他文化影响很小。CAS方法在识别文化敏感神经元方面优于现有方法。

Conclusion: 研究揭示了多模态表征的内部组织方式，发现了文化敏感神经元在解码器层中的聚集现象。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [24] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 该论文提出通过算法-硬件协同设计的方法，同时提升神经网络在资源受限平台上的效率和可靠性，包括模型压缩、近似贝叶斯推理、数字/模拟硬件优化等技术。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习在嵌入式等资源受限平台上面临计算需求增长和可靠性要求的双重挑战，贝叶斯神经网络虽然能提供不确定性量化，但计算开销更大。

Method: 采用算法和硬件协同优化：Galen系统进行自动层特定压缩；扩展噪声训练到非平稳条件；开发解析和集成近似替代昂贵采样；提出概率光子计算利用模拟噪声作为熵源。

Result: 实现了资源高效且鲁棒的推理，为可信赖、高能效的机器学习系统奠定了基础。

Conclusion: 效率和可靠性可以通过算法-硬件协同设计共同推进，为下一代可信赖、高能效的机器学习系统铺平道路。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [25] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型的低维结构，发现现代语言模型在logits矩阵上表现出低秩特性，并证明了可以利用这种结构通过无关甚至无意义提示的线性组合来生成目标提示的响应。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型固有的低维结构是一个重要问题，作者希望从模型无关的角度研究语言模型作为序列概率模型的低维特性。

Method: 首先实证证明了现代语言模型的logits矩阵具有低秩结构，然后利用这种结构通过线性组合不同提示的输出来生成目标提示的响应。

Result: 实验表明广泛的语言模型都表现出低秩特性，并且可以利用这种特性进行生成任务，即使使用无关或荒谬的提示也能有效生成目标响应。

Conclusion: 语言模型的低秩结构提供了一个简单的通用抽象，其理论预测与实验结果一致，并且作者给出了可证明的学习保证。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [26] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 提出了一种基于排名条件的委员会框架，利用抗体构象排名为每个排名分配深度神经网络委员会，从而区分认知不确定性和构象不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习辅助定向进化方法通常依赖单一构象或单一委员会，无法有效区分构象不确定性和认知不确定性，限制了抗体适应性景观的探索效率。

Method: 开发了排名条件委员会框架，根据构象排名为每个排名分配独立的深度神经网络委员会，实现认知不确定性和构象不确定性的原则性分离。

Result: 在SARS-CoV-2抗体对接任务中验证了该方法，相比基线策略取得了显著改进。

Conclusion: 该方法为治疗性抗体发现提供了可扩展的途径，同时直接解决了构象不确定性建模的挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [27] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出基于博弈论的端到端特征选择框架，通过评估特征的协同作用和边际贡献来确定特征重要性，显著降低计算成本同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 数据量指数增长导致机器学习模型训练计算成本急剧上升，许多特征对模型性能无正面贡献却消耗大量计算资源。

Method: 基于合作博弈论构建特征选择框架，将特征建模为玩家，通过评估协同交互和边际贡献确定特征重要性，包含样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 实验结果表明该方法在保持预测性能的同时实现了显著的计算减少，为大规模机器学习计算挑战提供了高效解决方案。

Conclusion: 提出的博弈论特征选择框架能够有效解决大规模机器学习中的计算效率问题，在降低计算成本的同时维持模型性能。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [28] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将每个去噪步骤视为顺序假设检验，为扩散策略提供统计意义上的风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中通常使用缺乏统计风险概念的启发式方法进行采样指导，需要一种具有用户可解释风险预算的原则性风险控制方法。

Method: 在DDPM结构下保持训练不变，在推理时引入LRT指导：累积对数似然比，并使用逻辑控制器对条件均值进行门控，阈值τ通过H0下的校准来满足用户指定的Type-I水平α。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion相比强Q指导基线改善了回报-OOD权衡，同时满足期望的α水平。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL中的扩散策略添加了原则性、校准的风险控制。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [29] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 该研究提出了一种结合实时癫痫检测和预测的新方法，使用机器学习算法在CHB-MIT头皮EEG数据库上取得了良好效果，其中逻辑回归检测准确率达90.9%，LSTM预测准确率达89.26%。


<details>
  <summary>Details</summary>
Motivation: 传统癫痫检测方法仅在癫痫发作后识别，限制了早期干预机会。本研究旨在开发能够同时进行实时检测和预测的方法，实现从被动管理向主动预防的转变。

Method: 使用多种监督机器学习算法（KNN、逻辑回归、随机森林、SVM）进行癫痫检测，采用LSTM网络进行癫痫预测，基于CHB-MIT头皮EEG数据库（969小时记录，173次癫痫发作）。

Result: 逻辑回归检测准确率90.9%，召回率89.6%；随机森林和SVM准确率94.0%但召回率为0%；LSTM预测准确率89.26%。结果表明在医学ML模型中仅用准确率评估不足。

Conclusion: 该方法展示了开发可访问实时监测工具的潜力，能够检测并预测癫痫发作，实现从被动管理向主动预防的转变，让患者能够预见癫痫并采取预防措施。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [30] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应状态轨迹分割实现选项发现，在Four-Rooms和Pinball任务中表现出加速收敛、更高累积回报和更好的选项专业化。


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中自主发现语义上有意义的子目标和学习最优选项终止边界的挑战，提升决策在长时程任务中的可扩展性。

Method: 集成自监督Transformer变化点检测模块到Option-Critic框架，利用内在信号生成启发式伪标签推断环境动态的潜在变化，通过变化点稳定终止函数梯度、预训练内部选项策略、强制执行功能专业化。

Result: 在Four-Rooms和Pinball任务中，CPD引导的智能体表现出加速收敛、更高累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验能够在复杂环境中产生更可解释、样本效率更高和更稳健的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [31] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统分析了矩阵白化优化器的性能优势，发现方差自适应是解释性能差距的关键因素，而非仅靠准确的谱归一化。


<details>
  <summary>Details</summary>
Motivation: 解构各种近似矩阵白化变换的优化器，识别解释其性能优势的关键组件。

Method: 通过实验比较各种矩阵白化优化器与逐元素优化器（如Adam），分析谱归一化和方差自适应组件的作用。

Result: 所有矩阵白化方法都可靠地优于逐元素优化器；方差自适应版本始终优于其符号下降对应版本；低秩方差估计器能有效降低内存成本且无性能损失。

Conclusion: 矩阵白化的性能优势主要来自方差自适应组件，而非仅靠准确的谱归一化；低秩方差估计是有效的内存优化策略。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [32] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一种深度多编码器自编码器，能够将多区域神经记录分解为共享和私有潜在子空间，通过新颖的对齐和解缠损失来分离网络级动态与区域特定活动。


<details>
  <summary>Details</summary>
Motivation: 在多区域神经数据建模中，分离共享网络级动态与区域特定活动是一个核心挑战。

Method: 引入SPIRE模型，使用深度多编码器自编码器架构，配备新颖的对齐和解缠损失函数，仅使用基线数据进行训练。

Result: 在具有真实潜在变量的合成基准测试中，SPIRE在非线性扭曲和时间错位下优于经典概率模型。在颅内深部脑刺激记录中，SPIRE显示共享潜在变量可靠地编码刺激特异性特征，并能跨位点和频率泛化。

Conclusion: SPIRE为分析刺激下多区域神经动态提供了一个实用、可复现的工具。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [33] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 基于放射组学的机器学习模型在临床决策支持中具有潜力，但容易受到成像协议、定位和分割变化引起的分布偏移影响。本研究系统评估了五种MRI序列下放射组学模型的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下能保持高性能，而数据增强可显著改善不确定性估计质量。


<details>
  <summary>Details</summary>
Motivation: 放射组学模型在临床应用中面临成像协议、分割策略等变化导致的分布偏移问题，这会影响模型的可靠性和泛化能力。需要系统研究这些因素对模型性能的影响，并开发鲁棒的解决方案。

Method: 使用16种水果的体模，评估了：(1) 五种MRI序列的协议变化；(2) 分割变化（完整、部分、旋转）；(3) 观察者间变异性。训练XGBoost分类器，比较协议不变特征与序列特定特征在不同域条件下的性能。

Result: 使用协议不变特征训练的模型在分布偏移下F1分数保持>0.85，而使用所有特征的模型在协议变化下性能下降40%。数据增强使预期校准误差降低35%，且不牺牲准确性。温度缩放校准效果有限。

Conclusion: 协议感知的特征选择和受控体模研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [34] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出了一种基于因果效应估计任务的有向无环混合图距离度量方法，用于评估因果发现方法在潜在混杂下的性能


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法难以在潜在混杂下进行有效评估，需要一种基于实际因果效应估计任务的图距离度量

Method: 使用基于固定识别的符号验证器，量化图差异对不同处理-结果对因果效应估计量的扭曲程度

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较

Conclusion: 提出的图距离度量能够有效评估因果发现方法在潜在混杂下的性能

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [35] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为DWMGrad的新型优化算法，通过动态历史数据引导机制自适应调整动量和学习率，解决了传统优化算法在处理复杂模型和非凸优化问题时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习研究中，SGD和Adam等优化算法在处理学习效率波动、复杂模型需求和非凸优化问题时存在明显不足，特别是在处理复杂数据结构和模型时面临学习率选择困难、局部最优规避和高维空间导航等挑战。

Method: 基于传统方法基础，引入依赖历史数据的动态引导机制，动态更新动量和学习率，使优化器能够灵活调整对历史信息的依赖，适应不同的训练场景。

Result: 经过大量实验验证，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法通过动态引导机制有效提升了优化器对变化环境和任务复杂度的适应能力，在收敛性能和精度方面表现出显著优势。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [36] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: BOND是一种扰动方法，用于估计无法访问计算图的网络结构中的偏导数，比现有方法更准确和可扩展，支持集成黑盒函数的新架构探索。


<details>
  <summary>Details</summary>
Motivation: 现有扰动方法在估计网络结构中不可访问计算图的偏导数时存在精度和可扩展性问题，限制了集成黑盒函数的可训练架构探索。

Method: 提出Bounded Numerical Differentiation (BOND)方法，通过扰动技术估计偏导数，支持将固定未训练网络作为黑盒函数集成到模型中。

Result: BOND方法比现有扰动方法更准确和可扩展；集成黑盒函数（固定未训练网络）可提升模型性能而不增加可训练参数数量，且无需对黑盒函数进行大量优化。

Conclusion: 固定不可训练模块有潜力扩展模型容量，为结合模拟和数字设备扩展网络提供了可行路径。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [37] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐系统中LoRA持续学习的新方法，通过近端正则化在适应新数据与保留旧知识之间取得平衡，特别适合用户偏好动态变化的场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的持续学习方法主要关注保持对过去任务的性能，但推荐系统的目标是预测当前偏好而非过去偏好，过时的偏好信息在当前兴趣显著变化时甚至会损害性能。

Method: 提出PESO方法，引入近端正则化器将当前适配器锚定到其最近的冻结状态，使模型能够灵活平衡适应性和保留性，更好地捕捉最近的用户行为。

Result: 理论上证明该近端设计在LoRA子空间中提供数据感知、方向感知的指导；实证上PESO持续优于现有的基于LoRA的持续学习方法。

Conclusion: PESO通过近端正则化有效解决了推荐系统中持续学习的特殊挑战，在动态变化的用户偏好场景中表现出色。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [38] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: FairMIB是一个多视图信息瓶颈框架，通过分解图数据为特征、结构和扩散视图来缓解GNN中的复杂偏见，使用对比学习和条件信息瓶颈来平衡任务效用和公平性。


<details>
  <summary>Details</summary>
Motivation: GNN在处理关系数据时容易放大训练数据偏见，传播歧视性属性和结构不平衡到不公平结果中。现有公平性方法通常将偏见视为单一来源，忽略了属性和结构的不同影响，导致公平性和效用的次优权衡。

Method: 提出FairMIB框架：1）将图分解为特征、结构和扩散三个视图；2）使用对比学习最大化跨视图互信息以学习无偏见表示；3）集成多视角条件信息瓶颈目标来最小化与敏感属性的互信息；4）在扩散视图中引入逆概率加权邻接校正来减少偏见传播。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。

Conclusion: FairMIB通过多视图分解和信息瓶颈方法有效缓解了GNN中的复杂偏见问题，在保持任务效用的同时显著提升了公平性表现。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [39] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 该论文研究发现，在某些情况下，训练和测试数据分布比例不匹配（分布偏移）反而能提升测试性能，即使各组件之间没有相关性或迁移学习。作者识别了最优训练比例，并量化了这种分布偏移的益处程度。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习假设训练和测试数据来自相同分布，但实际应用中分布偏移很常见。本研究挑战了"分布匹配总是最优"的假设，探索分布偏移可能带来的性能提升。

Method: 通过理论分析和多种场景下的实验，研究训练和测试混合分布比例不匹配的情况。分析包括识别最优训练比例，并扩展到组合设置中不同"技能"组件分布的差异。

Result: 研究表明，在某些设置中分布偏移确实是有益的，测试性能可以因训练比例不匹配而提升。作者量化了这种益处的程度，并确定了最优训练比例。

Conclusion: 分布偏移不总是有害的，在特定条件下反而能带来性能提升。这一发现挑战了传统机器学习假设，为实际应用中的分布不匹配问题提供了新的视角。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [40] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出神经微分流形(NDM)架构，将神经网络重新概念化为微分流形，其中层作为局部坐标图，参数直接参数化黎曼度量张量，通过几何正则化增强泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络使用欧几里得参数空间，缺乏几何结构。NDM旨在将几何结构显式融入网络设计，提供内在正则化和更好的可解释性。

Method: 三层架构：坐标层实现可逆变换的平滑图表转换；几何层通过辅助子网络动态生成流形度量；演化层通过双目标损失函数优化任务性能和几何简洁性。

Result: 该框架支持与学习流形几何对齐的自然梯度下降优化，并为内部表示提供清晰的几何意义，实现前所未有的可解释性。

Conclusion: 神经微分流形代表了向几何结构化、可解释和高效深度学习系统的根本转变，虽然在计算挑战，但为优化效率、持续学习和科学发现应用提供了潜力。

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [41] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 本文提出了一个统一的对抗学习双层模型，从数据扰动角度解释聚类模型中的对抗攻击机制，并分析了δ-度量在测量攻击效果时的良好定义性。


<details>
  <summary>Details</summary>
Motivation: 由于大多数机器学习模型结构复杂，对抗攻击的机制尚未得到很好解释，如何衡量攻击效果仍不清楚。

Method: 提出统一的对抗学习双层模型，从数据扰动角度研究聚类模型中的对抗攻击，分析δ-度量的良好定义性。

Result: 发现当数据扰动较小时聚类模型具有鲁棒性，而扰动较大时聚类结果会改变从而导致攻击。

Conclusion: δ-度量可以用于所提出的聚类模型对抗学习双层模型中，有效衡量攻击效果。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [42] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出了一种用于双曲波传播物理数据的降维方法，使用低秩神经表示(LRNR)架构，能够从数据中学习波传播的低维高效表示，并揭示可解释的物理特征分解。


<details>
  <summary>Details</summary>
Motivation: 针对物理基础的双曲波传播数据，寻找高效的降维表示方法，利用理论证明的波类高效表示存在性来指导神经网络架构设计。

Method: 在超网络框架中使用专门的低秩神经表示(LRNR)架构，结合深度学习技术直接从数据中学习波传播的低维表示，利用低秩张量表示自然出现的特点。

Result: 训练后的LRNR中自然出现低秩张量表示，揭示了波传播的新分解方式，每个分解模式对应可解释的物理特征，并支持通过压缩方案进行高效推理。

Conclusion: LRNR架构能够有效学习波传播的低维表示，不仅提供物理可解释性，还具备高效推理能力，适用于性能要求较高的应用场景。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [43] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，将序列编码器与图神经网络结合，通过token级交叉注意力层实现细粒度的邻居间事件消息传递，在友谊预测和欺诈检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常同时具有序列性和关系性，但现有方法往往忽略其中一个模态。作者认为序列和图不是分离的问题，而是同一数据集的互补方面，应该联合学习。

Method: 提出BRIDGE架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两者间流动。添加TOKENXATTN层实现邻居序列间事件级别的细粒度消息传递。

Result: 在友谊预测（Brightkite）和欺诈检测（Amazon）两个设定中，BRIDGE在排序和分类指标上始终优于静态GNN、时序图方法和仅序列基线。

Conclusion: 序列和图应该联合建模，BRIDGE通过端到端耦合序列编码器和GNN，实现了更好的表示学习效果。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [44] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，将数据增强与因果推断相结合，证明当结果生成机制对数据增强选择不变时，数据增强可视为对治疗生成机制的干预，有助于减少隐藏混杂因素带来的因果效应估计偏差。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强主要用于i.i.d.设置下的正则化，但在存在未观测混杂因素时，工具变量通常难以获得。本文旨在探索数据增强在跨干预泛化中的应用潜力。

Method: 通过适当正则化基于工具变量的估计器，引入IV-like回归概念；将参数化数据增强建模为IV-like回归问题，并通过组合模拟最坏情况的数据增强应用。

Result: 理论分析和模拟实验表明，该方法在因果估计和泛化任务上优于简单数据增强，在有限样本情况下也表现良好。

Conclusion: 数据增强可以超越传统i.i.d.设置，在因果推断中作为工具变量的替代方案，有效缓解混杂偏差并提高跨干预的预测性能。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [45] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种Lipschitz感知的线性嫁接方法，通过将线性函数嫁接到非线性激活函数中来消除近似误差，从而获得更紧的局部Lipschitz常数，提高认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在神经网络验证中面临近似误差问题，这阻碍了获得紧的局部Lipschitz常数，而紧的Lipschitz常数对于认证鲁棒性至关重要。线性嫁接可以减少不稳定神经元数量，但缺乏理论分析解释其如何改善认证鲁棒性。

Method: 提出Lipschitz感知的线性嫁接方法，将线性函数嫁接到非线性激活函数中，消除主导的近似误差源。该方法基于理论分析，通过减少近似误差来收紧局部Lipschitz常数。

Result: 实验表明，将线性性嫁接到有影响力的激活函数中能够收紧l∞局部Lipschitz常数，并增强认证鲁棒性，即使没有进行认证训练。

Conclusion: 线性嫁接通过消除近似误差来收紧局部Lipschitz常数，从而改善认证鲁棒性。该方法提供了理论解释和实证验证，展示了在认证鲁棒性方面的有效性。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [46] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习引导的框架，用于快速解决最优停电问题，通过利用问题实例间的共享模式，在管理野火风险的同时减少负荷削减。


<details>
  <summary>Details</summary>
Motivation: 为了缓解野火风险，电力公司需要在高风险区域断电。最优停电问题是计算复杂的混合整数线性规划问题，需要在操作环境中快速频繁求解。由于特定电力系统的OPS实例具有共同结构但参数变化，这促使使用机器学习来利用实例间的共享模式。

Method: 开发了机器学习引导的框架，扩展了现有的ML引导MILP求解方法，同时整合了关于通电和断电线路数量的领域知识。

Result: 在大型现实的加州合成测试系统上的结果显示，所提出的ML引导方法比传统优化方法更快地产生高质量解决方案。

Conclusion: 机器学习引导的方法能够有效解决最优停电问题，在管理野火风险的同时提高求解效率。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [47] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于深度时间序列预测的选择性学习策略，通过双掩码机制筛选可泛化的时间步来避免过拟合


<details>
  <summary>Details</summary>
Motivation: 深度模型在时间序列预测中容易因噪声和异常值而严重过拟合，传统方法对所有时间步进行统一优化会学习不确定和异常的时间步

Method: 使用双掩码机制：不确定性掩码利用残差熵过滤不确定时间步，异常掩码使用残差下界估计排除异常时间步，只选择可泛化的时间步子集计算MSE损失

Result: 在8个真实数据集上的实验表明，该方法显著提升了主流深度模型的预测性能，Informer的MSE降低37.4%，TimesNet降低8.4%，iTransformer降低6.5%

Conclusion: 选择性学习策略能有效指导模型关注可泛化的时间步，避免学习不确定和异常的时间步，从而显著改善深度时间序列预测性能

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [48] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种基于自适应损失加权的成本敏感多类正无标签学习方法，通过为正向和推断负向损失分量分配不同的数据依赖权重，实现无偏风险估计。


<details>
  <summary>Details</summary>
Motivation: 在多类正无标签学习场景中，许多现有方法无法确保无偏风险估计，这限制了性能和稳定性。真实应用中标注可靠负样本困难或成本高。

Method: 在经验风险最小化框架下，为正向和从未标记混合数据中推断出的负向损失分量分配不同的数据依赖权重，使经验目标成为目标风险的无偏估计器。

Result: 在八个公共数据集上的广泛实验表明，在不同类别先验和类别数量下，相比强基线方法在准确性和稳定性方面都取得了持续提升。

Conclusion: 提出的自适应损失加权方法能够有效解决多类正无标签学习中的无偏风险估计问题，提高了分类性能和稳定性。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [49] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 提出BSFA框架，通过差异化缩放Hessian矩阵不同特征子空间中的参数更新，在主导子空间抑制更新以增强稳定性，在主体子空间放大更新以加速收敛。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化中存在根本性二分法：沿Hessian矩阵顶部特征方向的参数更新虽然幅度大但对损失减少贡献小，而正交方向的小幅度更新却驱动大部分学习进展。

Method: 引入BSFA框架，使用PCA对历史更新进行高效子空间估计，采用分块策略在参数块级别应用估计，差异化缩放不同子空间的更新分量。

Result: 在各种任务中展示加速效果，特别是在WikiText-103上预训练LLaMA-72M和在OpenWebText上预训练LLaMA-134M时，相比vanilla AdamW实现了约2倍的加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，通过智能地调节不同特征子空间中的参数更新，有效加速深度学习训练过程。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [50] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 提出了两种改进贝叶斯网络结构采样的技术：高效实现基本移动操作和通过预处理剪枝可能父集来加速求和计算


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络结构推断通常通过马尔可夫链采样进行，但现有方法在采样效率和计算成本方面存在改进空间

Method: 1. 高效实现添加、删除或反转单条弧的基本移动操作；2. 设计预处理方法剪枝可能父集，近似保持求和结果

Result: 实证研究表明，相比之前的方法，这些技术能带来显著的效率提升

Conclusion: 所提出的两种技术能有效提高贝叶斯网络结构采样的效率

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [51] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈原理的IB-Inspired Normalization (IBNorm)，通过有界压缩操作在保持训练稳定性的同时提升表征的信息含量，在理论和实验上均优于传统的方差中心化归一化方法。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法如BatchNorm、LayerNorm和RMSNorm主要关注方差中心化（零均值和单位方差），虽然能稳定训练但无法控制表征如何捕捉任务相关信息。

Method: 基于信息瓶颈原理，引入有界压缩操作，鼓励嵌入保留预测信息同时抑制无关变异性，在保持标准归一化稳定性和兼容性的前提下获得更具信息量的表征。

Result: 在大型语言模型（LLaMA、GPT-2）和视觉模型（ResNet、ViT）上，IBNorm持续优于BatchNorm、LayerNorm和RMSNorm，互信息分析证实了其优越的信息瓶颈行为。

Conclusion: IBNorm在理论和实验上均证明比方差中心化方法具有更高的信息瓶颈值和更紧的泛化边界，为深度学习归一化提供了新的有效方法。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [52] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 该论文通过敏感性分析的统一视角，研究神经网络在输入和参数层面的扰动响应，结合Lipschitz网络、随机平滑和曲率正则化来解决深度学习的稳定性和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型存在不稳定性和脆弱性：输入的微小变化可能严重影响预测结果，而优化过程可能受到尖锐损失曲面的阻碍。

Method: 采用Lipschitz网络约束输入敏感性，引入基于损失函数曲率的正则化技术，探索随机平滑作为增强决策边界鲁棒性的概率方法。

Result: 开发了统一框架，结合Lipschitz连续性、随机平滑和曲率正则化，提出了高效谱范数计算、新颖Lipschitz约束层和改进的认证程序等实用方法。

Conclusion: 通过敏感性分析的统一视角，为深度学习模型的稳定性、鲁棒性和泛化能力提供了理论和实践上的解决方案。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [53] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律发现，通过两级架构分别学习PDE的基本符号组件及其组合，并嵌入已知物理定律保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 建模复杂时空动态系统面临挑战，传统PDE难以从第一原理推导，现有数据驱动方法存在物理不一致性和数据需求大的问题，需要能系统整合部分物理知识的新方法。

Method: 采用两级分层架构：第一级学习PDE的基本符号组件，第二级学习其组合；基于自适应傅里叶神经算子构建框架，能捕获非局部依赖和高阶算子；通过计算图直接嵌入已知物理定律。

Result: 该框架在正向时空预测和逆向物理定律发现方面都有显著进展，提高了数据效率，保证了物理一致性，并能通过符号回归可解释地发现控制方程。

Conclusion: 分层物理嵌入学习框架为复杂时空动态系统的建模提供了新范式，通过结构化解耦已知和未知项，实现了物理一致的数据驱动建模和可解释的物理定律发现。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [54] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出了一种多目标强化学习算法，在最大化期望回报的同时，确保策略在目标状态上产生分散的边际状态分布。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法主要关注最大化期望回报，导致策略可能过度利用少数奖励源。但在许多自然场景中，需要学习既能达到高回报，又能均匀访问所有目标状态的策略。

Method: 提出基于离线RL的新算法，通过优化自定义奖励函数来学习高回报的策略混合，使边际状态分布在目标状态上分散。算法在每次迭代中基于当前策略混合计算奖励，并使用采样轨迹更新策略。

Result: 理论分析证明了算法在优化包含期望回报和状态分布分散度的自然目标函数时的收敛性能保证。在合成MDP和标准RL环境中的实验验证了算法的有效性。

Conclusion: 该工作为多目标强化学习问题提供了有效的解决方案，能够在保证高回报的同时实现目标状态访问的均匀分布。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [55] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出了一种基于循环矩阵和对角矩阵乘积的新型可逆线性层，显著降低了参数复杂度和计算复杂度，并在此基础上构建了CDFlow模型，在自然图像数据集上取得了良好的密度估计效果。


<details>
  <summary>Details</summary>
Motivation: 设计能够增强表达能力同时保持雅可比行列式和逆矩阵高效计算的可逆线性层是归一化流模型的关键挑战。

Method: 使用循环矩阵和对角矩阵的乘积分解来构建可逆线性层，利用快速傅里叶变换将矩阵求逆的时间复杂度从O(n³)降低到O(mn log n)，计算对数行列式的时间复杂度从O(n³)降低到O(mn)。

Result: 基于该层构建的CDFlow在自然图像数据集上实现了强大的密度估计，对具有周期性结构的数据建模效果显著，并显著加速了归一化流中的关键操作。

Conclusion: 提出的循环-对角分解方法为可逆线性层提供了一种高效且表达能力强的解决方案，为可扩展生成建模带来了实际效益。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [56] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 该论文提出了解决信息级联流行度预测中三个关键问题的方法：时间泄漏、特征贫乏数据集和计算效率低下，通过时间顺序分割策略、新数据集Taoke和轻量级框架CasTemp实现了无泄漏评估下的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测存在三个关键限制：时间泄漏导致评估不真实、数据集缺乏下游转化信号、图计算方法训练效率低下，这些问题限制了实际应用。

Method: 提出三方面解决方案：时间顺序数据分割策略防止未来信息泄漏；构建包含丰富属性和购买转化的电商数据集Taoke；开发轻量级框架CasTemp，使用时间游走、Jaccard邻居选择和GRU编码建模级联动态。

Result: 在无泄漏评估下，CasTemp在四个数据集上达到最优性能，训练速度提升数个数量级，特别擅长预测第二阶段的流行度转化这一实际关键任务。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个层面的问题，实现了更真实、实用且高效的信息级联流行度预测，为实际应用提供了可靠解决方案。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [57] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文对随机几何超图上的变分学习进行了渐近一致性分析，提出了高阶超图学习(HOHL)方法，通过骨架图的拉普拉斯幂进行多尺度平滑正则化。


<details>
  <summary>Details</summary>
Motivation: 超图为建模高阶交互提供了自然框架，但在半监督学习中的理论基础仍然有限，需要确保超图学习的适定性并分析其收敛性。

Method: 提出了高阶超图学习(HOHL)，通过骨架图的拉普拉斯幂进行正则化以实现多尺度平滑，该方法收敛到高阶Sobolev半范数。

Result: 理论分析表明超图学习收敛到加权p-拉普拉斯方程，实证结果显示HOHL在标准基准测试中表现优异。

Conclusion: 该研究为超图学习提供了坚实的理论基础，提出的HOHL方法在理论和实证上都表现出色，为高阶交互建模提供了有效工具。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [58] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 该论文提出在知识图谱嵌入模型中使用模型合并方法（特别是加权平均）来替代传统的集成学习方法，以降低计算开销并提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法需要训练多个模型，导致计算开销大、延迟高和内存占用多。模型合并方法提供了一种有前景的替代方案，无需训练多个模型。

Method: 提出了两种加权平均方法：1）从训练周期开始维护模型参数的运行平均值；2）仅在验证集上泛化性能改善时选择性更新集成模型参数的运行平均值。

Result: 在链接预测任务中，与最先进的基准集成方法相比，提出的加权平均方法一致地提升了性能，并在多种评估设置中表现良好。

Conclusion: 模型合并方法（特别是加权平均）是知识图谱嵌入模型集成学习的有效替代方案，能够降低计算开销同时提高性能。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [59] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出了一种基于损失函数从初始非凸性向最优解附近凸性转换假设的两阶段优化算法，通过检测转换点来分别使用Adam和共轭梯度法，显著提升了收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习中损失函数经常存在非凸区域，导致广泛使用Adam等非凸方法。但局部最小值附近函数是凸的，可以利用二阶方法获得超线性收敛。假设实际任务中损失函数会从初始非凸性向最优解附近的凸性转换。

Method: 设计两阶段优化算法：通过观察梯度范数与损失的关系来检测凸性转换点，在非凸区域使用Adam算法，在凸区域使用共轭梯度法。

Result: 计算实验证实了这种简单凸性结构在实际中足够常见，能够被有效利用来显著改善收敛性和准确性。

Conclusion: 利用损失函数从非凸到凸的转换特性，提出的两阶段优化框架能够有效提升机器学习模型的优化性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [60] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: 该论文提出生物学信息机器学习(BIML)，作为物理信息机器学习(PIML)的扩展，旨在应对生物建模中的独特挑战，如不确定的先验知识、异构数据和复杂网络。


<details>
  <summary>Details</summary>
Motivation: 将PIML成功应用于物理领域的经验激励了其在生物学中的应用，但生物建模面临多方面的挑战，需要新的方法。

Method: 提出BIML框架，基于四个基础支柱：不确定性量化、情境化、约束潜在结构推断和可扩展性，利用基础模型和大型语言模型作为关键赋能工具。

Result: BIML为PIML在生物学中的应用提供了原则性扩展，能够处理软性、概率形式的先验知识。

Conclusion: BIML不应被视为PIML的替代，而是其适应生物学现实的重构，建议建立BIML生态系统以推动科学和社会相关挑战的创新。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [61] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 该论文研究了学习函数空间之间映射（算子）的问题，提出了多算子学习和单算子学习两种机制，并开发了新的网络架构和理论分析框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习中大多数问题关注有限维空间之间的映射学习，但科学应用需要近似函数空间之间的算子映射。本文旨在为学习算子集合提供理论和实践基础。

Method: 提出了两种新架构MNO和MONet用于多算子学习，建立了在连续、可积和Lipschitz算子情况下的通用逼近理论。对于单算子学习，开发了平衡子网络架构复杂度的框架。

Result: 理论分析表明新架构具有通用逼近能力，并推导了网络规模与逼近精度之间的显式缩放规律。在参数化PDE基准测试中的实验证实了所提架构的强大表达能力和效率。

Conclusion: 这项工作为跨多个算子的可扩展神经算子学习建立了统一的理论和实践基础，证明了所提方法在科学计算应用中的有效性。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [62] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大语言模型的优化方法，通过在多样化的贝叶斯优化参数化合成数据集上微调LLM，使其具备连续黑盒优化能力，无需参数调整即可超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 解决昂贵、无导数黑盒函数全局优化所需的极高样本效率问题，同时克服传统贝叶斯优化需要针对每个应用领域进行参数调优的局限性。

Method: 通过在大规模合成数据集上微调大语言模型，这些数据集来自多样化的贝叶斯优化参数化配置，利用LLM的预训练能力实现跨优化任务的泛化。

Result: 在各种黑盒优化基准测试中，GPTOpt超越了传统优化器，展示了LLM在高级数值推理方面的能力。

Conclusion: GPTOpt为全局优化提供了一个无需参数调优的灵活框架，证明了LLM在连续黑盒优化任务中的潜力。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [63] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出了效用校准框架，通过特定效用函数评估多类分类器的校准误差，统一并改进了现有校准指标。


<details>
  <summary>Details</summary>
Motivation: 现有多类校准评估方法要么关注预测的特定方面（如top-class置信度、类级校准），要么使用计算复杂的变分公式，需要更可扩展的评估框架。

Method: 提出效用校准框架，通过定义反映最终用户目标的效用函数来测量校准误差，能够统一现有校准指标并支持更丰富的下游效用评估。

Result: 该框架能够统一和重新解释现有校准指标，特别是允许构建更稳健的top-class和类级校准指标，并支持评估更丰富的下游效用类别。

Conclusion: 效用校准提供了一个通用且可扩展的框架来评估多类校准，超越了传统的二值化方法，能够更好地满足实际应用需求。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [64] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 本文提出了梯度-权重对齐（GWA）指标，通过量化每个样本梯度与模型权重之间的一致性来跟踪泛化性能，无需验证集即可实现早期停止、模型比较和识别有影响力的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在深度学习领域，需要稳健的验证指标来检测过拟合、监控训练动态。本文研究训练数据与模型权重之间的相互作用是否能产生既能跟踪泛化性能又能归因个体样本贡献的指标。

Method: 引入梯度-权重对齐（GWA）方法，量化每个样本梯度与模型权重之间的一致性。有效学习对应一致的对齐，而错位表示泛化性能下降。该方法可在训练期间高效计算。

Result: 大量实验表明，GWA能准确预测最优早期停止点，实现基于原则的模型比较，并识别有影响力的训练样本，提供了一种无需验证集的模型分析方法。

Conclusion: GWA提供了一种直接从训练数据中分析模型的验证集无关方法，能够有效跟踪泛化性能并识别样本贡献。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [65] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 本文提出原型神经符号架构，通过原型学习理论解决神经符号AI中的推理捷径问题，确保模型基于正确概念而非伪相关进行推理。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI容易学习推理捷径，即利用伪相关性满足符号约束而非学习正确概念，这在高风险应用中存在安全隐患。

Method: 采用原型学习方法，训练模型在满足背景知识的同时考虑输入与少量标注数据的相似性，从而避免推理捷径。

Result: 在rsbench基准测试中，包括合成任务和真实高风险任务，该方法在极低数据量下显著改善了正确概念的学习效果。

Conclusion: 原型接地是一种有效且标注高效的策略，为安全可靠的神经符号学习开辟了新途径。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [66] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN是一个基于线性循环神经网络的单变量时间序列基础模型，仅使用合成数据预训练，在零样本时间序列预测中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本时间序列预测基础模型在高效长时预测和可复现性方面面临挑战，现有仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。

Method: 使用带有状态编织的GatedDeltaProduct架构的线性RNN，通过完全并行化训练跨序列长度，无需窗口化或汇总技术，同时保持稳健的时间状态跟踪。构建了统一的合成数据流水线，包含随机微分方程、高斯过程和音频合成等多种生成器。

Result: 在Gift-Eval基准测试的零样本评估中，TempoPFN实现了顶级竞争性能，超越了所有现有的仅使用合成数据的方法，并超过了绝大多数使用真实世界数据训练的模型，同时通过完全并行化训练和推理比现有基线更高效。

Conclusion: TempoPFN为时间序列预测提供了一个可复现的基础模型框架，开源了完整的数据生成流水线和训练代码，为未来研究奠定了基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [67] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法预测员工倦怠风险，在三种算法中SVM表现最佳（R²=0.84），并开发了交互界面供非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 倦怠综合征严重影响个人福祉和组织绩效，需要早期检测和干预。

Method: 使用HackerEarth员工倦怠挑战数据集，评估KNN、随机森林和SVM三种监督学习算法，通过30折交叉验证评估模型性能。

Result: SVM模型表现最佳（R²=0.84），在统计上显著优于KNN和随机森林，并开发了基于Streamlit的交互界面。

Conclusion: 机器学习在组织环境中支持倦怠早期检测和数据驱动的心理健康策略方面具有潜力。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [68] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出了一种具有模型内在机制概念解释的新模型，强调概念解释的忠实性，概念在类别间共享，并能从任何层忠实追踪其对logit的贡献和输入可视化。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的后处理方法在理解深度网络工作原理时存在忠实性问题，且对模型学习的概念做出限制性假设（如类别特异性、小空间范围或与人类期望对齐）。

Method: 开发了具有模型内在机制概念解释的新模型，概念在类别间共享，能从任何层忠实追踪其对logit的贡献和输入可视化，并利用基础模型提出了新的概念一致性度量C²-Score。

Result: 与先前工作相比，该方法的概念在数量上更一致，用户认为概念更具可解释性，同时在ImageNet上保持竞争力。

Conclusion: 提出的模型内在概念解释方法能够提供更忠实和一致的概念理解，同时保持模型性能。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [69] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了一种基于核引导互信息(KG-MI)的新目标函数，结合多头注意力机制，能够在多项式时间内收敛到全局最优解，从序列数据中恢复有向无环图(DAG)的底层结构。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力的图结构学习方法仅限于树状图，难以扩展到具有多个父节点的通用DAG。主要挑战在于设计训练目标，使不同的注意力头能够分别学习多个不同的父子关系。

Method: 引入基于f-散度的核引导互信息(KG-MI)度量，结合多头注意力框架，每个头与不同的边缘转移核相关联，有效建模多样化的父子依赖关系。

Result: 理论证明：对于K-父节点DAG生成的序列，通过梯度上升训练单层多头transformer能在多项式时间内收敛到全局最优。当f-散度特化为KL散度时，学习到的注意力分数准确反映真实邻接矩阵。

Conclusion: 该方法为从序列数据中恢复通用DAG结构提供了理论保证，实验验证了理论发现，扩展了transformer在图结构学习中的应用范围。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [70] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典循环神经网络(QRNN)，其中整个循环核心由参数化量子电路实现，隐藏状态存在于指数级大的希尔伯特空间中，通过酉动力学更新状态。


<details>
  <summary>Details</summary>
Motivation: 构建一个物理一致且紧凑的量子循环神经网络，统一了酉循环作为高容量记忆、通过中间电路测量的部分观测以及非线性经典控制用于输入条件参数化。

Method: 使用参数化量子电路作为循环核心，隐藏状态是n量子比特PQC的量子态，通过中间电路读数和前馈网络提供经典非线性，PQC通过酉动力学更新隐藏状态。

Result: 在情感分析、MNIST、置换MNIST、复制记忆和语言建模等任务上进行了评估，在序列到序列模型中设计了软注意力机制，在机器翻译中表现出有效性，在广泛的序列学习任务中取得了与强经典基线竞争的性能。

Conclusion: 这是第一个基于量子操作并在广泛序列学习任务中取得与强经典基线竞争性能的模型，展示了量子循环神经网络在序列学习中的潜力。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [71] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: 本研究将大气预测模型Aurora迁移到海洋领域，用于预测加那利上升流系统的海表温度，通过精细化调优实现了低计算成本下的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 传统海洋数值模型计算成本高且可扩展性有限，需要开发更高效的数据驱动预测方法。

Method: 采用分阶段精细化调优策略，结合纬度加权误差指标和超参数优化，将预训练的大气预测模型Aurora适配到海洋温度预测任务。

Result: 模型在SST预测中达到0.119K的RMSE和约0.997的异常相关系数，能准确捕捉大尺度温度结构，但在沿海区域细节预测方面存在挑战。

Conclusion: 证明了深度学习模型在不同领域间迁移的可行性，为数据驱动的海洋预测开辟了新途径，未来可集成更多变量并提高分辨率。

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [72] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出一个统一框架，从随机PAC-Bayesian保证中提取单个假设的保证，解决了经典PAC-Bayes只能提供随机采样假设期望风险保证的问题。


<details>
  <summary>Details</summary>
Motivation: 经典PAC-Bayes框架只提供随机采样假设的期望风险保证，需要随机预测，无法在实际部署单个确定性假设时使用。

Method: 提出统一框架，包括一般oracle边界、数值边界和多数投票特化方法，从随机PAC-Bayesian保证中提取单个确定性假设的保证。

Result: 经验表明该方法在确定性分类器的泛化边界方面始终优于流行基线（最高可达2倍）。

Conclusion: 该框架成功地将随机PAC-Bayesian保证转化为单个确定性假设的保证，解决了实际部署中的限制问题。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [73] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 本文系统研究了低秩伪逆在噪声环境下的谱范数鲁棒性，推导了尖锐的非渐近扰动界，揭示了误差如何随特征间隙、谱衰减和噪声对齐而缩放。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵通常带有噪声，但低秩逆近似的谱范数鲁棒性仍缺乏深入理解，需要系统研究噪声对低秩逆近似的影响。

Method: 引入轮廓积分技术的新应用来处理非全纯函数f(z)=1/z，在温和的噪声假设下推导非渐近扰动界。

Result: 提出的边界比经典全逆边界的朴素适应改进高达√n倍，经验上能紧密跟踪真实扰动误差，而基于经典结果的估计往往显著高估。

Conclusion: 研究结果为噪声计算环境中的低秩逆近似提供了实用的、谱感知的保证，改进了对低秩伪逆鲁棒性的理解。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [74] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Orlicz几何结构的广义Sobolev IPM方法，通过Musielak正则化将复杂优化问题简化为单变量优化，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有Sobolev IPM方法受限于L^p几何结构，无法融入其他结构先验。为了克服这一限制，需要发展能够容纳多样化几何先验的广义框架。

Method: 通过Orlicz几何结构推广Sobolev IPM，建立Orlicz-Sobolev范数与Musielak范数的理论联系，并利用图结构将优化问题简化为单变量优化。

Result: 提出的GSI-M方法比流行的Orlicz-Wasserstein计算快几个数量级，在文档分类和拓扑数据分析任务中表现出实际优势。

Conclusion: 该研究成功克服了传统Sobolev IPM的计算瓶颈，提供了一种高效灵活的度量学习方法，能够适应多样化的几何结构先验。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [75] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出了一种基于SVD分解的结构化局部学习框架，在低秩流形上训练深度神经网络，减少可训练参数数量，同时保持与反向传播相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播全局误差传播和全参数化带来的高内存计算开销，以及直接反馈对齐方法在深层架构中反馈结构不明确和扩展性差的问题。

Method: 在SVD分解的权重矩阵低秩流形上训练，对SVD分量应用包含交叉熵、子空间对齐和正交正则化的复合损失函数，构建与SVD结构匹配的反馈矩阵。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上达到与反向传播相当的准确率，同时减少了可训练参数数量。

Conclusion: 低秩流形上的局部学习为全秩梯度训练提供了原理性和可扩展的替代方案。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [76] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出基于核评分规则的不确定性度量框架，统一了多种现有方法，并为设计具有特定行为特征的新度量提供了原则性指导。


<details>
  <summary>Details</summary>
Motivation: 回归任务在安全关键领域需要准确的不确定性量化，但现有研究主要关注分类问题。

Method: 基于核评分规则构建了总不确定性、偶然不确定性和认知不确定性的度量族，通过核函数选择控制度量的尾部敏感性、鲁棒性和分布外响应等行为特征。

Result: 实验证明这些度量在下游任务中有效，并揭示了不同实例化之间的权衡关系，包括鲁棒性和分布外检测性能。

Conclusion: 该框架为回归任务的不确定性量化提供了统一的理论基础和实用的设计指南，能够根据具体任务需求定制合适的度量方法。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [77] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了FP和INT量化格式在不同粒度下的性能，发现在8位细粒度量化中MXINT8优于FP格式，而在4位量化中FP格式通常具有精度优势，但通过异常值缓解技术NVINT4可以超越NVFP4。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件越来越多采用低精度FP格式处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式的权衡，包括在不同粒度下的性能比较，引入对称裁剪方法解决细粒度低比特INT训练中的梯度偏差问题。

Result: 发现关键性能交叉点：FP在粗粒度量化中表现优异，但在细粒度（块级）量化中MXINT8在算法精度和硬件效率上都优于FP对应格式；对于4位格式，FP通常具有精度优势，但应用Hadamard旋转等异常值缓解技术后NVINT4可以超越NVFP4。

Conclusion: 挑战当前硬件发展趋势，证明一刀切的FP方法不是最优选择，主张细粒度INT格式（特别是MXINT8）为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [78] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是基于贝叶斯最优学习阈值(BOLT)对WGAN框架的简单有效改进，使用Lipschitz连续判别器，在四个标准图像生成基准上比WGAN获得10-60%更低的FID分数。


<details>
  <summary>Details</summary>
Motivation: 改进WGAN框架的训练稳定性，通过BOLT原则增强GAN训练效果。

Method: 在WGAN框架中引入贝叶斯最优学习阈值(BOLT)，使用Lipschitz连续判别器，隐式最小化不同于Wasserstein距离的度量距离。

Result: 在CIFAR-10、CelebA-64、LSUN Bedroom-64和LSUN Church-64四个基准测试中，BOLT-GAN始终优于WGAN，FID分数降低10-60%。

Conclusion: BOLT是一个广泛适用于增强GAN训练的原则，BOLT-GAN在训练稳定性和生成质量方面都有显著提升。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [79] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文系统研究了VLA模型在动作微调过程中的表征保留问题，发现简单的动作微调会导致视觉表征退化，并提出了一种简单有效的方法来缓解这种退化并提高对分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型的成功，人们期望预训练的VLM能够为智能体提供可迁移的世界知识和视觉语言基础，但尚不清楚在适应动作模态时，这些VLM的原始视觉语言表征和知识能在多大程度上得到保留。

Method: 通过探测VLA的隐藏表征和分析注意力图来表征和测量这些效应，设计了一组针对性任务和方法来对比VLA模型与其对应的VLM，分离出动作微调引起的视觉语言能力变化，并评估了一系列视觉表征对齐策略。

Result: 研究发现简单的动作微调会导致视觉表征退化，但提出的简单方法能够有效缓解这种退化，并在分布外场景中展现出改进的泛化能力。

Conclusion: 该分析阐明了动作微调与视觉语言表征退化之间的权衡关系，并强调了恢复继承的视觉语言能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [80] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出FedLap框架，通过拉普拉斯平滑在谱域中利用全局结构信息，解决联邦学习中图结构数据的隐私和可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习图数据处理方法存在隐私风险（需要交换敏感节点嵌入）或计算复杂度高的问题，特别是在互联子图场景下

Method: 使用拉普拉斯平滑在谱域中捕获节点间依赖关系，确保隐私保护和可扩展性

Result: 在基准数据集上的实验表明，FedLap在效用上达到或优于现有技术，且具有强隐私保证

Conclusion: FedLap是首个具有强隐私保证的子图联邦学习方案，有效平衡了隐私、可扩展性和性能

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [81] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 本文建立了对称矩阵谱范数扰动的新边界，改进了经典的Eckart-Young-Mirsky定理，并在差分隐私PCA中实现了更好的效用保证。


<details>
  <summary>Details</summary>
Motivation: 机器学习中需要理解噪声如何影响低秩近似，特别是在谱范数下。传统方法使用Frobenius范数误差可能高估或低估真实子空间失真，而谱范数能捕捉最坏情况的方向误差并提供最强的效用保证。

Method: 采用复分析中的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱函数类，建立了高概率谱范数扰动边界。

Result: 在温和的特征间隙和范数条件下，新边界对‖(A+E)_p - A_p‖给出了锐利估计，改进因子高达√n。实证结果表明新边界能紧密跟踪不同扰动机制下的实际谱误差。

Conclusion: 新边界改进了经典理论，解决了差分隐私PCA中的开放性问题，并为谱函数类提供了更精确的扰动分析框架。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [82] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: 该研究展示了循环神经网络如何通过噪声维持的轨道动力学来模拟隐马尔可夫模型的离散状态转换，揭示了RNN通过"踢神经元"和随机共振机制实现概率计算的新机制。


<details>
  <summary>Details</summary>
Motivation: 过去RNN研究主要关注简单、确定性行为，而自然行为往往具有自发性和随机性。HMM模型揭示了行为被分割为离散潜在状态，这种离散动力学与RNN的连续状态空间看似矛盾，需要探索RNN如何实现这种复杂行为。

Method: 首先训练RNN复制HMM的发射统计特性，然后对训练好的网络进行逆向工程分析。研究在无输入和有随机输入条件下分析RNN的动力学特性，并考察了多种HMM架构（全连接、循环、线性链）。

Result: 训练后的RNN在无输入时活动会坍缩到单个固定点；在随机输入驱动下，轨迹表现出噪声维持的闭合轨道动力学。网络发展出高度结构化的连接性，少数"踢神经元"负责在不同区域间发起转换。这种机制通过随机共振实现概率计算。

Conclusion: RNN能够通过模块化重用相同的动力学基元来模拟复杂的离散潜在动力学，这为理解神经网络如何实现概率计算提供了新的组成性原则。

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [83] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: 提出了基于图神经网络的结构模拟器GNSS，用于动态结构问题的代理建模，在波传播主导的结构模拟中表现出色


<details>
  <summary>Details</summary>
Motivation: 图神经网络在计算流体力学中已有应用，但在结构问题特别是动态案例中研究较少，需要填补这一空白

Method: 采用编码-处理-解码范式，包含三个关键特性：节点固定局部坐标系、符号感知回归损失、波长感知连接半径

Result: 在50kHz脉冲激励的梁案例中，GNSS能准确再现数百个时间步的物理特性，并泛化到未见过的载荷条件，相比有限元基准获得显著推理加速

Conclusion: 具有物理一致性更新规则的局部保持图神经网络是动态波主导结构模拟的竞争性替代方案

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [84] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出卷积脉冲GRU（CS-GRU）单元，结合卷积操作和脉冲神经元，在保持局部结构的同时提升时序数据处理性能，在多个数据集上优于现有GRU变体。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时会丢失局部细节，现有方法如SpikGRU无法捕捉基于事件的时空数据中的细粒度局部依赖关系。

Method: 引入卷积脉冲GRU单元，利用卷积操作保持局部结构和依赖关系，同时整合脉冲神经元的时间精度和GRU的高效门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，比SpikGRU效率高69%。

Conclusion: CS-GRU是一个多功能架构，在保持局部细节的同时有效处理时序和时空数据，显著优于现有方法。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [85] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了一种利用李对称性精确求解初边值问题的方法，通过对称变换构建模型，直接学习初始和边界数据，比PINNs更快更准确。


<details>
  <summary>Details</summary>
Motivation: 传统方法在求解偏微分方程初边值问题时存在效率低和精度不足的问题，需要一种能够精确满足物理定律且收敛性更好的方法。

Method: 利用李对称性变换构建模型，通过对称性自动满足偏微分方程约束，直接从初始和边界数据学习解，损失函数直接衡量模型精度。

Result: 开发的LieSolver在线性齐次偏微分方程上表现优异，比物理信息神经网络更快更准确，并能进行严格的误差估计。

Conclusion: 该方法显著提高了偏微分方程约束问题的计算效率和预测可靠性，能够生成紧凑模型并实现高效优化。

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [86] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: MLPrE是一个基于SparkDataFrames的机器学习预处理和探索性数据分析工具，使用JSON配置文件描述数据处理步骤，包含69个处理阶段，能够处理多种数据格式并支持大规模数据处理。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习需求的增长，现有工具在数据预处理方面存在开销大、可扩展性差的问题，无法很好地集成到大型处理管道中，需要开发一个健壮、可扩展且轻量级的预处理工具。

Method: 利用SparkDataFrames存储处理数据确保可扩展性，采用通用JSON输入文件格式描述对DataFrame的逐步更改，实现了输入输出、过滤、基础统计、特征工程和探索性数据分析等处理阶段。

Result: 成功实现了69个处理阶段，并在六个不同数据集上验证了关键阶段的功能，展示了处理平面文件中多个字段并重新组合的能力，以及为图数据库准备数据的能力。

Conclusion: MLPrE提供了一个通用且可扩展的预处理和早期数据分析工具，填补了机器学习应用中此类工具的关键空白，能够加速和简化大型工作流中的早期开发阶段。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [87] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 论文指出传统多示例学习方法在处理医学图像时忽略实例间的上下文关系，设计了一个合成分类任务来验证这一点，并展示了现有方法相比贝叶斯最优估计器的性能差距。


<details>
  <summary>Details</summary>
Motivation: 传统多示例学习方法在医学图像分类中处理图像块或切片时，忽略了相邻实例之间的上下文关系，而这些关系在实际应用中可能至关重要。

Method: 设计了一个合成分类任务，其中相邻实例的特征对准确预测至关重要，并将现有多示例方法与可得到闭式解的最优贝叶斯估计器进行性能比较。

Result: 实证研究表明，即使是较新的相关多示例方法，在从数万个实例从头开始训练时，仍然难以达到最优的泛化性能。

Conclusion: 当前的多示例学习方法在处理需要利用实例间上下文关系的任务时存在局限性，需要进一步改进以更好地捕捉这些重要关系。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [88] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了神经随机流（NSF）及其潜在变体，通过条件归一化流直接学习SDE转移规律，实现任意时间点的一步采样，相比传统数值方法获得两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统SDE建模需要昂贵的数值求解器在任意时间点采样，计算成本高。

Method: 使用具有架构约束的条件归一化流直接学习SDE转移规律，保持随机流的数学特性。

Result: 在合成SDE模拟和真实世界跟踪、视频数据上，NSF在保持分布精度可比的同时，大幅减少了任意时间点采样的计算量。

Conclusion: NSF方法能够高效准确地建模SDE，为处理噪声和不规则采样时间序列提供了有效解决方案。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>
