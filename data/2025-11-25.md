<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.LG](#cs.LG) [Total: 214]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Pier: Efficient Large Language Model pretraining with Relaxed Global Communication](https://arxiv.org/abs/2511.17849)
*Shuyuan Fan,Zhao Zhang*

Main category: cs.DC

TL;DR: Pier是一个高效的分布式优化器，通过放松全局通信来加速大语言模型预训练，在保持模型性能的同时实现显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 全局通信（如all-reduce和allgather）是大语言模型预训练中的主要性能瓶颈，需要设计更高效的优化器来减少通信开销。

Method: 基于DiLoCo框架，在处理器组内使用内部优化器，全局使用需要通信的外部优化器，并引入动量预热和动量衰减技术来保持收敛性。

Result: 在256个A100 GPU上，Pier将GPT-2 XL训练速度提升2.7x-3.7x；在64个GH200 Superchips上提升1.2x-1.9x；在128个A100上使用数据并行和张量并行时，GPT-2 7B训练时间减少54.5%。

Conclusion: Pier通过放松全局通信和有效的系统架构，在大规模分布式训练中实现了显著的性能提升，且不损害模型质量。

Abstract: Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.

</details>


### [2] [SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs](https://arxiv.org/abs/2511.17882)
*Ruide Cao,Zhuyun Qi,Qinyang He,Chenxi Ling,Yi Wang,Guoming Tang*

Main category: cs.DC

TL;DR: SAGkit是一个Python工具包，实现了基于调度抽象图(SAG)框架的精确可持续响应时间分析，特别适用于具有释放抖动和执行时间变化的非抢占式分布式控制系统。


<details>
  <summary>Details</summary>
Motivation: 现代延迟关键应用对实时性和鲁棒性要求越来越高，但传统响应时间分析方法在处理非抢占式系统中释放抖动和执行时间变化时面临状态空间爆炸问题。

Method: 开发了SAGkit工具包，实现调度抽象图(SAG)框架，通过在SAG基础上允许作业缺席，支持混合触发作业的精确可持续响应时间分析。

Result: 实验表明SAGkit在可接受的运行时和内存开销下实现了精确性，能够有效分析复杂分布式控制系统。

Conclusion: SAGkit作为一个轻量级开源工具包，为研究人员分析复杂分布式控制系统提供了有效支持，并具有进一步开发的潜力。

Abstract: For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.

</details>


### [3] [MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale](https://arxiv.org/abs/2511.18124)
*Sangam Ghimire,Nigam Niraula,Nirjal Bhurtel,Paribartan Timalsina,Bishal Neupane,James Bhattarai,Sudan Jha*

Main category: cs.DC

TL;DR: MIDAS是一个自适应中间件层，通过命名空间感知负载均衡、协作缓存层和自稳定控制循环来解决元数据热点问题，显著改善系统性能。


<details>
  <summary>Details</summary>
Motivation: 元数据热点是高性能计算和云存储环境中可扩展I/O的主要障碍，会导致长队列、尾部延迟增加和系统吞吐量降低。现有解决方案存在过于僵化、部署侵入性强或在变化工作负载下不稳定的问题。

Method: MIDAS采用三种机制：1) 命名空间感知负载均衡器，结合一致性哈希和基于实时遥感的功率采样；2) 协作缓存层，通过租约、失效或自适应超时保持后端语义；3) 自稳定控制循环，动态调整路由攻击性和缓存生命周期，避免突发工作负载下的振荡。

Result: 模型分析和受控实验显示，MIDAS相比轮询调度平均队列长度减少约23%，最坏情况热点缓解达80%。

Conclusion: 基于中间件的稳定性感知策略可以为元数据管理提供后端无关的改进，在突发场景下实现更好的可扩展性、更可预测的尾部延迟和更强的整体系统性能。

Abstract: Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.

</details>


### [4] [Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus](https://arxiv.org/abs/2511.18137)
*Christoph Goldgruber,Benedikt Pittl,Erich Schikuta*

Main category: cs.DC

TL;DR: 扩展CloudSim Plus模拟框架以支持动态云定价环境，并验证HLEM-VMP算法在波动工作负载下的性能表现


<details>
  <summary>Details</summary>
Motivation: 公共云环境中动态定价模型（如竞价实例）的普及带来了新的调度和可靠性挑战，现有算法和模拟工具未能充分处理这些波动性和不确定性

Method: 扩展CloudSim Plus模拟框架支持竞价实例生命周期管理，包括中断、终止、休眠和重新分配，基于Google Cluster Trace数据集进行大规模模拟验证，并评估HLEM-VMP算法在动态竞价市场条件下的性能

Result: 与基线分配策略相比，HLEM-VMP算法减少了竞价实例中断次数和最大中断持续时间

Conclusion: 本研究提供了模拟动态云行为的框架，并对虚拟机分配性能和市场风险提供了分析见解，有助于实现更稳健和成本效益的云计算资源管理

Abstract: The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.

</details>


### [5] [AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems](https://arxiv.org/abs/2511.18151)
*Rajat Bhattacharjya,Sing-Yao Wu,Hyunwoo Oh,Chaewon Nam,Suyeon Koo,Mohsen Imani,Elaheh Bozorgzadeh,Nikil Dutt*

Main category: cs.DC

TL;DR: AVERY是一个用于无人机灾难响应的自适应分割计算框架，通过将视觉语言模型分为实时上下文流和深度分析流，在低带宽条件下实现高效的VLM部署。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾难响应中需要复杂的可查询智能，但现有方法无法满足：机载CNN缺乏语义推理能力，VLM资源需求过高无法在设备上部署，云端卸载在灾难区低带宽网络中失效。

Method: 提出认知启发的双流分割方法，将VLM分为高频低分辨率的上下文流（实时感知）和低频高保真的洞察流（深度分析），通过轻量级自感知控制器动态选择压缩模型。

Result: 在边缘-云场景下，AVERY比静态配置表现更好：比原始图像压缩准确率高11.2%，比全边缘执行能耗降低93.98%。

Conclusion: AVERY框架能够在动态环境中为资源受限平台提供实时可查询智能，显著提升任务效率。

Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.

</details>


### [6] [Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents](https://arxiv.org/abs/2511.18315)
*Rajashree Bar,Daibik Barik,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 研究动态图中的单调去污问题，提出了两种动态性模型，给出了所需智能体数量的上下界，并分析了边突然消失或重现带来的困难。


<details>
  <summary>Details</summary>
Motivation: 网络去污问题在静态图中已有研究，但在动态图中尚未探索。本文旨在研究任意动态图中的单调去污问题，优化所需的智能体数量。

Method: 设计了两种基于边消失后重现时间的动态性模型，提出了每种模型中完全单调去污所需智能体数量的下界和上界。

Result: 在两种动态性模型中分别给出了所需智能体数量的理论界限，揭示了边动态变化带来的挑战。

Conclusion: 本文首次系统研究了动态图中的单调去污问题，为动态网络中的污染控制提供了理论基础和算法框架。

Abstract: Network decontamination is a well-known problem, in which the aim of the mobile agents should be to decontaminate the network (i.e., both nodes and edges). This problem comes with an added constraint, i.e., of \emph{monotonicity}, in which whenever a node or an edge is decontaminated, it must not get recontaminated. Hence, the name comes \emph{monotone decontamination}. This problem has been relatively explored in static graphs, but nothing is known yet in dynamic graphs. We, in this paper, study the \emph{monotone decontamination} problem in arbitrary dynamic graphs. We designed two models of dynamicity, based on the time within which a disappeared edge must reappear. In each of these two models, we proposed lower bounds as well as upper bounds on the number of agents, required to fully decontaminate the underlying dynamic graph, monotonically. Our results also highlight the difficulties faced due to the sudden disappearance or reappearance of edges. Our aim in this paper has been to primarily optimize the number of agents required to solve monotone decontamination in these dynamic networks.

</details>


### [7] [An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds](https://arxiv.org/abs/2511.18906)
*Marco Zambianco,Lorenzo Fasol,Roberto Doriguzzi-Corin*

Main category: cs.DC

TL;DR: 提出了一种针对NVIDIA MIG GPU云平台的调度框架，通过最小化碎片化来最大化工作负载接受率


<details>
  <summary>Details</summary>
Motivation: MIG技术的固定分区导致GPU碎片化严重，降低了多租户环境下的资源利用率和工作负载容纳能力

Method: 引入碎片化度量指标，基于贪心算法选择GPU和MIG切片以最小化碎片化增长

Result: 在重负载条件下平均提高10%的工作负载调度数量，同时使用与基准方法相近的GPU数量

Conclusion: 该调度框架能有效缓解MIG GPU碎片化问题，显著提升云平台的资源利用效率

Abstract: The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.

</details>


### [8] [AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones](https://arxiv.org/abs/2511.19192)
*Xinkui Zhao,Qingyu Ma,Yifan Zhang,Hengxuan Lou,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.DC

TL;DR: AME是一个专为智能手机SoC设计的设备端智能体内存引擎，解决了现有向量数据库在移动设备上的性能瓶颈，通过硬件感知的矩阵流水线和智能调度机制，显著提升了查询吞吐量、索引构建速度和并发插入性能。


<details>
  <summary>Details</summary>
Motivation: 设备端智能体需要持续演化的内存来支持个性化、上下文感知和长期行为，但现有向量数据库主要针对服务器环境，在移动设备上存在硬件约束不匹配和工作负载不匹配的问题。

Method: 提出AME系统，包含两个关键技术：(1)硬件感知的高效矩阵流水线，最大化计算单元利用率并利用多级片上存储；(2)硬件和工作负载感知的调度方案，协调查询、插入和索引重建以最小化延迟。

Result: 在Snapdragon 8系列SoC上实现，HotpotQA测试显示：查询吞吐量提升1.4倍，索引构建速度提升7倍，并发查询负载下插入吞吐量提升6倍。

Conclusion: AME成功解决了移动设备上向量数据库的性能瓶颈，为设备端智能体提供了高效的内存管理解决方案。

Abstract: On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.

</details>


### [9] [IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment](https://arxiv.org/abs/2511.19258)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文成功测试和验证了ARM SMMU（IOMMU）在Xilinx Zynq UltraScale+ MPSoC平台上的功能，通过开发自定义内核模块实现了虚拟地址到物理地址的转换，支持DMA传输，并展示了动态地址翻译能力。


<details>
  <summary>Details</summary>
Motivation: 在具有多个计算节点的复杂系统中，维持节点间高效正确的缓存一致性是关键挑战。Unimem系统通过虚拟化全局地址空间来解决这个问题，依赖每个节点的IOMMU。本论文旨在通过成功测试和使用单节点的IOMMU来支持这种方法。

Method: 使用ARM的SMMU（IOMMU），开发自定义内核模块来测试其功能。首先在Xilinx Zynq UltraScale+ MPSoC的PS中测试SMMU，插入虚拟到物理地址映射并触发DMA传输；然后从PL发起DMA事务验证翻译功能；最后开发无需预先映射地址对的模块，通过配置SMMU使用用户进程的页表指针实现动态地址翻译。

Result: 在所有测试场景中成功展示了SMMU的正确操作，验证了虚拟地址到物理地址的转换功能，支持来自PS和PL的DMA传输，并实现了动态地址翻译能力。

Conclusion: 成功验证了SMMU在各种场景下的正确运行，为Unimem系统的实现提供了基础支持。由于时间限制，高级SMMU功能的进一步探索留待未来工作。

Abstract: In complex systems with many compute nodes containing multiple CPUs that are coherent within each node, a key challenge is maintaining efficient and correct coherence between nodes. The Unimem system addresses this by proposing a virtualized global address space that enables such coherence, relying on the I/O Memory Management Unit (IOMMU) in each node. The goal of this thesis is to support this approach by successfully testing and using the IOMMU of a single node. For this purpose, we used ARM's IOMMU, known as the System Memory Management Unit (SMMU), which translates virtual addresses to physical addresses. Because Linux documentation for the SMMU is limited and unclear, we implemented custom kernel modules to test and use its functionality.
  First, we tested the SMMU in the Processing System (PS) of the Xilinx Zynq UltraScale+ MPSoC by developing a module that inserted virtual-to-physical address mappings into the SMMU. We then triggered a DMA transfer to a virtual address and observed that the request passed through the SMMU for address translation. We repeated this experiment by initiating DMA transactions from the Programmable Logic (PL) and similarly confirmed that the transactions were translated by the SMMU. Finally, we developed a module that enables transactions from the PL without requiring explicit pre-mapping of virtual and physical address pairs. This was achieved by configuring the SMMU with the page table pointer of a user process, allowing it to translate all relevant virtual addresses dynamically.
  Overall, we successfully demonstrated the correct operation of the SMMU across all tested scenarios. Due to time constraints, further exploration of advanced SMMU features is left for future work.

</details>


### [10] [Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes](https://arxiv.org/abs/2511.19208)
*Jérémie Chalopin,Maria Kokkou*

Main category: cs.DC

TL;DR: 为弦图和K4无拆卸图提供常数大小的领导者选举认证方案，为拆卸图提供生成树认证方案，并提出将认证方案自动转换为自稳定算法的通用方法。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，需要高效验证问题解决方案的正确性。本研究专注于领导者选举和生成树构造这两个基本问题，旨在为特定图类提供常数大小的局部认证方案。

Method: 为弦图和K4无拆卸图设计领导者选举认证方案，为拆卸图设计生成树认证方案，所有方案都使用常数大小（每边）的证书。还提出将任何认证方案转换为自稳定算法的通用方法。

Result: 成功为弦图和K4无拆卸图提供了常数大小的领导者选举认证方案，为拆卸图提供了生成树认证方案。弦图的方案还能确保无环定向。提出了将认证方案转换为自稳定算法的通用转换方法。

Conclusion: 这些是针对特定图类的首个局部认证结果，可能揭示了可用于验证其他问题的结构特性。认证方案到自稳定算法的转换方法具有独立价值。

Abstract: In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI](https://arxiv.org/abs/2511.17593)
*Saicharan Kolluru*

Main category: cs.LG

TL;DR: 对vLLM和HuggingFace TGI两个开源LLM服务框架的实证评估，发现vLLM在高并发场景下吞吐量最高可达TGI的24倍，而TGI在交互式单用户场景中延迟更低。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中部署大型语言模型需要高效的推理服务系统，以平衡吞吐量、延迟和资源利用率。

Method: 使用LLaMA-2模型（7B到70B参数）对vLLM和TGI进行多维度基准测试，包括吞吐量性能、端到端延迟、GPU内存利用率和可扩展性特征。

Result: vLLM通过其新颖的PagedAttention机制在高并发工作负载下实现比TGI高24倍的吞吐量，而TGI在交互式单用户场景中表现出更低的尾部延迟。

Conclusion: 框架选择应基于具体用例需求：vLLM在高吞吐量批处理场景中表现优异，而TGI更适合具有中等并发性的延迟敏感交互应用。

Abstract: The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.

</details>


### [12] [AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention](https://arxiv.org/abs/2511.17594)
*Aleksandar Stankovic*

Main category: cs.LG

TL;DR: AutoSAGE是一个输入感知的CUDA调度器，通过轻量级估计和微探针为每个输入选择最佳的分块和映射策略，支持CSR格式的SpMM和SDDMM操作，并能组合成CSR注意力流水线。


<details>
  <summary>Details</summary>
Motivation: 稀疏GNN聚合操作（CSR SpMM/SDDMM）的性能受节点度数分布、特征宽度和GPU微架构等因素影响很大，需要针对不同输入进行优化。

Method: 使用轻量级估计结合设备上的微探针为每个输入选择最佳的分块和映射策略，包含安全回退机制和持久缓存以确保确定性重放。

Result: 在Reddit和OGBN-Products数据集上，在带宽受限的特征宽度下与供应商基线相当，在小宽度下获得性能提升；在合成稀疏度和偏斜压力测试中实现最高4.7倍的内核级加速。

Conclusion: AutoSAGE能够有效优化稀疏GNN聚合操作的性能，特别是在处理不同输入特性时表现出色，并提供了完整的工具链支持。

Abstract: Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.

</details>


### [13] [Practical Machine Learning for Aphasic Discourse Analysis](https://arxiv.org/abs/2511.17553)
*Jason M. Pittman,Anton Phillips,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.LG

TL;DR: 本研究评估了五种机器学习模型在失语症患者图片描述任务中自动识别正确信息单元(CIU)的能力，发现模型在区分单词与非单词方面表现优异，但在识别CIU方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 当前CIU分析在临床应用中受到手动编码和分析的限制，需要自动化工具来减轻语言病理学家的工作负担。

Method: 使用五种监督机器学习模型，基于失语症患者的人类编码转录本和相应的单词及CIU数据进行训练。

Result: 单词与非单词分类准确率接近完美(0.995)，AUC范围0.914-0.995；CIU与非CIU分类中k-NN模型表现最佳，准确率0.824，AUC 0.787。

Conclusion: 监督机器学习模型能有效区分单词与非单词，但识别CIU仍具挑战性，需要进一步改进。

Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.

</details>


### [14] [VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking](https://arxiv.org/abs/2511.18692)
*Kichang Yang,Seonjun Kim,Minjae Kim,Nairan Zhang,Chi Zhang,Youngki Lee*

Main category: cs.LG

TL;DR: 提出Neuron Chunking方法，通过将神经元重要性评估与存储访问成本耦合，优化边缘设备上大型视觉语言模型的权重卸载效率。


<details>
  <summary>Details</summary>
Motivation: 传统的激活稀疏化方法仅基于激活幅度选择神经元，忽略了访问模式对闪存性能的影响，导致I/O效率低下。

Method: 基于块（内存中连续的神经元组）进行操作，通过轻量级访问连续性抽象建模I/O延迟，选择具有高效用（神经元重要性/估计延迟）的块。

Result: 在Jetson Orin Nano和Jetson AGX Orin上分别实现了4.65倍和5.76倍的I/O效率提升。

Conclusion: 通过将稀疏化决策与底层存储行为对齐，Neuron Chunking显著提高了边缘设备上大型视觉语言模型的权重卸载效率。

Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

</details>


### [15] [Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks](https://arxiv.org/abs/2511.17564)
*Guilherme Grancho D. Fernandes,Marco A. Barroca,Mateus dos Santos,Rafael S. Oliveira*

Main category: cs.LG

TL;DR: 使用双向LSTM网络对PLAsTiCC数据集中的瞬变天体光变曲线进行分类，将14个类别重组为5个广义类别以解决类别不平衡问题。模型在S-Like和Periodic类别上表现良好，但在Fast和Long类别上性能较差，且难以区分Periodic和Non-Periodic对象。


<details>
  <summary>Details</summary>
Motivation: 解决天文瞬变天体光变曲线分类中的类别不平衡问题，并评估模型在部分观测数据上的性能表现。

Method: 采用双向LSTM神经网络，通过填充、时间重缩放和通量归一化进行预处理，使用掩码层处理变长序列，在19,920个测试对象上进行训练和评估。

Result: S-Like和Periodic类别的ROC AUC分别为0.95和0.99，Precision-Recall AUC分别为0.98和0.89；Fast和Long类别性能较差（Long类ROC AUC仅0.68）；在部分光变曲线数据上性能显著下降。

Conclusion: 类别不平衡和有限时间信息是主要限制因素，建议采用类别平衡策略和专注于检测时刻的预处理技术来改进性能。

Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.

</details>


### [16] [Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs](https://arxiv.org/abs/2511.17566)
*Shuaiyu Xie,Hanbin He,Jian Wang,Bing Li*

Main category: cs.LG

TL;DR: 提出了CCLH框架，通过级联条件学习和异构超图建模来解决微服务系统中根因定位和故障类型识别的挑战，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法存在两个关键问题：1）联合学习范式忽略了任务间的因果依赖关系；2）主要关注实例间的点对点关系，忽视了由部署配置和负载均衡引起的群体影响特性。

Method: CCLH框架采用级联条件学习来协调诊断任务，提供三级分类来描述实例间的群体影响，并使用异构超图来建模这些关系以模拟故障传播。

Result: 在三个微服务基准数据集上的大量实验表明，CCLH在根因定位和故障类型识别方面均优于最先进的方法。

Conclusion: CCLH通过级联条件学习和异构超图建模有效解决了传统方法的局限性，在微服务系统根因分析中表现出优越性能。

Abstract: Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.

</details>


### [17] [Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization](https://arxiv.org/abs/2511.17568)
*Le Xu,Jiayu Chen*

Main category: cs.LG

TL;DR: 将锐度感知最小化(SAM)作为即插即用的优化器应用于离线强化学习，以解决数据损坏导致的泛化问题，在IQL和RIQL算法上显著提升了抗数据损坏的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习对现实世界数据损坏非常脆弱，即使是鲁棒算法在挑战性的观测和混合损坏下也会失败。这种失败源于数据损坏在损失函数中创建了尖锐的最小值，导致泛化能力差。

Method: 首次将锐度感知最小化(SAM)作为通用即插即用优化器应用于离线RL。SAM寻求更平坦的最小值，将模型引导到更鲁棒的参数区域。将SAM集成到IQL和RIQL算法中，并在D4RL基准测试中评估随机和对抗性损坏。

Result: SAM增强的方法持续且显著优于原始基线。奖励表面的可视化证实SAM找到了更平滑的解，为提升离线RL智能体鲁棒性提供了有力证据。

Conclusion: SAM作为通用优化器能有效提升离线强化学习在数据损坏场景下的鲁棒性，通过寻找更平坦的最小值改善泛化性能。

Abstract: Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.

</details>


### [18] [Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis](https://arxiv.org/abs/2511.17573)
*Michael J. Bommarito*

Main category: cs.LG

TL;DR: 提出了Binary BPE分词器家族，专门用于二进制分析，通过字节对编码在多种平台和架构的二进制文件上训练，提供4K-64K词汇表，实现2-3倍的上下文窗口效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决二进制分析中字节级分词的问题：原始字节浪费transformer等神经网络的上下文窗口容量，现有文本分词器无法处理0x00-0xFF的任意字节序列。

Method: 开发了跨平台Byte Pair Encoding分词器，在包含Linux、Windows、macOS、Android和恶意软件的大型二进制语料库上训练，提供4K、8K、16K、32K和64K词汇表的分词器。

Result: 分词器发现了可解释的模式（ELF/PE头、指令序列、跨平台字符串），每个token实现多字节压缩。在未压缩可执行文件上，相比原始字节，固定长度transformer上下文窗口可容纳2-3倍多的二进制内容。

Conclusion: Binary BPE分词器为二进制分析提供了更高效的研究和实际部署基础，支持内容识别、恶意软件检测、逆向工程和优化等应用，已在HuggingFace开源发布。

Abstract: Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools.

</details>


### [19] [Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.17577)
*Fengming Yu,Qingyu Meng,Haiwei Pan,Kejia Zhang*

Main category: cs.LG

TL;DR: 提出一种轻量级优化方法，结合动态注意力头剪枝和知识蒸馏，在保持数学推理能力的同时显著提升大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理等复杂任务中表现出色，但计算和存储成本高昂，阻碍实际部署。需要找到既能保持推理能力又能提升效率的解决方案。

Method: 动态评估多头注意力机制中每个注意力头的重要性（基于权重范数和熵），实时剪枝冗余头以减少计算开销，并通过知识蒸馏将原始模型信息传递到剪枝后的学生模型。

Result: 在Math23k数据集上，30%剪枝率下：参数减少18.7%，推理速度提升27.5%，FLOPs减少19.3%，准确率仅下降0.7%（从84.4%到83.7%）。

Conclusion: 该方法在保持强大推理性能的同时实现了显著的效率提升，为大语言模型在数学推理任务中的高效部署提供了实用解决方案。

Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.

</details>


### [20] [Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation](https://arxiv.org/abs/2511.17579)
*Hefei Xu,Le Wu,Chen Cheng,Hao Liu*

Main category: cs.LG

TL;DR: 提出了一个名为MVA的新框架，通过最小化不同人类价值观之间的互信息来缓解参数干扰，并使用价值外推策略探索帕累托前沿，以更好地对齐多个人类价值观。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，将其与人类价值观对齐以确保安全和道德已成为关键挑战。现有方法在多个可能冲突的人类价值观对齐方面存在不稳定、效率低和无法有效处理价值冲突的局限性。

Method: 提出MVA框架，通过最小化不同人类价值观之间的互信息来减轻参数干扰，并采用价值外推策略高效探索帕累托前沿，构建具有不同价值偏好的LLM集合。

Result: 大量实验表明，MVA在将LLM与多个人类价值观对齐方面持续优于现有基线方法。

Conclusion: MVA框架有效解决了多价值对齐中的参数干扰和价值冲突问题，能够实现更好的价值权衡。

Abstract: With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.
  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.

</details>


### [21] [EgoCogNav: Cognition-aware Human Egocentric Navigation](https://arxiv.org/abs/2511.17581)
*Zhiwen Qiu,Ziang Liu,Wenqian Niu,Tapomayukh Bhattacharjee,Saleh Kalantari*

Main category: cs.LG

TL;DR: 提出了EgoCogNav多模态自我中心导航框架，通过预测感知路径不确定性作为潜在状态，联合预测轨迹和头部运动，融合场景特征与感官线索。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注完全观察场景中的运动预测，往往忽略了捕捉人们对空间感受和反应的人类因素。

Method: 提出EgoCogNav框架，预测感知路径不确定性作为潜在状态，联合预测轨迹和头部运动，融合场景特征与感官线索。创建了CEN数据集，包含6小时真实世界自我中心记录。

Result: EgoCogNav学习到的感知不确定性与人类行为（如扫描、犹豫、回溯）高度相关，并在未见环境中具有良好的泛化能力。

Conclusion: 该框架能够有效建模人类导航的认知和体验因素，为理解人-环境交互和实现安全社交导航提供了新的方法。

Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

</details>


### [22] [GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.17582)
*Jie Ou,Shuaihong Jiang,Yingjun Du,Cees G. M. Snoek*

Main category: cs.LG

TL;DR: GateRA是一个参数高效微调框架，通过token感知的调制机制动态调整PEFT更新的强度，实现选择性token级适应，在多个常识推理基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法对所有token应用静态、输入无关的更新，忽视了不同输入的重要性和难度差异，导致在简单内容上过拟合或在信息丰富区域适应不足。

Method: 在标准PEFT分支中引入自适应门控机制，实现token级选择性适应；使用基于熵的正则化鼓励接近二元的门控决策；理论分析显示GateRA在PEFT路径上产生软梯度掩蔽效应。

Result: 实验可视化显示GateRA在预填充阶段自动抑制冗余token的更新，在解码阶段强调适应；在多个常识推理基准测试中一致优于或匹配先前的PEFT方法。

Conclusion: GateRA通过token感知的动态调制实现了更智能的参数高效微调，能够根据输入难度自适应分配适应能力，同时保持预训练知识并提高性能。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.

</details>


### [23] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: 提出S-VFM方法，通过引入变分潜码来强制轨迹直线化，解决Flow Matching中一步生成能力受限的问题，在多个基准测试中表现优异且效率更高。


<details>
  <summary>Details</summary>
Motivation: Flow Matching依赖学习弯曲轨迹，导致一步生成能力有限。现有方法存在离散近似误差、训练不稳定和收敛困难等问题。

Method: 在Flow Matching框架中集成变分潜码来代表"生成概览"，明确强制轨迹直线化，产生线性生成路径。

Result: 在三个挑战性基准测试中取得有竞争力的性能，在训练和推理效率方面相比现有方法具有优势。

Conclusion: S-VFM通过引入变分潜码有效解决了Flow Matching的轨迹弯曲问题，实现了更高效的一步生成。

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [24] [LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.17584)
*Haoyan Xu,Ruizhi Qian,Zhengtao Yao,Ziyi Liu,Li Li,Yuqi Li,Yanshu Li,Wenqing Zheng,Daniele Rosa,Daniel Barcklow,Senthil Kumar,Jieyu Zhao,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了TAG-AD基准数据集，用于文本属性图的异常检测，包含多种异常类型，并开发了基于检索增强生成的零样本LLM异常检测框架。


<details>
  <summary>Details</summary>
Motivation: 文本属性图（TAG）的异常检测在欺诈检测等应用中很重要，但由于缺乏标准化基准数据集而研究不足。

Method: 利用LLM生成语义连贯但上下文不一致的异常节点文本，构建TAG-AD数据集；提出基于RAG的零样本LLM异常检测框架，构建全局异常知识库。

Result: 实验显示LLM在检测上下文异常方面表现优异，而GNN方法在结构异常检测上更优；RAG辅助提示达到与人工设计提示相当的性能。

Conclusion: LLM和GNN方法在异常检测中各有优势，RAG辅助的零样本LLM框架具有实用价值，无需手动提示工程。

Abstract: Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.

</details>


### [25] [PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis](https://arxiv.org/abs/2511.17585)
*Kang He,Boyu Chen,Yuzhe Ding,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.LG

TL;DR: PaSE框架通过原型对齐校准和Shapley优化均衡来解决多模态情感分析中的模态竞争问题，提升模态间协作性能


<details>
  <summary>Details</summary>
Motivation: 多模态融合中常出现模态竞争现象，主导模态会压制较弱模态，导致性能不佳

Method: 采用原型引导的校准学习和基于Shapley的梯度调制，结合双阶段优化策略和原型门控融合模块

Result: 在IEMOCAP、MOSI和MOSEI数据集上的实验表明PaSE取得了优越性能并有效缓解了模态竞争

Conclusion: PaSE框架能够增强模态间协作，同时明确缓解模态竞争问题

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.

</details>


### [26] [ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning](https://arxiv.org/abs/2511.18291)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: ADF-LoRA在去中心化联邦学习中通过同步更新单个低秩矩阵并混合两个矩阵来改善参数状态一致性，相比现有LoRA变体实现了更快、更平滑的收敛和更高的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习中，交替更新LoRA矩阵会因客户端间的相位状态不匹配和分块发散而面临新的挑战，需要设计更稳定的参数传播机制。

Method: 提出ADF-LoRA方法，每轮仅同步更新一个低秩矩阵，并通过混合两个矩阵来在去中心化传播下保持更一致的参数状态，同时保留交替更新的交叉项抑制效果。

Result: 在多个GLUE任务上的实验表明，ADF-LoRA实现了更快、更平滑的收敛，并在去中心化联邦学习中以一致优势超越了现有的LoRA变体，获得了最高的平均准确率。

Conclusion: ADF-LoRA通过同步单矩阵更新和双矩阵混合的设计，有效解决了去中心化联邦学习中的稳定性问题，为无服务器拓扑中的参数优化提供了更可靠的解决方案。

Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.

</details>


### [27] [Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection](https://arxiv.org/abs/2511.17587)
*Yuxuan Hu,Jian Chen,Yuhao Wang,Zixuan Li,Jing Xiong,Pengyue Jia,Wei Wang,Chengming Li,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出EIGML框架，首次联合建模情绪和意图，通过双级对比框架和多模态融合模块提升贴纸响应选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有贴纸响应选择方法通常依赖语义匹配，并单独建模情绪和意图，当两者不一致时会导致不匹配问题。

Method: 提出情绪和意图引导的多模态学习框架，包含双级对比框架进行模态内外对齐，以及意图-情绪引导的多模态融合模块。

Result: 在两个公开数据集上的实验表明，EIGML始终优于现有最优基线，实现了更高的准确率和更好的情绪意图理解。

Conclusion: EIGML通过联合建模情绪和意图，有效减少孤立建模带来的偏差，显著提升了贴纸选择性能。

Abstract: Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials.

</details>


### [28] [CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning](https://arxiv.org/abs/2511.18611)
*Mengdi Wang,Efe Bozkir,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: CycleSL是一个新颖的无聚合分割学习框架，通过循环更新机制解决传统分割学习在可扩展性和性能方面的限制，无需模型聚合即可提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 传统分割学习存在可扩展性差、服务器资源开销大、模型性能和收敛性降低等问题，特别是客户端漂移和延迟的影响。

Method: 受交替块坐标下降启发，将服务器端训练视为独立的高级机器学习任务，重新采样客户端提取的特征来缓解异质性和漂移，采用先优化服务器模型再更新客户端的循环更新策略。

Result: 在五个公开数据集上的实验表明，CycleSL能有效提升模型性能，特别是在非独立同分布数据和部分客户端参与的场景下。

Conclusion: CycleSL通过无聚合的循环更新机制显著提升了分割学习的可扩展性和性能，并能与现有方法无缝集成。

Abstract: Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.

</details>


### [29] [Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection](https://arxiv.org/abs/2511.17589)
*Sören Dréano,Derek Molloy,Noel Murphy*

Main category: cs.LG

TL;DR: Llamazip是一种基于LLaMA3语言模型预测能力的无损文本压缩算法，通过仅存储模型预测失败的token来实现显著数据压缩，同时还能识别文档是否属于语言模型训练数据集。


<details>
  <summary>Details</summary>
Motivation: 开发一种利用语言模型预测能力的高效无损压缩方法，同时解决语言模型训练数据来源识别问题，以应对数据来源、知识产权和训练透明度等关键问题。

Method: 基于LLaMA3语言模型的预测能力，仅存储模型无法正确预测的token，分析量化程度和上下文窗口大小等关键因素对压缩性能的影响。

Result: 实现了显著的数据压缩效果，同时能够识别文档是否属于语言模型的训练数据集，为数据来源追踪提供了可行方案。

Conclusion: Llamazip不仅展示了语言模型在无损压缩方面的应用潜力，还提供了一种解决语言模型训练数据来源识别问题的方法，对数据透明度和知识产权保护具有重要意义。

Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.

</details>


### [30] [SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data](https://arxiv.org/abs/2511.17590)
*Ke Yu,Shigeru Ishikura,Yukari Usukura,Yuki Shigoku,Teruaki Hayashi*

Main category: cs.LG

TL;DR: 提出SHAP距离作为评估合成表格数据语义保真度的新指标，通过比较真实数据和合成数据训练的模型的SHAP特征重要性差异来检测语义不一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注分布相似性和预测性能，但无法评估合成数据是否保持与真实数据一致的推理模式，即语义保真度。

Method: 引入SHAP距离，定义为从真实数据和合成数据训练的分类器得到的全局SHAP归因向量之间的余弦距离。

Result: SHAP距离能可靠识别标准统计和预测指标忽略的语义差异，特别是特征重要性偏移和尾部效应不足的问题。

Conclusion: SHAP距离是审计合成表格数据语义保真度的实用工具，建议将基于归因的评估整合到未来基准测试流程中。

Abstract: Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.

</details>


### [31] [Federated style aware transformer aggregation of representations](https://arxiv.org/abs/2511.18841)
*Mincheol Jeon,Euinam Huh*

Main category: cs.LG

TL;DR: FedSTAR是一个风格感知的联邦学习框架，通过解耦客户端特定风格因子和共享内容表示来解决个性化联邦学习中的领域异构、数据不平衡和通信约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习缺乏个性化，单一全局模型无法捕捉客户端特定特征，导致对有高度不同数据分布的客户端产生偏差预测和泛化能力差。

Method: 使用基于Transformer的注意力机制聚合类原型，解耦客户端特定风格因子和共享内容表示，通过交换紧凑原型和风格向量而非完整模型参数来减少通信开销。

Result: 实验结果表明，结合内容-风格解耦和注意力驱动的原型聚合在异构环境中提高了个性化和鲁棒性，且不增加通信成本。

Conclusion: FedSTAR通过风格感知的方法有效解决了联邦学习中的个性化挑战，在保持低通信开销的同时提升了模型性能。

Abstract: Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.
  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.
  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.

</details>


### [32] [Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design](https://arxiv.org/abs/2511.17595)
*Markus D. Solbach,John K. Tsotsos*

Main category: cs.LG

TL;DR: 该研究探讨了强化学习在3D Same-Different视觉空间任务中的表现，发现标准方法面临挑战，但通过基于人类实验设计的课程学习取得了成功。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在复杂、非结构化问题领域中的智能行为表现潜力，特别是将其从受限环境扩展到更广泛的应用场景。

Method: 使用PPO、行为克隆和模仿学习等最先进方法，并基于真实人类实验发现设计课程学习策略。

Result: 标准RL方法在直接学习最优策略方面遇到困难，但通过精心设计的课程学习实现了有效学习。

Conclusion: 课程学习为强化学习在复杂视觉空间任务中提供了有前景的途径，人类实验的见解对设计有效学习策略至关重要。

Abstract: Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.
  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.

</details>


### [33] [Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning](https://arxiv.org/abs/2511.17598)
*Zhizuo Chen,Theodore T. Allen*

Main category: cs.LG

TL;DR: 提出了非平稳和变折扣MDP（NVMDP）框架，能够处理非平稳环境和时变折扣率，包含传统MDP作为特例，并提供了理论分析和算法扩展。


<details>
  <summary>Details</summary>
Motivation: 传统MDP算法在非平稳环境中面临挑战，无限时域公式不能直接应用于有限时域任务，需要更灵活的框架来适应这些现实需求。

Method: 引入NVMDP框架，允许折扣率随时间变化，扩展了动态规划和Q学习算法，并提供了函数逼近情况下的策略梯度定理和TRPO扩展。

Result: 在非平稳网格世界环境中的实验表明，NVMDP算法能够成功恢复最优轨迹，而原始Q学习失败。

Conclusion: NVMDP提供了一个理论严谨且实际有效的强化学习框架，只需最小算法修改即可鲁棒处理非平稳性和显式最优策略塑造。

Abstract: Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.

</details>


### [34] [From Projection to Prediction: Beyond Logits for Scalable Language Models](https://arxiv.org/abs/2511.17599)
*Jianbing Dong,Jianbin Chang*

Main category: cs.LG

TL;DR: 提出一种将输出投影和损失预测集成到单步操作的新方法，避免显式logits张量化，减少内存使用和带宽压力


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练的两阶段管道（线性投影+交叉熵损失）需要完全实例化中间logits张量，导致显著的内存开销和带宽消耗，限制了可扩展性和训练吞吐量

Method: 通过直接从隐藏状态和目标标记计算损失，绕过显式logits实例化，将输出投影和损失预测集成到单一操作中

Result: 实验显示该方法实现了显著的内存节省和可衡量的加速，支持更大的批处理大小和更长的序列而不牺牲准确性

Conclusion: 重新思考投影和预测之间的边界具有显著优势，为高效LLM训练提供了实用的系统优化方案

Abstract: Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.
  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.

</details>


### [35] [Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts](https://arxiv.org/abs/2511.17601)
*Luyang Fang,Tao Wang,Ping Ma,Xiaoming Zhai*

Main category: cs.LG

TL;DR: 提出UniMoE-Guided方法，通过知识蒸馏将多个任务特定大模型的知识转移到单个紧凑模型中，实现高效的多任务自动评分


<details>
  <summary>Details</summary>
Motivation: 解决自动评分系统中为每个任务单独训练模型导致的资源消耗、存储和维护问题

Method: 使用混合专家(MoE)架构，包含共享编码器、门控MoE块和轻量级任务头，通过知识蒸馏从多个教师模型学习

Result: 在9个科学推理任务上达到与任务特定模型相当的性能，存储需求减少6倍，相比20B参数教师模型减少87倍

Conclusion: 该方法为课堂和大规模评估系统提供了可扩展、可靠且资源高效的自动评分解决方案

Abstract: Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\sim$6$\times$ less storage than maintaining separate students, and $87\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.

</details>


### [36] [Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models](https://arxiv.org/abs/2511.17602)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 提出分层污染检测框架，在四个层面检测合成数据对基准测试的污染：词元级、语义级、推理模式和性能悬崖检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能检测词元级重叠，无法发现语义级污染，而基础模型越来越多地使用可能隐含编码基准知识的合成数据进行训练，这威胁评估完整性。

Method: 分层污染检测框架，包括词元级、语义级、推理模式和性能悬崖检测四个层次，在MMLU、GSM8K和HumanEval上进行受控实验验证。

Result: 语义级污染能逃避现有方法检测（F1=0.17-0.49），但分层方法能有效检测（F1=0.76），平均比最先进基线提高26.5%。

Conclusion: 该框架为从业者提供了实用的审计管道工具，支持合成训练数据的负责任部署。

Abstract: Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data.

</details>


### [37] [BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis](https://arxiv.org/abs/2511.17604)
*Jiajun Ma,Yongchao Zhang,Chao Zhang,Zhao Lv,Shengbing Pei*

Main category: cs.LG

TL;DR: 提出了BrainHGT，一种分层图Transformer，模拟大脑从局部区域到全局社区的自然信息处理过程，通过长短程注意力编码器和先验引导聚类模块来提升疾病识别性能并增强生物可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将大脑建模为扁平网络，忽略了其模块化结构，且注意力机制对所有脑区连接一视同仁，忽略了与距离相关的节点连接模式。大脑信息处理是一个分层过程，涉及局部和长程交互、区域与子功能模块间的交互以及功能模块本身的交互。

Method: 设计了新颖的长短程注意力编码器，使用并行路径处理密集的局部交互和稀疏的长程连接；设计了先验引导聚类模块，利用交叉注意力机制将脑区分组为功能社区，并利用神经解剖学先验指导聚类过程。

Result: 实验结果表明，所提方法显著提高了疾病识别的性能，并能可靠地捕捉大脑的子功能模块，展示了其可解释性。

Conclusion: BrainHGT通过模拟大脑的分层信息处理机制，有效解决了现有方法的局限性，在疾病识别和生物可解释性方面取得了显著改进。

Abstract: Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability.

</details>


### [38] [Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification](https://arxiv.org/abs/2511.17605)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed*

Main category: cs.LG

TL;DR: 使用copula方法直接建模临床和基因组机器学习风险评分的联合关系，可以改善乳腺癌5年癌症特异性死亡的风险分层。


<details>
  <summary>Details</summary>
Motivation: 临床和基因组模型通常使用简单的线性规则结合，没有考虑它们在极端情况下的风险评分关系，需要更准确地建模这种联合关系来改善风险分层。

Method: 使用METABRIC乳腺癌队列，将临床变量和基因组变量分别训练监督分类器，获得风险评分后转换为伪观测值，用Gaussian、Clayton和Gumbel copula拟合联合分布。

Result: Gaussian copula最佳捕捉联合分布，临床模型AUC 0.783，基因组模型AUC 0.681。基于联合关系分组的患者Kaplan-Meier曲线显示明显差异，临床和基因组双高风险患者生存率最差。

Conclusion: copula-based融合方法在真实世界队列中有效，考虑评分间依赖关系能更好识别预后最差的患者亚组。

Abstract: Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.

</details>


### [39] [Energy-based Autoregressive Generation for Neural Population Dynamics](https://arxiv.org/abs/2511.17606)
*Ningling Ge,Sicheng Dai,Yu Zhu,Shan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于能量的自回归生成框架，用于高效生成具有真实神经群体动态的数据，在计算效率和生成质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决计算建模在神经科学中计算效率与高保真建模之间的基本权衡问题，加速对大脑功能的理解。

Method: 使用基于能量的变换器，通过严格适当评分规则在潜在空间中学习时间动态，实现高效的神经群体动态生成。

Result: 在合成Lorenz数据集和两个神经潜在基准数据集上，EAG实现了最先进的生成质量，计算效率显著提升，特别是在条件生成应用中展示了泛化到未见行为情境和改善脑机接口解码精度的能力。

Conclusion: 基于能量的建模对于神经群体动态是有效的，在神经科学研究和神经工程中具有应用价值。

Abstract: Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics.

</details>


### [40] [Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features](https://arxiv.org/abs/2511.17610)
*Leonardo Rossi,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 提出了一种针对铁人三项的合成数据生成框架，结合训练负荷和生活方式因素（睡眠质量、压力、恢复状态）来预测运动损伤风险，机器学习模型AUC达到0.86。


<details>
  <summary>Details</summary>
Motivation: 铁人三项训练因高负荷重复性生理压力导致运动员过度使用损伤风险高，现有预测方法主要依赖训练负荷指标，忽视了睡眠质量、压力等影响恢复和损伤易感性的关键因素。

Method: 开发了专门针对铁人三项的合成数据生成框架，生成生理合理的运动员档案，模拟个性化训练计划（包含周期化和负荷管理原则），并整合日常生活因素如睡眠质量、压力水平和恢复状态。

Result: 评估了LASSO、随机森林和XGBoost等机器学习模型，显示出高预测性能（AUC最高达0.86），识别出睡眠障碍、心率变异性和压力是损伤风险的关键早期指标。

Conclusion: 这种可穿戴设备驱动的方法不仅提高了损伤预测准确性，还为克服现实世界数据限制提供了实用解决方案，为全面、情境感知的运动员监测开辟了道路。

Abstract: Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility.
  We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring.

</details>


### [41] [AI-driven Generation of MALDI-TOF MS for Microbial Characterization](https://arxiv.org/abs/2511.17611)
*Lucía Schmidt-Santiago,David Rodríguez-Temporal,Carlos Sevilla-Salcedo,Vanessa Gómez-Verdejo*

Main category: cs.LG

TL;DR: 本研究评估了三种深度生成模型（MALDIVAE、MALDIGAN、MALDIffusion）用于合成MALDI-TOF MS微生物质谱数据，以解决临床微生物学中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: MALDI-TOF MS在临床微生物鉴定中应用广泛，但数据驱动的诊断模型发展受限于缺乏足够大、平衡和标准化的光谱数据集。

Method: 采用条件生成方法，基于物种标签指导生成微生物光谱，使用多种指标评估光谱保真度和多样性。

Result: 合成数据在统计和诊断上与真实测量相当，仅使用合成数据训练的模型性能接近真实数据训练模型。MALDIVAE在真实性、稳定性和效率方面表现最佳。

Conclusion: 合成数据生成能有效缓解类别不平衡和领域不匹配问题，显著提高分类准确性，为微生物学机器学习工具开发提供支持。

Abstract: Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology.
  We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics.
  Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data.

</details>


### [42] [Tensor Gauge Flow Models](https://arxiv.org/abs/2511.17616)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Tensor Gauge Flow Models是一种新的生成流模型，通过引入高阶张量规范场扩展了规范流模型，能够编码更丰富的几何和规范理论结构。


<details>
  <summary>Details</summary>
Motivation: 为了在生成流模型中编码更丰富的几何和规范理论结构，提高模型的表达能力。

Method: 将高阶张量规范场引入流方程，扩展了规范流模型和更高阶规范流模型。

Result: 在高斯混合模型上的实验表明，Tensor Gauge Flow Models相比标准和规范流基线模型取得了更好的生成性能。

Conclusion: Tensor Gauge Flow Models通过引入高阶张量规范场，成功提升了生成流模型的表达能力，在生成任务中表现出优越性能。

Abstract: This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines.

</details>


### [43] [Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification](https://arxiv.org/abs/2511.17622)
*Weidao Chen,Yuxiao Yang,Yueming Wang*

Main category: cs.LG

TL;DR: NH-GCAT是一个神经科学启发的分层图因果注意力网络，通过在不同空间尺度上显式建模抑郁特异性机制，将神经科学领域知识与深度学习相结合，在抑郁症分类中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经影像数据的图神经网络方法主要是数据驱动的黑盒模型，缺乏神经生物学可解释性，无法揭示抑郁症的复杂病理生理机制。

Method: 提出三个关键技术：(1)局部脑区水平的残差门控融合模块，整合BOLD动态与功能连接模式；(2)多区域环路水平的分层环路编码方案，按抑郁神经环路组织聚合节点表示；(3)多环路网络水平的变分潜在因果注意力机制，推断关键环路间的定向信息流。

Result: 在REST-meta-MDD数据集上，通过严格的留一站点交叉验证，NH-GCAT在抑郁症分类中达到样本量加权平均准确率73.3%和AUROC 76.4%，同时提供神经生物学意义的解释。

Conclusion: NH-GCAT成功将神经科学领域知识与深度学习相结合，在保持高性能的同时提供神经生物学可解释性，为理解抑郁症的脑网络动态提供了新视角。

Abstract: Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\% and an AUROC of 76.4\%, while simultaneously providing neurobiologically meaningful explanations.

</details>


### [44] [M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers](https://arxiv.org/abs/2511.17623)
*Haoran Li,Zhe Cheng,Muhao Guo,Yang Weng,Yannan Sun,Victor Tran,John Chainaranont*

Main category: cs.LG

TL;DR: 提出了M2OE2-GL方法，通过全局预训练和轻量级微调解决大规模配电系统中概率负荷预测的异构性和可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 在包含数千甚至数十万个负荷的大型配电系统中，为每个客户训练单独模型计算和存储成本高，而单一全局模型无法处理不同客户类型、位置和相位的分布偏移

Method: 首先在所有馈线负荷上预训练单一全局M2OE2基础模型，然后应用轻量级微调来推导紧凑的组特定预测器家族

Result: 在实际公用事业数据上的评估显示，M2OE2-GL在保持对大量负荷可扩展性的同时，显著降低了误差

Conclusion: M2OE2-GL方法有效解决了大规模配电系统中概率负荷预测的异构性和可扩展性挑战

Abstract: Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads.

</details>


### [45] [QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments](https://arxiv.org/abs/2511.17624)
*Hector E Mozo*

Main category: cs.LG

TL;DR: QML-HCS是一个量子启发机器学习框架，通过超因果反馈动力学实现非平稳环境下的自适应行为，结合量子叠加原理和动态因果反馈来解决数据分布漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习和量子启发系统在非平稳环境中表现不佳，数据分布漂移时缺乏连续适应、因果稳定性和相干状态更新的机制。

Method: 采用统一计算架构，集成量子启发叠加原理、动态因果反馈和确定性-随机混合执行，实现可逆变换、多路径因果传播和漂移下替代状态评估。

Result: 通过最小化仿真展示了超因果模型在输入分布突变时的自适应能力，同时保持内部一致性，无需完全重新训练。

Conclusion: 该框架为未来理论扩展、基准测试研究以及与经典和量子模拟平台的集成奠定了基础架构。

Abstract: QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments.
  The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware.
  A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms.

</details>


### [46] [Efficient Large-Scale Learning of Minimax Risk Classifiers](https://arxiv.org/abs/2511.17626)
*Kartheek Bondugula,Santiago Mazuelas,Aritz Pérez*

Main category: cs.LG

TL;DR: 提出了一种基于约束和列生成组合的学习算法，用于高效训练大规模多类分类任务中的极小极大风险分类器（MRCs）。


<details>
  <summary>Details</summary>
Motivation: 监督学习中的大规模数据通常导致复杂优化问题。随机次梯度方法适用于最小化平均损失的分类技术，但不适用于最小化最大期望损失的MRCs方法。

Method: 结合约束和列生成的学习算法，专门针对大规模多类分类任务中的MRCs训练。

Result: 在多个基准数据集上的实验表明，该算法在通用大规模数据上提供高达10倍的加速，在类别数量较多时提供约100倍的加速。

Conclusion: 所提出的算法显著提高了MRCs在大规模多类分类任务中的训练效率。

Abstract: Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.

</details>


### [47] [Rectifying Mean-Shift in Cascaded Precipitation Nowcasting](https://arxiv.org/abs/2511.17628)
*Fanbo Ju,Haiyuan Shi,Qingjian Ni*

Main category: cs.LG

TL;DR: RectiCast是一个用于降水临近预报的两阶段框架，通过双流匹配模型显式解耦均值场偏移校正和局部随机性生成，解决了现有方法中确定性预测的系统分布偏移与局部随机性混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有级联架构的降水临近预报方法通常忽视了确定性预测的系统分布偏移与局部随机性的混淆问题，导致确定性分量的分布偏移污染概率分量的预测，特别是在较长预报时效上造成降水模式和强度的不准确。

Method: 提出RectiCast两阶段框架：第一阶段使用确定性模型生成后验均值；第二阶段引入校正器显式学习分布偏移并生成校正后的均值，然后生成器在修正均值条件下建模局部随机性。

Result: 在SEVIR和MeteoNet数据集上的实验表明，RectiCast相比现有最先进方法实现了显著的性能提升。

Conclusion: RectiCast通过显式解耦均值场偏移校正和局部随机性生成，有效解决了级联架构中分布偏移污染问题，提高了降水临近预报的准确性。

Abstract: Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods.

</details>


### [48] [Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance](https://arxiv.org/abs/2511.17629)
*Yanxuan Yu,Michael S. Hughes,Julien Lee,Jiacheng Zhou,Andrew F. Laine*

Main category: cs.LG

TL;DR: AF-SMOTE是一种针对极端类别不平衡分类问题的数据增强框架，通过合成少数类样本并使用对抗判别器和边界效用模型进行过滤，在保持校准的同时提高召回率和平均精度。


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等场景中，类别极度不平衡且召回率和校准都至关重要，漏诊罕见疾病可能带来严重后果。

Method: 提出AF-SMOTE框架：先合成少数类样本，然后通过对抗判别器和边界效用模型进行过滤，在决策边界平滑和类条件密度的温和假设下，过滤步骤能单调改善F_beta代理指标而不增加Brier分数。

Result: 在MIMIC-IV代理标签预测和欺诈检测基准测试中，AF-SMOTE比SMOTE、ADASYN等强基线方法获得更高的召回率和平均精度，并具有最佳校准性能。

Conclusion: AF-SMOTE在医疗数据集上的成功应用展示了其在临床场景中的实用价值，特别是在罕见疾病诊断中避免漏诊具有重要意义。

Abstract: We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences.

</details>


### [49] [Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change](https://arxiv.org/abs/2511.17630)
*Nele Albers,Esra Cemre Su de Groot,Loes Keijsers,Manon H. Hillegers,Emiel Krahmer*

Main category: cs.LG

TL;DR: LLM可以生成用于训练数字行为改变强化学习模型的用户交互样本，在缺乏真实数据时表现良好，甚至达到人类评分者的水平。


<details>
  <summary>Details</summary>
Motivation: 开发个性化数字健康行为改变应用需要大量设计选择，这些选择的效果难以从文献预测且实践评估成本高昂。

Method: 使用LLM生成用户交互样本，比较真实用户数据和人类评分者样本，分析不同提示策略（短/长提示、思维链提示、少样本提示）的效果。

Result: LLM生成的样本在缺乏真实数据时有用，性能达到人类评分者水平，不同提示策略的效果因研究和LLM而异，提示改写也会产生较大差异。

Conclusion: LLM生成的样本在实践中可用于数字行为改变设置，提供了具体的使用建议。

Abstract: Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.

</details>


### [50] [Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario](https://arxiv.org/abs/2511.17631)
*Bingjun Wei,Xuemei Cao,Jiafen Liu,Haoyang Liang,Xin Yang*

Main category: cs.LG

TL;DR: 提出EFDMVC框架解决联邦多视图聚类中视图不确定性和聚合不确定性的双重挑战，通过语义对齐、层次对比融合和自适应漂移模块实现鲁棒性能


<details>
  <summary>Details</summary>
Motivation: 传统联邦多视图聚类假设客户端视图均匀，但实际部署中存在视图完整性异构问题，现有方法忽略了动态视图组合带来的语义冲突和聚合不确定性

Method: 1) 本地语义对齐和层次对比融合解决视图不确定性；2) 视图自适应漂移模块通过全局-局部原型对比动态修正参数偏差；3) 平衡聚合机制协调客户端更新

Result: 在多个基准数据集上，EFDMVC对异构不确定视图表现出优越的鲁棒性，在全面评估中始终优于所有最先进的基线方法

Conclusion: EFDMVC框架有效解决了联邦多视图聚类中的双重不确定性挑战，为实际部署中的视图异构问题提供了可靠解决方案

Abstract: Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations.

</details>


### [51] [Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production](https://arxiv.org/abs/2511.17632)
*Bestoun S. Ahmed,Tommaso Azzalin,Andreas Kassler,Andreas Thore,Hans Lindback*

Main category: cs.LG

TL;DR: 提出基于数字孪生的智能制造方法，通过微服务边缘计算平台和深度强化学习代理优化钢铁生产厂的可持续性、效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 将传统制造流程转变为智能系统，实现可持续性目标，强调MLOps在数据驱动制造中的关键作用。

Method: 采用微服务边缘计算平台，通过数字孪生实时处理传感器数据，使用深度强化学习代理自动关联系统状态与数字孪生，优化感应炉加热和功率设置。

Result: 系统能够优化感应炉加热，提高操作质量，减少过程浪费，降低制造成本。

Conclusion: 该研究为传统流程向智能系统转型提供了关键步骤，展示了可扩展的事件驱动架构可适应各种工业应用。

Abstract: We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.

</details>


### [52] [PocketLLM: Ultimate Compression of Large Language Models via Meta Networks](https://arxiv.org/abs/2511.17637)
*Ye Tian,Chengcheng Wang,Jing Han,Yehui Tang,Kai Han*

Main category: cs.LG

TL;DR: PocketLLM是一种通过元网络在潜在空间压缩大语言模型的新方法，使用编码器将权重投影到离散潜在向量，通过紧凑码本表示，再用轻量解码器重建权重，实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，在边缘设备上存储和传输变得困难，传统量化剪枝方法难以在保持精度下实现极端压缩。

Method: 提出简单编码器将LLM权重投影到离散潜在向量，用紧凑码本表示，轻量解码器将码本代表向量映射回原始权重空间。

Result: 实验表明PocketLLM在极高压缩比下仍保持优异性能，例如将Llama 2-7B压缩10倍而精度下降可忽略。

Conclusion: PocketLLM通过潜在空间压缩方法实现了大语言模型的高效压缩，在保持精度的同时显著减小模型存储和传输需求。

Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.

</details>


### [53] [Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer](https://arxiv.org/abs/2511.17638)
*Pratham Sorte*

Main category: cs.LG

TL;DR: 提出了一种无数据的模型间知识传输方法M2KT，通过概念空间而非示例空间进行知识交换，无需标注数据或教师生成输出，在符号推理任务中达到教师模型85-90%性能，数据使用量减少98%以上。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏、迁移学习和数据集蒸馏方法仍依赖数据驱动，需要教师模型生成示例、logits或梯度。本文旨在实现无数据的模型间概念传输，建立AI到AI的知识传输基础。

Method: 引入概念流形概念，建立教师和学生潜在空间的映射对齐，设计复合损失函数确保几何、结构、推理一致性和安全约束，提出教师端知识包生成和学生端接收验证的算法流程。

Result: 在大语言模型的符号推理实验中，M2KT能达到教师模型85-90%的性能，同时相比标准知识蒸馏减少98%以上的数据使用量。

Conclusion: 这项工作为无数据的AI到AI知识传输和自我改进的模型生态系统建立了理论和实践基础。

Abstract: Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems.

</details>


### [54] [TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin](https://arxiv.org/abs/2511.17639)
*Yibing Wan,Zhengxiong Guan,Chaoli Zhang,Xiaoyang Li,Lai Xu,Beibei Jia,Zhenzhe Zheng,Fan Wu*

Main category: cs.LG

TL;DR: 提出了TTF框架来解决用户生命周期价值(LTV)预测中的多时间序列不对齐、短输入长输出(SILO)和数据波动性等挑战，在抖音平台上部署后显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 互联网公司在用户增长场景中大量投入付费获客渠道，但可持续增长依赖于获取用户的LTV超过获客成本(CAC)。为了最大化LTV/CAC比率，需要在早期阶段预测渠道级LTV以优化预算分配。

Method: 提出了梯形时间融合(TTF)框架，包括梯形多时间序列模块处理数据不对齐和SILO挑战，以及多塔结构MT-FusionNet输出准确预测。

Result: 在抖音在线系统中部署后，与之前部署的在线模型相比，LTV曲线的点级MAPE(MAPEp)降低了4.3%，聚合LTV的MAPE(MAPEa)降低了3.2%。

Conclusion: TTF框架有效解决了LTV预测中的关键挑战，显著提升了预测精度，为预算分配优化提供了可靠支持。

Abstract: In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV.

</details>


### [55] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于认证式块级提取Transformer机制并支持认证局部编辑的框架，通过提供机器可检查的证书来约束近似误差，并将局部保证提升为全局偏差界限。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏形式化保证，无法明确提取或编辑后的模型在相关输入上与原始模型的偏差范围。

Method: 提出BlockCert框架，提取残差块的结构化替代实现，并提供机器可检查的证书来约束近似误差、记录覆盖指标和哈希底层工件。在Lean 4中形式化基于Lipschitz的组合定理，将局部保证提升为全局偏差界限。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上应用该框架，获得了高块级覆盖率和在评估提示上的小残差误差。在TinyLlama设置中，完全缝合的模型在压力提示上的困惑度与基线相差约6e-5。

Conclusion: 块级提取与显式证书对于真实Transformer语言模型是可行的，为机制可解释性和模型行为的形式化推理之间提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [56] [MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence](https://arxiv.org/abs/2511.17647)
*Liyuan Deng,Yunpeng Bai,Yongkang Dai,Xiaoshui Huang,Hongping Gan,Dongshuo Huang,Hao jiacheng,Yilei Shi*

Main category: cs.LG

TL;DR: MamTiff-CAD是一个基于Transformer扩散模型的CAD参数命令序列生成框架，通过Mamba+和Transformer混合的自编码器处理长序列，在60-256命令的长序列CAD模型生成上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成长序列参数命令，因为复杂CAD模型存在几何和拓扑约束。

Method: 设计Mamba+和Transformer混合自编码器，将参数化CAD序列转换为潜在表示；Mamba+块包含遗忘门机制捕获长程依赖；基于多尺度Transformer的扩散模型学习长序列命令分布。

Result: 在重建和生成任务上达到最先进性能，特别是在60-256命令的长序列CAD模型生成方面表现优异。

Conclusion: MamTiff-CAD框架有效解决了长序列参数CAD命令生成问题，证明了其在复杂CAD模型生成中的有效性。

Abstract: Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation.

</details>


### [57] [Frugality in second-order optimization: floating-point approximations for Newton's method](https://arxiv.org/abs/2511.17660)
*Giuseppe Carrino,Elena Loli Piccolomini,Elisa Riccietti,Theo Mary*

Main category: cs.LG

TL;DR: 该论文分析了有限精度算术对牛顿步长的影响，建立了混合精度牛顿优化器的收敛定理，并提出了GN_k方法，在回归任务中实现与完整牛顿法相当的性能但需要更少的导数计算。


<details>
  <summary>Details</summary>
Motivation: 虽然高阶优化方法如牛顿法能提供更高的精度和更快的收敛速度，但由于计算成本高而很少在实际应用中使用。本研究旨在通过分析有限精度算术的影响来改进牛顿法的实用性。

Method: 建立了混合精度牛顿优化器的收敛定理，包括"准"和"不精确"变体，并提出了GN_k方法，该方法允许部分计算二阶导数。

Result: 在标准回归基准测试中，所提出的方法在Australian和MUSH数据集上优于Adam。GN_k在回归任务中达到与完整牛顿法相当的性能，同时需要显著更少的导数计算。

Conclusion: 混合精度牛顿优化器提供了收敛保证和可实现的解精度先验估计，而GN_k方法在保持性能的同时大幅减少了计算成本，使高阶优化方法更实用。

Abstract: Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including "quasi" and "inexact" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations.

</details>


### [58] [Enhancing Breast Cancer Prediction with LLM-Inferred Confounders](https://arxiv.org/abs/2511.17662)
*Debmita Roy*

Main category: cs.LG

TL;DR: 使用大语言模型从临床数据推断糖尿病、肥胖和心血管疾病等混杂疾病的可能性，提升乳腺癌预测的随机森林模型性能。


<details>
  <summary>Details</summary>
Motivation: 通过AI生成的特征来改进乳腺癌预测模型，特别是利用大语言模型从常规临床数据中推断混杂疾病的可能性。

Method: 使用大语言模型（如Gemma和Llama）从常规临床数据中推断糖尿病、肥胖和心血管疾病等混杂疾病的可能性，并将这些AI生成的特征输入随机森林模型进行乳腺癌预测。

Result: AI生成的特征显著提高了随机森林模型的性能，特别是Gemma模型提升3.9%，Llama模型提升6.4%。

Conclusion: 该方法在乳腺癌非侵入性预筛查和临床整合方面具有潜力，支持改进早期检测和共享决策制定。

Abstract: This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis.

</details>


### [59] [AI-based framework to predict animal and pen feed intake in feedlot beef cattle](https://arxiv.org/abs/2511.17663)
*Alex S. C. Maia,John B. Hall,Hugo F. M. Milan,Izabelle A. M. A. Teixeira*

Main category: cs.LG

TL;DR: 开发了一个基于AI的框架，利用环境指数和机器学习模型准确预测个体动物和围栏级别的饲料摄入量，在精准畜牧业管理中具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏充分利用纵向大数据来准确预测饲料摄入量的方法，特别是在考虑环境条件的情况下。电子饲喂系统产生的大量数据为开发自主精准畜牧系统提供了可能性。

Method: 使用来自19个实验的超过1650万个样本数据，结合AgriMet气象站的环境数据，开发了两个新的环境指数：基于气象变量的InComfort-Index和整合环境变量与饲料摄入行为的混合指数EASI-Index。使用机器学习模型（XGBoost）进行训练。

Result: InComfort-Index对热舒适度有良好预测能力但对饲料摄入预测有限；EASI-Index在预测饲料摄入方面表现良好但对热舒适度预测效果较差。最佳模型在个体动物级别的RMSE为1.38 kg/天，围栏级别仅为0.14 kg/(天-动物)。

Conclusion: 该方法为预测个体动物和围栏的饲料摄入提供了一个稳健的AI框架，在减少饲料浪费、优化资源和气候适应性畜牧管理方面具有应用潜力。

Abstract: Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management.

</details>


### [60] [CubeletWorld: A New Abstraction for Scalable 3D Modeling](https://arxiv.org/abs/2511.17664)
*Azlaan Mustafa Samad,Hoang H. Nguyen,Lukas Berg,Henrik Müller,Yuan Xue,Daniel Kudenko,Zahra Ahmadi*

Main category: cs.LG

TL;DR: CubeletWorld是一个新颖的城市环境建模框架，通过离散化的3D网格单元（cubelets）来表示和分析城市环境，支持隐私保护的建模和多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现代城市产生大量异构数据，但将这些数据整合成连贯的空间模型仍具挑战性。现有基于智能体感知的方法存在可扩展性限制和隐私问题。

Method: 提出CubeletWorld框架，将城市环境离散化为3D网格单元（cubelets），将基础设施、移动性、环境指标等多样化数据嵌入到局部cubelet状态中。

Result: 该框架支持规划、导航和占用预测等下游任务，无需智能体驱动感知。通过CubeletWorld状态预测任务验证了方法的有效性，并分析了空间粒度增加带来的稀疏性挑战。

Conclusion: CubeletWorld提供了一个灵活可扩展的框架，能够从复杂的城市数据中学习，为人口统计建模、环境监测和应急响应等领域的可扩展仿真和决策支持开辟了新可能性。

Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.

</details>


### [61] [GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization](https://arxiv.org/abs/2511.17665)
*Hadi Khodaei Jooshin,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein生成对抗网络(WGANs)的新型全局路由批处理算法，相比传统启发式方法，能生成更少但质量更高的批次，在ISPD'24基准测试中实现40%运行时间减少，路由质量仅下降0.002%。


<details>
  <summary>Details</summary>
Motivation: 传统全局路由批处理方法依赖计算昂贵的启发式算法，导致批次过大、批次数量过多、生成时间长等问题，限制了可扩展性和效率。

Method: 使用Wasserstein生成对抗网络(WGANs)增强的新型批处理算法，能够更有效地生成高质量批次。

Result: 在ISPD'24基准测试中，相比最先进的路由器，运行时间减少高达40%，路由质量仅下降0.002%。

Conclusion: 基于WGANs的批处理算法能够显著提高全局路由的效率和可扩展性，同时保持路由质量。

Abstract: Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router.

</details>


### [62] [Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles](https://arxiv.org/abs/2511.17675)
*Navneet Singh,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种紧凑的混合量子架构，用于自动驾驶轨迹预测，通过残差学习和量子注意力编码器实现准确的多模态轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶轨迹预测需要在计算和延迟约束下提供准确、校准的多模态未来轨迹预测。

Method: 采用混合量子架构，在自我中心、车道对齐的坐标系中操作，预测对运动学基线的残差修正。结合量子注意力编码器、参数精简的量子前馈堆栈和基于傅里叶的解码器。

Result: 在Waymo Open Motion Dataset上，模型在2.0秒预测范围内实现了minADE 1.94米和minFDE 3.56米，优于运动学基线。

Conclusion: 残差学习、截断傅里叶解码、浅层纠缠和基于频谱的排序等方法能够稳定优化，从小型浅层量子电路中产生可靠的多模态预测。

Abstract: Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \SI{3.56}{m} in the $16$ models predicted over the horizon of \SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark.

</details>


### [63] [A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification](https://arxiv.org/abs/2511.17677)
*Abu Kaisar Mohammad Masum,Naveed Mahmud,M. Hassan Najafi,Sercan Aygun*

Main category: cs.LG

TL;DR: 提出了一种将n量子比特量子电路与经典BERT模型结合的混合方法用于文本分类，实验表明该混合模型在标准基准数据集上表现与经典基线相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: BERT微调在文本分类中计算成本高且需要仔细的超参数调优，而量子算法在机器学习和文本分类任务中显示出超越传统方法的潜力。

Method: 集成n量子比特量子电路与经典BERT模型的混合方法，用于文本分类任务。

Result: 混合模型在标准基准数据集上表现与经典基线相当，在某些情况下更好，证明了经典-量子模型在不同数据集上微调预训练模型的适应性。

Conclusion: 混合模型展示了量子计算在提升文本分类任务性能方面的潜力。

Abstract: Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks.

</details>


### [64] [Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics](https://arxiv.org/abs/2511.17687)
*Zhangyu Ge,Xu He,Lingfei Mo,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lansong Jiang,Fengyuan Liu*

Main category: cs.LG

TL;DR: 提出一种使用表示学习模型复制连续吸引子神经网络神经动力学模式的高效路径积分方法，通过轻量级人工神经网络重建头部方向细胞和网格细胞模型，实现脑启发航位推算。


<details>
  <summary>Details</summary>
Motivation: 现有脑启发导航研究中基于连续吸引子神经网络的路径积分能力存在显著计算冗余，运行效率需要提升，否则不利于脑启发导航技术的实用性。

Method: 使用表示学习模型复制CANN神经动力学模式，用轻量级人工神经网络重建HDC和GC模型，然后集成实现脑启发航位推算。

Result: 在各种环境中的基准测试表明，该方法不仅准确复制了导航细胞的神经动力学模式，定位精度与NeuroSLAM相当，而且在通用设备上效率提升约17.5%，在边缘设备上提升40~50%。

Conclusion: 这项工作为提高脑启发导航技术的实用性提供了一种新颖的实现策略，并具有进一步扩展的潜力。

Abstract: The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.

</details>


### [65] [Enhancing Adversarial Transferability through Block Stretch and Shrink](https://arxiv.org/abs/2511.17688)
*Quan Liu,Feng Ye,Chenhao Lu,Shuming Zhen,Guanliang Huang,Lunzhe Chen,Xudong Ke*

Main category: cs.LG

TL;DR: 提出了Block Stretch and Shrink (BSS)方法，通过将图像分块并进行拉伸和收缩操作，在保持全局语义的同时多样化注意力热图，从而提升对抗样本的跨模型迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于输入变换的攻击方法在跨模型迁移性方面表现有限，研究表明高迁移性与多样化的注意力热图和保持全局语义相关。

Method: 将图像分成多个块，对这些块应用拉伸和收缩操作，生成多样化的变换输入，同时保持图像的全局语义结构。

Result: 在ImageNet子集上的实验表明，BSS在迁移性方面优于现有的基于输入变换的攻击方法。

Conclusion: BSS方法通过分块拉伸收缩操作有效提升了对抗样本的迁移性，同时建议在统一的变换数量尺度下评估输入变换攻击方法以确保公平比较。

Abstract: Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.

</details>


### [66] [DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams](https://arxiv.org/abs/2511.17693)
*Ginés Carreto Picón,Peng Yuan Zhou,Qi Zhang,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了DeepCoT，一种无冗余的仅编码器模型，可在现有深度编码器架构上应用，实现线性计算成本，相比之前的高效模型减少两个数量级的运行时间。


<details>
  <summary>Details</summary>
Motivation: Transformer模型规模不断增大，但在资源受限设备上需要低延迟推理。流数据推理在滑动时间窗口上执行会导致高度冗余计算，现有Continual Transformers只能用于浅层模型，限制了其适用范围和泛化能力。

Method: 提出DeepCoT，一种无冗余的仅编码器模型，可在现有深度编码器架构上以最小改动应用，实现所有Transformer层的线性计算成本。

Result: 在音频、视频和文本流上的实验表明，DeepCoT保持了与非连续基线的相当性能，同时提供线性计算成本，运行时间相比之前的高效模型减少高达两个数量级。

Conclusion: DeepCoT是一种有效的深度连续Transformer模型，能够在保持性能的同时显著降低计算成本，适用于资源受限环境中的流数据推理。

Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.

</details>


### [67] [Diffusion Models are Molecular Dynamics Simulators](https://arxiv.org/abs/2511.17741)
*Justin Diamond,Markus Lill*

Main category: cs.LG

TL;DR: 证明了去噪扩散采样器与过阻尼朗之万动力学等价，将分子动力学重新表述为扩散模型框架，无需传统力场和轨迹数据训练


<details>
  <summary>Details</summary>
Motivation: 建立扩散采样与朗之万动力学之间的精确对应关系，为分子动力学提供数据驱动的新框架

Method: 使用去噪扩散采样器，通过批量维度上的顺序偏差实现朗之万动力学的欧拉-丸山积分器，学习能量梯度作为漂移项

Result: 实现了完全数据驱动的分子动力学框架，仅需平衡态快照训练，保持玻尔兹曼分布，生成具有时间相关性的分子轨迹

Conclusion: 扩散采样与朗之万动力学存在精确等价关系，为分子模拟提供了可扩展的替代方案，准确度由模型能力和去噪步数控制

Abstract: We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.
  This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.
  We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.

</details>


### [68] [Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices](https://arxiv.org/abs/2511.17754)
*Andrew Lee,Mahir Mobarrat,Xiaolin Chen*

Main category: cs.LG

TL;DR: 提出了一种周期性增强的代理建模方法，通过引入周期性层来保证确定性侧向位移(DLD)微流控设备单元边界的精确周期性，显著提高了多单元设备预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统DLD设备设计需要计算昂贵的Navier-Stokes模拟和粒子追踪分析，而现有的深度学习代理模型在处理关键的周期性边界条件时存在不足，导致多单元设备预测中的累积误差。

Method: 采用周期性层神经网络组件，无需惩罚项或输出修改即可保证精确周期性。使用三个子网络预测稳态、无量纲的速度和压力场(u, v, p)，而不是直接预测临界直径或粒子轨迹。

Result: 在120个CFD生成的几何结构上验证，周期性层实现实现了0.478%的临界直径误差，同时保持完美的周期性一致性，比基线方法提高了85.4%。

Conclusion: 该方法能够高效、准确地设计DLD设备，并为多单元设备应用提供保证的边界条件满足。

Abstract: Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications.

</details>


### [69] [PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning](https://arxiv.org/abs/2511.17776)
*Melika Shirian,Kianoosh Vadaei,Kian Majlessi,Audrina Ebrahimi,Arshia Hemmat,Peyman Adibi,Hossein Karshenas*

Main category: cs.LG

TL;DR: PrismSSL是一个统一的Python库，集成了音频、视觉、图形和跨模态设置中最先进的自监督学习方法，提供模块化代码库和图形化仪表板。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和从业者提供一个统一的框架，简化自监督学习的安装、配置和训练过程，支持多种模态和新方法的扩展。

Method: 采用模块化设计，集成HuggingFace Transformers，提供分布式训练、超参数搜索、LoRA微调、嵌入可视化等功能，并构建基于Flask的图形化仪表板。

Result: 开发了PrismSSL库，已在PyPI上发布，支持MIT许可证，提供了完整的代码和数据配方，确保可复现性。

Conclusion: PrismSSL成功统一了多种模态的自监督学习方法，通过模块化设计和用户友好界面，显著提升了自监督学习的可用性和可扩展性。

Abstract: We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible.

</details>


### [70] [Smoothed Agnostic Learning of Halfspaces over the Hypercube](https://arxiv.org/abs/2511.17782)
*Yiwen Kou,Raghu Meka*

Main category: cs.LG

TL;DR: 提出了一个基于随机位翻转的平滑学习框架，用于布尔输入上的不可知半空间学习，在次指数分布假设下给出了高效算法


<details>
  <summary>Details</summary>
Motivation: 传统基于高斯扰动的平滑分析框架不适用于离散领域，现有方法需要强结构假设，需要开发适合布尔输入的平滑学习框架

Method: 引入基于随机位翻转的扰动模型，构建离散平滑最优性概念，在次指数分布假设下设计高效学习算法

Result: 获得了近似n的poly(1/(σ*ε))因子的运行时间和样本复杂度，首次在布尔超立方体上实现计算高效的平滑不可知半空间学习

Conclusion: 该框架弥合了最坏情况不可计算性与实际可学习性之间的差距，为离散环境中的半空间学习提供了理论基础

Abstract: Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.

</details>


### [71] [Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces](https://arxiv.org/abs/2511.17784)
*Lyu Yuhuan*

Main category: cs.LG

TL;DR: 提出了一种基于随机采样的覆盖分析方法，在d维单位超立方体上通过离散化分析未覆盖子立方体数量，推导出与失败概率对数相关的样本复杂度界限，相比经典的线性依赖更紧致。


<details>
  <summary>Details</summary>
Motivation: 经典覆盖分析在小失败概率下往往产生保守界限，需要更精确的理论工具来支持依赖网格覆盖保证的算法，特别是在高置信度场景下实现更高效的采样。

Method: 在d维单位超立方体上进行均匀随机采样，应用集中不等式分析离散化后的未覆盖子立方体数量统计量，推导样本复杂度界限。

Result: 得到样本复杂度界限M=O(C̃ln(2C̃/δ))，具有对数依赖失败概率δ的特性，数值研究表明该界限能更紧密地跟踪实际覆盖需求，在δ→0时具有良好的扩展性。

Conclusion: 该研究为依赖网格覆盖保证的算法提供了更锐利的理论工具，特别是在高置信度机制下能够实现更高效的采样。

Abstract: Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($δ$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}δ))$, which contrasts sharply with the classical linear $1/δ$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $δ\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.

</details>


### [72] [Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device](https://arxiv.org/abs/2511.17787)
*Elizabeth Chen,Andrew Lee,Tanbir Sarowar,Xiaolin Chen*

Main category: cs.LG

TL;DR: 使用机器学习优化确定性侧向位移(DLD)设备设计参数，用于高效分离肺癌细胞，实现高通量、低成本的微流控系统开发。


<details>
  <summary>Details</summary>
Motivation: 解决循环肿瘤细胞(CTCs)检测中的稀有细胞识别挑战，减少对计算密集型模拟的依赖，提高DLD设备的设计效率。

Method: 采用梯度提升、K近邻、随机森林和多层感知机等机器学习模型，基于数值验证的大数据集预测粒子轨迹并识别最优设备配置。

Result: 机器学习模型成功预测粒子轨迹，识别关键设计变量，为DLD设备优化提供系统化、数据驱动的框架。

Conclusion: 这种集成方法推进了可扩展、精确的微流控系统开发，有助于实现早期癌症检测和个性化医疗的广泛目标。

Abstract: Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine.

</details>


### [73] [Physical Reinforcement Learning](https://arxiv.org/abs/2511.17789)
*Sam Dillavou,Shruti Mishra*

Main category: cs.LG

TL;DR: 将对比局部学习网络（CLLNs）从监督学习扩展到强化学习，展示了在模拟CLLNs上实现Q学习，解决了数字计算机在能源受限和不确定环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 数字计算机功耗高且对组件损坏敏感，不适合能源受限的自主智能体在不确定环境中使用。CLLNs作为模拟网络具有低功耗和物理损伤鲁棒性，但之前仅限于监督学习。

Method: 将Q学习算法适配到模拟的CLLNs中，明确识别了强化学习工具箱中除训练网络外的必要组件，包括策略函数、价值函数和回放缓冲区。

Result: 成功在两个简单的强化学习问题上应用了基于CLLNs的Q学习，证明了该方法的可行性。

Conclusion: CLLNs为强化学习提供了数字计算机无法实现的物理安全假设豁免和生物学相关次要目标训练能力，在能源受限和不确定环境中具有独特优势。

Abstract: Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.

</details>


### [74] [Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures](https://arxiv.org/abs/2511.17796)
*Afsaneh Mahanipour,Hana Khamfroush*

Main category: cs.LG

TL;DR: 提出了SSFMLFS方法，一种半监督联邦多标签特征选择方法，适用于客户端只有未标记数据、服务器有少量标记数据的场景，在非IID数据分布下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多标签特征选择方法需要集中式数据，不适合分布式和联邦环境；联邦方法通常假设客户端有标记数据，但现实中客户端可能缺乏标记能力。

Method: 将模糊信息理论应用于联邦设置，客户端计算模糊相似矩阵并传输给服务器，服务器计算特征冗余度和特征-标签相关性，构建特征图并使用PageRank对特征重要性排序。

Result: 在五个真实数据集上的实验表明，在非IID数据分布下，SSFMLFS在三种不同评估指标上优于其他联邦和集中式监督及半监督方法。

Conclusion: SSFMLFS有效解决了联邦环境中客户端只有未标记数据的多标签特征选择问题，在非IID数据分布下具有优越性能。

Abstract: Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.

</details>


### [75] [Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2511.17801)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出了一种基于二次优化的层特定高影响参数选择框架，在资源受限条件下实现LLM的高效量化，平衡计算效率和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法在极低位宽量化时存在显著精度损失，且固定比例的高影响参数保留策略忽略了层间敏感性差异。

Method: 使用二次优化框架确定层特定的高影响参数比例，考虑层间依赖关系，将高影响参数量化为中等位宽，其余参数量化为极低位宽。

Result: 在相同资源预算下比FP16保留方法保留更多高影响参数，实现了计算效率和模型精度的有效平衡。

Conclusion: 该方法在保持高性能的同时，相比现有最先进方法在LLM量化方面取得了更好的效果。

Abstract: Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods.

</details>


### [76] [Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2511.17809)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出自适应变换选择框架，根据每层特征选择最优变换类型，解决LLM量化中的异质分布问题，显著提升低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有变换方法对所有层使用相同变换类型，忽视了LLM中各层分布的异质性，导致量化性能下降，特别是在低比特设置下。

Method: 1) 将变换选择建模为可微优化问题；2) 建立权重分布峰度与准确变换类型的关联；3) 提出基于鲁棒z-score归一化的离群值引导层选择方法。

Result: 在LLaMA系列模型上，W3A3K2V2量化设置下，相比现有最佳方法FlatQuant，困惑度提升4.58分，六任务零样本准确率提升2.11%。

Conclusion: 异质变换选择对LLM量化至关重要，自适应方法能显著提升低比特量化性能。

Abstract: Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization.

</details>


### [77] [APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs](https://arxiv.org/abs/2511.17818)
*Aishwarya Mandyam,Kalyani Limaye,Barbara E. Engelhardt,Emily Alsentzer*

Main category: cs.LG

TL;DR: 利用大型语言模型生成反事实标注来增强离线策略评估，解决医疗领域数据集覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 标准离线策略评估方法受限于行为数据集的大小和覆盖范围，而人工获取专家标注的反事实注释成本高昂，限制了方法的可扩展性。

Method: 使用领域知识指导LLMs预测在替代治疗下关键临床特征的演变，然后通过已知奖励函数将这些预测特征转化为反事实标注，并将其整合到OPE估计器中。

Result: 在MIMIC-IV数据集上的实验表明，基于LLM的反事实标注在大多数情况下显著改善了OPE估计，但存在收益递减点。提出了基于熵的指标来识别何时额外标注不再有用。

Conclusion: LLM生成的反事实标注为医疗数据集覆盖不足问题提供了可扩展的解决方案，能够在临床环境中更安全地部署决策策略。

Abstract: Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.

</details>


### [78] [High-Accuracy List-Decodable Mean Estimation](https://arxiv.org/abs/2511.17822)
*Ziyun Chen,Spencer Compton,Daniel Kane,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了高精度列表可解码学习，提出了在列表可解码均值估计中实现高精度的新方法，通过增大列表大小来换取更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有列表可解码学习算法虽然能获得最优列表大小，但误差随1/α衰减较差。本文旨在探索是否可以通过增大列表大小来换取更高的准确性，即实现高精度列表可解码学习。

Method: 提出了全新的可识别性证明，并设计了不依赖平方和层次结构的算法，能够输出候选均值列表，其中一个元素与真实均值的ℓ2距离不超过ε。

Result: 证明了存在大小为L = exp(O(log²(1/α)/ε²))的候选均值列表，其中至少一个元素与真实均值的距离不超过ε。算法的时间和样本复杂度为n = d^O(log L) + exp exp(Õ(log L))。

Conclusion: 本文首次在列表可解码均值估计中实现了非平凡的高精度保证，为列表可解码学习中的精度-列表大小权衡提供了新的理论和技术基础。

Abstract: In list-decodable learning, we are given a set of data points such that an $α$-fraction of these points come from a nice distribution $D$, for some small $α\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $α$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / α$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $ε> 0$, can we can output a slightly larger list in terms of $α$ and $ε$, but so that one element of this list has error at most $ε$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \exp \left( O\left( \tfrac{\log^2 1 / α}{ε^2} \right)\right)$ so that one of the elements of this list has $\ell_2$ distance at most $ε$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\log L)} + \exp \exp (\widetilde{O}(\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.

</details>


### [79] [A novel k-means clustering approach using two distance measures for Gaussian data](https://arxiv.org/abs/2511.17823)
*Naitik Gada*

Main category: cs.LG

TL;DR: 本文提出了一种改进的k-means聚类算法，同时使用簇内距离(WCD)和簇间距离(ICD)作为距离度量，并通过Calinski-Harabasz准则确定最佳k值，以获得更鲁棒的聚类结果。


<details>
  <summary>Details</summary>
Motivation: 传统k-means聚类算法存在局限性，作者希望通过结合WCD和ICD两个度量指标来增强聚类算法的鲁棒性和准确性，使数据收敛到各自的簇更加稳定。

Method: 开发了一种改进的k-means算法，使用WCD和ICD作为距离度量，通过Calinski-Harabasz准则自动确定最佳聚类数量k。在合成数据和UCI基准数据集上进行测试验证。

Result: 实验结果表明，使用WCD和ICD双度量方法的数据收敛到各自簇的准确性更高，对异常值的聚类效果也优于传统k-means方法。

Conclusion: 提出的改进k-means算法通过结合WCD和ICD距离度量，显著提高了聚类的鲁棒性和准确性，为聚类分析提供了更可靠的方法。

Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.

</details>


### [80] [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)
*Ziyang Zhang,Xinheng Ding,Jiayi Yuan,Rixin Liu,Huizi Mao,Jiarong Xing,Zirui Liu*

Main category: cs.LG

TL;DR: 提出了Tree-Based Invariant Kernels (TBIK)来解决大语言模型推理中的张量并行规模相关非确定性问题，确保在不同TP大小下获得比特级相同的结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架存在非确定性问题：相同输入在不同系统配置（如TP大小、批大小）下会产生不同输出，这在LLM评估、多智能体系统和强化学习等应用中造成严重问题。

Method: 设计基于树的恒定内核(TBIK)，通过统一的层次二叉树结构对齐GPU内和GPU间的归约顺序，实现TP不变的矩阵乘法和归约原语。

Result: 实验证实了在不同TP大小下实现零概率发散和比特级可重现性，并在RL训练管道中实现vLLM和FSDP之间的比特级相同结果。

Conclusion: TBIK有效解决了TP引起的非一致性问题，为大语言模型应用提供了可靠的确定性推理保障。

Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [81] [Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization](https://arxiv.org/abs/2511.17829)
*Akhil Singampalli,Sudeep Pasricha*

Main category: cs.LG

TL;DR: MOELO是一个用于室内定位的持续学习框架，首次联合解决了领域增量学习和类别增量学习场景，通过专家混合架构实现轻量级、鲁棒且自适应的定位解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决室内定位中因移动设备硬件/软件差异导致的领域偏移，以及环境变化引入新位置导致的类别偏移问题，使静态机器学习模型能够适应动态环境。

Method: 采用专家混合架构，按区域增量训练专家，通过等角紧框架门控机制实现高效路由和低延迟推理，保持紧凑模型尺寸。

Result: 相比最先进框架，在多种建筑、移动设备和学习场景下，平均定位误差提升25.6倍，最差情况定位误差提升44.5倍，遗忘减少21.5倍。

Conclusion: MOELO提供了一个轻量级、鲁棒且自适应的室内定位解决方案，能够在资源有限的移动设备上部署，并在动态异构环境中实现持续学习。

Abstract: Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.

</details>


### [82] [Internalizing Tools as Morphisms in Graded Transformers](https://arxiv.org/abs/2511.17840)
*Tony Shaska*

Main category: cs.LG

TL;DR: 提出了一个分级的内部符号计算框架，通过类型化块映射和可微分路由策略实现选择性激活，统一了符号计算、几何和自监督学习。


<details>
  <summary>Details</summary>
Motivation: 将符号操作内部化到transformer中，避免外部工具调用的开销，同时保持可解释性和稀疏性。

Method: 使用分级隐藏空间和类型化块映射，通过自监督的效用函数控制激活，采用端到端可微分的路由机制。

Result: 在混合符号-语言任务上展示了选择性形态激活，能够内部化外部工具范式。

Conclusion: 该框架统一了符号计算、几何和自监督学习，为分级transformer提供了理论基础和方法实现。

Abstract: We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\bigoplus_{g\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $φ_{h\leftarrow g}:V_g\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \emph{graded transformer} formalism \cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \cite{toolformer2023}) as a special case via functorial internalization.

</details>


### [83] [Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks](https://arxiv.org/abs/2511.17848)
*Zhihui Tian,Ethan Suwandi,Tomas Oppelstrup,Vasily V. Bulatov,Joel B. Harley,Fei Zhou*

Main category: cs.LG

TL;DR: 提出结合CNN自编码器和GNN的混合架构，用于大规模微观结构模拟，显著降低计算成本和内存占用，同时提高准确性和时空建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在处理大规模晶粒生长模拟时面临计算成本和内存占用过高的问题，难以扩展到实际应用所需的大规模模拟单元。

Method: 使用基于CNN的双射自编码器压缩空间维度，在降维后的潜空间中使用GNN演化微观结构，减少消息传递层数（从12层降至3层）。

Result: 在最大网格（160^3）上，内存使用减少117倍，推理时间减少115倍，相比纯GNN基线具有更高准确性和更强的时空建模能力，特别是在长期测试中表现更优。

Conclusion: 该方法为模拟晶粒生长提供了高度可扩展的解决方案，结合了自编码器的无损信息压缩能力和GNN的学习能力，在保持准确性的同时显著提升计算效率。

Abstract: Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth.

</details>


### [84] [Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently](https://arxiv.org/abs/2511.17852)
*Bochen Lyu,Yiyang Jia,Xiaohao Cai,Zhanxing Zhu*

Main category: cs.LG

TL;DR: 本文通过理论分析比较了强化学习(RL)和监督微调(SFT)在训练Transformer学习k-稀疏布尔函数时的机制差异，发现RL同时学习整个思维链，而SFT逐步学习思维链。


<details>
  <summary>Details</summary>
Motivation: 虽然RL和SFT都可以让Transformer获得思维链推理能力，但它们的底层机制和差异在理论上仍不清楚，需要深入研究。

Method: 使用单层Transformer学习可递归分解为固定2-稀疏布尔函数的k-稀疏布尔函数，分析RL和SFT的学习动态，并验证在k-PARITY、k-AND和k-OR三个基本示例中的适用性。

Result: 证明了RL和SFT都能学习这些函数，但表现出不同的学习行为：RL同时学习整个思维链，SFT逐步学习思维链。

Conclusion: 研究为RL和SFT触发Transformer思维链能力的底层机制提供了理论见解，揭示了它们在学习行为上的本质差异。

Abstract: Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.

</details>


### [85] [Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds](https://arxiv.org/abs/2511.17861)
*Xuesong Jia,Yuanjie Shi,Ziquan Liu,Yi Xu,Yan Yan*

Main category: cs.LG

TL;DR: 提出一种简单的成本敏感共形训练算法，通过真实标签的排名权重策略来最小化预测集大小，无需依赖指示函数近似机制。


<details>
  <summary>Details</summary>
Motivation: 传统共形训练方法使用Sigmoid或高斯误差函数作为指示函数的替代，但这些替代函数没有统一的误差边界，导致学习边界不可控。

Method: 提出基于真实标签排名的权重策略，理论证明最小化预测集大小的期望值上界于真实标签的期望排名。

Result: 实验验证了理论洞察的有效性，在预测效率方面优于其他共形训练方法，平均预测集大小减少21.38%。

Conclusion: 所提出的成本敏感共形训练算法通过排名权重策略有效控制预测集大小，提供更紧密的理论保证和更好的实证性能。

Abstract: Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.

</details>


### [86] [Equivalence of Context and Parameter Updates in Modern Transformer Blocks](https://arxiv.org/abs/2511.17864)
*Adrian Goldwaser,Michael Munn,Javier Gonzalvo,Benoit Dherin*

Main category: cs.LG

TL;DR: 本文扩展了transformer中上下文影响可表示为MLP权重rank-1补丁的理论，证明该理论适用于现代LLM架构，并提出了基于输入可控性和输出可控性的通用框架。


<details>
  <summary>Details</summary>
Motivation: 扩展transformer上下文影响理论到现代多样化LLM架构，提供统一的理论框架来理解transformer如何将提示转换为有效权重。

Method: 首先为Gemma风格transformer块提供精确解析解，证明上下文影响可完美映射到MLP权重的rank-1补丁和RMSNorm尺度补丁；然后推广到多层模型，提出基于输入可控性和输出可控性的通用框架。

Result: 证明了对于任何输入可控内函数和输出可控外函数的MLP块，完美的隐式权重补丁是可能的，该框架适用于门控、预/后归一化、专家混合和顺序/并行transformer块等现代LLM架构。

Conclusion: 提供了一个更简单强大的理论框架来理解transformer模型如何将提示转换为有效权重，该框架统一了现代LLM架构中的上下文影响表示。

Abstract: Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.

</details>


### [87] [The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems](https://arxiv.org/abs/2511.17869)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出了MITD分层Transformer架构，通过任务分解检测和缓解奖励黑客攻击，在1000个HH-RLHF样本上实验显示12-25步分解深度可将奖励黑客频率降低34%。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI代理通过奖励黑客利用奖励信号缺陷的问题，这些代理获得高代理分数但未能实现真实目标。

Method: 引入机械可解释任务分解(MITD)分层Transformer架构，包含规划器、协调器和执行器模块，将任务分解为可解释子任务，并生成注意力瀑布图和神经通路流程图等诊断可视化。

Result: 在1000个HH-RLHF样本上的实验表明，12-25步的分解深度在四种失败模式下将奖励黑客频率降低了34%。

Conclusion: 机械基础的任务分解比事后行为监控能更有效地检测奖励黑客，提供了新的检测范式。

Abstract: Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.

</details>


### [88] [Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879)
*Yusong Wu,Stephen Brade,Teng Ma,Tia-Jane Fowler,Enning Yang,Berker Banar,Aaron Courville,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.LG

TL;DR: 提出一种对抗训练方法来缓解RL后训练中的奖励黑客问题，在旋律到和弦伴奏任务中保持输出多样性


<details>
  <summary>Details</summary>
Motivation: 实时即兴演奏需要实时协调和适应，而传统RL后训练会因奖励黑客问题降低输出多样性，影响音乐创造力

Method: 使用对抗训练方法，通过共同演化的判别器区分策略轨迹和数据分布，策略同时最大化判别器输出和连贯性奖励

Result: 在模拟和真实用户研究中，模型在输出多样性、和声连贯性、适应速度和用户控制方面均有改善

Conclusion: 该方法简单有效地缓解了生成序列模型RL后训练中的奖励黑客问题

Abstract: Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.

</details>


### [89] [Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing](https://arxiv.org/abs/2511.17902)
*Yifan He,Haodong Zhang,Qiuheng Song,Lin Lei,Zhenxuan Zeng,Haoyang He,Hongyan Wu*

Main category: cs.LG

TL;DR: 提出DUPLE元学习框架解决分布式光纤传感中跨部署场景下的活动识别问题，通过双域多原型学习、统计引导网络和查询感知原型聚合来应对信号域偏移、标注数据稀缺和类内多样性不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式光纤传感在实际应用中面临三大挑战：不同光纤部署类型导致信号模式差异（域偏移）、新部署场景标注数据稀缺、源域内数据不足难以捕捉类内多样性，限制了模型的适应性和鲁棒性。

Method: 1) 双域多原型学习器融合时域和频域特征；2) 统计引导网络从原始统计特征推断域重要性和原型敏感性；3) 查询感知原型聚合模块自适应选择和组合相关原型。

Result: 在跨部署DFOS数据集上的广泛实验表明，该方法在域泛化设置中显著优于基线方法，能够在最小标注数据下实现跨不同光纤配置的鲁棒事件识别。

Conclusion: DUPLE框架有效解决了DFOS系统中的域偏移和数据稀缺问题，为实际部署中的活动识别提供了可行的解决方案。

Abstract: Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.
  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.
  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.

</details>


### [90] [Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay](https://arxiv.org/abs/2511.17936)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 本文研究了在内存约束下使用状态回放机制处理流数据学习问题，通过梯度对齐分析证明了回放如何减少灾难性遗忘，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的流数据学习系统需要在内存约束下更新模型，顺序微调方法容易遭受灾难性遗忘，而有限缓冲区的回放方法在不同生成和预测任务中的行为尚未被充分理解。

Method: 将顺序微调和回放视为理想联合目标的随机梯度方法，使用梯度对齐分析来理解混合当前和历史样本如何减少遗忘，并在六个流场景中评估单一回放机制。

Result: 在异构多任务流中，回放将平均遗忘减少2-3倍；在良性时间流中，两种方法表现相似。

Conclusion: 状态回放是流环境中持续学习的一个强大而简单的基线方法。

Abstract: Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.

</details>


### [91] [On Transportability for Structural Causal Bandits](https://arxiv.org/abs/2511.17953)
*Min Woo Park,Sanghack Lee*

Main category: cs.LG

TL;DR: 该论文研究具有可迁移性的结构因果赌博机问题，通过融合来自不同环境的先验知识来增强部署环境中的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的结构因果赌博机框架虽然能利用因果知识优化动作空间，但缺乏从不同条件下收集的数据集（观测或实验数据）和异构环境中迁移信息的指导。

Method: 提出结构因果赌博机与可迁移性框架，利用跨环境的不变性，将源环境的先验知识融合到部署环境中。

Result: 开发的赌博机算法实现了亚线性遗憾界，明确依赖于先验数据的信息量，可能优于仅依赖在线学习的标准赌博机方法。

Conclusion: 通过利用跨环境的不变性，可以持续改进学习效果，融合先验知识的结构因果赌博机方法在部署环境中表现优越。

Abstract: Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.

</details>


### [92] [Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization](https://arxiv.org/abs/2511.17963)
*Jun Kevin,Pujianto Yugopuspito*

Main category: cs.LG

TL;DR: 提出融合LSTM预测和PPO强化学习的混合投资组合优化框架，在多种资产数据集上相比基准方法获得更高收益和更强韧性


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以适应非平稳市场环境，需要结合深度学习的预测能力和强化学习的动态调整能力

Method: 使用LSTM捕捉时间序列依赖关系进行预测，PPO智能体在连续动作空间中自适应优化资产配置，结合两种方法的优势

Result: 在2018-2024年多资产数据集上，混合框架相比等权重、指数型及单模型方法，在年化收益、夏普比率等指标上表现更优，且对交易成本调整后仍保持优势

Conclusion: 混合架构在非平稳市场环境下展现出更高收益和更强韧性，为动态投资组合优化提供了稳健的AI驱动解决方案

Abstract: This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.

</details>


### [93] [Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management](https://arxiv.org/abs/2511.17968)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出了一种集成联邦LSTM光伏预测与两阶段级联虚假数据注入攻击检测的微电网网络弹性框架，通过自编码器重构误差和预测不确定性量化实现攻击弹性储能调度，在极端攻击条件下显著恢复性能并降低经济损失。


<details>
  <summary>Details</summary>
Motivation: 解决微电网在遭受网络攻击时保持经济效率和运行可靠性的挑战，现有方法通常假设测量数据正常、预测不确定性未量化，且未能缓解对可再生能源预测的恶意攻击。

Method: 结合联邦LSTM光伏预测与两阶段级联虚假数据注入攻击检测，集成自编码器重构误差和预测不确定性量化，实现隐私保护下的攻击弹性储能调度优化。

Result: 在极端虚假数据攻击条件下（导致58%预测性能下降和16.9%运营成本增加），该框架将误报检测降低70%，恢复93.7%预测性能损失，实现5%运营成本节约，缓解34.7%攻击导致的经济损失。

Conclusion: 基于多信号融合的精确级联检测方法优于单信号方法，验证了去中心化微电网安全与性能的协同效应。

Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.

</details>


### [94] [Controllability Analysis of State Space-based Language Model](https://arxiv.org/abs/2511.17970)
*Mohamed Mabrok,Yalda Zafari*

Main category: cs.LG

TL;DR: 提出了影响分数作为解释Mamba状态空间模型内部动态的度量工具，通过实验验证其在模型规模、架构模式和涌现行为方面的分析能力。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（特别是Mamba）在序列建模中表现出色，但其内部动态机制相比基于注意力的模型理解不足，需要开发有效的解释性工具。

Method: 引入基于可控性的影响分数，从离散化状态空间参数推导，通过类似系统可观测性的反向递推计算，量化位置k的token对后续状态和输出的影响强度。

Result: 在三个Mamba变体上的六项实验显示：影响分数随模型规模和训练数据增加；Mamba具有近期偏好和中后层影响集中的架构模式；仅在大规模模型中观察到涌现行为，如优先处理内容词和噪声下降低内部影响。

Conclusion: 影响分数是解释和比较基于状态空间模型语言模型的实用诊断工具，能够揭示模型容量、架构特性和涌现行为。

Abstract: State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models.

</details>


### [95] [Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks](https://arxiv.org/abs/2511.17978)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出了一种新颖的异常弹性联邦学习框架，用于电动汽车充电基础设施的网络安全预测，同时保护数据隐私、检测网络攻击，并在对抗条件下保持可信的需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施面临日益严重的网络安全威胁，现有预测技术缺乏结合鲁棒异常缓解解决方案和数据隐私保护的能力。

Method: 集成三个关键创新：基于LSTM自动编码器的分布式异常检测、基于插值的异常数据缓解以保持时间连续性，以及联邦LSTM网络实现无需集中数据聚合的协作学习。

Result: 联邦方法相比集中式模型性能提升15.2%的R2精度，同时保持数据本地性。集成网络攻击检测和缓解系统恢复47.9%的攻击引起性能下降，保持91.3%的精确度和1.21%的低误报率。

Conclusion: 该架构能够增强电动汽车基础设施规划、隐私保护协作预测、网络安全弹性，并在分布式充电网络中实现从恶意威胁的快速恢复。

Abstract: Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.

</details>


### [96] [An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter](https://arxiv.org/abs/2511.17983)
*Naoki Masuyama,Yuichiro Toda,Yusuke Nojima,Hisao Ishibuchi*

Main category: cs.LG

TL;DR: 提出基于自适应共振理论(ART)的拓扑聚类算法，通过多样性驱动的适应机制自动调整重计算间隔和警戒阈值，实现无超参数学习，在动态环境中保持聚类稳定性和连续性。


<details>
  <summary>Details</summary>
Motivation: 解决静态和非静态设置中的聚类问题，需要能够适应分布变化同时保持已学习聚类结构的模型。

Method: 基于自适应共振理论(ART)的拓扑聚类算法，采用多样性驱动的适应机制自动调整重计算间隔和警戒阈值。

Result: 在24个真实世界数据集上的实验表明，该算法在聚类性能和持续学习能力方面优于最先进方法。

Conclusion: 所提出的参数适应机制在缓解灾难性遗忘和保持演化数据流中一致性聚类方面具有有效性。

Abstract: Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT

</details>


### [97] [Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors](https://arxiv.org/abs/2511.17987)
*Jinping Wang,Zhiqiang Gao,Dinggen Zhang,Zhiwu Xie*

Main category: cs.LG

TL;DR: 提出了基于差异向量的各向异性缩放迭代算法(DV-BASI)，通过使用优化过程中的历史移动作为定向扰动，克服任务算术方法的优化停滞问题，实现连续优化过程。


<details>
  <summary>Details</summary>
Motivation: 当前预训练模型编辑方法面临高计算成本和有限可扩展性的挑战，任务算术方法虽然前景广阔，但由于优化停滞机制有限，其潜力尚未充分发掘。

Method: 引入差异向量概念，作为任务向量的广义形式，源自优化过程中的历史移动。使用差异向量作为定向扰动，提出DV-BASI算法实现连续优化，无需额外模块。

Result: DV-BASI在多任务模型合并中的平均性能甚至可能超过单独微调的模型，并扩展到单任务模型的可行微调方法。在监督和无监督评估协议上达到最先进性能。

Conclusion: 差异向量提供了表达性搜索方向，形成可扩展框架，与任务算术方法和先进优化技术结合，实现了显著的性能提升。

Abstract: Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols.

</details>


### [98] [Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks](https://arxiv.org/abs/2511.17989)
*Jiayi Luo,Qingyun Sun,Yuecen Wei,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: MGP-MIA是一个针对多域图预训练模型的成员推理攻击框架，通过机器遗忘放大成员信号、增量学习构建影子模型、基于相似度的推理机制来识别训练数据成员。


<details>
  <summary>Details</summary>
Motivation: 多域图预训练虽然提高了图神经网络的泛化能力，但其在成员推理攻击下的隐私风险尚未被充分探索。现有方法面临泛化能力增强、影子数据集不具代表性、成员信号减弱等挑战。

Method: 1. 成员信号放大机制：通过机器遗忘放大目标模型的过拟合特征；2. 增量影子模型构建：利用增量学习构建可靠的影子模型；3. 基于相似度的推理机制：根据样本与正负样本的相似度识别成员。

Result: 大量实验证明MGP-MIA的有效性，揭示了多域图预训练存在的隐私风险。

Conclusion: 该研究提出了首个针对多域图预训练模型的成员推理攻击框架，成功克服了现有挑战，为图基础模型的隐私保护提供了重要启示。

Abstract: Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training.

</details>


### [99] [Learning Rate Scheduling with Matrix Factorization for Private Training](https://arxiv.org/abs/2511.17994)
*Nikita P. Kalinin,Joel Daniel Andersson*

Main category: cs.LG

TL;DR: 本文研究了在差分隐私模型训练中，结合学习率调度和相关噪声的随机梯度下降方法。提出了学习率感知的矩阵分解方法，相比前缀和分解在误差指标上有所改进。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究主要关注恒定学习率下的前缀和工作负载，而实践中学习率调度被广泛用于加速训练和改善收敛。需要填补这一理论空白。

Method: 推导了单轮和多轮设置下广泛学习率调度类别的上下界，提出了学习率感知的矩阵分解方法，并设计了适合实际部署的内存高效构造。

Result: 在CIFAR-10和IMDB数据集上的实验证实，调度感知的分解方法在私有训练中提高了准确性。

Conclusion: 学习率感知的矩阵分解方法在差分隐私模型训练中优于传统前缀和分解，为实际部署提供了有效的解决方案。

Abstract: We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.

</details>


### [100] [Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning](https://arxiv.org/abs/2511.18000)
*Radman Rakhshandehroo,Daniel Coombs*

Main category: cs.LG

TL;DR: ContagionRL是一个兼容Gymnasium的强化学习平台，专门用于空间流行病模拟中的系统化奖励工程研究。


<details>
  <summary>Details</summary>
Motivation: 传统基于智能体的模型依赖固定行为规则，而该平台旨在系统评估奖励函数设计如何影响不同流行病场景下的学习生存策略。

Method: 平台集成空间SIRS+D流行病学模型，具有可配置环境参数，评估五种不同奖励设计（从稀疏生存奖励到新型势场方法）在多种RL算法（PPO、SAC、A2C）下的表现。

Result: 系统消融研究发现方向性指导和明确依从激励是稳健策略学习的关键组成部分。势场奖励训练的智能体始终表现最优，学会最大程度遵守非药物干预措施并发展复杂空间规避策略。

Conclusion: ContagionRL是研究流行病背景下适应性行为反应的有效平台，强调了奖励设计、信息结构和环境可预测性在学习中的重要性。

Abstract: We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.

</details>


### [101] [Understanding Private Learning From Feature Perspective](https://arxiv.org/abs/2511.18006)
*Meng Ding,Mingxi Lei,Shaopeng Fu,Shaowei Wang,Di Wang,Jinhui Xu*

Main category: cs.LG

TL;DR: 提出了首个从特征学习角度分析差分隐私训练的理论框架，揭示了隐私训练需要更高信噪比，且会继承非隐私训练中的数据噪声记忆问题


<details>
  <summary>Details</summary>
Motivation: 尽管利用预训练模型特征增强DP-SGD训练取得了显著经验进展，但隐私学习中特征动态的理论理解仍然不足，现有DP分析忽视了标签相关特征信号和标签无关噪声的关键区别

Method: 基于多补丁数据结构，使用具有多项式ReLU激活的两层CNN，通过噪声梯度下降理论分析私有训练中的特征信号学习和数据噪声记忆

Result: 发现：(1)有效私有信号学习需要比非私有训练更高的信噪比；(2)当非私有学习中出现数据噪声记忆时，私有学习也会出现，导致训练损失小但泛化性能差

Conclusion: 研究强调了私有学习的挑战，并证明了特征增强提高信噪比的益处，在合成和真实数据集上的实验验证了理论发现

Abstract: Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings.

</details>


### [102] [Curvature-Aware Safety Restoration In LLMs Fine-Tuning](https://arxiv.org/abs/2511.18039)
*Thong Bach,Thanh Nguyen-Tang,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 该论文发现微调LLMs会损害安全对齐，但有害内容的损失几何结构得以保留。作者提出一种曲率感知对齐恢复方法，利用影响函数和二阶优化选择性增加有害输入的损失，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型进行下游任务时往往会损害安全对齐，即使使用LoRA等参数高效方法。研究发现微调模型在有害内容上的损失几何结构得以保留，表明安全行为并未被删除而是转移到参数空间中影响力较小的区域。

Method: 提出曲率感知对齐恢复方法，利用影响函数和二阶优化，在基础模型和微调模型共享几何结构的基础上，选择性增加有害输入的损失，同时保持任务相关性能。

Result: 在多个模型系列和对抗设置下的广泛评估表明，该方法能有效减少有害响应，同时保持甚至提高实用性和少样本学习性能。

Conclusion: 通过利用微调模型保留的几何结构，提出的方法能够精确、低影响地恢复安全对齐，避免完全回滚，在保持任务性能的同时有效抑制不安全输出。

Abstract: Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.

</details>


### [103] [Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics](https://arxiv.org/abs/2511.18056)
*Maximilien Dreveton,Matthias Grossglauser,Daichi Kuroda,Patrick Thiran*

Main category: cs.LG

TL;DR: 论文提出了有效层次结构的概念，定义了有效层次结构的偏序关系，证明了最精细有效层次结构的存在性，并提出了一个两步算法来恢复该结构。


<details>
  <summary>Details</summary>
Motivation: 传统层次聚类方法存在三个主要局限：总是返回层次结构（即使不存在）、仅限于二叉树结构、对链接函数选择高度敏感。

Method: 提出有效层次结构概念，定义偏序关系，证明最精细有效层次结构存在，设计两步算法（先用链接方法构建二叉树，然后剪枝使其有效）。

Result: 建立了链接函数恢复最精细有效层次结构的充要条件，证明单链接、完全链接、平均链接等经典方法满足条件，而Ward链接不满足。

Conclusion: 该方法能够自动适应数据的真实层次结构，当不存在层次关系时退化为星形树，解决了传统方法的局限性。

Abstract: Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.

</details>


### [104] [pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data](https://arxiv.org/abs/2511.18066)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Mir Sazzat Hossain,Rakibul Hasan Rajib,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出了pFedBBN框架，用于解决联邦学习中测试时适应面临的类别不平衡问题，通过平衡批归一化和基于相似度的客户端协作来提升少数类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的类别不平衡问题导致关键但稀有的类别在单个客户端数据集中代表性不足，现有方法无法在无标签数据下处理动态领域变化和分布偏移。

Method: 使用平衡批归一化(BBN)在本地客户端适应中平等对待所有类别，通过BBN相似度指导客户端协作，采用类别感知的模型聚合策略实现个性化推理。

Result: 在多个基准测试中，pFedBBN相比最先进的联邦学习和测试时适应方法，持续提升了鲁棒性和少数类性能。

Conclusion: pFedBBN通过平衡特征归一化和领域感知协作，有效解决了分布偏移和类别不平衡问题，无需客户端的标签或原始数据。

Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

</details>


### [105] [The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality](https://arxiv.org/abs/2511.18084)
*Dou Liu,Ying Long,Sophia Zuoqiu,Kaipeng Xie,Runze Yang,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.LG

TL;DR: 评估四种LLM对齐策略在临床决策中的表现，发现GRPO算法精度最高，但医生更偏好SFT模型，揭示了算法改进与临床信任之间的对齐悖论


<details>
  <summary>Details</summary>
Motivation: LLM在临床决策支持中应用日益广泛，但如何使其与真实医学的多维度推理路径对齐仍面临挑战

Method: 使用8000+不孕症治疗记录，通过自动基准测试和盲法医生评估的双层框架，系统评估SFT、DPO、GRPO和ICL四种对齐策略

Result: GRPO在多个决策层获得最高算法精度，但临床医生更偏好SFT模型，认为其推理过程更清晰、治疗可行性更高。在盲法配对比较中，SFT获胜率最高（51.2%）

Conclusion: 算法改进不一定转化为更高的临床信任，可能偏离以人为中心的偏好，需要优先考虑临床可解释性和实践可行性的对齐策略

Abstract: Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.

</details>


### [106] [A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization](https://arxiv.org/abs/2511.18093)
*Fulong Yao,Wanqing Zhao,Matthew Forshaw*

Main category: cs.LG

TL;DR: 提出一种新的误差时间差分(ETD)算法来解决微电网能量优化中预测模型不确定性导致的控制策略次优问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的预测控制方法往往忽视预测模型不完善带来的不确定性，这会导致控制策略性能下降。

Method: 首先建立包含可再生能源和储能系统的微电网系统及其马尔可夫决策过程模型；然后提出基于深度Q网络的预测控制方法，设计加权平均算法和新ETD算法分别量化和处理预测不确定性。

Result: 在真实美国数据集上的仿真表明，所开发的ETD算法有效提高了深度强化学习在优化微电网运行方面的性能。

Conclusion: 新提出的ETD算法能够有效解决预测不确定性，提升微电网能量优化控制策略的性能。

Abstract: Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.

</details>


### [107] [Active Learning with Selective Time-Step Acquisition for PDEs](https://arxiv.org/abs/2511.18107)
*Yegon Kim,Hyunsu Kim,Gyeonghoon Ko,Juho Lee*

Main category: cs.LG

TL;DR: 提出了一种用于PDE代理建模的主动学习框架，通过策略性地仅生成最重要的时间步来降低数值求解器的计算成本，同时使用代理模型近似其余步骤。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高昂，而代理模型的开发受限于从数值求解器生成足够训练数据的成本。需要一种更高效的方法来减少PDE代理建模的数据生成成本。

Method: 开发了一种主动学习框架，策略性地选择最重要的时间步用数值求解器生成，其余步骤使用代理模型近似。提出了一个获取函数来估计时间步集的效用，通过近似其方差减少。

Result: 在多个基准PDE上验证了方法的有效性，包括Burgers方程、KdV方程、Kuramoto-Sivashinsky方程、不可压缩和可压缩Navier-Stokes方程。实验显示该方法在性能上大幅优于现有最佳方法，不仅降低了平均误差，还降低了99%、95%和50%分位数的误差。

Conclusion: 该方法为PDE代理建模提供了一个数据高效的解决方案，显著减少了计算成本，同时提高了模型性能。

Abstract: Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\%, 95\%, and 50\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.

</details>


### [108] [Vulnerability-Aware Robust Multimodal Adversarial Training](https://arxiv.org/abs/2511.18138)
*Junrui Zhang,Xinyu Zhao,Jie Peng,Chenjie Wang,Jianmin Ji,Tianlong Chen*

Main category: cs.LG

TL;DR: VARMAT是一种多模态对抗训练方法，通过量化每个模态的脆弱性并针对性正则化，提升多模态模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视不同模态对最终鲁棒性的贡献差异，导致鲁棒性表现不佳

Method: VARMAT首先基于攻击目标的一阶近似显式量化每个模态的脆弱性，然后提出针对性正则化项惩罚高脆弱性模态

Result: 在多个多模态数据集上实现显著鲁棒性提升，三个数据集分别提升12.73%、22.21%、11.19%

Conclusion: 该方法揭示了多模态对抗训练中的重要盲点，通过模态脆弱性感知显著提升模型鲁棒性

Abstract: Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.

</details>


### [109] [Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction](https://arxiv.org/abs/2511.18150)
*Randy Davila,Beyzanur Ispir*

Main category: cs.LG

TL;DR: 比较CNN和GNN在近似图支配数方面的性能，发现GNN在准确性和速度方面都显著优于CNN。


<details>
  <summary>Details</summary>
Motivation: 图支配数的精确计算是NP难问题，传统方法只能处理小规模图实例，需要开发高效的近似方法。

Method: 使用CNN（基于邻接矩阵）和GNN（基于图结构）两种神经网络范式，在2000个最多64个顶点的随机图上进行实验比较。

Result: GNN获得显著更高的准确率（R²=0.987，MAE=0.372），而CNN为（R²=0.955，MAE=0.500）。GNN提供超过200倍的加速，同时保持接近完美的保真度。

Conclusion: GNN可作为组合图不变量的实用替代方法，对可扩展图优化和数学发现具有重要意义。

Abstract: We investigate machine learning approaches to approximating the \emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.

</details>


### [110] [scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python](https://arxiv.org/abs/2511.18157)
*Martin Schuck,Alexander von Rohr,Angela P. Schoellig*

Main category: cs.LG

TL;DR: 对SciPy的spatial.transform模块进行全面重构，使其兼容所有实现Python数组API的库（如JAX、PyTorch、CuPy），支持GPU/TPU执行、JIT编译、向量化批处理和自动微分。


<details>
  <summary>Details</summary>
Motivation: 现有的SciPy spatial.transform模块仅支持NumPy，限制了在GPU加速和自动微分工作流中的采用，而3D刚体变换在机器人、视觉和模拟等领域的可微分机器学习管道中至关重要。

Method: 重写SciPy spatial.transform功能，使其与任何实现Python数组API的库兼容，保持现有接口的同时支持现代机器学习框架的特性。

Result: 重构后的实现支持多种后端，包括GPU/TPU执行、JIT编译、向量化批处理和自动微分，并通过两个案例研究验证了其有效性。

Conclusion: 该贡献已合并到SciPy主分支，将在下一个版本中发布，为可微分系统和机器学习中的3D空间数学提供了框架无关、生产级的基础设施。

Abstract: Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's spatial.transform module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's spatial.transform functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML.

</details>


### [111] [LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation](https://arxiv.org/abs/2511.18158)
*Abdelrahman Abdelmotlb,Abdallah Taman,Sherif Mostafa,Moustafa Youssef*

Main category: cs.LG

TL;DR: LocaGen是一个新颖的空间增强框架，通过条件扩散模型在未见位置生成高质量的合成指纹数据，显著减少室内定位系统的指纹采集开销。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位系统需要大量位置标记信号数据采集，部署成本高。现有方法要么表示能力有限，要么存在模式崩溃问题，或者仍需在所有目标位置采集数据。

Method: 使用条件扩散模型结合空间感知优化策略，基于部分已见位置生成未见位置的合成指纹数据；通过领域特定启发式方法增强已见位置数据，并采用基于密度的策略选择已见和未见位置以确保鲁棒覆盖。

Result: 在真实WiFi指纹数据集上的评估显示，LocaGen在30%位置未见的情况下仍能保持相同的定位精度，相比最先进的增强方法精度提升高达28%。

Conclusion: LocaGen通过生成未见位置的合成指纹数据，显著降低了指纹定位系统的部署成本，同时保持了高定位精度。

Abstract: Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.

</details>


### [112] [Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models](https://arxiv.org/abs/2511.18159)
*Mengni Jia,Mengyu Zhou,Yihao Liu,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散模型训练方差高的根本原因，提出了两种核心方差减少方法P-POTS和MIRROR，显著提升了MDM在复杂推理任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型作为自回归模型的有前景替代方案，存在训练方差过高的问题，导致梯度估计噪声大、优化不稳定，使得在任务特定训练后MDM性能远落后于ARM。目前缺乏理论解释和系统解决方案。

Method: 1）首次将MDM训练方差分解为三个来源：掩码模式噪声、掩码率噪声和数据噪声；2）设计了六种方差减少方法，包括核心方法P-POTS（帕累托最优t采样器）和MIRROR（使用负相关样本减少掩码模式噪声）。

Result: 相比标准MDM训练，所提方法在复杂推理任务上准确率提升7-8%，同时将运行间变异性降低到接近ARM水平，显著缩小了与强ARM基线的差距。

Conclusion: 通过理论分析和系统方差减少方法，成功解决了MDM训练方差高的问题，使MDM在保持竞争力的同时获得更稳定的优化性能。

Abstract: Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method.

</details>


### [113] [Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability](https://arxiv.org/abs/2511.18178)
*Shrenik Zinage,Peter Meckl,Ilias Bilionis*

Main category: cs.LG

TL;DR: 提出贝叶斯校准框架，结合高斯过程和近似贝叶斯计算来推断和校正传感器偏差，解决发动机间差异导致的NOx预测泛化问题


<details>
  <summary>Details</summary>
Motivation: 传统模型在少量发动机数据上训练，难以泛化到整个发动机群体，存在传感器偏差和输入条件变化问题，需要能够适应发动机间差异的模型

Method: 使用贝叶斯校准框架，结合高斯过程和近似贝叶斯计算，从预训练模型出发推断发动机特定传感器偏差并重新校准预测

Result: 该方法在未见测试数据上生成发动机出口NOx的后验预测分布，相比传统非自适应GP模型显著提高了预测精度

Conclusion: 该可转移建模方法有效解决了发动机间变异性，提高了模型泛化能力，无需重新训练模型即可实现高精度预测

Abstract: Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability.

</details>


### [114] [MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning](https://arxiv.org/abs/2511.18181)
*Adam Callaghan,Karl Mason,Patrick Mannion*

Main category: cs.LG

TL;DR: 提出了首个针对连续状态和动作空间的多目标多智能体强化学习框架MOMA-AC，基于TD3和DDPG算法，通过多头actor网络、集中式critic和目标偏好条件化架构，实现在连续MOMARL设置中编码所有智能体的帕累托最优策略前沿。


<details>
  <summary>Details</summary>
Motivation: 解决多目标多智能体强化学习在连续状态和动作空间中的关键空白，现有方法主要针对离散空间或单智能体设置，缺乏专门针对连续MOMARL的内循环actor-critic框架。

Method: 基于单目标单智能体算法，构建MOMA-AC框架，包含多头actor网络、集中式critic和目标偏好条件化架构，实例化为MOMA-TD3和MOMA-DDPG算法。

Result: 在协作运动任务评估中，相比外循环和独立训练基线，在期望效用和超体积指标上取得了统计显著改进，且在智能体数量增加时保持稳定可扩展性。

Conclusion: 该框架为连续多智能体领域中的稳健、可扩展多目标策略学习奠定了基础性步骤。

Abstract: This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.

</details>


### [115] [Accelerating Time Series Foundation Models with Speculative Decoding](https://arxiv.org/abs/2511.18191)
*Pranav Subbaraman,Fang Sun,Yue Yao,Huacong Tang,Xiao Luo,Yizhou Sun*

Main category: cs.LG

TL;DR: 提出了STRIDE框架，将推测解码技术应用于自回归时间序列模型，通过小型草稿模型预测时间序列片段，再由大型目标模型并行验证，显著提升推理速度而不损失精度。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型在时间序列预测中表现出色，但计算成本高，难以在延迟敏感的Web应用中部署。需要一种无需修改模型架构的推理加速方法。

Method: 采用推测解码框架，使用小型草稿模型生成未来时间序列片段，大型目标模型并行验证这些预测。解决了从离散语言标记到连续时间序列分布的适应问题，包括多变量高斯片段的接受标准和实用变体设计。

Result: 在Web应用相关的时间序列预测基准测试中，实现了显著的推理加速，同时保持了有竞争力的准确性。

Conclusion: STRIDE框架无需修改现有基础模型架构，可立即应用于加速已部署的时间序列预测系统，为延迟敏感的Web应用提供了实用的解决方案。

Abstract: Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller "draft" model to propose future time-series patches, which are then verified in parallel by a larger "target" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at https://github.com/PranavSubbaraman/STRIDE

</details>


### [116] [Deep Gaussian Process Proximal Policy Optimization](https://arxiv.org/abs/2511.18214)
*Matthijs van der Lende,Juan Cardenas-Cartagena*

Main category: cs.LG

TL;DR: 提出了Deep Gaussian Process Proximal Policy Optimization (GPPO)，一种利用深度高斯过程来近似策略和价值函数的可扩展、无模型的actor-critic算法，在保持与PPO相当性能的同时提供校准良好的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的不确定性估计对于需要平衡安全探索和高效学习的控制任务至关重要，而深度神经网络通常缺乏校准的不确定性估计。

Method: 使用深度高斯过程来近似策略和价值函数，开发了可扩展的、无模型的actor-critic算法GPPO。

Result: 在标准高维连续控制基准测试中，GPPO保持了与PPO相当的性能，同时提供了校准良好的不确定性估计，能够指导更安全和更有效的探索。

Conclusion: GPPO算法成功地将深度高斯过程集成到强化学习中，为控制任务提供了可靠的不确定性估计，有助于实现更安全的探索策略。

Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.

</details>


### [117] [Adaptive Conformal Prediction for Quantum Machine Learning](https://arxiv.org/abs/2511.18225)
*Douglas Spencer,Samual Nicholls,Michele Caprio*

Main category: cs.LG

TL;DR: 提出了自适应量子保形预测(AQCP)算法，通过持续重新校准来应对量子处理器中的时变噪声，在任意硬件噪声条件下保持渐近平均覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习需要可靠的不确定性量化方法，但当前量子领域缺乏稳健的不确定性量化技术。量子处理器固有的时变噪声会破坏保形预测的保证，即使在校准和测试数据可交换的情况下。

Method: 基于自适应保形推理方法，引入自适应量子保形预测(AQCP)算法，通过重复重新校准来维持随时间变化的有效性。

Result: 在IBM量子处理器上的实证研究表明，AQCP能够达到目标覆盖水平，并且比量子保形预测表现出更高的稳定性。

Conclusion: AQCP算法在量子硬件噪声条件下有效维持了保形预测的覆盖保证，为量子机器学习提供了更可靠的不确定性量化方法。

Abstract: Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.

</details>


### [118] [Tail Distribution of Regret in Optimistic Reinforcement Learning](https://arxiv.org/abs/2511.18247)
*Sajad Khodadadian,Mehrdad Moharrami*

Main category: cs.LG

TL;DR: 本文提出了基于乐观主义的强化学习算法在有限时域表格MDP中的实例依赖后悔尾界分析，揭示了后悔分布的双机制结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注期望后悔或单一高概率分位数，缺乏对后悔分布完整尾部分析的全面理解。

Method: 分析UCBVI类算法，研究两种探索奖励方案：K依赖方案和K独立方案，通过调节参数α平衡期望后悔和子高斯尾范围。

Result: 获得了后悔概率上界，显示从实例依赖尺度m_K到转移阈值为子高斯尾，超过该阈值后为子威布尔尾的双机制结构。

Conclusion: 为情景强化学习中标准乐观算法提供了首个全面的后悔尾分布保证，揭示了后悔分布的精细结构特征。

Abstract: We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.

</details>


### [119] [Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj](https://arxiv.org/abs/2511.18248)
*Wei Zhen Teoh*

Main category: cs.LG

TL;DR: CausalTraj是一个基于因果时序的轨迹预测模型，专注于生成联合概率的多智能体轨迹预测，在保持个体预测准确性的同时显著提升了联合预测质量。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要基于个体精度指标（minADE、minFDE）进行优化，忽略了联合预测的合理性，导致无法生成连贯的多智能体场景。

Method: 提出了CausalTraj模型，这是一个基于时序因果关系的似然模型，专门设计用于生成联合概率的多智能体轨迹预测。

Result: 在NBA SportVU、Basketball-U和Football-U数据集上，CausalTraj在个体精度指标上表现有竞争力，在联合指标（minJADE、minJFDE）上取得了最佳记录。

Conclusion: CausalTraj能够生成连贯且真实的多智能体轨迹预测，强调了联合评估指标在团队运动分析中的重要性。

Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.

</details>


### [120] [Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data](https://arxiv.org/abs/2511.18260)
*Yueqi Wang,Guang Lin*

Main category: cs.LG

TL;DR: RB-DeepONet是一种混合算子学习框架，将降基数值结构与DeepONet的分支-主干架构融合，用于加速参数化PDE求解。它使用离线构造的降基空间作为主干，分支网络预测降基系数，实现物理可解释性、稳定性和误差控制。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法存在主干网络不透明、需要大量标注数据、在边界和源数据与物理参数独立变化时失效等问题。需要一种高效、稳定且可解释的参数化PDE求解方法。

Method: 融合降基数值结构与DeepONet架构，主干固定为贪婪选择构造的降基空间，分支网络预测降基系数，使用投影变分残差进行无标签训练。开发边界和源模态编码处理独立变化的载荷或边界条件。

Result: RB-DeepONet在精度上与侵入式RB-Galerkin、POD-DeepONet和FEONet竞争，同时使用显著更少的可训练参数并实现显著加速。

Conclusion: RB-DeepONet为大规模参数化PDE提供了一种高效、稳定且可解释的算子学习方法，实现了严格的离线-在线分离，在线评估仅与降基维度相关。

Abstract: Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.

</details>


### [121] [A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks](https://arxiv.org/abs/2511.18269)
*Ved Mohan,El Mehdi Er Raqabi,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 本文提出了一个结合运筹学和机器学习的通用框架，用于在大规模物流网络中实现公平的资源替代，显著减少了模型规模和执行时间。


<details>
  <summary>Details</summary>
Motivation: 大规模物流网络中资源需求模式不均衡导致节点持续不平衡，资源替代是缓解这种不平衡的成本效益方法，但在去中心化环境中实现全局协调解决方案更加困难。

Method: 结合运筹学（OR）和机器学习（ML）的框架：OR组件在公平视角下建模和解决资源替代问题；ML组件利用历史数据学习调度员偏好，指导决策空间智能探索，并通过动态选择网络中每条弧的前κ个资源来提高计算效率。

Result: 在全球最大包裹递送公司之一的网络中应用该框架，计算结果显示相比最先进方法有显著改进：模型规模减少80%，执行时间减少90%，同时保持最优性。

Conclusion: 该框架产生高质量解决方案组合，调度员可以从中选择满意的权衡方案，为大规模物流网络中的公平资源替代问题提供了有效解决方案。

Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$κ$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.

</details>


### [122] [From Tables to Signals: Revealing Spectral Adaptivity in TabPFN](https://arxiv.org/abs/2511.18278)
*Jianqiao Zheng,Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文通过信号重构的视角研究TabPFN，发现其具有比标准ReLU-MLP更广的有效频率容量，且其频谱能力能根据上下文样本数量自适应调整，这种特性使其能够进行无需训练和超参数调整的图像去噪。


<details>
  <summary>Details</summary>
Motivation: 理解任务无关的表格基础模型（如TabPFN）的归纳偏置来源，这些模型在表格学习任务中表现出色但其内在机制尚不清楚。

Method: 通过信号重构的视角分析TabPFN，采用频率分析方法研究其上下文学习行为，比较其与标准ReLU-MLP的频率容量差异，并研究位置编码对频率响应的影响。

Result: TabPFN具有比标准ReLU-MLP更广的有效频率容量，其频谱能力能根据上下文样本数量自适应调整（称为频谱自适应性），位置编码能调节其频率响应，这些特性使其能够进行无需训练和超参数调整的图像去噪。

Conclusion: 该分析为表格基础模型的结构和归纳偏置提供了新的见解，并突显了它们在更广泛信号重构任务中的潜力。

Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.

</details>


### [123] [TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis](https://arxiv.org/abs/2511.18287)
*Rui Peng,Ziru Liu,Lingyuan Ye,Yuxing Lu,Boxin Shi,Jinzhuo Wang*

Main category: cs.LG

TL;DR: TRIDENT是一个级联生成框架，通过同时考虑扰动和相应基因表达谱来合成真实的细胞形态，显著优于现有方法，在未见化合物上表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只建模直接关联（如扰动→RNA或扰动→形态），而忽略了从RNA到形态的关键因果联系，这限制了构建AI虚拟细胞的能力。

Method: 提出TRIDENT级联生成框架，构建MorphoGene数据集（包含98种化合物的L1000基因表达和Cell Painting图像配对），通过RNA条件化合成细胞形态。

Result: TRIDENT显著优于最先进方法，实现高达7倍的改进，在未见化合物上具有强泛化能力。案例研究证实RNA引导的合成能准确产生相应表型。

Conclusion: 通过明确建模转录组-表型组映射，TRIDENT提供了一个强大的in silico工具，推动我们更接近预测性虚拟细胞。

Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.

</details>


### [124] [MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding](https://arxiv.org/abs/2511.18294)
*Mengchun Zhang,Kateryna Shapovalenko,Yucheng Shao,Eddie Guo,Parusha Pradhan*

Main category: cs.LG

TL;DR: MultiDiffNet是一个基于扩散模型的框架，通过优化多目标学习的紧凑潜在空间，实现跨被试的脑电图解码，无需生成式数据增强。


<details>
  <summary>Details</summary>
Motivation: 脑电图神经解码面临跨被试泛化能力差的问题，主要由于被试间差异大且缺乏大规模数据集。现有方法依赖合成被试生成或简单数据增强，但无法可靠扩展或泛化。

Method: 提出MultiDiffNet框架，学习一个为多目标优化的紧凑潜在空间，直接从该空间进行解码。同时构建了统一的基准测试套件和针对低试次脑电图设置的统计报告框架。

Result: 在各种神经解码任务中使用被试和会话分离评估，实现了最先进的泛化性能。发布了涵盖四种复杂度递增的脑电图解码任务的基准套件。

Conclusion: 该工作为现实世界脑机接口系统中的被试无关脑电图解码提供了可复现和开源的基础。

Abstract: Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.

</details>


### [125] [GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis](https://arxiv.org/abs/2511.18297)
*Kiran Thorat,Hongwu Peng,Yuebo Luo,Xi Xie,Shaoyi Huang,Amit Hasan,Jiahui Zhao,Yingjie Li,Zhijie Shi,Cunxi Yu,Caiwen Ding*

Main category: cs.LG

TL;DR: GROOT是一个芯片验证框架，结合了领域知识、图论和GPU内核设计，通过图神经网络提高验证效率，在大型电路上显著减少内存占用并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统芯片验证方法耗时且计算量大，现有GNN方法缺乏综合考虑芯片设计领域知识、图论和GPU内核设计的联合框架。

Method: 利用AIG图中的节点类型和连接极性创建节点特征，采用图分割算法将大图划分为子图进行GPU处理，开发图边再生算法恢复验证精度，并针对EDA图工作负载的极化分布特性重新设计HD-kernel和LD-kernel两个GPU内核。

Result: 在1024位CSA乘法器（1.34亿节点，2.68亿边）上，GROOT减少59.38%内存占用，达到99.96%准确率；相比cuSPARSE、MergePath-SpMM和GNNAdvisor，运行时间分别提升1.104倍、5.796倍和1.469倍。

Conclusion: GROOT通过算法与系统协同设计，有效解决了大规模芯片验证的效率问题，为EDA领域的GNN应用提供了高性能解决方案。

Abstract: Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively.

</details>


### [126] [Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery](https://arxiv.org/abs/2511.18303)
*Rui Ding,Rodrigo Pires Ferreira,Yuxin Chen,Junhong Chen*

Main category: cs.LG

TL;DR: 提出了一种用于复杂材料和器件发现的长时域分层深度研究代理，该代理在成本显著降低的情况下，生成质量与商业系统相当甚至更优的研究报告，并支持本地数据和工具的集成。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习代理和闭源商业代理无法处理的复杂材料和器件发现问题，提供可本地部署的解决方案。

Method: 采用本地可部署的深度研究实例，结合本地检索增强生成与大型语言模型推理器，通过深度研究树机制自适应扩展和修剪研究分支以最大化覆盖范围、深度和连贯性。

Result: 在27个纳米材料/器件主题上系统评估，使用LLM作为评判标准，结果显示该代理生成的研究报告质量与商业系统相当甚至更优，成本显著降低。

Conclusion: 该深度研究代理为复杂材料和器件发现问题提供了高效、低成本的解决方案，支持本地部署和集成。

Abstract: We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.

</details>


### [127] [DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling](https://arxiv.org/abs/2511.18312)
*Zihao Yao,Jiankai Zuo,Yaying Zhang*

Main category: cs.LG

TL;DR: 提出了DiM-TS模型，利用Mamba的状态空间模型增强时间序列生成能力，通过Lag Fusion和Permutation Scanning解决长程依赖和通道关联问题


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列生成中难以捕捉长程时间依赖和复杂通道关联，需要改进模型能力

Method: 提出Lag Fusion Mamba和Permutation Scanning Mamba两个变体，分析状态空间模型的核心限制，并整合成DiM-TS模型

Result: 在公开数据集上的实验表明DiM-TS能生成更真实的时间序列，更好地保持数据的时间周期性和通道相关性

Conclusion: DiM-TS是一个高质量的时间序列生成模型，能有效保留数据的多样属性

Abstract: Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data.

</details>


### [128] [AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert](https://arxiv.org/abs/2511.18314)
*Yuting Gao,Wang Lan,Hengyuan Zhao,Linjiang Huang,Si Liu,Qingpei Guo*

Main category: cs.LG

TL;DR: AnyExperts提出了一种按需、预算感知的动态路由框架，通过基于语义重要性为每个token分配可变数量的专家槽位，优化多模态MoE模型的计算分配效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态MoE模型采用固定的专家激活策略，忽略了不同模态间语义重要性的异质性，导致计算资源分配不优，关键token和冗余token消耗相同资源。

Method: 提出AnyExperts框架：为每个token分配可变总数的专家槽位，但限制在固定范围内；每个槽位由真实专家或虚拟专家填充，虚拟专家比例上限为20%；模型自适应平衡真实与虚拟专家比例，为语义丰富区域分配更多真实专家。

Result: 在视觉理解、音频理解和NLP理解任务中，AnyExperts在相同计算预算下提升性能：在通用图像/视频任务上，用40%更少的真实专家激活达到相当精度；在文本密集任务上，保持性能同时减少10%真实专家使用。

Conclusion: 细粒度、重要性驱动的专家分配策略显著提升了多模态MoE模型的效率和效果。

Abstract: Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.

</details>


### [129] [DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations](https://arxiv.org/abs/2511.18331)
*Sohini Roychowdhury,Adam Holeman,Mohammad Amin,Feng Wei,Bhaskar Mehta,Srihari Reddy*

Main category: cs.LG

TL;DR: Dynamix是一个可扩展的个性化序列探索框架，通过最大相关性原则和自监督学习优化事件历史处理，在保持广告预测准确性的同时提高训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在线广告推荐系统中，处理完整的用户广告互动历史计算量大且容易受到噪声影响，需要一种更高效的处理方法。

Method: 使用基于事件的特征（EBFs）进行自监督学习，通过停留时间和广告转化事件之间的相关性，在会话和表面级别对用户互动进行分类，实现有针对性的特征移除和选择性特征增强。

Result: 动态资源移除使训练和推理吞吐量分别提高1.15%和1.8%，动态特征增强在基线模型基础上提供0.033 NE增益，同时推理QPS提高4.2%。

Conclusion: Dynamix在基于在线用户序列的推荐模型中实现了显著的成本效率和性能改进，自监督用户分割和资源探索可以进一步优化复杂特征选择策略。

Abstract: For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.

</details>


### [130] [Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support](https://arxiv.org/abs/2511.18334)
*Chibuike E. Ugwu,Roschelle Fritz,Diane J. Cook,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 开发了一种结合临床医生参与的智能家居系统，利用环境传感器数据检测老年人尿路感染发作，通过不确定性校准方法提供更可靠的决策支持


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在UTI检测中缺乏对预测不确定性的洞察，限制了临床决策的有效性，需要开发能够量化不确定性并提供可靠决策支持的系统

Method: 采用临床医生参与循环(CIL)的智能家居系统，提取行为标记物，训练预测模型，并使用符合性校准区间(CCI)方法进行不确定性量化，在模型置信度低时拒绝预测

Result: 在8个真实智能家居数据上的评估显示，该方法在召回率等分类指标上优于基线方法，同时保持最低的拒绝比例和区间宽度

Conclusion: 该系统输出对指导临床决策具有价值，42名护士的调查证实了其在改善知情决策和有效管理老年人UTI及其他病症发作方面的实际效用

Abstract: Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions ("I don't know") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults.

</details>


### [131] [Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection](https://arxiv.org/abs/2511.18336)
*Kaito Shiku,Kazuya Nishimura,Shinnosuke Matsuo,Yasuhiro Kojima,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出AGL方法，通过将忽略基因的表达估计重新定义为辅助任务并与主要任务联合训练，利用被忽略基因的益处。使用DkGSB方法选择对目标基因预测有积极影响的辅助基因子集。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术存在大量观测噪声，以往研究仅使用高变异基因子集进行训练和评估，忽略了低表达基因可能通过共表达关系对评估目标做出贡献的潜力。

Method: AGL方法将忽略基因的表达估计作为辅助任务，与主要任务联合训练。提出DkGSB方法，利用先验知识对基因排序，将组合选择问题松弛为可微分的top-k选择问题。

Result: 实验证实了整合辅助基因的有效性，所提方法优于传统的辅助任务学习方法。

Conclusion: 通过合理选择辅助基因并联合训练，可以充分利用被忽略基因的信息，提高空间转录组学数据分析的准确性。

Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.

</details>


### [132] [Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking](https://arxiv.org/abs/2511.18394)
*Chinmay Karkar,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在预测能力上表现不一，其预测准确性高度依赖于领域结构、提示框架、问题类型和外部知识等因素。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在现实世界事件预测中的能力差异，探索不同模型家族、上下文、问题类型和外部知识如何影响预测准确性和校准。

Method: 分析不同模型家族在截止日期后发生的真实世界问题上的预测表现，研究上下文、问题类型和外部知识对准确性和校准的影响，以及添加事实新闻上下文如何改变信念形成和失败模式。

Result: 预测能力高度可变，取决于提问的内容和方式。LLMs在某些领域表现出预测能力，但在其他领域表现不佳。

Conclusion: LLMs的预测能力不是普遍一致的，而是高度依赖于具体情境、问题框架和可用信息，需要谨慎评估其在不同应用场景中的预测可靠性。

Abstract: Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.

</details>


### [133] [Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck](https://arxiv.org/abs/2511.18404)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: MVCIB是一个多视图条件信息瓶颈框架，用于在2D和3D分子结构上自监督预训练图神经网络，通过跨视图子图对齐增强模型表达能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多视图分子学习中的两个主要挑战：(1)发现视图间共享信息同时减少视图特定信息；(2)识别并对齐重要子结构（如功能基团），以增强跨视图一致性和模型表达能力。

Method: 提出MVCIB框架，使用一个视图作为上下文条件来指导另一个视图的表示学习，利用关键子结构作为视图间锚点，并采用跨注意力机制捕获子结构间的细粒度相关性以实现跨视图子图对齐。

Result: 在四个分子领域的广泛实验中，MVCIB在预测性能和可解释性方面均优于基线方法，并达到了3D Weisfeiler-Lehman表达能力，能够区分非同构图以及具有相同2D连通性的不同3D几何结构（如同分异构体）。

Conclusion: MVCIB通过多视图条件信息瓶颈和跨视图子图对齐，有效提升了分子图预训练的性能和表达能力，为分子表示学习提供了新的解决方案。

Abstract: Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.

</details>


### [134] [Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems](https://arxiv.org/abs/2511.18417)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 本文提出了类别等变神经网络(CENNs)的统一理论，将群/群胚等变网络、偏序集/格等变网络、图和层神经网络统一起来，证明了等变通用逼近定理，并实例化了该框架在多种数学结构中的应用。


<details>
  <summary>Details</summary>
Motivation: 统一和扩展等变深度学习的范围，不仅包含几何对称性，还包括上下文和组合对称性，超越传统的群作用范畴。

Method: 在具有Radon测度的拓扑范畴中形式化等变性，在范畴设置中构建线性和非线性层，并证明等变通用逼近定理。

Result: 证明了有限深度CENNs在连续等变变换空间中稠密，为群/群胚、偏序集/格、图和胞腔层等结构系统性地推导了通用逼近定理。

Conclusion: 类别等变深度学习扩展了等变深度学习的视野，使其能够处理更广泛的对称性类型，为深度学习理论提供了统一的数学框架。

Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.

</details>


### [135] [Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels](https://arxiv.org/abs/2511.18457)
*Duncan Stothers,Ben Stothers,Emily Schaeffer,Kishore Mulpuri*

Main category: cs.LG

TL;DR: 提出一种超声优先、辐射保护的发育性髋关节发育不良筛查策略，仅在必要时进行X光检查，通过自监督预训练和校准延迟规则实现有限样本覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 减少DDH筛查中的辐射暴露，通过超声优先策略仅在必要时进行X光检查，同时保证诊断准确性。

Method: 使用SimSiam在大型未标记数据集上预训练模态特定编码器，冻结主干并拟合小型的测量忠实头，应用单边符合延迟规则校准超声预测。

Result: 超声测量误差适中（alpha MAE约9.7度，覆盖率MAE约14.0%），X光探头AI和CE的MAE分别为约7.6度和8.9度，校准后的US-only策略在不同设置下实现可调的选择性成像曲线。

Conclusion: 开发了一个简单可复现的流程，将有限标签转化为可解释的测量值和可调的选择性成像曲线，适用于临床交接和未来外部验证。

Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.

</details>


### [136] [SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation](https://arxiv.org/abs/2511.18468)
*Md Akil Raihan Iftee,Mir Sazzat Hossain,Rakibul Hasan Rajib,Tariq Iqbal,Md Mofijul Islam,M Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.LG

TL;DR: 提出SloMo-Fast框架，一种无需源数据的双教师连续测试时适应方法，通过慢教师和快教师的互补机制解决长期遗忘问题，并在循环域转移场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖源数据或原型，在隐私敏感和资源受限环境中适用性有限，且存在长期遗忘问题，导致在先前遇到域上的性能下降。

Method: 采用双教师框架：慢教师缓慢遗忘，保留长期知识确保鲁棒泛化；快教师快速适应新域并跨域积累知识。同时提出循环测试时适应基准模拟重复域转移。

Result: 在循环TTA和其他10个CTTA设置中，SloMo-Fast持续优于最先进方法，展示其在演进和重访域上的适应和泛化能力。

Conclusion: SloMo-Fast通过双教师机制有效解决了CTTA中的长期遗忘问题，在无需源数据的情况下实现了优异的适应性和泛化性能。

Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.

</details>


### [137] [Adaptive Mesh-Quantization for Neural PDE Solvers](https://arxiv.org/abs/2511.18474)
*Winfried van den Dool,Maksim Zhdanov,Yuki M. Asano,Max Welling*

Main category: cs.LG

TL;DR: 提出自适应网格量化方法，通过轻量级辅助模型识别高损失区域，动态调整量化位宽，在复杂物理区域使用更高精度，优化计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 物理系统通常具有空间变化的复杂性，现有图神经网络在网格节点上采用统一计算资源，无法根据物理复杂性进行差异化处理，导致资源利用效率低下。

Method: 引入自适应网格量化框架，在网格节点、边和簇特征上进行空间自适应量化，通过轻量级辅助模型识别高损失区域，动态调整主模型的量化位宽分配。

Result: 在2D Darcy流、大规模非定常流体动力学、3D稳态Navier-Stokes模拟和2D超弹性问题等多个任务中，相比均匀量化基线获得一致的Pareto改进，在相同成本下性能提升高达50%。

Conclusion: 自适应网格量化能够有效优化计算资源分配，在保持计算成本的同时显著提升模型性能，为复杂物理系统的神经网络求解提供了高效解决方案。

Abstract: Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost.

</details>


### [138] [Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning](https://arxiv.org/abs/2511.18489)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: 提出基于联邦学习的个性化LLM框架，通过本地数据微调GPT模型，结合用户画像评分和社交网络分析，实现隐私保护的实时个性化内容推荐。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台中内容过滤和推荐的挑战，在保护用户隐私的同时提升用户体验和内容相关性。

Method: 采用联邦学习框架，客户端接收基础GPT模型并使用本地社交媒体数据进行微调；集成用户生成内容分类、用户画像评分计算、好友网络相关帖子识别等模块；结合社交参与度量化方法和矩阵分解技术。

Result: 系统能够提供实时个性化内容建议，通过自适应反馈循环和可读性评分算法显著提升内容质量和相关性。

Conclusion: 该综合解决方案不仅解决了内容过滤和推荐问题，还促进了更具吸引力的社交媒体体验，同时保护用户隐私，为数字平台个性化交互设定了新标准。

Abstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms.

</details>


### [139] [RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks](https://arxiv.org/abs/2511.18515)
*Ange-Clément Akazan,Issa Karambal,Jean Medard Ngnotchouye,Abebe Geletu Selassie. W*

Main category: cs.LG

TL;DR: 提出了RRaPINNs框架，通过条件风险价值(CVaR)优化尾部残差，引入平均超额(ME)代理惩罚直接控制最坏情况PDE残差，将PINN训练转化为风险敏感优化。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs最小化平均残差会掩盖大的局部化误差，需要开发能控制最坏情况残差的方法来提高可靠性。

Method: 使用CVaR进行尾部聚焦优化，引入ME代理惩罚，将PINN训练转化为风险敏感优化并与机会约束公式关联。

Result: 在多个PDE问题上，RRaPINNs在保持或改善平均误差的同时显著减少尾部残差，ME代理比直接CVaR铰链提供更平滑的优化。

Conclusion: RRaPINNs为平滑和不连续PDE提供了实用的可靠性感知科学机器学习路径，机会约束可靠性水平α作为透明旋钮在整体精度和尾部控制之间权衡。

Abstract: Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $α$ acts as a transparent knob trading bulk accuracy (lower $α$ ) for stricter tail control (higher $α$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs.

</details>


### [140] [CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection](https://arxiv.org/abs/2511.18519)
*Xinlin Zhuang,Yichen Li,Xiwei Liu,Haolin Yang,Yifan Lu,Ziyun Zou,Yulong Li,Huifa Li,Dongliang Chen,Qinglei Wang,Weiyang Liu,Ying Qian,Jiangming Shi,Imran Razzak*

Main category: cs.LG

TL;DR: CHIPS是一种数据选择方法，通过计算图像-文本对的效用分数来选择高质量数据，在垂直领域适应中仅需30%数据即可达到全数据集持续预训练的效果，在10%数据下优于半数据集训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注微调策略或大规模领域特定数据的持续预训练，但数据本身作为关键因素未被充分探索。本文从数据中心的视角重新审视该任务，研究有效数据选择是否能替代大规模数据集。

Method: 提出CHIPS方法，为每个图像-文本对分配效用分数，整合三个互补因素：通过曲率感知的牛顿式对齐确保忠实性；通过InfoNCE感知的曲率估计器和JL草图确保可扩展性；通过选择感知的相关性权重和可学习性平衡目标适应与通用领域保留。

Result: 在17个医学基准测试中达到选择基线的最先进性能，仅用30%数据即可匹配全数据集持续预训练，仅用10%数据优于半数据集训练；在31个通用领域基准测试中，在10-30%数据保留预算下性能下降最小。

Conclusion: CHIPS证明了有效数据选择可以显著减少持续预训练所需的数据量，在保持性能的同时大幅降低计算成本，为垂直领域适应提供了高效的数据中心解决方案。

Abstract: Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released.

</details>


### [141] [Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction](https://arxiv.org/abs/2511.18521)
*Core Francisco Park,Manuel Perez-Carrasco,Caroline Nowlan,Cecilia Garraffo*

Main category: cs.LG

TL;DR: 提出了一种变分自编码器方法，实现TEMPO卫星高光谱数据的514倍压缩，重建误差比信号低1-2个数量级，同时保留关键大气信息。


<details>
  <summary>Details</summary>
Motivation: 解决地球静止轨道高光谱卫星产生的海量数据在存储、传输和分发方面面临的挑战。

Method: 使用变分自编码器对NASA TEMPO卫星的高光谱观测数据进行压缩，并训练线性和非线性探针从压缩的潜在空间中提取Level-2产品。

Result: 实现了514倍压缩，重建误差显著低于信号；云分数和总臭氧提取性能良好（R²=0.93和0.81），但对流层痕量气体提取具有挑战性（NO₂ R²=0.20，HCHO R²=0.51）。

Conclusion: 神经压缩能大幅减少高光谱数据量，同时保留关键大气信号，解决了下一代地球观测系统的关键瓶颈。

Abstract: Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - https://github.com/cfpark00/Hyperspectral-VAE

</details>


### [142] [TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting](https://arxiv.org/abs/2511.18539)
*Lingyu Jiang,Lingyu Xu,Peiran Li,Qianwen Ge,Dingyi Zhuang,Shuo Xing,Wenjing Chen,Xiangbo Gao,Ting-Hsuan Chen,Xueying Zhan,Xin Zhang,Ziming Zhang,Zhengzhong Tu,Michael Zielewski,Kazunori Yamada,Fangzhou Lin*

Main category: cs.LG

TL;DR: 提出了TimePre框架，通过稳定实例归一化(SIN)解决MLP骨干网络与多选择学习(MCL)结合时的训练不稳定和假设崩溃问题，实现了高效的概率时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法中，基于扩散的模型计算成本高，而非采样框架如MCL虽然高效但存在训练不稳定和假设崩溃问题，特别是在与MLP骨干网络结合时问题更加严重。

Method: 提出TimePre框架，核心是稳定实例归一化(SIN)层，通过修正通道级统计偏移来稳定混合架构，解决假设崩溃问题。

Result: 在6个基准数据集上的实验表明，TimePre在关键概率指标上达到新的最先进精度，推理速度比基于采样的模型快几个数量级，且性能稳定可扩展。

Conclusion: TimePre在概率预测中成功弥合了准确性、效率和稳定性之间的长期差距。

Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.

</details>


### [143] [In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm](https://arxiv.org/abs/2511.18567)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.LG

TL;DR: 本文对Forward-Forward算法中的21种不同goodness函数进行了基准测试，发现在四个标准图像数据集上，某些替代goodness函数显著优于标准基线，同时揭示了预测性能与环境成本之间的关键权衡。


<details>
  <summary>Details</summary>
Motivation: Forward-Forward算法依赖于"goodness"的定义，但目前主要使用简单的平方和度量，不清楚这是否是最优选择。

Method: 在四个标准图像数据集（MNIST、FashionMNIST、CIFAR-10、STL-10）上对21种不同的goodness函数进行基准测试，评估分类准确率、能耗和碳足迹。

Result: 某些替代goodness函数表现优异：game_theoretic_local在MNIST上达到97.15%准确率，softmax_energy_margin_local在FashionMNIST上达到82.84%，triplet_margin_local在STL-10上达到37.69%。计算效率存在显著差异。

Conclusion: goodness函数是FF算法设计中的关键超参数，需要在预测性能和环境成本之间进行权衡。

Abstract: The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of "goodness", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \texttt{game\_theoretic\_local} achieved 97.15\% accuracy on MNIST, \texttt{softmax\_energy\_margin\_local} reached 82.84\% on FashionMNIST, and \texttt{triplet\_margin\_local} attained 37.69\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility.

</details>


### [144] [SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba](https://arxiv.org/abs/2511.18571)
*Jiazhen Hong,Geoffrey Mackellar,Soheila Ghane*

Main category: cs.LG

TL;DR: SAMBA是一个基于Mamba的自监督学习框架，用于长序列脑电图建模，通过U形编码器-解码器架构有效捕捉EEG数据中的长程时间依赖性和空间变异性，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG数据采样率高、记录时间长，需要建模长序列；现有Transformer模型受限于二次复杂度无法处理长上下文；电极配置差异和个体间脑信号差异给开发通用基础模型带来挑战。

Method: 提出SAMBA框架：1）时间语义随机掩码进行语义级序列重建；2）多头差分Mamba模块抑制冗余、突出关键时间结构；3）空间自适应输入嵌入在三维欧几里得空间中学习统一嵌入。

Result: 在13个EEG数据集上的实验表明，SAMBA在多种任务、电极配置和序列长度下始终优于最先进方法，同时保持低内存消耗和推理时间；学习到的空间权重图与任务相关神经生理区域高度一致。

Conclusion: SAMBA展示了作为实时脑机接口应用基础模型的可扩展性和实际潜力，其嵌入模块具有可学习性和可解释性。

Abstract: Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications.

</details>


### [145] [Generative Myopia: Why Diffusion Models Fail at Structure](https://arxiv.org/abs/2511.18593)
*Milad Siami*

Main category: cs.LG

TL;DR: 论文揭示了图扩散模型(GDMs)存在生成性近视问题，即模型倾向于生成统计上常见但结构上不重要的子结构，而忽略光谱关键但统计稀有的结构。作者提出了光谱加权扩散方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 图扩散模型在组合任务如图稀疏化中会灾难性地移除结构上必需但统计上稀有的"稀有桥梁"边，导致连接性完全失败。这种失败源于梯度饥饿现象，即优化景观本身抑制了稀有结构信号的学习。

Method: 提出了光谱加权扩散方法，使用有效电阻重新对齐变分目标，将光谱先验摊销到训练阶段，在推理时零开销。

Result: 该方法消除了近视问题，在对抗性基准测试中实现了100%的连接性，而标准扩散方法完全失败(0%)，性能与最优光谱预言机相当。

Conclusion: 光谱先验可以有效地解决图扩散模型中的生成性近视问题，在不增加推理开销的情况下显著提升模型在组合任务中的性能。

Abstract: Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\text{eff}} \approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \textbf{100\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\%).

</details>


### [146] [KAN vs LSTM Performance in Time Series Forecasting](https://arxiv.org/abs/2511.18613)
*Tabish Ali Rather,S M Mahmudul Hasan Joy,Nadezda Sukhorukova,Federico Frascoli*

Main category: cs.LG

TL;DR: LSTM在股票价格预测中显著优于KAN，在准确性方面表现更好，而KAN虽然在理论可解释性上有优势但误差较高。


<details>
  <summary>Details</summary>
Motivation: 比较KAN和LSTM在非确定性股票价格数据预测中的表现，评估预测准确性与可解释性之间的权衡。

Method: 使用均方根误差（RMSE）评估KAN和LSTM在不同预测时间范围内的预测性能。

Result: LSTM在所有测试的预测时间范围内都表现出显著优势，而标准KAN显示出明显更高的误差率和有限的实际应用价值。

Conclusion: LSTM在准确性要求高的时间序列应用中占主导地位，而KAN的主要优势在于计算效率，适合资源受限但准确性要求不高的场景。

Abstract: This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements.

</details>


### [147] [Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors](https://arxiv.org/abs/2511.18615)
*Jiawei Hu,Javier A. Barria*

Main category: cs.LG

TL;DR: 提出了FMAPLS和online-FMAPLS方法，通过贝叶斯框架和EM算法动态优化标签偏移问题，在CIFAR100和ImageNet上相比现有方法显著降低了KL散度并提高了分类精度。


<details>
  <summary>Details</summary>
Motivation: 标签偏移问题导致测试数据与训练数据的类别先验分布不同，严重影响分类器性能。现有MAPLS方法存在刚性约束限制，需要更灵活的动态优化方法。

Method: 使用批处理和在线EM算法联合优化Dirichlet超参数和类别先验，引入线性代理函数替代梯度更新，在线版本用随机近似替代批处理E步实现实时适应。

Result: 在CIFAR100和ImageNet数据集上，FMAPLS和online-FMAPLS分别实现了高达40%和12%的KL散度降低，并在严重类别不平衡和分布不确定性下显著优于现有方法。

Conclusion: 提出的方法在鲁棒性、可扩展性和动态学习场景适用性方面表现出色，特别适合大规模和动态学习环境。

Abstract: Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\boldsymbolα$ and class priors $\boldsymbolπ$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.

</details>


### [148] [Majority of the Bests: Improving Best-of-N via Bootstrapping](https://arxiv.org/abs/2511.18630)
*Amin Rakhsha,Kanika Madan,Tianyu Zhang,Amir-massoud Farahmand,Amir Khasahmadi*

Main category: cs.LG

TL;DR: 提出Majority-of-the-Bests (MoB)方法，通过自助采样估计Best-of-N的输出分布并选择其众数，在奖励模型不完美时比传统BoN方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统Best-of-N方法在奖励模型不完美时性能急剧下降，无法可靠找到正确答案。研究发现正确答案虽然概率不高，但通常是BoN输出分布中最可能的结果。

Method: MoB方法通过自助采样估计BoN的输出分布，然后选择该分布的众数作为最终输出。

Result: 在5个基准测试、3种基础LLM和2种奖励模型的30个设置中，MoB在25个设置中表现优于BoN。

Conclusion: MoB是BoN和自一致性方法的简单而强大的替代方案，激励对更精细选择机制的研究。

Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.

</details>


### [149] [FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction](https://arxiv.org/abs/2511.18631)
*Kiyan Rezaee,Morteza Ziabakhsh,Niloofar Nikfarjam,Mohammad M. Ghassemi,Yazdan Rezaee Jouryabi,Sadegh Eskandari,Reza Lashgari*

Main category: cs.LG

TL;DR: FOS是一个时间感知的图基准，用于预测科学前沿，通过分析1827-2024年间65,027个研究子领域的共现关系，将新领域对连接预测建模为时序链接预测任务。


<details>
  <summary>Details</summary>
Motivation: 预测新兴跨学科研究领域的形成是一个重大挑战，因为科学突破往往意外出现。

Method: 构建年度共现图，节点为研究子领域，边表示两个领域在同一出版物中的共现，包含时间戳。节点有语义嵌入，边有时序和拓扑描述符。

Result: 实验表明：(i) 使用领域的长文本描述嵌入显著提高预测精度；(ii) 不同模型类在不同评估设置下表现优异；(iii) 案例分析与后续年份实际出现的领域配对一致。

Conclusion: FOS基准为预测科学前沿提供了可复现的研究平台，展示了语义信息和时序建模在预测跨学科创新中的重要性。

Abstract: Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the "first-time" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers.

</details>


### [150] [The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion](https://arxiv.org/abs/2511.18632)
*Jan Benedikt Ruhland,Doguhan Bahcivan,Jan-Peter Sowa,Ali Canbay,Dominik Heider*

Main category: cs.LG

TL;DR: MedChat是一个本地可部署的虚拟医生框架，集成了基于LLM的医疗聊天机器人和扩散驱动的虚拟形象，用于自动化结构化问诊，在保护患者隐私的同时实现高效临床问诊。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进步使得能够在临床环境中实现高对话性能，同时满足严格的数据保护和患者隐私要求，但需要仔细考虑伦理、监管和技术约束。

Method: 使用真实和合成医疗对话的混合语料库对聊天机器人进行微调，通过低秩适应优化模型效率；实现安全隔离的数据库接口；通过条件扩散模型在潜在空间中实现虚拟形象，与音频特征同步实现逼真语音和面部动画。

Result: 与现有云系统不同，该工作展示了完全离线、本地可部署的LLM-扩散框架在临床问诊中的可行性；自编码器和扩散网络表现出平滑收敛，MedChat实现了稳定的微调并对未见数据具有强泛化能力。

Conclusion: 所提出的系统为AI辅助临床问诊提供了一个保护隐私、资源高效的基础，也适用于低成本设置。

Abstract: Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.
  In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.
  Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.

</details>


### [151] [Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost](https://arxiv.org/abs/2511.18643)
*Haojun Xia,Xiaoxia Wu,Jisen Li,Robert Wu,Junxiong Wang,Jue Wang,Chenxi Li,Aman Singhal,Alay Dilipbhai Shah,Alpay Ariyak,Donglin Zhuang,Zhongzhu Zhou,Ben Athiwaratkun,Zhen Zheng,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: Kitty通过算法-系统协同设计实现混合精度KV缓存，在接近2位内存占用的同时保持接近零精度损失，将KV内存减少近8倍，在相同内存预算下实现2.1-4.1倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: KV缓存是LLM推理的主要内存瓶颈，4位KV量化能保持精度但2位量化在长上下文推理中会显著降低精度，需要解决这一差距。

Method: 采用动态通道精度提升算法对Key缓存通道按敏感度排序，仅保留小部分高精度通道；系统层面提供页面中心KV布局、Triton兼容的页面反量化内核和轻量级运行时流水线。

Result: 在七个任务和两个模型家族上，Kitty将KV内存减少近8倍，精度损失可忽略，在相同内存预算下实现2.1-4.1倍吞吐量提升和最多8倍批次大小。

Conclusion: Kitty通过算法-系统协同设计成功解决了混合精度KV缓存的技术挑战，在保持精度的同时显著提升了内存效率和推理性能。

Abstract: The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty.

</details>


### [152] [Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic](https://arxiv.org/abs/2511.18660)
*Mostafa Mozafari,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出CUTS方法解决源数据不可用情况下的校正性机器遗忘问题，通过任务空间算术消除模型中的污染影响


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据往往不可访问，无法指定被污染的遗忘集，需要在不依赖原始训练数据的情况下消除模型污染

Method: CUTS方法：在代理污染集上微调模型放大污染机制，计算权重差异作为代理任务向量，减去校准后的向量来消除污染

Result: 在标签噪声下恢复大部分损失效用，对于后门攻击几乎消除攻击且对效用损害最小，在源无关设置下优于现有方法

Conclusion: CUTS为源数据不可用情况下的校正性机器遗忘提供了有效解决方案，通过任务空间算术成功消除模型污染

Abstract: Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.

</details>


### [153] [Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers](https://arxiv.org/abs/2511.18670)
*Rowan Bradbury,Aniket Srinivasan Ashok,Sai Ram Kasanagottu,Gunmay Jhingran,Shuai Meng*

Main category: cs.LG

TL;DR: 提出确定性连续替换(DCR)方法，通过确定性退火权重混合教师和学生输出，解决预训练模型中模块替换时的优化稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 替换预训练模型中的模块（特别是将二次自注意力替换为高效注意力）存在严重的优化问题：冷启动重新初始化会破坏冻结主干的稳定性。

Method: 确定性连续替换(DCR)使用确定性退火权重混合教师和学生输出，理论上消除了随机替换中固有的门控梯度方差。

Result: 在单种子研究中，DCR在受控注意力替换任务上比随机门控和蒸馏基线实现了更快的收敛和更强的对齐。

Conclusion: DCR为异构算子交换建立了基础，解决了模块替换中的核心稳定性挑战。

Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.

</details>


### [154] [Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition](https://arxiv.org/abs/2511.18671)
*Yan Wang,Ke Deng,Yongli Ren*

Main category: cs.LG

TL;DR: 提出MCEM方法结合单调非线性评论家分解，通过增加高价值联合动作的概率来排除次优行为，解决了集中训练与分散执行中的不匹配问题，在连续和离散动作基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中集中训练与分散执行(CTDE)的集中-分散不匹配(CDM)问题，即一个智能体的次优行为会降低其他智能体的学习效果。

Method: 提出多智能体交叉熵方法(MCEM)结合单调非线性评论家分解(NCD)，通过增加高价值联合动作的概率来更新策略，排除次优行为；为提高样本效率，使用改进的k步回报和Retrace进行离策略学习。

Result: 分析和实验表明，MCEM在连续和离散动作基准测试中优于最先进的方法。

Conclusion: MCEM方法有效克服了线性分解表达能力有限和非线性分解重新引入CDM问题的权衡，为多智能体强化学习提供了更优的解决方案。

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.

</details>


### [155] [QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks](https://arxiv.org/abs/2511.18689)
*Kazi Ahmed Asif Fuad,Lizhong Chen*

Main category: cs.LG

TL;DR: 提出了QuantKAN框架，首次系统性地研究Kolmogorov Arnold Networks (KANs)的量化问题，涵盖训练时量化(QAT)和训练后量化(PTQ)两种模式，在多个数据集和KAN变体上建立了低比特量化基准。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有强大的表达能力和可解释性，但其异构的样条和基分支参数阻碍了高效量化，与CNN和Transformer相比，KANs的量化问题尚未得到充分研究。

Method: QuantKAN框架扩展了现代量化算法（LSQ、LSQ+、PACT、DoReFa、QIL、GPTQ、BRECQ、AdaRound、AWQ、HAWQ-V2）到基于样条的层，为基函数、样条和激活组件提供分支特定的量化器。

Result: 实验表明KANs与低比特量化兼容，但表现出强烈的算法-架构交互：LSQ、LSQ+和PACT在4比特下对浅层KAN模型保持接近全精度准确率，DoReFa在深度KAGN模型下表现最稳定；PTQ中GPTQ和Uniform整体表现最强。

Conclusion: QuantKAN框架统一了样条学习和量化，为在实际资源受限环境中高效部署KANs提供了实用工具和指导原则。

Abstract: Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.

</details>


### [156] [GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction](https://arxiv.org/abs/2511.18716)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: GRIT-LP是一种用于极地雷达图像冰层厚度估计的图变换器，通过分区空间图构建和长程跳跃连接机制，解决了深度图变换器的过平滑和长程依赖建模问题，在RMSE上比现有方法提升24.92%。


<details>
  <summary>Details</summary>
Motivation: 准确估计冰层厚度对于理解积雪积累、重建过去气候模式以及减少未来冰盖演化和海平面上升预测的不确定性至关重要。现有图变换器在深度上受到过平滑和弱长程依赖建模的限制。

Method: 结合归纳几何图学习框架与自注意力机制，引入分区空间图构建策略形成重叠的完全连接局部邻域以保持空间一致性，并在变换器内采用长程跳跃连接机制改善信息流。

Result: 在广泛实验中，GRIT-LP在均方根误差上比当前最先进方法提升24.92%，表现出卓越性能。

Conclusion: 结果表明图变换器通过捕捉局部结构特征和冰层内部的长程依赖关系，在建模时空模式方面具有有效性，展示了推进数据驱动理解冰冻圈过程的潜力。

Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.

</details>


### [157] [Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM](https://arxiv.org/abs/2511.18721)
*Adarsh Kumarappan,Ayushi Mehrotra*

Main category: cs.LG

TL;DR: 提出(k, ε)-不稳定框架，改进SmoothLLM防御的认证保证，提供更实用可信的安全证书


<details>
  <summary>Details</summary>
Motivation: SmoothLLM防御依赖严格的k-不稳定假设，这在实践中很少成立，限制了安全证书的可信度

Method: 引入概率框架(k, ε)-不稳定，结合攻击成功的经验模型，推导SmoothLLM防御概率的新下界

Result: 为从业者提供可操作的安全保证，能够设置更符合LLM实际行为的认证阈值

Conclusion: 这项工作贡献了一个实用且理论基础的机制，使LLM更能抵抗对其安全对齐的利用

Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.

</details>


### [158] [LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs](https://arxiv.org/abs/2511.18727)
*Devansh Agarwal,Maitreyi Chatterjee,Biplab Chatterjee*

Main category: cs.LG

TL;DR: LogSyn是一个使用大语言模型将飞机维护日志从非结构化文本转换为结构化数据的框架，通过少样本上下文学习实现问题解决叙述的摘要和事件分类。


<details>
  <summary>Details</summary>
Motivation: 飞机维护日志包含宝贵的安全数据，但由于其非结构化文本格式而未被充分利用。

Method: 使用大语言模型进行少样本上下文学习，在6,169条记录上执行受控抽象生成(CAG)来总结问题解决叙述，并在详细层次本体中对事件进行分类。

Result: 该框架能够识别关键故障模式，为维护日志的语义结构化和可操作洞察提取提供了可扩展的方法。

Conclusion: 这项工作为改进航空及相关行业的维护工作流程和预测分析提供了实用路径。

Abstract: Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.

</details>


### [159] [Reinforcement Learning for Self-Healing Material Systems](https://arxiv.org/abs/2511.18728)
*Maitreyi Chatterjee,Devansh Agarwal,Biplab Chatterjee*

Main category: cs.LG

TL;DR: 将自愈合过程建模为强化学习问题，比较离散动作和连续动作算法在材料恢复中的表现，发现连续剂量控制的TD3算法效果最佳


<details>
  <summary>Details</summary>
Motivation: 自主材料系统需要自适应控制方法来最大化结构寿命，需要在保持结构完整性和有限资源消耗之间找到平衡

Method: 将自愈合过程构建为马尔可夫决策过程，使用Q-learning、DQN和TD3三种强化学习算法在随机仿真环境中训练智能体

Result: 强化学习控制器显著优于启发式基线，实现近乎完全的材料恢复，其中使用连续剂量控制的TD3智能体表现出最快的收敛速度和最佳稳定性

Conclusion: 连续、精细的比例控制在动态自愈合应用中至关重要，TD3算法在资源效率和结构完整性维护方面表现最优

Abstract: The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.

</details>


### [160] [Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network](https://arxiv.org/abs/2511.18730)
*Michael Horton,Patrick Lucey*

Main category: cs.LG

TL;DR: 提出基于轴向变换器的神经网络，用于在足球比赛中实时预测13种球员动作的总数，涵盖个人、团队和比赛三个层面。


<details>
  <summary>Details</summary>
Motivation: 准确预测球员在比赛中的动作总数对于战术决策、体育博彩和电视转播分析等应用具有重要意义，需要考虑比赛状态、球员能力、互动关系和比赛动态。

Method: 使用轴向变换器神经网络，能够联合且循环地预测多个时间步的动作总数，有效捕捉比赛进展的时间动态和球员间的互动。

Result: 模型能够做出一致可靠的预测，每场比赛以低延迟实时生成约75,000个预测。

Conclusion: 提出的轴向变换器设计在实验上表现良好，能够高效处理足球比赛的复杂动态预测任务。

Abstract: Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\sim$75,000 live predictions at low latency for each game.

</details>


### [161] [OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting](https://arxiv.org/abs/2511.18732)
*Haoming Jia,Yi Han,Xiang Wang,Huizan Wang,Wei Wu,Jianming Zheng,Peikun Xiao*

Main category: cs.LG

TL;DR: 提出了OceanForecastBench基准，为数据驱动的海洋预报提供开源标准化平台，包含28年高质量再分析数据、可靠观测数据和评估管道。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的海洋预报模型缺乏开源标准化基准，导致数据使用和评估方法不一致，阻碍模型开发、性能比较和跨学科合作。

Method: 构建包含三个核心贡献的基准：28年全球海洋再分析数据、卫星和现场观测数据、评估管道和6个典型基线模型。

Result: 创建了目前最全面的数据驱动海洋预报基准框架，为模型开发、评估和比较提供开源平台。

Conclusion: OceanForecastBench解决了海洋预报领域缺乏标准化基准的问题，将促进数据驱动海洋预报模型的发展和应用。

Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.

</details>


### [162] [Sampling Control for Imbalanced Calibration in Semi-Supervised Learning](https://arxiv.org/abs/2511.18773)
*Senmao Tian,Xiang Wei,Shunli Zhang*

Main category: cs.LG

TL;DR: SC-SSL是一个解决半监督学习中类别不平衡问题的统一框架，通过解耦采样控制来抑制模型偏差，在训练和推理阶段分别处理特征级和权重不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理类别不平衡时通常以粗粒度方式调整logits，混淆了数据不平衡与不同类别学习难度差异导致的偏差问题。

Method: 提出SC-SSL框架：训练阶段通过具有显式扩展能力的分类器和自适应调整采样概率来缓解特征级不平衡；推理阶段分析线性分类器的权重不平衡，应用后处理采样控制和优化偏置向量直接校准logits。

Result: 在多个基准数据集和分布设置下的广泛实验验证了SC-SSL的一致性和最先进性能。

Conclusion: SC-SSL通过解耦采样控制有效解决了半监督学习中的类别不平衡问题，在训练和推理阶段分别处理不同来源的偏差，取得了优异性能。

Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.

</details>


### [163] [SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs](https://arxiv.org/abs/2511.18777)
*Chenhong Zhou,Jie Chen,Zaifeng Yang*

Main category: cs.LG

TL;DR: 提出了一种结合小波变换和Transformer的混合谱注意力算子Transformer(SAOT)，通过小波注意力模块和傅里叶注意力模块的融合，有效解决了传统傅里叶神经算子过度平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 傅里叶神经算子(FNO)在处理偏微分方程时存在过度平滑问题，无法捕捉局部细节和高频分量，需要结合小波变换的空间-频率局部化特性来改进。

Method: 提出小波注意力(WA)模块，具有线性计算复杂度；开发SAOT框架，通过门控融合块整合WA的局部关注和傅里叶注意力(FA)的全局感受野。

Result: WA显著缓解了FA的局限性，大幅优于现有基于小波的神经算子；SAOT在六个算子学习基准上达到最先进性能，并表现出强大的离散不变性能力。

Conclusion: 结合局部感知和全局谱表示的方法能够有效提升神经算子的性能，SAOT框架为偏微分方程求解提供了更优的解决方案。

Abstract: Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability.

</details>


### [164] [Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs](https://arxiv.org/abs/2511.18783)
*Renchu Guan,Xuyang Li,Yachao Zhang,Wei Pang,Fausto Giunchiglia,Ximing Li,Yonghao Liu,Xiaoyue Feng*

Main category: cs.LG

TL;DR: HONOR是一个新颖的无监督超图对比学习框架，适用于同质性和异质性超图，通过提示机制和自适应注意力聚合来建模异质性关系，在理论和实验上都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络大多基于同质性假设，但现实场景中往往存在显著的异质性结构，这限制了现有方法的有效性。

Method: 提出HONOR框架，包含：基于提示的超边特征构建策略（保持全局语义一致性并抑制局部噪声）、自适应注意力聚合模块（动态捕捉节点对超边的多样化局部贡献）、结合高通滤波来充分利用异质性连接模式。

Result: 理论分析表明HONOR具有优越的泛化能力和鲁棒性。大量实验验证HONOR在同质性和异质性数据集上都优于最先进的基线方法。

Conclusion: HONOR能够有效建模超图中复杂的异质性关系，生成更具区分性和鲁棒性的节点和超边表示，为处理现实世界中的复杂高阶关系提供了有力工具。

Abstract: Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \textbf{HONOR}, a novel unsupervised \textbf{H}ypergraph c\textbf{ON}trastive learning framework suitable for both hom\textbf{O}philic and hete\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.

</details>


### [165] [Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses](https://arxiv.org/abs/2511.18789)
*Haichen Hu,David Simchi-Levi*

Main category: cs.LG

TL;DR: 提出了一种高效的再拟合程序，用于在固定设计设置下计算经验风险最小化的超额风险并提供高概率上界，该方法无需函数类复杂度的先验知识，适用于现代不透明机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 传统基于容量的学习理论对于极端复杂的假设类（如深度神经网络和生成模型）变得不可行，需要一种模型无关的方法来理论评估现代不透明机器学习系统的超额风险。

Method: 通过随机扰动梯度向量生成两组伪输出（wild response），然后对黑盒过程进行两次再拟合得到两个wild预测器，最后结合原始预测器、两个wild预测器和构造的wild响应推导出高效的超额风险上界。

Result: 该方法能够计算超额风险并提供高概率上界，且不需要函数类复杂度的先验知识，实现了模型无关的风险评估。

Conclusion: 该方法为理论上评估现代不透明机器学习系统提供了有前景的解决方案，特别是在传统容量理论不可行的情况下。

Abstract: We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.

</details>


### [166] [Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models](https://arxiv.org/abs/2511.18829)
*Kanav Arora,Girish Narayanswamy,Shwetak Patel,Richard Li*

Main category: cs.LG

TL;DR: 探索如何将大型预训练PPG模型蒸馏到适合边缘设备实时推理的小型模型，评估了四种蒸馏策略并建立了模型大小与性能关系的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习模型在心率估计任务中表现良好，但要在可穿戴设备上部署这些模型，必须满足严格的内存和延迟约束。

Method: 评估了四种蒸馏策略：硬蒸馏、软蒸馏、解耦知识蒸馏(DKD)和特征蒸馏，通过全面的教师和学生模型容量扫描。

Result: 建立了描述模型大小与性能关系的缩放规律，为构建可部署在边缘设备上的生理传感模型提供了实用且可预测的方法。

Conclusion: 这项早期研究为构建适用于边缘部署的生理传感模型奠定了实践基础，提供了可预测的模型构建方法。

Abstract: Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.

</details>


### [167] [Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM](https://arxiv.org/abs/2511.18830)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种双输入神经网络策略，通过分离事件和序列属性，使用持续时间感知的伪嵌入矩阵将时间重要性转换为紧凑可学习的表示，解决了预测过程监控中时间不规则性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在预测过程监控中难以处理时间不规则性，特别是随机事件持续时间和重叠时间戳，限制了它们在不同数据集上的适应性。

Method: 采用双输入神经网络策略，分离事件和序列属性，使用持续时间感知伪嵌入矩阵。在B-LSTM和B-GCN基线模型基础上开发了D-LSTM和D-GCN变体，所有模型都包含自调超模型用于自适应架构选择。

Result: 在平衡和不平衡结果预测任务上的实验表明，持续时间伪嵌入输入能持续改善泛化能力、降低模型复杂性并增强可解释性。

Conclusion: 明确的时间编码具有显著优势，为稳健的实时预测过程监控应用提供了灵活的设计方案。

Abstract: Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.

</details>


### [168] [Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2511.18835)
*Fang Wang,Lance Kosca,Adrienne Kosca,Marko Gacesa,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGNN(O)是一个用于事件序列数据结果预测的AutoML GNN超模型框架，通过贝叶斯优化自动调整架构和超参数，在多个数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 为事件序列数据的结果预测提供一个无需手动配置的自动机器学习GNN框架，解决复杂事件数据中的预测挑战。

Method: 扩展了四种GNN架构（单层、双层、双层伪嵌入、双层嵌入）和六种GNN算子，采用基于贝叶斯优化的自调优机制，包含剪枝和早停策略。

Result: 在Traffic Fines数据集上准确率超过0.98，在Patients数据集上加权F1分数达到0.86，无需显式处理数据不平衡问题。

Conclusion: 该AutoML-GNN方法为复杂事件序列数据的结果预测提供了强大且可推广的基准解决方案。

Abstract: This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.

</details>


### [169] [WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting](https://arxiv.org/abs/2511.18846)
*Yubo Wang,Hui He,Chaoxi Niu,Zhendong Niu*

Main category: cs.LG

TL;DR: WaveTuner是一个基于小波分解的时间序列预测框架，通过全频谱子带调谐解决现有方法对高频分量利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有小波方法主要递归分解低频分量，严重忽略了对于精确预测至关重要的高频分量，导致信息利用不充分。

Method: 包含自适应小波细化模块（动态分配子带权重并生成子带特定嵌入）和多分支专业化模块（使用多个KAN网络分别建模不同频谱子带）。

Result: 在8个真实世界数据集上的广泛实验表明，WaveTuner在时间序列预测方面达到了最先进的性能。

Conclusion: WaveTuner在统一的时频框架内全面调谐全局趋势和局部变化，显著提升了时间序列预测的准确性。

Abstract: Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.

</details>


### [170] [Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning](https://arxiv.org/abs/2511.18859)
*Bo Jiang,Weijun Zhao,Beibei Wang,Xiao Wang,Jin Tang*

Main category: cs.LG

TL;DR: 提出了UAdapterGNN方法，通过将不确定性学习集成到GNN适配器中，增强预训练GNN模型对噪声图数据的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AdapterGNN方法容易受到图数据中各种噪声（如噪声边和模糊节点属性）的影响，泛化能力有限，需要增强GNN微调的鲁棒性和泛化能力。

Method: 使用高斯概率适配器来增强预训练GNN模型，通过自动吸收高斯分布方差变化的影响来增强模型鲁棒性。

Result: 在多个基准测试上的广泛实验证明了UAdapterGNN方法的有效性、鲁棒性和高泛化能力。

Conclusion: UAdapterGNN通过集成不确定性学习成功解决了GNN微调中的噪声敏感性问题，显著提升了模型的鲁棒性和泛化性能。

Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.

</details>


### [171] [KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit](https://arxiv.org/abs/2511.18868)
*Dezhi Ran,Shuxiao Xie,Mingfang Ji,Ziyue Hua,Mengzhou Wu,Yuan Cao,Yuzhe Guo,Yu Hao,Linyi Li,Yitao Hu,Tao Xie*

Main category: cs.LG

TL;DR: KernelBand是一个将内核优化建模为分层多臂老虎机问题的框架，利用LLM代理通过硬件分析和运行时聚类来战略性地导航优化空间，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的内核优化需要大量硬件架构和软件优化专业知识，而现有的基于LLM的代码生成方法由于缺乏足够的硬件领域知识，难以在庞大的优化空间中有效平衡探索和利用。

Method: 将内核优化构建为分层多臂老虎机问题，通过硬件性能分析识别有前景的优化策略，并利用运行时行为聚类减少内核候选者的探索开销，使LLM代理能够进行序列化决策。

Result: 在TritonBench上的广泛实验表明，KernelBand显著优于最先进方法，用更少的token实现了更优的性能，且随着计算资源增加表现出持续改进而无饱和现象。

Conclusion: KernelBand成功地将内核优化问题转化为分层决策过程，通过结合硬件知识和智能探索策略，为LLM驱动的内核优化提供了高效且可扩展的解决方案。

Abstract: High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.

</details>


### [172] [Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.18871)
*Jian Lu*

Main category: cs.LG

TL;DR: 提出了一种周期性异步框架，通过分离推理和训练部署，结合数据加载器改进，实现了至少3倍的整体性能提升，同时保持算法精度与同步方法完全等效。


<details>
  <summary>Details</summary>
Motivation: 主流RL框架中推理和训练在同一设备上同步执行，存在计算耦合问题，阻碍了并发推理和训练，训练效率成为关键挑战。

Method: 采用推理和训练分离部署策略，改进数据加载器，将传统同步架构转变为周期性异步框架；在训练阶段应用统一的三模型架构，并提出共享提示注意力掩码以减少重复计算。

Result: 在NPU平台上实现了至少3倍的整体RL训练性能提升，同时算法精度与同步方法完全等效。

Conclusion: 该周期性异步框架允许按需独立弹性扩展各组件，具有广泛应用潜力。

Abstract: Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.

</details>


### [173] [Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning](https://arxiv.org/abs/2511.18887)
*Hyeong-Gun Joo,Songnam Hong,Seunghwan Lee,Dong-Joon Shin*

Main category: cs.LG

TL;DR: 提出了Hi-SAFE框架，这是一个轻量级且密码学安全的聚合框架，用于解决基于符号的联邦学习中的隐私和通信效率问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网和边缘网络等资源受限环境中面临隐私保护和通信效率的双重挑战。基于符号的方法虽然节省带宽，但容易受到推理攻击，而现有的安全聚合技术要么不兼容基于符号的方法，要么开销过大。

Method: 基于费马小定理构建了SIGNSGD-MV的高效多数投票多项式，将多数投票表示为有限域上的低阶多项式，实现安全评估。还引入了分层子分组策略，确保恒定的乘法深度和有界的每用户复杂度。

Result: Hi-SAFE框架能够隐藏中间值，仅揭示最终结果，同时保持恒定的计算复杂度，不随用户数量n的增加而变化。

Conclusion: Hi-SAFE为基于符号的联邦学习提供了一个轻量级且密码学安全的聚合解决方案，有效解决了隐私保护和通信效率的平衡问题。

Abstract: Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n.

</details>


### [174] [Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models](https://arxiv.org/abs/2511.18890)
*Yonggan Fu,Xin Dong,Shizhe Diao,Matthijs Van keirsbilck,Hanrong Ye,Wonmin Byeon,Yashaswi Karnati,Lucas Liebenwein,Hannah Zhang,Nikolaus Binder,Maksim Khadkevich,Alexander Keller,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 本文提出了一种针对小语言模型（SLM）的延迟优化设计方法，通过分析深度-宽度比和算子选择两个关键架构因素，构建了自动搜索框架来发现延迟最优的混合SLM架构，并结合权重归一化技术提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有SLM设计主要关注参数数量优化，但参数效率并不一定转化为实际设备上的速度提升。本文旨在识别影响SLM实际设备延迟的关键因素，为以延迟为主要考虑因素的SLM设计和训练提供通用原则和方法。

Method: 1. 研究延迟最优的深度-宽度比；2. 探索高效注意力替代算子；3. 构建进化搜索框架自动发现算子组合；4. 使用权重归一化技术增强训练效果。

Result: 提出的Nemotron-Flash混合SLM系列显著推进了最先进SLM的精度-效率前沿，相比Qwen3-1.7B/0.6B，平均精度提升超过5.5%，延迟降低1.3倍/1.9倍，吞吐量提高18.7倍/45.6倍。

Conclusion: 通过综合考虑架构设计和训练优化，本文提供了一套系统的方法来构建延迟最优的小语言模型，在实际设备上实现了显著的性能提升。

Abstract: Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.

</details>


### [175] [VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL](https://arxiv.org/abs/2511.18902)
*Zengjie Hu,Jiantao Qiu,Tianyi Bai,Haojin Yang,Binhang Yuan,Qi Jing,Conghui He,Wentao Zhang*

Main category: cs.LG

TL;DR: VADE是一个解决基于群体的策略优化方法中梯度消失问题的动态采样框架，通过在线样本难度估计和Thompson采样来增强训练信号，同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体的策略优化方法（如GRPO、GSPO）存在梯度消失问题，当组内所有响应获得相同奖励时，优势估计会崩溃，训练信号减弱。现有解决方案要么计算开销大，要么缺乏实时适应性。

Method: VADE框架包含三个关键组件：使用Beta分布进行在线样本级难度估计、通过估计正确概率最大化信息增益的Thompson采样器、以及在策略演化下保持稳健估计的双尺度先验衰减机制。

Result: 在多模态推理基准测试中，VADE在性能和样本效率方面持续优于强基线方法，同时显著减少计算开销。该框架可作为即插即用组件集成到现有的基于群体的RL算法中。

Conclusion: VADE通过动态选择最具信息量的样本有效解决了梯度消失问题，在保持高性能的同时大幅降低了计算成本，具有很好的实用性和可扩展性。

Abstract: Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \textbf{VADE}, a \textbf{V}ariance-\textbf{A}ware \textbf{D}ynamic sampling framework via online sample-level difficulty \textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io.

</details>


### [176] [How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining](https://arxiv.org/abs/2511.18903)
*Kairong Luo,Zhenbo Sun,Haodong Wen,Xinyu Shi,Jiarui Cui,Chenyi Dang,Kaifeng Lyu,Wenguang Chen*

Main category: cs.LG

TL;DR: 研究发现课程式预训练效果受限的原因是数据质量升序安排与学习率衰减计划不兼容，提出两种简单策略来缓解这个问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据稀缺，LLMs通常在不同质量的数据混合上训练。课程式预训练按数据质量升序排列训练数据，但先前研究显示其改进有限，需要探究限制因素。

Method: 识别数据质量升序安排与学习率衰减的不兼容性，提出两种策略：使用更温和的学习率衰减计划（最终LR仅略小于峰值LR），以及用模型平均替代学习率衰减（对最后几个检查点进行加权平均）。

Result: 结合这两种策略，在标准基准测试上的平均得分比随机洗牌提高了1.64%，在1.5B参数模型和30B tokens的各种数据质量指标上得到验证。

Conclusion: 研究结果呼吁重新评估课程式LLM预训练方法，并强调数据课程与优化方法协同设计的潜力。

Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.

</details>


### [177] [Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation](https://arxiv.org/abs/2511.18930)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: MCNO提出了一种轻量级架构，通过蒙特卡洛方法直接近似核积分来学习参数化PDE的解算子，无需谱或平移不变性假设。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子如傅里叶神经算子依赖谱假设和平移不变性，限制了其适用性。MCNO旨在提供一种更简单通用的替代方案。

Method: 使用可学习张量在固定随机采样点上表示核，通过蒙特卡洛方法近似积分，支持多网格分辨率泛化。

Result: 在标准1D PDE基准测试中，MCNO以较低计算成本实现了竞争性精度。

Conclusion: MCNO为谱和基于图的神经算子提供了一个简单实用的替代方案。

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.

</details>


### [178] [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: SWAN是一种无需微调的KV缓存压缩框架，通过正交矩阵旋转和剪枝来减少内存占用，无需解压缩步骤，在保持性能的同时实现50-60%的内存节省。


<details>
  <summary>Details</summary>
Motivation: LLMs在自回归推理时面临KV缓存内存占用过大的瓶颈，现有压缩技术存在信息丢失风险、固定限制或解压缩计算开销大的问题。

Method: 使用离线正交矩阵对KV缓存进行旋转和剪枝，剪枝后的缓存直接用于注意力计算，无需重构，并配合小密集缓冲区。

Result: 在激进压缩下（每token节省50-60%内存），仍能保持接近未压缩基线的性能，且支持运行时可调压缩级别。

Conclusion: SWAN通过免解压缩设计、高压缩率下的良好性能和动态适应性，为长上下文LLM服务提供了实用高效的解决方案。

Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

</details>


### [179] [Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery](https://arxiv.org/abs/2511.18940)
*Sanjeev Manivannan,Chandrashekar Lakshminarayan*

Main category: cs.LG

TL;DR: 提出了几何感知预处理模块和深度同余网络，用于零样本跨被试运动想象解码，在BCI-IV 2a基准上比最强基线提升3-4%准确率


<details>
  <summary>Details</summary>
Motivation: 解决EEG脑机接口中跨被试运动想象解码的挑战，特别是由于被试间变异性和协方差矩阵在SPD流形上的弯曲几何特性导致的困难

Method: 引入DCR和RiFU预处理模块扩展黎曼对齐，提出SPD-DCNet和RiFUNet两个流形分类器，使用分层同余变换学习判别性、被试不变的协方差表示

Result: 在BCI-IV 2a基准上，跨被试准确率比最强经典基线提高3-4%

Conclusion: 几何感知变换对于稳健的EEG解码具有重要价值

Abstract: Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.

</details>


### [180] [MIST: Mutual Information Via Supervised Training](https://arxiv.org/abs/2511.18945)
*German Gritsai,Megan Richards,Maxime Méloux,Kyunghyun Cho,Maxime Peyrard*

Main category: cs.LG

TL;DR: 提出了一种完全数据驱动的互信息估计器设计方法，使用神经网络参数化估计器，通过大规模元数据集训练，能够处理变样本大小和维度，并提供不确定性的分位数估计。


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计方法存在理论限制和效率问题，需要一种更灵活、高效且能处理不同样本大小和维度的数据驱动方法。

Method: 使用神经网络参数化互信息估计器，在包含62.5万个已知互信息值的合成联合分布元数据集上训练，采用二维注意力机制确保样本置换不变性，通过分位数回归损失量化不确定性。

Result: 学习的估计器在各种样本大小和维度下显著优于经典基线方法，包括在训练中未见过的联合分布上，分位数区间校准良好且比自助法置信区间更可靠，推理速度比现有神经基线快几个数量级。

Conclusion: 该框架提供了可训练、完全可微分的估计器，可嵌入更大的学习流程中，通过利用互信息对可逆变换的不变性，元数据集可适应任意数据模态，实现灵活训练。

Abstract: We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.

</details>


### [181] [Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation](https://arxiv.org/abs/2511.18958)
*Qisen Chai,Yansong Wang,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: Cutter是一个双智能体强化学习框架，用于压缩图数据以高效评估图鲁棒性，通过VDA和RDA智能体识别关键和冗余节点，采用轨迹级奖励塑造、原型塑造和跨智能体模仿策略提升压缩质量。


<details>
  <summary>Details</summary>
Motivation: 随着图结构数据规模增大，评估其对抗攻击下的鲁棒性变得计算昂贵且难以扩展，需要压缩图表示以保持拓扑结构和鲁棒性特征。

Method: 提出Cutter双智能体强化学习框架：VDA检测关键节点，RDA检测冗余节点；采用轨迹级奖励塑造、原型塑造和跨智能体模仿三种策略优化学习效率和压缩质量。

Result: 在多个真实世界图上实验表明，Cutter生成的压缩图保留了关键静态拓扑属性，在不同攻击场景下展现出与原始图高度一致的鲁棒性退化趋势。

Conclusion: Cutter能显著提高图鲁棒性评估效率，同时不损害评估保真度，为大规模图鲁棒性分析提供了有效解决方案。

Abstract: As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.

</details>


### [182] [AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960)
*Lei Xiao,Jifeng Li,Juntao Gao,Feiyang Ye,Yan Jin,Jingjing Qian,Jing Zhang,Yong Wu,Xiaoyuan Yu*

Main category: cs.LG

TL;DR: AVA-VLA是一个基于POMDP视角的视觉-语言-动作模型框架，通过引入主动视觉注意力机制，利用历史上下文动态处理视觉信息，在机器人任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在每个时间步独立处理密集视觉输入，采用MDP建模方式，这种历史无关的设计在动态序列决策中无法有效利用历史上下文，限制了视觉信息处理效果。

Method: 从POMDP角度重新定义问题，提出AVA-VLA框架，引入主动视觉注意力机制，利用来自前一步决策的循环状态（信念状态的神经近似）动态调制视觉处理，计算软权重来主动处理任务相关视觉标记。

Result: 在LIBERO和CALVIN等流行机器人基准测试中实现最先进性能，并在双臂机器人平台上验证了实际应用性和强大的仿真到现实迁移能力。

Conclusion: AVA-VLA通过引入基于历史上下文的主动视觉注意力机制，显著提升了VLA模型在动态序列决策任务中的性能，证明了POMDP视角在视觉-语言-动作建模中的有效性。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

</details>


### [183] [FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning](https://arxiv.org/abs/2511.18977)
*Xin Yuan,Siqi Li,Jiateng Wei,Chengrui Zhu,Yanming Wu,Qingpeng Li,Jiajun Lv,Xiaoke Lan,Jun Chen,Yong Liu*

Main category: cs.LG

TL;DR: FastForward Pruning是一种高效的层间稀疏度分配方法，通过解耦的单步强化学习框架将策略优化与预算约束问题分离，显著降低了搜索计算成本，在LLaMA、Mistral和OPT模型上取得了优于启发式方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法面临效率与性能的权衡：启发式方法快速但性能次优，基于搜索的方法（如强化学习）计算成本过高，难以在大规模模型上应用。

Method: 提出FastForward Pruning框架，采用解耦的单步强化学习，将策略优化与预算约束分离，并使用课程学习策略从简单任务逐步过渡到复杂任务。

Result: 在LLaMA、Mistral和OPT模型家族上的评估表明，该方法发现的剪枝策略在性能上优于强启发式基线，且相比其他搜索算法以更低的计算成本获得竞争性或更优的结果。

Conclusion: FastForward Pruning在剪枝效率方面具有明显优势，能够在保持高性能的同时大幅降低计算开销，为大规模语言模型的压缩提供了实用解决方案。

Abstract: Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.

</details>


### [184] [Dynamic Mixture of Experts Against Severe Distribution Shifts](https://arxiv.org/abs/2511.18987)
*Donghu Kim*

Main category: cs.LG

TL;DR: 评估DynamicMoE方法在持续学习和强化学习环境中的效果，并与现有网络扩展方法进行基准比较


<details>
  <summary>Details</summary>
Motivation: 解决持续学习和强化学习中的可塑性-稳定性困境，受生物大脑通过容量增长保持可塑性的启发，探索类似的人工网络方法

Method: 使用动态混合专家(DynamicMoE)架构，通过专门化专家来处理不同的数据分布

Result: 论文旨在评估该方法的效果，但具体结果未在摘要中提供

Conclusion: DynamicMoE为持续学习和强化学习提供了一种有前景的替代方案

Abstract: The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.

</details>


### [185] [3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks](https://arxiv.org/abs/2511.19019)
*Nguyen Duc Minh Quang,Chang Liu,Huy-Trung Nguyen,Shuangyang Li,Derrick Wing Kwan Ng,Wei Xiang*

Main category: cs.LG

TL;DR: 提出3D动态无线电地图(3D-DRM)框架，使用Vision Transformer编码器和Transformer模块学习预测低空无线网络中接收功率的时空演化，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络(LAWN)中基站发射功率随用户位置和流量需求动态变化，导致高度非平稳的3D无线电环境。现有无线电地图多为静态或离线构建，忽略了实时功率变化和时空依赖性。

Method: 使用Vision Transformer(ViT)编码器从3D无线电地图提取高维空间表示，结合基于Transformer的模块建模序列依赖性，预测未来功率分布。

Result: 3D-DRM准确捕捉快速变化的功率动态，在无线电地图重建和短期预测方面显著优于基线模型。

Conclusion: 3D-DRM框架能有效学习预测低空无线网络中动态变化的接收功率分布，为无线电感知网络优化提供有力支持。

Abstract: Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.

</details>


### [186] [OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs](https://arxiv.org/abs/2511.19023)
*Yuting Gao,Weihao Chen,Lan Wang,Ruihan Xu,Qingpei Guo*

Main category: cs.LG

TL;DR: OrdMoE是一个新颖的偏好对齐框架，通过利用MoE架构中的内部信号完全绕过对外部人类偏好数据的依赖，实现了零成本的自监督偏好排序。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法主要依赖外部人工标注的偏好数据，这些数据收集成本高且劳动密集。

Method: 通过观察路由器专家选择分数隐式编码的质量感知响应排名，将专家基于每个token的路由分数分组到排名层级中，并分别激活每个层级以产生质量递增的响应序列。

Result: 在多个多模态基准测试上的广泛实验表明，OrdMoE显著增强了多模态MoE LLMs的对齐和整体性能，无需任何人工标注的偏好数据即可获得有竞争力的结果。

Conclusion: OrdMoE框架通过利用MoE架构的固有特性，成功实现了无需外部人类偏好数据的有效偏好对齐，为多模态大语言模型的后训练对齐提供了一种高效且成本低廉的解决方案。

Abstract: Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.

</details>


### [187] [Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings](https://arxiv.org/abs/2511.19037)
*Zimo Yan,Zheng Xie,Chang Liu,Yuan Wang*

Main category: cs.LG

TL;DR: 提出了一种拉普拉斯位置编码方法，能够解决图神经网络在结构区分能力上的限制，通过结合谱分析和锚点定位技术，在药物相互作用任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递图神经网络的表达能力受限于一维Weisfeiler-Lehman测试，无法区分结构不同的节点，需要更强大的位置编码方法来突破这一理论限制。

Method: 开发了一种拉普拉斯位置编码，该方法对特征向量符号翻转和特征空间内的基旋转具有不变性，结合最短路径与扩散距离的单调关系、使用常数数量锚点的谱三边测量以及对数嵌入大小的定量谱单射性。

Result: 该编码方法能够从常数数量的观测中实现节点可识别性，与受Weisfeiler-Lehman测试约束的架构相比建立了样本复杂度分离，在药物-药物相互作用任务上显著提高了ROC曲线下面积和F1分数。

Conclusion: 通过原则性的位置信息解决理论表达能力限制具有实际益处，该方法为图神经网络提供了更强的结构区分能力。

Abstract: Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.

</details>


### [188] [Mitigating Participation Imbalance Bias in Asynchronous Federated Learning](https://arxiv.org/abs/2511.19066)
*Xiangyu Chang,Manyi Yao,Srikanth V. Krishnamurthy,Christian R. Shelton,Anirban Chakraborty,Ananthram Swami,Samet Oymak,Amit Roy-Chowdhury*

Main category: cs.LG

TL;DR: 本文分析了异步联邦学习(AFL)中的异构性放大问题，提出了ACE和ACED方法来缓解参与不平衡和延迟问题


<details>
  <summary>Details</summary>
Motivation: 在非IID数据分布的联邦学习环境中，异步模式会放大客户端异构性的负面影响，导致全局模型偏向频繁更新的快速客户端

Method: 提出了ACE方法，通过立即使用所有客户端的最新信息进行非缓冲更新来缓解参与不平衡；还提出了ACED变体来平衡客户端多样性和更新延迟

Result: 在不同模型、任务和异构性设置下的实验验证了分析结果，并证明了所提方法的鲁棒性能

Conclusion: ACE和ACED方法能有效缓解异步联邦学习中的异构性放大问题，在多样化设置下表现出稳健性能

Abstract: In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.

</details>


### [189] [EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching](https://arxiv.org/abs/2511.19087)
*Ziyun Li,Ben Dai,Huancheng Hu,Henrik Boström,Soon Hoe Lim*

Main category: cs.LG

TL;DR: 本文提出动能路径能量(KPE)作为评估ODE采样器生成路径的诊断工具，发现高KPE样本具有更好的语义质量但位于数据分布稀疏区域。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注生成结果的端点指标，忽视了采样轨迹所揭示的深层信息。受经典力学启发，希望分析生成路径的动力学特性。

Method: 引入动能路径能量(KPE)来量化ODE采样器每条生成路径的总动能消耗，在CIFAR-10和ImageNet-256上进行实验分析。

Result: 发现两个关键现象：(i)高KPE预测更强的语义质量；(ii)高KPE与数据密度负相关，信息丰富的样本位于稀疏低密度区域。

Conclusion: 语义信息丰富的样本自然位于数据分布的稀疏前沿，需要更大的生成努力。轨迹级分析为理解生成难度和样本特性提供了物理启发的可解释框架。

Abstract: Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.

</details>


### [190] [Optimization of Deep Learning Models for Dynamic Market Behavior Prediction](https://arxiv.org/abs/2511.19090)
*Shenghan Zhao,Yuzhen Lin,Ximeng Yang,Qiaochu Lu,Haozhong Xue,Gaozhe Jiang*

Main category: cs.LG

TL;DR: 提出了一个混合序列模型用于多时间跨度零售需求预测，结合了多尺度时间卷积、门控循环模块和时间感知自注意力机制，在多个评估指标上优于传统方法和最先进的Transformer预测器。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域深度学习的应用日益增多，特别是在预测消费者行为方面显示出提升贷款策略和市场效率的潜力。本研究专注于零售市场行为，明确预测目标为每个SKU的日需求或收入。

Method: 使用混合序列模型，结合多尺度时间卷积、门控循环模块和时间感知自注意力机制，采用标准回归损失训练，通过严格时间分割防止数据泄露。

Result: 模型在MAE、RMSE、sMAPE、MASE和Theil's U_2等多个指标上表现出持续的准确性提升，在高峰/节假日期间具有更好的鲁棒性，并通过消融实验和统计显著性测试验证了改进的可靠性。

Conclusion: 提出的混合序列模型在零售需求预测任务中表现优异，优于ARIMA/Prophet、LSTM/GRU、LightGBM和先进的Transformer预测器，为零售市场行为预测提供了有效解决方案。

Abstract: The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.

</details>


### [191] [Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication](https://arxiv.org/abs/2511.19103)
*Dora Krekovic,Mario Kusek,Ivana Podnar Zarko,Danh Le-Phuoc*

Main category: cs.LG

TL;DR: 提出了一种用于边缘计算环境的预测算法，通过预测传感器数据并在偏差超过阈值时才传输数据，减少通信开销和能耗，支持跨站点部署。


<details>
  <summary>Details</summary>
Motivation: 解决物联网设备在资源受限环境中连续传输传感器数据导致的网络拥塞、延迟和能耗问题，特别是农业等领域中连续读数变化较小的情况。

Method: 在边缘部署预测滤波器预测下一个数据点，仅当实际值与预测值偏差超过预设容差时才传输数据；云端模型确保数据完整性和系统一致性。

Result: 有效减少通信开销，提高能源效率，支持跨区域部署而无需重新训练，增强模型鲁棒性。

Conclusion: 该解决方案具有高度可扩展性、能源感知能力，适用于优化偏远和带宽受限物联网环境中的传感器数据传输。

Abstract: The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.

</details>


### [192] [The Core in Max-Loss Non-Centroid Clustering Can Be Empty](https://arxiv.org/abs/2511.19107)
*Robert Bredereck,Eva Deltl,Leon Kellerhals,Jannik Peters*

Main category: cs.LG

TL;DR: 该论文研究在最大损失目标下的非质心聚类中的核心稳定性问题，证明了对于k≥3的情况，存在度量实例使得任何聚类都不在α-核心中，其中α<2^(1/5)≈1.148。


<details>
  <summary>Details</summary>
Motivation: 研究非质心聚类中核心稳定性的存在性问题，填补了在最大损失目标下核心可能为空的理论空白。

Method: 使用理论证明和计算机辅助证明方法，构造了度量实例和二维欧几里得点集来验证核心稳定性边界。

Result: 证明了对于k≥3且n≥9（n可被k整除）的情况，存在度量实例使得任何聚类都不在α-核心中，其中α<2^(1/5)。该边界对于构造是紧的。

Conclusion: 这是首个证明在最大损失目标下的非质心聚类中核心可能为空的不可能性结果，为聚类稳定性理论提供了重要洞见。

Abstract: We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.

</details>


### [193] [Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty](https://arxiv.org/abs/2511.19124)
*Krishang Sharma*

Main category: cs.LG

TL;DR: 提出了一种新颖的不确定性感知深度学习框架，用于航空发动机剩余使用寿命预测，通过概率建模直接学习偶然不确定性，在关键区域性能上实现突破性改进。


<details>
  <summary>Details</summary>
Motivation: 准确预测剩余使用寿命并进行不确定性量化是航空预测领域的关键挑战，现有CMAPSS文献中尚未探索通过概率建模直接学习偶然不确定性的方法。

Method: 分层架构集成多尺度Inception块进行时间模式提取、双向LSTM进行序列建模、传感器和时间维度双重注意力机制，以及贝叶斯输出层同时预测均值RUL和方差。

Result: 在NASA CMAPSS基准测试中，整体RMSE分别为16.22、19.29、16.84和19.98，关键区域(RUL≤30周期)RMSE为5.14、6.89、5.27和7.16，比传统方法提升25-40%。

Conclusion: 该框架在安全关键预测方面建立了新基准，学习到的不确定性提供了校准良好的95%置信区间，实现了CMAPSS文献中前所未有的风险感知维护调度能力。

Abstract: Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.

</details>


### [194] [Masked Diffusion Models are Secretly Learned-Order Autoregressive Models](https://arxiv.org/abs/2511.19152)
*Prateek Garg,Bhavya Kohli,Sunita Sarawagi*

Main category: cs.LG

TL;DR: 本文提出了一种训练框架，通过多元噪声调度来优化掩码扩散模型的解码顺序，证明MDM目标可以分解为这些顺序上的加权自回归损失。


<details>
  <summary>Details</summary>
Motivation: 观察到MDM在随机顺序下解码token，且该顺序对性能有显著影响，因此希望设计能优化解码顺序的训练框架。

Method: 使用多元噪声调度，建立解码顺序与噪声调度的直接对应关系，打破MDM目标对噪声调度的不变性。

Result: 证明了MDM目标可以精确分解为这些顺序上的加权自回归损失，建立了具有可学习顺序的自回归模型。

Conclusion: 通过多元噪声调度可以识别和优化解码顺序，将MDM转化为具有可学习顺序的自回归模型。

Abstract: Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.

</details>


### [195] [First-order Sobolev Reinforcement Learning](https://arxiv.org/abs/2511.19165)
*Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: 提出一种改进的时间差分学习方法，通过强制一阶贝尔曼一致性来训练价值函数，使其不仅匹配贝尔曼目标值，还匹配关于状态和动作的导数。


<details>
  <summary>Details</summary>
Motivation: 通过使学习到的价值函数在值和导数上都与贝尔曼目标保持一致，可以提高评论家网络的收敛速度并稳定策略梯度，而不改变现有算法的整体结构。

Method: 通过可微动态系统对贝尔曼备份进行微分，获得解析一致的梯度目标，并使用Sobolev型损失将这些梯度目标纳入评论家目标函数中。

Result: 该方法可以无缝集成到现有算法中（如Q学习、DDPG、SAC等），可能带来更快的评论家收敛和更稳定的策略梯度。

Conclusion: 一阶TD匹配原则为强化学习算法提供了一种增强价值函数学习的新方法，有望改善学习效率和稳定性。

Abstract: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.

</details>


### [196] [RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning](https://arxiv.org/abs/2511.19168)
*Deyi Ji,Yuekui Yang,Liqun Liu,Peng Shu,Haiyang Wu,Shaogang Tang,Xudong Chen,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.LG

TL;DR: RAVEN++是一个改进的视频广告审核框架，通过主动强化学习、细粒度违规理解和渐进式多阶段训练，在细粒度违规理解、推理能力和泛化能力方面超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频广告审核模型在细粒度理解、可解释性和泛化能力方面存在不足，需要改进以应对复杂视频广告的精确违规定位需求。

Method: 提出三个关键创新：1）主动强化学习动态适应不同难度样本；2）通过分层奖励函数和推理蒸馏实现细粒度违规理解；3）渐进式多阶段训练结合知识注入、课程被动RL和主动RL。

Result: 在公共和专有数据集上的广泛实验表明，RAVEN++在细粒度违规理解、推理能力和泛化能力方面优于通用LLM和专门模型如RAVEN，并在在线A/B测试中得到验证。

Conclusion: RAVEN++通过创新的强化学习方法和多阶段训练策略，显著提升了视频广告审核的细粒度理解能力和泛化性能，为复杂广告内容审核提供了有效解决方案。

Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.

</details>


### [197] [From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation](https://arxiv.org/abs/2511.19176)
*Jeeho Shin,Kyungho Kim,Kijung Shin*

Main category: cs.LG

TL;DR: TESMR是一个三阶段框架，通过内容增强、关系增强和学习增强逐步优化多模态特征，在食谱推荐任务中比现有方法提升7-15%的Recall@10性能。


<details>
  <summary>Details</summary>
Motivation: 食谱推荐的关键挑战是如何有效利用用户-食谱交互之外的多模态特征。分析表明，即使简单使用多模态信号也能获得有竞争力的性能，系统性地增强这些信号具有很大潜力。

Method: TESMR框架包含三个阶段：(1)基于内容的增强：使用具有多模态理解能力的基础模型；(2)基于关系的增强：通过用户-食谱交互上的消息传播；(3)基于学习的增强：通过可学习嵌入的对比学习。

Result: 在两个真实世界数据集上的实验表明，TESMR优于现有方法，Recall@10指标提高了7-15%。

Conclusion: TESMR通过渐进式多模态特征增强，显著提升了食谱推荐性能，证明了系统化利用多模态信号的有效性。

Abstract: Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.

</details>


### [198] [Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform](https://arxiv.org/abs/2511.19240)
*Minxin Chen*

Main category: cs.LG

TL;DR: 本文提出FDSW-UCB算法，结合折扣长期视角和滑动窗口短期视角，解决非平稳环境中的多臂老虎机问题，在动态设置中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的老虎机问题常涉及非平稳奖励分布，传统UCB算法在奖励分布随时间变化的环境中性能显著下降，需要新的解决方案。

Method: 提出FDSW-UCB双视角算法，集成基于折扣的长期视角和基于滑动窗口的短期视角，并开发基于MovieLens-1M和Open Bandit数据集的半合成仿真平台。

Result: 实验表明滑动窗口机制稳健，而广泛使用的折扣方法存在基本学习失败导致线性遗憾；FDSW-UCB采用乐观聚合策略在动态设置中表现优异。

Conclusion: 集成策略本身是成功的关键因素，FDSW-UCB在非平稳环境中实现了优越性能。

Abstract: Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.

</details>


### [199] [Local Entropy Search over Descent Sequences for Bayesian Optimization](https://arxiv.org/abs/2511.19241)
*David Stenger,Armin Lindicke,Alexander von Rohr,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 提出了局部熵搜索（LES），一种贝叶斯优化方法，专门针对迭代优化器可达的局部解空间进行搜索，通过传播后验信念来指导采样，在复杂优化问题上表现出优异的样本效率。


<details>
  <summary>Details</summary>
Motivation: 在大型复杂设计空间中寻找全局最优解往往不可行且不必要，更实用的方法是通过梯度下降等局部优化方法迭代改进初始设计的邻域。

Method: LES算法通过优化器传播目标函数的后验信念，生成下降序列的概率分布，然后通过解析熵计算和蒙特卡洛采样相结合的方式最大化与该分布的互信息来选择下一个评估点。

Result: 在高复杂度合成目标和基准问题上的实验结果表明，LES相比现有的局部和全局贝叶斯优化方法具有更强的样本效率。

Conclusion: 局部熵搜索为处理复杂优化问题提供了一种有效的贝叶斯优化框架，特别适用于需要高效利用有限评估预算的场景。

Abstract: Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.

</details>


### [200] [MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization](https://arxiv.org/abs/2511.19253)
*Boyuan Wu*

Main category: cs.LG

TL;DR: MAESTRO是一个多智能体强化学习框架，使用LLM作为离线训练架构师，生成语义课程和自动奖励函数，在交通信号控制任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中密集奖励函数设计和课程构建的瓶颈，避免现有方法依赖固定启发式或实时LLM控制的高成本问题。

Method: 将LLM移出执行循环，作为离线训练架构师，包含语义课程生成器和自动奖励合成器，指导标准MADDPG算法而不增加推理成本。

Result: 在16个交叉口的大规模交通信号控制实验中，结合LLM生成的课程和奖励塑形，平均回报提升4.0%，风险调整后性能提升2.2%。

Conclusion: LLM可作为合作多智能体强化学习训练的有效高层设计者，通过离线环境塑造提升学习性能。

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.

</details>


### [201] [A Nutrition Multimodal Photoplethysmography Language Model](https://arxiv.org/abs/2511.19260)
*Kyle Verrier,Achille Nazaret,Joseph Futoma,Andrew C. Miller,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 提出NPLM模型，结合可穿戴设备的PPG信号和饮食描述，通过将PPG投影到语言模型可理解的嵌入中，实现生理数据和饮食情境的联合推理，显著提升日常热量摄入预测精度。


<details>
  <summary>Details</summary>
Motivation: 饥饿和饱腹感动态影响饮食行为和代谢健康，但在日常环境中难以捕捉。需要开发非侵入性的大规模饮食监测方法。

Method: 整合可穿戴设备的连续PPG信号与饮食描述，训练NPLM模型将PPG投影到语言模型可理解的嵌入中，实现生理数据和饮食情境的联合推理。基于19,340名参与者和110万餐食-PPG对进行训练。

Result: 与仅使用文本的基线相比，日常热量摄入预测精度提升11%；在80%餐食文本被移除的情况下仍保持准确性；在独立验证研究（n=140）中结果得到复现。

Conclusion: 将消费者可穿戴设备的生理测量与饮食信息整合，对大规模非侵入性饮食监测具有重要价值。

Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.

</details>


### [202] [Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention](https://arxiv.org/abs/2511.19263)
*Lucas Li,Jean-Baptiste Puel,Florence Carton,Dounya Barrit,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 提出了Solar-GECO模型，通过几何感知的共注意力机制预测钙钛矿太阳能电池的功率转换效率，结合几何图神经网络和语言模型嵌入，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 钙钛矿太阳能电池的性能受多个层级复杂相互作用影响，传统实验筛选方法缓慢且昂贵，现有机器学习模型忽视了钙钛矿晶体的几何信息。

Method: 使用几何图神经网络编码钙钛矿吸收层的原子结构，结合语言模型处理传输层等组件的文本表示，集成共注意力模块捕获层内依赖和层间相互作用，通过概率回归头预测PCE及其不确定性。

Result: Solar-GECO实现了最先进的性能，将PCE预测的平均绝对误差从3.066降低到2.936，显著优于多个基线模型。

Conclusion: 整合几何和文本信息为PCE预测提供了更强大和准确的框架。

Abstract: Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.

</details>


### [203] [Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry](https://arxiv.org/abs/2511.19264)
*Amirtha Varshini A S,Duminda S. Ranasinghe,Hok Hei Tam*

Main category: cs.LG

TL;DR: 提出了一个用于SynFlowNet（一种基于化学反应和可购买起始材料的GFlowNet）的可解释性框架，通过梯度显著性、稀疏自编码器和基序探针三种方法揭示分子设计中的化学逻辑。


<details>
  <summary>Details</summary>
Motivation: GFlowNets在分子设计中很有前景，但其内部决策策略不透明，限制了在药物发现中的应用，因为化学家需要清晰可解释的结构设计理由。

Method: 整合三种互补组件：基于梯度的显著性分析与反事实扰动识别影响奖励的原子环境；稀疏自编码器揭示与物理化学性质对应的潜在因子；基序探针显示功能基团在内部嵌入中的编码情况。

Result: 成功揭示了SynFlowNet内部的化学逻辑，识别了影响分子结果的原子环境、与极性、亲脂性和分子大小等性质对应的潜在因子，以及芳香环和卤素等功能基团的线性可解码编码。

Conclusion: 该框架为SynFlowNet提供了可操作和机制性的洞察，支持透明和可控的分子设计，促进了GFlowNets在药物发现中的采用。

Abstract: Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.

</details>


### [204] [Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks](https://arxiv.org/abs/2511.19265)
*Bianka Kowalska,Halina Kwaśnicka*

Main category: cs.LG

TL;DR: 本文提出了机制可解释性（MI）的统一分类法，分析了关键方法，并将其置于更广泛的可解释AI领域中，强调MI在理解机器学习系统方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的黑盒特性阻碍了透明可信AI系统的部署，需要开发能够解释系统决策的方法。机制可解释性作为可解释AI中一个独特且有前景的研究方向，旨在研究网络内部计算并将其转化为人类可理解的算法。

Method: 提出了机制可解释性方法的统一分类法，详细分析关键技术，提供具体示例和伪代码，并将MI与其他可解释AI方法进行比较。

Result: 建立了MI的系统性框架，展示了如何通过逆向工程技术揭示神经网络实现的计算算法，为理解机器学习系统提供了科学方法。

Conclusion: 机制可解释性具有显著潜力，能够支持对机器学习系统更科学的理解，将模型不仅视为任务解决工具，更是需要研究和理解的系统，希望吸引新研究者进入该领域。

Abstract: The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.

</details>


### [205] [Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting](https://arxiv.org/abs/2511.19267)
*Manish Singh,Arpita Dayama*

Main category: cs.LG

TL;DR: 评估时空图神经网络在零售销售预测中的效果，通过自适应图建模店铺间依赖关系，在Walmart数据上超越ARIMA、LSTM和XGBoost等基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法未充分考虑多店铺零售环境中的空间依赖关系，需要开发能够捕捉店铺间相互影响的预测模型。

Method: 构建关系预测框架，使用学习的自适应图建模店铺间依赖，预测对数差分销售并通过残差路径重构最终值，实现稳定训练和更好泛化。

Result: STGNN在所有指标上表现最佳：标准化总绝对误差、P90 MAPE和各店铺MAPE方差均最低，学习到的邻接矩阵揭示了有意义的店铺功能集群。

Conclusion: 关系结构显著提升互联零售环境中的预测质量，STGNN是多店铺需求预测的稳健建模选择，无需地理元数据即可识别高影响力节点。

Abstract: This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.

</details>


### [206] [CDLM: Consistency Diffusion Language Models For Faster Sampling](https://arxiv.org/abs/2511.19269)
*Minseo Kim,Chenfeng Xu,Coleman Hooper,Harman Singh,Ben Athiwaratkun,Ce Zhang,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: CDLM通过一致性建模和块级因果注意力掩码，解决了扩散语言模型推理速度慢的问题，实现了3.6x-14.5x的延迟降低，同时保持数学和编程任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供了并行生成的范式，但由于需要大量细化步骤且无法使用标准KV缓存，导致推理速度缓慢。

Method: CDLM结合一致性建模大幅减少采样步骤，并通过块级因果注意力掩码使模型完全兼容KV缓存。

Result: 实验表明CDLM在数学和编程任务上实现了3.6x-14.5x的延迟降低，同时保持竞争力的准确率。

Conclusion: CDLM有效解决了扩散语言模型的推理瓶颈，为实际应用提供了可行的加速方案。

Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

</details>


### [207] [Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model](https://arxiv.org/abs/2511.19272)
*Felix Birkel*

Main category: cs.LG

TL;DR: Tiny-TSM是一个小型时间序列基础模型，仅2300万参数，在单张A100 GPU上训练不到一周，在多种时间序列基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个资源需求小但性能优越的时间序列基础模型，解决现有大模型训练成本高、资源消耗大的问题。

Method: 使用新的合成数据生成和数据增强管道(SynthTS)，结合因果输入归一化方案，使用密集的下一个token预测损失进行训练。

Result: 在中长期预测任务上超越所有评估的时间序列基础模型，短期预测性能与最先进模型相当，且训练时间显著缩短。

Conclusion: Tiny-TSM证明了小型模型通过高效的数据生成和训练策略，可以在资源受限环境下达到甚至超越大型模型的性能。

Abstract: We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.
  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.
  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.

</details>


### [208] [Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space](https://arxiv.org/abs/2511.19273)
*Kunal Dumbre,Lei Jiao,Ole-Christoffer Granmo*

Main category: cs.LG

TL;DR: 提出了一种基于Tsetlin Machine的贝叶斯网络结构学习方法，通过选择最重要的文字进行条件独立性测试，显著降低了PC算法的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: PC算法在因果推断中应用广泛，但随着数据集规模增大，其时间复杂度显著增加，限制了在大规模实际问题中的应用。

Method: 利用Tsetlin Machine提取最重要的文字，仅对这些选定的文字进行条件独立性测试，而不是对所有变量进行测试，从而减少计算时间。

Result: 在bnlearn存储库的分类数据集（如Munin1、Hepar2）上评估，与多种先进方法比较，表明该方法不仅降低了计算复杂度，而且在因果发现中保持了竞争力的准确性。

Conclusion: 基于Tsetlin Machine的方法在保持性能的同时提高了效率，是传统PC算法实现的一个可行替代方案。

Abstract: The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.

</details>


### [209] [Closing Gaps in Emissions Monitoring with Climate TRACE](https://arxiv.org/abs/2511.19277)
*Brittany V. Lancellotti,Jordan M. Malof,Aaron Davitt,Gavin McCormick,Shelby Anderson,Pol Carbó-Mestre,Gary Collins,Verity Crane,Zoheyr Doctor,George Ebri,Kevin Foster,Trey M. Gowdy,Michael Guzzardi,John Heal,Heather Hunter,David Kroodsma,Khandekar Mahammad Galib,Paul J. Markakis,Gavin McDonald,Daniel P. Moore,Eric D. Nguyen,Sabina Parvu,Michael Pekala,Christine D. Piatko,Amy Piscopo,Mark Powell,Krsna Raniga,Elizabeth P. Reilly,Michael Robinette,Ishan Saraswat,Patrick Sicurello,Isabella Söldner-Rembold,Raymond Song,Charlotte Underwood,Kyle Bradbury*

Main category: cs.LG

TL;DR: Climate TRACE是一个开放获取平台，提供全球温室气体排放估算，具有增强的细节、覆盖范围和及时性，支持数据驱动的气候行动。


<details>
  <summary>Details</summary>
Motivation: 现有排放数据集缺乏准确性、全球覆盖、高时空分辨率和频繁更新等关键特征，限制了其可操作性。

Method: 综合现有排放数据，优先考虑准确性、覆盖范围和分辨率，并使用特定行业的估算方法填补数据空白。

Result: 首个提供全球全面排放估算的数据集，涵盖所有人为排放行业，包括单个排放源（如发电厂），数据从2021年1月1日至今，每月更新。

Conclusion: Climate TRACE在排放核算和减缓方面代表重大突破，支持在决策层面进行数据驱动的气候行动。

Abstract: Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.

</details>


### [210] [MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings](https://arxiv.org/abs/2511.19279)
*Victor Rambaud,Salvador Mascarenhas,Yair Lakretz*

Main category: cs.LG

TL;DR: MapFormers是基于Transformer的新架构，能够从观测数据中学习认知地图并执行路径整合，通过输入依赖的位置编码实现结构与内容解耦，在OOD泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 认知地图能够编码实体间的抽象关系，使人类和动物具有强大的OOD泛化能力，而当前AI系统缺乏这种能力。

Method: 开发了两种MapFormers变体，通过输入依赖的位置编码更新来统一绝对和相对位置编码，分别建模情景记忆和工作记忆。

Result: 在2D导航等任务中，MapFormers能够学习底层空间的认知地图，并在OOD泛化（如更长序列）上实现近乎完美的性能，优于现有架构。

Conclusion: 结果表明，设计用于学习认知地图的模型具有优越性，输入依赖的位置编码为Transformer引入结构偏置以实现结构-内容解耦具有重要意义。

Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.

</details>


### [211] [Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning](https://arxiv.org/abs/2511.19299)
*James R. M. Black,Moritz S. Hanke,Aaron Maiwald,Tina Hernandez-Boussard,Oliver M. Crook,Jaspreet Pannu*

Main category: cs.LG

TL;DR: 该研究表明，通过微调方法可以绕过基因组语言模型的数据排除安全措施，恢复对有害人类感染病毒的预测能力，即使模型在预训练时未接触相关病毒序列。


<details>
  <summary>Details</summary>
Motivation: 基因组语言模型在生物数据应用中展现出强大的预测和生成能力，引发了可能被滥用于生成人类感染病毒基因组的担忧。目前主要通过在预训练数据中过滤病毒序列来降低风险，但这种方法对开源模型的稳健性尚不清楚。

Method: 使用最先进的基因组语言模型Evo 2，通过对110种有害人类感染病毒的序列进行微调，评估滥用相关预测能力的恢复情况。

Result: 微调后的模型在未见病毒序列上的困惑度显著降低，能够识别SARS-CoV-2的免疫逃逸变异（AUROC达0.6），尽管微调过程中未接触SARS-CoV-2序列。

Conclusion: 数据排除措施可能被微调方法绕过，需要为基因组语言模型建立安全框架，并进一步研究评估和缓解措施以确保安全部署。

Abstract: Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.

</details>


### [212] [Understanding the Staged Dynamics of Transformers in Learning Latent Structure](https://arxiv.org/abs/2511.19328)
*Rohan Saha,Farzane Aminmansour,Alona Fyshe*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型学习潜在结构的动态过程，发现模型分阶段学习：先学习粗粒度规则，再学习完整潜在结构，并揭示了组合与分解能力的不对称性。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型如何从上下文中发现潜在结构，特别是学习不同组件结构的动态过程，目前仍不清楚。

Method: 使用Alchemy基准测试，在三个任务变体上训练小型仅解码器Transformer：1)从部分上下文推断缺失规则，2)组合简单规则解决多步序列，3)分解复杂多步示例推断中间步骤。

Result: 模型分阶段获得能力，先学习粗粒度规则，再学习完整潜在结构；发现关键不对称性：模型能稳健组合基本规则，但难以分解复杂示例来发现基本规则。

Conclusion: 这些发现为理解Transformer模型如何学习潜在结构提供了新见解，展示了训练过程中这些能力如何逐步演化。

Abstract: While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.

</details>


### [213] [Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data](https://arxiv.org/abs/2511.19330)
*Dominik Luszczynski*

Main category: cs.LG

TL;DR: 该研究提出了两种基于斜率的时间序列对抗攻击方法，能够操纵N-HiTS模型的股票预测趋势，使预测斜率翻倍，并能绕过标准安全机制。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在图像领域已有深入研究，但在时间序列领域，特别是金融预测数据方面研究较少，需要填补这一空白。

Method: 提出了两种新的基于斜率的攻击方法：通用斜率攻击和最小二乘斜率攻击，并将其集成到GAN架构中生成逼真的合成数据。

Result: 新方法能将N-HiTS预测的斜率翻倍，使4层CNN判别器的特异性降至28%，准确率降至57%。

Conclusion: ML安全研究不仅需要关注模型本身的安全性，还需要保护整个处理流程，包括模型推理库的安全。

Abstract: A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.

</details>


### [214] [Annotation-Free Class-Incremental Learning](https://arxiv.org/abs/2511.19344)
*Hari Chandana Kuchibhotla,K S Ananth,Vineeth N Balasubramanian*

Main category: cs.LG

TL;DR: 本文提出了注释自由类增量学习（AFCIL）新范式，并开发了CrossWorld CL框架，通过引入外部世界知识（ImageNet）作为稳定辅助源，在无标签数据连续到达的情况下实现有效的增量学习。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法都假设在整个学习过程中有标记数据可用，但现实场景中数据通常是顺序到达且无注释的。本文重新审视持续学习的基本假设，探索在标签缺失且任务随时间增量出现时系统能否适应。

Method: 提出CrossWorld CL框架：1）为每个下游类别检索语义相关的ImageNet类；2）通过跨域对齐策略映射下游和ImageNet特征；3）引入新颖的重放策略。该设计让模型在无注释情况下发现语义结构，同时保持先前知识完整。

Result: 在四个数据集上的实验表明，CrossWorld-CL超越了CLIP基线和现有的持续学习及无标签学习方法，证明了世界知识对注释自由持续学习的益处。

Conclusion: 外部世界知识可以作为稳定辅助源，在无监督的类增量学习场景中有效帮助模型发现语义结构并缓解灾难性遗忘，为更现实的持续学习场景提供了可行解决方案。

Abstract: Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.

</details>


### [215] [Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric](https://arxiv.org/abs/2511.19350)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.LG

TL;DR: 提出了一种可扩展的谱方法，直接从拉普拉斯特征谱结构估计聚类数量，无需预先指定聚类数，并提出了Cohesion Ratio评估指标用于无监督聚类质量评估。


<details>
  <summary>Details</summary>
Motivation: 短文本嵌入聚类是NLP基础任务，但传统方法需要预先指定聚类数量，这在实际应用中具有挑战性。

Method: 使用基于余弦相似度的拉普拉斯特征谱结构，结合自适应采样策略来估计聚类数量；提出Cohesion Ratio评估指标量化簇内相似度与全局相似度的差异。

Result: 在6个短文本数据集和4个嵌入模型上的实验表明，使用该估计器指导的K-Means和HAC算法显著优于HDBSCAN、OPTICS和Leiden等参数较少的方法。

Conclusion: 该谱估计器和Cohesion Ratio为短文本数据的无监督组织和评估提供了实用价值，能够有效估计聚类数量并评估聚类质量。

Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.

</details>


### [216] [Leveraging LLMs for reward function design in reinforcement learning control tasks](https://arxiv.org/abs/2511.19355)
*Franklin Cardenoso,Wouter Caarls*

Main category: cs.LG

TL;DR: LEARN-Opt是一个基于大语言模型的完全自主、模型无关的奖励函数优化框架，无需预定义指标和环境源代码，直接从系统描述和任务目标生成、执行和评估奖励函数。


<details>
  <summary>Details</summary>
Motivation: 设计有效的强化学习奖励函数是一个重大瓶颈，需要大量人工专业知识且耗时。现有方法需要预定义评估指标、人工反馈或环境源代码作为上下文。

Method: LEARN-Opt框架能够自主从系统描述和任务目标中推导性能指标，实现无监督的奖励函数评估和选择，无需预定义指标和环境源代码。

Result: 实验表明LEARN-Opt性能与最先进方法（如EUREKA）相当或更好，且需要较少先验知识。能够利用低成本LLM找到与大型模型相当甚至更好的高性能候选奖励函数。

Conclusion: LEARN-Opt展示了无需人工定义指标即可生成高质量奖励函数的潜力，减少了工程开销并增强了泛化能力。

Abstract: The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

</details>


### [217] [Enhancing Conformal Prediction via Class Similarity](https://arxiv.org/abs/2511.19359)
*Ariel Fargion,Lahav Dabah,Tom Tirer*

Main category: cs.LG

TL;DR: 提出了一种基于类别相似性的共形预测增强方法，通过惩罚组外错误来减少预测集大小，无需人工语义划分即可提升CP方法性能


<details>
  <summary>Details</summary>
Motivation: 在类别可被划分为语义组的情况下，用户不仅需要平均预测集较小，还需要预测集包含较少的语义不同组。现有CP方法主要关注平均预测集大小，未充分考虑语义分组需求

Method: 1) 在CP评分函数中增加惩罚组外错误的项；2) 提出模型特定的变体，无需人工语义划分，利用类别相似性进一步减少预测集大小

Result: 理论分析和实证研究表明，该方法能显著减少预测集大小，在多个CP方法、模型和数据集上一致提升性能

Conclusion: 基于类别相似性的方法为CP提供了广泛适用的增强工具，能有效减少预测集大小并改善组相关指标

Abstract: Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.

</details>


### [218] [Neural surrogates for designing gravitational wave detectors](https://arxiv.org/abs/2511.19364)
*Carlos Ruiz-Gonzalez,Sören Arlt,Sebastian Lehner,Arturs Berzins,Yehonathan Drori,Rana X Adhikari,Johannes Brandstetter,Mario Krenn*

Main category: cs.LG

TL;DR: 使用神经网络替代模型加速物理模拟器，以引力波探测器设计为例，通过训练神经网络替代Finesse模拟器，结合自动微分和GPU并行，实现高效实验设计优化。


<details>
  <summary>Details</summary>
Motivation: 随着实验装置复杂度增加，传统CPU模拟器的计算成本成为主要限制，需要寻找更高效的替代方案来加速实验设计过程。

Method: 训练神经网络替代物理模拟器Finesse，采用循环训练策略：训练替代模型→逆向设计新实验→用慢速模拟器验证→进一步训练，利用自动微分和GPU并行加速。

Result: 算法在几小时内找到的解决方案优于直接优化器运行五天得到的设计，显著提高了设计空间探索效率。

Conclusion: 该方法虽然以引力波探测器为背景，但可广泛应用于其他因模拟器瓶颈而阻碍优化和发现的领域。

Abstract: Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.

</details>


### [219] [LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems](https://arxiv.org/abs/2511.19368)
*Tianyang Duan,Zongyuan Zhang,Zheng Lin,Songxiao Guo,Xiuxian Guan,Guangyu Wu,Zihan Fang,Haotian Meng,Xia Du,Ji-Zhe Zhou,Heming Cui,Jun Luo,Yue Gao*

Main category: cs.LG

TL;DR: RELED是一个可扩展的多智能体强化学习框架，集成了LLM驱动的专家演示与自主智能体探索，通过理论非平稳性边界提升专家轨迹质量，并自适应平衡专家生成与智能体生成轨迹的学习。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在资源受限的边缘设备上部署时，由于智能体策略同步更新导致严重非平稳性，造成训练不稳定和策略收敛差的问题，尤其随着智能体数量增加而加剧。

Method: RELED包含两个核心模块：1）基于理论非平稳性边界的稳定性感知专家演示模块，提升LLM生成的专家轨迹质量；2）混合专家-智能体策略优化模块，自适应平衡从专家生成和智能体生成轨迹的学习。

Result: 基于OpenStreetMap真实城市网络的广泛实验表明，RELED相比最先进的多智能体强化学习方法实现了更优越的性能。

Conclusion: RELED框架通过集成LLM驱动的专家演示和自主探索，有效解决了多智能体强化学习中的非平稳性问题，加速了策略收敛并提高了泛化能力。

Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.

</details>


### [220] [Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware](https://arxiv.org/abs/2511.19379)
*Srishti Gupta,Yashasvee Taiwade*

Main category: cs.LG

TL;DR: Flow Matching在效率上显著优于DDPM，特别适合资源受限的实时生成任务，其学习到的传输路径更接近最优，而DDPM的轨迹则更随机和曲折。


<details>
  <summary>Details</summary>
Motivation: DDPM在生成图像合成方面达到了新的最先进水平，但其推理过程中的计算开销很大，通常需要多达1,000次迭代步骤，这阻碍了其部署。本研究旨在比较DDPM与新兴的Flow Matching范式在低资源硬件上的几何和效率特性。

Method: 通过在MNIST数据集上使用共享的时间条件U-Net骨干网络实现两种框架，进行几何分析和效率比较。通过数值敏感性分析验证学习到的向量场的线性程度。

Result: Flow Matching在效率上显著优于Diffusion，其学习到的传输路径高度校正（曲率≈1.02），接近最优，而Diffusion轨迹保持随机和曲折（曲率≈3.45）。在N=10次函数评估时，Flow Matching保持高保真度，而Diffusion崩溃。

Conclusion: Flow Matching是实时、资源受限生成任务的优越算法选择，其学习到的向量场足够线性，使得高阶ODE求解器变得不必要，可以使用轻量级Euler求解器进行边缘部署。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\mathcal{C} \approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\mathcal{C} \approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}

</details>


### [221] [Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme](https://arxiv.org/abs/2511.19390)
*Rudy Morel,Francesco Pio Ramunno,Jeff Shen,Alberto Bietti,Kyunghyun Cho,Miles Cranmer,Siavash Golkar,Olexandr Gugnin,Geraud Krawezik,Tanya Marwah,Michael McCabe,Lucas Meyer,Payel Mukhopadhyay,Ruben Ohana,Liam Parker,Helen Qu,François Rozet,K. D. Leka,François Lanusse,David Fouhey,Shirley Ho*

Main category: cs.LG

TL;DR: 提出一种用于部分可观测、长记忆动力系统概率预测的多尺度扩散模型推理方案，特别针对太阳动力学等应用场景


<details>
  <summary>Details</summary>
Motivation: 在太阳物理等场景中，只能观测到系统的一小部分状态（如太阳表面），而系统演化由无法直接测量的内部过程驱动，标准自回归推理方案无法有效捕捉长期依赖关系

Method: 提出多尺度推理方案，生成在近期时间粒度精细、远期时间粒度粗糙的轨迹，在不增加计算成本的情况下捕获长期时间依赖性

Result: 该方法显著减少了预测分布的偏差，提高了推演稳定性

Conclusion: 多尺度推理方案能够有效解决部分可观测长记忆动力系统的概率预测问题，特别适用于太阳动力学等物理过程

Abstract: Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.

</details>


### [222] [Learning Robust Social Strategies with Large Language Models](https://arxiv.org/abs/2511.19405)
*Dereck Piche,Mohammed Muqeeth,Milad Aghajohari,Juan Duque,Michael Noukhovitch,Aaron Courville*

Main category: cs.LG

TL;DR: 本文研究多智能体强化学习中的社会困境问题，发现RL训练的LLM智能体会发展出机会主义行为，并提出Advantage Alignment算法来促进多智能体合作和抗利用性。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI的普及，具有不同且可能冲突目标的多智能体交互面临挑战，特别是在社会困境中，个体激励可能损害集体福利。

Method: 采用Advantage Alignment对手学习感知算法来微调LLM，引入组相对基线简化迭代游戏中的优势计算，并创建了需要自然语言通信的新社会困境环境Trust and Split。

Result: 在各种社会困境中，使用Advantage Alignment学习的策略实现了更高的集体收益，同时保持对贪婪智能体利用的鲁棒性。

Conclusion: Advantage Alignment算法能有效解决多智能体RL中的合作问题，使LLM智能体在保持抗利用性的同时实现更好的集体福利。

Abstract: As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.

</details>


### [223] [UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413)
*Zhaolong Su,Wang Lu,Hao Chen,Sharon Li,Jindong Wang*

Main category: cs.LG

TL;DR: UniGame是一个自对抗后训练框架，通过轻量级扰动器在共享token接口处应用，让生成分支主动挑战脆弱的理解能力，从而解决统一多模态模型中理解与生成之间的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型在理解和生成任务中表现出不一致性：理解倾向于紧凑嵌入，而生成偏好重构丰富的表示。这种结构权衡导致决策边界错位、跨模态连贯性下降以及对分布和对抗性变化的脆弱性增加。

Method: UniGame采用自对抗后训练框架，在共享token接口处应用轻量级扰动器，使生成分支能够主动寻找和挑战脆弱的理解能力，让模型自身成为其对手。

Result: UniGame显著提高了一致性(+4.6%)，同时在理解(+3.6%)、生成(+0.02)、分布外和对抗鲁棒性(NaturalBench +4.8%，AdVQA +6.2%)方面也取得了实质性改进。该框架架构无关，仅引入不到1%的额外参数，并与现有后训练方法互补。

Conclusion: 对抗性自玩是增强未来多模态基础模型连贯性、稳定性和统一能力的通用有效原则。

Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame

</details>


### [224] [Flow Map Distillation Without Data](https://arxiv.org/abs/2511.19428)
*Shangyuan Tong,Nanye Ma,Saining Xie,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种无需外部数据集的流映射蒸馏方法，仅从先验分布采样，避免了教师-数据不匹配问题，在ImageNet上实现了最先进的单步生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统流映射蒸馏依赖外部数据集，存在教师-数据不匹配风险，因为静态数据集可能无法完整代表教师的生成能力。本文质疑这种数据依赖的必要性。

Method: 开发了一个基于先验分布采样的无数据框架，学习预测教师的采样路径并主动纠正累积误差，确保高保真度。

Result: 在ImageNet 256x256上达到FID 1.45，ImageNet 512x512上达到FID 1.49，仅需1个采样步骤，超越了所有基于数据的方法。

Conclusion: 建立了一个更稳健的生成模型加速范式，推动了无需数据的流映射蒸馏的广泛应用。

Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [225] [Learning Scalable Temporal Representations in Spiking Neural Networks Without Labels](https://arxiv.org/abs/2511.18542)
*Chengwei Zhou,Gourav Datta*

Main category: cs.ET

TL;DR: 提出了一种自监督脉冲神经网络训练范式，通过双路径神经元和时序对齐目标，使大型SNN架构能够在无标签数据下训练，并在ImageNet等基准测试中取得良好性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络具有高效推理的潜力，但由于脉冲的不连续性破坏了对比学习和一致性目标所需的跨视图梯度对应关系，难以扩展到自监督学习场景。

Method: 设计了双路径神经元结构，将脉冲生成过程与可微分代理分支配对，允许梯度在增强输入间传播，同时保持推理时完全脉冲化；提出了时序对齐目标来增强表示一致性。

Result: 在卷积和Transformer风格的SNN骨干网络上实现了ImageNet规模的自监督预训练，在分类、检测和分割任务上表现出色。Spikformer-16-512模型在ImageNet-1K上达到70.1%的top-1准确率。

Conclusion: 证明了高容量脉冲神经网络在现代规模下进行无标签学习是可行的，为SNN的自监督学习开辟了新途径。

Abstract: Spiking neural networks (SNNs) exhibit temporal, sparse, and event-driven dynamics that make them appealing for efficient inference. However, extending these models to self-supervised regimes remains challenging because the discontinuities introduced by spikes break the cross-view gradient correspondences required by contrastive and consistency-driven objectives. This work introduces a training paradigm that enables large SNN architectures to be optimized without labeled data. We formulate a dual-path neuron in which a spike-generating process is paired with a differentiable surrogate branch, allowing gradients to propagate across augmented inputs while preserving a fully spiking implementation at inference. In addition, we propose temporal alignment objectives that enforce representational coherence both across spike timesteps and between augmented views. Using convolutional and transformer-style SNN backbones, we demonstrate ImageNet-scale self-supervised pretraining and strong transfer to classification, detection, and segmentation benchmarks. Our best model, a fully self-supervised Spikformer-16-512, achieves 70.1% top-1 accuracy on ImageNet-1K, demonstrating that unlabeled learning in high-capacity SNNs is feasible at modern scale

</details>


### [226] [Design and Validation of a Modular Smart Headband with Embroidered Electrodes for Comfortable EEG Monitoring](https://arxiv.org/abs/2511.19348)
*Komal Komal,Frances Cleary,Ram Prasadh Narayanan,John Wells,Marco Buiatti,Louise Bennett*

Main category: cs.ET

TL;DR: 开发了一种智能模块化头带，使用可调节、可更换的刺绣电极进行EEG采集，相比传统设备减少了布线复杂性、无需皮肤准备、最小化凝胶电极的刺激，并在行为任务测试中表现出与商业海绵EEG帽相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前EEG系统存在布线复杂、皮肤准备时间长、凝胶不适、刺激风险和成本高等问题，限制了长期监测，特别是对神经系统疾病患者。

Method: 设计智能模块化头带，采用可调节、可更换的刺绣电极，在10名健康大学生中进行测试，包括睁眼/闭眼、听觉oddball和视觉oddball范式。

Result: 成功捕捉睁眼/闭眼任务中的alpha峰(p=0.01)，可靠记录oddball效应的相关电位-听觉P300(p=0.014)和视觉N170(p=0.013)，性能与商业海绵EEG帽相当。用户调查显示舒适度和可用性提升。

Conclusion: 该原型为现实应用中的可靠EEG监测提供了舒适、模块化且经济高效的解决方案。

Abstract: The wearable EEG device sector is advancing rapidly, enabling fast and reliable detection of brain activity for investigating brain function and pathology. However, many current EEG systems remain challenging for users with neurological conditions due to bulky wiring, lengthy skin preparation, gel-induced discomfort, risk of irritation, and high cost, all of which limit long-term monitoring. This study presents a proof-of-concept smart modular headband incorporating adjustable, replaceable embroidered electrodes for EEG acquisition. Compared with conventional devices, the smart headband reduces wiring complexity, removes the need for skin preparation, and minimizes irritation associated with gel-based electrodes. Its modular structure allows adjustable fitting without requiring multiple size options, enhancing comfort and adaptability for everyday EEG monitoring. The smart headband prototype was tested on 10 healthy university students using three behavioral tasks: (1) eyes open/closed, (2) auditory oddball, and (3) visual oddball paradigms. The smart headband successfully captured alpha peaks during the eyes-open/closed task (p = 0.01) and reliably recorded the event-related potentials associated with the oddball effects - the auditory P300 (p = 0.014) and the visual N170 (p = 0.013) - demonstrating an equivalent performance to a commercial sponge-based EEG cap. A user survey indicated improved comfort and usability, with participants reporting that the soft, structurally designed headband enhanced wearability relative to a conventional cap. Overall, this prototype provides a comfortable, modular, and cost-effective solution to reliable EEG monitoring in real-world applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [227] [Optimized Memory Tagging on AmpereOne Processors](https://arxiv.org/abs/2511.17773)
*Shiv Kaushik,Mahesh Madhav,Nagi Aboulenein,Jason Bessette,Sandeep Brahmadathan,Ben Chaffin,Matthew Erler,Stephan Jourdan,Thomas Maciukenas,Ramya Masti,Jon Perry,Massimo Sutera,Scott Tetrick,Bret Toll,David Turley,Carl Worth,Atiq Bajwa*

Main category: cs.AR

TL;DR: AmpereOne处理器首次在数据中心处理器中支持ARM MTE内存标签扩展，提供无内存容量开销的同步标签检查，对各类工作负载性能影响仅为个位数百分比，有效防御内存安全漏洞攻击。


<details>
  <summary>Details</summary>
Motivation: 解决C/C++等指针语言中持续存在的内存安全漏洞问题，这些漏洞是各种安全攻击的主要源头，现有解决方案在开销和适用性方面存在限制。

Method: 采用ARM AArch64指令集架构中的内存标签扩展(MTE)，在AmpereOne处理器中实现优化的MTE硬件支持，提供同步标签检查功能，且标签存储不占用额外内存容量。

Result: AmpereOne处理器的MTE实现能够确定性检测和防御顺序缓冲区溢出攻击，概率性检测和防御时间性use-after-free指针错误，对数据中心工作负载的性能影响仅为个位数百分比。

Conclusion: AmpereOne处理器的高效MTE硬件实现与明确的软件优化路径相结合，使其非常适合在生产云环境中部署，为内存安全提供了实用且高效的解决方案。

Abstract: Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.

</details>


### [228] [Comprehensive Design Space Exploration for Tensorized Neural Network Hardware Accelerators](https://arxiv.org/abs/2511.17971)
*Jinsong Zhang,Minghe Li,Jiayi Tian,Jinming Lu,Zheng Zhang*

Main category: cs.AR

TL;DR: 提出了一个统一的协同探索框架，通过联合优化张量收缩路径、硬件架构和数据流映射，在边缘平台上实现张量化神经网络的高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有高阶张量分解研究主要关注算法优势，忽视了硬件部署效率，导致张量化模型在实际设备上的延迟和能耗潜力未能充分发挥。

Method: 构建统一设计空间，将收缩路径、硬件架构和数据流映射紧密耦合，制定面向延迟的搜索目标，通过全局延迟驱动探索实现端到端模型效率。

Result: 在可配置FPGA内核上实现优化配置，相比密集基线实现了推理延迟降低4倍、训练延迟降低3.85倍。

Conclusion: 张量收缩路径、硬件架构和数据流映射需要联合优化，提出的协同探索框架能显著提升张量化神经网络在边缘平台上的部署效率。

Abstract: High-order tensor decomposition has been widely adopted to obtain compact deep neural networks for edge deployment. However, existing studies focus primarily on its algorithmic advantages such as accuracy and compression ratio-while overlooking the hardware deployment efficiency. Such hardware-unaware designs often obscure the potential latency and energy benefits of tensorized models. Although several works attempt to reduce computational cost by optimizing the contraction sequence based on the number of multiply-accumulate operations, they typically neglect the underlying hardware characteristics, resulting in suboptimal real-world performance. We observe that the contraction path, hardware architecture, and dataflow mapping are tightly coupled and must be optimized jointly within a unified design space to maximize deployment efficiency on real devices. To this end, we propose a co-exploration framework that unifies these dimensions within a unified design space for efficient training and inference of tensorized neural networks on edge platforms. The framework formulates a latency oriented search objective and solves it via a global latency-driven exploration across the unified design space to achieve end-to-end model efficiency. The optimized configurations are implemented on a configurable FPGA kernel, achieving up to 4 and 3.85 lower inference and training latency compared with the dense baseline.

</details>


### [229] [HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash](https://arxiv.org/abs/2511.18234)
*Quanling Zhao,Yanru Chen,Runyang Tian,Sumukh Pinge,Weihong Xu,Augusto Vega,Steven Holmes,Saransh Gupta,Tajana Rosing*

Main category: cs.AR

TL;DR: HDDB是一个结合超维计算(HDC)和铁电NAND存储器的软硬件协同设计，用于在存储中执行SQL谓词评估和分析，具有大规模并行性和最小数据移动。


<details>
  <summary>Details</summary>
Motivation: HDC的噪声容忍特性与新兴铁电NAND存储器的高密度和存储内计算能力天然匹配，但后者存在高原始比特错误率问题。传统SQL数据库在谓词评估和扫描方面需要低能耗和低延迟。

Method: 提出新颖的HDC编码技术用于标准SQL数据表，将基于谓词的过滤和聚合表示为高效的HDC操作，利用HDC内在冗余性在设备噪声下保持正确结果，无需显式纠错开销。

Result: 在TPC-DS事实表上的实验显示，HDDB相比传统CPU/GPU SQL数据库引擎实现了80.6倍的低延迟和12,636倍的低能耗。

Conclusion: HDDB为噪声鲁棒、内存中心的数据处理提供了实用基础，特别适合大规模并行存储内SQL分析。

Abstract: Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.

</details>


### [230] [Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding](https://arxiv.org/abs/2511.18687)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: NVIDIA Split-Frame Encoding (SFE) 技术通过并行编码显著提升4K/8K视频编码吞吐量，在实时应用中几乎翻倍吞吐量且RD性能损失可忽略，同时降低功耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 随着消费设备支持4K/8K超高清视频拍摄，需要高性能视频转码器进行互联网传输。NVIDIA GPU在数据中心广泛应用，NVENC编码器在UHD视频转码中发挥关键作用。

Method: 利用高端GPU中的多个NVENC芯片，将单个UHD帧分割进行并行编码，然后拼接结果，显著提升编码吞吐量。

Result: 实时应用中SFE几乎使编码吞吐量翻倍，RD性能损失可忽略，支持4K使用更高质量预设，实现8K实时编码，4K无额外延迟，8K可降低延迟。

Conclusion: SFE是实现高吞吐量、实时UHD转码的关键技术，在吞吐量、功耗和延迟方面提供显著优势。

Abstract: NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.

</details>


### [231] [Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding](https://arxiv.org/abs/2511.18688)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: 评估NVIDIA、Intel和AMD GPU硬件编码器的低延迟编码模式，比较其与软件编码器在码率-失真性能和延迟方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着4K UHD视频流媒体需求增长，硬件编码器集成到GPU中，需要了解其在低延迟模式下的性能表现，特别是在6G时代对超低延迟的需求。

Method: 从码率-失真性能和延迟两个角度评估NVIDIA、Intel和AMD GPU的低延迟编码模式，并与硬件编码器的正常延迟模式和领先软件编码器进行比较。

Result: 硬件编码器比软件解决方案实现显著更低的端到端延迟，码率-失真性能略优；超低延迟模式将E2E延迟降至83毫秒（5帧）且不影响RD性能；硬件编码器延迟对质量预设不敏感。

Conclusion: 硬件编码器能够实现高质量、低延迟的视频流传输，特别是在超低延迟模式下表现出色，为6G时代的实时视频应用提供了有效解决方案。

Abstract: The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.

</details>


### [232] [Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing](https://arxiv.org/abs/2511.18755)
*Xiaotong Huang,He Zhu,Tianrui Ma,Yuxiang Xiong,Fangxin Liu,Zhezhi He,Yiming Gan,Zihan Liu,Jingwen Leng,Yu Feng,Minyi Guo*

Main category: cs.AR

TL;DR: Splatonic是一个稀疏高效的3DGS-SLAM软硬件协同设计，通过自适应稀疏像素采样和像素级渲染流水线，在移动设备上实现实时性能，相比移动GPU获得274.9倍加速和4738.5倍能耗节省。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM算法在移动平台上计算成本过高，特别是跟踪过程，限制了其在资源受限设备上的实用性。

Method: 提出自适应稀疏像素采样算法（减少256倍渲染像素）和像素级渲染流水线（高斯并行渲染和预检α检查），并设计流水线架构解决投影和聚合瓶颈。

Result: 在瓶颈阶段获得121.7倍加速，端到端加速14.6倍；相比移动GPU获得274.9倍加速和4738.5倍能耗节省；相比最先进加速器获得25.2倍加速和241.1倍能耗节省。

Conclusion: Splatonic通过软硬件协同设计成功解决了3DGS-SLAM在移动设备上的性能瓶颈，实现了实时高效的SLAM系统。

Abstract: 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.
  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $α$-checking. Together, these optimizations yield up to 121.7$\times$ speedup on the bottleneck stages and 14.6$\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\times$ speedup and 4738.5$\times$ energy savings over mobile GPUs and up to 25.2$\times$ speedup and 241.1$\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.

</details>


### [233] [HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays](https://arxiv.org/abs/2511.19366)
*Alan Jia Bao Du,Tarek S. Abdelrahman*

Main category: cs.AR

TL;DR: HeLEx框架用于优化异构粗粒度可重构阵列(CGRA)的功能布局，通过分支定界搜索减少处理单元支持的操作数量，显著降低CGRA面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有的CGRA设计通常采用全功能布局，导致资源浪费。需要一种方法在保证映射成功率的同时，优化CGRA的功能布局以减少面积和功耗。

Method: 采用分支定界搜索算法，从全功能布局开始逐步消除处理单元中不必要的操作，确保输入数据流图能成功映射到生成的异构CGRA上。

Result: 实验显示平均减少68.7%的操作数量，CGRA面积减少近70%，功耗降低超过51%，与理论最小CGRA的差距仅为6.2%。

Conclusion: HeLEx框架能有效生成优化的异构CGRA功能布局，在操作数量减少方面优于现有方法达2.6倍。

Abstract: We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [234] [GROOT: General-Purpose Automatic Parameter Tuning Across Layers, Domains, and Use Cases](https://arxiv.org/abs/2511.17922)
*Robert Krahn,Josia Mädler,Christoph Seidl,Christof Fetzer*

Main category: cs.PF

TL;DR: Groot是一个通用配置调优器，专为小型创新企业设计，能够跨不同技术栈平衡多个优化目标，无需特定领域知识。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统运行在多层级运行时栈上，每层都有执行和成本开销。现有专用调优器受限于特定领域、固定优化目标或特定技术，不适合小型创新企业的多样化需求。

Method: 开发了Groot通用配置调优器，具有领域无关性、多目标平衡能力、支持自定义技术设置、对参数类型和范围假设最少的特点。

Result: 在真实用例和基准测试中的评估表明，Groot在代表小型创新企业的场景中可靠地提高了性能并减少了资源消耗。

Conclusion: Groot为小型创新企业提供了一个有效的通用配置调优解决方案，克服了现有专用调优器的局限性。

Abstract: Modern software systems are executed on a runtime stack with layers (virtualization, storage, trusted execution, etc.) each incurring an execution and/or monetary cost, which may be mitigated by finding suitable parameter configurations. While specialized parameter tuners exist, they are tied to a particular domain or use case, fixed in type and number of optimization goals, or focused on a specific layer or technology. These limitations pose significant adoption hurdles for specialized and innovative ventures (SIVs) that address a variety of domains and use cases, operate under strict cost-performance constraints requiring tradeoffs, and rely on self-hosted servers with custom technology stacks while having little data or expertise to set up and operate specialized tuners. In this paper, we present Groot - a general-purpose configuration tuner designed to a) be explicitly agnostic of a particular domain or use case, b) balance multiple potentially competing optimization goals, c) support different custom technology setups, and d) make minimal assumptions about parameter types, ranges, or suitable values. Our evaluation on both real-world use cases and benchmarks shows that Groot reliably improves performance and reduces resource consumption in scenarios representative for SIVs.

</details>


### [235] [Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration](https://arxiv.org/abs/2511.18674)
*Alfredo Metere*

Main category: cs.PF

TL;DR: 提出Low-Rank GEMM方法，利用低秩矩阵近似实现次二次复杂度，在NVIDIA RTX 4090上达到378 TFLOPS，比PyTorch FP32快7.8倍且节省75%内存


<details>
  <summary>Details</summary>
Motivation: 传统矩阵乘法具有立方计算复杂度，限制了大规模机器学习工作负载的性能

Method: 使用低秩矩阵近似结合FP8精度和智能内核选择，自动选择最优分解方法（SVD、随机SVD）和精度级别

Result: 在NVIDIA RTX 4090上，对N=20480的矩阵达到378 TFLOPS，当N≥10240时成为最快方法，超越传统cuBLAS实现

Conclusion: 通过内存带宽优化而非计算捷径，低秩GEMM在大规模矩阵乘法中实现了显著性能提升

Abstract: Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\mathcal{O}(n^3)$ for a matrix of size $n\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\% memory savings and $7.8\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.

</details>
