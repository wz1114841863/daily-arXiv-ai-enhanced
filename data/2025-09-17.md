<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 14]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [An Analysis of Resource Allocation and User Association Strategies in Space-Air-Ground Integrated Networks](https://arxiv.org/abs/2509.12657)
*Siri Vennela Geddam,Sruthi Ilapuram,Kamesh Namuduri,K L V Sai Prakash Sakuru*

Main category: cs.ET

TL;DR: 本文对空天地一体化网络(SAGIN)的资源管理框架进行了详细分析，比较了交替优化(AO)、阻尼迭代注水(DIWF)和遗传算法(GA)等资源分配方法，并通过MATLAB仿真验证了这些算法在10,000次试验中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着智能交通、医疗保健、智慧城市和灾难响应等应用对无缝数据连接的需求日益增长，需要研究空天地一体化网络的资源管理和用户关联策略，以设计弹性和高效的下一代网络。

Method: 采用文献综述和MATLAB仿真方法，对交替优化(AO)、阻尼迭代注水(DIWF)和遗传算法(GA)三种资源分配算法进行了10,000次试验的基准测试，并分析了紧急情况和网络过载时的用户关联策略。

Result: 仿真结果表明，所评估的资源分配算法能够实现鲁棒、公平和低延迟的资源分配，为紧急场景下的用户关联策略提供了深入分析。

Conclusion: 研究为空天地一体化网络的设计提供了指导，未来研究方向包括卫星切换和多域编排等SAGIN部署相关问题的研究。

Abstract: Space-Air-Ground-Integrated Networks (SAGIN) enable seamless data
connectivity for applications such as smart transport, healthcare, smart
cities, and disaster response through the coordinated use of low-earth orbit
(LEO) satellites, base stations mounted with uncrewed aerial vehicles (UAV),
and terrestrial infrastructure. This paper provides a detailed analysis of
resource management frameworks, reviews the literature, and evaluates key
methods such as alternating optimization (AO), damped iterative water filling
(DIWF), and genetic algorithms (GA) for resource allocation. MATLAB simulation
results benchmark these algorithms across 10,000 trials, demonstrating robust,
fair, and low-latency resource allocation. In addition, this paper also
analyzes strategies for user association with terrestrial and aerial base
stations during emergencies and network overloads. The main contributions
include a comparative assessment of resource allocation strategies in SAGIN and
an in-depth analysis of user association policies for emergency scenarios. The
study provides guidance for designing resilient and efficient next-generation
networks. Potential future research directions include investigating satellite
handover and multi-domain orchestration for SAGIN deployments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: PowerGrow是一个用于生成电力系统测试案例的协同生成框架，通过依赖分解和分层图扩散过程，有效合成电网拓扑结构和动态负载，显著降低计算成本并保持运行有效性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统日益动态化，但公开测试案例稀缺，需要生成工具来联合合成电网结构和节点动态，同时保持物理可行性和避免过高计算成本。

Method: 采用依赖分解方法，将复杂联合分布分解为条件分布链，使用分层图beta扩散过程进行结构合成，配合时间自编码器将时间序列数据嵌入紧凑潜在空间。

Result: 在基准测试中，PowerGrow在保真度和多样性方面优于先前的扩散模型，达到98.9%的潮流收敛率和改进的N-1事故韧性。

Conclusion: PowerGrow能够生成运行有效且现实的电网场景，为电力系统研究提供了高质量的测试案例生成工具。

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [3] [Scaling Up Data Parallelism in Decentralized Deep Learning](https://arxiv.org/abs/2509.12213)
*Bing Xie,Junqi Yin,Zhenyu Zhou,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 本文提出了DBench基准测试框架和Ada自适应方法，发现在大规模去中心化DNN训练中，模型精度与通信图连接数和参数张量方差密切相关，并通过动态调整通信图实现了与中心化学习相当的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化学习在理论上得到广泛探索，但由于在大规模DNN训练中缺乏稳定性、可扩展性和通用性，尚未在生产环境中广泛应用。本研究旨在探索去中心化学习在生产环境中的实际应用。

Method: 引入DBench基准测试框架，包含中心化和去中心化DNN训练；提出基准测试方法分析模型精度与参数张量方差的相关性；基于观察结果提出Ada方法，在去中心化SGD训练中动态调整通信图。

Result: 研究发现：(1)去中心化训练存在可扩展性和通用性问题；(2)模型精度与通信图连接数相关；(3)模型精度对参数张量方差异常敏感。Ada方法在1008个GPU上训练ResNet50时，能获得最佳收敛率，且模型精度与中心化学习相当。

Conclusion: 去中心化学习在大规模DNN训练中具有生产应用潜力，通过动态调整通信图的Ada方法能够有效解决可扩展性和精度问题，实现与中心化学习相当的性能表现。

Abstract: Although it has been extensively explored in theory, decentralized learning
is not yet green-lighted for production use, largely due to a lack of
stability, scalability, and generality in large scale DNN training. To shed
light on the production use of decentralized learning, this work studies
decentralized data parallel training at scale. To this end, we introduce a
benchmarking framework, namely DBench, to host both centralized and
decentralized DNN training. Building upon DBench, we introduce a benchmarking
methodology to uncover the correlations between model accuracy and the
variances of parameter tensors by varying communication graphs and training
scales. Based on the benchmarking results, we observe that, (1) Similar to
centralized learning, decentralized data parallel training also presents the
issues of scalability and generality when the training scales up; (2) The model
accuracy of decentralized learning is correlated to the number of connections
in a communication graph; (3) The model accuracy of decentralized learning is
surprisingly sensitive to the variance of parameter tensors across model
replicas. Built upon the observations, we propose Ada, a decentralized adaptive
approach that performs large scale DNN training following a decentralized SGD
method and adapting the communication graph in use dynamically throughout
training iterations. We apply Ada on large scale training and observe that Ada
can obtain the best convergence rates consistently in decentralized DNN
training, and delivers equally or comparably good model accuracy for all sample
applications as centralized learning does, even when training ResNet50 for
ImageNet-1K on the scale of 1008 GPUs.

</details>


### [4] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: MEUV框架通过分解单一拒绝方向为多个主题对齐的、近乎正交的向量，实现了细粒度的主题级能力激活，在保持高攻击成功率的同时大幅减少跨主题泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐机制会同时拒绝恶意请求和合法的高风险场景使用，而之前的单一拒绝方向编辑方法缺乏语义控制，会无差别解锁所有危险主题。

Method: 提出相互排斥解锁向量(MEUV)框架，将单一拒绝方向分解为多个主题对齐的近乎正交向量，通过多任务目标（包含差分消融边界、跨主题惩罚、正交性惩罚等）在单轮训练中学习。

Result: 在双语恶意提示基准测试中，MEUV在多个模型上达到不低于87%的攻击成功率，同时相比最佳单方向基线减少高达90%的跨主题泄漏，且中英文向量可几乎无损迁移。

Conclusion: MEUV证明了通过最小化效用损失实现细粒度主题级能力激活的可行性，为安全敏感领域的受控LLM部署铺平了道路。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [5] [Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems](https://arxiv.org/abs/2509.12222)
*Binquan Guo,Junteng Cao,Marie Siew,Binbin Chen,Tony Q. S. Quek,Zhu Han*

Main category: cs.LG

TL;DR: 提出基于离散时间图的按需调度框架，用于卫星网络中的联邦学习加速，相比传统方法减少14.20%-41.48%的训练时间


<details>
  <summary>Details</summary>
Motivation: 大规模低轨卫星系统支持广域AI协同训练，但受隐私限制无法集中数据。联邦学习虽保护隐私，但卫星网络动态拓扑和有限带宽导致参数聚合延迟，训练时间过长

Method: 构建离散时间图按需调度框架，动态分配通信资源来加速联邦学习过程

Result: 仿真显示相比传统统计复用策略，训练轮次时间减少14.20%-41.48%，模型越大、客户端越多加速效果越明显

Conclusion: 所提方法能有效解决卫星网络中联邦学习的通信瓶颈问题，具有良好的可扩展性，特别适合大规模模型和多客户端场景

Abstract: Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.

</details>


### [6] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: TripOptimizer是一个完全可微分的深度学习框架，用于从车辆点云数据直接进行快速气动分析和形状优化，通过变分自编码器和三平面隐式神经表示实现高保真3D几何重建和阻力系数预测。


<details>
  <summary>Details</summary>
Motivation: 传统基于计算流体动力学的气动形状优化计算成本过高，严重限制了设计空间探索，需要开发更高效的优化方法。

Method: 使用变分自编码器结合三平面隐式神经表示进行高保真3D几何重建，并添加阻力系数预测头。在包含8000个独特车辆几何形状和对应阻力系数的大型数据集DrivAerNet++上进行训练，学习编码气动相关几何特征的潜在表示。提出通过修改编码器参数子集来引导初始几何形状朝向目标阻力值的优化策略。

Result: 优化设计实现了高达11.8%的阻力系数降低，结果通过独立的高保真CFD模拟（超过1.5亿个网格单元）验证。隐式表示对几何缺陷具有固有鲁棒性，能够优化非水密网格。

Conclusion: 该框架实现了更敏捷的气动形状优化工作流程，减少了对计算密集型CFD模拟的依赖，特别是在早期设计阶段，为传统伴随方法难以处理的非水密网格优化提供了有效解决方案。

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [7] [A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics](https://arxiv.org/abs/2509.12226)
*Aiping Zhong,Baike She,Philip E. Paré*

Main category: cs.LG

TL;DR: 提出了基于物理信息神经网络(PINNs)的模型预测控制(MPC)框架，用于SIR传染病模型的联合状态和参数实时估计与最优控制，解决了现有研究中只能估计状态或参数的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MPC流行病控制研究通常假设要么动态状态可测量（学习参数），要么模型参数已知（学习状态）。本文旨在解决在MPC框架内仅使用噪声感染状态数据，联合实时估计状态和参数的问题。

Method: 提出了MPC-PINNs框架及两种新PINNs算法：1) MPC-LS-PINNs使用对数尺度损失函数提高抗噪鲁棒性；2) MPC-SI-PINNs利用积分算子和状态耦合有效重建完整流行病状态信息。针对不同假设条件扩展了算法。

Result: 实验结果表明所提方法在不同设置下均有效，能够同时估计流行病状态和参数，并生成最优控制策略。

Conclusion: 该研究成功将PINNs集成到MPC框架中，实现了SIR模型的联合状态参数估计和最优控制，为流行病控制提供了新的有效方法。

Abstract: This work introduces a physics-informed neural networks (PINNs)-based model
predictive control (MPC) framework for susceptible-infected-recovered ($SIR$)
spreading models. Existing studies in MPC design for epidemic control often
assume either 1) measurable states of the dynamics, where the parameters are
learned, or 2) known parameters of the model, where the states are learned. In
this work, we address the joint real-time estimation of states and parameters
within the MPC framework using only noisy infected states, under the assumption
that 1) only the recovery rate is known, or 2) only the basic reproduction
number is known. Under the first assumption, we propose MPC-PINNs and two novel
PINNs algorithms, all of which are integrated into the MPC framework. First, we
introduce MPC-PINNs, which are designed for $SIR$ models with control. We then
propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss
function to improve robustness against noise. Next, we present split-integral
PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in
the neural network training process to effectively reconstruct the complete
epidemic state information. Building upon these methods, we further extend our
framework for the second assumption. We establish the necessary conditions and
extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs
(MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we
simultaneously estimate the epidemic states and parameters while generating
optimal control strategies. Experiment results demonstrate the effectiveness of
the proposed methods under different settings.

</details>


### [8] [Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction](https://arxiv.org/abs/2509.12227)
*Marzieh Ajirak,Oded Bein,Ellen Rose Bowen,Dora Kanellopoulos,Avital Falk,Faith M. Gunning,Nili Solomonov,Logan Grosenick*

Main category: cs.LG

TL;DR: 提出了一个自适应路由框架，用于处理多任务多模态预测中的数据异质性和任务交互变化，在心理治疗应用中表现出色


<details>
  <summary>Details</summary>
Motivation: 解决心理治疗中结构化评估和非结构化临床笔记共存、数据部分缺失和结果相关性的问题，需要个性化自适应信息处理

Method: 基于路由的架构，动态选择模态处理路径和任务共享策略，定义多种模态路径（原始和融合的文本/数值特征表示），学习为每个输入选择最信息丰富的专家组合

Result: 在合成数据和真实心理治疗笔记数据上均优于固定多任务或单任务基线，学习到的路由策略提供了模态相关性和任务结构的可解释性洞察

Conclusion: 该框架解决了个性化医疗中的关键挑战，通过考虑数据异质性和任务相关性的逐样本自适应信息处理，可改善心理健康结果、提高治疗分配精度和临床成本效益

Abstract: We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.

</details>


### [9] [Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study](https://arxiv.org/abs/2509.12229)
*MSR Avinash*

Main category: cs.LG

TL;DR: 在8GB显存的消费级GPU上对Qwen2.5-1.5B模型进行LoRA/QLoRA微调的系统性能分析，显示分页优化器可提升25%吞吐量，bf16效率低于fp16，2048序列长度可行


<details>
  <summary>Details</summary>
Motivation: 当前在消费级GPU（特别是8GB显存限制下）进行参数高效微调（如LoRA/QLoRA）的效率研究不足，需要为资源受限的研究者和实践者提供可复现的基准和实用指南

Method: 使用单块NVIDIA RTX 4060 GPU，在Qwen2.5-1.5B-Instruct模型上进行控制性性能分析，系统变化批次大小、序列长度、优化器选择（AdamW vs PagedAdamW）和精度（fp16 vs bf16）

Result: 分页优化器相比基线提升25%吞吐量（628 tok/s vs 500 tok/s），bf16效率低于fp16，在8GB限制下使用参数高效策略可实现最长2048 tokens的序列长度

Conclusion: 这是首个在消费级GPU上系统研究LLM微调效率的案例研究，为资源受限环境提供了可复现的基准和实用指导，证明在有限硬件条件下仍可有效进行模型微调

Abstract: Fine-tuning large language models (LLMs) with parameter-efficient techniques
such as LoRA and QLoRA has enabled adaptation of foundation models on modest
hardware. Yet the efficiency of such training on consumer-grade GPUs,
especially under strict 8 GB VRAM limits, remains underexplored. We present a
controlled profiling study of LoRA/QLoRA fine-tuning using the
Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three
representative configurations, we systematically vary batch size, sequence
length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16).
We report throughput (tokens/s), time per 10k tokens, and VRAM footprint,
alongside energy estimates derived from GPU board power limits. Our results
show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500
tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB
constraints, sequence lengths up to 2048 tokens were feasible using
parameter-efficient strategies. To our knowledge, this is the first systematic
case study of LLM fine-tuning efficiency on consumer GPUs, providing
reproducible benchmarks and practical guidelines for resource-constrained
researchers and practitioners.

</details>


### [10] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: PerM-MoE是一种新型稀疏专家混合方法，通过为每种模态使用独立路由器，在阿尔茨海默病多模态神经影像数据缺失情况下显著提升认知衰退预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在临床环境中经常面临多种模态数据缺失的问题，导致预测准确性下降，需要提高模型在高度模态缺失情况下的灵活性。

Method: 提出PerM-MoE方法，使用独立路由器处理每种模态，替代传统的单一路由器，利用T1加权MRI、FLAIR、淀粉样蛋白PET和tau PET等多模态神经影像数据。

Result: 在阿尔茨海默病神经影像倡议(ADNI)数据上，PerM-MoE在大多数模态缺失变化情况下优于最先进的Flex-MoE方法，并展示了更有效的专家利用效率。

Conclusion: PerM-MoE方法通过独立模态路由器设计，显著提升了多模态模型在临床实际数据缺失场景下的预测性能和实用性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [11] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究发现SFT-RL两阶段微调中，SFT早期OOD性能最佳但随后下降，RL主要起OOD恢复作用而非创造新能力，奇异向量旋转是性能变化的关键机制


<details>
  <summary>Details</summary>
Motivation: 探索SFT和RL两阶段微调范式中性能演变的机制，挑战"SFT记忆、RL泛化"的简化观点

Method: 使用SVD分析参数矩阵，手动编辑参数并观察性能影响，分析不同训练阶段的OOD性能变化

Result: 发现OOD性能在SFT早期达到峰值后下降，RL主要恢复SFT丢失的OOD能力而非创造新能力，奇异向量旋转而非奇异值变化是主要机制

Conclusion: 重新定义了SFT和RL在两阶段微调中的角色，发现奇异向量旋转是性能变化的关键机制，为优化训练策略提供新见解

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [12] [Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction](https://arxiv.org/abs/2509.12237)
*Changqing Liu,Kaining Dai,Zhiwei Zhao,Tianyi Wu,Yingguang Li*

Main category: cs.LG

TL;DR: 提出了一种基于微分同胚嵌入神经算子的新框架NDNO，用于高效预测不同几何形状结构件的加工变形，解决了传统数值方法计算成本高和神经算子直接应用于变化几何域的限制问题。


<details>
  <summary>Details</summary>
Motivation: 结构件加工变形预测对保证尺寸精度和可靠性至关重要，但传统数值方法计算成本高，特别是处理不同几何形状时。神经算子虽能高效求解偏微分方程，但直接应用于变化几何域存在理论和实践限制。

Method: 通过受平滑性和可逆性约束的微分同胚神经网络，将复杂三维几何显式映射到公共参考域，然后在参考域上训练神经算子学习残余应力引起的变形场。

Result: 该方法能准确预测主方向和多方向变形场，在不同几何形状（包括组件类型、尺寸和特征）的零件上实现了高精度和高效率。

Conclusion: NDNO框架为变化几何形状结构件的变形预测提供了有效且计算高效的解决方案，训练后的微分同胚神经网络和神经算子都展现出高效的预测能力，能快速适应不同几何形状。

Abstract: Accurate prediction of machining deformation in structural components is
essential for ensuring dimensional precision and reliability. Such deformation
often originates from residual stress fields, whose distribution and influence
vary significantly with geometric complexity. Conventional numerical methods
for modeling the coupling between residual stresses and deformation are
computationally expensive, particularly when diverse geometries are considered.
Neural operators have recently emerged as a powerful paradigm for efficiently
solving partial differential equations, offering notable advantages in
accelerating residual stress-deformation analysis. However, their direct
application across changing geometric domains faces theoretical and practical
limitations. To address this challenge, a novel framework based on
diffeomorphic embedding neural operators named neural diffeomorphic-neural
operator (NDNO) is introduced. Complex three-dimensional geometries are
explicitly mapped to a common reference domain through a diffeomorphic neural
network constrained by smoothness and invertibility. The neural operator is
then trained on this reference domain, enabling efficient learning of
deformation fields induced by residual stresses. Once trained, both the
diffeomorphic neural network and the neural operator demonstrate efficient
prediction capabilities, allowing rapid adaptation to varying geometries. The
proposed method thus provides an effective and computationally efficient
solution for deformation prediction in structural components subject to varying
geometries. The proposed method is validated to predict both main-direction and
multi-direction deformation fields, achieving high accuracy and efficiency
across parts with diverse geometries including component types, dimensions and
features.

</details>


### [13] [Interpretable Data Mining of Follicular Thyroid Cancer Ultrasound Features Using Enhanced Association Rules](https://arxiv.org/abs/2509.12238)
*Songlin Zhou,Tao Zhou,Xin Li,Stephen Shing-Toung Yau*

Main category: cs.LG

TL;DR: 本研究通过改进关联规则挖掘方法，结合SHAP可解释机器学习思想，分析了滤泡性甲状腺癌的临床数据，发现了多个与恶性相关的临床指标，为术前诊断提供参考。


<details>
  <summary>Details</summary>
Motivation: 滤泡性甲状腺癌缺乏特征性超声征象，术前诊断比乳头状甲状腺癌更困难，相关临床研究较少。研究旨在通过数据挖掘方法识别有助于术前诊断的临床指标。

Method: 基于2010-2023年北京大学第三医院收集的病例数据进行回顾性分析，改进传统关联规则挖掘方法，结合SHAP可解释机器学习思想提出新的分析指标。

Result: 数据集包含1673个结节（1414个良性，259个恶性），发现除了常见指标外，结节内结节模式、小梁模式、低TSH评分等指标具有强恶性关联，Hashimoto甲状腺炎组合也有强恶性关联。

Conclusion: 在疑似滤泡性甲状腺癌结节的术前诊断中，应考虑多个临床指标以提高诊断准确性。研究发现的多样化恶性关联可为临床医生提供参考。

Abstract: Purpose: Thyroid cancer has been a common cancer. Papillary thyroid cancer
and follicular thyroid cancer are the two most common types of thyroid cancer.
Follicular thyroid cancer lacks distinctive ultrasound signs and is more
difficult to diagnose preoperatively than the more prevalent papillary thyroid
cancer, and the clinical studies associated with it are less well established.
We aimed to analyze the clinical data of follicular thyroid cancer based on a
novel data mining tool to identify some clinical indications that may help in
preoperative diagnosis. Methods: We performed a retrospective analysis based on
case data collected by the Department of General Surgery of Peking University
Third Hospital between 2010 and 2023. Unlike traditional statistical methods,
we improved the association rule mining, a classical data mining method, and
proposed new analytical metrics reflecting the malignant association between
clinical indications and cancer with the help of the idea of SHAP method in
interpretable machine learning. Results: The dataset was preprocessed to
contain 1673 cases (in terms of nodes rather than patients), of which 1414 were
benign and 259 were malignant nodes. Our analysis pointed out that in addition
to some common indicators (e.g., irregular or lobulated nodal margins, uneven
thickness halo, hypoechogenicity), there were also some indicators with strong
malignant associations, such as nodule-in-nodule pattern, trabecular pattern,
and low TSH scores. In addition, our results suggest that the combination of
Hashimoto's thyroiditis may also have a strong malignant association.
Conclusion: In the preoperative diagnosis of nodules suspected of follicular
thyroid cancer, multiple clinical indications should be considered for a more
accurate diagnosis. The diverse malignant associations identified in our study
may serve as a reference for clinicians in related fields.

</details>


### [14] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: InJecteD是一个用于解释DDPM去噪过程的框架，通过分析2D点云生成过程中的样本轨迹来增强模型透明度，支持人类-AI协作调试生成模型。


<details>
  <summary>Details</summary>
Motivation: 提高去噪扩散概率模型的可解释性，通过分析去噪过程中的轨迹特性来增强模型透明度，支持实践者调试和优化生成模型。

Method: 使用简化的DDPM架构，分析三个数据集（靶心、恐龙、圆形）的去噪轨迹，量化位移、速度、聚类和漂移场动态等特性，采用Wasserstein距离和余弦相似度等统计指标。

Result: 实验揭示了不同的去噪阶段：初始噪声探索、快速形状形成和最终细化，不同数据集表现出特定行为（如靶心的同心收敛vs恐龙的复杂轮廓形成），傅里叶基嵌入提高了轨迹稳定性和重建质量。

Conclusion: InJecteD框架成功提供了对DDPM去噪过程的可解释性分析，傅里叶嵌入方法在改善轨迹稳定性和生成质量方面表现优异，为生成模型的调试和优化提供了有效工具。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising
Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during
the denoising process of 2D point cloud generation. We apply this framework to
three datasets from the Datasaurus Dozen bullseye, dino, and circle using a
simplified DDPM architecture with customizable input and time embeddings. Our
approach quantifies trajectory properties, including displacement, velocity,
clustering, and drift field dynamics, using statistical metrics such as
Wasserstein distance and cosine similarity. By enhancing model transparency,
InJecteD supports human AI collaboration by enabling practitioners to debug and
refine generative models. Experiments reveal distinct denoising phases: initial
noise exploration, rapid shape formation, and final refinement, with
dataset-specific behaviors example, bullseyes concentric convergence vs. dinos
complex contour formation. We evaluate four model configurations, varying
embeddings and noise schedules, demonstrating that Fourier based embeddings
improve trajectory stability and reconstruction quality

</details>


### [15] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文从理论上证明了JEPA架构在确定性MDP中不会出现表示塌陷，并通过辅助回归头确保非等价观测映射到不同的潜在表示。


<details>
  <summary>Details</summary>
Motivation: Joint-Embedding Predictive Architecture (JEPA) 在视觉表示学习和基于模型的强化学习中应用日益广泛，但其行为机制仍缺乏理论理解，需要系统性的理论分析。

Method: 提出一个简单的JEPA变体，包含与潜在动力学联合训练的辅助回归头。在确定性MDP框架下进行理论分析，证明训练损失趋近于零时的表示特性。

Result: 证明了"无病态表示塌陷定理"：在确定性MDP中，当潜在转移一致性损失和辅助回归损失都趋近于零时，任何具有不同转移动力学或辅助标签的非等价观测都必须映射到不同的潜在表示。

Conclusion: 辅助任务锚定了表示必须保留的区分信息，联合训练JEPA模型与辅助头比单独训练能产生更丰富的表示，为改进JEPA编码器提供了理论指导。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [16] [Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE](https://arxiv.org/abs/2509.12255)
*Mihir Tare,Clemens Rattasits,Yiming Wu,Euan Wielewski*

Main category: cs.LG

TL;DR: 本文展示了GraphSAGE图神经网络在银行交易网络中的实际应用，能够处理动态异构交易数据，生成可解释的节点嵌入，并在洗钱检测等下游任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统图嵌入方法难以处理银行动态复杂的交易网络数据，需要可扩展且能泛化到未见节点的解决方案。

Method: 使用GraphSAGE归纳式图神经网络框架，构建匿名客户和商户交易网络，训练模型生成节点嵌入。

Result: 嵌入显示出与地理和人口属性对齐的可解释聚类，在洗钱检测模型中提高了高风险账户的优先级排序效果。

Conclusion: GraphSAGE框架具有归纳能力、可扩展性和可解释性，为金融机构利用图机器学习在交易生态系统中获得可操作洞察提供了蓝图。

Abstract: Financial institutions increasingly require scalable tools to analyse complex
transactional networks, yet traditional graph embedding methods struggle with
dynamic, real-world banking data. This paper demonstrates the practical
application of GraphSAGE, an inductive Graph Neural Network framework, to
non-bipartite heterogeneous transaction networks within a banking context.
Unlike transductive approaches, GraphSAGE scales well to large networks and can
generalise to unseen nodes which is critical for institutions working with
temporally evolving transactional data. We construct a transaction network
using anonymised customer and merchant transactions and train a GraphSAGE model
to generate node embeddings. Our exploratory work on the embeddings reveals
interpretable clusters aligned with geographic and demographic attributes.
Additionally, we illustrate their utility in downstream classification tasks by
applying them to a money mule detection model where using these embeddings
improves the prioritisation of high-risk accounts. Beyond fraud detection, our
work highlights the adaptability of this framework to banking-scale networks,
emphasising its inductive capability, scalability, and interpretability. This
study provides a blueprint for financial organisations to harness graph machine
learning for actionable insights in transactional ecosystems.

</details>


### [17] [Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction](https://arxiv.org/abs/2509.12259)
*Kenneth G. Young II*

Main category: cs.LG

TL;DR: 量子启发的堆叠集成概念图模型(QISICGM)利用量子启发技术，在PIMA印第安人糖尿病数据集上实现了高精度的糖尿病风险预测，F1分数0.8933，AUC 0.8699，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病风险预测中的类别不平衡问题，并通过量子启发技术提升模型性能和效率，为AI辅助临床分诊提供可信赖的解决方案。

Method: 使用PIMA数据集并添加2000个合成样本缓解类别不平衡，构建自改进概念图与堆叠集成模型(RF、ET、transformer、CNN、FFNN)，采用量子启发的相位特征映射和邻域序列建模技术。

Result: 在2768个样本(1949个阳性)上获得OOF F1分数0.8933和AUC 0.8699，CPU推理效率达8.5行/秒，性能超越传统方法。

Conclusion: QISICGM为糖尿病AI辅助临床分诊提供了潜在基准，通过校准、可解释性和开源可复现性强调了可信赖AI的重要性。

Abstract: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an
innovative machine learning framework that harnesses quantum-inspired
techniques to predict diabetes risk with exceptional accuracy and efficiency.
Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic
samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),
QISICGM integrates a self-improving concept graph with a stacked ensemble
comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional
neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach
achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,
outperforming traditional methods. Quantum inspired elements, such as phase
feature mapping and neighborhood sequence modeling, enrich feature
representations, enabling CPU-efficient inference at 8.5 rows per second. This
paper presents a detailed architecture, theoretical foundations, code insights,
and performance evaluations, including visualizations from the outputs
subfolder. The open-source implementation (v1.0.0) is available at
https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential
benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,
this work emphasizes trustworthy AI through calibration, interpretability, and
open-source reproducibility.

</details>


### [18] [Explainable Fraud Detection with GNNExplainer and Shapley Values](https://arxiv.org/abs/2509.12262)
*Ngoc Hieu Dao*

Main category: cs.LG

TL;DR: 本文提出开发可解释的欺诈检测器，以应对数字支付增加带来的金融欺诈风险，满足监管透明度和分析师可理解解释的需求。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付的普及，金融欺诈风险增加。虽然人工智能系统广泛用于欺诈检测，但社会和监管机构要求更高的透明度以验证可靠性，同时欺诈分析师需要简洁易懂的解释来提高调查效率。

Method: 专注于开发可解释的欺诈检测器（具体方法未在摘要中详细说明）。

Result: 未在摘要中明确说明具体结果。

Conclusion: 需要开发可解释的欺诈检测器来解决当前欺诈检测系统在透明度和可解释性方面的挑战。

Abstract: The risk of financial fraud is increasing as digital payments are used more
and more frequently. Although the use of artificial intelligence systems for
fraud detection is widespread, society and regulators have raised the standards
for these systems' transparency for reliability verification purposes. To
increase their effectiveness in conducting fraud investigations, fraud analysts
also profit from having concise and understandable explanations. To solve these
challenges, the paper will concentrate on developing an explainable fraud
detector.

</details>


### [19] [Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning](https://arxiv.org/abs/2509.12269)
*Jinmeiyang Wang,Jing Dong,Li Zhou*

Main category: cs.LG

TL;DR: MT-DQN模型整合Transformer、时序图神经网络和DQN，在短视频推荐中显著优于传统方法，F1-score提升10.97%，NDCG@5提升8.3%，但存在计算成本和延迟敏感性问题


<details>
  <summary>Details</summary>
Motivation: 解决短视频环境中用户行为预测和推荐策略优化的挑战，传统串联模型和经典强化学习方法效果有限

Method: 提出MT-DQN模型，整合Transformer、时序图神经网络(TGNN)和深度Q网络(DQN)三种技术

Result: 相比Concat-Modal模型平均F1-score提升10.97%，NDCG@5提升8.3%；相比Vanilla-DQN模型MSE降低34.8%，MAE降低26.5%

Conclusion: MT-DQN在推荐性能上表现优异，但实际部署面临计算成本和在线推理延迟敏感性的挑战，需要通过未来架构优化来解决

Abstract: This paper proposes the MT-DQN model, which integrates a Transformer,
Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the
challenges of predicting user behavior and optimizing recommendation strategies
in short-video environments. Experiments demonstrated that MT-DQN consistently
outperforms traditional concatenated models, such as Concat-Modal, achieving an
average F1-score improvement of 10.97% and an average NDCG@5 improvement of
8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN
reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize
challenges in deploying MT-DQN in real-world scenarios, such as its
computational cost and latency sensitivity during online inference, which will
be addressed through future architectural optimization.

</details>


### [20] [Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach](https://arxiv.org/abs/2509.12285)
*Jiyong Ma*

Main category: cs.LG

TL;DR: 提出基于最大似然估计的方法来确定Transformer模型中的value向量，将value、key和query向量序列建模为高斯分布序列，为scaled-dot-product和softmax函数提供新的概率解释。


<details>
  <summary>Details</summary>
Motivation: 为Transformer架构中的scaled-dot-product函数和softmax函数提供新的概率理论基础和解释，从最大似然估计的角度理解attention机制。

Method: 将value向量序列、key向量序列和query向量建模为高斯分布序列，其中每个高斯分布的方差依赖于时间步、对应key向量和query向量，均值依赖于时间步和对应value向量。

Result: 提出了一个基于最大似然估计的框架，能够为Transformer中的attention机制提供概率解释，并与最大熵方法形成对比。

Conclusion: 该方法为Transformer模型的核心组件提供了新的概率视角，可能有助于更好地理解和改进attention机制。

Abstract: In this paper, we present a maximum likelihood estimation approach to
determine the value vector in transformer models. We model the sequence of
value vectors, key vectors, and the query vector as a sequence of Gaussian
distributions. The variance in each Gaussian distribution depends on the time
step, the corresponding key vector, and the query vector. The mean value in
each Gaussian distribution depends on the time step, and the corresponding
value vector. This analysis may offer a new explanation of the
scaled-dot-product function or softmax function used in transformer
architectures [1]. Another explanation, inspired by [4], is based on the
maximum entropy approach in natural language processing [5]. In this approach,
a query vector and key vectors are used to derive the feature functions for the
maximum entropy model.

</details>


### [21] [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)
*Wilfrid Sougrinoma Compaoré,Yaya Etiabi,El Mehdi Amhoud,Mohamad Assaad*

Main category: cs.LG

TL;DR: 提出了一种面向物联网的联邦学习框架，通过有限块长传输、模型量化和错误感知聚合机制，显著提升能源效率和通信可靠性，能耗降低75%的同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，存在能源有限、通信不可靠等问题，传统联邦学习假设无限块长传输不切实际，需要设计更高效的解决方案。

Method: 集成有限块长传输、模型量化技术和错误感知聚合机制，优化上行传输功率以平衡能耗和模型性能。

Result: 仿真结果显示，相比标准联邦学习模型，能耗降低高达75%，同时保持鲁棒的模型精度。

Conclusion: 该框架为实际物联网部署中高效可靠的联邦学习实现提供了可行解决方案，推动了联邦学习在资源受限环境中的应用。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling
collaborative machine learning while preserving data privacy, making it
particularly suitable for Internet of Things (IoT) environments. However,
resource-constrained IoT devices face significant challenges due to limited
energy,unreliable communication channels, and the impracticality of assuming
infinite blocklength transmission. This paper proposes a federated learning
framework for IoT networks that integrates finite blocklength transmission,
model quantization, and an error-aware aggregation mechanism to enhance energy
efficiency and communication reliability. The framework also optimizes uplink
transmission power to balance energy savings and model performance. Simulation
results demonstrate that the proposed approach significantly reduces energy
consumption by up to 75\% compared to a standard FL model, while maintaining
robust model accuracy, making it a viable solution for FL in real-world IoT
scenarios with constrained resources. This work paves the way for efficient and
reliable FL implementations in practical IoT deployments. Index Terms:
Federated learning, IoT, finite blocklength, quantization, energy efficiency.

</details>


### [22] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 本研究应用量子生成对抗网络(QGANs)进行股价预测，通过量子计算技术生成合成数据，在收敛速度和预测精度方面优于传统LSTM和GAN模型。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度复杂性和波动性，传统模型难以捕捉其复杂模式。量子计算与生成模型的结合为金融预测提供了新的可能性。

Method: 使用AWS Braket SV1模拟器训练QGAN电路，基于股票指数价格数据实现定制化的QGAN模型进行股价预测。

Result: QGAN能够生成与真实市场行为高度相似的合成数据，在收敛速度和预测准确性方面显著优于经典LSTM和GAN模型。

Conclusion: 量子计算在金融预测领域具有重要应用前景，为交易员、金融分析师和研究人员提供了更先进的市场分析工具，在速度和精度方面具有潜在优势。

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [23] [C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction](https://arxiv.org/abs/2509.12289)
*Yuting Liu,Qiang Zhou,Hanzhe Li,Chenqi Gong,Jingjing Gu*

Main category: cs.LG

TL;DR: 提出C3DE模型，使用神经控制微分方程解决长期城市人流预测中的累积采样误差问题，通过双路径NCDE和因果效应估计器处理POI与人群流的多时间尺度异步动态关系


<details>
  <summary>Details</summary>
Motivation: 长期城市人流预测面临累积采样误差问题，同时POI演化对人流有重要影响，但两者存在多时间尺度异步动态关系和潜在虚假因果关系，传统NCDE方法难以有效处理

Method: 提出C3DE模型：1）双路径NCDE主干网络捕捉多时间尺度协作信号；2）基于反事实的因果效应估计器量化POI对人流的因果影响；3）融合POI和人群流协作信号的预测器进行长期预测

Result: 在三个真实世界数据集上的实验表明C3DE具有优越性能，特别是在人流波动显著的城市中表现突出

Conclusion: C3DE通过结合神经控制微分方程和因果推理，有效解决了长期城市人流预测中的累积误差和多时间尺度异步动态问题，为城市智能交通管理提供了有效工具

Abstract: Long-term urban crowd flow prediction suffers significantly from cumulative
sampling errors, due to increased sequence lengths and sampling intervals,
which inspired us to leverage Neural Controlled Differential Equations (NCDEs)
to mitigate this issue. However, regarding the crucial influence of Points of
Interest (POIs) evolution on long-term crowd flow, the multi-timescale
asynchronous dynamics between crowd flow and POI distribution, coupled with
latent spurious causality, poses challenges to applying NCDEs for long-term
urban crowd flow prediction. To this end, we propose Causal-aware Collaborative
neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,
we introduce a dual-path NCDE as the backbone to effectively capture the
asynchronous evolution of collaborative signals across multiple time scales.
Then, we design a dynamic correction mechanism with the counterfactual-based
causal effect estimator to quantify the causal impact of POIs on crowd flow and
minimize the accumulation of spurious correlations. Finally, we leverage a
predictor for long-term prediction with the fused collaborative signals of POI
and crowd flow. Extensive experiments on three real-world datasets demonstrate
the superior performance of C3DE, particularly in cities with notable flow
fluctuations.

</details>


### [24] [Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs](https://arxiv.org/abs/2509.12326)
*Michael Freedman,Michael Mulligan*

Main category: cs.LG

TL;DR: 研究发现传统单隐藏层神经网络训练过程中会自发产生Kolmogorov-Arnold几何结构，通过分析雅可比矩阵的外幂统计特性来量化这种几何特征。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何有机地学习为下游处理准备输入数据，并研究KA几何结构的出现规律以通过及时调整超参数来加速学习。

Method: 通过分析雅可比矩阵J(x)的外幂统计特性，包括零行数量和次要统计量，来量化KA几何结构。研究在函数复杂度和模型超参数空间中KA几何的出现位置。

Result: 发现传统单隐藏层神经网络训练时经常会产生KA几何结构，能够对这种几何结构的出现条件有初步理解。

Conclusion: 这项研究是KA-Networks（KANs）的"另一面"，不是将KA工程化到神经网络中，而是观察KA在浅层MLP中的自然涌现。

Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but
highly non-smooth inner functions (the first layer map) in a single
(non-linear) hidden layer neural network. Such universal functions have a
distinctive local geometry, a "texture," which can be characterized by the
inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the
data. It is natural to ask if this distinctive KA geometry emerges through
conventional neural network optimization. We find that indeed KA geometry often
is produced when training vanilla single hidden layer neural networks. We
quantify KA geometry through the statistical properties of the exterior powers
of $J(\mathbf{x})$: number of zero rows and various observables for the minor
statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of
$J(\mathbf{x})$. This leads to a rough understanding for where KA geometry
occurs in the space of function complexity and model hyperparameters. The
motivation is first to understand how neural networks organically learn to
prepare input data for later downstream processing and, second, to learn enough
about the emergence of KA geometry to accelerate learning through a timely
intervention in network hyperparameters. This research is the "flip side" of
KA-Networks (KANs). We do not engineer KA into the neural network, but rather
watch KA emerge in shallow MLPs.

</details>


### [25] [Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets](https://arxiv.org/abs/2509.12339)
*Xianchen Liu,Tianhui Zhang,Xinyu Zhang,Lingmin Hou,Zhen Guo,Yuanhao Tian,Yang Liu*

Main category: cs.LG

TL;DR: 结合LSTM和PSO算法优化生鲜超市定价与补货策略，通过注意力机制增强预测可解释性，实现利润最大化和食品浪费减少


<details>
  <summary>Details</summary>
Motivation: 解决生鲜食品零售中动态定价和库存管理的挑战，平衡利润最大化和减少食品浪费的需求，为易腐商品提供可扩展的解决方案

Method: 使用带注意力机制的LSTM网络预测7天内的销量、价格趋势和损耗率，然后将预测结果作为PSO算法的输入来优化定价和补货策略，结合成本加成定价实现动态调整

Result: 该框架不仅实现了利润最大化，还减少了食品浪费，提高了超市运营的可持续性，注意力机制增强了模型的可解释性和决策准确性

Conclusion: 该方法弥合了预测建模和优化之间的差距，为生鲜食品零售和其他处理易腐商品的行业提供了动态定价和库存管理的可扩展解决方案

Abstract: This paper presents a novel approach to optimizing pricing and replenishment
strategies in fresh food supermarkets by combining Long Short-Term Memory
(LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model,
enhanced with an attention mechanism, is used to predict sales volumes, pricing
trends, and spoilage rates over a seven-day period. The predictions generated
by the LSTM model serve as inputs for the PSO algorithm, which iteratively
optimizes pricing and replenishment strategies to maximize profitability while
adhering to inventory constraints. The integration of cost-plus pricing allows
for dynamic adjustments based on fixed and variable costs, ensuring real-time
adaptability to market fluctuations. The framework not only maximizes profits
but also reduces food waste, contributing to more sustainable supermarket
operations. The attention mechanism enhances the interpretability of the LSTM
model by identifying key time points and factors influencing sales, improving
decision-making accuracy. This methodology bridges the gap between predictive
modeling and optimization, offering a scalable solution for dynamic pricing and
inventory management in fresh food retail and other industries dealing with
perishable goods.

</details>


### [26] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 提出了FEDONet，在DeepONet架构中引入傅里叶嵌入主干网络，通过随机傅里叶特征映射增强空间表示能力，显著提升了PDE求解精度。


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet使用全连接线性层的主干网络在捕捉复杂空间结构方面存在局限，需要改进其空间表示能力。

Method: 在DeepONet架构中引入傅里叶嵌入主干网络，利用随机傅里叶特征映射来丰富空间表示能力。

Result: 在多个PDE数据集上表现优于传统DeepONet，相对L2性能平均提升2-3倍，解重构精度显著改善。

Conclusion: 傅里叶嵌入能有效增强神经算子学习，为PDE代理建模提供了鲁棒且广泛适用的方法。

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [27] [Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification](https://arxiv.org/abs/2509.12346)
*Liam Ressel,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 本文研究了在工程师薪资预测挑战中，针对高维词嵌入特征和有限训练样本的分类问题，提出了分区LDA方法，通过将词嵌入分块处理并结合收缩正则化，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 工程师薪资预测任务中，300维词嵌入特征导致维度灾难，且训练样本有限，使得传统线性降维方法在表格数据分类中的表现有待深入研究。

Method: 研究了PCA和LDA两种线性降维方法，提出了Partitioned-LDA方法：将词嵌入分成等大小的块，在每个块上分别进行LDA，减小协方差矩阵规模，并结合收缩正则化技术。

Result: PCA在合适子空间维度下优于原始嵌入；无正则化的LDA性能较差但收缩后显著改善；Partitioned-LDA结合收缩后在竞赛公开排行榜上达到top-10准确率。

Conclusion: 分区LDA方法能有效提升有限训练样本下词嵌入在表格数据分类中的性能，为解决高维小样本分类问题提供了有效方案。

Abstract: The Engineers' Salary Prediction Challenge requires classifying salary
categories into three classes based on tabular data. The job description is
represented as a 300-dimensional word embedding incorporated into the tabular
features, drastically increasing dimensionality. Additionally, the limited
number of training samples makes classification challenging. Linear
dimensionality reduction of word embeddings for tabular data classification
remains underexplored. This paper studies Principal Component Analysis (PCA)
and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate
subspace dimension, can outperform raw embeddings. LDA without regularization
performs poorly due to covariance estimation errors, but applying shrinkage
improves performance significantly, even with only two dimensions. We propose
Partitioned-LDA, which splits embeddings into equal-sized blocks and performs
LDA separately on each, thereby reducing the size of the covariance matrices.
Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves
top-10 accuracy on the competition public leaderboard. This method effectively
enhances word embedding performance in tabular data classification with limited
training samples.

</details>


### [28] [Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields](https://arxiv.org/abs/2509.12358)
*Hong Sun,Joshua A. Vita,Amit Samanta,Vincenzo Lordi*

Main category: cs.LG

TL;DR: 提出了MEAGraph模型，一种无监督方法，用于分析原子数据集并有效去除采样偏差，通过多核变换和注意力机制实现原子环境的有效聚类和剪枝。


<details>
  <summary>Details</summary>
Motivation: 在计算化学和材料科学中，数据集生成技术容易在势能面上过采样特定区域，传统聚类和剪枝方法在高维原子描述符下难以有效识别不同区域，导致信息丢失或偏差去除不彻底。

Method: 开发了Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph)模型，结合多线性核变换和基于注意力的消息传递，无需标签或大量训练即可捕获几何敏感性并进行有效数据集剪枝。

Result: 在铌、钽和铁数据集上的应用表明，MEAGraph能有效分组相似原子环境，使基础剪枝技术能够成功去除采样偏差。

Conclusion: MEAGraph为表示学习和聚类提供了有效方法，可用于数据分析、异常检测和数据集优化，解决了原子数据集中的采样偏差问题。

Abstract: Constructing a chemically diverse dataset while avoiding sampling bias is
critical to training efficient and generalizable force fields. However, in
computational chemistry and materials science, many common dataset generation
techniques are prone to oversampling regions of the potential energy surface.
Furthermore, these regions can be difficult to identify and isolate from each
other or may not align well with human intuition, making it challenging to
systematically remove bias in the dataset. While traditional clustering and
pruning (down-sampling) approaches can be useful for this, they can often lead
to information loss or a failure to properly identify distinct regions of the
potential energy surface due to difficulties associated with the high
dimensionality of atomic descriptors. In this work, we introduce the
Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an
unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple
linear kernel transformations with attention-based message passing to capture
geometric sensitivity and enable effective dataset pruning without relying on
labels or extensive training. Demonstrated applications on niobium, tantalum,
and iron datasets show that MEAGraph efficiently groups similar atomic
environments, allowing for the use of basic pruning techniques for removing
sampling bias. This approach provides an effective method for representation
learning and clustering that can be used for data analysis, outlier detection,
and dataset optimization.

</details>


### [29] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 提出基于联邦学习的智能农业框架，用于明尼苏达州农场的作物病害检测，在保护数据隐私的同时实现高精度分类


<details>
  <summary>Details</summary>
Motivation: 农业领域对数据驱动决策的需求日益增长，但农场对数据隐私的担忧限制了数据共享。需要一种既能利用机器学习技术又能保护敏感农场数据隐私的解决方案

Method: 采用联邦学习框架，包括：从明尼苏达农场收集数据、应用本地深度学习算法、迁移学习技术、中央聚合服务器进行模型精炼

Result: 预期实现：病害检测精度提升、良好的农业场景泛化能力、降低通信和训练时间成本、早期病害识别和干预

Conclusion: 该框架填补了先进机器学习技术与农民实际隐私需求之间的空白，利用联邦学习优势为智能农业系统提供安全高效的病害检测方法

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [30] [Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder](https://arxiv.org/abs/2509.12372)
*Konstantinos Vasili,Zachery T. Dahm,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出基于LSTM自编码器和双重注意力机制的无监督方法，用于核反应堆辐射监测系统的异常事件检测、定位和特征分析


<details>
  <summary>Details</summary>
Motivation: 下一代核反应堆需要远程自主控制系统，但现有ML/DL方法缺乏可解释性、真实数据访问和异常事件稀缺等问题，阻碍了在安全关键领域的应用

Method: 使用LSTM自编码器结合特征注意力和时间注意力机制，特征注意力识别异常辐射传感器，时间注意力定位异常发生时间点

Result: 在PUR-1研究反应堆的真实数据集上进行评估，能够同时识别受影响传感器和异常持续时间

Conclusion: 该框架为核反应堆异常监测提供了可解释的解决方案，通过双重注意力机制实现了异常检测和定位的统一网络

Abstract: The nuclear industry is advancing toward more new reactor designs, with
next-generation reactors expected to be smaller in scale and power output.
These systems have the potential to produce large volumes of information in the
form of multivariate time-series data, which could be used for enhanced
real-time monitoring and control. In this context, the development of remote
autonomous or semi-autonomous control systems for reactor operation has gained
significant interest. A critical first step toward such systems is an accurate
diagnostics module capable of detecting and localizing anomalies within the
reactor system. Recent studies have proposed various ML and DL approaches for
anomaly detection in the nuclear domain. Despite promising results, key
challenges remain, including limited to no explainability, lack of access to
real-world data, and scarcity of abnormal events, which impedes benchmarking
and characterization. Most existing studies treat these methods as black boxes,
while recent work highlights the need for greater interpretability of ML/DL
outputs in safety-critical domains. Here, we propose an unsupervised
methodology based on an LSTM autoencoder with a dual attention mechanism for
characterization of abnormal events in a real-world reactor radiation area
monitoring system. The framework includes not only detection but also
localization of the event and was evaluated using real-world datasets of
increasing complexity from the PUR-1 research reactor. The attention mechanisms
operate in both the feature and temporal dimensions, where the feature
attention assigns weights to radiation sensors exhibiting abnormal patterns,
while time attention highlights the specific timesteps where irregularities
occur, thus enabling localization. By combining the results, the framework can
identify both the affected sensors and the duration of each anomaly within a
single unified network.

</details>


### [31] [Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data](https://arxiv.org/abs/2509.12375)
*Julian Ripper,Ousama Esbel,Rafael Fietzek,Max Mühlhäuser,Thomas Kreutz*

Main category: cs.LG

TL;DR: 本文提出了一种混合生成方法，结合自回归和非自回归技术，使用去噪扩散概率模型(DDPM)生成汽车CAN总线时间序列数据，既能生成高质量合成数据，又能修复损坏样本。


<details>
  <summary>Details</summary>
Motivation: 在小规模且包含损坏样本的时间序列数据集上训练深度学习模型具有挑战性，需要生成真实合成数据并修复损坏样本的方法。

Method: 提出混合生成方法，结合自回归和非自回归技术，采用DDPM架构并进行了多项改进，使用三个量化物理正确性和测试轨道遵循度的指标进行评估。

Result: 最佳模型在物理正确性方面甚至优于训练数据，显示出合理的驾驶行为，并成功修复了训练数据中物理上不合理的区域。

Conclusion: DDPM能有效解决汽车时间序列数据生成和修复任务，提出的方法在有限样本情况下表现出色，提高了数据质量。

Abstract: Training deep learning methods on small time series datasets that also
include corrupted samples is challenging. Diffusion models have shown to be
effective to generate realistic and synthetic data, and correct corrupted
samples through imputation. In this context, this paper focuses on generating
synthetic yet realistic samples of automotive time series data. We show that
denoising diffusion probabilistic models (DDPMs) can effectively solve this
task by applying them to a challenging vehicle CAN-dataset with long-term data
and a limited number of samples. Therefore, we propose a hybrid generative
approach that combines autoregressive and non-autoregressive techniques. We
evaluate our approach with two recently proposed DDPM architectures for time
series generation, for which we propose several improvements. To evaluate the
generated samples, we propose three metrics that quantify physical correctness
and test track adherence. Our best model is able to outperform even the
training data in terms of physical correctness, while showing plausible driving
behavior. Finally, we use our best model to successfully impute physically
implausible regions in the training data, thereby improving the data quality.

</details>


### [32] [Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](https://arxiv.org/abs/2509.12387)
*Mohamed Zayaan S*

Main category: cs.LG

TL;DR: CSML框架通过元学习因果结构，实现从少量样本中快速适应新任务，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型依赖虚假相关性、泛化能力差、需要大量数据的问题，通过因果机制实现人类级别的鲁棒、样本高效学习

Method: 包含三个核心模块：感知模块（输入到解耦符号表示）、可微分因果归纳模块（发现因果图）、基于图的推理模块（利用因果图进行预测），通过元学习跨任务共享的因果世界模型

Result: 在CausalWorld新基准测试中，CSML显著优于最先进的元学习和神经符号基线方法，特别是在需要真正因果推理的任务上

Conclusion: 因果符号元学习框架通过发现和利用潜在因果结构，实现了样本高效的学习和强大的泛化能力，是迈向人类级别智能的重要一步

Abstract: Modern deep learning models excel at pattern recognition but remain
fundamentally limited by their reliance on spurious correlations, leading to
poor generalization and a demand for massive datasets. We argue that a key
ingredient for human-like intelligence-robust, sample-efficient learning-stems
from an understanding of causal mechanisms. In this work, we introduce
Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer
the latent causal structure of a task distribution. CSML comprises three key
modules: a perception module that maps raw inputs to disentangled symbolic
representations; a differentiable causal induction module that discovers the
underlying causal graph governing these symbols and a graph-based reasoning
module that leverages this graph to make predictions. By meta-learning a shared
causal world model across a distribution of tasks, CSML can rapidly adapt to
novel tasks, including those requiring reasoning about interventions and
counterfactuals, from only a handful of examples. We introduce CausalWorld, a
new physics-based benchmark designed to test these capabilities. Our
experiments show that CSML dramatically outperforms state-of-the-art
meta-learning and neuro-symbolic baselines, particularly on tasks demanding
true causal inference.

</details>


### [33] [Evaluating the printability of stl files with ML](https://arxiv.org/abs/2509.12392)
*Janik Henn,Adrian Hauptmannl,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 开发AI模型检测3D模型中的常见打印问题，帮助新手用户在打印前识别可能导致打印失败的几何特征


<details>
  <summary>Details</summary>
Motivation: 3D打印技术正从专业人士向普通用户普及，但现有切片软件的检查功能有限，新手用户缺乏经验难以识别模型中的潜在打印问题

Method: 训练人工智能模型来分析和检测3D模型中可能导致打印失败的几何特征，在打印开始前提供预警

Result: 提出了一种新的支持层，通过AI技术增强3D打印前的模型检查能力

Conclusion: 该方法能够有效帮助经验不足的用户避免打印失败，提高3D打印的成功率和用户体验

Abstract: 3D printing has long been a technology for industry professionals and
enthusiasts willing to tinker or even build their own machines. This stands in
stark contrast to today's market, where recent developments have prioritized
ease of use to attract a broader audience. Slicing software nowadays has a few
ways to sanity check the input file as well as the output gcode. Our approach
introduces a novel layer of support by training an AI model to detect common
issues in 3D models. The goal is to assist less experienced users by
identifying features that are likely to cause print failures due to difficult
to print geometries before printing even begins.

</details>


### [34] [Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation](https://arxiv.org/abs/2509.12394)
*Qingchun Gong,Robert Bogdan Staszewski,Kai Xu*

Main category: cs.LG

TL;DR: 提出了ASGE（自适应空间goodness编码）框架，用于无反向传播的CNN训练，解决了通道维度爆炸问题，在多个数据集上达到最佳FF方法性能，并首次成功应用于ImageNet。


<details>
  <summary>Details</summary>
Motivation: 现有的Forward-Forward算法扩展方法在处理CNN时存在表示能力有限和可扩展性差的问题，主要原因是通道维度爆炸。需要开发新的训练框架来解决这些问题。

Method: ASGE利用特征图计算每层的空间感知goodness表示，实现逐层监督，关键是将分类复杂度与通道维度解耦，从而避免通道爆炸问题。

Result: 在多个基准测试中优于所有其他FF方法：MNIST 99.65%、FashionMNIST 93.41%、CIFAR-10 90.62%、CIFAR-100 65.42%。首次在ImageNet上实现FF训练，Top-1准确率26.21%，Top-5准确率47.49%。

Conclusion: ASGE框架完全消除了反向传播，显著缩小了与BP训练模型的性能差距，为可扩展的无反向传播CNN训练奠定了可行基础。

Abstract: The Forward-Forward (FF) algorithm offers a promising alternative to
backpropagation (BP). Despite advancements in recent FF-based extensions, which
have enhanced the original algorithm and adapted it to convolutional neural
networks (CNNs), they often suffer from limited representational capacity and
poor scalability to large-scale datasets, primarily due to exploding channel
dimensionality. In this work, we propose adaptive spatial goodness encoding
(ASGE), a new FF-based training framework tailored for CNNs. ASGE leverages
feature maps to compute spatially-aware goodness representations at each layer,
enabling layer-wise supervision. Crucially, this approach decouples
classification complexity from channel dimensionality, thereby addressing the
issue of channel explosion and achieving competitive performance compared to
other BP-free methods. ASGE outperforms all other FF-based approaches across
multiple benchmarks, delivering test accuracies of 99.65% on MNIST, 93.41% on
FashionMNIST, 90.62% on CIFAR-10, and 65.42% on CIFAR-100. Moreover, we present
the first successful application of FF-based training to ImageNet, with Top-1
and Top-5 accuracies of 26.21% and 47.49%. By entirely eliminating BP and
significantly narrowing the performance gap with BP-trained models, the ASGE
framework establishes a viable foundation toward scalable BP-free CNN training.

</details>


### [35] [Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning](https://arxiv.org/abs/2509.12406)
*Mohammad Nooraiepour*

Main category: cs.LG

TL;DR: 提出了贝叶斯参数化矩阵模型（B-PMMs），为科学机器学习中的谱方法提供不确定性量化，解决了传统确定性方法在安全关键应用中缺乏置信度估计的问题。


<details>
  <summary>Details</summary>
Motivation: 当前谱学习方法仅提供点估计而缺乏不确定性量化，限制了其在需要预测置信度的安全关键应用中的使用。参数化矩阵模型虽然性能优异，但其确定性特性阻碍了在不确定性量化应用中的部署。

Method: 开发了贝叶斯参数化矩阵模型框架，包含：(i) 具有正则化矩阵扰动界的自适应谱分解，(ii) 使用流形感知矩阵变量高斯后验的结构化变分推理算法，(iii) 具有显式谱间隙和问题条件依赖的有限样本校准保证。

Result: 在5x5到500x500矩阵维度上的实验验证显示完美收敛率，B-PMMs实现了优异的不确定性校准（ECE < 0.05），同时保持良好的可扩展性。即使在谱条件不良和近退化情况下也能提供可靠的不确定性估计。

Conclusion: 该框架支持在不确定性关键领域的鲁棒谱学习，为更广泛的贝叶斯谱机器学习奠定了基础，解决了标准贝叶斯方法在矩阵特征值问题中由于谱分解几何约束而失效的根本挑战。

Abstract: Scientific machine learning increasingly uses spectral methods to understand
physical systems. Current spectral learning approaches provide only point
estimates without uncertainty quantification, limiting their use in
safety-critical applications where prediction confidence is essential.
Parametric matrix models have emerged as powerful tools for scientific machine
learning, achieving exceptional performance by learning governing equations.
However, their deterministic nature limits deployment in uncertainty
quantification applications. We introduce Bayesian parametric matrix models
(B-PMMs), a principled framework that extends PMMs to provide uncertainty
estimates while preserving their spectral structure and computational
efficiency. B-PMM addresses the fundamental challenge of quantifying
uncertainty in matrix eigenvalue problems where standard Bayesian methods fail
due to the geometric constraints of spectral decomposition. The theoretical
contributions include: (i) adaptive spectral decomposition with regularized
matrix perturbation bounds that characterize eigenvalue uncertainty
propagation, (ii) structured variational inference algorithms using
manifold-aware matrix-variate Gaussian posteriors that respect Hermitian
constraints, and (iii) finite-sample calibration guarantees with explicit
dependence on spectral gaps and problem conditioning. Experimental validation
across matrix dimensions from 5x5 to 500x500 with perfect convergence rates
demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <
0.05) while maintaining favorable scaling. The framework exhibits graceful
degradation under spectral ill-conditioning and provides reliable uncertainty
estimates even in near-degenerate regimes. The proposed framework supports
robust spectral learning in uncertainty-critical domains and lays the
groundwork for broader Bayesian spectral machine learning.

</details>


### [36] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: SRI是一种新的统计推断方法，通过构建无结构化数据的低维表征来校正机器学习标注中的偏差，显著降低标准误，并能处理人工标注中的非差分测量误差。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习标注方法存在标准误较大且需要无误差人工标注的问题，需要一种能够有效校正偏差并提供可靠统计推断的新方法。

Method: 提出代理表征推断(SRI)方法，假设无结构化数据完全中介人工标注与结构化变量之间的关系，通过神经网络学习满足代理假设的低维表征，支持半参数有效估计。

Result: 模拟研究和实际应用显示，SRI在机器学习预测精度中等时能将标准误降低50%以上，即使在人工标注存在非差分测量误差时也能提供有效推断。

Conclusion: SRI方法通过利用无结构化数据的低维表征，显著改善了机器学习标注的统计推断质量，为处理标注偏差和测量误差提供了有效的解决方案。

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [37] [On the Regularity and Fairness of Combinatorial Multi-Armed Bandit](https://arxiv.org/abs/2509.12457)
*Xiaoyi Wu,Bin Li*

Main category: cs.LG

TL;DR: 提出了一种参数化的正则公平学习算法，用于组合多臂老虎机问题，同时优化累积奖励、保证公平性和奖励规律性。


<details>
  <summary>Details</summary>
Motivation: 受无线网络应用启发，需要同时最大化累积奖励、保证各臂的公平性（最小平均奖励要求）和确保奖励规律性（奖励频率）。

Method: 算法线性组合虚拟队列长度（跟踪公平性违反）、时间-自-上次-奖励（TSLR）指标和上置信界（UCB）估计，利用Lyapunov函数进行理论分析。

Result: 理论分析表明算法实现了零累积公平性违反、良好的奖励规律性和累积遗憾性能，并通过两个真实数据集仿真验证。

Conclusion: 所提算法能有效平衡探索-利用权衡，同时满足公平性和规律性约束，适用于需要多目标优化的实际应用场景。

Abstract: The combinatorial multi-armed bandit model is designed to maximize cumulative
rewards in the presence of uncertainty by activating a subset of arms in each
round. This paper is inspired by two critical applications in wireless
networks, where it's not only essential to maximize cumulative rewards but also
to guarantee fairness among arms (i.e., the minimum average reward required by
each arm) and ensure reward regularity (i.e., how often each arm receives the
reward). In this paper, we propose a parameterized regular and fair learning
algorithm to achieve these three objectives. In particular, the proposed
algorithm linearly combines virtual queue-lengths (tracking the fairness
violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound
(UCB) estimates in its weight measure. Here, TSLR is similar to
age-of-information and measures the elapsed number of rounds since the last
time an arm received a reward, capturing the reward regularity performance, and
UCB estimates are utilized to balance the tradeoff between exploration and
exploitation in online learning. By exploring a key relationship between
virtual queue-lengths and TSLR metrics and utilizing several non-trivial
Lyapunov functions, we analytically characterize zero cumulative fairness
violation, reward regularity, and cumulative regret performance under our
proposed algorithm. These theoretical outcomes are verified by simulations
based on two real-world datasets.

</details>


### [38] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: 提出了非局部神经正切核(NNTK)框架，用参数空间中的非局部交互近似替代局部梯度，扩展了NTK理论到非光滑函数、随机估计器和更广泛的模型类别


<details>
  <summary>Details</summary>
Motivation: 传统NTK框架依赖于网络对参数可微的假设，这在考虑非光滑目标函数或表现出不可微行为的参数化模型时会失效，需要扩展理论适用范围

Method: 提出非局部神经正切核(NNTK)，用非局部梯度替代标准梯度，探索固定核和基于注意力的非局部算子公式化

Result: 新框架允许NTK理论扩展到非光滑函数、随机估计器和更广泛的模型家族，并通过数值研究进行了验证

Conclusion: NNTK框架成功扩展了NTK理论的应用范围，为分析非光滑系统和更广泛类型的神经网络提供了理论基础

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


### [39] [Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.12483)
*Oscar Rincón-Cardeno,Gregorio Pérez Bernal,Silvana Montoya Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本研究比较了边界元法(BEM)和物理信息神经网络(PINNs)在求解二维Helmholtz方程波散射问题中的性能，发现PINNs训练时间比BEM长42倍，但评估速度快204倍，且泛化能力较差


<details>
  <summary>Details</summary>
Motivation: 评估BEM和PINNs在相同条件下求解Helmholtz方程波散射问题的性能表现，为波动传播问题的研究方法选择提供定量依据

Method: 使用BEM进行边界离散化求解，同时通过超参数优化配置PINNs并最小化控制方程和边界条件的残差进行训练，比较两种方法的精度、计算时间和泛化能力

Result: 在相当精度下，PINNs训练时间比BEM长约42倍，但评估时间快204倍；PINNs在训练域外泛化时相对误差从7.46×10⁻²增加到8.22，而BEM在扩展区域保持相似误差水平

Conclusion: PINNs在评估阶段具有显著速度优势，但训练耗时且泛化能力有限；BEM在计算效率和泛化性方面表现更稳定，为波动传播问题的方法选择提供了重要参考

Abstract: Purpose - This study compares the Boundary Element Method (BEM) and
Physics-Informed Neural Networks (PINNs) for solving the two-dimensional
Helmholtz equation in wave scattering problems. The objective is to evaluate
the performance of both methods under the same conditions.
  Design/methodology/approach - We solve the Helmholtz equation using BEM and
PINNs for the same scattering problem. The PINNs are trained by minimizing the
residual of the governing equations and boundary conditions, with their
configuration determined through hyperparameter optimization, while the BEM is
applied using boundary discretization. Both methods are evaluated in terms of
solution accuracy, computation time, and generalization capacity.
  Findings - Numerical experiments were conducted by varying the number of
integration points for BEM and the number of layers and neurons per layer for
PINNs. Hyperparameter tuning provided further insight into suitable
configurations for wave scattering problems. At comparable accuracy, PINNs
produced consistent solutions but required training times approximately 42
times longer than BEM. However, once trained, PINNs achieved evaluation times
up to 204 times faster. The generalization capacity was also assessed outside
the PINN training domain, where the relative error increased from $7.46 \times
10^{-2}$ to 8.22, while BEM maintained a similar error level in the extended
region.
  Originality/value - This work presents a direct comparison between PINNs and
BEM for the Helmholtz equation. The analysis provides quantitative data on the
performance of both methods, supporting their selection in future research on
wave propagation problems and establishing future challenges and directions.

</details>


### [40] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: 提出了一种名为NTM的新型神经网络架构，用于计算图结构随机微分博弈中的纳什均衡，通过嵌入非可训练组件来减少参数数量并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 图结构多智能体系统在金融、机器人、能源和社会动力学等领域广泛存在，但现有方法在大规模稀疏设置下计算效率低下，需要更高效且可解释的解决方案。

Method: 设计NTM架构，在图结构上施加稀疏化约束，嵌入固定的非可训练组件与图拓扑对齐，并将其集成到Direct Parameterization和Deep BSDE两种游戏求解器中。

Result: 理论证明了NTM在静态图博弈中的通用逼近性质，数值实验表明NTM方法在性能上与全可训练对应方法相当，但计算效率显著提升。

Conclusion: NTM架构通过非可训练组件有效减少了参数数量，在保持性能的同时提高了计算效率，为大规模图结构随机微分博弈提供了实用的解决方案。

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [41] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 本研究评估了基础模型与传统方法在fMRI脑信号预测和因果发现中的表现，发现基础模型在零样本预测方面具有竞争力，并能更精确地检测因果交互。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的兴起，需要了解它们在脑信号预测和因果分析方面与传统方法（如Wiener-Granger因果性）相比的表现，以及是否能在零样本设置下应用。

Method: 使用合成时间序列（逻辑映射耦合和Ornstein-Uhlenbeck过程）验证方法，测试基础模型在零样本和微调设置下的预测能力，并将模型的类Granger估计与标准Granger因果性进行比较。

Result: 基础模型在零样本预测fMRI时间序列方面表现竞争性（对照组MAPE为0.55，患者组为0.27），虽然标准Granger因果性未显示模型间的明显定量差异，但基础模型能更精确地检测因果交互。

Conclusion: 基础模型具有多功能性、强大的零样本性能，在时间序列数据的预测和因果发现方面具有潜在应用价值。

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [42] [Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://arxiv.org/abs/2509.12521)
*Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为Preference Hijacking (Phi)的新型攻击方法，通过精心优化的图像在推理时任意操纵多模态大语言模型的输出偏好，无需修改模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)的广泛应用带来了严重的安全隐患，研究者发现其输出偏好可以通过优化图像被任意操纵，产生难以检测的偏见响应。

Method: 提出Preference Hijacking (Phi)方法，使用偏好劫持图像在推理时操纵MLLM响应偏好，并开发了可嵌入不同图像的通用劫持扰动组件。

Result: 在各种任务上的实验结果表明该方法具有显著效果，能够成功劫持MLLM响应朝向攻击者指定的偏好。

Conclusion: 这项工作揭示了MLLMs存在新的安全风险，即输出偏好可能被图像攻击任意操纵，需要开发相应的防御机制。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have gained significant
attention across various domains. However, their widespread adoption has also
raised serious safety concerns. In this paper, we uncover a new safety risk of
MLLMs: the output preference of MLLMs can be arbitrarily manipulated by
carefully optimized images. Such attacks often generate contextually relevant
yet biased responses that are neither overtly harmful nor unethical, making
them difficult to detect. Specifically, we introduce a novel method, Preference
Hijacking (Phi), for manipulating the MLLM response preferences using a
preference hijacked image. Our method works at inference time and requires no
model modifications. Additionally, we introduce a universal hijacking
perturbation -- a transferable component that can be embedded into different
images to hijack MLLM responses toward any attacker-specified preferences.
Experimental results across various tasks demonstrate the effectiveness of our
approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.

</details>


### [43] [Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design](https://arxiv.org/abs/2509.12527)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了首个选择性分类下的信息提升证书理论，通过PAC-Bayes子伽马分析、骨架敏感性定理等方法，在多个数据集和模型家族上验证了假设，将弃权率降低了12-15%同时保持相同风险，运行时间开销低于20%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生看似合理但错误的输出，现有的启发式方法（如HallBayes）缺乏形式化保证，需要开发具有理论保证的可靠性验证方法。

Method: 开发了信息提升证书的综合理论，包括：(i) PAC-Bayes子伽马分析扩展标准Bernstein边界；(ii) 显式骨架敏感性定理量化错误设定的鲁棒性；(iii) 假设违反下的故障模式保证；(iv) 骨架构建的原则性变分方法。

Result: 在六个数据集和多个模型家族上经验验证了假设，在相同风险水平下将弃权率降低了12-15%，运行时间开销保持在20%以下（通过批处理进一步降低）。

Conclusion: 该研究为大型语言模型的可靠性验证提供了首个具有形式化保证的理论框架，显著提高了选择性分类的效率和效果，为实际应用提供了可行的解决方案。

Abstract: Large language models often produce plausible but incorrect outputs. Existing
heuristics such as HallBayes lack formal guarantees. We develop the first
comprehensive theory of \emph{information-lift certificates} under selective
classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma}
analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton
sensitivity theorems quantifying robustness to misspecification; (iii)
failure-mode guarantees under assumption violations; and (iv) a principled
variational method for skeleton construction. Across six datasets and multiple
model families, we validate assumptions empirically, reduce abstention by
12--15\% at the same risk, and maintain runtime overhead below 20\% (further
reduced via batching).

</details>


### [44] [Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs](https://arxiv.org/abs/2509.12530)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: GRAPHITE是一个通过图变换直接提升图同质性的新框架，专门解决异质图上的GNN性能问题，在异质图上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异质图（连接节点特征或标签不相似）上表现不佳，甚至不如简单的MLP，需要超越架构设计的新方法来直接解决异质性问题

Method: 提出GRAPHITE框架，通过创建特征节点来促进具有相似特征节点之间的同质性消息传递，直接转换图结构来提高同质性

Result: 理论和实验证明GRAPHITE能显著提高异质图的同质性，图大小仅轻微增加。在挑战性数据集上显著优于SOTA方法，在同质图上达到可比性能

Conclusion: GRAPHITE是首个显式转换图结构来直接改善图同质性的方法，为解决图异质性提供了新的有效范式

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling
graph-structured data. However, existing GNNs often struggle with heterophilic
graphs, where connected nodes tend to have dissimilar features or labels. While
numerous methods have been proposed to address this challenge, they primarily
focus on architectural designs without directly targeting the root cause of the
heterophily problem. These approaches still perform even worse than the
simplest MLPs on challenging heterophilic datasets. For instance, our
experiments show that 21 latest GNNs still fall behind the MLP on the Actor
dataset. This critical challenge calls for an innovative approach to addressing
graph heterophily beyond architectural designs. To bridge this gap, we propose
and study a new and unexplored paradigm: directly increasing the graph
homophily via a carefully designed graph transformation. In this work, we
present a simple yet effective framework called GRAPHITE to address graph
heterophily. To the best of our knowledge, this work is the first method that
explicitly transforms the graph to directly improve the graph homophily.
Stemmed from the exact definition of homophily, our proposed GRAPHITE creates
feature nodes to facilitate homophilic message passing between nodes that share
similar features. Furthermore, we both theoretically and empirically show that
our proposed GRAPHITE significantly increases the homophily of originally
heterophilic graphs, with only a slight increase in the graph size. Extensive
experiments on challenging datasets demonstrate that our proposed GRAPHITE
significantly outperforms state-of-the-art methods on heterophilic graphs while
achieving comparable accuracy with state-of-the-art methods on homophilic
graphs.

</details>


### [45] [Cross-Modal Deep Metric Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2509.12540)
*Wei Li,Zheze Yang*

Main category: cs.LG

TL;DR: 提出基于跨模态深度度量学习的时间序列异常检测方法，通过特征聚类模型和vMF分布提高检测灵敏度和效率


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中灵敏度低和耗时高的问题

Method: 构建跨模态深度度量学习特征聚类模型，包含输入层、三元组选择层和损失函数计算层；使用平方欧氏距离计算聚类中心距离，采用随机梯度下降优化模型；利用主成分方向向量内积作为异常度量，应用vMF分布描述时间序列数据方向特征

Result: 实验结果表明该方法能准确分类不同属性的时间序列数据，对异常具有高灵敏度，检测精度高、速度快、鲁棒性强

Conclusion: 该方法有效解决了时间序列异常检测的灵敏度和效率问题，具有很好的实用价值

Abstract: To effectively address the issues of low sensitivity and high time
consumption in time series anomaly detection, we propose an anomaly detection
method based on cross-modal deep metric learning. A cross-modal deep metric
learning feature clustering model is constructed, composed of an input layer, a
triplet selection layer, and a loss function computation layer. The squared
Euclidean distances between cluster centers are calculated, and a stochastic
gradient descent strategy is employed to optimize the model and classify
different time series features. The inner product of principal component
direction vectors is used as a metric for anomaly measurement. The von
Mises-Fisher (vMF) distribution is applied to describe the directional
characteristics of time series data, and historical data is used to train and
obtain evaluation parameters. By comparing the principal component direction
vector of actual time series data with the threshold, anomaly detection is
performed. Experimental results demonstrate that the proposed method accurately
classifies time series data with different attributes, exhibits high
sensitivity to anomalies, and achieves high detection accuracy, fast detection
speed, and strong robustness.

</details>


### [46] [iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining](https://arxiv.org/abs/2509.12553)
*Xiang Xue,Yatu Ji,Qing-dao-er-ji Ren,Bao Shi,Min Lu,Nier Wu,Xufei Zhuang,Haiteng Xu,Gan-qi-qi-ge Cha*

Main category: cs.LG

TL;DR: 提出了iCD方法，通过Gram矩阵挖掘logits中的可解释结构知识，无需真实标签或特征对齐，在细粒度分类任务中性能提升显著


<details>
  <summary>Details</summary>
Motivation: 解决Logit知识蒸馏方法决策过程可解释性有限的问题，希望在不依赖中间特征对齐的情况下提升模型的可解释性

Method: 使用解耦的局部logit表示的Gram矩阵，让学生模型学习潜在的语义结构模式，无需真实标签或特征空间对齐

Result: 在基准数据集上的广泛实验显示iCD在不同师生架构中均有效，特别是在细粒度分类任务中表现突出，相比基线最高提升5.08%

Conclusion: iCD是一种简单有效的可解释知识蒸馏方法，能够成功挖掘和传递logits中的结构知识，提升模型性能的同时增强可解释性

Abstract: Logit Knowledge Distillation has gained substantial research interest in
recent years due to its simplicity and lack of requirement for intermediate
feature alignment; however, it suffers from limited interpretability in its
decision-making process. To address this, we propose implicit Clustering
Distillation (iCD): a simple and effective method that mines and transfers
interpretable structural knowledge from logits, without requiring ground-truth
labels or feature-space alignment. iCD leverages Gram matrices over decoupled
local logit representations to enable student models to learn latent semantic
structural patterns. Extensive experiments on benchmark datasets demonstrate
the effectiveness of iCD across diverse teacher-student architectures, with
particularly strong performance in fine-grained classification tasks --
achieving a peak improvement of +5.08% over the baseline. The code is available
at: https://github.com/maomaochongaa/iCD.

</details>


### [47] [No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction](https://arxiv.org/abs/2509.12573)
*Tim Bary,Benoît Macq,Louis Petit*

Main category: cs.LG

TL;DR: 提出了一种基于共形预测的训练免费、模型无关的专家延迟框架，通过预测集识别标签不确定性并选择最具区分度的专家，在CIFAR10-H和ImageNet16-H上达到99%+准确率，减少专家工作量达11倍。


<details>
  <summary>Details</summary>
Motivation: 现有学习延迟方法对专家组成变化敏感且需要大量重新训练，需要一种更灵活、无需训练的专家延迟解决方案。

Method: 使用共形预测器生成预测集来识别标签特定不确定性，通过可分离性准则选择最能区分剩余可能标签的专家。

Result: 在CIFAR10-H和ImageNet16-H上分别达到99.57±0.10%和99.40±0.52%准确率，专家工作量减少最多11倍，在专家性能下降时仍保持稳健。

Conclusion: 该方法为现实世界人机协作提供了可扩展、无需重新训练的学习延迟替代方案。

Abstract: AI systems often fail to deliver reliable predictions across all inputs,
prompting the need for hybrid human-AI decision-making. Existing Learning to
Defer (L2D) approaches address this by training deferral models, but these are
sensitive to changes in expert composition and require significant retraining
if experts change. We propose a training-free, model- and expert-agnostic
framework for expert deferral based on conformal prediction. Our method uses
the prediction set generated by a conformal predictor to identify
label-specific uncertainty and selects the most discriminative expert using a
segregativity criterion, measuring how well an expert distinguishes between the
remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that
our method consistently outperforms both the standalone model and the strongest
expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while
reducing expert workload by up to a factor of $11$. The method remains robust
under degraded expert performance and shows a gradual performance drop in
low-information settings. These results suggest a scalable, retraining-free
alternative to L2D for real-world human-AI collaboration.

</details>


### [48] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本研究系统分析了在模型访问受限和计算资源有限条件下训练数据归因(TDA)方法的可行性，提出了使用代理模型等解决方案，并证明无需在目标数据集上训练的模型仍能提供有信息量的归因分数。


<details>
  <summary>Details</summary>
Motivation: 现有梯度基TDA方法(如影响函数)需要完整模型访问和高计算成本，这在商业模型不公开且计算资源有限的现实场景中存在重大应用障碍。

Method: 通过设计代理模型等适当解决方案，研究在不同访问约束级别下执行TDA的可行性，并验证未在目标数据集上训练的模型产生的归因分数的有效性。

Result: 研究发现即使在有限访问条件下，通过适当方法仍能获得有信息量的归因分数，且这些分数在一系列任务中保持有效性。

Conclusion: 该研究为在现实环境中部署TDA提供了实用指导，提高了在有限访问条件下的可行性和效率，推动了TDA在实际应用中的更广泛采用。

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [49] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: MICE是一个多模态基础模型，通过协作专家机制整合病理图像、临床报告和基因组数据，在泛癌预后预测中显著优于现有方法，展现出优异的泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以有效利用多模态数据的丰富信息，提取的表示泛化性差，需要开发能够整合病理图像、临床报告和基因组数据的更有效方法。

Method: 采用多模态数据集成协作专家(MICE)框架，使用功能多样的专家模块全面捕捉跨癌症和癌症特异性见解，结合对比学习和监督学习增强泛化性。

Result: 在11,799名患者30种癌症类型数据上，MICE在内部队列C-index提升3.8%-11.2%，独立队列提升5.8%-8.8%，显著优于单模态和最先进的多专家多模态模型。

Conclusion: MICE建立了有效且可扩展的泛癌预后预测基础，具有个性化定制治疗和改善治疗结果的强大潜力。

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [50] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 提出FedFD方法，通过频域高能量集中特性过滤冗余高频信息，在联邦学习中降低通信成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中传统空间域方法存在冗余信息和噪声的问题，降低通信负担同时保护数据隐私

Method: 基于离散余弦变换的高能量集中特性，使用二进制掩码保留低频成分，通过频域分布对齐和真实数据驱动的合成分类损失优化低频成分质量

Result: 在5个图像和语音数据集上优于现有方法，在CIFAR-10数据集上通信成本降低37.78%，性能提升10.88%

Conclusion: 频域感知的联邦学习方法FedFD能有效减少通信成本并提升模型性能，为数据异构环境下的联邦学习提供了有效解决方案

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [51] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: TimeRep是一种新颖的时间序列异常检测方法，利用时间序列基础模型(TSFMs)的中间层表示，通过计算与参考集合的距离来检测异常，在UCR异常档案数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法通常只使用基础模型的最后一层表示，而忽略了中间层可能包含的丰富信息。TimeRep旨在利用中间层表示来提高异常检测性能。

Method: 1) 选择预训练TSFM中最具信息量的中间层和补丁标记位置；2) 从训练数据构建中间表示参考集合，并使用核心集策略减少集合大小；3) 推理时通过计算输入数据中间表示与参考集合的距离来得到异常分数；4) 集成适应机制处理概念漂移。

Result: 在包含250个单变量时间序列的UCR异常档案数据集上进行广泛实验，TimeRep持续优于包括非深度学习、深度学习和基于基础模型的最先进基线方法。

Conclusion: TimeRep通过有效利用时间序列基础模型的中间层表示，提供了一种强大的异常检测方法，在处理概念漂移方面表现出色，在多个基准测试中达到最先进性能。

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [52] [Instance-level Randomization: Toward More Stable LLM Evaluations](https://arxiv.org/abs/2509.12678)
*Yiyang Li,Yonghuang Wu,Ying Luo,Liangtai Sun,Zishu Qin,Lin Qiu,Xuezhi Cao,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出实例级随机化(ILR)方法来解决大语言模型评估中的不稳定性问题，通过为每个实例随机化所有影响因素并多次实验取平均，减少方差并增强模型比较的公平性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估存在不稳定性，随机因素的微小变化会导致评分剧烈波动甚至模型排名变化。不同模型对特定随机因素设置有不同偏好，固定设置可能导致不公平比较。

Method: 提出实例级随机化(ILR)方法：不为整个基准测试使用固定设置，而是为每个实例随机化所有影响评估分数的因素，进行多次实验并报告平均分数。

Result: 理论分析和实证结果表明，ILR能够减少随机因素引起的方差和不公平比较，并以不到一半的计算成本达到与先前方法相似的鲁棒性水平。

Conclusion: ILR方法有效解决了LLM评估中的不稳定性问题，提供了更公平、更稳健的模型比较方案，同时显著降低了计算成本。

Abstract: Evaluations of large language models (LLMs) suffer from instability, where
small changes of random factors such as few-shot examples can lead to drastic
fluctuations of scores and even model rankings. Moreover, different LLMs can
have different preferences for a certain setting of random factors. As a
result, using a fixed setting of random factors, which is often adopted as the
paradigm of current evaluations, can lead to potential unfair comparisons
between LLMs. To mitigate the volatility of evaluations, we first theoretically
analyze the sources of variance induced by changes in random factors. Targeting
these specific sources, we then propose the instance-level randomization (ILR)
method to reduce variance and enhance fairness in model comparisons. Instead of
using a fixed setting across the whole benchmark in a single experiment, we
randomize all factors that affect evaluation scores for every single instance,
run multiple experiments and report the averaged score. Theoretical analyses
and empirical results demonstrate that ILR can reduce the variance and unfair
comparisons caused by random factors, as well as achieve similar robustness
level with less than half computational cost compared with previous methods.

</details>


### [53] [Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry](https://arxiv.org/abs/2509.12679)
*Oliver Knitter,Dan Zhao,Stefan Leichenauer,Shravan Veerapaneni*

Main category: cs.LG

TL;DR: 本文研究了基于transformer的神经量子态(NQS)在量子化学应用中的缩放规律，发现模型大小与训练时间的关系高度依赖于损失度量和ansatz选择，与语言模型的线性关系不同。


<details>
  <summary>Details</summary>
Motivation: 随着神经量子态(NQS)越来越多地采用基于大语言模型(LLM)的组件，需要理解NQS的缩放规律，以阐明NQS ansatze的可扩展性和最优性能-资源权衡。

Method: 识别基于transformer的NQS在二次量子化量子化学应用中的缩放规律，通过计算约束优化获得的参数曲线来分析性能与问题大小的关系。

Result: 发现模型大小与训练时间的关系高度依赖于损失度量(绝对误差和V-score)和ansatz选择，与语言模型中发现的近似线性关系不同。

Conclusion: NQS的缩放规律与语言模型存在显著差异，表明需要针对量子计算应用开发专门的缩放理论和优化策略。

Abstract: Scaling laws have been used to describe how large language model (LLM)
performance scales with model size, training data size, or amount of
computational resources. Motivated by the fact that neural quantum states (NQS)
has increasingly adopted LLM-based components, we seek to understand NQS
scaling laws, thereby shedding light on the scalability and optimal
performance--resource trade-offs of NQS ansatze. In particular, we identify
scaling laws that predict the performance, as measured by absolute error and
V-score, for transformer-based NQS as a function of problem size in
second-quantized quantum chemistry applications. By performing analogous
compute-constrained optimization of the obtained parametric curves, we find
that the relationship between model size and training time is highly dependent
on loss metric and ansatz, and does not follow the approximately linear
relationship found for language models.

</details>


### [54] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: ZTree是一种基于统计假设检验的新型决策树学习框架，用统计显著的子组识别替代传统的纯度分裂方法，通过交叉验证控制多重检验，仅需一个参数（z阈值）即可控制树复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统CART决策树基于纯度分裂缺乏统计严谨性，需要复杂的剪枝和后处理。ZTree旨在提供统计上更严谨、参数更简单、更易解释的决策树框架。

Method: 在每个节点使用假设检验（z检验、t检验等）评估候选子组与补集的统计差异，采用交叉验证方法处理多重检验问题，通过z阈值（p值）控制分裂停止。

Result: 在五个大规模UCI数据集上的实验表明，ZTree在低数据量情况下表现优异，相比CART能生成更简单的树而不牺牲性能。

Conclusion: ZTree为决策树学习提供了统计基础更坚实的替代方案，通过假设检验和多重检验校正实现了高效灵活的框架，参数调优直观简单。

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [55] [Soft Graph Transformer for MIMO Detection](https://arxiv.org/abs/2509.12694)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: SGT是一种针对MIMO检测的软输入软输出神经网络架构，通过将消息传递集成到图感知注意力机制中，接近最大似然检测性能并超越现有Transformer方法


<details>
  <summary>Details</summary>
Motivation: 传统最大似然检测计算复杂度太高，消息传递算法依赖大系统渐近性和随机矩阵假设，现有Transformer检测器无法利用MIMO因子图结构和解码器软信息

Method: 将消息传递直接集成到图感知注意力机制中，支持通过软输入嵌入进行解码器知情更新，实现有效的软输出生成

Result: 作为独立检测器，SGT接近最大似然检测性能，超越先前的Transformer方法

Conclusion: SGT设计在保持计算效率的同时实现了有效的软输出生成，解决了现有方法的局限性

Abstract: We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural
architecture tailored for MIMO detection. While Maximum Likelihood (ML)
detection achieves optimal accuracy, its prohibitive exponential complexity
renders it impractical for real-world systems. Conventional message passing
algorithms offer tractable alternatives but rely on large-system asymptotics
and random matrix assumptions, both of which break down under practical
implementations. Prior Transformer-based detectors, on the other hand, fail to
incorporate the MIMO factor graph structure and cannot utilize decoder-side
soft information, limiting their standalone performance and their applicability
in iterative detection-decoding (IDD). To overcome these limitations, SGT
integrates message passing directly into a graph-aware attention mechanism and
supports decoder-informed updates through soft-input embeddings. This design
enables effective soft-output generation while preserving computational
efficiency. As a standalone detector, SGT closely approaches ML performance and
surpasses prior Transformer-based approaches.

</details>


### [56] [Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach](https://arxiv.org/abs/2509.12697)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang*

Main category: cs.LG

TL;DR: 聚合基础模型的双层个性化学习框架，通过客户端个性化微调和服务器端根据相似用户的个性化聚合，解决小规模数据下的个性化与聚合的调和问题。


<details>
  <summary>Details</summary>
Motivation: 小规模用户群体或专门场景的数据有限，在聚合基础模型微调中个性化与聚合之间的调和问题更加敏感，需要新方法来解决这一挑战。

Method: 提出双层个性化框架：客户端使用私有数据进行个性化微调，服务器端通过客户特定任务向量测量用户相似度，进行个性化聚合，减少非IID数据中无关或利益冲突客户的干扰。

Result: 在标准数据集上进行了广泛实验分析，证明了所提算法的有效性。

Conclusion: 双层个性化框架能够在聚合基础模型微调中有效实现个性化与聚合的调和，特别适用于小规模用户群体和专门场景。

Abstract: Federated foundation models represent a new paradigm to jointly fine-tune
pre-trained foundation models across clients. It is still a challenge to
fine-tune foundation models for a small group of new users or specialized
scenarios, which typically involve limited data compared to the large-scale
data used in pre-training. In this context, the trade-off between
personalization and federation becomes more sensitive. To tackle these, we
proposed a bi-level personalization framework for federated fine-tuning on
foundation models. Specifically, we conduct personalized fine-tuning on the
client-level using its private data, and then conduct a personalized
aggregation on the server-level using similar users measured by client-specific
task vectors. Given the personalization information gained from client-level
fine-tuning, the server-level personalized aggregation can gain group-wise
personalization information while mitigating the disturbance of irrelevant or
interest-conflict clients with non-IID data. The effectiveness of the proposed
algorithm has been demonstrated by extensive experimental analysis in benchmark
datasets.

</details>


### [57] [NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification](https://arxiv.org/abs/2509.12704)
*Mohammad Abdul Hafeez Khan,Twisha Bhattacharyya,Omar Khan,Noorah Khan,Alina Aziz Fatima Khan,Mohammed Qutub Khan,Sujoy Ghosh Hajra*

Main category: cs.LG

TL;DR: 提出NORA方法，结合监督对比学习和随机森林分类器，利用常规非肾脏临床变量进行慢性肾病早期分类，在两个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病早期检测在门诊环境中具有挑战性，因为缺乏实验室肾脏生物标志物，需要探索常规收集的非肾脏临床变量的预测潜力。

Method: 提出Nephrology-Oriented Representation leArning (NORA)方法，结合监督对比学习和非线性随机森林分类器，从表格化电子健康记录数据中提取判别性患者表示，用于下游CKD分类。

Result: NORA提高了类别可分性和整体分类性能，特别是在早期CKD的F1分数方面有显著提升，并在不同患者队列中展示了良好的泛化能力。

Conclusion: NORA方法能够有效利用常规非肾脏临床变量进行慢性肾病风险分层，为门诊环境中的早期CKD检测提供了有前景的解决方案。

Abstract: Chronic Kidney Disease (CKD) affects millions of people worldwide, yet its
early detection remains challenging, especially in outpatient settings where
laboratory-based renal biomarkers are often unavailable. In this work, we
investigate the predictive potential of routinely collected non-renal clinical
variables for CKD classification, including sociodemographic factors, comorbid
conditions, and urinalysis findings. We introduce the Nephrology-Oriented
Representation leArning (NORA) approach, which combines supervised contrastive
learning with a nonlinear Random Forest classifier. NORA first derives
discriminative patient representations from tabular EHR data, which are then
used for downstream CKD classification. We evaluated NORA on a clinic-based EHR
dataset from Riverside Nephrology Physicians. Our results demonstrated that
NORA improves class separability and overall classification performance,
particularly enhancing the F1-score for early-stage CKD. Additionally, we
assessed the generalizability of NORA on the UCI CKD dataset, demonstrating its
effectiveness for CKD risk stratification across distinct patient cohorts.

</details>


### [58] [Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting](https://arxiv.org/abs/2509.12708)
*Pratik Nag*

Main category: cs.LG

TL;DR: 提出了一个基于PyTorch的时空DeepKriging框架，用于欧洲降水数据的插值和多步预测，能够处理时空不规则性并生成高分辨率结果。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够有效处理欧洲降水数据时空不规则性的插值和预测框架，为气候数据分析提供可靠的工具。

Method: 使用PyTorch平台实现时空DeepKriging（STDK）框架，开发了可复现的代码模块，包括插值和预测两个独立实现。

Result: 通过对每日降水测量的广泛评估，证明了该方法在预测性能和鲁棒性方面的有效性。

Conclusion: 该框架能够成功处理时空不规则性，生成高质量的插值和多步预测结果，为类似气候数据集的应用提供了便利。

Abstract: A detailed analysis of precipitation data over Europe is presented, with a
focus on interpolation and forecasting applications. A Spatio-temporal
DeepKriging (STDK) framework has been implemented using the PyTorch platform to
achieve these objectives. The proposed model is capable of handling
spatio-temporal irregularities while generating high-resolution interpolations
and multi-step forecasts. Reproducible code modules have been developed as
standalone PyTorch implementations for the
interpolation\footnote[2]{Interpolation -
https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and
forecasting\footnote[3]{Forecasting -
https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader
application to similar climate datasets. The effectiveness of this approach is
demonstrated through extensive evaluation on daily precipitation measurements,
highlighting predictive performance and robustness.

</details>


### [59] [Unbiased Online Curvature Approximation for Regularized Graph Continual Learning](https://arxiv.org/abs/2509.12727)
*Jie Yin,Ke Sun,Han Wu*

Main category: cs.LG

TL;DR: 本文提出了一个基于Fisher信息矩阵的图持续学习正则化框架，并提出了一种新的在线曲率近似方法，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决图持续学习中的灾难性遗忘问题，特别是在无回放、类增量设置的挑战性场景中，现有方法基于对角近似的Fisher信息矩阵存在局限性

Method: 建立基于Fisher信息矩阵曲率参数空间的一般正则化框架，提出无偏在线曲率近似方法直接估计正则化项，无需显式计算和存储Fisher信息矩阵

Result: 在三个图数据集上的大量实验表明，该方法显著优于现有的基于正则化的方法，在稳定性（保留旧知识）和可塑性（获取新知识）之间实现了更好的平衡

Conclusion: 提出的在线曲率近似方法能够更好地捕捉学习新任务时的损失景观，同时保留先前任务学到的知识，为图持续学习提供了有效的解决方案

Abstract: Graph continual learning (GCL) aims to learn from a continuous sequence of
graph-based tasks. Regularization methods are vital for preventing catastrophic
forgetting in GCL, particularly in the challenging replay-free,
class-incremental setting, where each task consists of a set of unique classes.
In this work, we first establish a general regularization framework for GCL
based on the curved parameter space induced by the Fisher information matrix
(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its
variants are a special case within this framework, using a diagonal
approximation of the empirical FIM based on parameters from previous tasks. To
overcome their limitations, we propose a new unbiased online curvature
approximation of the full FIM based on the model's current learning state. Our
method directly estimates the regularization term in an online manner without
explicitly evaluating and storing the FIM itself. This enables the model to
better capture the loss landscape during learning new tasks while retaining the
knowledge learned from previous tasks. Extensive experiments on three graph
datasets demonstrate that our method significantly outperforms existing
regularization-based methods, achieving a superior trade-off between stability
(retaining old knowledge) and plasticity (acquiring new knowledge).

</details>


### [60] [A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs](https://arxiv.org/abs/2509.12730)
*Francesco Zola,Jon Ander Medina,Andrea Venturi,Amaia Gil,Raul Orduna*

Main category: cs.LG

TL;DR: 本文提出了一种结合图机器学习和网络分析的方法，通过四步预处理框架和Graph Autoencoders来检测金融交易图中的拓扑模式，以应对数字生态系统中复杂的金融犯罪活动。


<details>
  <summary>Details</summary>
Motivation: 数字生态系统的发展使金融部门面临不断演变的滥用和犯罪策略，传统基于规则的系统缺乏检测复杂或协调犯罪行为的适应性，需要分析参与者互动来发现可疑活动并提取其作案手法。

Method: 提出四步预处理框架：(i)提取图结构，(ii)考虑数据时间性管理大节点集，(iii)检测社区，(iv)应用自动标注策略生成弱标注。然后实现三种不同的Graph Autoencoder变体来区分已知拓扑模式。

Result: 初步结果表明，这种以模式为中心、拓扑驱动的方法能有效检测复杂的金融犯罪方案，为传统基于规则的检测系统提供了有前景的替代方案。

Conclusion: 该方法通过图机器学习和网络分析的结合，解决了传统金融数据集稀疏、无标注的挑战，为金融犯罪检测提供了更有效的解决方案。

Abstract: The rise of digital ecosystems has exposed the financial sector to evolving
abuse and criminal tactics that share operational knowledge and techniques both
within and across different environments (fiat-based, crypto-assets, etc.).
Traditional rule-based systems lack the adaptability needed to detect
sophisticated or coordinated criminal behaviors (patterns), highlighting the
need for strategies that analyze actors' interactions to uncover suspicious
activities and extract their modus operandi. For this reason, in this work, we
propose an approach that integrates graph machine learning and network analysis
to improve the detection of well-known topological patterns within
transactional graphs. However, a key challenge lies in the limitations of
traditional financial datasets, which often provide sparse, unlabeled
information that is difficult to use for graph-based pattern analysis.
Therefore, we firstly propose a four-step preprocessing framework that involves
(i) extracting graph structures, (ii) considering data temporality to manage
large node sets, (iii) detecting communities within, and (iv) applying
automatic labeling strategies to generate weak ground-truth labels. Then, once
the data is processed, Graph Autoencoders are implemented to distinguish among
the well-known topological patterns. Specifically, three different GAE variants
are implemented and compared in this analysis. Preliminary results show that
this pattern-focused, topology-driven method is effective for detecting complex
financial crime schemes, offering a promising alternative to conventional
rule-based detection systems.

</details>


### [61] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 提出了一种基于AI的端到端癌症通路分析框架，使用时间序列机器学习模型预测癌症严重程度和突变进展，推荐治疗方案，无需依赖湿实验室数据。


<details>
  <summary>Details</summary>
Motivation: 癌症是第二大死因，传统通路分析依赖耗时的手工湿实验室数据，需要开发高效、经济的方法来预测癌症进展和推荐治疗。

Method: 从TCGA数据库提取突变序列，使用新颖的预处理算法按突变频率筛选关键突变，通过RNN预测癌症严重程度，结合药物靶点数据库预测未来突变和推荐治疗。

Result: 框架达到超过60%的准确率，ROC曲线表现稳健，预处理成功识别出每个癌症阶段约几百个关键驱动突变，与现有研究一致。

Conclusion: 这是首个无需湿实验室工作的端到端框架，能有效预测癌症进展和提供治疗建议，具有高效和经济优势。

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [62] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 提出了一种新的SDM激活函数，在标准softmax基础上增加了相似性和距离感知，提高了神经网络的鲁棒性和可解释性


<details>
  <summary>Details</summary>
Motivation: 标准softmax激活函数在面对协变量偏移和分布外输入时缺乏鲁棒性，需要更强大的激活函数来提升模型性能和可解释性

Method: 在softmax基础上引入相似性感知（正确预测的深度匹配）和距离到训练分布感知，形成相似性-距离-幅度（SDM）激活函数

Result: SDM激活函数比softmax对协变量偏移和分布外输入更具鲁棒性，提供基于范例的可解释性，并支持选择性分类中的类别召回率保护

Conclusion: SDM激活函数是softmax的有效替代方案，特别适用于选择性分类任务，即使在考虑后校准方法时也表现更优

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [63] [EmbeddedML: A New Optimized and Fast Machine Learning Library](https://arxiv.org/abs/2509.12774)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.LG

TL;DR: EmbeddedML是一个训练时间优化的机器学习库，通过数学重写算法，在保持精度的同时显著减少训练时间，相比scikit-learn在回归和分类任务上实现了数倍到数百倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习库如scikit-learn在处理大型数据集时训练时间过长，需要优化训练效率而不损失精度。

Method: 通过数学重写算法（如Multiple Linear Regression、Logistic Regression和SVM），对机器学习算法进行数学增强和优化，减少计算复杂度。

Result: 相比scikit-learn：Multiple Linear Regression速度提升约数倍；SVM在小数据集上训练时间减少2倍，大数据集上减少800倍；Logistic Regression训练时间减少4倍，且均无精度损失。

Conclusion: EmbeddedML提供了经过数学重写和优化的回归、分类、聚类和降维算法，能显著减少训练时间，是高效的机器学习库解决方案。

Abstract: Machine learning models and libraries can train datasets of different sizes
and perform prediction and classification operations, but machine learning
models and libraries cause slow and long training times on large datasets. This
article introduces EmbeddedML, a training-time-optimized and mathematically
enhanced machine learning library. The speed was increased by approximately
times compared to scikit-learn without any loss in terms of accuracy in
regression models such as Multiple Linear Regression. Logistic Regression and
Support Vector Machines (SVM) algorithms have been mathematically rewritten to
reduce training time and increase accuracy in classification models. With the
applied mathematical improvements, training time has been reduced by
approximately 2 times for SVM on small datasets and by around 800 times on
large datasets, and by approximately 4 times for Logistic Regression, compared
to the scikit-learn implementation. In summary, the EmbeddedML library offers
regression, classification, clustering, and dimensionality reduction algorithms
that are mathematically rewritten and optimized to reduce training time.

</details>


### [64] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文对基于投影的安全过滤器在强化学习中的两种集成策略（SE-RL和SP-RL）进行了理论比较，重点分析了动作别名效应对策略梯度的影响，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于投影的安全过滤器在安全关键场景中广泛使用，但对其两种主要集成策略（SE-RL和SP-RL）的差异缺乏形式化理解，需要理论分析来指导方法选择和改进。

Method: 提出了统一的actor-critic算法形式化框架，理论分析了两种方法的策略梯度估计，特别关注动作别名效应的影响，并比较了包括新颖惩罚基改进在内的缓解策略。

Result: 实证结果表明动作别名效应对SP-RL的损害更大，但通过适当的改进策略，SP-RL可以在各种环境中匹配或超越改进的SE-RL性能。

Conclusion: 研究结果为根据任务特性选择和优化基于投影的安全RL方法提供了可操作的见解，SP-RL在适当改进后具有竞争优势。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [65] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: Tool-R1是一个基于强化学习的框架，通过生成可执行Python代码使大语言模型能够进行通用、组合式和多步骤的工具使用，在GAIA基准测试中比强基线提升约10%的准确率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要最新知识、精确操作或专业工具使用的现实任务中存在局限，需要解决工具增强推理的可靠性和效率问题

Method: 提出强化学习框架，支持用户自定义工具和标准库集成，通过基于结果的奖励函数（结合LLM答案判断和代码执行成功）指导策略优化，使用动态样本队列缓存和重用高质量轨迹

Result: 在GAIA基准测试中显著提高了准确性和鲁棒性，比强基线提升约10%，在复杂多步骤任务上提升更大

Conclusion: Tool-R1在实现现实应用中可靠高效的工具增强推理方面具有巨大潜力

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [66] [TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems](https://arxiv.org/abs/2509.12895)
*Christian L. Hines,Samuel Spillard,Daniel P. Martin*

Main category: cs.LG

TL;DR: TimeCluster是一种可视化分析技术，通过将多变量时间序列的滑动窗口投影到低维空间来发现结构。本文证明了当使用PCA作为降维技术时，该方法在数学上等同于经典的线性子空间识别方法（块汉克尔矩阵加SVD分解）。


<details>
  <summary>Details</summary>
Motivation: 探索TimeCluster可视化技术与传统子空间系统识别方法之间的数学等价性，为两种方法的交叉应用提供理论基础。

Method: 首先回顾TimeCluster方法和子空间系统识别理论，然后证明时间序列的滑动窗口矩阵形成汉克尔矩阵，应用PCA（通过SVD）可以恢复与子空间识别相同的主方向。通过合成和真实动态信号实验验证两种嵌入方法的一致性。

Result: 实验证实TimeCluster的聚类坐标与子空间识别方法完全一致，两种方法提取相同的低维线性子空间。

Conclusion: 建立了TimeCluster与子空间识别方法的数学等价性，为未来开发包括状态空间预测、流式处理、外部输入整合和鲁棒趋势可视化等新功能提供了机会。

Abstract: TimeCluster is a visual analytics technique for discovering structure in long
multivariate time series by projecting overlapping windows of data into a
low-dimensional space. We show that, when Principal Component Analysis (PCA) is
chosen as the dimensionality reduction technique, this procedure is
mathematically equivalent to classical linear subspace identification
(block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both
approaches, the same low-dimensional linear subspace is extracted from the time
series data. We first review the TimeCluster method and the theory of subspace
system identification. Then we show that forming the sliding-window matrix of a
time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix
recovers the same principal directions as subspace identification. Thus the
cluster coordinates from TimeCluster coincide with the subspace identification
methods. We present experiments on synthetic and real dynamical signals
confirming that the two embeddings coincide. Finally, we explore and discuss
future opportunities enabled by this equivalence, including forecasting from
the identified state space, streaming/online extensions, incorporating and
visualising external inputs and robust techniques for displaying underlying
trends in corrupted data.

</details>


### [67] [Reversible Deep Equilibrium Models](https://arxiv.org/abs/2509.12917)
*Sam McCallum,Kamran Arora,James Foster*

Main category: cs.LG

TL;DR: 可逆深度均衡模型（RevDEQs）通过引入可逆性解决了传统深度均衡模型梯度计算近似的问题，实现了精确梯度计算、无需正则化且函数评估次数更少，在语言建模和图像分类任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度均衡模型（DEQs）虽然通过单层迭代多次的方式在大规模任务中表现优于显式模型，但其梯度计算是近似的，导致训练不稳定，需要正则化或大量函数评估来修复。

Method: 引入可逆深度均衡模型（RevDEQs），通过可逆性设计使得梯度计算变得精确，消除了对正则化的需求，并显著减少了函数评估次数。

Result: RevDEQs在语言建模和图像分类任务上实现了最先进的性能，超越了可比较的隐式和显式模型。

Conclusion: 可逆深度均衡模型通过精确梯度计算和更少的计算开销，为隐式模型提供了更稳定和高效的训练方案，在大规模机器学习任务中具有重要应用价值。

Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model
where the model output is implicitly defined as the fixed point of a learned
function. These models have been shown to outperform explicit (fixed-depth)
models in large-scale tasks by trading many deep layers for a single layer that
is iterated many times. However, gradient calculation through DEQs is
approximate. This often leads to unstable training dynamics and requires
regularisation or many function evaluations to fix. Here, we introduce
Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient
calculation, no regularisation and far fewer function evaluations than DEQs. We
show that RevDEQs achieve state-of-the-art performance on language modelling
and image classification tasks against comparable implicit and explicit models.

</details>


### [68] [Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression](https://arxiv.org/abs/2509.12920)
*Huseyin Karaca,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出了一种软梯度提升框架，在提升过程中嵌入可学习的线性特征变换，特别适用于高维数据稀缺场景，能同时优化特征选择和提升过程。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据稀缺场景下传统梯度提升方法可能无法发现最相关输入表示的问题，通过端到端优化特征变换和提升过程来提高性能并避免过拟合。

Method: 在每个提升迭代中训练软决策树并同时学习线性输入特征变换Q，可扩展到可微分非线性变换。

Result: 在合成和真实数据集上证明该方法能有效且高效地提升性能，避免过拟合。

Conclusion: 该方法通过集成特征变换和提升过程，在高维数据稀缺场景中表现出色，代码已公开以支持可重复性和未来工作。

Abstract: We propose a soft gradient boosting framework for sequential regression that
embeds a learnable linear feature transform within the boosting procedure. At
each boosting iteration, we train a soft decision tree and learn a linear input
feature transform Q together. This approach is particularly advantageous in
high-dimensional, data-scarce scenarios, as it discovers the most relevant
input representations while boosting. We demonstrate, using both synthetic and
real-world datasets, that our method effectively and efficiently increases the
performance by an end-to-end optimization of feature selection/transform and
boosting while avoiding overfitting. We also extend our algorithm to
differentiable non-linear transforms if overfitting is not a problem. To
support reproducibility and future work, we share our code publicly.

</details>


### [69] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 提出了一个统一的评估框架，比较PPO、DPO、ORPO、KTO等LLM对齐方法在事实性、安全性、简洁性、主动性和多样性五个维度上的表现，发现不同方法在不同维度各有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一技术或特定维度，缺乏对LLM对齐方法内在权衡的整体评估，需要开发一个全面的评估框架来指导更平衡可靠的LLM开发。

Method: 使用统一的评估框架，在分布内和分布外数据集上比较PPO、DPO、ORPO、KTO等对齐方法，采用经过人工研究验证的LLM-as-Judge提示进行多维评估。

Result: DPO和KTO在事实准确性方面表现最佳，PPO和DPO在安全性方面领先，PPO在简洁性和主动性之间取得最佳平衡。

Conclusion: 研究揭示了常见对齐方法的内在权衡，为开发更平衡可靠的LLM提供了重要指导，不同方法在不同应用场景下各有优势。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [70] [Sy-FAR: Symmetry-based Fair Adversarial Robustness](https://arxiv.org/abs/2509.12939)
*Haneen Najjar,Eyal Ronen,Mahmood Sharif*

Main category: cs.LG

TL;DR: 该论文提出Sy-FAR方法，通过追求对称性而非完美公平性来提升对抗性鲁棒性的公平性，在面部识别等安全关键任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性鲁棒性提升方法通常导致不公平的鲁棒性——某些类别或群体更容易受到攻击。在现实公平关键任务中，实现完美公平往往不可行，因此需要更可行的对称性目标。

Method: 开发Sy-FAR技术，通过鼓励对称性（即从类别i到j的攻击成功率与从j到i相同）来同时优化对抗性鲁棒性和公平性。

Result: 在五个数据集、三种模型架构上评估，Sy-FAR相比最先进方法显著改善了公平对抗性鲁棒性，运行更快且结果更一致，还能减轻目标类别脆弱性问题。

Conclusion: 对称性是比完美公平性更可行的目标，Sy-FAR方法能有效提升对抗性鲁棒性的公平性，特别是在安全关键的实际应用中。

Abstract: Security-critical machine-learning (ML) systems, such as face-recognition
systems, are susceptible to adversarial examples, including real-world
physically realizable attacks. Various means to boost ML's adversarial
robustness have been proposed; however, they typically induce unfair
robustness: It is often easier to attack from certain classes or groups than
from others. Several techniques have been developed to improve adversarial
robustness while seeking perfect fairness between classes. Yet, prior work has
focused on settings where security and fairness are less critical. Our insight
is that achieving perfect parity in realistic fairness-critical tasks, such as
face recognition, is often infeasible -- some classes may be highly similar,
leading to more misclassifications between them. Instead, we suggest that
seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful
as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable
because class resemblance is a symmetric relation in most domains.
Additionally, as we prove theoretically, symmetry between individuals induces
symmetry between any set of sub-groups, in contrast to other fairness notions
where group-fairness is often elusive. We develop Sy-FAR, a technique to
encourage symmetry while also optimizing adversarial robustness and extensively
evaluate it using five datasets, with three model architectures, including
against targeted and untargeted realistic attacks. The results show Sy-FAR
significantly improves fair adversarial robustness compared to state-of-the-art
methods. Moreover, we find that Sy-FAR is faster and more consistent across
runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover
in this work -- target classes that adversarial examples are likely to be
classified into become significantly less vulnerable after inducing symmetry.

</details>


### [71] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 提出了一种基于概率框架的结构化时空动力学建模方法，整合神经ODE、图神经网络和神经过程，用于从稀疏观测中重建和预测心脏运动。


<details>
  <summary>Details</summary>
Motivation: 需要从稀疏的医学影像观测中准确建模心脏的时空动态，同时处理不确定性、时间连续性和解剖结构信息。

Method: 使用时空多重图表示动态系统，通过GNN参数化的向量场建模潜在轨迹，基于节点和边级别的稀疏上下文观测推断潜在初始状态和控制变量的分布。

Result: 在合成系统和真实心脏影像数据上验证，能准确重建轨迹并从一个观测周期外推未来周期，在ACDC分类任务上达到99%准确率，UK Biobank房颤检测达到67%准确率。

Conclusion: 该方法为分析心脏运动提供了灵活框架，为基于图学习的结构化生物医学时空时间序列数据分析奠定了基础。

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [72] [BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning](https://arxiv.org/abs/2509.12964)
*Honghong Zeng,Jiong Lou,Zhe Wang,Hefeng Zhou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了BAPFL，这是首个专门针对原型联邦学习(PFL)框架的后门攻击方法，通过原型污染策略和触发器优化机制，在保持主任务准确性的同时显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 原型联邦学习(PFL)虽然能有效处理数据异构性问题，但其对后门攻击的鲁棒性尚未得到充分研究。作者发现PFL对现有后门攻击具有固有抵抗性，因此需要专门设计针对PFL的攻击方法。

Method: BAPFL结合了原型污染策略和触发器优化机制。原型污染策略操纵全局原型轨迹来误导良性客户端的原型训练；触发器优化机制为每个潜在目标标签学习独特且隐蔽的触发器，使触发样本的原型与目标标签的全局原型对齐。

Result: 在多个数据集和PFL变体上的实验表明，BAPFL相比传统后门攻击实现了35%-75%的攻击成功率提升，同时保持了主任务准确性。

Conclusion: BAPFL证明了PFL框架在面对专门设计的后门攻击时的脆弱性，强调了在PFL系统中加强安全防护的必要性。

Abstract: Prototype-based federated learning (PFL) has emerged as a promising paradigm
to address data heterogeneity problems in federated learning, as it leverages
mean feature vectors as prototypes to enhance model generalization. However,
its robustness against backdoor attacks remains largely unexplored. In this
paper, we identify that PFL is inherently resistant to existing backdoor
attacks due to its unique prototype learning mechanism and local data
heterogeneity. To further explore the security of PFL, we propose BAPFL, the
first backdoor attack method specifically designed for PFL frameworks. BAPFL
integrates a prototype poisoning strategy with a trigger optimization
mechanism. The prototype poisoning strategy manipulates the trajectories of
global prototypes to mislead the prototype training of benign clients, pushing
their local prototypes of clean samples away from the prototypes of
trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns
a unique and stealthy trigger for each potential target label, and guides the
prototypes of trigger-embedded samples to align closely with the global
prototype of the target label. Experimental results across multiple datasets
and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in
attack success rate compared to traditional backdoor attacks, while preserving
main task accuracy. These results highlight the effectiveness, stealthiness,
and adaptability of BAPFL in PFL.

</details>


### [73] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: 本文提出基于分位数部分效应(QPE)的因果发现方法，通过分析观测分布的形状不对称性来识别因果方向，无需考虑噪声机制或马尔可夫假设。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法需要假设噪声机制或函数形式，限制了应用范围。本文旨在开发一种直接在观测层面利用分布形状不对称性的因果识别方法。

Method: 利用QPE统计量，假设因果效应位于有限线性空间中，通过基函数检验估计的QPE来区分因果方向。多变量情况下使用Fisher信息作为统计量确定因果顺序。

Result: 在大量双变量因果发现数据集上实验证明有效，在多变量合成和真实数据集上验证了使用Fisher信息识别因果顺序的可行性。

Conclusion: QPE方法提供了一种新的因果发现框架，直接从观测分布的形状特性中提取因果信息，扩展了现有因果模型的可识别性结果。

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [74] [Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder](https://arxiv.org/abs/2509.12991)
*Ya Zhou,Yujie Yang,Xiaohan Fan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出了一种简单有效的后训练方法，显著提升ECG基础模型ECGFounder在PTB-XL基准测试上的性能表现


<details>
  <summary>Details</summary>
Motivation: ECG基础模型虽然流行，但其临床适用性受到性能限制，即使在大规模ECG数据集上预训练并在目标数据上微调后，仍不如任务特定模型，主要原因是缺乏有效的后训练策略

Method: 为ECGFounder模型（在700多万ECG记录上预训练）设计了一种后训练方法，包含随机深度和预览线性探测等关键组件

Result: 在PTB-XL基准测试中，相比基线微调策略，宏观AUROC提升1.2%-3.3%，宏观AUPRC提升5.3%-20.9%；仅使用10%训练数据时，宏观AUROC提升9.1%，宏观AUPRC提升34.9%；优于多个最先进方法

Conclusion: 后训练策略有潜力显著改善ECG基础模型性能，该方法更稳定且样本效率更高，有助于ECG领域基础模型的持续发展

Abstract: ECG foundation models are increasingly popular due to their adaptability
across various tasks. However, their clinical applicability is often limited by
performance gaps compared to task-specific models, even after pre-training on
large ECG datasets and fine-tuning on target data. This limitation is likely
due to the lack of an effective post-training strategy. In this paper, we
propose a simple yet effective post-training approach to enhance ECGFounder, a
state-of-the-art ECG foundation model pre-trained on over 7 million ECG
recordings. Experiments on the PTB-XL benchmark show that our approach improves
the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in
macro AUPRC. Additionally, our method outperforms several recent
state-of-the-art approaches, including task-specific and advanced
architectures. Further evaluation reveals that our method is more stable and
sample-efficient compared to the baseline, achieving a 9.1% improvement in
macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the
training data. Ablation studies identify key components, such as stochastic
depth and preview linear probing, that contribute to the enhanced performance.
These findings underscore the potential of post-training strategies to improve
ECG foundation models, and we hope this work will contribute to the continued
development of foundation models in the ECG domain.

</details>


### [75] [Ensemble Visualization With Variational Autoencoder](https://arxiv.org/abs/2509.13000)
*Cenyang Wu,Qinhan Yu,Liang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于变分自编码器(VAE)的数据集合可视化方法，通过在潜在空间中构建结构化概率表示来分析空间数据特征


<details>
  <summary>Details</summary>
Motivation: 传统的数据集合可视化方法难以有效处理高维空间数据特征，需要一种能够构建结构化概率表示并支持分析计算的方法来更好地理解和可视化数据集合

Method: 使用变分自编码器(VAE)进行特征空间转换和无监督学习，将空间特征转换到遵循多元标准高斯分布的潜在空间，从而支持置信区间分析和密度估计

Result: 在天气预报集合数据上的初步结果表明该方法具有有效性和通用性，能够成功构建概率表示并支持分析计算

Conclusion: 该方法为数据集合可视化提供了一种新的有效途径，通过潜在空间的结构化概率表示实现了对数据生成分布的分析和可视化

Abstract: We present a new method to visualize data ensembles by constructing
structured probabilistic representations in latent spaces, i.e.,
lower-dimensional representations of spatial data features. Our approach
transforms the spatial features of an ensemble into a latent space through
feature space conversion and unsupervised learning using a variational
autoencoder (VAE). The resulting latent spaces follow multivariate standard
Gaussian distributions, enabling analytical computation of confidence intervals
and density estimation of the probabilistic distribution that generates the
data ensemble. Preliminary results on a weather forecasting ensemble
demonstrate the effectiveness and versatility of our method.

</details>


### [76] [ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory](https://arxiv.org/abs/2509.13007)
*Qitan Shi,Cheng Jin,Jiawei Zhang,Yuantao Gu*

Main category: cs.LG

TL;DR: ReTrack是一种针对扩散模型的高效数据遗忘方法，通过重要性采样和近似优化目标，在保持生成质量的同时有效移除特定数据的影响


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在训练数据记忆化问题，引发隐私和安全担忧。数据遗忘技术可以在不从头训练的情况下移除特定数据的影响，但需要更高效的解决方案

Method: 使用重要性采样构建更高效的微调损失函数，通过保留主导项进行近似，获得可解释的目标函数，将去噪轨迹重定向到k近邻方向

Result: 在MNIST T-Shirt、CelebA-HQ、CIFAR-10和Stable Diffusion等数据集上达到最先进性能，在遗忘强度和生成质量保持之间取得最佳平衡

Conclusion: ReTrack提供了一种快速有效的扩散模型数据遗忘方法，解决了隐私安全问题，同时保持了模型的生成能力

Abstract: Diffusion models excel at generating high-quality, diverse images but suffer
from training data memorization, raising critical privacy and safety concerns.
Data unlearning has emerged to mitigate this issue by removing the influence of
specific data without retraining from scratch. We propose ReTrack, a fast and
effective data unlearning method for diffusion models. ReTrack employs
importance sampling to construct a more efficient fine-tuning loss, which we
approximate by retaining only dominant terms. This yields an interpretable
objective that redirects denoising trajectories toward the $k$-nearest
neighbors, enabling efficient unlearning while preserving generative quality.
Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show
that ReTrack achieves state-of-the-art performance, striking the best trade-off
between unlearning strength and generation quality preservation.

</details>


### [77] [Spiking Vocos: An Energy-Efficient Neural Vocoder](https://arxiv.org/abs/2509.13049)
*Yukun Chen,Zhaoxi Mu,Andong Li,Peilin Li,Xinyu Yang*

Main category: cs.LG

TL;DR: 提出Spiking Vocos，一种基于脉冲神经网络的超低能耗神经声码器，通过Spiking ConvNeXt模块和振幅捷径路径解决信息瓶颈问题，采用自架构蒸馏策略缩小与ANN的性能差距，能耗仅为传统方法的14.7%


<details>
  <summary>Details</summary>
Motivation: 传统神经声码器在合成速度和保真度方面取得显著进展，但高能耗限制了其在计算资源受限的边缘设备上的实际部署。脉冲神经网络(SNN)因其事件驱动特性而具有高能效，为低资源场景提供了有前景的解决方案

Method: 1) 基于高效Vocos框架构建脉冲神经声码器；2) 设计Spiking ConvNeXt模块减少MAC操作；3) 引入振幅捷径路径保持关键信号动态；4) 采用自架构蒸馏策略进行知识迁移；5) 集成轻量级时序移位模块增强时序维度信息融合

Result: 模型性能与ANN对应版本相当，UTMOS和PESQ分数分别为3.74和3.45，同时能耗仅为传统方法的14.7%

Conclusion: Spiking Vocos成功实现了高性能与超低能耗的结合，为边缘设备上的神经声码器部署提供了可行的解决方案，证明了SNN在音频处理任务中的巨大潜力

Abstract: Despite the remarkable progress in the synthesis speed and fidelity of neural
vocoders, their high energy consumption remains a critical barrier to practical
deployment on computationally restricted edge devices. Spiking Neural Networks
(SNNs), widely recognized for their high energy efficiency due to their
event-driven nature, offer a promising solution for low-resource scenarios. In
this paper, we propose Spiking Vocos, a novel spiking neural vocoder with
ultra-low energy consumption, built upon the efficient Vocos framework. To
mitigate the inherent information bottleneck in SNNs, we design a Spiking
ConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate
an amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to
bridge the performance gap with its Artificial Neural Network (ANN)
counterpart, we introduce a self-architectural distillation strategy to
effectively transfer knowledge. A lightweight Temporal Shift Module is also
integrated to enhance the model's ability to fuse information across the
temporal dimension with negligible computational overhead. Experiments
demonstrate that our model achieves performance comparable to its ANN
counterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while
consuming only 14.7% of the energy. The source code is available at
https://github.com/pymaster17/Spiking-Vocos.

</details>


### [78] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 提出了一种名为Traces Propagation (TP)的前向传播、内存高效、可扩展且完全局部的学习规则，用于脉冲神经网络训练，解决了时空信用分配问题，无需辅助层间矩阵。


<details>
  <summary>Details</summary>
Motivation: 现有SNN训练方法如BPTT与生物神经系统的时空局部性不符，计算和内存需求高；而现有的局部学习规则虽然实现了时间信用分配，但无法解决空间信用分配问题，需要辅助层间矩阵，增加了内存开销并限制了可扩展性。

Method: TP方法结合了资格迹和层间对比损失，无需辅助层间矩阵，实现了完全局部的前向传播学习规则。

Result: TP在NMNIST和SHD数据集上优于其他完全局部学习规则；在更复杂的DVS-GESTURE和DVS-CIFAR10数据集上表现出竞争力，并能有效扩展到VGG-9等更深架构；在Google Speech Commands数据集上适用于实际微调任务。

Conclusion: TP为边缘设备上的高效学习提供了可行方案，具有内存效率高、可扩展性强、完全局部化的特点，适用于实际应用场景。

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [79] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 通过反转1000个正向推理样本构建高质量逆向推理数据集r1k，研究双向推理目标下SFT和DPO对模型对齐的影响。发现混合推理数据会产生冲突监督信号，需要方向感知的对齐策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单向监督微调(SFT)，忽视了不同推理模式之间的复杂相互作用，需要研究双向推理对模型对齐的影响。

Method: 构建逆向推理数据集r1k，通过反转s1k中的1000个正向样本获得。分别进行SFT和DPO实验，分析在双向推理目标下的表现。

Result: 在r1k上进行SFT相比s1k在多个基准测试中准确率提升1.6%-6.8%。但简单混合正逆向数据会削弱方向区分度，DPO虽能部分恢复区分度但会抑制较少偏好的推理路径。

Conclusion: 混合推理数据会引入冲突的监督信号，强调需要开发鲁棒且方向感知的对齐策略来处理双向推理问题。

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [80] [Discovering Mathematical Equations with Diffusion Language Model](https://arxiv.org/abs/2509.13136)
*Xiaoxu Han,Chengzhen Ning,Jinghui Zhong,Fubiao Yang,Yu Wang,Xin Mu*

Main category: cs.LG

TL;DR: DiffuSR是一个基于连续状态扩散语言模型的符号回归预训练框架，通过扩散过程将离散数学符号映射到连续潜在空间，利用交叉注意力机制注入数值数据指导，在标准基准测试中达到与最先进自回归方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 符号回归在科学发现中至关重要，但由于搜索空间巨大以及准确性与复杂性之间的权衡，该任务仍然具有挑战性。现有方法需要改进以更好地发现有效且有意义的数学方程。

Method: DiffuSR采用可训练的嵌入层在扩散过程中将离散数学符号映射到连续潜在空间，通过迭代去噪将初始噪声序列转换为符号方程，并使用交叉注意力机制注入数值数据指导。还设计了有效的推理策略，将logit先验注入遗传编程。

Result: 在标准符号回归基准测试中，DiffuSR达到了与最先进自回归方法竞争的性能，并生成更具可解释性和多样性的数学表达式。

Conclusion: DiffuSR框架通过扩散模型有效解决了符号回归问题，在保持竞争力的同时提供了更好的可解释性和多样性，为科学发现中的数学方程发现提供了新的解决方案。

Abstract: Discovering valid and meaningful mathematical equations from observed data
plays a crucial role in scientific discovery. While this task, symbolic
regression, remains challenging due to the vast search space and the trade-off
between accuracy and complexity. In this paper, we introduce DiffuSR, a
pre-training framework for symbolic regression built upon a continuous-state
diffusion language model. DiffuSR employs a trainable embedding layer within
the diffusion process to map discrete mathematical symbols into a continuous
latent space, modeling equation distributions effectively. Through iterative
denoising, DiffuSR converts an initial noisy sequence into a symbolic equation,
guided by numerical data injected via a cross-attention mechanism. We also
design an effective inference strategy to enhance the accuracy of the
diffusion-based equation generator, which injects logit priors into genetic
programming. Experimental results on standard symbolic regression benchmarks
demonstrate that DiffuSR achieves competitive performance with state-of-the-art
autoregressive methods and generates more interpretable and diverse
mathematical expressions.

</details>


### [81] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: 提出一种从粗到细的课程学习方法，通过先在粗网格上训练GNN，然后逐步引入更高分辨率数据，显著加速收敛并减少50%训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在高分辨率非结构化网格CFD模拟中训练成本过高，需要寻找更高效的训练方法。

Method: 采用课程学习策略，保持GNN模型不变，仅逐步提高训练数据的网格分辨率（从粗到细，最高达30万节点）。

Result: 在保持相当泛化精度的同时，总训练时间减少高达50%，且在模型容量不足时能突破训练平台期。

Conclusion: 课程学习是提升GNN在CFD应用中训练效率的有效方法，无需改变模型架构即可显著加速收敛。

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [82] [Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges](https://arxiv.org/abs/2509.13139)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 本文分析了图异质性对消息传递图神经网络性能的影响，通过添加自循环和平行边来更新异质图，研究GCN在不同异质网络上的性能趋势与图谱特性的关系。


<details>
  <summary>Details</summary>
Motivation: 图异质性对MP-GNNs性能构成挑战，特别是低通滤波器如GCN在异质图上性能下降，需要深入分析其性能与图谱特性的关系。

Method: 通过在异质图上添加自循环和平行边来更新图结构，观察图拉普拉斯矩阵特征值的变化，并研究GCN在不同基准异质网络上的性能趋势。

Result: 研究发现添加自循环和平行边会分别降低和增加图拉普拉斯特征值，GCN性能呈现相应的增加或下降趋势，建立了图谱与低通滤波器性能趋势的联系。

Conclusion: 通过观察低通滤波器的性能趋势可以无缝评估图谱特性，无需昂贵的特征值分解，为理解图异质性对GNN性能的影响提供了新视角。

Abstract: Graph heterophily poses a formidable challenge to the performance of
Message-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters
like Graph Convolutional Networks (GCNs) face performance degradation, which
can be attributed to the blending of the messages from dissimilar neighboring
nodes. The performance of the low-pass filters on heterophilic graphs still
requires an in-depth analysis. In this context, we update the heterophilic
graphs by adding a number of self-loops and parallel edges. We observe that
eigenvalues of the graph Laplacian decrease and increase respectively by
increasing the number of self-loops and parallel edges. We conduct several
studies regarding the performance of GCN on various benchmark heterophilic
networks by adding either self-loops or parallel edges. The studies reveal that
the GCN exhibited either increasing or decreasing performance trends on adding
self-loops and parallel edges. In light of the studies, we established
connections between the graph spectra and the performance trends of the
low-pass filters on the heterophilic graphs. The graph spectra characterize the
essential intrinsic properties of the input graph like the presence of
connected components, sparsity, average degree, cluster structures, etc. Our
work is adept at seamlessly evaluating graph spectrum and properties by
observing the performance trends of the low-pass filters without pursuing the
costly eigenvalue decomposition. The theoretical foundations are also discussed
to validate the impact of adding self-loops and parallel edges on the graph
spectrum.

</details>


### [83] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp是第一个完全开源的金融搜索和推理智能体基准测试，包含三个真实世界金融分析师工作流程任务，由70位金融专家标注，评估了21个模型在635个问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有开源金融数据集缺乏对端到端智能体数据搜索能力的评估，而金融领域需要复杂、多步骤的搜索和时间敏感的数据处理，是评估搜索能力和知识推理的理想场景。

Method: 构建包含三个任务（时间敏感数据获取、简单历史查询、复杂历史调查）的基准测试，聘请70位金融专家进行标注，实施严格的多阶段质量保证流程，涵盖全球和大中华区市场的635个问题。

Result: Grok 4 (web)在全球子集上表现最佳，接近专家级准确率；DouBao (web)在大中华区子集上领先。实验分析显示，为智能体配备网络搜索和金融插件能显著提升性能，模型和工具的国家来源对性能有显著影响。

Conclusion: FinSearchComp通过与真实分析师任务对齐并提供端到端评估，为复杂金融搜索和推理提供了一个专业、高难度的测试平台。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [84] [On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models](https://arxiv.org/abs/2509.13165)
*Alessandro Antonucci,Eric Rossetto,Ivan Duvnjak*

Main category: cs.LG

TL;DR: 该研究探讨了生成式概率分类器中的个体公平性，通过分析后验推断对私有特征扰动的鲁棒性，发现鲁棒性与预测准确性之间存在相关性，并提出使用马尔可夫随机场解决计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 研究个体公平性在生成概率分类器中的表现，探索后验推断对私有特征扰动的鲁棒性，旨在理解鲁棒性与预测准确性之间的关系，为缓解公平性与准确性之间的传统权衡提供新方向。

Method: 使用贝叶斯网络作为基础生成模型，在14个具有公平性关注的数据集上进行实证评估。为解决多私有特征鲁棒性分析的计算复杂度问题，将问题重新表述为辅助马尔可夫随机场中的最可能解释任务。

Result: 实验证实了鲁棒性与预测准确性之间存在相关性的假设，即表现出更大鲁棒性的实例更可能被准确分类。

Conclusion: 研究结果为缓解公平性与准确性之间的传统权衡提供了新的方向，通过鲁棒性分析可以同时提升模型的公平性和预测性能。

Abstract: We investigate individual fairness in generative probabilistic classifiers by
analysing the robustness of posterior inferences to perturbations in private
features. Building on established results in robustness analysis, we
hypothesise a correlation between robustness and predictive accuracy,
specifically, instances exhibiting greater robustness are more likely to be
classified accurately. We empirically assess this hypothesis using a benchmark
of fourteen datasets with fairness concerns, employing Bayesian networks as the
underlying generative models. To address the computational complexity
associated with robustness analysis over multiple private features with
Bayesian networks, we reformulate the problem as a most probable explanation
task in an auxiliary Markov random field. Our experiments confirm the
hypothesis about the correlation, suggesting novel directions to mitigate the
traditional trade-off between fairness and accuracy.

</details>


### [85] [CoVariance Filters and Neural Networks over Hilbert Spaces](https://arxiv.org/abs/2509.13178)
*Claudio Battiloro,Andrea Cavallo,Elvin Isufi*

Main category: cs.LG

TL;DR: 该论文提出了针对无限维希尔伯特空间中信号的卷积学习框架，基于协方差算子构建希尔伯特协方差滤波器和网络，并证明了其能够恢复功能主成分分析。


<details>
  <summary>Details</summary>
Motivation: 现有的协方差神经网络主要针对有限维希尔伯特空间，对于无限维空间中的信号处理缺乏理论支持，需要扩展相关理论和方法。

Method: 构建希尔伯特协方差滤波器(HVFs)和网络(HVNs)，采用基于协方差算子的卷积操作，设计滤波器组和非线性激活函数的堆叠结构，并提出离散化处理程序。

Result: 理论证明经验HVFs能够恢复滤波信号的功能主成分分析(FPCA)，在合成和真实时间序列分类任务中表现出比MLP和FPCA分类器更稳健的性能。

Conclusion: 该框架为无限维希尔伯特空间中的信号处理提供了有效的卷积学习方法，具有理论保证和实际应用价值，在多个领域展现出良好的适应性。

Abstract: CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical
covariance matrix of signals defined over finite-dimensional Hilbert spaces,
motivated by robustness and transferability properties. Yet, little is known
about how these arguments extend to infinite-dimensional Hilbert spaces. In
this work, we take a first step by introducing a novel convolutional learning
framework for signals defined over infinite-dimensional Hilbert spaces,
centered on the (empirical) covariance operator. We constructively define
Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)
as stacks of HVF filterbanks with nonlinear activations. We propose a
principled discretization procedure, and we prove that empirical HVFs can
recover the Functional PCA (FPCA) of the filtered signals. We then describe the
versatility of our framework with examples ranging from multivariate
real-valued functions to reproducing kernel Hilbert spaces. Finally, we
validate HVNs on both synthetic and real-world time-series classification
tasks, showing robust performance compared to MLP and FPCA-based classifiers.

</details>


### [86] [Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy](https://arxiv.org/abs/2509.13185)
*Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Jenq-Neng Hwang,Serge Belongie,Lei Li*

Main category: cs.LG

TL;DR: 本文通过理论分析和实验验证，证明了元学习在有限熵监督设置下比全类训练具有更紧的泛化边界，对标签噪声和异构任务更鲁棒，并提出了MINO元学习框架来提升无监督性能。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明全类训练策略训练的模型在少样本分类任务中可以达到与元学习相当的性能，为了证明元学习的价值，需要建立公平的比较环境。

Method: 建立熵限制监督设置进行公平比较，通过理论分析和实验验证元学习的优势，并提出MINO框架（使用DBSCAN自适应聚类算法和动态头进行无监督任务构建，以及基于稳定性的元缩放器来抵抗标签噪声）。

Result: 元学习在有限熵下更高效，对标签噪声和异构任务更鲁棒，适合无监督任务。MINO框架在多个无监督少样本和零样本任务中表现出色。

Conclusion: 元学习相比全类训练具有理论优势，特别是在无监督场景下，提出的MINO框架有效提升了无监督学习性能。

Abstract: Meta-learning is a powerful paradigm for tackling few-shot tasks. However,
recent studies indicate that models trained with the whole-class training
strategy can achieve comparable performance to those trained with meta-learning
in few-shot classification tasks. To demonstrate the value of meta-learning, we
establish an entropy-limited supervised setting for fair comparisons. Through
both theoretical analysis and experimental validation, we establish that
meta-learning has a tighter generalization bound compared to whole-class
training. We unravel that meta-learning is more efficient with limited entropy
and is more robust to label noise and heterogeneous tasks, making it
well-suited for unsupervised tasks. Based on these insights, We propose MINO, a
meta-learning framework designed to enhance unsupervised performance. MINO
utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for
unsupervised task construction and a stability-based meta-scaler for robustness
against label noise. Extensive experiments confirm its effectiveness in
multiple unsupervised few-shot and zero-shot tasks.

</details>


### [87] [TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data](https://arxiv.org/abs/2509.13192)
*Minghui Lu,Yanyong Huang,Minbo Ma,Dongjie Wang,Xiuwen Yi,Tianrui Li*

Main category: cs.LG

TL;DR: 提出TRUST-FS方法，通过张量分解统一处理多视图无监督特征选择、缺失变量填补和视图权重学习，利用主观逻辑获取可信的跨视图相似性信息


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法存在三个主要问题：无法处理缺失变量场景、填补和特征选择过程分离、缺失数据导致相似图不准确

Method: 提出自适应加权CP分解，在统一张量分解框架中同时进行特征选择、缺失变量填补和视图权重学习，利用主观逻辑获取可信的跨视图相似性信息

Result: 综合实验结果表明该方法优于现有最先进方法

Conclusion: TRUST-FS方法有效解决了多视图数据中缺失变量的问题，通过统一框架提升了特征选择和缺失值填补的性能

Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative
features from multi-view unlabeled data, has attracted increasing research
interest in recent years. Although great efforts have been devoted to MUFS,
several challenges remain: 1) existing methods for incomplete multi-view data
are limited to handling missing views and are unable to address the more
general scenario of missing variables, where some features have missing values
in certain views; 2) most methods address incomplete data by first imputing
missing values and then performing feature selection, treating these two
processes independently and overlooking their interactions; 3) missing data can
result in an inaccurate similarity graph, which reduces the performance of
feature selection. To solve this dilemma, we propose a novel MUFS method for
incomplete multi-view data with missing variables, termed Tensorized Reliable
UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new
adaptive-weighted CP decomposition that simultaneously performs feature
selection, missing-variable imputation, and view weight learning within a
unified tensor factorization framework. By utilizing Subjective Logic to
acquire trustworthy cross-view similarity information, TRUST-FS facilitates
learning a reliable similarity graph, which subsequently guides feature
selection and imputation. Comprehensive experimental results demonstrate the
effectiveness and superiority of our method over state-of-the-art methods.

</details>


### [88] [B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data](https://arxiv.org/abs/2509.13202)
*Francis Ndikum Nji,Vandana Janaja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合双向时序图注意力变换器(B-TGAT)的时间分布混合U-Net自编码器，用于高维多变量时空气候数据的高效时序聚类


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理高维时空气候数据时难以同时捕捉局部和全局时间关系并保持空间上下文，需要新的方法来应对复杂的时间依赖、演化的空间交互和非平稳动态

Method: 使用ConvLSTM2D模块提取联合时空特征，通过U-Net跳跃连接保持多尺度空间细节，在瓶颈处集成基于图的空间建模和注意力驱动的时间编码(B-TGAT)，实现自适应时间邻居加权和长短距离依赖捕捉

Result: 在三个不同的时空气候数据集上实验表明，相比最先进的基线方法，该方法在聚类可分性、时间稳定性和与已知气候转变的对齐方面表现更优

Conclusion: ConvLSTM2D、U-Net跳跃连接和B-TGAT的集成提升了时序聚类性能，同时为复杂的时空变异性提供可解释的见解，推动了方法学发展和气候科学应用

Abstract: Clustering high-dimensional multivariate spatiotemporal climate data is
challenging due to complex temporal dependencies, evolving spatial
interactions, and non-stationary dynamics. Conventional clustering methods,
including recurrent and convolutional models, often struggle to capture both
local and global temporal relationships while preserving spatial context. We
present a time-distributed hybrid U-Net autoencoder that integrates a
Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient
temporal clustering of multidimensional spatiotemporal climate datasets. The
encoder and decoder are equipped with ConvLSTM2D modules that extract joint
spatial--temporal features by modeling localized dynamics and spatial
correlations over time, and skip connections that preserve multiscale spatial
details during feature compression and reconstruction. At the bottleneck,
B-TGAT integrates graph-based spatial modeling with attention-driven temporal
encoding, enabling adaptive weighting of temporal neighbors and capturing both
short and long-range dependencies across regions. This architecture produces
discriminative latent embeddings optimized for clustering. Experiments on three
distinct spatiotemporal climate datasets demonstrate superior cluster
separability, temporal stability, and alignment with known climate transitions
compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net
skip connections, and B-TGAT enhances temporal clustering performance while
providing interpretable insights into complex spatiotemporal variability,
advancing both methodological development and climate science applications.

</details>


### [89] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: HAM是一种新颖的分层适配器合并框架，通过动态合并不同任务的适配器来解决持续学习中的灾难性遗忘问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中灾难性遗忘问题，特别是当面对新数据分布时，现有参数高效微调方法在动态学习场景和长任务序列中面临扩展性和干扰挑战。

Method: 提出分层适配器合并(HAM)框架，维护固定组别层次化整合新适配器，为每个任务训练低秩适配器和重要性标量，基于适配器相似性动态分组，在组内进行适配器修剪、缩放和合并。

Result: 在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进方法，特别是在任务数量增加时表现更加突出。

Conclusion: HAM框架通过动态合并适配器的分层方法，有效解决了持续学习中的扩展性问题，在多个任务场景下展现出优越性能。

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [90] [Density-Aware Farthest Point Sampling](https://arxiv.org/abs/2509.13213)
*Paolo Climaco,Jochen Garcke*

Main category: cs.LG

TL;DR: 提出了一种新的密度感知最远点采样方法(DA-FPS)，用于在标签数据有限的情况下选择训练集，通过最小化加权填充距离来降低回归模型的预测误差上界。


<details>
  <summary>Details</summary>
Motivation: 在机器学习回归任务中，由于计算约束或标注成本高昂，标签训练数据有限，需要从无标签数据中选择合适的训练集来平衡性能与效率。

Method: 推导了Lipschitz连续回归模型预测误差的上界，该上界与训练集的加权填充距离线性相关。提出了DA-FPS采样方法，该方法通过数据特征估计加权填充距离，并提供近似最小化解。

Result: 在三个数据集上使用两种回归模型进行实验，结果显示DA-FPS相比其他采样策略显著降低了平均绝对预测误差。

Conclusion: DA-FPS是一种有效的被动且模型无关的采样方法，能够通过优化加权填充距离来提升回归模型在有限标签数据场景下的性能。

Abstract: We focus on training machine learning regression models in scenarios where
the availability of labeled training data is limited due to computational
constraints or high labeling costs. Thus, selecting suitable training sets from
unlabeled data is essential for balancing performance and efficiency. For the
selection of the training data, we focus on passive and model-agnostic sampling
methods that only consider the data feature representations. We derive an upper
bound for the expected prediction error of Lipschitz continuous regression
models that linearly depends on the weighted fill distance of the training set,
a quantity we can estimate simply by considering the data features. We
introduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel sampling
method. We prove that DA-FPS provides approximate minimizers for a data-driven
estimation of the weighted fill distance, thereby aiming at minimizing our
derived bound. We conduct experiments using two regression models across three
datasets. The results demonstrate that DA-FPS significantly reduces the mean
absolute prediction error compared to other sampling strategies.

</details>


### [91] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: FOSSIL是一个统一的样本权重框架，通过单一可解释公式整合类别不平衡校正、难度感知课程、增强惩罚和预热动态，在小样本和不平衡数据场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决小样本和不平衡数据场景中的挑战，现有方法如过采样、焦点损失或元加权只处理孤立方面且脆弱复杂，需要统一框架。

Method: 提出FOSSIL框架，通过样本敏感重要性学习，将类别不平衡校正、难度感知课程学习、增强惩罚和预热动态集成到单一公式中，无需架构更改。

Result: 在合成和真实数据集上一致优于ERM、课程学习和元加权基线方法，并提供基于遗憾的理论保证。

Conclusion: FOSSIL提供了一个理论保证的统一框架，在小样本和不平衡数据场景中实现稳定性能提升，具有很好的实用性和可解释性。

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [92] [On the Out-of-Distribution Backdoor Attack for Federated Learning](https://arxiv.org/abs/2509.13219)
*Jiahao Xu,Zikai Zhang,Rui Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新型联邦学习后门攻击OBA，使用OOD数据作为毒化样本和触发器，并开发了SoDa方法提高隐蔽性。同时提出了BNGuard防御机制，利用批归一化层统计偏差检测恶意模型更新。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击需要可见触发器和物理修改目标对象，攻击场景受限。为了解决这一局限性，需要开发更隐蔽、更实用的后门攻击方法。

Method: OBA使用OOD数据同时作为毒化样本和触发器；SoDa通过正则化恶意本地模型的幅度和方向来提高隐蔽性；BNGuard利用批归一化层运行统计的显著偏差来识别恶意模型更新。

Result: 实验证明OBA能有效绕过最先进的防御机制，同时保持主任务的高准确率。BNGuard在各种设置下都能有效防御SoDa攻击。

Conclusion: 该研究提出了更隐蔽的后门攻击方法和相应的防御机制，揭示了联邦学习系统中的安全漏洞，并为增强联邦学习的后门鲁棒性提供了有效解决方案。

Abstract: Traditional backdoor attacks in federated learning (FL) operate within
constrained attack scenarios, as they depend on visible triggers and require
physical modifications to the target object, which limits their practicality.
To address this limitation, we introduce a novel backdoor attack prototype for
FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which
uses OOD data as both poisoned samples and triggers simultaneously. Our
approach significantly broadens the scope of backdoor attack scenarios in FL.
To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,
which regularizes both the magnitude and direction of malicious local models
during local training, aligning them closely with their benign versions to
evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively
circumvents state-of-the-art defenses while maintaining high accuracy on the
main task.
  To address this security vulnerability in the FL system, we introduce
$\mathtt{BNGuard}$, a new server-side defense method tailored against
$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data
causes significant deviations in the running statistics of batch normalization
layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and
exclude them from aggregation, thereby enhancing the backdoor robustness of FL.
Extensive experiments across various settings show the effectiveness of
$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available
at https://github.com/JiiahaoXU/SoDa-BNGuard.

</details>


### [93] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: SPO是一种单流策略优化方法，通过持久化KL自适应值跟踪器和全局优势归一化，解决了传统分组方法GRPO的退化组问题和可扩展性限制，在数学推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略优化方法（如GRPO）存在两个关键缺陷：频繁出现的退化组会消除学习信号，同步障碍限制了可扩展性。需要一种更稳定、高效的优化方法。

Method: SPO采用单流设计，使用持久化的KL自适应值跟踪器替代每组的基线，并在整个批次中全局归一化优势值，为每个样本提供稳定、低方差的学习信号。

Result: 在Qwen3-8B模型上，SPO在五个困难数学基准测试中平均maj@32比GRPO提升3.4个百分点，在BRUMO 25上提升7.3pp，AIME 25上提升4.4pp，HMMT 25上提升3.3pp。

Conclusion: SPO的成功挑战了为RL算法增加复杂性的主流趋势，表明通过基本原则而非架构变通可以推动LLM推理的下一个进步浪潮。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [94] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 该论文提出了一种将重复推理片段转化为可重用"行为"的方法，通过模型自身的元认知分析创建"行为手册"，在推理时提供相关行为提示，显著减少推理token数量并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决多步问题时经常重复推导相同的中间步骤，导致token使用量和延迟增加，上下文窗口饱和限制了探索能力。需要一种机制来避免重复推理，提高效率。

Method: 通过模型的元认知分析将重复推理片段转化为简洁可重用的"行为"（名称+指令），存储在"行为手册"中。在推理时提供相关行为上下文提示，或通过监督微调将其蒸馏到参数中。

Result: 1) 行为条件推理：减少推理token达46%，同时保持或提高准确率；2) 行为引导自改进：无参数更新情况下准确率提高10%；3) 行为条件SFT：比普通SFT更有效地将非推理模型转化为推理模型。

Conclusion: 将缓慢的推导过程转化为快速的过程提示，使LLM能够记住如何推理，而不仅仅是记住结论，显著提高了推理效率和效果。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [95] [Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning](https://arxiv.org/abs/2509.13240)
*Bo Yin,Xingyi Yang,Xinchao Wang*

Main category: cs.LG

TL;DR: NoRA是首个直接调整预训练Transformer模型中非线性激活函数的参数高效微调框架，通过可学习有理函数替代固定激活函数，仅需更新0.4%参数即可达到或超越全参数微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法主要调整权重矩阵而保持激活函数固定，限制了模型适应能力。本文旨在探索激活函数空间的调优作为权重调优的补充方案。

Method: 提出NoRA框架，用可学习有理函数替换固定激活函数，对分子和分母系数应用结构化低秩更新，采用分组设计实现局部适应和稳定性提升。

Result: 在CIFAR数据集上仅更新0.02M参数(0.4%)即达到或超越全微调性能，准确率提升0.17%-0.27%；与LoRA结合(NoRA++)在LLaMA3-8B指令调优中持续提升生成质量，MMLU平均增益0.3%-0.8%。

Conclusion: 激活空间调优是权重调优的互补且高度参数高效的替代方案，应将激活函数作为模型适应的一等公民对象。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt
weight matrices while keeping activation functions fixed. We introduce
\textbf{NoRA}, the first PEFT framework that directly adapts nonlinear
activation functions in pretrained transformer-based models. NoRA replaces
fixed activations with learnable rational functions and applies structured
low-rank updates to numerator and denominator coefficients, with a group-wise
design that localizes adaptation and improves stability at minimal cost. On
vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds
full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving
accuracy gains of +0.17\% and +0.27\%. When combined with LoRA
(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets
by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++
consistently improves generation quality, yielding average MMLU gains of
+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We
further show that NoRA constrains adaptation to a low-dimensional functional
subspace, implicitly regularizing update magnitude and direction. These results
establish activation-space tuning as a complementary and highly
parameter-efficient alternative to weight-based PEFT, positioning activation
functions as first-class objects for model adaptation.

</details>


### [96] [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](https://arxiv.org/abs/2509.13262)
*Zhizhong Zhao,Ke Chen*

Main category: cs.LG

TL;DR: 提出了一种后处理单前向传播框架SPC-UQ，通过分割点分析(SPA)分解预测残差，计算平均绝对残差(MARs)和自一致性差异分数(SDS)，联合捕捉任意性和认知性不确定性，无需重新训练预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法要么计算成本高（如贝叶斯或集成方法），要么只能提供部分任务特定的估计（如单前向传播技术），需要一种高效且全面的不确定性量化框架。

Method: 使用分割点分析(SPA)将预测残差分解为上下子集，计算各侧的平均绝对残差(MARs)。通过自一致性差异分数(SDS)进行细粒度认知性估计。对于回归任务使用分位数回归获得预测区间，对于分类任务基于SPA校准身份调整softmax输出。

Result: 在多种回归和分类基准测试中，该框架匹配或超越了多个最先进的不确定性量化方法，同时计算开销极小。

Conclusion: SPC-UQ框架提供了一种高效、全面的不确定性量化解决方案，能够同时处理任意性和认知性不确定性，适用于各种深度学习任务。

Abstract: Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet
existing methods are either computationally intensive, such as Bayesian or
ensemble methods, or provide only partial, task-specific estimates, such as
single-forward-pass techniques. In this paper, we propose a post-hoc
single-forward-pass framework that jointly captures aleatoric and epistemic
uncertainty without modifying or retraining pretrained models. Our method
applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals
into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)
on each side. We prove that, under ideal conditions, the total MAR equals the
harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency
Discrepancy Score} (SDS) for fine-grained epistemic estimation across
regression and classification. For regression, side-specific quantile
regression yields prediction intervals with improved empirical coverage, which
are further calibrated via SDS. For classification, when calibration data are
available, we apply SPA-based calibration identities to adjust the softmax
outputs and then compute predictive entropy on these calibrated probabilities.
Extensive experiments on diverse regression and classification benchmarks
demonstrate that our framework matches or exceeds several state-of-the-art UQ
methods while incurring minimal overhead.
  Our source code is available at https://github.com/zzz0527/SPC-UQ.

</details>


### [97] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: 提出了JANUS框架，通过局部特征流形对齐和全局结构语义一致性，解决现有节点注入攻击方法的局部短视问题，显著提升了攻击效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络节点注入攻击方法依赖间接代理指标实现隐蔽性，缺乏对注入内容本质特征的考虑，且仅关注局部结构模仿，存在局部短视问题。

Method: 提出双约束隐蔽节点注入框架JANUS：局部层面使用特征流形对齐策略实现特征空间几何一致性；全局层面引入结构化潜变量并通过最大化互信息确保注入结构与原始图语义模式一致；将注入攻击建模为序列决策过程，使用强化学习优化。

Result: 在多个标准数据集上的实验表明，JANUS框架在攻击效果和隐蔽性方面均显著优于现有方法。

Conclusion: JANUS框架通过同时考虑局部和全局结构一致性，有效解决了节点注入攻击中的隐蔽性问题，为图神经网络安全研究提供了新思路。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [98] [LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt](https://arxiv.org/abs/2509.13268)
*Rodrigo M Carrillo-Larco*

Main category: cs.LG

TL;DR: 大型语言模型通过思维链提示和参数高效微调，仅基于文本描述就能准确预测食物营养含量，为低负担的饮食监测提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI营养估算工具主要依赖图像输入，本研究探索大型语言模型是否能够仅通过文本描述准确预测营养值，以简化饮食监测过程。

Method: 使用NHANES青少年24小时饮食回顾数据，采用10-shot思维链提示的开源量化LLM，并应用参数高效微调(PEFT)技术来预测能量和五种宏量营养素。

Result: 原始LLM预测效果差（能量MAE为652.08，Lin's CCC<0.46），但经过微调的模型表现显著改善（能量MAE降至171.34-190.90，Lin's CCC>0.89）。

Conclusion: 通过思维链提示和PEFT微调，开源LLM仅基于文本输入就能准确预测饮食营养值，这为开发低负担的文本式饮食监测工具提供了可能。

Abstract: BACKGROUND: Most artificial intelligence tools used to estimate nutritional
content rely on image input. However, whether large language models (LLMs) can
accurately predict nutritional values based solely on text descriptions of
foods consumed remains unknown. If effective, this approach could enable
simpler dietary monitoring without the need for photographs. METHODS: We used
24-hour dietary recalls from adolescents aged 12-19 years in the National
Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM
was prompted using a 10-shot, chain-of-thought approach to estimate energy and
five macronutrients based solely on text strings listing foods and their
quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate
whether predictive accuracy improved. NHANES-calculated values served as the
ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber
and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,
mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean
absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across
endpoints. In contrast, the fine-tuned model performed substantially better,
with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC
exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a
chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed
solely to text input can accurately predict energy and macronutrient values
from 24-hour dietary recalls. This approach holds promise for low-burden,
text-based dietary monitoring tools.

</details>


### [99] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor是一种后训练方法，通过生成高不确定性任务、RFT冷启动和DUPO算法，使开源模型在复杂信息搜索任务中达到与专有代理相当的性能。


<details>
  <summary>Details</summary>
Motivation: 超越人类认知限制是LLM训练的关键前沿。专有代理系统在复杂信息搜索基准上表现出超人类能力，而开源模型缺乏系统化处理极端不确定性的推理模式。

Method: 通过结构化采样和信息模糊化生成新颖的高不确定性任务，采用RFT冷启动和高效的代理RL训练算法DUPO（复制采样策略优化）的集成管道。

Result: WebSailor在复杂信息搜索任务中显著优于所有开源代理，匹配专有代理的性能，缩小了能力差距。

Conclusion: 该方法成功地将专有代理的关键能力移植到开源模型中，证明了系统化不确定性处理在复杂信息搜索中的重要性。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [100] [A Scalable Architecture for Efficient Multi-bit Fully Homomorphic Encryption](https://arxiv.org/abs/2509.12676)
*Jiaao Ma,Ceyu Xu,Lisa Wu Wills*

Main category: cs.AR

TL;DR: Taurus是一个硬件加速器，通过支持10位密文、优化FFT单元和内存带宽，显著提升了多比特TFHE计算的效率，在隐私保护计算领域实现了突破性性能提升。


<details>
  <summary>Details</summary>
Motivation: 在云计算时代，隐私保护计算卸载至关重要。虽然多比特TFHE方案提供了更好的灵活性和性能，但当前实现受限于较窄的数值表示范围，无法满足需要更宽数值表示的应用需求。

Method: 设计了Taurus硬件加速器，支持高达10位的密文，采用新颖的FFT单元和通过密钥重用策略优化内存带宽。同时提出了带有操作去重功能的编译器来改善内存利用率。

Result: 实验结果显示，Taurus相比CPU实现了2600倍加速，相比GPU实现了1200倍加速，比之前最先进的TFHE加速器快7倍。首次实现了GPT-2等大型语言模型的隐私保护推理。

Conclusion: Taurus的突破使隐私保护计算在云环境中变得更加实用和可扩展，为敏感数据的安全处理提供了高效的解决方案。

Abstract: In the era of cloud computing, privacy-preserving computation offloading is
crucial for safeguarding sensitive data. Fully Homomorphic Encryption (FHE)
enables secure processing of encrypted data, but the inherent computational
complexity of FHE operations introduces significant computational overhead on
the server side. FHE schemes often face a tradeoff between efficiency and
versatility. While the CKKS scheme is highly efficient for polynomial
operations, it lacks the flexibility of the binary TFHE (Torus-FHE) scheme,
which offers greater versatility but at the cost of efficiency. The recent
multi-bit TFHE extension offers greater flexibility and performance by
supporting native non-polynomial operations and efficient integer processing.
However, current implementations of multi-bit TFHE are constrained by its
narrower numeric representation, which prevents its adoption in applications
requiring wider numeric representations.
  To address this challenge, we introduce Taurus, a hardware accelerator
designed to enhance the efficiency of multi-bit TFHE computations. Taurus
supports ciphertexts up to 10 bits by leveraging novel FFT units and optimizing
memory bandwidth through key reuse strategies. We also propose a compiler with
operation deduplication to improve memory utilization. Our experiment results
demonstrate that Taurus achieves up to 2600x speedup over a CPU, 1200x speedup
over a GPU, and up to 7x faster compared to the previous state-of-the-art TFHE
accelerator. Moreover, Taurus is the first accelerator to demonstrate
privacy-preserving inference with large language models such as GPT-2. These
advancements enable more practical and scalable applications of
privacy-preserving computation in cloud environments.

</details>


### [101] [HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large Language Models Inference](https://arxiv.org/abs/2509.12993)
*Cenlin Duan,Jianlei Yang,Rubing Yang,Yikun Wang,Yiou Wang,Lingkun Long,Yingjie Qi,Xiaolin He,Ao Zhou,Xueyan Wang,Weisheng Zhao*

Main category: cs.AR

TL;DR: HPIM是一个针对大语言模型推理的内存中心异构内存处理加速器，通过SRAM-PIM和HBM-PIM子系统协同工作，实现了22.8倍于A100 GPU的性能提升


<details>
  <summary>Details</summary>
Motivation: 传统GPU等计算中心加速器在大语言模型自回归解码阶段存在严重的资源利用不足和内存带宽瓶颈问题，无法满足内存密集型工作负载的需求

Method: 采用软硬件协同设计方法，结合专用编译器框架和异构硬件架构，根据操作特性智能分配工作负载：延迟敏感的注意力操作映射到SRAM-PIM子系统，权重密集的GEMV计算分配到HBM-PIM子系统

Result: 通过精确周期模拟器评估，HPIM相比NVIDIA A100 GPU实现了最高22.8倍的加速，且优于同期其他基于PIM的加速器

Conclusion: HPIM证明了作为大规模LLM推理加速解决方案的高度实用性和可扩展性潜力，有效解决了自回归解码阶段的串行依赖问题

Abstract: The deployment of large language models (LLMs) presents significant
challenges due to their enormous memory footprints, low arithmetic intensity,
and stringent latency requirements, particularly during the autoregressive
decoding stage. Traditional compute-centric accelerators, such as GPUs, suffer
from severe resource underutilization and memory bandwidth bottlenecks in these
memory-bound workloads. To overcome these fundamental limitations, we propose
HPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)
accelerator that integrates SRAM-PIM and HBM-PIM subsystems designed
specifically for LLM inference. HPIM employs a software-hardware co-design
approach that combines a specialized compiler framework with a heterogeneous
hardware architecture. It intelligently partitions workloads based on their
characteristics: latency-critical attention operations are mapped to the
SRAM-PIM subsystem to exploit its ultra-low latency and high computational
flexibility, while weight-intensive GEMV computations are assigned to the
HBM-PIM subsystem to leverage its high internal bandwidth and large storage
capacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy
across SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,
thereby significantly mitigating serial dependency of the autoregressive
decoding stage. Comprehensive evaluations using a cycle-accurate simulator
demonstrate that HPIM significantly outperforms state-of-the-art accelerators,
achieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.
Moreover, HPIM exhibits superior performance over contemporary PIM-based
accelerators, highlighting its potential as a highly practical and scalable
solution for accelerating large-scale LLM inference.

</details>


### [102] [Orthrus: Dual-Loop Automated Framework for System-Technology Co-Optimization](https://arxiv.org/abs/2509.13029)
*Yi Ren,Baokang Peng,Chenhao Xue,Kairong Guo,Yukun Wang,Guoyao Cheng,Yibo Lin,Lining Zhang,Guangyu Sun*

Main category: cs.AR

TL;DR: Orthrus是一个双循环自动化框架，通过系统级和技术级协同优化，在7nm技术上实现了12.5%的延迟降低和61.4%的功耗节省


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律收益递减，系统技术协同优化(STCO)成为维持VLSI行业扩展趋势的有前景方法，但现有研究缺乏有效的STCO方法学，特别是在处理设计层次间的信息鸿沟和探索跨层设计空间方面

Method: 提出Orthrus双循环框架：系统级使用新颖机制优先优化关键标准单元，并通过贝叶斯优化探索的帕累托前沿法向指导技术级优化；技术级利用系统感知洞察优化标准单元库，采用神经网络辅助的增强差分进化算法优化技术参数

Result: 在7nm技术上的实验结果表明，与基线方法相比，Orthrus在等功耗下实现12.5%的延迟降低，在等延迟下实现61.4%的功耗节省，建立了新的STCO帕累托前沿

Conclusion: Orthrus框架成功解决了系统技术协同优化中的关键挑战，通过双循环自动化方法实现了显著的性能提升，为VLSI设计的持续扩展提供了有效解决方案

Abstract: With the diminishing return from Moore's Law, system-technology
co-optimization (STCO) has emerged as a promising approach to sustain the
scaling trends in the VLSI industry. By bridging the gap between system
requirements and technology innovations, STCO enables customized optimizations
for application-driven system architectures. However, existing research lacks
sufficient discussion on efficient STCO methodologies, particularly in
addressing the information gap across design hierarchies and navigating the
expansive cross-layer design space. To address these challenges, this paper
presents Orthrus, a dual-loop automated framework that synergizes system-level
and technology-level optimizations. At the system level, Orthrus employs a
novel mechanism to prioritize the optimization of critical standard cells using
system-level statistics. It also guides technology-level optimization via the
normal directions of the Pareto frontier efficiently explored by Bayesian
optimization. At the technology level, Orthrus leverages system-aware insights
to optimize standard cell libraries. It employs a neural network-assisted
enhanced differential evolution algorithm to efficiently optimize technology
parameters. Experimental results on 7nm technology demonstrate that Orthrus
achieves 12.5% delay reduction at iso-power and 61.4% power savings at
iso-delay over the baseline approaches, establishing new Pareto frontiers in
STCO.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [103] [IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph Isomorphism](https://arxiv.org/abs/2509.12208)
*Boran Zhao,Zihang Yuan,Yanbin Hu,Haiming Zhai,Haoruo Zhang,Wenzhe Zhao,Tian Xia,Pengju Ren*

Main category: cs.DC

TL;DR: IsoSched是首个支持抢占式多DNN调度的TSS架构框架，通过ILP建模、子图同构、负载均衡和优化算法，在延迟、吞吐量和能效方面优于现有LTS-PRM方法


<details>
  <summary>Details</summary>
Motivation: 现有Tile Spatial Scheduling (TSS) 方法缺乏对抢占的支持，而传统抢占方案基于Layer Temporal Scheduling (LTS) 存在高开销问题，无法满足新兴应用中关键任务的严格延迟要求

Method: 1) 将复杂拓扑图调度建模为整数线性规划和子图同构问题；2) 应用Layer Concatenate and Split (LCS) 进行瓦片流水线负载均衡；3) 使用基于Ullmann算法和蒙特卡洛树搜索加速子图匹配，采用压缩稀疏行编码减少内存使用

Result: IsoSched在Latency-Bound Throughput (LBT)、加速比和能效方面优于LTS-PRM方法（PREMA、Planaria、CD-MSA、MoCA），在不同任务复杂度下比TSS-NPRM（HASP）实现更高的关键任务满足率

Conclusion: IsoSched成功解决了TSS架构中抢占支持的挑战，为复杂拓扑的多DNN并发执行提供了高效调度框架，特别适合自动驾驶等对延迟敏感的应用场景

Abstract: Deploying deep neural network (DNN) accelerators with Layer Temporal
Scheduling (LTS) often incurs significant overheads (e.g., energy and latency),
as intermediate activations must be cached in DRAM. To alleviate this, Tile
Spatial Scheduling (TSS) reduces such costs by fragmenting inter-layer data
into smaller tiles communicated via on-chip links.However, many emerging
applications require concurrent execution of multiple DNNs with complex
topologies, where critical tasks must preempt others to meet stringent latency
requirements (e.g., in autonomous driving, obstacle detection must complete
within tens of milliseconds). Existing TSS works lack support for preemption,
while prior preemption schemes rely on LTS and thus inherit its overheads. This
highlights the need for preemptive and efficient TSS-based frameworks. Yet,
realizing such systems is challenging due to the complexity of enabling
preemption in graphs with large-scale topologies (e.g., modern large language
models may contain tens of thousands of edges). To tackle this, we present
IsoSched, the first framework enabling preemptive multi-DNN scheduling on TSS
architecture. IsoSched first formulates scheduling of complex-topology graphs
as an integer-linear program (ILP) and subgraph isomorphism problem; second, it
applies Layer Concatenate and Split (LCS) for load balancing in tile pipelines;
third, it employs an Ullmann-based algorithm enhanced by Monte Carlo Tree
Search (MCTS) to accelerate subgraph matching, and uses compact matrix encoding
(i.e., Compressed Sparse Row, CSR) to reduce memory usage. IsoSched outperforms
LTS-PRM approaches (i.e., PREMA, Planaria, CD-MSA, MoCA) in Latency-Bound
Throughput (LBT), speedup, and energy efficiency, and achieves higher critical
task satisfaction than TSS-NPRM (i.e., HASP) across varying task complexities.

</details>


### [104] [A Proposal for High-Level Architectural Model Capable of Expressing Various Data Collaboration Platform and Data Space Concepts](https://arxiv.org/abs/2509.12210)
*Masaru Dobashi,Kohei Toshimitsu,Hirotsugu Seike,Miki Kanno,Genki Horie,Noboru Koshizuka*

Main category: cs.DC

TL;DR: 提出了数据空间高层架构模型(DS-HLAM)，用于表达跨区域实施的各种数据协作平台，通过有限状态自动机理论形式化成功条件，确保互操作性同时满足数字主权要求。


<details>
  <summary>Details</summary>
Motivation: 为了解决不同数据协作平台之间的互操作性问题，同时确保数字主权要求得到满足，需要建立一个统一的、数学严谨的架构模型。

Method: 采用有限状态自动机理论来形式化定义数据空间架构，建立数学严谨的定义和成功条件，构建数据空间高层架构模型(DS-HLAM)。

Result: 开发了一个能够表达多样化数据协作平台的统一框架，实现了跨区域实施的互操作性，同时保证了数字主权要求的维护。

Conclusion: DS-HLAM框架为数据协作平台提供了一个数学严谨的建模方法，成功解决了互操作性与数字主权之间的平衡问题，为跨区域数据协作提供了理论基础。

Abstract: This paper proposes "Data Space High-Level Architecture Model" (DS-HLAM) for
expressing diverse data collaboration platforms across regional
implementations. The framework introduces mathematically rigorous definitions
with success conditions formalized through finite state automata theory,
enabling interoperability while preserving digital sovereignty requirements.

</details>


### [105] [TinyServe: Query-Aware Cache Selection for Efficient LLM Serving](https://arxiv.org/abs/2509.12211)
*Dong Liu,Yanxuan Yu*

Main category: cs.DC

TL;DR: TinyServe是一个轻量级LLM服务系统，通过查询感知页面选择机制和结构化KV稀疏性，实现了3.4倍加速和2倍内存节省，且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自回归解码过程中，KV缓存访问带来高内存和延迟开销，特别是在资源受限硬件上部署时效率低下。

Method: 提出查询感知页面选择机制，利用边界框元数据估计查询与KV缓存块的注意力相关性；开发融合CUDA内核，集成页面评分、稀疏内存访问和掩码注意力；支持结构化KV稀疏性和插件式token选择。

Result: 实验显示达到3.4倍加速和超过2倍内存节省，精度损失可忽略；缓存重用、页面命中率和多GPU扩展分析证实了其实用性。

Conclusion: TinyServe作为系统级设计，为资源受限硬件上的LLM训练和推理研究提供了高效解决方案，无需模型修改即可显著提升效率。

Abstract: Serving large language models (LLMs) efficiently remains challenging due to
the high memory and latency overhead of key-value (KV) cache access during
autoregressive decoding. We present \textbf{TinyServe}, a lightweight and
extensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)
with support for structured KV sparsity, plugin-based token selection, and
hardware-efficient attention kernels. Unlike prior simulation frameworks,
TinyServe executes real-time decoding with configurable sparsity strategies and
fine-grained instrumentation.
  To reduce decoding cost, we introduce a \textit{query-aware page selection}
mechanism that leverages bounding-box metadata to estimate attention relevance
between the query and KV cache blocks. This enables selective KV loading with
minimal overhead and no model modifications. Our fused CUDA kernel integrates
page scoring, sparse memory access, and masked attention in a single pass.
  Experiments show that TinyServe achieves up to \textbf{3.4x} speedup and over
\textbf{2x} memory savings with negligible accuracy drop. Additional analysis
of cache reuse, page hit rate, and multi-GPU scaling confirms its practicality
as an efficient system-level design for LLM training and inference research on
resource-constrained hardware.

</details>


### [106] [Research on fault diagnosis and root cause analysis based on full stack observability](https://arxiv.org/abs/2509.12231)
*Jian Hou*

Main category: cs.DC

TL;DR: 本文综述了AIOps中根因分析的两大主流方法，提出了融合动态因果发现和多模态融合的KylinRCA框架，通过时序因果发现和跨模态图学习实现可解释的故障诊断。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和大规模数据中心的发展，系统故障频繁且具有级联传播特性，需要高效、准确、可解释的根因分析方法来处理可观测性数据。

Method: 提出KylinRCA框架，结合动态因果发现和多模态融合思想：使用时序因果发现描绘传播链，通过跨模态图学习实现全局根因定位和类型识别，结合基于掩码的解释方法输出可审计证据链。

Result: 设计了多维实验方案，明确了评估指标，讨论了工程挑战，为全栈可观测性下的故障诊断提供了有效解决方案。

Conclusion: KylinRCA框架整合了现有方法的优势，提供了可解释的根因分析解决方案，能够有效处理大规模复杂系统中的故障诊断问题。

Abstract: With the rapid development of cloud computing and ultra-large-scale data
centers, the scale and complexity of systems have increased significantly,
leading to frequent faults that often show cascading propagation. How to
achieve efficient, accurate, and interpretable Root Cause Analysis (RCA) based
on observability data (metrics, logs, traces) has become a core issue in AIOps.
This paper reviews two mainstream research threads in top conferences and
journals over the past five years: FaultInsight[1] focusing on dynamic causal
discovery and HolisticRCA[2] focusing on multi-modal/cross-level fusion, and
analyzes the advantages and disadvantages of existing methods. A KylinRCA
framework integrating the ideas of both is proposed, which depicts the
propagation chain through temporal causal discovery, realizes global root cause
localization and type identification through cross-modal graph learning, and
outputs auditable evidence chains combined with mask-based explanation methods.
A multi-dimensional experimental scheme is designed, evaluation indicators are
clarified, and engineering challenges are discussed, providing an effective
solution for fault diagnosis under full-stack observability.

</details>


### [107] [Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization](https://arxiv.org/abs/2509.12232)
*Gianmarco Accordi,Jens Domke,Theresa Pollinger,Davide Gadioli,Gianluca Palermo*

Main category: cs.DC

TL;DR: 本文评估了现代CPU架构的向量化性能，通过分子对接应用案例研究，比较了编译器自动向量化和显式向量化在x86和ARM平台上的性能表现。


<details>
  <summary>Details</summary>
Motivation: HPC领域新CPU架构的向量化能力提升需要代码优化才能达到峰值性能，这对性能可移植性提出了挑战，需要研究如何在不同架构上实现高性能科学计算应用。

Method: 选择分子对接应用作为案例研究，评估编译器自动向量化和显式向量化在现代长向量CPU上的性能表现，分析代码、编译器和硬件之间的复杂交互。

Result: 研究显示某些代码转换可以实现可移植的自动向量化，性能接近显式向量化。x86 CPU由于更宽的向量化单元通常获得更高执行性能，但ARM架构在能耗和成本效益方面表现更具竞争力。

Conclusion: 研究为科学计算应用在HPC领域的未来发展提供了技术挑战、架构趋势和优化策略的重要见解，强调了在不同CPU架构上平衡性能和能效的重要性。

Abstract: Recent trends in the HPC field have introduced new CPU architectures with
improved vectorization capabilities that require optimization to achieve peak
performance and thus pose challenges for performance portability. The
deployment of high-performing scientific applications for CPUs requires
adapting the codebase and optimizing for performance. Evaluating these
applications provides insights into the complex interactions between code,
compilers, and hardware. We evaluate compiler auto-vectorization and explicit
vectorization to achieve performance portability across modern CPUs with long
vectors. We select a molecular docking application as a case study, as it
represents computational patterns commonly found across HPC workloads. We
report insights into the technical challenges, architectural trends, and
optimization strategies relevant to the future development of scientific
applications for HPC. Our results show which code transformations enable
portable auto-vectorization, reaching performance similar to explicit
vectorization. Experimental data confirms that x86 CPUs typically achieve
higher execution performance than ARM CPUs, primarily due to their wider
vectorization units. However, ARM architectures demonstrate competitive energy
consumption and cost-effectiveness.

</details>


### [108] [SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance Orchestration for AI Inference](https://arxiv.org/abs/2509.12252)
*Foteini Stathopoulou,Aggelos Ferikoglou,Manolis Katsaragakis,Dimosthenis Masouros,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.DC

TL;DR: SynergAI是一个在异构边缘到云基础设施上进行性能和架构感知推理服务的新框架，通过动态工作负载分配减少QoS违规2.4倍


<details>
  <summary>Details</summary>
Motivation: AI/ML推理服务需求快速增长，传统云部署面临网络拥塞、高能耗和隐私问题，边缘计算虽然低延迟但计算资源有限，需要智能调度方案

Method: 基于现代推理引擎性能特征分析，结合离线和在线决策策略，在Kubernetes生态系统中实现架构感知的轻量级智能调度

Result: 在Kubernetes环境中实现，相比最先进解决方案平均减少2.4倍的QoS违规，实现优化的架构感知部署

Conclusion: 架构驱动的推理服务能够在新兴硬件平台上实现优化部署，有效解决异构边缘到云基础设施的性能调度挑战

Abstract: The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)
has significantly heightened computational demands, particularly for
inference-serving workloads. While traditional cloud-based deployments offer
scalability, they face challenges such as network congestion, high energy
consumption, and privacy concerns. In contrast, edge computing provides
low-latency and sustainable alternatives but is constrained by limited
computational resources. In this work, we introduce SynergAI, a novel framework
designed for performance- and architecture-aware inference serving across
heterogeneous edge-to-cloud infrastructures. Built upon a comprehensive
performance characterization of modern inference engines, SynergAI integrates a
combination of offline and online decision-making policies to deliver
intelligent, lightweight, and architecture-aware scheduling. By dynamically
allocating workloads across diverse hardware architectures, it effectively
minimizes Quality of Service (QoS) violations. We implement SynergAI within a
Kubernetes-based ecosystem and evaluate its efficiency. Our results demonstrate
that architecture-driven inference serving enables optimized and
architecture-aware deployments on emerging hardware platforms, achieving an
average reduction of 2.4x in QoS violations compared to a State-of-the-Art
(SotA) solution.

</details>


### [109] [The Entropy of Parallel Systems](https://arxiv.org/abs/2509.12256)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 本文使用熵概念量化超级计算机集群中组件间的不兼容性，建立了基于图论和对数的数学模型，发现系统熵与计算性能呈显著负相关。


<details>
  <summary>Details</summary>
Motivation: 受香农信息论启发，研究者希望用熵来描述并行计算集群中组件不兼容性导致的噪声和混乱现象，为超级计算机性能评估提供新视角。

Method: 基于图论和对数建立数学模型，量化集群中每个系统的熵，并将该模型应用于Top500榜单前10的超级计算机进行熵计算。

Result: 发现系统熵与计算性能存在统计学显著的负相关关系，LINPACK基准测试显示强负相关（r=-0.7832, p=0.0077），MLPerf和HPCC测试也显示中等负相关。

Conclusion: 熵框架能够有效评估超级计算机性能，低熵系统具有更高的计算效率，该框架可扩展到传统密集线性代数之外的工作负载评估。

Abstract: Ever since Claude Shannon used entropy for his "Mathematical Theory of
Communication", entropy has become a buzzword in research circles with
scientists applying entropy to describe any phenomena that are reminiscent of
disorder. In this paper, we used entropy to describe the incompatibility
between components in the computer, which can cause noise and disorder within
the parallel cluster. We develop a mathematical theory, primarily based on
graph theory and logarithms, to quantify the entropy of a parallel cluster by
accounting for the entropy of each system within the cluster. We proceed using
this model to calculate the entropy of the Top 10 supercomputers in the Top500
list. Our entropy framework reveals a statistically significant negative
correlation between system entropy and computational performance across the
world's fastest supercomputers. Most notably, the LINPACK benchmark
demonstrates a strong negative correlation (r = -0.7832, p = 0.0077) with our
entropy measure, indicating that systems with lower entropy consistently
achieve higher computational efficiency, this Relationship is further supported
by moderate correlations with MLPerf mixed-precision benchmarks (r = -0.6234)
and HPCC composite scores (r = -0.5890), suggesting the framework's
applicability extends beyond traditional dense linear algebra workloads.

</details>


### [110] [An End to End Edge to Cloud Data and Analytics Strategy](https://arxiv.org/abs/2509.12296)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.DC

TL;DR: 提出端到端安全的边缘到云数据和分析策略，为物联网设备层、边缘层和云层提供参考架构


<details>
  <summary>Details</summary>
Motivation: 物联网设备指数级增长，需要实时数据做出关键决策，企业快速采用云计算，需要开发安全高效的策略和架构来充分利用云和边缘资产的能力

Method: 提供端到端安全的边缘到云数据和分析策略，为设备层、边缘层和云层设计参考架构

Result: 提出了可实现实际部署的参考架构方案

Conclusion: 该策略和架构能够满足物联网环境下对安全性和效率的迫切需求，支持实时数据处理和关键决策

Abstract: There is an exponential growth of connected Internet of Things (IoT) devices.
These have given rise to applications that rely on real time data to make
critical decisions quickly. Enterprises today are adopting cloud at a rapid
pace. There is a critical need to develop secure and efficient strategy and
architectures to best leverage capabilities of cloud and edge assets. This
paper provides an end to end secure edge to cloud data and analytics strategy.
To enable real life implementation, the paper provides reference architectures
for device layer, edge layer and cloud layer.

</details>


### [111] [Exploring Distributed Vector Databases Performance on HPC Platforms: A Study with Qdrant](https://arxiv.org/abs/2509.12384)
*Seth Ockerman,Amal Gueroudji,Song Young Oh,Robert Underwood,Nicholas Chia,Kyle Chard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 本文对向量数据库在HPC系统上的性能进行了实证研究，使用Qdrant在Polaris超级计算机上评估了插入、索引构建和查询延迟性能。


<details>
  <summary>Details</summary>
Motivation: 向量数据库在现代AI工作流中扮演重要角色，但在高性能计算系统上的性能特征尚不明确，需要为大规模科学研究提供性能指导。

Method: 在Argonne Leadership Computing Facility的Polaris超级计算机上，使用BV-BRC的生物文本工作负载和peS2o语料库的Qwen3-Embedding-4B嵌入，选择Qdrant评估最多32个工作节点的插入、索引构建和查询延迟。

Result: 研究提供了分布式向量数据库在HPC平台上的性能特征分析，为未来研究和优化提供了实践经验和指导。

Conclusion: 这项工作首次表征了向量数据库在HPC平台上的性能，为大规模科学应用中的向量数据库使用提供了重要参考和优化方向。

Abstract: Vector databases have rapidly grown in popularity, enabling efficient
similarity search over data such as text, images, and video. They now play a
central role in modern AI workflows, aiding large language models by grounding
model outputs in external literature through retrieval-augmented generation.
Despite their importance, little is known about the performance characteristics
of vector databases in high-performance computing (HPC) systems that drive
large-scale science. This work presents an empirical study of distributed
vector database performance on the Polaris supercomputer in the Argonne
Leadership Computing Facility. We construct a realistic biological-text
workload from BV-BRC and generate embeddings from the peS2o corpus using
Qwen3-Embedding-4B. We select Qdrant to evaluate insertion, index construction,
and query latency with up to 32 workers. Informed by practical lessons from our
experience, this work takes a first step toward characterizing vector database
performance on HPC platforms to guide future research and optimization.

</details>


### [112] [AI Factories: It's time to rethink the Cloud-HPC divide](https://arxiv.org/abs/2509.12849)
*Pedro Garcia Lopez,Daniel Barcelona Pons,Marcin Copik,Torsten Hoefler,Eduardo Quiñones,Maciej Malawski,Peter Pietzutch,Alberto Marti,Thomas Ohlson Timoudas,Aleksander Slominski*

Main category: cs.DC

TL;DR: 本文提出在超级计算机中采用双栈方法，整合HPC和云原生技术，以弥合高性能计算与云计算之间的鸿沟，实现高性能与易用性的结合。


<details>
  <summary>Details</summary>
Motivation: 随着各国政府推动主权AI计划，AI工厂建设需求激增。欧洲EuroHPC投资数亿欧元在HPC超级计算机上建设AI工厂，但传统HPC系统在可用性、可访问性和面向AI服务方面存在不足，而AI从业者习惯的云原生技术难以在HPC环境中集成。

Method: 采用双栈方法，在超级计算机中同时集成HPC和云原生技术。研究HPC的云挑战（无服务器HPC）和云技术的高性能挑战（高性能云），实现两种范式的融合与相互增强。

Result: 通过这种融合方法，能够将高性能和硬件加速与易用性和面向服务的界面相结合，使HPC和云计算范式相互促进和增强。

Conclusion: 双栈方法是解决AI工厂建设中HPC与云计算鸿沟的有效途径，能够满足主权AI计划对高性能基础设施和易用性服务的双重需求，为欧洲及其他地区的AI发展提供技术支持。

Abstract: The strategic importance of artificial intelligence is driving a global push
toward Sovereign AI initiatives. Nationwide governments are increasingly
developing dedicated infrastructures, called AI Factories (AIF), to achieve
technological autonomy and secure the resources necessary to sustain robust
local digital ecosystems.
  In Europe, the EuroHPC Joint Undertaking is investing hundreds of millions of
euros into several AI Factories, built atop existing high-performance computing
(HPC) supercomputers. However, while HPC systems excel in raw performance, they
are not inherently designed for usability, accessibility, or serving as
public-facing platforms for AI services such as inference or agentic
applications. In contrast, AI practitioners are accustomed to cloud-native
technologies like Kubernetes and object storage, tools that are often difficult
to integrate within traditional HPC environments.
  This article advocates for a dual-stack approach within supercomputers:
integrating both HPC and cloud-native technologies. Our goal is to bridge the
divide between HPC and cloud computing by combining high performance and
hardware acceleration with ease of use and service-oriented front-ends. This
convergence allows each paradigm to amplify the other. To this end, we will
study the cloud challenges of HPC (Serverless HPC) and the HPC challenges of
cloud technologies (High-performance Cloud).

</details>


### [113] [Analysis and Optimization of Wireless Multimodal Federated Learning on Modal Heterogeneity](https://arxiv.org/abs/2509.12930)
*Xuefeng Han,Wen Chen,Jun Li,Ming Ding,Qingqing Wu,Kang Wei,Xiumei Deng,Yumeng Shao,Qiong Wu*

Main category: cs.DC

TL;DR: 提出联合客户端调度和带宽分配算法(JCSBA)，解决无线多模态联邦学习中模态异构性和通信约束问题，提升多模态和单模态精度


<details>
  <summary>Details</summary>
Motivation: 多模态联邦学习面临客户端模态异构性（每个客户端只拥有部分模态）、固定延迟需求和有限通信带宽等挑战，传统单模态联邦学习方法不适用

Method: 基于决策级融合架构，添加单模态损失函数；推导性能上界，在延迟、能量和带宽约束下通过JCSBA算法最小化该上界

Result: 在多模态数据集上，JCSBA算法相比传统算法，多模态精度提升4.06%，单模态精度提升2.73%

Conclusion: JCSBA算法能有效优化无线多模态联邦学习性能，解决模态异构性和通信约束问题，显著提升模型精度

Abstract: Multimodal federated learning (MFL) is a distributed framework for training
multimodal models without uploading local multimodal data of clients, thereby
effectively protecting client privacy. However, multimodal data is commonly
heterogeneous across diverse clients, where each client possesses only a subset
of all modalities, renders conventional analysis results and optimization
methods in unimodal federated learning inapplicable. In addition, fixed latency
demand and limited communication bandwidth pose significant challenges for
deploying MFL in wireless scenarios. To optimize the wireless MFL performance
on modal heterogeneity, this paper proposes a joint client scheduling and
bandwidth allocation (JCSBA) algorithm based on a decision-level fusion
architecture with adding a unimodal loss function. Specifically, with the
decision results, the unimodal loss functions are added to both the training
objective and local update loss functions to accelerate multimodal convergence
and improve unimodal performance. To characterize MFL performance, we derive a
closed-form upper bound related to client and modality scheduling and minimize
the derived bound under the latency, energy, and bandwidth constraints through
JCSBA. Experimental results on multimodal datasets demonstrate that the JCSBA
algorithm improves the multimodal accuracy and the unimodal accuracy by 4.06%
and 2.73%, respectively, compared to conventional algorithms.

</details>


### [114] [Asymmetric Grid Quorum Systems for Heterogeneous Processes](https://arxiv.org/abs/2509.12942)
*Michael Senn,Christian Cachin*

Main category: cs.DC

TL;DR: 提出了非对称网格仲裁系统，允许分布式系统中的进程独立指定异构的信任假设，无需协调即可实现兼容性


<details>
  <summary>Details</summary>
Motivation: 传统仲裁系统要求所有进程共享相同的故障假设，而新兴系统允许进程选择主观或非对称的故障假设，但需要解决如何在没有初始兼容假设的情况下达成兼容的难题

Method: 基于描述进程差异的定性属性构建非对称网格仲裁系统，每个进程可以选择与其主观视图最匹配的仲裁系统，这些选择在设计上就是兼容的

Result: 打破了循环依赖问题，使进程能够在没有初始协调的情况下独立选择兼容的故障假设

Conclusion: 非对称网格仲裁系统为从云平台到区块链网络等多种应用场景提供了解决异构信任假设兼容性的有效方案

Abstract: Quorum systems are a common way to formalize failure assumptions in
distributed systems. Traditionally, these assumptions are shared by all
involved processes. More recently, systems have emerged which allow processes
some freedom in choosing their own, subjective or asymmetric, failure
assumptions. For such a system to work, individual processes' assumptions must
be compatible. However, this leads to a Catch-22-style scenario: How can
processes collaborate to agree on compatible failure assumptions when they have
no compatible failure assumptions to start with?
  We introduce asymmetric grid quorum systems that allow a group of processes
to specify heterogeneous trust assumptions independently of each other and
without coordination. They are based on qualitative attributes describing how
the processes differ. Each process may select a quorum system from this class
that aligns best with its subjective view. The available choices are designed
to be compatible by definition, thereby breaking the cycling dependency.
Asymmetric grid quorum systems have many applications that range from cloud
platforms to blockchain networks.

</details>


### [115] [Space-Time Trade-off in Bounded Iterated Memory](https://arxiv.org/abs/2509.13157)
*Guillermo Toyos-Marfurt,Petr Kuznetsov*

Main category: cs.DC

TL;DR: 本文研究了有界共享内存模型中实现无界全信息协议所需的轮次复杂度，建立了内存位容量与轮次需求之间的精确关系，并提出了渐近最优的有界全信息算法。


<details>
  <summary>Details</summary>
Motivation: 异步可计算性定理(ACT)假设无界容量的共享内存变量，但实际系统中内存是有限的。虽然已有研究表明有界变量可以通过额外轮次实现相同的计算能力，但内存位容量与实现全信息协议所需轮次的确切关系仍未知。

Method: 通过分析协议复形（表示可达状态的组合结构），推导必要条件并设计针对可用位数b的有界全信息算法。研究基于迭代共享内存模型，包括常规读写寄存器、原子快照和即时快照模型。

Result: 对于n>2个进程，实现全信息协议所需的轮次复杂度为Ω((n!)^{r-1} · 2^{n-b})。提出的有界全信息算法在迭代收集模型中渐近最优，在基于快照的模型中与最优解相差线性因子n。

Conclusion: 本文建立了有界共享内存容量与实现全信息协议所需轮次之间的精确渐近关系，为实际分布式系统中有限资源下的异步计算提供了理论基础和优化算法。

Abstract: The celebrated asynchronous computability theorem (ACT) characterizes tasks
solvable in the read-write shared-memory model using the unbounded
full-information protocol, where in every round of computation, each process
shares its complete knowledge of the system with the other processes.
Therefore, ACT assumes shared-memory variables of unbounded capacity. It has
been recently shown that boundedvariables can achieve the same computational
power at the expense of extra rounds. However, the exact relationship between
the bit capacity of the shared memory and the number of rounds required in
order to implement one round of the full-information protocol remained unknown.
  In this paper, we focus on the asymptotic round complexity of bounded
iterated shared-memory algorithms that simulate, up to isomorphism, the
unbounded full-information protocol. We relate the round complexity to the
number of processes $n$, the number of iterations of the full information
protocol $r$, and the bit size per shared-memory entry $b$. By analyzing the
corresponding protocol complex, a combinatorial structure representing
reachable states, we derive necessary conditions and present a bounded
full-information algorithm tailored to the bits available $b$ per shared memory
entry. We show that for $n>2$, the round complexity required to implement the
full-information protocol satisfies $\Omega((n!)^{r-1} \cdot 2^{n-b})$. Our
results apply to a range of iterated shared-memory models, from regular
read-write registers to atomic and immediate snapshots. Moreover, our bounded
full-information algorithm is asymptotically optimal for the iterated collect
model and within a linear factor $n$ of optimal for the snapshot-based models.

</details>


### [116] [Scaling Up Throughput-oriented LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2509.13201)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 论文提出了一种通过机会资源池和上下文管理来优化LLM应用执行效率的方法，减少98.1%的执行时间


<details>
  <summary>Details</summary>
Motivation: 传统LLM应用需要大量静态资源分配，导致用户要么等待长队列，要么购买昂贵硬件，加剧供需问题。但并非所有LLM应用都对延迟敏感，可以通过吞吐量导向的方式执行

Method: 采用普遍上下文管理方法，利用LLM应用中的通用计算上下文，提供机制和策略在机会资源上实现无缝上下文重用

Result: 评估显示，在机会资源上使用普遍上下文管理的LLM应用将执行时间减少了98.1%

Conclusion: 通过机会资源池和上下文重用机制，可以显著提高LLM应用的执行效率，避免资源浪费和硬件成本

Abstract: The widespread growth in LLM developments increasingly demands more
computational power from clusters than what they can supply. Traditional LLM
applications inherently require huge static resource allocations, which force
users to either wait in a long job queue and accept progress delay, or buy
expensive hardware to fulfill their needs and exacerbate the demand-supply
problem. However, not all LLM applications are latency-sensitive and can
instead be executed in a throughput-oriented way. This throughput orientation
allows a dynamic allocation that opportunistically pools available resources
over time, avoiding both the long queue and expensive GPU purchases.
Effectively utilizing opportunistic resources brings numerous challenges
nevertheless. Our solution, pervasive context management, exploits the common
computational context in LLM applications and provides mechanisms and policies
that allow seamless context reuse on opportunistic resources. Our evaluation
shows an LLM application with pervasive context management on opportunistic
resources reduces its execution time by 98.1%.

</details>
