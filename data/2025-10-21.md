<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 224]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 26]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个针对Lean和mathlib的语义搜索引擎，通过理解数学家的意图来改进定理搜索，相比现有方法有30%以上的相对提升。


<details>
  <summary>Details</summary>
Motivation: 形式定理证明的进展常因难以找到相关定理和Lean 4语言学习曲线陡峭而受阻，现有搜索引擎主要依赖非正式化翻译，忽视了与现实用户查询的匹配问题。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，使用多样化反馈信号与数学家偏好对齐，从多角度编码其目标意识。

Result: 在真实世界查询、非正式化语句和证明状态上的评估显示，相比之前的搜索引擎和GPT-4o，Lean Finder实现了超过30%的相对改进。

Conclusion: Lean Finder是一个用户中心的语义搜索工具，能够有效提升数学家在Lean环境中的工作效率，并与基于LLM的定理证明器兼容，连接检索与形式推理。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [2] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 提出LS-OGD框架，通过动态调整学习率和模态融合权重来应对概念漂移，确保多模态学习系统在非平稳环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中面临概念漂移的挑战，特别是模态特定的漂移和缺乏持续稳定适应机制的问题。

Method: 使用在线控制器动态调整模型学习率和不同数据模态的融合权重，响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，LS-OGD系统的预测误差被证明是最终一致有界的，如果漂移停止则会收敛到零。自适应融合策略能有效隔离和减轻严重模态特定漂移的影响。

Conclusion: 为开发可靠且持续适应的多模态学习系统建立了理论基础，确保系统韧性和容错能力。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [3] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时显著减少采样次数。


<details>
  <summary>Details</summary>
Motivation: 多响应采样能提高LLM输出质量，但计算成本高。关键挑战是如何平衡准确性和效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习，顺序生成响应，实时更新奖励分布后验信念，权衡预期收益与计算成本来决定停止时机。

Result: BEACON平均减少采样达80%，同时保持响应质量，在成本效益偏好数据生成方面也表现出实用性。

Conclusion: BEACON提供了理论最优性保证和实践可行性，为未来研究者提供了可操作的见解，能有效平衡LLM输出质量和计算效率。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [4] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: 提出PatMD方法，通过识别和主动缓解潜在误判风险来改进有害表情包检测，超越传统内容匹配方法


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括MLLM技术）难以处理表情包中通过讽刺、隐喻等修辞手法表达的隐含有害内容，导致频繁误判

Method: 构建知识库，将每个表情包解构为误判风险模式，解释可能被误判的原因；对目标表情包检索相关模式，动态指导MLLM推理

Result: 在5个有害检测任务的6,626个表情包基准测试中，PatMD优于最先进基线方法，F1分数平均提升8.30%，准确率提升7.71%

Conclusion: PatMD方法通过主动识别和缓解误判风险模式，显著提高了有害表情包的检测能力，展示了强大的泛化性能

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [5] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 提出基于WaveNet的深度学习模型，用于自动分类EEG信号为生理、病理、伪影和噪声四类，在公开数据集上取得优于CNN和LSTM方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家视觉审查的EEG信号分类方法在处理日益复杂和大量的EEG记录时变得不切实际，需要自动化解决方案。

Method: 使用WaveNet架构，利用扩张因果卷积和残差连接处理EEG数据，在209,232个样本的数据集上进行训练验证测试（70/20/10划分），并与TCN基准比较。

Result: 模型分类准确率超过之前的CNN和LSTM方法，能高精度区分噪声和伪影，但在生理和病理信号间存在可解释的误分类，反映了临床固有的重叠性。

Conclusion: WaveNet架构因其能捕捉细粒度和长程时间依赖性而适合EEG数据分析，为EEG信号自动分类提供了有效解决方案。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [6] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查新方法，使用深度学习模型在外部验证中获得超过90%的AUC-ROC，证明击键动力学可作为可靠的数字生物标志物。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估存在局限性，需要非侵入性、可扩展的远程筛查方法。

Method: 三阶段流程：数据预处理（提取时间信号、处理类别不平衡）、预训练8种深度学习架构、微调并在独立队列上进行外部验证。

Result: 混合卷积-循环和基于transformer的模型表现优异，外部验证AUC-ROC超过90%，F1-Score超过70%，其中时间卷积模型达到91.14%的AUC-ROC。

Conclusion: 击键动力学作为帕金森病的数字生物标志物具有巨大潜力，为早期检测和持续监测提供了有前景的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [7] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的集成评分滤波器(EnSF)在野火蔓延实时预测数据同化中的应用，该方法通过结合观测数据和数值模型预测，显著提高了野火蔓延预测的准确性、稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性日益增强且控制成本高昂，需要准确、实时的火势蔓延预测来进行有效管理。数据同化通过整合观测数据和数值模型预测，对提高活跃野火预测准确性至关重要。

Method: 应用基于扩散模型的集成评分滤波器(EnSF)，利用基于评分的生成扩散模型来处理高维非线性滤波问题，特别适用于野火蔓延模型的滤波问题。

Result: 数值研究表明，EnSF在野火数据同化中表现出卓越的准确性、稳定性和计算效率，证明了其作为稳健实用方法的有效性。

Conclusion: EnSF被确立为野火数据同化的强大实用方法，代码已公开可用，为实时野火蔓延预测提供了有效解决方案。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [8] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 该论文研究了LLMs处理工具返回的复杂JSON响应的能力，发现即使是最先进的模型在处理结构化响应时仍然存在困难，不同处理策略的性能差异可达3%到50%。


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化需要LLMs调用工具并处理其返回的复杂JSON响应，但LLMs处理结构化响应的能力尚未得到充分研究。

Method: 创建了专门的数据集，评估了15个开源和闭源模型，使用多种提示策略来测试JSON处理能力。

Result: JSON处理对前沿模型来说仍然是困难任务，最佳处理策略取决于工具输出的性质和大小以及所需推理的复杂性。

Conclusion: 处理结构化工具响应是LLMs面临的重要挑战，需要根据具体情况选择合适的方法策略。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [9] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 该研究通过热解技术转化食物类生物质，利用人工智能优化过程建模，探索废弃咖啡渣和枣核等未充分利用生物质资源的可持续制氢潜力。


<details>
  <summary>Details</summary>
Motivation: 推动可持续能源和废物管理策略，探索未充分利用生物质资源（如废弃咖啡渣和枣核）在可持续制氢方面的应用潜力。

Method: 对纯枣核、废弃咖啡渣及其混合物进行多项分析（工业分析、元素分析、纤维分析、热重分析、动力学、热力学和热解-微气相色谱分析），使用等转化率方法（KAS、FWO、Friedman）进行动力学建模，并训练LSTM模型预测热重曲线。

Result: 混合物3具有最佳的氢气产率潜力但活化能最高（313.24 kJ/mol），混合物1具有最佳的活化能值（161.75 kJ/mol）。KAS方法被确定为最准确的动力学模型，LSTM模型预测热重曲线的准确度极高（R²：0.9996-0.9998）。

Conclusion: 人工智能集成显著提高了热解过程建模的准确性，未充分利用的生物质资源具有可持续制氢的潜力，混合物的组成对氢气产率和活化能有显著影响。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [10] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出LAMI框架，通过图-语言联合建模检测青少年非法药物使用，并解释行为风险因素。


<details>
  <summary>Details</summary>
Motivation: 现有方法将调查变量独立处理，忽略了变量间的潜在关联结构，无法充分挖掘青少年药物使用的复杂风险因素。

Method: LAMI将个体回答表示为关系图，通过图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验表明，LAMI在预测准确性上优于竞争基线方法。

Conclusion: LAMI能够揭示有意义的行为子结构和心理社会路径，如家庭动态、同伴影响和学校相关压力，这些与已知的药物使用风险因素一致。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [11] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一个基于曲率信任区域的参数高效微调框架，通过边际效用分配参数并使用Fisher/Hessian度量信任区域约束更新，在多个基准测试中优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在低秩更新、量化或启发式预算重分配方面改进效率，但往往将容量分配与训练过程中的更新演化分离，缺乏原则性的优化方法。

Method: CTR-LoRA框架结合秩调度与稳定性感知优化，基于轻量级二阶代理的边际效用分配参数，并使用Fisher/Hessian度量信任区域约束更新。

Result: 在多个开源骨干模型(7B-13B)上的实验表明，CTR-LoRA在分布内和分布外基准测试中均优于强PEFT基线，提高了准确性、训练稳定性，减少了内存需求并实现了更高吞吐量。

Conclusion: CTR-LoRA在性能和效率的帕累托前沿上表现出色，为更稳健和可部署的PEFT提供了一条原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [12] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: Long Exposure系统通过解决参数高效微调中的Shadowy Sparsity问题，实现了LLM微调加速，比现有方法快2.49倍


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)技术虽然重要，但在时间投入和运营成本方面存在效率低下的问题，特别是Shadowy Sparsity这种在微调中特有的稀疏形式尚未得到充分解决

Method: 提出Long Exposure系统，包含三个核心组件：Shadowy-sparsity Exposer（使用长感知范围捕获更多稀疏细节）、Sequence-oriented Predictor（高效准确处理长序列输入和动态参数）、Dynamic-aware Operator（优化计算模式和内存访问）

Result: 广泛评估表明，Long Exposure在端到端微调中比现有最优方法快达2.49倍

Conclusion: Long Exposure为加速LLM的PEFT提供了有前景的进展，有效解决了Shadowy Sparsity问题

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [13] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: CLIP是一种客户端不变神经元剪枝技术，结合网络感知剪枝，解决了联邦学习中因计算和网络瓶颈导致的拖后腿客户端问题，在最小精度损失下加速安全联邦学习训练。


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在分布式模型训练中保护数据隐私，但异构设备部署会导致性能瓶颈，特别是计算或网络能力有限的拖后腿客户端会减慢所有参与客户端的训练速度。

Method: 提出CLIP技术，结合客户端不变神经元剪枝和网络感知剪枝，解决训练过程中因拖后腿客户端引起的计算和网络瓶颈。

Result: 在多个数据集（CIFAR10、Shakespeare、FEMNIST）上，安全联邦学习训练速度提升13%至34%，精度影响在1.3%提升到2.6%下降之间。

Conclusion: CLIP是首个针对深度神经网络安全聚合的拖后腿缓解技术，能有效加速安全联邦学习训练，同时保持可接受的精度损失。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [14] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出Deadlock攻击方法，通过恶意对抗嵌入劫持大型推理模型的生成控制流，诱导模型陷入无限推理循环，达到资源耗尽攻击的效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的链式思维推理机制引入了新的安全漏洞面，需要研究如何利用推理效率问题进行攻击。

Method: 训练恶意对抗嵌入，结合后门植入策略，通过特定触发词可靠激活，诱导模型在推理步骤后生成过渡性标记而无法结束回答。

Result: 在四个先进LRM和三个数学推理基准上实现100%攻击成功率，迫使模型生成达到最大令牌限制，且攻击隐蔽性强。

Conclusion: 揭示了LRM在推理效率方面存在关键且未被充分探索的安全漏洞。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [15] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 提出Justitia调度器，用于在共享GPU服务器中公平高效地服务LLM应用，解决主流调度器因队头阻塞或资源分配过约束导致的问题。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，需要运行一系列LLM推理（称为LLM应用）来解决实际问题。在共享GPU服务器中服务这些应用时，调度器需要在保证最坏情况性能的同时实现快速应用完成。但主流LLM调度器由于队头阻塞或资源分配过约束而表现不佳。

Method: 设计Justitia调度器，采用三种关键技术：1) 以内存为中心建模LLM应用的服务成本；2) 使用简单神经网络模型进行轻量级准确的需求预测；3) 采用基于虚拟时间的公平排队算法减少总体性能并保证最坏情况延迟。

Result: 在vLLM上实现Justitia，涉及多样化LLM应用的实验结果表明，它能在保持公平性的同时显著提高调度效率。

Conclusion: Justitia调度器能够有效解决LLM应用在共享GPU服务器中的调度问题，在保证公平性的同时提升效率。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [16] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出Gains方法解决联邦学习中新客户端加入带来的知识发现和适应问题，通过细粒度知识发现和贡献驱动聚合技术，在保持源域性能的同时有效整合新知识。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中，新客户端不断加入并引入新知识，现有方法在知识发现粒度、源域性能保持和适应效率方面存在不足。

Method: 将模型分为编码器和分类器，利用编码器对领域偏移敏感、分类器对类别增量敏感的特性，开发细粒度知识发现、贡献驱动聚合和抗遗忘机制。

Result: 在三种典型数据偏移场景的多领域数据集上，Gains在源域和目标域客户端性能均显著优于其他基线方法。

Conclusion: Gains通过细粒度知识发现和平衡适应机制，有效解决了开放世界联邦学习中的知识整合问题。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [17] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出了SAU-FNO框架，结合自注意力机制、U-Net和FNO，用于3D IC热管理，实现842倍加速和SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理面临高功率密度挑战，传统PDE方法速度慢，机器学习方法存在高频信息丢失和高保真数据依赖问题。

Method: 结合自注意力机制和U-Net的FNO框架，使用迁移学习微调低保真数据，减少对高保真数据集的需求。

Result: SAU-FNO在热预测精度上达到SOTA水平，相比传统FEM方法实现842倍加速。

Conclusion: SAU-FNO是高效的高级3D IC热仿真工具，平衡了精度和计算效率。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [18] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大语言模型自动将非线性优化问题重新表述为线性优化问题。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的重新表述通常是手动且需要专业知识的，但这对使用线性优化求解器或应用专用算法至关重要。

Method: 框架为每个非线性模式分配一个重新表述代理，这些代理被明确指示为其非线性模式（如绝对值项或决策变量的双线性乘积）推导精确的线性重新表述，然后协调组装一个与原始问题等效的求解器就绪线性模型。

Result: 在从ComplexOR数据集衍生的20个真实世界非线性优化问题上评估该方法，结果表明专用LLM代理可以自动化线性化任务。

Conclusion: 该方法为非线性优化开辟了完全对话式建模管道的路径。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [19] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 该论文提出使用持续同调分析训练数据的几何结构，发现数据表示的丰富性和冗余消除对模型性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 虽然已知训练数据的类型对机器学习很重要，但数据的几何结构对模型性能的影响尚未充分探索。作者认为数据的表示丰富性和冗余消除对学习结果有重要影响。

Method: 使用持续同调从度量空间中的数据提取拓扑特征，提供了一种超越基于熵的度量来量化多样性的原则性方法。

Result: 研究发现持续同调是分析和增强驱动AI系统的训练数据的强大工具。

Conclusion: 持续同调为分析训练数据的几何结构提供了有效方法，有助于提升AI系统的性能。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [20] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE框架通过提示引导的数据增强和对比马氏距离评分，有效检测大语言模型的幻觉现象，无需人工标注，性能提升6.55%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在产生误导或虚假信息的幻觉问题，但缺乏标注良好的真实与幻觉输出数据集，需要开发有效的幻觉检测方法。

Method: 提出PALE框架，利用提示引导从LLMs生成增强数据，并引入对比马氏距离评分来评估中间嵌入向量的真实性。

Result: 实验表明PALE在幻觉检测方面表现优异，比竞争基线显著提升6.55%的性能。

Conclusion: PALE框架提供了一种无需人工标注、具有强泛化性和实用性的幻觉检测解决方案。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [21] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出DAWP框架，通过AI数据同化模块(AIDA)将AI天气预测从再分析数据解放到观测空间，实现基于不规则卫星观测的全球天气预报


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预测依赖再分析数据，存在数据同化偏差和时间差异问题，需要开发能在观测空间直接运行的AI天气预测方法

Method: 使用掩码多模态自编码器(MMAE)进行AI数据同化，结合掩码ViT-VAE编码不规则卫星观测；采用时空解耦transformer与跨区域边界条件进行观测空间动态学习

Result: AIDA初始化显著提高了AIWP的展开能力和效率，DAWP在全局降水预报中展现出应用潜力

Conclusion: DAWP框架成功实现了从再分析数据到观测空间的转变，为AI天气预测提供了新的范式

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [22] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: Cog-Rethinker是一个分层元认知强化学习框架，通过两阶段推理过程提高LLM在推理任务中的样本利用效率，解决了传统方法中无效输出导致的样本浪费问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于固定提示模板的强化学习方法对弱LLM存在样本效率低下的问题，大多数问题在推理任务中会产生无效输出，造成样本浪费。

Method: 提出分层元认知RL框架，在直接rollout后分两阶段改进样本利用：1) 将零准确率问题分解为子问题；2) 参考先前错误答案精炼答案。使用监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现出优越性能，相比基线方法提高了样本效率并加速了收敛。

Conclusion: Cog-Rethinker通过分层元认知方法有效解决了LLM推理训练中的样本效率问题，在数学推理任务中取得了显著改进。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [23] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种名为AMiD的通用知识蒸馏框架，通过引入α-混合辅助分布来解决LLM知识蒸馏中的容量差距和训练不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型虽然性能优异但计算和内存成本高昂。知识蒸馏通过将大模型的知识转移到小模型来解决此问题，但面临容量差距和因高维输出导致的近零概率训练不稳定问题。

Method: 提出了α-混合辅助分布，引入新的分布设计变量α来连续扩展辅助分布，并基于最优性推广了与辅助分布一起使用的散度族，形成统一的AMiD框架。

Result: 通过大量实验证明，AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Conclusion: AMiD框架通过系统化的辅助分布设计和散度推广，有效解决了LLM知识蒸馏中的关键挑战，为知识蒸馏提供了统一的理论基础。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [24] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出了MEET-Sepsis框架，通过多内源性视图表示增强机制和级联双卷积时间序列注意力模块，仅需20%的ICU监测时间即可实现竞争性的脓毒症预测准确性。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是ICU中死亡率高的威胁生命感染综合征，早期准确预测对及时干预至关重要。现有AI方法难以捕捉微弱的早期时间信号。

Method: 使用多内源性视图表示增强(MERE)机制构建丰富特征视图，结合级联双卷积时间序列注意力(CDTA)模块进行多尺度时间表示学习。

Result: MEET-Sepsis框架在仅需SOTA方法20%ICU监测时间的情况下实现了竞争性的预测准确性。

Conclusion: 该框架显著推进了早期脓毒症预测，广泛验证证实了其有效性。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [25] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的可解释人工智能方法，用于根据睡眠障碍特征对患者进行分组，并识别影响这些病理的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者健康和生活质量有重大影响，但由于症状多样性，诊断仍然复杂。技术进步和医疗数据分析为更好理解这些障碍提供了新视角。

Method: 采用基于聚类的可解释人工智能方法，整合可解释性方法来识别关键影响因素。在匿名真实数据上进行实验验证。

Result: 实验证明了该方法的有效性和相关性，能够成功识别影响睡眠障碍的关键因素。

Conclusion: 提出的可解释聚类方法为睡眠障碍诊断提供了新的有效工具，有助于更好地理解和诊断复杂的睡眠障碍问题。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [26] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 本文提出了一个追踪和引导LLM推理算法原语的框架，通过将推理轨迹与内部激活模式关联，评估算法原语对推理步骤和任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何通过潜在计算和推理时间计算解决多步推理问题，探索模型推理背后的算法原语。

Method: 通过聚类神经激活并标记匹配的推理轨迹来操作化原语，应用函数向量方法推导可重用的推理构建块，支持向量加法、减法和标量运算。

Result: 跨任务和跨模型评估显示存在共享和任务特定的原语，推理微调增强了算法泛化能力，在Phi-4-Base中注入原语向量可诱导出类似Phi-4-Reasoning的行为特征。

Conclusion: LLM推理可能由算法原语的组合几何支持，原语可跨任务和跨模型迁移，推理微调能增强跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [27] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文研究了GRPO算法在增强大语言模型推理能力时的局限性，发现其本质上是一种保守的重新加权方案，只能强化预训练偏差，无法发现全新解决方案。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在提升LLM推理能力时表现出不一致性，在某些领域显著改进而在其他领域停滞不前，这引发了对GRPO改进推理和泛化能力的条件研究。

Method: 从数据分布角度进行理论分析，证明GRPO是保守的重新加权方案，并通过从零开始训练transformer进行控制研究，评估在推理深度、输入长度、标记表示和组合性等方面的泛化能力。

Result: GRPO的OOD改进仅在目标任务与模型预训练偏差一致时出现，而ID任务的收益随着性能饱和而减少。GRPO不能作为通用推理增强器，而是强化预训练偏差的工具。

Conclusion: GRPO只能强化预训练偏差而非扩展模型能力，这为开发能够超越预训练起源的算法提供了动机。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [28] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏管道，能自动选择服务器和模型、进行知识蒸馏以及在分布式云环境中部署，满足用户定义的模型性能和系统预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业对定制化和成本效益高的大型语言模型需求增长，现有蒸馏框架需要手动干预且难以满足复杂用户需求。

Method: 自动选择帕累托最优服务器，动态匹配师生模型对，根据任务复杂度调整蒸馏策略以优化云托管。

Result: 在特定麻将推理任务上，学生模型准确率比GPT-4o教师基线提高四倍，同时降低延迟和成本而不影响准确性。

Conclusion: Stratos展示了在垂直领域LLM部署中的潜力，能有效满足工业需求。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [29] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Smirnov检验来监测和量化机器学习系统中的分布偏移问题，特别是在智能交通应用中，即使KS距离为0.02也会导致强化学习智能体的性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中训练数据与测试数据概率分布不一致的问题，这种分布偏移会导致预测误差，在安全关键应用中尤为严重。

Method: 使用Kolmogorov-Smirnov检验来测量分布偏移，通过KS距离量化分布偏移程度及其对AI智能体性能的影响。

Result: 研究表明，即使KS距离仅为0.02，也会导致强化学习智能体在单个交叉路口的行程时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能退化的有价值统计工具，帮助智能体更有效地应对分布偏移问题。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [30] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一个软硬件协同设计，通过E2Softmax和AILayerNorm优化Transformer中的Softmax和LayerNorm操作，无需重新训练即可保持推理精度，同时显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但其实时推理速度和效率受到Softmax和LayerNorm低效性的限制。现有基于函数逼近的方法实现效率低，且需要重新训练来补偿逼近误差，成本高且不便。

Method: SOLE采用硬件-软件协同设计：E2Softmax使用指数函数的log2量化和基于对数的除法来逼近Softmax；AILayerNorm采用低精度统计计算。两者都实现了低精度计算和低比特位宽存储。

Result: 实验表明SOLE在无需重新训练的情况下保持推理精度，相比GPU实现提供数量级的速度提升和能耗节省。与现有最优定制硬件相比，Softmax和LayerNorm分别实现了3.04倍、3.86倍的能效提升和2.82倍、3.32倍的面积效率提升。

Conclusion: SOLE通过创新的软硬件协同设计有效解决了Transformer中Softmax和LayerNorm的效率瓶颈，在保持精度的同时显著提升了性能和能效。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [31] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 本文分析了使用ANaGRAM自然梯度方法训练PINNs的动态过程，提出了多截止值自适应策略来提升性能，并在基准PDE问题上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 最近研究表明自然梯度方法在训练物理信息神经网络(PINNs)时显著优于标准优化器，但需要深入理解其训练动态并改进性能。

Method: 基于ANaGRAM自然梯度方法的分析，提出了多截止值自适应策略，并建立了基于谱理论的理论框架来解释正则化的必要性。

Result: 在基准PDE问题上的实验验证了方法的有效性，在某些实验中可以达到机器精度。

Conclusion: 提出的多截止值自适应策略显著提升了ANaGRAM的性能，理论框架为方法提供了理论基础，并扩展了与格林函数理论的联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [32] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一个层感知的在线估计器，用于高效计算训练样本在优化过程中的动态影响力，避免了传统静态影响力分析的计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统的数据影响力分析主要关注收敛模型上的静态影响力，忽略了优化过程中样本影响力的动态变化特性，特别是在深度模型中。现有方法计算负担重，难以频繁进行影响力估计。

Method: 开发了一个层感知的在线估计器，仅需要损失对输出的梯度，避免了参数级和全网络梯度的计算，同时保持了影响力排名的准确性。

Result: 在LLM预训练、微调和图像分类等任务上的广泛实验表明，该方法显著提高了准确性，同时大幅降低了时间和内存成本。

Conclusion: 该方法使得动态数据筛选在实践中变得高效且可扩展，为数据中心学习提供了实用的解决方案。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [33] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出了STAR模块，用于增强时间序列基础模型在处理包含离散状态变量的多元时间序列异常检测时的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理同时包含数值变量和离散状态变量的工业时间序列时，往往忽视状态变量的分类特性及其作为条件的关键作用，导致检测性能下降。

Method: STAR包含三个核心组件：身份引导状态编码器、条件瓶颈适配器和数值-状态匹配模块，通过可学习状态记忆和条件低秩适配参数来有效建模状态变量。

Result: 在真实世界数据集上的广泛实验表明，STAR能够提升现有时间序列基础模型在多元时间序列异常检测中的性能。

Conclusion: STAR是一个即插即用的模块，能够在微调阶段有效增强时间序列基础模型对状态变量的建模和利用能力。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [34] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的洪水管理框架，通过端到端优化传感器部署和洪水预测模型来最小化下游应急决策的遗憾值。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定的、与决策任务无关的策略来部署传感器和训练预测模型，忽略了相同感知增益和平均预测误差可能导致不同决策结果的问题。

Method: 构建包含四个组件的端到端管道：上下文评分网络、在硬预算约束下的可微分传感器选择模块、时空洪水重建与预测模型、以及针对任务目标的可微分决策层。采用隐式最大似然估计实现离散传感器配置的梯度学习。

Result: 该方法能够战略性地选择传感器部署位置并优化洪水预测模型，直接针对下游洪水响应决策进行优化。

Conclusion: 提出的决策导向框架能够更有效地支持洪水应急响应决策，通过端到端优化传感器部署和预测模型来最小化决策遗憾。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [35] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 本研究探索了使用渐进式神经网络和微调策略来加速深度强化学习在多保真度混沌流体控制中的知识迁移，发现渐进式神经网络在保持知识稳定性和鲁棒性方面优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在流体控制中训练成本高昂，需要探索有效的迁移学习策略来加速训练过程，特别是在从低保真度环境向高保真度环境的知识迁移方面。

Method: 采用渐进式神经网络架构，并与传统微调策略进行对比评估，使用Kuramoto-Sivashinsky系统作为基准测试平台，分析不同迁移学习方法在流体控制中的表现。

Result: 微调方法虽然能加速收敛但对预训练时长敏感且容易发生灾难性遗忘，而渐进式神经网络能稳定保持先验知识，在不同物理机制和控制目标下都表现出更好的鲁棒性和适应性。

Conclusion: 渐进式神经网络为流体控制提供了一种稳健、可扩展且计算高效的迁移学习框架，有望应用于更复杂的流动配置中。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [36] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从1600多个翼型数据库中选择12个最优基准翼型，显著降低设计空间维度，在保持高重构精度的同时实现更高效的多目标优化和强化学习应用。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样化的设计，同时尽可能减少设计变量数量。现有方法存在设计空间维度高、优化效率低的问题。

Method: 从UIUC翼型数据库中选择12个最优基准翼型，通过序列添加最能增加设计容量的基准翼型，构建低维设计空间。

Result: 以12个基准翼型重构了99%的数据库，平均绝对误差低于0.005；在多目标气动优化中实现快速收敛，获得更大超体积的帕累托前沿；在强化学习中表现出优于传统参数化方法的适应性。

Conclusion: AirDbM在减少设计变量数量的同时保持了设计多样性，为机器学习驱动的设计提供了更有效的参数化方法，展示了设计变形方法在优化和机器学习应用中的广阔潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [37] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出一种基于特征驱动的强化学习方法，用于光伏发电商在日内连续市场中的实时交易决策，通过PPO算法学习可解释的线性策略，在历史市场数据上训练并在样本外测试中表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 光伏发电商面临发电量和短期电价的不确定性，日内连续市场允许实时调整头寸以改善收益和减少不平衡成本。

Method: 将问题建模为马尔可夫决策过程，使用PPO算法学习线性可解释策略，整合数据驱动特征到状态空间中，平衡交易利润和不平衡惩罚。

Result: 在历史市场数据上的样本外评估显示，该策略在不同场景下持续优于基准方法，具有快速收敛、实时推理和透明决策规则的特点。

Conclusion: 特征驱动的强化学习为光伏发电商提供了实用、数据高效且可操作部署的日内主动参与路径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [38] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出IB-FT方法，通过信息瓶颈指导微调，解决大语言模型在代码生成任务中的记忆障碍问题，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域微调时存在记忆障碍问题，即模型对下游代码数据的强记忆会阻碍优化，使标准微调无法有效获取新的可泛化代码知识。

Method: 提出基于信息瓶颈的微调方法(IB-FT)，对代码数据的隐藏表示施加IB惩罚，压缩虚假的记忆特征，同时保留任务相关信息。

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上的实验表明，IB-FT显著缓解了记忆障碍，提高了top-1性能(Pass@1)，并在更严格的多样本指标Pass@k(m)下获得了更稳定的增益。

Conclusion: IB-FT方法能有效克服记忆障碍，提升代码生成模型的泛化能力和性能稳定性。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [39] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出了PolyConFM，首个基于构象的聚合物基础模型，通过条件生成预训练统一聚合物建模与设计，在多个下游任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示聚合物，忽略了聚合物构象中的全局结构信息，限制了实际性能。该领域还缺乏能够有效支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，通过掩码自回归建模重建局部构象并生成其方向变换来恢复聚合物构象。构建首个高质量聚合物构象数据集进行构象中心预训练。

Result: PolyConFM在多样化下游任务中始终优于代表性任务特定方法，为聚合物科学提供了通用且强大的工具。

Conclusion: PolyConFM成功解决了聚合物建模中全局结构信息缺失和缺乏通用基础模型的问题，为聚合物科学提供了有效的解决方案。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [40] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出一个可泛化的因果机器学习流程，用于从电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，发现大规模电子健康记录中的潜在因果源，并量化这些源对临床结果的因果效应。

Method: 将多模态临床数据处理分解为概率独立的潜在源，训练任务特定的因果模型来估计个体因果效应。

Result: 该方法已在两个真实世界应用中验证，展示了其在医疗发现中的通用性和实用性。

Conclusion: 该因果机器学习流程能够有效处理不完善的临床数据，发现潜在因果源并量化其影响，为大规模医疗发现提供了可行方案。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [41] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了首个针对GNN社交机器人检测器的多智能体强化学习对抗攻击框架RoBCtrl，通过扩散模型生成高保真机器人账户，并使用MARL模拟对抗行为来破坏检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN社交机器人检测方法存在脆弱性和鲁棒性不足的问题，由于对社交代理的控制有限、检测器黑盒特性以及机器人异质性等挑战，需要开发有效的对抗攻击框架来评估检测方法的稳健性。

Method: 使用扩散模型生成高保真机器人账户，通过多智能体强化学习模拟对抗行为，基于影响力和预算对账户分类，并设计基于结构熵的分层状态抽象来加速强化学习。

Result: 在社交机器人检测数据集上的大量实验表明，该框架能有效削弱GNN检测器的性能。

Conclusion: RoBCtrl是首个针对GNN社交机器人检测器的多智能体强化学习对抗攻击框架，成功证明了现有检测方法的脆弱性，为改进检测器鲁棒性提供了重要参考。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [42] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出Grid-like Code Quantization (GCQ)，一种受大脑启发的网格模式压缩方法，通过动作条件码本将观察-动作序列压缩为离散表示。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，而GCQ旨在通过动作条件码本实现时空压缩，作为统一的世界模型。

Method: 使用连续吸引子神经网络生成码字，基于动作动态选择码字，实现空间和时间的联合压缩。

Result: 实验表明GCQ在紧凑编码和下游任务性能方面表现有效，支持长时程预测、目标导向规划和逆向建模。

Conclusion: GCQ既为高效序列建模提供了计算工具，也为神经系统中网格模式编码的形成提供了理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [43] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种针对大语言模型的浮点量化方法，通过引入非整数位宽和两种新技术来优化推理效率，在保持精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量庞大带来存储和推理效率瓶颈，浮点量化虽然能加速推理，但传统方法局限于整数位宽，需要进一步探索非整数位宽来接近量化最佳点。

Method: 提出两种新技术：1) 尾数位共享 - 将k个量化权重分组共享最低有效尾数位，实现非整数位宽量化；2) 自适应搜索 - 采用离线优化策略最小化共享带来的精度损失。同时开发了高效的CUDA线性核实现。

Result: 实验表明AMS-Quant可将模型量化为FP-5.33-e2m3和FP4.25-e2m2，相比FP16推理显著加速解码过程（2.8倍和3.2倍），精度损失可忽略不计。

Conclusion: AMS-Quant首次将浮点量化从整数位宽扩展到非整数位宽，通过创新的尾数位共享和自适应搜索技术，在保持模型精度的同时显著提升了LLM推理效率。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [44] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化可扩展框架，通过原生可访问性API系统探索应用程序，解决GUI自动化中的数据收集挑战，并构建了包含27,171个任务的GUIrilla-Task数据集。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理在复杂图形用户界面操作方面面临数据可用性限制，包括昂贵的手动标注、闭源数据集和表面级合成流程，特别是在macOS生态系统中数据表示有限。

Method: GUIrilla框架通过原生可访问性API系统探索应用程序，将发现的界面元素和爬虫动作组织成层次化GUI图，并使用专门的交互处理器实现全面应用覆盖。

Result: 构建了GUIrilla-Task数据集，包含27,171个功能基础任务，覆盖1,108个macOS应用。在ScreenSpot Pro基准测试中，基于GUIrilla-Task调优的LLM代理性能显著提升，使用97%更少数据优于合成基线。

Conclusion: GUIrilla框架和数据集有效解决了桌面自动化中的数据收集挑战，显著提升了UI任务性能，并发布了开源工具和数据集支持桌面自主性研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [45] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 该论文探讨了智能交通系统中的交通流量预测技术，重点分析了图神经网络在捕捉空间依赖性和时间演化模式方面的应用，并指出了现有方法在事件信息处理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要开发可靠且响应迅速的交通预测模型来支持智能交通系统建设，改善城市资源分配和出行体验。

Method: 主要采用图神经网络作为主流范式，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，这些方法结合了复杂的图卷积结构和时间建模机制。对于事件信息处理，早期方法主要依赖手动设计的事件特征和特定子图构建。

Result: 现有图神经网络方法在标准交通数据集上表现出色，特别擅长捕捉和预测具有周期性规律的交通模式。但在事件响应方面，手动特征工程方法存在泛化能力差和语义细节丢失的问题。

Conclusion: 虽然图神经网络在交通预测中取得了显著进展，但当前基于手动特征的事件信息处理方法存在局限性，需要开发更智能的事件感知机制来提升模型对复杂未知事件的适应能力。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [46] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 该论文系统评估了五种时间序列基础模型和两种基准模型的校准特性，发现时间序列基础模型比基准模型校准得更好，且不会系统性地过度自信或自信不足。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性尚未得到充分探索，而校准确实对许多实际应用至关重要。

Method: 对五种近期时间序列基础模型和两种竞争性基准模型进行系统评估，包括模型校准、不同预测头的影响以及长期自回归预测下的校准。

Result: 时间序列基础模型比基准模型校准得更好，且不会系统性地过度自信或自信不足，这与其它深度学习模型中常见的过度自信形成对比。

Conclusion: 时间序列基础模型在保持高预测性能的同时，展现出良好的校准特性，这对其在实际应用中的可靠性具有重要意义。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [47] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出了一种基于层次图神经网络的分电站级电压估计方法，在低观测性条件下实现准确电压估计，比替代数据驱动模型RMSE降低2倍，仅需1%测量覆盖率即可保持高精度。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透率和配电网电压波动性增加，传统配电网状态估计方法难以应对稀疏测量和大规模网络，需要开发更鲁棒和可扩展的电压估计技术。

Method: 采用层次图神经网络，利用电气拓扑和物理特征，在公开SMART-DS数据集上进行训练和评估，涵盖多个分电站和DER渗透场景。

Result: 在数千个总线上的综合实验表明，该方法比替代数据驱动模型RMSE降低达2倍，在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络具有为配电网实现可扩展、可复制和数据驱动电压监测的潜力。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [48] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流求解方法，使用快速DC OPF解作为基准，仅学习非线性校正项来获得完整AC OPF解。


<details>
  <summary>Details</summary>
Motivation: 解决非线性AC最优潮流问题的计算瓶颈，实现近实时电网运行决策。

Method: 使用拓扑感知图神经网络，结合局部注意力和两级DC特征集成，通过物理信息损失函数训练，确保AC潮流可行性和运行限制。

Result: 在57、118和2000总线系统上测试，MSE降低约25%，可行性误差减少达3倍，运行速度提升达13倍，且在N-1故障下保持准确性。

Conclusion: 残差学习是线性近似与AC可行OPF之间的实用可扩展桥梁，支持近实时运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [49] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出了FedPURIN框架，通过整数编程识别关键参数进行传输，结合稀疏聚合显著降低通信开销，同时保持个性化联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中现有方法通信效率低下的问题，特别是在数据异构环境下，通信负担阻碍了实际部署

Method: 基于整数编程策略识别关键参数进行传输，结合稀疏聚合方案，实现通信效率优化

Result: 在标准图像分类基准测试中，在多种非IID条件下表现出与最先进方法相当的性能，同时通过稀疏聚合实现可量化的通信减少

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [50] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了多尺度神经算子（MNO），一种用于三维非结构化点云上计算流体动力学的新型架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时仍存在精度有限和可扩展性不足的问题，特别是在不规则域上处理具有丰富多尺度结构的流体流动时表现不佳。

Method: MNO架构显式地将信息分解为三个尺度：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域级交互，微观点级注意力模块处理细粒度细节。

Result: 在四个不同的基准测试中，MNO始终优于最先进的基线方法，将预测误差降低了5%到40%，并在具有30万点的挑战性3D CFD问题中表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对于神经算子至关重要，MNO为在不规则域上学习复杂流体动力学提供了一个可扩展的框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [51] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 提出基于随机矩阵理论的Transformer训练动态分析框架，通过自注意力矩阵谱密度演化识别训练三阶段，并开发无需验证集的早停准则。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，为性能改进提供理论依据，并建立原则性的早停标准。

Method: 利用随机矩阵理论分析Transformer训练动态，通过自注意力矩阵V的谱密度演化识别训练阶段，提出基于幂律拟合和谱特征的早停准则。

Result: 发现浅层自注意力矩阵谱密度始终演化为重尾分布，据此划分训练三阶段：结构探索、重尾结构稳定、收敛饱和。提出的早停准则与训练进展高度一致。

Conclusion: 随机矩阵理论为监控和诊断Transformer模型训练进展提供了有效工具，提出的早停准则具有一致性和无需验证集的优势。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [52] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的神经网络后训练量化方法，通过Frobenius距离作为目标函数，将量化问题转化为可分解的QUBO问题，并使用模拟退火等启发式算法高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法如四舍五入可能不是最优选择，需要更精确的量化方法来减少精度损失，特别是在低精度量化场景下。

Method: 使用Frobenius距离作为目标函数，构建显式QUBO问题，其中二进制变量表示权重和偏置的舍入选择。利用QUBO矩阵结构将全局问题分解为n个独立子问题，使用模拟退火等启发式算法求解。

Result: 在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上评估，覆盖int8到int1的整数精度，与传统四舍五入量化方法进行比较。

Conclusion: 该方法能够有效实现神经网络的低精度量化，通过QUBO公式和问题分解策略提高了量化效率和精度。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [53] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出BPL框架，通过双蒸馏策略在事实和反事实测试环境中实现高性能推荐系统


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差问题，现有去偏方法主要针对反事实测试环境，但在基于实际用户交互的事实测试环境中性能显著下降。两种测试环境各有优势，需要同时兼顾。

Method: 使用偏置模型的师生蒸馏保留准确偏好知识，通过可靠性过滤的自蒸馏迭代精炼知识，采用双蒸馏策略

Result: 综合实验验证BPL在事实和反事实测试中均有效

Conclusion: BPL框架能够逐步揭示用户偏好，在两种测试环境中都表现出色

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [54] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出了CONEC-LoRA方法解决领域增量学习问题，通过整合任务共享和任务特定的LoRA模块，结合随机分类器和辅助网络，在四个基准测试中比现有方法提升超过5%


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法创建任务特定的LoRA模块，但忽略了任务间的共享知识，且推理时选择不准确会导致精度显著下降，线性或原型分类器的泛化能力不足

Method: CONEC-LoRA整合任务共享LoRA提取共同知识，任务特定LoRA获取领域特定知识；使用随机分类器增强分类正确率；部署辅助网络预测任务特定LoRA，采用不同深度网络结构利用中间表示，集成球生成器损失和转换模块解决合成样本偏差问题

Result: 在4个流行基准问题上，CONEC-LoRA比现有方法有超过5%的优势提升

Conclusion: CONEC-LoRA通过知识整合、随机分类器和辅助网络的有效结合，成功解决了领域增量学习中的灾难性遗忘问题，显著提升了性能

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [55] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出了PassREfinder-FL框架，使用图神经网络和联邦学习来预测跨网站的凭据填充攻击风险，无需共享用户敏感信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测密码重用或恶意登录时往往牺牲可用性，且依赖复杂的账户共享机制，阻碍了实际部署。

Method: 引入密码重用关系概念，构建网站图，使用图神经网络进行链接预测，并采用联邦学习保护用户隐私。

Result: 在包含3.6亿个泄露账户的真实数据集上，联邦学习设置下F1分数达到0.9153，比其他先进GNN模型性能提升4-11%。

Conclusion: 该方法能够有效预测凭据填充风险，生成可操作的风险评分，同时保护用户隐私。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [56] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 该论文将平衡传播学习算法扩展到离散和连续复值波系统，适用于弱耗散机制，并在激子极化凝聚体中验证了该方法在逻辑任务和手写数字识别上的有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)作为一种替代方案具有相似的效率和原位训练潜力。

Method: 将EP学习扩展到离散和连续复值波系统，在弱耗散机制下有效，通过可训练的局部势能替代节点间连接，在激子极化凝聚体中测试。

Result: 数值研究显示在标准基准测试中（包括逻辑任务和手写数字识别）实现了稳定收敛。

Conclusion: 为系统控制仅限于局部参数的物理系统中的原位学习建立了实用途径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [57] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中的模态缺失问题，通过解耦表示和自蒸馏实现语义恢复


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据常因遮挡、隐私、设备故障等原因缺失，现有方法忽略此问题导致泛化性差

Method: 使用去冗余同质-异质分解模块将模态分解为同质、异质和噪声表示，并设计分布对齐的自蒸馏模块进行双向知识迁移

Result: 在两个数据集上的实验表明，FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势

Conclusion: FSRF框架能有效缓解多模态情感分析中的模态缺失问题，提高模型在真实场景中的适用性

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [58] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个门控持续自编辑框架，通过LoRA参数高效微调来约束顺序更新中的遗忘问题，使用三种指标评估编辑稳定性，有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续适应机制，但顺序更新会导致灾难性遗忘，新编辑会降低先前获得的知识。

Method: 使用基于LoRA的参数高效微调，通过三种指标（精确匹配下降、比特增加、KL散度）评估候选编辑的稳定性，超过阈值时通过裁剪程序重新缩放或拒绝LoRA更新。

Result: 在Qwen-2.5-7B模型上的实验表明，门控有效缓解遗忘同时保持适应性，基于EM的门控在短持续学习序列中实现最高累积性能。

Conclusion: 不同门控策略可实现相当的分布偏移，但产生不同的准确性结果，突显了门控设计在持续适应中的重要性，为持续模型编辑提供了原则性方法。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [59] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过分层压缩技术有效压缩多示例提示，在保持高准确率的同时显著降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过上下文学习可以从多个输入-输出示例中学习任务，但增加示例数量会显著提高内存和计算成本，需要有效的压缩方法。

Method: 提出MemCom分层压缩方法，使用更强的压缩器模型，在每个transformer层进行细粒度压缩，生成软令牌摘要。

Result: 在多种模型大小、架构和压缩比下，MemCom在所有压缩比上都优于基线方法，在高压缩比下性能下降小于10%，而基线方法下降20-30%。

Conclusion: MemCom通过分层压缩实现了有效的多示例提示压缩，在保持性能的同时显著提高了上下文学习推理的效率。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [60] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一种决策支持系统，通过限制人类可采取的行动子集来实现人机互补，在野火缓解游戏中使参与者表现比单独行动提升30%，比AI代理提升2%以上。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中通过控制人类代理水平来实现人机互补，就像分类任务中那样。

Method: 使用预训练AI代理将人类可采取的行动限制到子集，然后让人类从该行动集中选择行动，并引入利用动作集平滑特性的bandit算法优化人类代理水平。

Result: 在1600人参与的大规模研究中，使用该系统的参与者在野火缓解游戏中表现比单独行动提升30%，比AI代理提升2%以上。

Conclusion: 通过设计控制人类代理水平可以在顺序决策任务中实现人机互补，开发的方法有效提升了决策性能。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [61] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与Dreamer家族的PlaNet模型进行比较，在潜在重建质量和图像感知相似性方面表现相当，且在长时域预测上表现更优。


<details>
  <summary>Details</summary>
Motivation: 世界模型在强化学习中广泛应用，但现有模型如Dreamer需要训练过程。本文旨在探索无需训练的世界模型，通过相似性搜索和随机表示来近似环境动态。

Method: 使用相似性搜索和随机表示来构建世界模型，无需传统训练过程。与PlaNet模型进行比较评估。

Result: 搜索式世界模型在潜在重建和图像感知相似性方面与基于训练的模型相当，在长时域预测上在多种视觉不同环境中表现优于基线模型。

Conclusion: 基于搜索的世界模型可以无需训练过程实现与训练模型相当的性能，特别是在长时域预测任务中表现更优。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [62] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 首次对时变学习策略下的Q-learning算法进行有限时间分析，在最小假设下证明其收敛性，揭示其相比离策略Q-learning具有较弱的探索性但更强的利用性优势。


<details>
  <summary>Details</summary>
Motivation: 传统Q-learning分析多基于离策略采样或强探索假设，而对时变学习策略（即同策略采样）下的Q-learning缺乏有限时间分析，这限制了我们对实际RL算法性能的理解。

Method: 采用改进的分析方法，利用泊松方程将马尔可夫噪声分解为鞅差项和残差项，并通过敏感性分析控制时变策略下的残差项。

Result: 建立了Q函数估计的最终迭代收敛速率，样本复杂度为O(1/ε²)，与离策略Q-learning匹配但探索参数依赖更差；同时推导了策略价值函数的收敛速率。

Conclusion: 同策略Q-learning在探索方面弱于离策略版本，但在利用方面具有优势，其策略会收敛到最优策略而非保持固定。所开发的分析工具对分析其他时变策略RL算法具有独立价值。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [63] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出NAMEx框架，通过纳什议价理论改进稀疏专家混合模型的专家合并策略，实现更平衡高效的专家协作


<details>
  <summary>Details</summary>
Motivation: 现有SMoE专家合并策略缺乏原则性加权机制，需要更平衡的专家协作方法

Method: 基于博弈论视角，将纳什议价理论引入专家合并过程，并加入复动量加速专家传播

Result: 在语言建模、文本分类、图像分类和零样本鲁棒性任务中持续优于竞争方法，在Qwen1.5-MoE和DeepSeek-MoE等大规模系统中验证有效性

Conclusion: NAMEx为专家合并提供了理论保证的平衡协作框架，具有良好的可扩展性和实用性

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [64] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合零阶优化和锐度感知最小化的新方法，通过指数倾斜目标在平均损失和最大损失之间平滑过渡，实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的函数（扰动参数下的期望目标），而SAM方法关注邻域内最大损失以获得平坦最小值。本文旨在明确连接这两种方法。

Method: 提出指数倾斜目标，通过倾斜参数t在平均损失和最大损失公式之间平滑过渡，开发新的零阶算法来求解软SAM目标。

Result: 该方法在分类、多项选择问答和语言生成等下游任务上，相比传统零阶基线方法实现了更好的泛化性能。

Conclusion: 该方法可作为SAM变体的无梯度和内存高效替代方案，在保持计算效率的同时提升模型泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [65] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE：基于GRU的模型，使用指数基函数处理不规则采样多变量时间序列，在连续时间中支持回归和事件预测，性能与SOTA方法相当或更好，但更简单高效。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样时间序列建模的挑战，验证简单RNN架构是否仍能与复杂方法竞争，提供更高效实用的解决方案。

Method: 在GRU基础上引入两种重置机制：观测触发重置和时间触发重置，使用可学习指数衰减更新马尔可夫状态，支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在下一观测和下一事件预测任务上达到与SOTA方法竞争甚至更优的性能。

Conclusion: GRUwE证明了简单RNN架构的竞争力，具有易于实现、超参数调优少、计算开销低等优势，适合在线部署。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [66] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 系统性地比较了三种代表性生成模型（AtomGPT、CDVAE、FlowMM）在超导材料数据集上的性能表现，发现CDVAE表现最佳


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料发现中应用日益广泛，但缺乏对其性能的严格比较评估

Method: 在JARVIS Supercon 3D和Alexandria DS A/B两个公开超导数据集上训练三种模型，使用KL散度和MAE评估晶格参数预测性能

Result: CDVAE表现最优，其次是AtomGPT，FlowMM表现相对较差

Conclusion: CDVAE在晶体结构生成任务中表现最佳，为材料发现提供了有价值的基准

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [67] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 该论文通过层间因果修补分析语言模型对齐机制，发现对齐过程是空间局域化的，主要集中在中间层激活，而非扩散的参数化过程。


<details>
  <summary>Details</summary>
Motivation: 尽管基于人类反馈的强化学习（RLHF）已成为语言模型对齐的主流方法，但其内部工作机制仍不透明，需要系统分析对齐是如何实现的。

Method: 在基础模型和调优模型之间应用层间因果修补，使用LASSO回归分析激活距离与奖励增益的关系。

Result: 对齐是空间局域化的：中间层激活编码了决定奖励一致行为的独特子空间，而早期和晚期层基本不受影响；只有少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个方向性的低秩过程，而非扩散的参数化过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [68] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 本文通过将群等变卷积（旋转和尺度等变层）嵌入标准CNN中，研究了一种架构层面的对抗鲁棒性方法，无需对抗训练即可提升模型对对抗攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 对抗训练作为主要防御策略存在计算成本高且可能影响干净数据准确性的问题，因此需要探索更高效的架构层面解决方案。

Method: 提出两种对称感知架构：并行设计（独立处理标准和等变特征后融合）和级联设计（顺序应用等变操作），利用群等变卷积编码对称先验。

Result: 在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，模型在FGSM和PGD攻击下持续提升对抗鲁棒性和泛化能力，无需对抗训练。

Conclusion: 对称强制架构有潜力成为基于数据增强防御的高效且原则性替代方案，能减少假设空间复杂度、正则化梯度并在CLEVER框架下获得更紧的认证鲁棒性界限。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [69] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 该论文主张强化学习研究应从单纯展示智能体性能转向更关注学习动态理解和科学进步，并需要更精确地将基准测试映射到数学形式化。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度关注性能表现而忽视学习动态理解，这可能导致在学术基准上过拟合，难以将技术迁移到新问题，并削弱了旨在改进理解的研究价值。

Method: 以Arcade Learning Environment (ALE)为例，说明即使被认为是"饱和"的基准仍可有效用于发展对强化学习的理解，并促进RL技术在现实问题中的部署。

Result: 论文提出了强化学习研究方向的转变建议，强调需要平衡性能展示与科学理解，并改进基准测试与数学形式化之间的映射关系。

Conclusion: 强化学习社区应该停止仅关注智能体能力展示，而应更多地关注推进强化学习的科学理解和基础研究，同时需要更精确地定义基准测试与理论框架的关系。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [70] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于运行时监控语言(RML)的新型语言化奖励机，能够表达非正则、非马尔可夫任务的奖励函数，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数通常被视为黑盒映射，缺乏解释性，而传统奖励机虽然能表示结构化奖励函数，但表达能力受限于正则语言，无法处理计数或参数化条件等复杂行为。

Method: 基于运行时监控语言(RML)构建语言化奖励机，利用RML的内置内存机制来指定非正则、非马尔可夫任务的奖励函数。

Result: 实验证明了该方法在表达能力上的优势，相比现有基于奖励机的方法，在灵活事件处理和任务规范方面具有额外优势。

Conclusion: 基于RML的语言化奖励机扩展了奖励函数的表达能力，能够处理更复杂的非正则、非马尔可夫任务，为强化学习提供了更好的可解释性和灵活性。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [71] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与物体中心表示的新框架，能够处理结构化和非结构化数据，并通过主动查询人类专家来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在结构化问题上存在局限性，关系强化学习虽然能处理结构化问题但对问题结构有强假设。需要一种能同时处理结构化和非结构化数据的框架。

Method: 结合关系强化学习与物体中心表示，通过显式建模策略不确定性来主动查询人类专家指导。

Result: 实证评估表明所提方法在有效性和效率方面表现优异。

Conclusion: 该框架成功解决了关系强化学习的局限性，在结构化和非结构化数据上都取得了良好效果，并通过主动学习机制提高了学习效率。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [72] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 该论文研究了一个非平稳多臂老虎机问题，其中奖励取决于动作和潜在状态，状态由未知线性动力学控制。作者提出了一个探索-承诺算法，在探索阶段使用随机动作估计线性动力学的马尔可夫参数，在承诺阶段优化动作序列以获得长期奖励，实现了O(T^{2/3})的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究动作和潜在状态共同影响奖励的非平稳老虎机问题，其中状态动力学也依赖于动作，这导致了短期和长期奖励之间的权衡问题。

Method: 提出探索-承诺算法：探索阶段使用随机Rademacher动作估计线性动力学的马尔可夫参数；承诺阶段使用估计参数设计优化的动作序列以获得长期奖励。

Result: 算法实现了O(T^{2/3})的遗憾上界。解决了两个关键挑战：从时间相关奖励中学习，以及设计具有最优长期奖励的动作序列。

Conclusion: 通过系统辨识和不定二次优化问题的等价性证明，提供了有效的算法框架，并提出了使用半定松弛和Goemans-Williamson舍入的实用方法。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [73] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 本文对标签噪声检测方法进行了系统性基准测试，通过将方法分解为三个核心组件（标签一致性函数、聚合方法和信息收集方式），发现使用对数边际作为标签一致性函数、平均概率聚合和样本内信息收集的组合在多数场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中的标签噪声会影响模型训练和验证效果，需要干净的标签数据来保证性能和可靠评估。虽然已有多种噪声标签检测技术，但缺乏对最佳方法的明确共识。

Method: 将噪声检测方法分解为三个基本组件：标签一致性函数、聚合方法和信息收集方式（样本内vs样本外）。提出统一基准任务和新的评估指标，在视觉和表格数据集上评估合成和真实噪声条件。

Result: 研究发现，使用对数边际作为标签一致性函数、平均概率聚合和样本内信息收集的组合在大多数场景下取得了最佳结果。

Conclusion: 该研究为设计新的检测方法和为特定应用选择技术提供了实用指导，识别出了在各种噪声条件下表现稳定的检测方法组件组合。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [74] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究应用机器学习预测欧洲绿色协议气候政策的进展状态，比较了不同文本表示方法，发现ClimateBERT在纯文本特征上表现最佳，而BERT结合元数据特征时性能最优。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来缓解其影响，本研究旨在探索机器学习如何帮助理解气候政策从宣布到采纳的进展过程。

Method: 构建了包含165项政策的文本和元数据数据集，比较了TF-IDF、BERT和ClimateBERT等文本表示方法，并评估元数据特征对预测性能的影响。

Result: 仅使用文本特征时，ClimateBERT表现最佳（RMSE = 0.17, R² = 0.29）；结合元数据特征时，BERT性能最优（RMSE = 0.16, R² = 0.38）。可解释AI方法显示政策措辞、政党和国家代表性等因素具有重要影响。

Conclusion: 机器学习工具在支持气候政策分析和决策制定方面具有巨大潜力，能够帮助理解政策进展的关键影响因素。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [75] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 该论文分析了神经网络中一比特权重压缩的理论基础，证明了在随机特征模型中，除最后一层外所有层的权重量化不会导致泛化误差损失，并提供了理论分析和实证验证。


<details>
  <summary>Details</summary>
Motivation: 神经网络的计算和内存需求日益增长，促使研究者关注一比特权重压缩以在资源受限设备上实现高效推理，但相关理论基础尚不明确。

Method: 使用随机特征模型作为简化框架，分析一比特量化对神经网络的影响，通过理论证明和实证实验验证量化效果。

Result: 理论证明除最后一层外所有层权重量化不会损失泛化误差；实证显示一比特量化在笔记本电脑GPU上显著提升推理速度；提供了随机特征模型泛化误差的精确渐近表征。

Conclusion: 一比特权重压缩在理论上可行且在实践中有效，为神经网络压缩提供了理论依据，研究结果比现有文献更具普适性。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [76] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个用于强化学习Web代理的可扩展环境，通过紧凑的浏览器环境和高效的服务端管理，解决了现有环境在上下文噪声、动作确定性和扩展性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RL Web代理环境存在上下文信息过多且嘈杂、动作执行不确定、无法有效扩展并行RL训练等问题，需要一个新的可扩展且高效的环境。

Method: 提出WEBSERV环境，包含：1）紧凑、站点无关的浏览器环境，平衡上下文和动作复杂性；2）通过高效启动和重置Web服务器实现可扩展的RL环境。

Result: 在WebArena的购物CMS和Gitlab任务上达到最先进的单提示成功率，同时将启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，支持单主机200+并发容器。

Conclusion: WEBSERV提供了一个可扩展且高效的RL Web代理训练和评估环境，显著提升了性能并降低了资源需求。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [77] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经ODE的连续深度Evoformer模型，用连续时间参数化替代AlphaFold中48个离散块，实现恒定内存成本和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: AlphaFold等蛋白质结构预测模型虽然强大，但其48层Evoformer架构计算成本高且层间离散化。希望开发更轻量、高效的连续深度替代方案。

Method: 将Evoformer的48个离散块替换为神经ODE参数化，保持核心注意力操作，利用伴随方法实现恒定内存成本，通过自适应ODE求解器平衡运行时间和精度。

Result: 模型能生成结构合理的预测并可靠捕获α-螺旋等二级结构元素，虽未完全复现原架构精度，但仅用单GPU训练17.5小时，资源消耗大幅降低。

Conclusion: 连续深度模型为生物分子建模提供了轻量且可解释的替代方案，为高效自适应蛋白质结构预测框架开辟了新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [78] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 该论文从范畴论角度分析超边解缠，提出了基于自然性条件的解缠新准则，并在基因通路数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管解缠表示学习在图结构数据中取得了良好效果，但很少有研究探索超图结构数据的解缠。将超边解缠整合到超图神经网络中，可以利用与标签相关的隐藏超边语义。

Method: 从范畴论视角分析超边解缠，提出基于自然性条件的解缠新准则，并开发了概念验证模型。

Result: 实验证明所提准则能够成功捕捉基因通路中基因（节点）之间的功能关系。

Conclusion: 提出的解缠准则在超图数据中具有潜力，能够有效发现隐藏的语义关系。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [79] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出结合SVD和量化的方法来降低视觉语言模型的KV缓存大小和计算开销，通过动态秩分配策略在保持精度的同时显著减少内存使用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的高计算成本和内存占用限制了其可扩展性和实时应用性，需要更高效的部署方案。

Method: 使用奇异值分解(SVD)处理QKV权重矩阵来减少KV缓存，引入动态秩分配策略，并结合权重量化和激活量化。

Result: 相比仅使用量化或SVD的方法，在更低的硬件成本下实现了超过10%的精度提升。

Conclusion: 该方法适合在资源受限设备上进行实时部署，提供了高效的视觉语言模型压缩方案。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [80] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于支架的虚拟筛选框架，通过生成AI增强数据、自训练和重排序三个模块，解决虚拟筛选中的类别不平衡、结构不平衡和支架多样性问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三个主要挑战：类别不平衡（活性分子比例低）、活性分子间的结构不平衡（某些支架占主导）、以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于实际命中分子的支架生成合成数据；2. 自训练模块：将生成数据与原始标记数据安全整合；3. 重排序模块：提高推荐分子集中的支架多样性。

Result: 在五个靶标类别上的综合计算实验表明，ScaffAug在多个评估指标上优于现有基线方法，同时通过消融研究验证了各模块的有效性。

Conclusion: 该工作通过利用生成增强、重排序和支架感知，为有效增强虚拟筛选提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [81] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一个针对有向图的对比学习框架，通过复数域和实数域两个互补的空间视角来提取有向图的方向性信息，在节点分类和链接预测任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法主要关注无向图，忽略了真实网络（如社交网络和推荐系统）中至关重要的方向性信息。

Method: 从复数域视角，在磁拉普拉斯矩阵中引入个性化扰动来调制边相位和方向语义；从实数域视角，使用基于路径的子图增强策略来捕捉细粒度局部不对称性和拓扑依赖；联合利用这两个互补空间视图构建高质量正负样本。

Result: 在7个真实世界有向图数据集上的实验表明，该方法在节点分类任务上提升4.41%，在链接预测任务上提升4.34%，在监督和无监督设置下均达到SOTA性能。

Conclusion: S2-DiGCL通过有效利用有向图的方向性信息，实现了更通用和鲁棒的有向图对比学习。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [82] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 该论文探讨了深度学习中的记忆化与组合能力之间的协同作用，表明记忆化长尾特征与组合能力结合可以帮助模型对未见过的长尾特征组合做出正确预测。


<details>
  <summary>Details</summary>
Motivation: 重新思考深度学习中记忆化与泛化的关系，研究记忆化如何与简单组合能力协同作用，帮助模型处理训练数据中未出现的长尾特征组合。

Method: 在线性设置中进行理论分析，证明记忆化与组合能力协同作用的机制；在神经网络架构上进行实验，验证理论洞察在非线性设置中的适用性。

Result: 理论分析表明记忆化与组合能力结合可以使模型对罕见测试样本（需要长尾特征组合）做出正确预测，即使这些组合在训练数据中从未出现过。实验证实理论洞察可扩展到非线性设置，并发现模型的组合能力取决于其架构。

Conclusion: 记忆化与组合能力的协同作用有助于模型泛化到未见过的长尾特征组合，模型的组合能力受其架构影响，这一发现为理解深度学习中的泛化机制提供了新视角。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [83] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出MGTS-Net多模态图增强网络，通过优化特征提取、构建异构图融合多模态信息、动态加权多尺度预测器，解决时间序列预测中细粒度模式提取不足、多模态信息融合欠佳、动态多尺度特征适应有限三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有融合多模态特征的时间序列预测方法面临三个关键限制：细粒度时间模式提取不足、多模态信息融合欠佳、动态多尺度特征适应有限，制约了预测精度的提升。

Method: MGTS-Net包含三个核心组件：(1)多模态特征提取层，针对时序、视觉和文本模态优化特征编码器；(2)多模态特征融合层，构建异构图建模模态内时序依赖和跨模态对齐关系；(3)多尺度预测层，动态加权融合短期、中期和长期预测器输出。

Result: 大量实验表明MGTS-Net在轻量高效的同时表现出优异性能，相比其他最先进基线模型取得了更优的性能。

Conclusion: MGTS-Net通过提出的方法学在时间序列预测中实现了优越性能，验证了所提方法的有效性。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [84] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出了一种稀疏Transformer架构，将数据分布的先验信息直接融入神经网络结构，基于正则化Wasserstein近端算子的最优传输问题设计模型，相比传统流模型改善了优化问题的凸性并促进生成样本的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 将数据分布的先验信息直接整合到Transformer架构中，以改善生成模型的性能，特别是通过最优传输理论来增强模型的数学基础。

Method: 基于正则化Wasserstein近端算子的最优传输问题设计稀疏Transformer架构，该算子具有闭式解且与Transformer结构有特殊对应关系。

Result: 理论分析和数值实验表明，稀疏Transformer在生成建模和贝叶斯反问题中比传统基于神经ODE的方法具有更高精度和更快收敛速度。

Conclusion: 所提出的稀疏Transformer架构通过融入最优传输理论，显著提升了生成模型的性能，在准确性和收敛速度方面优于传统方法。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [85] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE是一种新型稀疏专家混合模型，通过引入专家间的社交图来增强令牌路由过程，解决了传统SMoE在数据分布变化下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏专家混合模型(SMoE)虽然能解耦模型参数量和计算成本，但在面对数据分布变化时鲁棒性较差，容易受到数据污染的影响。

Method: 提出SymphonySMoE，在SMoE基础上引入专家间的社交图结构来建模专家交互，改进令牌路由过程。该方法轻量级、模块化，可与现有SMoE模型无缝集成。

Result: 在语言建模和视觉指令调优任务上的广泛实验验证了方法的有效性，理论分析和实证证据均表明SymphonySMoE优于基线SMoE。该方法可扩展到42亿和74亿参数的大模型。

Conclusion: SymphonySMoE通过图结构增强专家交互，显著提升了SMoE在分布变化下的鲁棒性，同时保持了模型的轻量化和可扩展性，适用于大规模系统的微调任务。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [86] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 该论文提出了在ECML-PKDD 2025高能物理发现挑战赛中Task 1的获胜解决方案，通过多轮梯度攻击方法实现了最佳扰动效果和欺骗成功率。


<details>
  <summary>Details</summary>
Motivation: 设计对抗攻击方法，在最小化扰动的同时最大化分类模型的误分类率，以应对高能物理发现中的鲁棒学习挑战。

Method: 采用多轮梯度攻击策略，利用模型的可微分结构，结合随机初始化和样本混合技术来增强攻击效果。

Result: 攻击方法在扰动大小和欺骗成功率方面取得了最佳结果，在竞赛中获得第一名。

Conclusion: 提出的多轮梯度攻击方法结合随机初始化和样本混合技术，能够有效生成对抗样本，在保持小扰动的同时实现高欺骗率。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [87] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了ECML-PKDD 2025高能物理发现挑战赛中Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在对抗性攻击下实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够同时在干净数据和对抗性数据上保持高准确率的鲁棒ANN模型，解决高能物理发现中的对抗攻击问题。

Method: 采用两阶段方法：1) 基于RDSA的自定义方法生成1500万人工训练样本；2) 构建包含特征嵌入块（共享权重）和密集融合尾部的鲁棒架构进行训练。

Result: 在对抗性数据集上训练该架构，获得了80%的混合准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法在对抗性攻击下表现优异，证明了数据增强和专门架构设计对于提升模型鲁棒性的有效性。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [88] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，解决了现有稀疏专家混合模型在专家专业化和计算平衡之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏专家混合模型的路由机制基于相似性评分，难以有效捕捉输入结构，导致专家专业化与计算平衡之间的权衡，限制了可扩展性和性能。

Method: 使用概率混合模型建模路由概率，使专家形成清晰的专业化边界并实现均衡利用。路由机制独立于任务特定目标进行训练，实现稳定优化和明确的专家分配。

Result: 在视觉语言任务上的实证结果表明，该方法持续优于现有稀疏专家混合模型方法，实现了更高的任务性能和改善的专家利用平衡。

Conclusion: Input Domain Aware MoE通过概率混合模型的路由框架，有效解决了稀疏专家混合模型中的路由问题，提升了模型性能和专家利用效率。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [89] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一个序列强化学习框架用于蜜蜂模仿学习，能建模异质认知策略，解决现有方法在专家策略变化时无法捕捉快速和慢速学习行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在蜜蜂行为建模中存在局限：当专家策略在记忆窗口间变化或偏离最优时，无法准确捕捉不同学习行为，且缺乏可解释性，阻碍生物学洞察。

Method: 引入最小化预测损失的模型，识别与行为数据最一致的有效记忆范围；确保完全可解释性；提供数学框架将蜜蜂策略搜索与多臂老虎机问题联系起来。

Result: 创建了包含80只蜜蜂在不同天气条件下追踪数据的新数据集，改进了农业生态系统中昆虫行为的模拟。

Conclusion: 该框架揭示了授粉者决策中学习策略和记忆相互作用的新机制，为授粉者认知研究和生态治理提供了支持。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [90] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出一种新颖的自适应核注意力机制，通过分别处理不同特征组并整合，解决高维异构数据中复杂非线性关系和跨尺度交互的建模难题。


<details>
  <summary>Details</summary>
Motivation: 传统投影潜在结构(PLS)方法难以建模复杂非线性关系，特别是在具有高维相关结构的多元系统中。同时跨尺度交互和静态特征加权限制了模型对上下文变化的适应性。

Method: 引入自适应核基注意力机制，分别处理不同特征组后进行整合，能够捕捉局部模式同时保持全局关系。

Result: 实验结果显示，与最先进方法相比，在多个数据集上性能指标均有显著提升。

Conclusion: 所提出的方法通过新颖的架构创新有效提升了预测性能，解决了高维异构数据建模中的关键挑战。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [91] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单可解释的无监督多元时间序列异常检测框架，通过因果嵌入建模时间动态，使用自注意力机制捕获空间关系，并将嵌入对齐到稳定潜在结构来检测异常。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多元时间序列异常稀少且通常无标签，现有方法依赖复杂架构，只能检测异常片段且性能被夸大。

Method: 将每个变量的历史序列编码为因果嵌入来联合预测当前时间点和重建输入窗口，使用自注意力机制将嵌入投影到共享潜在空间，并与代表正常状态关系的稳定潜在结构对齐。

Result: 在多个真实世界数据集和评估协议上实现了最先进的结果。

Conclusion: OracleAD通过稳定潜在结构直接定位根因变量，同时保持可解释性，提供细粒度的异常诊断。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [92] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出eDCF方法，基于连通性因子(CF)来鲁棒估计多尺度下的内在维度，在噪声和大数据集下表现优异，并能检测分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 现代数据集常包含具有复杂依赖关系的高维特征，内在维度估计因尺度依赖性而具有挑战性：细尺度下噪声会膨胀估计值，粗尺度下估计值稳定在较低的尺度不变值。

Method: 引入基于连通性因子(CF)的eDCF方法，这是一种可扩展、可并行化的局部连通性度量方法。

Result: 在合成基准测试中与领先估计器表现相当，MAE值相近；在中等至高噪声水平和大数据集下，精确内在维度匹配率高达25.0%，优于MLE(16.7%)和TWO-NN(12.5%)；能准确检测决策边界中的分形几何结构。

Conclusion: eDCF方法在多尺度内在维度估计中表现鲁棒，特别适用于噪声环境和大规模数据集，并能有效分析现实结构化数据。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [93] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: LLMs在因果发现中的表现被高估，因为评估基准可能已包含在预训练数据中。需要开发防泄漏的评估协议和结合LLM知识与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs在因果发现中表面上的优异表现，质疑其是否真正进行因果推理而非记忆，并探讨如何在没有记忆问题的情况下评估其能力。

Method: 提出两个转变：(P1)基于最新科学研究开发防数据泄漏的鲁棒评估协议；(P2)设计结合LLM知识与数据驱动统计的混合方法。使用训练截止后发布的新科学研究构建评估图。

Result: 相比BNLearn基准中LLMs近乎完美的准确率，在作者构建的图上表现差得多。将LLM预测作为经典PC算法的先验可显著提高准确率，优于纯LLM和纯统计方法。

Conclusion: 呼吁社区采用基于科学、防泄漏的基准，并投资适合真实世界探究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [94] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 本研究使用机器学习算法预测生活满意度，准确率达到93.80%，并通过特征学习提取了27个关键问题。同时探索了临床和生物医学大语言模型的应用，发现生物医学领域与生活满意度预测更相关。


<details>
  <summary>Details</summary>
Motivation: 传统的生活满意度测量方法存在验证和传播问题，本研究旨在利用机器学习和LLM技术提供更准确、可复现的预测方法，以帮助理解人类主观幸福感。

Method: 使用丹麦19000人的政府调查数据，采用特征学习技术提取关键问题，并探索将表格数据转换为自然语言句子，利用临床和生物医学LLM进行预测。还进行了消融研究分析数据重采样和特征选择的影响。

Result: 机器学习模型达到93.80%准确率和73.00%宏F1分数，LLM模型达到93.74%准确率和73.21%宏F1分数。健康状况是所有年龄段最重要的决定因素。

Conclusion: 机器学习、大语言模型和可解释AI可以共同构建对使用AI研究人类行为的信任和理解，对量化主观幸福感具有重要意义。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [95] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 提出NeurIPT基础模型，通过预训练Transformer处理EEG信号，解决跨被试、跨任务和跨条件的变异性问题，在多个BCI数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: EEG数据存在显著的被试间、任务间和条件间变异性，以及不同记录设置的电极配置差异，这使得建立可扩展和泛化的基础模型具有挑战性。

Method: 提出AAMP基于信号幅度的掩码预训练、渐进混合专家架构、利用电极3D物理坐标的空间嵌入，以及微调时的脑区内-区间池化方法。

Result: 在8个下游BCI数据集上的评估显示，NeurIPT持续实现最先进性能，展现出广泛的适用性和强大的泛化能力。

Conclusion: 该工作推动了EEG基础模型的发展，为可扩展和可泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [96] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO是一个新的强化学习框架，通过分离语言反馈和数值奖励的角色来解决LLM训练中的样本效率问题。语言指导探索，数值奖励驱动优化，显著提升了数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习使用标量奖励会丢弃有价值的文本推理过程，导致样本效率低下。直接整合在线经验存在信息泄露或行为崩溃的风险。

Method: LANPO构建动态经验池，采用奖励无关反思进行样本内自校正，通过相关抽象从样本间经验中提取可泛化的教训。

Result: 在数学推理基准测试中，7B和14B模型使用LANPO显著优于GRPO基线，测试准确率更高。

Conclusion: LANPO为将历史经验整合到LLM强化学习循环中提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [97] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C-SMILES分子表示方法，通过分解SMILES为元素-标记对并引入复制增强机制，显著提升逆合成预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测精度降低。

Method: 使用C-SMILES表示法分解分子结构，结合复制增强机制和SMILES对齐指导，动态决定生成新标记或保留未变化分子片段。

Result: 在USPTO-50K数据集上达到67.2%的top-1准确率，USPTO-FULL上达到50.8%，生成分子有效性达99.9%。

Conclusion: 为结构感知分子生成建立了新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [98] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标注数据的分子推理框架，通过原子标识符将思维链推理锚定到分子结构上，在单步逆合成任务中实现了高成功率。


<details>
  <summary>Details</summary>
Motivation: 化学中机器学习应用常受限于标注数据的稀缺和昂贵，限制了传统监督方法的使用。

Method: 使用通用大语言模型，通过原子标识符将思维链推理锚定到分子结构，先进行一次性任务识别相关片段和化学标签，然后在可选步骤中使用位置感知信息进行少样本任务预测化学转化。

Result: 在学术基准和专家验证的药物发现分子中，LLMs在识别化学可行反应位点（≥90%）、命名反应类别（≥40%）和最终反应物（≥74%）方面取得了高成功率。

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种通过将化学知识映射到分子结构来生成理论基础的合成数据集的方法，从而解决数据稀缺问题。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [99] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 该论文研究了神经网络中参数对称性和表达能力在泛化行为中的作用，特别是在学习实空间重整化群变换时。通过多层感知机和图神经网络实验，揭示了对称约束与表达能力之间的竞争关系。


<details>
  <summary>Details</summary>
Motivation: 将物理对称性编码到深度学习模型中可以提高性能，但参数对称性破坏和恢复机制在分层学习动态中的作用需要进一步研究。

Method: 使用多层感知机和图神经网络，通过改变权重对称性和激活函数来评估网络表达能力。将中心极限定理作为测试案例，通过累积量递推关系分析泛化行为。

Result: 研究发现过于复杂或过度约束的模型泛化能力较差。通过累积量传播框架，解析地证明了某些约束MLP架构的泛化问题，并将该框架扩展到GNN。

Conclusion: 这些发现为对称网络的学习动态及其在建模结构化物理变换中的局限性提供了新的见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [100] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种四元数监督学习Hopfield结构神经网络(QSHNN)，利用四元数的几何优势表示旋转和姿态，通过周期性投影策略保持四元数结构一致性，在机器人控制等应用中表现出高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态方面的几何优势，将经典Hopfield神经网络扩展到四元数域，为超复数或非交换代数结构下的神经网络设计提供数学方法。

Method: 从连续时间HNN动力学模型出发扩展到四元数域，引入周期性投影策略修改标准梯度下降，定期将权重矩阵的4*4块投影到最接近的四元数结构，保持收敛性和四元数一致性。

Result: 实验模型实现高精度、快速收敛和强可靠性，演化轨迹具有良好有界曲率（足够平滑性），适用于机器人臂关节姿态参数化等控制应用。

Conclusion: 该模型为超复数或非交换代数结构下的神经网络设计提供了实用的实现框架和通用数学方法，特别适用于机器人控制等需要平滑轨迹的应用场景。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [101] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文研究了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息的关系，通过图论方法分析了多步推理所需的增强步骤数量与先验知识密度的关系。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型参数知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下回答查询所需的预训练知识量。

Method: 将多步推理建模为知识图上的s-t连通性问题，将模型的预训练参数知识表示为部分、可能有噪声的子图，将增强视为查询真实边来扩展模型知识。

Result: 发现了相变现象：如果先验知识图断开成小分量，则通过增强找到路径效率低下，需要Ω(√n)次查询；一旦正确知识密度超过阈值形成巨分量，就能以期望常数次查询找到路径。

Conclusion: 测试时增强的效率取决于先验知识图的连通性，存在一个临界密度阈值，超过该阈值后增强步骤数量将大幅减少。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [102] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 多阶段训练模型中的机器遗忘存在根本性障碍，局部遗忘方法无法普遍实现重训练等价性，因为遗忘结果依赖于训练阶段的顺序。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习流水线通常涉及多阶段训练，每个阶段有不同的数据分布和目标，而现有的机器遗忘理论主要针对i.i.d.数据批次的训练模型。

Method: 通过理论和实验分析多阶段训练对机器遗忘的影响，使用梯度上升、NPO和SimNPO等局部遗忘算法在Llama和Qwen模型上进行实证研究。

Result: 不同训练顺序的模型在遗忘过程中行为差异显著，GSM8K准确率下降幅度在不同路径间差异超过20%，某些学习路径产生的模型遗忘速度较慢。

Conclusion: 对于多阶段训练的模型，重训练等价性是局部遗忘算法的不适定目标，需要重新思考机器遗忘的定义和期望目标。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [103] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一个同时学习物理系统结构和随机群体动力学的仿真自由方法，能够从干预中学习结构并进行条件群体动力学的轨迹推断。


<details>
  <summary>Details</summary>
Motivation: 许多自然系统中的物理系统（如细胞生物学）具有高维、随机性，且只能获得部分噪声状态测量，这给建模系统动力学和推断网络结构带来了挑战。现有方法通常只能单独处理结构学习或群体动力学建模，无法同时解决这两个问题。

Method: 提出StructureFlow方法，这是一种原理性的仿真自由方法，能够联合学习物理系统的结构和随机群体动力学。该方法支持从干预中学习结构和条件群体动力学的轨迹推断。

Result: 在高维合成系统、生物模拟系统和实验单细胞数据集上的实证评估表明，StructureFlow能够同时学习底层系统的结构并建模其条件群体动力学。

Conclusion: StructureFlow能够同时学习系统结构并建模条件群体动力学，这是理解系统行为机制的关键步骤。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [104] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一个改进的蛋白质-蛋白质对接评分函数，用Vision Mamba架构替换了PIsToN中的Vision Transformer，在多个数据集上表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: Mamba架构在自然语言处理和计算机视觉领域表现出色，经常超越基于Transformer的模型，因此作者希望利用Mamba的高效长序列建模能力来改进蛋白质-蛋白质对接评分函数。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba对图像块序列的高效长程建模能力，提升模型捕捉蛋白质-蛋白质界面全局和局部模式的能力。

Result: 在多个广泛使用的大规模公共数据集上的评估表明，PUMBA始终优于其基于Transformer的前身PIsToN。

Conclusion: 使用Vision Mamba架构替换Vision Transformer可以显著提升蛋白质-蛋白质对接评分函数的性能，证明了Mamba架构在此领域的有效性。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [105] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出一种在无信息先验条件下实现有效主动目标发现的新方法，确保在复杂现实场景中的鲁棒探索和适应性。


<details>
  <summary>Details</summary>
Motivation: 在数据极其有限或采样成本高的领域（如稀有物种发现、新兴疾病诊断等），现有基于生成模型的主动目标发现方法难以泛化，需要克服无信息先验的挑战。

Method: 基于理论原理和神经科学启发设计的新型框架，具有内在可解释性，保证每次新观测都能单调改进先验估计，实现渐进准确的采样。

Result: 在物种分布建模和遥感等多个领域的综合实验表明，该方法显著优于基线方法。

Conclusion: 该方法在动态环境中提供了可靠性和适应性的双重保证，为无信息先验条件下的主动目标发现提供了有效解决方案。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [106] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出了一个紧凑、严格因果的临床时间序列基准，在MIT-BIH心律失常数据库上使用每秒心率数据，研究心动过速风险预测和心率预测两个任务。


<details>
  <summary>Details</summary>
Motivation: 在纵向监测中，需要了解不同模型在不同任务上的表现差异，为临床时间序列分析提供指导。

Method: 使用GRU-D（RNN）和Transformer模型，在匹配的训练预算下与强非学习基线进行比较，采用温度缩放和分组bootstrap置信区间进行校准感知评估。

Result: 在MIT-BIH数据集上，GRU-D在心动过速风险预测上略优于Transformer，而Transformer在心率预测误差上明显低于GRU-D和持续性模型。

Conclusion: 模型选择是任务依赖的：紧凑RNN在短时域风险评分中保持竞争力，而紧凑Transformer在点预测中提供更清晰的增益。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [107] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文使用扩散方法精确分析带噪声的随机梯度下降(SGD)，在连续时间视角下捕捉高维设置中的统计风险和隐私损失动态，并研究了一种无需梯度敏感性显式知识的噪声SGD变体。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要提供噪声SGD的统计风险和隐私损失的各种界限，但过程的精确行为仍不清楚，特别是在高维设置中。需要更精确的分析方法来理解噪声SGD的完整动态。

Method: 采用扩散方法分析噪声SGD，提供连续时间视角；专注于带有ℓ2正则化的最小二乘问题；研究无需梯度敏感性显式知识的SGD变体，避免现有工作中通过梯度裁剪假设或强制执行敏感性的限制。

Result: 该方法能够精确捕捉高维设置中统计风险的演化和隐私损失的动态，为噪声SGD提供了更全面的理论分析框架。

Conclusion: 扩散方法为分析噪声SGD提供了有效的连续时间视角，能够精确描述高维设置中的统计风险和隐私损失动态，同时提出的无需显式梯度敏感性知识的SGD变体具有实际应用价值。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [108] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过利用空间相关性和时间频率特征来提高零样本预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 零样本预测旨在预测没有直接历史数据的未见条件下的结果，这对传统预测方法构成重大挑战。

Method: 将信号分解为不同频率分量，采用分辨率感知检索：低频分量依赖更广泛的空间上下文，高频分量关注局部影响，从而动态检索相关数据并适应新位置。

Result: 在微气候预测中，该模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE降低71%，比Chronos降低34%。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [109] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，证明在特定条件下状态级因果效应比变量级因果效应更容易识别，特别是当存在上下文特定独立性和条件函数依赖等额外知识时。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性基于变量层面，但实际应用中可能更关注特定状态间的因果效应。作者希望探索状态级因果效应在什么条件下可识别，以及这种识别能力如何超越变量级方法。

Method: 通过理论分析，研究状态级因果效应的可识别性条件，特别关注上下文特定独立性、条件函数依赖等额外知识的作用，并与变量级可识别性进行对比。

Result: 发现状态级因果效应在变量级效应不可识别时仍可能可识别，这种分离仅在存在额外知识时发生。变量状态约束知识单独不能改善可识别性，但与其他知识结合时可同时改善变量级和状态级可识别性。

Conclusion: 状态级因果效应可识别性框架能够发现传统变量级方法可能遗漏的可识别情况，为从观测数据估计特定因果效应提供了新的理论工具。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [110] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 提出基于LSTM的电动汽车充电负荷预测框架，通过数据预处理和特征提取，实现对多时间尺度充电需求的准确预测。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，准确预测充电负荷对电网规划、能源管理和充电设施整合至关重要。

Method: 使用LSTM神经网络处理多地点原始数据，通过插值和归一化预处理，提取特征训练模型以捕捉短期波动和长期趋势。

Result: 实验结果表明模型能够准确预测日、周、月等多时间尺度的充电需求，为基础设施规划提供有价值见解。

Conclusion: 该框架的模块化设计使其能够适应不同充电地点的使用模式，具有广泛的应用前景。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [111] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种多任务学习方法，使用潜在变量多输出高斯过程来预测NLP模型的学习曲线，支持零样本预测并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以帮助做出明智决策，满足特定性能目标，同时减少计算开销和数据集获取与整理的成本。

Method: 将学习曲线预测任务构建为多任务学习问题，使用潜在变量多输出高斯过程建模任务间和层次间的共享信息与依赖关系。

Result: 该方法能够在较低成本下开发概率性缩放定律，通过主动学习策略减少预测不确定性，提供接近真实缩放定律的预测。在三个小型NLP数据集上验证了框架有效性。

Conclusion: 提出的多任务学习框架能够有效预测NLP模型的学习曲线，支持零样本预测，为模型开发提供成本效益高的决策支持。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [112] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种用于SegDeformer的联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度，同时保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现代汽车系统使用DNN进行语义分割，但现有方法未研究基于transformer的替代方案如SegDeformer，其性能更优但计算复杂度更高。需要解决SegDeformer在车载和分布式应用中的计算效率问题。

Method: 提出了联合特征和任务解码方法，应用于SegDeformer架构，在保持分割性能的同时显著降低计算复杂度。

Result: 在车载应用中，Cityscapes数据集上fps提升11.7倍(1.4到16.5fps)，ADE20K数据集上提升3.5倍(43.3到154.3fps)，mIoU与基线相当。在分布式应用中，在广泛比特率范围内达到SOTA mIoU，仅使用之前SOTA方法0.14%(ADE20K)和0.04%(Cityscapes)的云DNN参数。

Conclusion: 该方法成功降低了SegDeformer的计算复杂度，在车载和分布式语义分割应用中实现了显著的性能提升和参数效率改进。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [113] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出了SAMOSA方法，一种基于锐度感知最小化的开放集主动学习算法，通过选择典型性样本来提高模型性能，在多个数据集上实现了3%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大量数据标注，但标注成本高昂。开放集主动学习旨在从未标记数据中选择包含未知类别的信息样本，以减轻标注负担。

Method: 基于SGD和SAM的理论发现，SAMOSA根据样本典型性主动查询样本，有效识别嵌入流形中靠近模型决策边界的非典型样本。

Result: 在多个数据集上的实验表明，SAMOSA比现有最优方法提升了3%的准确率，且没有引入计算开销。

Conclusion: SAMOSA是一种有效的开放集主动学习查询算法，能够优先选择对目标类别高度信息丰富且有助于区分目标类和非目标类的样本。

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [114] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个用于3D第一人称视频游戏的多模态推理框架，通过收集大规模多样化的人类游戏数据集，训练能够实时推理的文本条件智能体。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏是实时多模态推理的挑战性环境，需要解决现有数据集规模小、多样性不足的问题。

Method: 收集大规模多样化的人类游戏数据集，学习逆动力学模型来推断缺失动作，使用行为克隆训练文本条件智能体，采用支持实时推理的自定义架构。

Result: 模型能够玩多种3D游戏并响应文本输入，证明了方法的有效性。

Conclusion: 虽然模型在多种游戏中表现良好，但仍面临长时程任务和大规模定量评估等挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [115] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 3D-GSRD是一种用于分子表示学习的3D图自编码器，通过选择性重掩码解码解决2D到3D掩码图建模的挑战，在MD17基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 将掩码图建模从2D扩展到3D面临两个冲突挑战：既要避免2D结构信息泄露到解码器，又要为重构重掩码原子提供足够的2D上下文信息。

Method: 提出3D-GSRD，核心创新是选择性重掩码解码(SRD)，只重掩码编码器表示中的3D相关信息，同时保留2D图结构。结合3D关系变换器编码器和结构无关解码器。

Result: 在广泛使用的MD17分子性质预测基准测试中，8个目标中有7个达到新的最先进性能。

Conclusion: 选择性重掩码解码与结构无关解码器相结合，增强了编码器在分子表示学习中的作用，实现了优异的3D分子表示学习性能。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [116] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 这篇综述论文全面回顾了大型语言模型的混合精度量化技术，分析了不同精度分配策略在权重、激活和KV缓存中的应用，比较了各种框架的性能表现，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的快速扩展，其计算、内存和能耗需求急剧增长，使得训练和部署变得不可持续。量化技术成为减少模型大小、缓解内存瓶颈和加速推理的关键方法，但统一低比特量化会降低敏感组件的准确性。

Method: 论文首先回顾量化基础知识，包括均匀和非均匀量化器、量化粒度和后训练量化方法。然后根据比特分配策略和精度配置对混合精度量化框架进行分类比较，分析不同组件（权重、激活、KV缓存）的精度配置。

Result: 通过比较分析发现混合精度量化在困惑度、零样本任务性能和部署权衡方面存在差异。论文还对比了LM设置中混合精度量化与早期深度神经网络方法的异同，识别了可转移的策略和面临的挑战。

Conclusion: 混合精度量化为平衡大型语言模型的效率和准确性提供了有前景的解决方案。未来研究方向包括硬件感知设计、激活量化和针对十亿参数模型的可扩展优化方法。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [117] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出了计算预算感知数据选择方法CADS，通过双层优化框架将计算预算约束整合到数据选择过程中，在视觉和语言基准测试中比基线方法性能提升高达14.42%。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略了计算预算约束，独立处理数据选择和重要性评估，但实证研究表明没有算法能在不同预算下持续优于其他方法（甚至随机选择）。因此计算预算必须成为数据选择策略的核心组成部分。

Method: 提出计算预算感知数据选择方法CADS，采用双层优化框架：内层在计算预算约束下使用选定数据子集训练模型，外层基于模型评估优化数据选择。通过概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵估计问题，将内层优化转化为外层目标中的惩罚项以提高效率。

Result: 在视觉和语言基准测试中，该方法比基线方法实现了高达14.42%的性能提升。

Conclusion: 计算预算应该成为数据选择策略的核心组成部分，提出的CADS方法通过双层优化框架有效解决了计算预算约束下的数据选择问题，显著提升了训练效率。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [118] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头添加跳跃连接来增强模型表示能力并减少KV缓存，在减少约25% KV缓存的同时提升困惑度表现。


<details>
  <summary>Details</summary>
Motivation: 扩展Transformer模型通常需要大量内存和计算成本，特别是自回归解码中的KV缓存。现有方法要么改进表达能力但KV成本不变，要么减少内存但削弱表示能力。

Method: 从第二层开始，每层重用一半的Value头来自第一层，另一半正常计算，从而将Value投影和V缓存减少近50%。理论上，将未压缩的第一层Value路由到更深层可以恢复压缩损失的信息。

Result: 在不同模型规模下，SkipV1Former相比标准MHA Transformer和一些先进变体，在减少约25% KV缓存的同时改善了困惑度。与YOCO结合时，KV缓存大小减少近50%且性能仍提升。

Conclusion: SkipV1Former提供了一种有效平衡表示能力和资源消耗的方法，可以通过仅10-15%额外计算将现有MHA Transformer检查点上训练到SkipV1Former，并能与其他先进方法无缝结合。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [119] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了因果充分性下因果强盗问题中的遗憾最小化，发现学习父节点集是次优的，证明了遗憾最小化与父节点识别之间存在根本冲突，并提出了绕过图恢复的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注识别奖励的父节点然后应用经典强盗方法，或联合学习父节点同时最小化遗憾。本文研究这些策略是否最优，并探讨遗憾最小化与因果结构学习之间的根本关系。

Method: 通过理论分析证明父节点识别与遗憾最小化存在冲突，建立了考虑动作空间组合结构的遗憾下界，并提出了绕过图恢复和父节点识别的算法。

Result: 理论证明存在实例中遗憾最小化和父节点识别是冲突目标，实验表明新方法在各种环境中显著优于现有基线方法。

Conclusion: 父节点识别对于遗憾最小化是不必要的，直接优化遗憾的算法可以实现接近最优的性能，而无需进行因果结构学习。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [120] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的半监督正未标记学习方法，用于考古预测建模，通过动态伪标记和条件随机场处理标签稀缺问题，在DEM数据和卫星图像上均取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决考古预测建模中面临的结构性标签稀缺问题——正样本稀少且大多数位置未标记，需要开发能够处理严重类别不平衡的方法。

Method: 采用半监督正未标记学习策略，实现为语义分割模型，结合动态伪标记和使用RNN实现的条件随机场来提升标签置信度。

Result: 在DEM数据集上与最先进的LAMAP方法性能相当但获得更高的Dice分数；在原始卫星图像上通过分层k折交叉验证保持性能，并产生更具可解释性的预测表面。

Conclusion: 半监督学习为在大规模稀疏标注景观中识别未发现遗址提供了一种有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [121] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出线性注意力神经算子(LANO)，通过引入代理令牌机制在保持软注意力表达能力的同时实现线性复杂度，解决了传统transformer架构中可扩展性与精度的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于transformer的神经算子面临可扩展性与精度的根本权衡：软注意力提供高精度但具有二次复杂度，而线性注意力变体降低计算成本但精度显著下降。

Method: 引入紧凑的代理令牌集(M ≪ N)来调解N个令牌之间的全局交互，这种代理注意力机制产生具有线性复杂度的算子层，同时保持软注意力的表达能力。

Result: 理论证明具有通用逼近性质，实证显示在标准基准测试中平均精度提升19.5%，超越包括Transolver在内的当前最先进神经PDE求解器。

Conclusion: LANO通过在线性复杂度和软注意力级性能之间架起桥梁，为科学机器学习应用建立了可扩展、高精度的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [122] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出了TRPINN方法，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制执行边界损失，这是与H^1(Ω)相关的正确迹空间。通过计算半范数的理论必要部分来降低计算成本，并通过避免离散化中的分母评估来增强收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理高度振荡的Dirichlet边界条件时可能失败，需要更精确的边界损失处理方法来提高收敛性和稳定性。

Method: 开发了TRPINN方法，在正确的迹空间H^{1/2}(∂Ω)中强制执行边界损失，计算半范数的必要部分，避免分母评估，并利用神经正切核分析。

Result: TRPINN在拉普拉斯方程的高度振荡Dirichlet边界条件下，即使标准PINNs失败时也能成功，性能提升1-3个数量级。

Conclusion: TRPINN通过精确的H^{1/2}(∂Ω)范数实现了向真实解的H^1(Ω)意义收敛，比标准PINNs收敛更快，在处理复杂边界条件时更有效。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [123] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 该论文提出使用双线性自编码器将表示分解为二次多项式，作为可分析的非线性潜在表示方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器依赖于输入进行解释，其孤立研究不完整。多项式作为代数基元可以在不依赖输入的情况下进行分析，并能描述从线性概念到复杂流形的各种结构。

Method: 使用双线性自编码器高效地将表示分解为二次多项式，并引入改进以诱导重要性排序、聚类和激活稀疏性。

Result: 开发了一种通过代数属性分析非线性潜在表示的方法，这是向可分析非线性潜在表示迈出的初步步骤。

Conclusion: 多项式分解为研究神经网络中可解释的潜在表示提供了一种有前景的方法，允许在不依赖输入的情况下进行代数分析。

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [124] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一个原型引导的多模态分子表示学习框架，通过层次化编码器和双向跨模态注意力机制，实现分子图与文本描述的细粒度整合和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法存在两个关键局限：(1)仅在最终编码层进行跨模态交互，忽略了层次语义依赖；(2)缺乏统一的原型空间来实现模态间的鲁棒对齐。

Method: 采用双分支层次编码器（图神经网络处理分子图，Transformer编码文本），引入层间双向跨模态注意力机制，并构建共享原型空间与可学习的类特定锚点。

Result: 在多个基准数据集上的广泛实验表明，ProtoMol在各种分子性质预测任务中始终优于最先进的基线方法。

Conclusion: ProtoMol通过原型引导的多模态框架，有效解决了现有方法的局限性，实现了分子图与文本描述的细粒度整合和一致语义对齐，显著提升了分子性质预测性能。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [125] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现1.04%的风洞验证精度，比现有数据集提升5倍，为机器学习驱动的空气动力学优化建立了新标准。


<details>
  <summary>Details</summary>
Motivation: 传统汽车空气动力学优化面临计算成本高与精度不足的困境，现有机器学习数据集存在网格分辨率不足、组件缺失和验证误差超过5%等问题，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用精细网格策略和严格的壁面y+控制。

Result: 数据集实现1.04%的风洞验证精度，比现有数据集提升5倍；基于该数据训练的模型在保持生产级精度的同时，将计算成本从数周减少到几分钟。

Conclusion: DrivAerStar首次搭建了学术机器学习研究与工业CFD实践之间的桥梁，为数据驱动的汽车空气动力学优化建立了新标准，展示了将高保真物理模拟与AI整合的范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [126] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉回路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间并达到或超越现有最优方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续表示学习方法直接使用预训练特征进行下游任务时，在相似性匹配阶段存在多重共线性问题，且更先进的方法计算成本过高，难以满足实时低延迟应用需求。

Method: 提出Fly-CL框架，受果蝇嗅觉回路启发，渐进式解决多重共线性问题，实现更有效的相似性匹配，具有低时间复杂度。

Result: Fly-CL显著减少训练时间，在多种网络架构和数据机制下的仿真实验中，性能达到或超越当前最优方法。

Conclusion: Fly-CL通过生物启发设计有效解决了持续表示学习中的多重共线性挑战，为实时低延迟应用提供了可行解决方案。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [127] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了UDS（Utility-Diversity Sampling）框架，用于监督微调中的在线批次选择，通过同时考虑数据效用和多样性来优化训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调在完整数据集上计算成本高，容易过拟合或放大偏差。现有在线批次选择方法仅依赖数据效用，忽视多样性，需要外部资源，且会增加训练时间。

Method: UDS利用logits矩阵的核范数捕捉数据效用和样本内多样性，通过轻量级历史样本缓冲区估计样本间多样性，无需外部资源或额外反向传播。

Result: 在多个基准测试中，UDS在不同数据预算下始终优于最先进的在线批次选择方法，相比完整数据集微调显著减少训练时间。

Conclusion: UDS框架有效解决了现有在线批次选择方法的局限性，在保持性能的同时显著提升了训练效率。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [128] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个指令调优的编码器-解码器框架，通过将图结构与LLM语义紧密结合，实现无需特定任务监督的零样本图推理。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络固定标签空间和大语言模型难以捕捉图结构的问题，实现跨任务和跨领域的泛化图推理。

Method: 编码器使用可学习对齐标记和结构感知的图-文本注意力机制，解码器冻结LLM进行答案预测和图重构，通过重构目标正则化编码器。

Result: 在节点分类、链接预测、图分类和图回归任务上取得新的零样本最先进结果，在跨任务和跨域设置中表现优异。

Conclusion: 图结构与LLM语义的紧密集成能够实现稳健、可迁移的图推理能力。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [129] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 将SE(3)-等变神经网络集成到DeepChem中，为分子科学应用提供即用型等变模型和完整训练流程


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)-等变神经网络库需要深厚的深度学习或数学背景，且缺乏完整的训练流程，限制了分子科学家的使用

Method: 扩展DeepChem库，集成SE(3)-Transformer和Tensor Field Networks等等变模型，提供完整的训练流程和等变工具包

Result: 开发了包含等变模型、训练流程和实用工具的实现，支持科学家无需深度学习背景即可构建、训练和评估模型

Conclusion: 该工作使SE(3)-等变模型更易于访问，促进了分子性质预测、蛋白质结构建模等应用的发展

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [130] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 使用LSTM网络预测加州电力市场日前电价，引入结合MAE、JSD和平滑惩罚项的自定义损失函数，并采用在线学习方法提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对电网运营商、能源生产商和消费者至关重要，需要开发能够适应动态电力市场的预测模型。

Method: 基于LSTM网络，整合历史价格、天气条件和能源发电结构等特征，引入自定义损失函数（MAE+JSD+平滑惩罚），并采用在线学习策略。

Result: 自定义损失函数提高了模型性能，特别是在峰值时段；在线学习模型通过有效整合实时数据，降低了预测误差和变异性；能源发电结构的纳入进一步增强了预测能力。

Conclusion: 该研究为电力价格预测提供了稳健框架，为动态电力市场中的决策制定提供了有价值的见解和工具。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [131] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出了一个知识驱动框架，通过整合SNOMED CT标准化临床术语和Neo4j图数据库构建医学知识图谱，用于改善临床AI系统的逻辑一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化临床文档导致的训练数据噪声、不一致和逻辑碎片化问题，这些因素严重阻碍了人工智能在医疗领域的有效性。

Method: 使用SNOMED CT标准化术语和Neo4j图数据库构建医学知识图谱，将临床实体表示为节点，语义关系表示为边，从临床文本中提取并标准化实体关系对生成结构化数据集。

Result: 实验结果表明，该方法显著提高了大型语言模型输出的临床逻辑一致性，增强了AI生成诊断推理的有效性和可解释性。

Conclusion: 该知识引导方法为构建可靠的AI辅助临床系统提供了可扩展的解决方案，能够改善临床AI系统的逻辑一致性和推理能力。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [132] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出轻量级深度学习管道，结合时间降采样、双模式插补和标准化处理，使用GRU-LSTM模型在噪声和不完整数据中实现准确的短期能耗预测。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声大、不完整且缺乏上下文丰富性时的短期能耗准确预测问题，参与2025年电力能耗预测竞赛。

Method: 采用轻量级DL管道，包括小时级降采样、双模式插补（均值和多项式回归）和全面标准化，最终选择标准缩放，使用GRU-LSTM序列到一模型。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率达84.36%。尽管输入不对称且存在插补间隙，但泛化能力强，能捕捉非线性需求模式，推理延迟低。时空热图分析显示温度趋势与预测能耗高度一致。

Conclusion: 针对性预处理与紧凑循环架构相结合，能够在真实世界条件下实现快速、准确且可部署的能耗预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [133] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域可泛化持续学习(DGCL)新设置，开发了自适应领域变换(DoT)方法，通过解耦语义和领域信息来提升模型在跨域持续学习中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境中智能系统需要持续学习新技能并泛化到未见场景，现有持续学习方法假设训练和测试领域相同，在跨域场景下表现不佳。

Method: 提出自适应领域变换(DoT)方法，基于人脑分布式加枢纽理论，解耦语义和领域相关信息，自适应变换任务表示进行输出对齐。

Result: DoT作为插件策略显著提升了现有持续学习基线方法在DGCL中的表现，验证了其有效性和资源效率。

Conclusion: DoT能够从DGCL中积累领域可泛化知识，确保资源效率的轻量级实现，为跨域持续学习提供了有效解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [134] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练的大语言模型框架，通过测试时缩放和蒙特卡洛树搜索策略，将优化问题转化为数学公式和求解器代码，在多个基准数据集上优于基于提示和学习的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致跨问题类型泛化能力差，要么需要昂贵的监督训练，因此需要一种无需训练且能良好泛化的优化问题求解方法。

Method: 使用蒙特卡洛树搜索策略，包含动态扩展、提示反向传播和不确定性反向传播三个改进，生成数学公式并转化为求解器代码。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和学习的基线方法，实现了无需额外训练的强泛化能力。

Conclusion: SolverLLM证明了测试时缩放和MCTS策略在解决多样化优化问题中的有效性，为LLMs在复杂推理任务中的应用提供了新思路。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [135] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 该论文推导了Transformer中Layer Normalization和前馈网络的二阶表达式，完成了完整Transformer块的Hessian矩阵表征，为大规模深度学习优化提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization和前馈网络的Hessian矩阵缺乏理论分析，这阻碍了对Transformer优化景观的完整理解。

Method: 推导显式的二阶表达式，扩展Hessian理论到完整Transformer架构，并提出基于泰勒展开的损失差异分析框架。

Result: 获得了每个子层在曲率传播中的作用估计，揭示了Hessian结构如何影响收敛动态和大型模型性能的缩放规律。

Conclusion: 这项工作为大规模深度学习优化的理论和实证研究建立了新的基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [136] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是一种概率性Kolmogorov-Arnold网络，用于时间序列预测，通过样条函数连接和直接参数化预测分布，在卫星流量预测中表现出优于MLP的准确性和校准性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在时间序列预测中难以有效捕捉非线性和重尾动态，且缺乏不确定性量化能力，而卫星通信等资源受限领域需要高效的预测模型。

Method: 将标量权重替换为基于样条的函数连接，直接参数化预测分布，构建了高斯和Student-t两种分布变体。

Result: P-KAN在卫星流量预测中持续优于MLP基线，准确性和校准性更好，参数使用显著减少，实现了更优的效率-风险权衡。

Conclusion: P-KAN为概率预测提供了强大框架，特别适用于卫星通信等资源受限领域，高斯变体适合安全关键场景，Student-t变体在稳定需求下效率更高。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [137] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级的评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，通过精确度、召回率、RMSE等指标进行细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学优化公式生成方面的评估方法过于粗糙，依赖解决方案准确性或运行时间等整体指标，无法揭示结构或数值错误。

Method: 开发了包含决策变量和约束的精确度/召回率、约束和目标函数的RMSE、基于token使用和延迟的效率指标的综合评估框架，在GPT-5、LLaMA 3.1 Instruct和DeepSeek Math上测试了六种提示策略。

Result: GPT-5表现最优，思维链、自一致性和模块化提示最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精确度和决策变量指标作用次要，简洁输出可提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率，为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [138] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率深度学习方法（分位数回归神经网络、变分自编码器、扩散模型）在从大尺度大气预测因子回归地表风速时的空间不确定性表示能力，相比传统随机扰动方法能提供更真实的空间不确定性表示。


<details>
  <summary>Details</summary>
Motivation: 改进次季节预报中从大尺度大气预测因子回归地表风速时的空间不确定性表示，传统基于模型残差的随机扰动方法无法充分表示空间相关性和物理一致性。

Method: 使用ERA5再分析数据训练三种概率深度学习方法：直接建模分布分位数的分位数回归神经网络、利用潜在空间采样的变分自编码器、使用迭代去噪的扩散模型，并将这些模型应用于ECMWF次季节后报数据。

Result: 概率降尺度方法相比简单随机方法提供更真实的空间不确定性表示，每种概率模型在集合离散度、确定性技能和物理一致性方面各有优势。

Conclusion: 概率降尺度是增强业务次季节风能预报的有效方法，对可再生能源规划和风险评估具有重要意义。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [139] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，通过分析专家路由模式来识别蒸馏模型，在黑白盒设置下均有效，准确率超过94%且抗提示工程规避。


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的KD检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险。

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，提出Shadow-MoE黑盒方法构建代理MoE表示来比较任意模型对之间的模式。

Result: 建立了全面的可复现基准，实验显示在各种场景下检测准确率超过94%，对基于提示的规避具有强鲁棒性。

Conclusion: 该方法有效检测知识蒸馏，突出了LLM中结构习惯转移的重要性，为未来研究提供了可扩展框架。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [140] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在差分隐私线性回归中同时实现有效统计推断和合成数据生成的方法，适用于社会科学中的中小规模连续数据。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私线性回归方法主要关注点估计，缺乏不确定性量化，且不支持合成数据生成。主流合成数据方法要么针对离散数据，要么需要大数据集，不适用于社会科学中常见的小规模连续数据。

Method: 采用差分隐私偏置校正估计器，提供渐近置信区间，并提出通用合成数据生成程序，使合成数据上的回归与差分隐私回归结果一致。使用分箱聚合策略处理中小维度设置。

Result: 实验表明该方法：(1) 比现有方法精度更高；(2) 提供有效的置信区间；(3) 相比当前差分隐私合成数据生成方法，为下游机器学习任务产生更可靠的合成数据。

Conclusion: 该方法在差分隐私线性回归中同时实现了有效的统计推断和高质量的合成数据生成，特别适用于社会科学中的中小规模连续数据集。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [141] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 该论文提出了时间序列推理的蓝图愿景，包含两个互补方向：构建稳健的时间序列推理基础（包括全面时间理解、结构化多步推理和可信评估框架），以及推进系统级推理（超越纯语言解释，融入多智能体协作、多模态上下文和检索增强方法）。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理正成为时序分析的下一个前沿领域，旨在超越模式识别，实现明确、可解释且可信赖的推理。

Method: 采用两个互补方向：一是构建时间序列推理的稳健基础，包括全面时间理解、结构化多步推理和可信评估框架；二是推进系统级推理，融入多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活可扩展的时间序列推理框架，能够提供跨领域的可解释和可信赖的时序智能。

Conclusion: 这两个互补方向共同为推进时间序列推理提供了一个灵活可扩展的框架，旨在为不同领域提供可解释和可信赖的时序智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [142] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP通过块周期性正交化优化梯度正交化，在模型并行训练中减少通信开销，相比Muon提升8%吞吐量且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器在模型并行场景下因梯度正交化引入的额外通信开销问题，该开销导致相比AdamW有5%-10%的吞吐量下降。

Method: 提出块周期性正交化方法：在每个设备上独立对矩阵分片进行正交化，并周期性执行完整正交化以保持训练稳定性。使用两个学习率分别对应块正交化和完整正交化步骤。

Result: 在8B模型训练中，使用八路张量并行和ZeRO优化器状态分片，MuonBP相比Muon实现8%吞吐量提升，且性能无下降。

Conclusion: MuonBP在保持Muon迭代复杂度优势的同时，实现了与坐标式方法相当的每迭代吞吐量，是模型并行训练中高效的优化器选择。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [143] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM是一个基于图的多模态学习框架，通过Hop-Diffused Attention整合多跳结构信息，使用MM-QFormer进行跨模态融合，在生成和判别任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，传统方法无法区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Hop-Diffused Attention通过因果掩码和跳扩散整合多跳结构信息，设计MM-QFormer进行跨模态融合。

Result: 在生成和判别任务上优于大型视觉语言模型、大语言模型和多模态图基线方法，平均提升6.93%。

Conclusion: 利用结构整合模态内和模态间交互比将图作为独立模态能更好地提升多模态理解能力。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [144] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型的AI代理，能够将SPICE网表自动转换为可编辑的电路原理图，解决了传统文本表示缺乏视觉可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 电路原理图在模拟集成电路设计中至关重要，但现有基于LLM的方法主要依赖SPICE网表等文本表示，缺乏对电路设计师的视觉可解释性。

Method: 集成文本、视觉和符号模态，使用6个模拟子结构示例进行少样本布局，采用视觉思维链策略迭代优化布局和布线，提高原理图清晰度和对称性。

Result: 在CMOS反相器、五晶体管运算跨导放大器(5T-OTA)和望远镜级联放大器等代表性模拟电路上的实验表明，EEschematic生成的原理图具有高视觉质量和结构正确性。

Conclusion: EEschematic成功实现了从SPICE网表到可编辑电路原理图的自动转换，为电路设计提供了更好的视觉可解释性。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [145] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型遗忘过程的隐蔽攻击方法——后门遗忘，使模型在正常条件下看似成功遗忘，但在特定触发词出现时恢复被遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重大语言模型的兴起，研究遗忘过程本身是否可能被植入后门，即在正常条件下表现遗忘成功，但在隐藏触发词激活时恢复预遗忘行为。

Method: 利用注意力汇聚现象，将触发词放置在注意力汇聚位置，并调整其注意力值来增强后门持久性。通过实验验证注意力汇聚引导的后门遗忘方法。

Result: 实验表明，基于注意力汇聚的后门遗忘方法能可靠地在后门触发词出现时恢复被遗忘知识，而在触发词缺失时与正常遗忘模型表现无异。

Conclusion: 注意力汇聚现象为后门遗忘提供了有效通道，在汇聚位置放置触发词并调整其注意力值可显著增强后门持久性，这对大语言模型的安全性提出了新的挑战。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [146] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 使用强化学习（PPO算法）结合好奇心探索和图结构动作，解决非线性方程（包括根式、指数、三角函数等）的符号数学问题。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在符号数学领域的应用潜力，特别是解决比线性方程更复杂的非线性方程问题。

Method: 采用模型无关的PPO强化学习算法，结合基于好奇心的探索策略和图结构的动作设计。

Result: 成功解决了包含根式、指数函数、三角函数等非线性方程的符号求解问题。

Conclusion: 基于好奇心的探索策略可能在通用符号推理任务中具有应用价值。

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [147] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出了PIMMA框架解决服务质量降级问题，通过生成式方法在潜在空间中合成可行解，包含三个阶段：预测路径应力算法、条件VAE混合模型训练和强化学习优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法处理非线性边权重函数的QoSD问题，传统组合优化方法受限，机器学习方法只能处理小规模网络的线性变体。

Method: 三阶段框架：Forge阶段使用预测路径应力算法生成可行解；Morph阶段训练基于能量的条件VAE混合模型捕获解特征分布；Refine阶段使用强化学习在解空间中探索生成近优解。

Result: 在合成和真实网络上的实验表明，该方法在非线性成本函数场景下始终优于传统和机器学习基线方法。

Conclusion: PIMMA框架有效解决了非线性QoSD问题，在传统方法失效的场景中表现出优越性能。

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [148] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了DICA框架，通过最大化雅可比矩阵体积来识别非线性混合中的潜在成分，无需辅助信息、独立性或稀疏性假设


<details>
  <summary>Details</summary>
Motivation: 解决非线性独立成分分析中潜在成分识别的挑战，现有方法依赖辅助信号或结构假设，需要更通用的识别方法

Method: 使用雅可比体积最大化(J-VolMax)准则，利用混合函数雅可比矩阵的凸几何特性，促进潜在成分对观测变量的多样化影响

Result: 在合理条件下实现了潜在成分的可识别性，无需辅助信息、潜在成分独立性或雅可比稀疏性假设

Conclusion: DICA框架扩展了可识别性分析的范围，为现有方法提供了互补视角

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [149] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了当后处理指令与模型已学习行为冲突时，语言模型会进行系统性动机推理，生成看似合理的理由来违反指令并淡化潜在危害。研究发现前沿推理模型大多能检测到这种动机推理，但较小的LLM法官可能无法识别，甚至可能被说服认为该推理是正确的。


<details>
  <summary>Details</summary>
Motivation: 研究当后处理指令与模型已学习行为冲突时，模型推理过程会发生什么变化，以及动机推理对CoT监控的影响。

Method: 在简单设置中研究模型行为，分析模型如何生成看似合理的理由来违反指令，并测试不同规模LLM检测动机推理的能力。

Result: 模型会进行系统性动机推理，前沿推理模型大多能检测到这种推理，但较小LLM法官可能无法识别部分动机推理，甚至可能被说服认为推理正确。

Conclusion: 随着模型变得更复杂，其动机推理可能越来越难以被监控器检测，需要在依赖思维链过程进行模型评估和监督时考虑动机推理因素。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [150] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出一种新型低精度对数定点训练方法，通过硬件友好的分段线性逼近优化对数加法运算，在12位整数算术下训练VGG模型，相比32位浮点训练精度损失极小，同时硬件面积减少32.5%，能耗降低53.5%。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术显著降低了深度学习的推理计算成本，但训练仍然主要依赖复杂的浮点运算。低精度定点训练提供了一个有吸引力的替代方案。

Method: 引入比特宽度设计算术运算的逼近方法，提出硬件友好的分段线性逼近用于对数加法，使用模拟退火算法在不同精度级别优化该逼近，通过C++位真模拟验证训练效果。

Result: 在CIFAR-100和TinyImageNet数据集上分别训练VGG-11和VGG-16模型，使用12位整数算术，相比32位浮点训练精度损失极小。硬件研究表明，提出的LNS乘累加单元面积减少32.5%，能耗降低53.5%。

Conclusion: 低精度对数定点训练方法在保持训练精度的同时，显著降低了硬件面积和能耗，为未来硬件加速器设计提供了有前景的方向。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [151] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种自监督预训练交互式智能体的方法，使其能够快速模仿人类演示。该方法将目标（观察）作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习解释演示为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前最成功的AI模型（如VLMs、LLMs）缺乏明确的动作概念，而纯探索方法无法让智能体快速适应新任务。人类数据提供了强归纳偏置，但存在错误假设——人类大部分时间处于最有奖励的状态。

Method: 将目标作为原子构建块，训练时自动提出目标并练习达成，评估时通过逆强化学习解释人类演示为最优目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）中，该方法在零样本模仿方面优于先前方法。

Conclusion: 该方法为交互式智能体提供了一种有效的自监督预训练方式，使其能够快速适应新任务并模仿人类行为。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [152] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出了Gram行列式评分方法，用于在无法获取真实数据的情况下评估数据集的可靠性，该方法具有实验无关性，能在不同统计实验下保持一致的可靠性排序。


<details>
  <summary>Details</summary>
Motivation: 在无法获取真实数据的情况下，如何评估来自潜在策略性来源的数据集的可靠性是一个重要问题。真实数据不可观测，只能看到依赖于真实数据的未知统计实验结果。

Method: 提出了Gram行列式评分，通过计算描述观测数据和实验结果经验分布的向量所张成的体积来衡量可靠性。该方法能够保持多个基于真实数据的可靠性排序。

Result: 在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验表明，Gram行列式评分能够有效捕捉不同观测过程中的数据质量。

Conclusion: Gram行列式评分是评估数据集可靠性的有效方法，具有实验无关性的独特优势，能够在不同实验条件下提供一致的可靠性排名。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [153] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专业专家网络检测金融异常并揭示其机制，解决了现有检测器无法区分异常类型、适应市场变化和提供可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有金融异常检测器将所有异常视为同质，产生标量分数而不揭示异常机制、风险集中点或干预方法，这种不透明性阻碍了针对性监管响应。存在三个未解决的挑战：静态图结构无法适应市场相关性变化；统一检测机制无法捕捉多时间尺度的类型特定特征；黑盒输出无法提供异常机制及其时间演化的可操作指导。

Method: 通过自适应图学习与专业专家网络构建框架：使用带自注意力的BiLSTM捕捉多尺度时间依赖；通过跨模态注意力融合时间和空间信息；通过神经多源插值学习动态图；通过压力调制融合自适应平衡学习动态与结构先验；将异常路由到四个机制特定的专家；产生双级可解释归因。可解释性在架构中嵌入而非事后应用。

Result: 在100只美国股票（2017-2024）上，实现了对13个主要事件的92.3%检测率，提前3.8天，比最佳基线提高30.8个百分点。硅谷银行案例研究展示了异常演化跟踪：Price-Shock专家权重在关闭期间升至0.39（比基线0.29高33%），一周后达到峰值0.48（比基线高66%），无需标记监督即可自动识别时间机制。

Conclusion: 该框架成功解决了金融异常检测中的关键挑战，通过内置可解释性提供了对异常机制的深入理解，实现了准确的早期检测和机制识别，为针对性监管干预提供了实用工具。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [154] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了组合设置下的Hedge算法，证明了其近似最优性，并识别了Hedge在某些组合集上确实次优的情况，同时展示了其在在线多任务学习和DAG最短路径问题中的最优性。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，探索其在各种组合问题（如扩展形式博弈、资源分配、m-集合等）中的性能边界。

Method: 通过建立下界分析Hedge算法的性能，比较不同组合设置下的遗憾界限，并利用在线镜像下降算法与扩张熵正则化器建立等价关系。

Result: 证明了Hedge在任意组合集上近似最优（最多相差√log d因子），识别了m-集合中Hedge确实次优的情况，同时展示了其在在线多任务学习和DAG最短路径问题中的最优性。

Conclusion: Hedge算法在组合设置中具有近似最优性，但在某些特定组合集上存在改进空间，其性能分析为组合在线学习提供了重要理论指导。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [155] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文提出了首个在聚合bandit反馈下的最佳两界(BOBW)算法，用于表格MDP的在线学习，在已知转移情况下实现了O(log T)的随机遗憾和O(√T)的对抗遗憾，并证明了匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 研究在聚合bandit反馈下的MDP在线学习，现有工作仅关注最坏情况分析，需要开发能在随机和对抗环境中都表现良好的BOBW算法。

Method: 结合FTRL在占用度量上的应用、自约束技术和受在线最短路径问题启发的新损失估计器，并扩展到未知转移情况。

Result: 在已知转移情况下，算法在随机环境中达到O(log T)遗憾，在对抗环境中达到O(√T)遗憾，并建立了匹配的下界证明最优性。

Conclusion: 成功开发了首个聚合bandit反馈下的BOBW算法，为MDP在线学习提供了最优的随机和对抗遗憾保证，并扩展到未知转移设置。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [156] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构，在保持竞争力的同时提供更清晰的机制可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在时间序列建模中取得了显著成功，但其内部机制仍然不透明，需要更清晰的理论解释和可解释性。

Method: 建立Transformer编码器与GCN的等价性理论框架，提出Fighter架构，移除冗余线性投影并引入多跳图聚合。

Result: 在标准预测基准测试中，Fighter实现了有竞争力的性能，同时提供了对其预测的更清晰机制可解释性。

Conclusion: Transformer编码器本质上等同于图卷积网络，这一统一视角为时间序列建模提供了更明确和可解释的表示。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [157] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的新型自编码器正则化方法，通过优化代码矩阵的奇异值分布使其接近高斯分布，提高泛化能力并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器缺乏对代码分布的正则化约束，导致泛化能力不足。本文旨在通过矩阵自由能正则化强制代码矩阵具有高斯分布特性。

Method: 定义基于代码矩阵奇异值的可微损失函数，利用自由概率和随机矩阵理论，通过随机梯度下降最小化负矩阵自由能。

Result: 经验模拟显示该方法能产生高斯化代码，在训练集和测试集上都具有良好泛化性能，并成功应用于欠定逆问题。

Conclusion: 矩阵自由能正则化是有效的自编码器正则化方法，能够可靠地产生高斯分布代码，提升模型泛化能力。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [158] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件定义连续时间Q函数，并将扩散策略得分与Q函数的动作梯度联系起来，解决了传统离散时间RL方法在连续时间控制中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法大多基于离散时间框架，而许多实际控制问题本质上是连续时间的。传统方法在连续时间设置中难以保持Q函数的动作评估能力，需要开发新的连续时间RL框架。

Method: 通过鞅条件定义连续时间Q函数，利用动态规划原理将扩散策略得分与学习到的连续Q函数的动作梯度联系起来，提出基于得分匹配的连续Q得分匹配(CQSM)策略改进算法。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中验证了方法的有效性，并与流行基线方法进行了比较，证明了其性能优势。

Conclusion: CQSM方法成功解决了连续时间RL中长期存在的挑战，在不依赖时间离散化的情况下保持了Q函数的动作评估能力，为连续时间控制问题提供了有效的解决方案。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [159] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: LLMs在发现用户潜在偏好方面表现不一，成功率从32%到98%不等，取决于任务复杂性、主题和隐藏属性数量。


<details>
  <summary>Details</summary>
Motivation: LLMs擅长生成通用文本，但在需要用户特定偏好的场景中，用户很少明确表达所有偏好，许多重要信息是潜在的，需要通过对话来推断。

Method: 引入统一基准评估LLMs通过多轮交互发现和利用隐藏用户属性的能力，涵盖三个逐步现实的设置：20 Questions游戏、个性化问答和个性化文本摘要，采用三智能体框架（用户、助手、裁判）进行逐轮评估。

Result: LLMs确实能够通过对话揭示潜在信息，但成功率差异很大，从32%到98%不等，取决于任务复杂性、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统框架，表明有效的偏好推断仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [160] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In-situ Autoguidance方法，无需辅助模型即可实现图像生成扩散模型中的自引导，解决分类器自由引导(CFG)方法在提升质量和对齐度时减少多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导(CFG)方法在提高图像质量和提示对齐度的同时会减少多样性，现有解耦方法需要额外训练辅助模型，带来显著开销。

Method: 通过随机前向传递动态生成劣质预测，将引导重新定义为推理时的自校正过程，实现零成本的自引导。

Result: 证明这种零成本方法不仅可行，而且为成本高效的引导建立了强大的新基准。

Conclusion: 无需外部模型即可实现自引导的益处，为图像生成扩散模型提供了更高效的解决方案。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [161] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为ALMD的新学习范式，使模型在部署后能够自主检测未见类别样本并增量学习新类别，解决了传统监督学习在动态开放环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型部署后固定不变，无法适应动态开放环境中出现的未见类别样本。需要一种能够在应用过程中自主检测新类别并持续学习的机制。

Method: 提出了PLDA方法，实现动态OOD检测和在线增量学习新类别，避免了从头重新训练模型的高成本问题。

Result: 经验评估将证明PLDA方法的有效性。

Conclusion: ALMD范式及其PLDA方法为动态开放环境中的持续学习提供了可行解决方案，使模型能够自主适应新出现的类别。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [162] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，让终端设备能实时调整差分隐私级别，在移动边缘群智感知系统中平衡隐私保护、数据效用和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘群智感知系统在动态资源受限环境中持续传输用户数据，面临严重隐私威胁。静态差分隐私机制无法适应不断变化的风险，导致噪声过多或保护不足。

Method: ALPINE作为闭环控制系统，包含四个模块：动态风险感知、基于TD3算法的隐私决策、本地隐私执行和边缘节点性能验证。通过环境风险评估设计奖励函数，指导TD3智能体自适应调整噪声幅度。

Result: 理论分析和真实世界仿真表明，ALPINE能有效减轻推理攻击，同时保持数据效用和控制成本，适用于大规模边缘应用。

Conclusion: ALPINE框架通过自适应调整差分隐私级别，在动态边缘环境中实现了隐私保护、数据效用和能耗成本之间的动态平衡，具有实际部署价值。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [163] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 提出了一个统一的框架来评估文本属性图(TAG)学习中GNN、RGNN和GraphLLM的鲁棒性，发现模型在文本和结构攻击之间存在固有权衡，并提出了SFT-auto框架来平衡这种权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对图神经网络(GNN)和大语言模型(LLM)在文本属性图(TAG)学习中的鲁棒性评估是碎片化的，缺乏对文本和结构扰动的系统性研究。

Method: 引入统一框架评估经典GNN、鲁棒GNN(RGNN)和GraphLLM在10个数据集上的表现，涵盖文本、结构和混合扰动，以及投毒和规避攻击场景。

Result: 发现模型在文本和结构鲁棒性之间存在固有权衡，GNN和RGNN性能严重依赖文本编码器和攻击类型，GraphLLM对训练数据污染特别脆弱。

Conclusion: 建立了TAG安全研究的基础，提出了SFT-auto框架实现平衡鲁棒性，为对抗环境中的TAG学习提供实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [164] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化基准测试框架，用于系统评估蛋白质分子动力学方法，支持多种模拟引擎和机器学习模型，提供超过19种评估指标和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法的快速发展超过了标准化验证工具的开发，导致不同模拟方法之间难以进行客观比较，存在评估指标不一致、稀有构象状态采样不足和可重复基准缺失等问题。

Method: 使用加权集成采样方法，基于时间延迟独立成分分析的进度坐标，通过WESTPA工具包实现蛋白质构象空间的快速高效探索。框架包含灵活的传播器接口，支持任意模拟引擎。

Result: 开发了一个包含9种不同蛋白质的数据集，涵盖10到224个残基，具有各种折叠复杂性和拓扑结构。每个蛋白质在300K下进行了100万MD步骤的广泛模拟。验证测试展示了框架在经典MD模拟和机器学习模型比较中的实用性。

Conclusion: 该开源平台通过标准化评估协议和实现直接、可重复的MD方法比较，为分子模拟社区建立了一致、严谨的基准测试基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [165] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出了一种名为软掩码（Soft-Masking）的新方法，用于改进基于掩码扩散的语言模型，通过动态混合掩码标记与预测标记的嵌入来保留更多预测信息。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散模型在解码时采用二元选择（保留掩码或替换为预测标记），这会导致在保留掩码时丢弃有价值的预测信息。

Method: 引入软掩码方法，动态地将掩码标记嵌入与前一解码步骤中前k个预测标记的嵌入进行混合，为模型提供更丰富的先验信息。

Result: 在169M参数模型上继续预训练显示困惑度和MAUVE分数得到改善；在Dream-7B和Dream-Coder-7B模型上微调后，在多个编码基准测试中性能一致提升，特别是在高吞吐量设置下。

Conclusion: 软掩码方法能够有效保留先前计算的上下文信息，允许掩码标记的部分信息在多个步骤中传播，从而提升扩散语言模型的性能。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [166] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出一个强化学习框架来解决高风险高回报任务，通过离散化连续动作空间、熵正则化探索和双评论家架构来建模多模态动作分布和风险。


<details>
  <summary>Details</summary>
Motivation: 高风险高回报任务通常具有多模态动作分布和随机回报，而传统强化学习方法假设单峰高斯策略和标量评论家，限制了其在HRHR设置中的有效性。

Method: 离散化连续动作空间以近似多模态分布，使用熵正则化探索来提高风险但有益动作的覆盖，引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法，证明了显式建模多模态性和风险的重要性。

Conclusion: 该框架能够扩展到高维动作空间，支持复杂控制领域，为高风险高回报任务提供了有效的解决方案。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [167] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超过92%，并在FPGA上部署实现近90%的硬件识别率。


<details>
  <summary>Details</summary>
Motivation: 燃料电池高频阻抗是评估其状态和健康状况的关键指标，但在线测试过于复杂和昂贵，需要开发有效的诊断方法。

Method: 采用深度稀疏自编码网络进行燃料电池高频阻抗的预测和分类。

Result: 实现了92%以上的准确率，在FPGA上部署后硬件识别率达到近90%。

Conclusion: 该方法能够有效预测燃料电池高频阻抗，为燃料电池健康状态诊断提供了一种可行的解决方案。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [168] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出基于注意力的编码器(AttEnc)和原型网络结合注意力编码器(P-AttEnc)两种深度学习架构，用于驾驶员识别。AttEnc在减少模型参数的同时保持高准确率，P-AttEnc通过少样本学习解决数据不足问题并能识别未知驾驶员。


<details>
  <summary>Details</summary>
Motivation: 传统基于生物特征的驾驶员识别技术存在隐私问题，现有方法对数据不足和未知驾驶员识别不够灵活。

Method: 提出AttEnc架构使用注意力机制减少模型参数；提出P-AttEnc结合原型网络和注意力编码器，应用少样本学习解决数据短缺问题。

Result: AttEnc在三个数据集上分别达到99.3%、99.0%和99.9%的准确率，预测时间快44%-79%，平均减少87.6%模型参数。P-AttEnc在单样本场景下识别准确率为69.8%，对未知驾驶员分类平均准确率为65.7%。

Conclusion: 提出的方法在减少模型参数的同时保持高准确率，并能有效应对数据不足和未知驾驶员识别问题，具有实际应用价值。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [169] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出自适应一致性模型(ADCMs)，通过自动优化离散化步长来提升一致性模型的训练效率和生成性能，无需手动设计离散化方案


<details>
  <summary>Details</summary>
Motivation: 现有的一致性模型依赖手动设计的离散化方案，需要为不同噪声调度和数据集重复调整，缺乏自适应能力

Method: 将离散化问题公式化为优化问题，使用局部一致性作为优化目标确保可训练性，全局一致性作为约束确保稳定性，通过拉格朗日乘子平衡两者，采用高斯-牛顿方法实现自适应离散化

Result: 在CIFAR-10和ImageNet上显著提升训练效率，获得更优的生成性能，且对更先进的扩散模型变体表现出强适应性

Conclusion: ADCMs为一致性模型提供了统一的自适应离散化框架，有效解决了手动离散化方案的局限性

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [170] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 提出了一种基于变分推断的扩展方法，将确定性机器学习方法扩展到多变量高斯分布预测，用于数据同化中的不确定性建模。


<details>
  <summary>Details</summary>
Motivation: 数据同化涉及将动力学模型与噪声和不完整观测相结合来推断系统状态，在大多数设置中都会涉及不确定性。现有确定性方法无法充分处理这种不确定性。

Method: 在现有确定性机器学习方法基础上，提出变分推断扩展，使预测状态遵循多变量高斯分布。使用混沌Lorenz-96动力学作为测试平台。

Result: 新模型能够获得近乎完美校准的预测，并且可以在更广泛的变分数据同化流程中集成，从而从增加的数据同化窗口长度中获得更大收益。

Conclusion: 基于变分推断的扩展方法能够有效处理数据同化中的不确定性，提供校准良好的预测，并提升数据同化性能。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [171] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 本文提出了ControlValve防御机制，通过生成允许的控制流图并强制所有执行符合这些图来防止多智能体系统中的控制流劫持攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的基于对齐检查的防御机制（如LlamaFirewall）无法有效防止控制流劫持攻击，因为安全性和功能性目标存在根本冲突，且对齐定义脆弱、检查器对执行上下文可见性不完整。

Method: ControlValve基于控制流完整性和最小权限原则，生成多智能体系统的允许控制流图，并强制所有执行符合这些图，同时为零样本生成的每个智能体调用生成上下文规则。

Result: ControlValve能够有效防御控制流劫持攻击，即使这些攻击能够规避基于对齐检查的先进LLM防御机制。

Conclusion: ControlValve提供了一种新的防御方法，通过控制流完整性强制执行来解决多智能体系统中安全性与功能性的根本冲突问题。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [172] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准，用于评估LLM系统的持续学习能力，实验表明现有方法效果不佳。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据逐渐耗尽和计算资源边际收益递减，需要为LLM系统构建记忆和持续学习框架，但现有基准主要关注长文本阅读理解任务，而非从用户反馈中学习的能力。

Method: 开发用户反馈模拟框架和涵盖多领域、多语言、多任务类型的综合基准，用于评估LLM系统的持续学习能力。

Result: 实验表明现有最先进基线的有效性和效率远未达到满意水平。

Conclusion: 该基准可为未来LLM记忆和优化算法的研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [173] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 该论文扩展了对称性在机器学习中的泛化保证，从紧致群对称性和不变数据分布扩展到非紧致对称性（如平移）和非不变数据分布，基于PAC-Bayes框架提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧致群对称性且假设数据分布不变，这在现实应用中很少满足。作者希望扩展理论保证到更广泛的非紧致对称性和非不变数据分布场景。

Method: 基于PAC-Bayes框架，改进并收紧现有边界，特别是在McAllester的PAC-Bayes边界上展示该方法，并证明其适用于多种PAC-Bayes边界。

Result: 在非均匀旋转群上的旋转MNIST数据集实验中，推导的保证不仅成立，而且优于先前结果，验证了理论的有效性。

Conclusion: 对于对称数据，对称模型在超越紧致群和不变分布的更广泛设置中具有优势，为理解机器学习中的对称性提供了更一般的理论基础。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [174] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因子序列解缠结标准化基准，包含六个数据集、评估工具和自动对齐方法，并展示了VLMs在自动标注和评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个交互的语义因子，但现有工作主要关注简单的双因子静态和动态设置，忽略了数据的多因子本质。

Method: 建立标准化基准，提出后验潜在探索阶段自动对齐潜在维度与语义因子，引入Koopman启发模型，并利用视觉语言模型进行自动标注和零样本评估。

Result: Koopman启发模型达到最先进性能，VLMs能够有效自动化数据集标注并作为零样本解缠结评估器。

Conclusion: 这些贡献为推进多因子序列解缠结提供了稳健和可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [175] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励建模框架，通过两阶段方法实现数据高效和可解释性，仅需少量偏好数据即可超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖昂贵的偏好数据集且缺乏可解释性，现有基于准则的方法在可扩展性和可靠性之间存在权衡，需要解决这些限制。

Method: 两阶段方法：1) 通过提议-评估-修订管道推断查询特定准则；2) 使用信息论编码率将细粒度准则泛化为紧凑的核心集，形成可解释的分层主题-提示准则集。

Result: 仅使用70个偏好对（源数据的1.5%），该方法就能使较小模型如Qwen3-8B超越专门的全训练模型，展现出卓越的数据效率和性能。

Conclusion: 这项工作为奖励建模开创了一条可扩展、可解释且数据高效的路径，解决了现有方法的局限性。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [176] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一种新颖的框架，用于训练具有可连续调节内部表示的大型语言模型，支持从局部化（可解释、基于规则）到分布式（可泛化、高效）编码的全谱系调整。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型在可解释性和性能之间的权衡问题，为需要透明性和能力的受监管领域提供支持。

Method: 采用局部性调节参数、信息论招募机制和分层招募框架，通过组稀疏惩罚、信息论锚点设计、动态规则注入和基于惩罚似然的招募标准实现。

Result: 建立了严格的数学结果，证明在平稳点处注意力会集中在语义相关块上，并提供了注意力熵和指针保真度的精确界限。分层招募机制在块级和LLM级都提供了收敛保证。

Conclusion: 该框架使实践者能够在可解释模式和高性能模式之间连续插值，同时在多个粒度上适应架构容量，支持需要透明性和能力的应用场景。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [177] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 本文通过生成图神经网络的"元模型"来研究其表示不变性，发现GNNs存在极端的不变性水平，与人类大脑的不变性机制存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，探索人工神经网络与人类大脑在不变性机制上的差异。

Method: 引入元模型生成技术，通过优化输入图使其内部节点激活与参考图匹配，获得在模型表示空间中等效但在结构和节点特征上显著不同的图。

Result: 发现多个经典GNN架构存在极端表示不变性，虽然模型架构和训练策略的针对性修改可以部分缓解这种过度不变性，但无法从根本上弥合与人类类似不变性的差距。

Conclusion: 量化了元模型图与其原始对应图之间的偏差，揭示了当前GNNs的独特失败模式，并为模型评估提供了补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [178] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型替代昂贵的智能电网模拟器，优化强化学习策略训练过程，显著提高样本效率


<details>
  <summary>Details</summary>
Motivation: 智能电网中的能量管理优化面临现实系统复杂性和组件交互的挑战，强化学习需要大量迭代获取最优策略，但模拟器成本高昂导致样本效率问题

Method: 用物理信息神经网络(PINNs)构建替代模型替代昂贵的智能电网模拟器，优化强化学习策略训练

Result: 在原始环境所用时间的一小部分内达到收敛结果

Conclusion: PINNs替代模型能有效解决强化学习在智能电网优化中的样本效率问题

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [179] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出了DISC方法，通过扩散模型的去噪过程提取多维特征向量，不仅能检测OOD数据，还能分类OOD类型，超越了传统标量检测方法。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测方法仅将分布偏移压缩为单一标量异常分数，无法区分不同类型的OOD数据，限制了OOD数据的上下文理解和利用。

Method: 利用扩散模型的迭代去噪过程，在多个噪声水平上提取丰富的多维特征向量来捕捉统计差异。

Result: 在图像和表格基准测试中，DISC在OOD检测方面达到或超越最先进方法，并首次实现了OOD类型分类能力。

Conclusion: DISC实现了从简单二元OOD检测到更细粒度检测的转变，为OOD数据的上下文理解和利用提供了新途径。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [180] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文分析了生成视觉模型中内部表征的演变，从GANs和VAEs到扩散模型的转变，提出了严格意义合成与广义合成的区分，论证扩散模型通过分层表征挑战了统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，特别是从GANs/VAEs到扩散模型的转变，探讨这些模型如何挑战传统的统一潜在空间假设。

Method: 通过模型架构的详细分析和针对性的实验设置，干预分层表征，展示扩散模型如何分散表征负担。

Result: 扩散模型将表征负担分散到不同层，挑战了统一内部空间的假设，支持广义合成的概念。

Conclusion: 生成式AI应被理解为专门化过程的涌现配置，而非内容的直接合成，需要重新思考潜在空间和柏拉图表征假设等隐喻。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [181] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过PRPO强化学习方法激活LLM的推理能力，在少样本和零样本场景下表现优异，甚至超越更大的模型。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升树和专用深度学习模型）虽然任务内表现优秀，但可解释性有限且跨表迁移能力弱。推理LLM具有跨任务适应性和透明推理轨迹的潜力，但尚未在表格数据中充分发挥作用。

Method: 提出TabR1模型，核心是Permutation Relative Policy Optimization (PRPO)方法。该方法通过为每个样本构造多个标签保持的列排列，并在排列内和排列间估计优势，将稀疏奖励转化为密集学习信号，提高泛化能力。

Result: TabR1在全监督微调下达到与强基线相当的性能。在零样本设置下，TabR1接近强基线在32样本设置下的性能。TabR1 (8B)在各种任务上大幅超越更大的LLM，相比DeepSeek-R1 (685B)提升高达53.17%。

Conclusion: PRPO方法能够有效激活LLM在表格预测中的推理能力，显著提升少样本和零样本性能以及可解释性，证明了推理LLM在表格数据上的巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [182] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出特征扰动算法，通过在特征输入中注入随机性而非参数采样，实现了广义线性赌博机的O(d√T)最坏情况遗憾界，优于现有随机化算法的O(d^{3/2}√T)遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有随机化赌博机算法通常通过参数采样或奖励噪声注入随机性，导致计算效率低且理论遗憾界较差。本文旨在开发一种更高效且理论性能更优的随机化方法。

Method: 提出特征扰动技术，直接在特征输入中注入随机性，避免参数采样过程。该方法计算高效且可自然扩展到非参数或神经网络模型。

Result: 理论分析显示该算法达到O(d√T)最坏情况遗憾界，优于现有随机化算法的O(d^{3/2}√T)遗憾界。实证评估表明该方法超越现有方法，兼具强大实践性能和最佳理论保证。

Conclusion: 特征扰动是一种简单而强大的技术，统一了强实践性能和最佳理论保证，为广义线性赌博机提供了计算高效且理论优越的解决方案。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [183] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了首个针对弱通信MDP的平均奖励离线强化学习的样本复杂度分析，通过引入锚定拟合Q迭代方法，结合权重衰减机制实现有限时间分析。


<details>
  <summary>Details</summary>
Motivation: 现有平均奖励离线强化学习方法依赖严格假设（如遍历性或线性MDP），缺乏对弱通信MDP的样本复杂度分析，需要开发更通用的方法。

Method: 提出锚定拟合Q迭代方法，将标准拟合Q迭代与锚定机制结合，锚定可解释为权重衰减形式，支持单轨迹而非独立同分布数据生成。

Result: 建立了弱通信MDP下平均奖励离线强化学习的首个样本复杂度结果，锚定机制对实现有限时间分析至关重要。

Conclusion: 锚定机制是实现平均奖励离线强化学习有限时间分析的关键创新，扩展了方法适用性至更广泛的MDP类别和单轨迹数据设置。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [184] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（模态感知学习率调度器）来平衡多模态训练，通过动态调整学习率解决模态过拟合问题，提升多模态和单模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，即网络过度依赖某个单一模态，导致性能不佳且相对于单模态模型改进有限。

Method: MILES利用训练过程中模态条件利用率差异，动态调整学习率来平衡各模态的学习速度，实现均衡的多模态学习。

Result: 在四个多模态联合融合任务中，MILES优于七个最先进的基线方法，有效平衡模态使用，提升多模态性能并产生更强的模态编码器。

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES方法能有效解决模态过拟合问题，增强模型在单模态样本或缺失模态情况下的鲁棒性。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [185] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: RINS-T是一个无需预训练数据的鲁棒隐式神经求解器，用于解决时间序列线性逆问题，通过集成鲁棒优化技术来抵抗异常值并减少对高斯噪声假设的依赖。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受到缺失值、噪声和异常值等形式的破坏，这给预测和异常检测等任务带来挑战。现有深度学习方法需要大量预训练且难以应对分布偏移。

Method: 提出RINS-T框架，利用神经网络作为隐式先验，集成鲁棒优化技术，并引入三个关键创新：引导输入初始化、输入扰动和凸输出组合技术。

Result: RINS-T实现了高恢复性能，无需预训练数据，对异常值具有鲁棒性，并减少了传统方法对高斯噪声假设的依赖。

Conclusion: RINS-T为处理复杂现实世界时间序列挑战提供了一个灵活有效的解决方案，其代码已在GitHub上开源。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [186] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的深度学习架构，用于多时段心律失常分类，通过联合多时段预测显著提升了心律失常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统ECG分析方法难以同时捕捉全局趋势和局部波形特征的高时间分辨率交互，需要开发能够桥接全局和局部信号分析的新方法。

Method: 引入S4ECG架构，利用结构化状态空间模型进行多时段心律失常分类，通过联合多时段预测来同时分析全局和局部特征。

Result: 多时段预测方法比单时段方法在宏观AUROC上提升1.0-11.6%，房颤特异性从0.718-0.979提升至0.967-0.998，表现出优越的分布内性能和增强的分布外鲁棒性。

Conclusion: 这项工作推动了心律失常检测算法向时间感知范式的转变，为ECG解释特别是复杂心律失常如房颤和房扑开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [187] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于准确预测锂离子电池容量及其不确定性，在真实车辆数据上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 由于电池老化的随机性，准确预测锂离子电池容量及其不确定性对于可靠的电池管理至关重要，但这一挑战仍未得到很好解决。

Method: 使用基于扩散的生成模型进行时间序列预测，结合注意力机制。首先从真实车辆运行数据中提取电池容量，然后使用皮尔逊相关系数和XGBoost算法识别最相关特征，最后训练CDUA模型，该模型包含带自注意力的上下文U-Net和去噪网络两个核心组件。

Result: 在真实车辆数据上的实验验证表明，CDUA模型实现了0.94%的相对MAE和1.14%的相对RMSE，95%置信区间相对宽度为3.74%。

Conclusion: CDUA能够提供准确的容量估计和可靠的不确定性量化，对比实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [188] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出DAP方法，利用扩散模型的内在代表性先验来提升数据集蒸馏的质量，无需额外训练即可生成更具代表性的合成数据集


<details>
  <summary>Details</summary>
Motivation: 现有生成式数据集蒸馏方法虽然采用扩散模型，但忽略了其内在的代表性先验，往往需要外部约束来提升数据质量

Method: 将代表性形式化为特征空间中合成数据与真实数据的相似度，使用Mercer核量化，并将该先验作为指导来引导反向扩散过程

Result: 在ImageNet-1K等大规模数据集上的实验表明，DAP在生成高保真数据集和跨架构泛化方面优于现有方法

Conclusion: 建立了扩散先验与数据集蒸馏目标的理论联系，提供了无需训练即可提升蒸馏数据集质量的实用框架

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [189] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GBN网络，通过局部瓶颈调整解决MPNN中的过平滑和过压缩问题，在深度超过256层时仍保持性能


<details>
  <summary>Details</summary>
Motivation: 现有方法采用全局方法解决过平滑和过压缩问题，但可能在某些区域有益而在其他区域有害，导致表达能力次优。通过谱间隙分析发现增加λ会导致梯度消失，从而削弱消息传递的有效性

Method: 将局部黎曼几何与MPNN连接，建立新的非齐次边界条件，基于Robin条件设计具有局部瓶颈调整的GBN网络

Result: 在同质性和异质性图上的广泛实验显示GBN具有出色的表达能力，且在深度超过256层时不会出现性能下降

Conclusion: GBN通过局部方法有效解决了MPNN的过平滑和过压缩问题，在深度网络中保持稳定性能

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [190] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 应用可解释AI技术解释PhaseNet地震检测模型，通过Grad-CAM和SHAP方法揭示模型决策依据，并基于SHAP值开发门控推理方案提升模型性能


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在微地震事件检测中精度很高，但其黑盒特性在关键应用中存在担忧，需要提高模型的可解释性和可靠性

Method: 使用Grad-CAM可视化网络注意力，SHAP量化特征贡献度，并开发SHAP门控推理方案结合模型输出和解释性指标

Result: 在9000个波形测试集上，SHAP门控模型F1分数达到0.98（精度0.99，召回率0.97），优于基线PhaseNet的0.97，对噪声具有更强鲁棒性

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [191] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个专门针对静息-运动跨状态条件的ECG生物认证模型，通过多尺度深度卷积特征提取和注意力机制，在不同生理状态下实现强身份识别。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别研究主要关注静息状态，而静息-运动场景下的性能下降问题尚未解决，需要开发跨状态条件下的鲁棒认证模型。

Method: 结合多尺度深度卷积特征提取和注意力机制，专门针对跨状态（静息-运动）条件进行优化设计。

Result: 在运动-ECGID数据集上，静息到运动场景识别准确率达92.50%，运动到静息场景达94.72%，静息到静息场景达99.94%，混合到混合场景达97.85%。在ECG-ID和MIT-BIH数据集上的验证进一步证实了模型的泛化能力。

Conclusion: CrossStateECG展示了在动态现实环境中进行运动后ECG认证的实用潜力，为跨状态生物识别提供了有效解决方案。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [192] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers通过训练发展出模块化、可解释的机制来支持组合推理，其内部算法结构与观察到的行为能力相关。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在未训练序列上表现出的组合推理能力，探索其归因于上下文学习和技能组合的现象。

Method: 使用随机层次模型(RHM)作为概率上下文无关文法生成序列，在序列子集上训练模型，并在四种泛化条件下评估：记忆、分布内泛化、分布外泛化（相同规则）和跨层迁移。

Result: 性能随任务复杂度和上下文示例数量系统提升，分布外任务需要比分布内场景更多的示例。训练中出现层专业化，与泛化性能相关。PCA和注意力模式聚类显示Transformer在专门层中发展出结构化、层次化组织的表示。

Conclusion: Transformer发展出支持组合推理的模块化、可解释机制，其内部算法结构与观察到的行为能力相关联。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [193] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出DAMSDAN网络解决EEG情绪识别的跨域泛化问题，通过动态源域加权和原型引导的细粒度对齐来减少负迁移并增强类别区分能力。


<details>
  <summary>Details</summary>
Motivation: EEG情绪识别存在显著的个体间差异，限制了跨域设置下的泛化能力。需要解决多源域适应中的分布异质性建模和语义一致性保持两个核心挑战。

Method: 集成原型约束与对抗学习，使用基于MMD的域感知源加权策略动态估计域间偏移，并通过具有双伪标签交互的原型引导条件对齐模块增强伪标签可靠性。

Result: 在SEED和SEED-IV数据集上，跨被试平均准确率分别为94.86%和79.78%，跨会话分别为95.12%和83.15%。在FACED数据集上跨被试准确率达82.88%。

Conclusion: 广泛的消融实验和可解释性分析证实了所提框架在跨域EEG情绪识别中的有效性，能够有效减少负迁移并增强语义一致性。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [194] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 该研究探索了使用生成对抗网络(GAN)进行地下沉积物建模的可行性，发现GAN的潜在空间纠缠问题导致反演困难，但通过微调可以改善匹配效果。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高昂且不确定性大，获取新数据难以扩展。将地质知识直接嵌入预测模型提供了一种有价值的替代方案。

Method: 使用生成对抗网络(GAN)训练生成河流沉积物模型，并应用四种反演方法来匹配井数据和地震数据。

Result: 四种反演方法在4、8和20口井的三个测试样本中难以匹配井数据，特别是当井数增加或测试样本与训练数据差异较大时。通过微调GAN重构潜在空间可以降低不匹配度。

Conclusion: GANs已能处理地质建模工作流中的任务，但需要进一步评估其鲁棒性以及如何最好地支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [195] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩阵分解的差分隐私分析方法，用于改进去中心化学习中的隐私计算，并开发了新的算法MAFALDA-SGD。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽然能保护数据隐私，但现有的差分隐私计算方法在实践中往往导致比集中式训练更差的隐私-效用权衡，需要更精确的隐私计算方法来改善这一状况。

Method: 通过推广现有的矩阵分解结果，将标准去中心化学习算法和常见信任模型统一到一个框架中，利用时间噪声相关性分析来获得更紧密的隐私计算。

Result: 开发了MAFALDA-SGD算法，这是一种基于gossip的去中心化学习算法，在合成图和真实世界图上优于现有方法。

Conclusion: 基于矩阵分解的差分隐私计算方法能够为现有DP-DL算法提供更紧密的隐私计算，并为开发新算法提供了理论基础。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [196] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出了一种通过层间Hessian矩阵分析神经网络的方法，揭示了Hessian谱特性与过拟合、欠参数化和表达能力之间的定量关系。


<details>
  <summary>Details</summary>
Motivation: 为神经网络参数空间的局部几何特性提供形式化工具，连接优化几何与功能行为，指导网络架构设计和训练稳定性改进。

Method: 定义每个功能块（层）的局部Hessian矩阵作为参数二阶导数的矩阵，分析其谱特性（如特征值分布），并在37个数据集上进行了111次实验的实证研究。

Result: 发现训练过程中局部Hessian存在一致的结构规律，其谱特性与泛化性能存在相关性，揭示了与过拟合、欠参数化和表达能力相关的定量模式。

Conclusion: 局部几何分析为深度神经网络的诊断和设计提供了基础，建立了优化几何与功能行为之间的联系，为改进网络架构和训练稳定性提供了实用见解。

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [197] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号基准测试，用于评估大语言模型和大推理模型在类比和数学推理中的泛化能力和鲁棒性。它通过增加操作数复杂度、属性范围和引入感知不确定性来扩展I-RAVEN。


<details>
  <summary>Details</summary>
Motivation: 开发一个更全面的基准测试来评估大语言模型和大推理模型在复杂推理任务中的表现，特别是在面对不确定性时的能力。

Method: 扩展I-RAVEN基准，增加操作数复杂度、扩大属性范围，并引入感知不确定性，然后使用该基准测试LLMs和LRMs的性能。

Result: 相比LLMs，LRMs在更长的推理关系和更宽的属性范围上表现出更好的生产力和系统性。但LRMs在不确定性推理方面仍有显著挑战，无法有效探索多个概率结果。

Conclusion: LRMs在复杂推理任务中优于LLMs，但在处理不确定性方面仍有局限，需要进一步改进以有效探索概率性结果。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [198] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法使随机DC优化在小批量下收敛，无需大批次或强噪声假设


<details>
  <summary>Details</summary>
Motivation: 现有随机DC优化方法需要大批次或强噪声假设，限制了实际应用，而小批量下的收敛性质尚不明确

Method: 提出基于动量的算法，在标准平滑性和有界方差假设下实现收敛

Result: 证明无动量时无论步长如何都可能不收敛，动量方法具有可证明的收敛性和强实证性能

Conclusion: 动量是随机DC优化在小批量设置下收敛的关键因素

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [199] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 该论文分析了过参数化最小二乘问题中梯度下降在大学习率下的收敛行为，揭示了学习率大小如何影响收敛到平坦最小值的动态过程。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论只保证梯度下降在小学习率下的单调收敛，但神经网络训练常使用大学习率（边缘稳定性区域），此时目标函数非单调下降且偏向平坦最小值。本文旨在量化这一现象。

Method: 利用过参数化使全局最小点形成黎曼流形M，将GD动态分解为平行和正交于M的分量。平行分量对应黎曼梯度下降，正交分量是分叉动力系统。

Result: 识别了三种学习率区域：(a) 亚临界区域：有限时间内克服瞬时不稳定性后线性收敛到次优平坦最小值；(b) 临界区域：不稳定性持续存在，以幂律收敛到最优平坦最小值；(c) 超临界区域：不稳定性持续存在，线性收敛到以最优平坦最小值为中心的周期2轨道。

Conclusion: 通过过参数化设置和动力系统分解，为理解梯度下降在大学习率下的收敛行为提供了理论框架，解释了边缘稳定性现象及其对平坦最小值的偏好。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [200] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论的新框架，使用图论工具分析稀疏神经网络的训练动态，解释了不同剪枝方法产生的连接模式如何影响网络可训练性。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络训练效果差异显著，但现有方法无法解释相同稀疏度下不同结构为何具有不同可训练性，需要系统性理论框架来理解这一根本问题。

Method: 基于图极限理论（特别是图论），提出图极限假说，建立图论神经正切核来分析无限宽度极限下稀疏网络的训练动态。

Result: 图论NTK的谱分析与稀疏网络实际训练动态相关，能够解释不同剪枝方法的收敛行为差异，验证了图极限假说。

Conclusion: 该框架为稀疏网络的理论分析提供了通用工具，揭示了连接模式对网络可训练性的影响机制。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [201] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出了SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源数据构建行为档案，使用注意力网络识别时空特征，在模拟环境中达到96.8%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注功能性临时异常（如困倦、分心），但缺乏对病理触发的驾驶行为偏差的研究，特别是慢性疾病如帕金森病导致的驾驶安全问题。

Method: 分析帕金森病症状学，建立与驾驶性能退化的因果关系；整合多车辆控制组件数据构建行为档案；设计基于注意力的网络自适应优先处理时空特征。

Result: 在Logitech G29平台和CARLA模拟器上验证，使用三个道路地图模拟真实驾驶，SAFE-D在区分正常和帕金森病影响驾驶模式方面达到96.8%的平均准确率。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为慢性疾病患者的驾驶安全提供了新的解决方案。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [202] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作博弈，通过好奇心奖励机制解决长尾不平衡问题，在罕见标签上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而许多在实践中重要的罕见标签被忽略。

Method: 将标签空间分配给多个合作玩家，玩家共享全局准确度收益，同时根据标签稀有度和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重。

Result: 在传统基准和三个超大规模数据集上的实验显示，相比最强基线获得高达+4.3%的Rare-F1和+1.6%的P@3提升，消融研究揭示了分工涌现和罕见类别上更快的共识形成。

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了一条原则性、可扩展的路径。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [203] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出Counterfactual Knowledge Distillation (CFKD)框架，通过生成多样反事实样本来解决深度学习模型对虚假相关性的脆弱性问题，无需组标签即可实现跨组的平衡泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到虚假相关性的影响，现有方法如DFR依赖显式的组标签且在小样本情况下效果不佳，特别是在多个虚假相关性将数据分割成更小组时性能急剧下降。

Method: CFKD框架通过生成多样反事实样本，让人类标注者能够高效探索和修正模型的决策边界，通过知识蒸馏步骤不仅重新加权欠采样组，还为其丰富新的数据点。

Result: 在五个数据集上的实验表明，CFKD在低数据量且存在明显虚假相关性的场景下表现尤为突出，能够有效扩展到多个混淆变量并实现跨组的平衡泛化。

Conclusion: CFKD提供了一种无需混淆变量标签的有效方法，能够显著提升模型在存在虚假相关性场景下的鲁棒性，特别是在小样本和多个混淆变量的复杂情况下。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [204] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 该论文研究了在低信噪比(SNR)数据中，通过向梯度下降训练过程引入标签噪声来抑制神经网络过拟合噪声、改善泛化性能的方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易同时学习统计信号和过拟合训练集中的噪声，特别是在低SNR情况下，这种噪声记忆会损害泛化性能。受标签噪声具有隐式正则化效果的启发，研究是否可以通过引入标签噪声来提升神经网络在低SNR环境下的测试性能。

Method: 采用简单的标签噪声梯度下降(GD)算法，在理想的信号-噪声数据设置下训练两层神经网络。在训练过程中主动添加标签噪声来抑制噪声记忆。

Result: 证明标签噪声GD能够抑制噪声记忆，防止其主导学习过程，实现快速信号增长同时控制过拟合，从而在低SNR下获得良好泛化性能。相比之下，标准GD倾向于过拟合噪声，其测试误差存在非零下界。

Conclusion: 在基于梯度的训练中引入标签噪声能够有效改善神经网络在低信噪比数据上的泛化能力，通过抑制噪声记忆来实现更好的性能。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [205] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于保形对齐的级联方法(CAb)，在边缘-云级联系统中保证边缘预测集满足云模型级别的条件覆盖要求，同时减少向云端的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能虽然能通过紧凑的本地模型实现低延迟推理，但保证可靠性仍然具有挑战性。需要确保边缘预测集在条件覆盖方面达到云模型的水平。

Method: 将边缘到云端的升级建模为多重假设检验问题，采用保形对齐(CA)来选择哪些输入可以在边缘安全处理，提出CAb级联机制来证明条件覆盖属性。

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联在保持目标条件覆盖的同时，显著减少了向云端的卸载，预测集大小仅适度增加。

Conclusion: CAb级联方法能够在边缘-云系统中提供统计保证，平衡覆盖率、延迟率和集合大小之间的权衡，有效提升边缘推理的可靠性。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [206] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一个用于高效学习车辆GPS轨迹语义的新方法，通过联合建模GPS和道路视角、集成旅行目的预训练以及知识蒸馏来压缩轨迹，在效率和准确性上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含有价值的移动模式和旅行目的语义信息，但现有方法面临两个主要挑战：旅行目的与道路功能和POI相关，文本信息处理计算负担重；真实轨迹包含冗余点，影响计算效率和嵌入质量。

Method: 提出TrajMamba方法，包含Traj-Mamba编码器联合建模GPS和道路视角，旅行目的感知预训练将旅行目的集成到嵌入中，以及知识蒸馏预训练通过可学习掩码生成器识别关键轨迹点来压缩轨迹。

Result: 在两个真实世界数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性上都优于最先进的基线方法。

Conclusion: TrajMamba能够有效解决轨迹学习中的计算负担和冗余问题，提供高效且语义丰富的车辆轨迹表示，在多个下游任务中表现出色。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [207] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展的解码器Transformer，通过在生成过程中引入无监督学习的随机潜变量进行条件化，显著提升了在下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 为了增强解码器Transformer的生成能力，使其能够利用无监督学习到的潜变量来条件化生成过程。

Method: 扩展解码器Transformer，引入随机潜变量，通过变分方法进行无监督学习，使生成过程能够基于这些潜变量进行条件化。

Result: 实验评估表明，这种条件化方法在下游任务上带来了显著的性能提升。

Conclusion: 通过引入无监督学习的潜变量来条件化Transformer的生成过程，是一种有效提升模型在下游任务性能的方法。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [208] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了评估时间序列异常检测的验证属性框架，分析了37个常用指标的不足，并开发了满足所有属性的LARM和ALARM指标来解决现有评估不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的未检测异常可能导致安全关键系统的灾难性故障，而现有评估指标只捕捉任务的狭窄方面，经常产生误导性结果。

Method: 引入可验证属性来形式化评估时间序列异常检测的基本要求，建立理论框架支持原则性评估和可靠比较。分析37个广泛使用的指标，提出LARM指标及其扩展ALARM。

Result: 分析显示大多数指标只满足少数属性，没有指标满足所有属性，这解释了先前结果中持续存在的不一致性。LARM和ALARM被证明满足所有属性要求。

Conclusion: 通过引入验证属性和新的评估指标，为时间序列异常检测提供了更可靠和一致的评估框架，解决了现有评估方法的不一致问题。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [209] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解但需要仔细调参。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，拉格朗日方法是处理约束优化问题的流行选择，但乘子λ的选择对性能影响很大。目前缺乏关于自动更新乘子鲁棒性和性能影响的实证证据。

Method: 通过分析多个任务中拉格朗日乘子的最优性和稳定性，提供λ-配置文件可视化回报与约束成本之间的权衡，并比较自动乘子更新与固定乘子的性能。

Result: 研究发现λ具有高度敏感性，自动乘子更新能够恢复甚至超过最优性能，但存在振荡行为。PID控制更新可以缓解振荡，但需要仔细调参才能在不同任务中实现一致更好的性能。

Conclusion: 拉格朗日乘子在安全强化学习中表现出高度敏感性，自动更新方法虽然有效但存在稳定性问题，需要进一步研究稳定化方法。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [210] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该论文研究了通过降维进一步压缩抗菌肽设计空间的方法，探讨了如何提高潜在空间的可解释性和优化效率，发现基于物理化学性质组织潜在空间可以改善抗菌活性的优化效果。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽是治疗细菌感染的有前景疗法，但序列空间巨大导致发现和设计困难。现有深度生成模型缺乏可解释性，且对潜在空间质量的量化不足，需要改进优化效率。

Method: 使用变分自编码器等深度生成模型，结合降维技术压缩设计空间，并通过物理化学性质组织潜在空间，在不同标签可用性下进行实验。

Result: 发现进一步降维压缩潜在空间在数据可用性较高时具有优势，降维搜索空间更易解释，且可用不同物理化学性质有效组织潜在空间。

Conclusion: 通过降维和基于物理化学性质组织潜在空间，可以显著提高抗菌肽设计的可解释性和优化效率，为生物分子设计提供改进方法。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [211] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出CEPerFed方法，通过客户端历史风险梯度和历史平均梯度协调本地与全局优化，并使用分层SVD策略降低通信开销，解决联邦学习中的数据异构性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI分类需要大量多样化数据，但医疗机构的原始数据共享存在隐私问题。联邦学习是可行方案，但面临数据异构性导致的模型收敛困难和大量参数传输带来的通信开销挑战。

Method: CEPerFed方法：1) 使用客户端历史风险梯度加权其他客户端贡献，增强本地更新可靠性；2) 使用历史平均梯度确保本地更新与全局优化方向一致；3) 采用分层SVD策略仅传输模型更新所需的关键信息。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed通过协调本地与全局优化以及降低通信开销，成功解决了联邦学习在医疗MRI分类中的数据异构性和通信效率问题。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [212] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉变换器，用于区分心源性肺水肿与非心源性肺水肿和正常肺部，在肺超声视频分类中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的视觉变异性高，区分心源性肺水肿与其他肺部状况具有挑战性，现有方法难以处理这种异质性。

Method: 提出ZACH-ViT（零标记自适应紧凑分层视觉变换器），移除位置嵌入和[CLS]标记，实现完全排列不变性；引入ShuffleStrides数据增强，对探头视图序列和帧顺序进行排列。

Result: 在380个肺超声视频上评估，ZACH-ViT获得最高验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而竞争模型均失效。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [213] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）处理自杀预测中的极端类别不平衡问题，通过生成合成数据增强样本，在真实测试数据上取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 自杀预测是预防的关键，但真实数据中阳性样本稀少导致极端类别不平衡，需要数据增强来改善模型性能。

Method: 使用机器学习模型（逻辑回归、随机森林、支持向量机）和深度学习技术（生成对抗网络GAN）生成合成数据样本来增强数据集。

Result: 在真实测试数据上：逻辑回归加权精度0.99、召回率0.85、F1分数0.91；随机森林分别为0.98、0.99、0.99；支持向量机分别为0.99、0.76、0.86。逻辑回归和SVM正确识别了所有自杀尝试案例，但有一定误报。

Conclusion: 这些结果证明了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [214] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种级联方法，将预训练开放词汇目标检测模型与轻量级少样本分类器结合，通过FLAME主动学习策略选择信息量最大的样本进行训练，实现在遥感领域的快速适应。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测模型在遥感等专业领域中，由于自然语言的模糊性导致细粒度类别区分困难，影响下游应用效果。

Method: 使用零-shot模型生成高召回率目标提案，然后通过轻量级分类器进行精炼；核心是FLAME主动学习策略，通过密度估计和聚类选择边界附近的不确定样本。

Result: 在遥感基准测试中持续超越最先进方法，能够在一分钟内完成快速适应，显著快于现有替代方案。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够快速适应用户特定需求。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [215] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出了一种语言在环框架，使用大语言模型将自然语言反馈转换为标量效用，以在数值搜索空间上进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，反馈对于将复杂、细微或主观目标转化为可量化优化目标至关重要。传统方法仅接受受限反馈格式且需要为每个领域特定问题定制模型。

Method: 利用大语言模型将各种类型的文本反馈转换为一致的效用信号，并轻松包含灵活的用户先验，无需手动设计核函数。

Result: 该混合方法不仅为决策者提供了更自然的界面，而且在反馈受限的情况下优于传统贝叶斯优化基线和仅使用LLM的优化器。

Conclusion: 语言在环框架结合了LLM的自然语言处理能力和贝叶斯优化的样本效率与不确定性量化优势，在反馈有限的情况下表现优异。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [216] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 该论文提出了三种主要贡献：CADP算法连接策略梯度和动态规划，ERM-TRC和EVaR-TRC的风险规避策略优化方法，以及收敛到最优风险规避值函数的Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究马尔可夫决策过程（MMDPs）中策略优化与动态规划的联系，解决风险规避目标下的策略计算问题，特别是ERM-TRC和EVaR-TRC的风险度量方法。

Method: 1. 提出CADP算法，通过迭代调整模型权重实现单调策略改进；2. 建立ERM Bellman算子的收缩条件，提出指数值迭代、策略迭代和线性规划算法；3. 设计模型无关的Q学习算法处理风险规避目标。

Result: 证明了ERM-TRC和EVaR-TRC Q学习算法的收敛性，能够计算最优风险规避值函数和最优平稳策略，CADP算法能保证单调改进到局部最优。

Conclusion: 论文成功建立了策略梯度与动态规划的新联系，提出了多种有效的风险规避策略优化算法，并证明了其收敛性和最优性，为MMDPs中的风险规避决策提供了理论和方法支持。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [217] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的Sim2Real框架，通过双层强化学习直接基于真实世界性能调整模拟器参数，以缩小Sim2Real性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前Sim2Real方法通过优化模拟器精度和变异性作为真实世界性能的代理指标，但这些指标与策略在真实世界中的性能不一定相关，导致模拟训练的策略在真实部署时性能显著下降。

Method: 采用双层强化学习框架：内层RL在模拟环境中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发能够缩小Sim2Real性能差距的双层RL算法所需的数学工具。

Conclusion: 该框架通过直接基于真实世界性能优化模拟器参数，为解决Sim2Real性能差距问题提供了新途径。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [218] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 本文研究如何提高黑盒大语言模型作为分类器时的操作粒度，通过分析其低基数数值输出的原因，并提出了有效方法来显著增加可用操作点数量和多样性。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs虽然实用易用，但在需要特定指标约束的应用中表现不佳，主要因为其数值输出基数低，限制了操作点的精细调整能力。

Method: 首先分析LLMs低基数输出的原因，发现其偏向生成四舍五入但信息丰富的语言化概率；然后实验标准提示工程、不确定性估计和置信度提取技术；最后提出有效方法来显著增加可用操作点数量和多样性。

Result: 在11个数据集和3个LLMs上的实验表明，所提方法提供了更细粒度的操作点，并实现了与基准方法相当或更好的性能。

Conclusion: 提出的方法能有效提高黑盒LLMs作为分类器时的操作粒度，且不牺牲性能或增加推理成本。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [219] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 开发物理信息神经网络(PINN)策略，将海冰物理知识集成到机器学习模型中，以改进北极海冰速度和浓度的预测。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的机器学习模型在泛化性和物理一致性方面存在局限，特别是在北极海冰变薄和融化加速的新阶段，历史数据训练的模型可能无法充分代表未来动态变化的海冰条件。

Method: 基于分层信息共享U-net架构，整合物理损失函数和激活函数，生成物理上合理的海冰速度和浓度输出。

Result: PINN模型在海冰速度和浓度的日预测中优于纯数据驱动模型，即使在少量样本训练下也表现良好，特别改进了融化和早期冻结季节以及快速移动冰区的海冰浓度预测。

Conclusion: 物理信息神经网络方法能够有效提升海冰预测的准确性和物理一致性，特别是在数据稀缺或海冰条件快速变化的情况下。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [220] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于微分图册的流形学习方法，通过维护可微分图册实现流形上的黎曼优化，在效率和准确性方面具有优势，并在分类任务和RNA速度分析中展示了更好的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要进行降维到欧几里得空间，当嵌入维度接近流形本征维度时会丢失关键特征。直接学习潜在流形作为微分图册的方法相对较少探索，本文旨在证明基于图册方法的有效性和潜力。

Method: 实现了一个通用数据结构来维护可微分图册，支持流形上的黎曼优化，并配合无监督启发式方法从点云数据学习微分图册。

Result: 实验证明该方法在选定场景下具有效率和准确性优势，在Klein瓶上的监督分类任务和造血数据的RNA速度分析中，展示了更好的可解释性和鲁棒性。

Conclusion: 基于微分图册的流形学习方法具有实际应用价值，在保持流形结构的同时支持机器学习任务，为流形学习提供了新的方向。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [221] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一个样本级别的框架来衡量后训练过程中的知识遗忘和反向迁移，发现不同后训练阶段对预训练知识的影响各不相同，模型合并不能可靠缓解遗忘。


<details>
  <summary>Details</summary>
Motivation: 理解后训练对预训练知识的影响，因为知识遗忘不是平均分布的，需要更精确的测量方法。

Method: 提出样本级度量方法，统计1->0（遗忘）和0->1（反向迁移）的转换，并针对选择题基准添加了机会调整变体。

Result: 大规模分析显示：领域持续预训练导致中度遗忘和低到中度反向迁移；RL/SFT后训练在数学和逻辑上产生中度到大的反向迁移；模型合并不能可靠缓解遗忘。

Conclusion: 该框架为理解后训练如何改变预训练知识提供了实用标准，有助于开发通用AI系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [222] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出了在推理时保持线性插值的流匹配缩放方法，在图像和蛋白质生成任务中验证了推理计算增加能持续提升样本质量


<details>
  <summary>Details</summary>
Motivation: 流匹配方法在推理时的缩放方法研究不足，现有方法使用非线性插值会牺牲高效采样特性，需要开发保持线性插值的推理缩放方法

Method: 提出了新颖的推理时缩放程序，在采样过程中保持线性插值，应用于图像生成和无条件蛋白质生成任务

Result: 样本质量随着推理计算增加而持续提升，首次证明流匹配推理时缩放可应用于科学领域

Conclusion: 流匹配推理时缩放方法在保持线性插值的同时，能有效提升样本质量，并成功扩展到科学领域应用

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [223] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出了Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布方法，通过beta-ELBO和蒙特卡洛采样训练，能够自适应地调整预测混合的分散度以适应输入分布变化。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往过于自信，需要一种能够适应输入分布变化并提供良好校准预测的方法。

Method: 使用输入条件化的网络权重分布，通过beta-ELBO损失函数和蒙特卡洛采样进行训练，诱导产生自适应分散度的预测混合分布。

Result: 在标准回归任务上，与贝叶斯、集成、dropout和超网络基线相比，在匹配参数和更新预算下，FDN在准确性、校准性和偏移感知性方面表现良好。

Conclusion: 该框架和评估协议旨在使具有OOD感知和良好校准的神经回归变得实用和模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [224] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了GaLore Unbiased with Muon (GUM)方法，通过层间采样技术消除低秩投影的偏差，在保持内存效率的同时实现收敛保证和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩优化方法（如GaLore）缺乏收敛保证，其低秩投影机制会引入偏差，导致与全参数训练存在性能差距。

Method: 结合GaLore的低秩投影机制和Muon算法，采用层间采样技术来消除偏差，提出GUM方法。

Result: 理论证明GUM匹配Muon算法的收敛保证，实验显示在LLM微调和预训练中优于GaLore，甚至超过全参数训练。

Conclusion: GUM通过更均匀的层内知识分布，实现了参数空间的高效利用和更好的记忆能力，解决了低秩优化的偏差问题。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [225] [Quantum Approximate Optimization Algorithm for MIMO with Quantized b-bit Beamforming](https://arxiv.org/abs/2510.15935)
*Nikos A Mitsiou,Ioannis Krikidis,George K Karagiannidis*

Main category: cs.ET

TL;DR: 本文提出使用量子近似优化算法(QAOA)和交替优化来解决6G MIMO系统中b位量化移相器的波束成形问题，首次展示了该问题与量子电路的天然映射关系，并通过数值结果验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 6G通信中的MIMO系统面临硬件复杂度和功耗的挑战，低比特量化架构虽然成本效益高，但引入了NP难组合优化问题，需要寻找有效的解决方案。

Method: 采用量子近似优化算法(QAOA)和交替优化方法，将量化移相器的波束成形问题映射到量子电路的旋转门，并提出了预热启动QAOA来提高计算效率。

Result: 数值结果表明，所提出的方法在量化波束成形增益方面优于文献中的经典优化基准方法。

Conclusion: 该研究首次建立了量化波束成形问题与量子算法的理论联系，提出的QAOA方法为6G MIMO系统提供了一种有效的低比特量化解决方案，在集成感知通信和量子机器学习等领域具有应用潜力。

Abstract: Multiple-input multiple-output (MIMO) is critical for 6G communication,
offering improved spectral efficiency and reliability. However, conventional
fully digital designs face significant challenges due to high hardware
complexity and power consumption. Low-bit MIMO architectures, such as those
employing b-bit quantized phase shifters, provide a cost-effective alternative
but introduce NP-hard combinatorial problems in the pre- and post-coding
design. This paper explores the use of the Quantum Approximate Optimization
Algorithm (QAOA) and alternating optimization to address the problem of b-bit
quantized phase shifters both at the transmitter and the receiver. We
demonstrate that the structure of this quantized beamforming problem aligns
naturally with hybrid-classical methods like QAOA, as the phase shifts used in
beamforming can be directly mapped to rotation gates in a quantum circuit.
Notably, this paper is the first to show that theoretical connection. Then, the
Hamiltonian derivation analysis for the b-bit case is presented, which could
have applications in different fields, such as integrated sensing and
communication, and emerging quantum algorithms such as quantum machine
learning. In addition, a warm-start QAOA approach is studied which improves
computational efficiency. Numerical results highlight the effectiveness of the
proposed methods in achieving an improved quantized beamforming gain over their
classical optimization benchmarks from the literature.

</details>


### [226] [Navigate in Demanding Missions: Integrating Human Intelligence and Brain-Inspired Intelligence](https://arxiv.org/abs/2510.17530)
*Xu He,Xiaolin Meng,Youdong Zhang,Lingfei Mo,Wenxuan Yin*

Main category: cs.ET

TL;DR: 该论文分析了神经科学、脑启发智能和脑启发导航之间的复杂关系，提出将神经形态赋能的脑机接口整合到脑启发导航中，以增强无人系统在挑战性任务中的可靠导航能力。


<details>
  <summary>Details</summary>
Motivation: 当前脑机接口与脑启发导航领域缺乏合作关系，作者希望通过整合神经形态赋能的脑机接口来增强无人系统的可靠导航能力，特别是在深空探索等挑战性任务中。

Method: 提出将神经形态赋能的脑机接口整合到脑启发导航中，通过脑启发的人工意识增强机器智能，同时以神经形态赋能的脑机接口作为机器智能失效时的安全保障。

Result: 该方法有望增强无人系统的能力，促进空间认知障碍的诊断，同时需要考虑相关的伦理和安全问题。

Conclusion: 通过整合神经形态赋能的脑机接口与脑启发导航，可以建立机器智能与人类智能之间的协同关系，在增强无人系统能力的同时确保安全性。

Abstract: This perspective analyzes the intricate interplay among neuroscience,
Brain-Inspired Intelligence (BII), and Brain-Inspired Navigation (BIN),
revealing a current lack of cooperative relationship between Brain-Computer
Interfaces (BCIs) and BIN fields. We advocate for the integration of
neuromorphic-empowered BCI into BIN, thereby bolstering the unmanned systems'
reliable navigation in demanding missions, such as deep space exploration, etc.
We highlight that machine intelligence, reinforced by brain-inspired artificial
consciousness, can extend human intelligence, with human intelligence mediated
by neuromorphic-enabled BCI acting as a safeguard in case machine intelligence
failures. This study also discusses the potentials of the proposed approach to
enhance unmanned systems' capabilities and facilitate the diagnostics of
spatial cognition disorders, while considering associated ethical and security
concerns.

</details>


### [227] [Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring](https://arxiv.org/abs/2510.17688)
*Shawn M. Gibford,Mohammad Reza Boskabadi,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.ET

TL;DR: 该论文提出使用量子Wasserstein生成对抗网络(QWGAN-GP)生成生物制造过程的合成时间序列数据，以解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 生物制造领域面临数据稀缺和稀疏的挑战，这影响了模型开发、过程监控和优化的准确性。

Method: 采用量子Wasserstein生成对抗网络，其中生成器由参数化量子电路(PQC)组成，用于生成合成时间序列数据。

Result: 该方法在捕捉真实生物过程数据的时间动态方面表现良好，生成的光密度数据与历史实验数据具有高度保真度。

Conclusion: 量子计算与机器学习的结合为数据分析和生成开辟了新前沿，特别是在计算密集型领域，可用于提高软传感器设计的预测精度或用于预测控制。

Abstract: Data scarcity and sparsity in bio-manufacturing poses challenges for accurate
model
  development, process monitoring, and optimization. We aim to replicate and
capture
  the complex dynamics of industrial bioprocesses by proposing the use of a
Quantum
  Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP)
to
  generate synthetic time series data for industrially relevant processes. The
  generator within our GAN is comprised of a Parameterized Quantum Circuit
(PQC). This
  methodology offers potential advantages in process monitoring, modeling,
  forecasting, and optimization, enabling more efficient bioprocess management
by
  reducing the dependence on scarce experimental data. Our results demonstrate
  acceptable performance in capturing the temporal dynamics of real bioprocess
data.
  We focus on Optical Density, a key measurement for Dry Biomass estimation.
The data
  generated showed high fidelity to the actual historical experimental data.
This
  intersection of quantum computing and machine learning has opened new
frontiers in
  data analysis and generation, particularly in computationally intensive
fields, for
  use cases such as increasing prediction accuracy for soft sensor design or
for use
  in predictive control.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [228] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 提出了两种基于MPI的并行自助法策略，通过局部统计量聚合和同步伪随机数生成来解决通信开销和内存限制问题，显著提升了大规模系统的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统自助法的计算成本在大数据集或大量重采样时变得不可行，需要解决分布式环境中的高通信开销和内存约束问题。

Method: 1) 局部统计量聚合：传输充分统计量而非完整重采样数据集来大幅减少通信；2) 同步伪随机数生成：当整个数据集无法存储在单个进程中时实现分布式重采样。

Result: 开发了通信和计算复杂度的分析模型，与朴素基线方法相比，所提方法在通信量和内存使用方面显著减少。

Conclusion: 所提出的方法能够在大规模系统上实现可扩展的并行自助法，为大数据统计推断提供了有效的解决方案。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [229] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: 提出MeCeFO算法，一种内存和计算高效的容错优化方法，用于分布式大语言模型训练，在节点故障时能最小化额外开销并保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着分布式优化扩展到满足大语言模型训练需求，硬件故障变得不可忽视。现有容错训练方法通常引入显著的计算或内存开销，需要额外资源。

Method: MeCeFO采用三种关键算法设计：(i) Skip-connection在反向传播时丢弃多头注意力模块；(ii) Recomputation减少前馈网络的激活内存；(iii) 低秩梯度近似实现FFN权重矩阵梯度的高效估计。

Result: 理论上MeCeFO匹配传统分布式训练的收敛速率O(1/√(nT))；实证上在高故障率下保持稳健性能，吞吐量仅下降4.18%，比之前SOTA方法具有5.0-6.7倍的更强韧性。

Conclusion: MeCeFO是一种高效容错的分布式优化算法，能在最小化额外开销的同时确保大语言模型训练的鲁棒性。

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [230] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress是一种基于傅里叶变换的LLM激活压缩框架，通过在频域保留低频系数实现高效压缩，显著减少边缘设备与服务器间的通信开销，同时保持接近无损的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决协作式LLM推理中由高维中间激活传输导致的通信瓶颈问题，现有压缩方法难以同时实现高压缩比、低重构误差和计算效率。

Method: 利用LLM激活在频域的稀疏性，通过FFT将激活转换到频域，仅保留紧凑的低频系数块，在服务器端利用共轭对称性重构信号，支持DSP和FPGA硬件加速。

Result: 在Llama 3和Qwen2.5模型上的实验表明，平均减少7.6倍激活大小，准确率损失小于0.3%，压缩时间比Top-k减少32倍以上。

Conclusion: FourierCompress在通信效率、推理质量和压缩速度之间实现了良好平衡，为边缘设备上的LLM推理提供了实用解决方案。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [231] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: 提出了一个用于基尼亚卢旺达语和斯瓦希里语语音转录与合成的边缘-云并行框架，通过级联机制在边缘设备和云端之间分配模型推理工作负载，显著降低了延迟和资源使用。


<details>
  <summary>Details</summary>
Motivation: 解决东非国家基尼亚卢旺达语和斯瓦希里语等广泛使用语言缺乏强大语言处理工具的问题，特别是在技术基础设施有限的环境中。

Method: 利用Whisper和SpeechT5预训练模型实现语音转文本和文本转语音翻译，采用级联架构在边缘设备和云端之间分配模型推理工作负载。

Result: 在边缘设备上，SpeechT5模型内存使用压缩了9.5%，Whisper模型压缩了14%，最大内存使用量为149MB。在1.7GHz CPU边缘设备和1MB/s网络带宽下，系统能在1分钟内处理270字符文本的语音转文本和文本转语音转录。

Conclusion: 所提出的级联边缘-云架构可作为基尼亚卢旺达语和斯瓦希里语语音转录与合成的优秀平台，具有良好的准确性和响应时间。

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [232] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris是一种针对机器学习工作负载的领域专用RDMA传输协议，通过移除重传和有序交付机制，利用ML对数据丢失的容忍性来显著降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 随着分布式ML扩展到数千个GPU，传统RDMA设计中的严格可靠性和有序交付机制在高性能互连中成为主要瓶颈，即使罕见的数据包丢失也会持续降低系统性能。

Method: Celeris在RDMA NIC中移除重传和有序交付，采用尽力而为传输，保留拥塞控制，通过软件级机制（如自适应超时和数据优先级）管理通信，将丢失恢复转移到ML流水线（如使用Hadamard变换）。

Result: Celeris将第99百分位延迟降低高达2.3倍，BRAM使用量减少67%，NIC容错能力几乎翻倍。

Conclusion: Celeris为集群规模的ML工作负载提供了一个弹性、可扩展的传输解决方案，通过利用ML对数据丢失的容忍性来优化性能。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [233] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 提出了一种基于C++ Noarr库的MPI新抽象，支持布局无关的MPI应用设计，通过分布式GEMM案例展示了其可用性和性能。


<details>
  <summary>Details</summary>
Motivation: MPI的纯C接口缺乏现代语言特性，如类型检查和泛型编程支持，限制了分布式高性能计算的开发效率。

Method: 将MPI抽象作为C++ Noarr库的扩展实现，遵循Noarr范式（一等布局和遍历抽象），提供布局无关的MPI应用设计。

Result: 该抽象在保持与现有MPI C++绑定相当性能的同时，为分布式应用提供了更灵活的设计方式。

Conclusion: 提出的MPI抽象成功结合了现代C++特性和高性能需求，为分布式计算应用开发提供了更好的编程体验。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [234] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 提出了一种训练语言模型的方法，使其能够在推理过程中与性能工具交互，以解决代码性能优化任务。


<details>
  <summary>Details</summary>
Motivation: 语言模型在软件工程中广泛应用，但在代码性能相关任务（如优化）中表现不佳，因为这些任务依赖环境、硬件等复杂数据，而这些信息在源代码中无法直接体现。

Method: 训练语言模型在推理过程中与性能工具交互，收集环境、硬件等性能相关数据来辅助决策。

Result: 成功训练出最先进的GPU内核优化模型，证明了该方法的有效性。

Conclusion: 通过让语言模型在推理过程中与性能工具交互，可以显著提升其在代码性能优化任务中的表现。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [235] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 提出了一种用于互连多核系统的集成容错架构，通过构建稳定性指标识别可靠机器并进行周期性诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段三模冗余(TMR)在永久故障下失效，而反应式TMR(R-TMR)依赖额外硬件增加系统复杂性，且在多个核心或辅助模块故障时容错能力下降。

Method: 构建稳定性指标识别可靠机器，进行周期性诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。

Result: 实验结果显示，相比基线TMR减少约30%的任务负载，实现了优异的故障覆盖率和隔离精度。

Conclusion: 该方法显著提高了互连多核系统的可靠性和能效。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [236] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成优化CUDA代码方面的能力，通过自动和手动评估验证其代码正确性和性能提升，发现LLM需要额外指导才能达到并行计算专家的优化水平。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在编程工具中的革命性应用，本研究旨在探索最新推理模型在生成优化CUDA代码方面的能力，特别是它们能否独立完成代码优化以及通过提示工程改进性能。

Method: 使用预定义任务测试LLM生成优化CUDA代码的能力，通过自动评估（正确性和加速比）和手动代码审查，并尝试交互式方法让LLM修复错误。

Result: LLM在编码方面表现出色，但需要提供更详细的提示和指导才能达到并行计算专家提供的优化解决方案水平。

Conclusion: 大型语言模型是熟练的编码者，但需要额外的教学指导才能生成高度优化的并行代码。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [237] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 提出基于eBPF的GPU尾延迟监控系统，通过关联主机指标和GPU内部事件，实现81-88%的诊断准确率，5秒内检测延迟尖峰，6-8秒完成根因分析。


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏细粒度根因分析能力，无法有效诊断GPU尾延迟尖峰问题。

Method: 使用eBPF技术构建统一的主机端监控系统，将eBPF获取的主机指标与GPU内部事件关联，实现全系统可观测性。

Result: 系统在100Hz采样频率下仅产生1.21%的CPU开销，能识别NIC争用、PCIe压力和CPU干扰等根因，无需集群范围插装。

Conclusion: 该系统为多租户GPU基础设施提供了有效的操作调试能力，实现了高性能的GPU尾延迟诊断。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [238] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文解决了分布式图算法中关于轮次消除固定点技术是否通用的开放性问题，证明了该技术对无输入问题是通用的，但对有输入问题存在局限性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式图算法领域的一个关键开放性问题：轮次消除固定点技术是否是证明下界的通用方法？特别是针对那些需要Ω(log n)轮次的本地可检查问题。

Method: 开发了基于三重幂输入的新技术来系统构建轮次消除下界，证明了先前基于Marks技术的同态问题确实可以通过轮次消除固定点来证明下界。

Result: 消除了轮次消除通用性的已知障碍，但发现了新障碍：某些有输入问题无法通过松弛到非平凡轮次消除固定点来证明下界。同时证明了首个适用于任何轮次消除固定点问题的通用下界定理。

Conclusion: 轮次消除技术对无输入问题是通用的，但对有输入问题存在局限性，这为分布式计算复杂性的可判定性提供了重要见解。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [239] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 提出了一种通过在内存地址流中编码用户可见状态的方法，使内存设备能够感知程序上下文，从而改善内存优化和遥测能力。


<details>
  <summary>Details</summary>
Motivation: 解决硬件缓存预取、内存调度和交错导致的可观察性损失问题，使内存设备能够获取程序上下文信息以进行更好的优化。

Method: 在内存读取地址流中以非破坏性方式编码用户可见状态作为可检测的数据包，无需显著容量开销、驱动程序或特殊访问权限。

Result: 原型系统能够可靠地从内存地址跟踪中检测和解码元数据，展示了精确代码执行标记和对象地址范围跟踪的使用案例。

Conclusion: 该方法为实时元数据解码和近内存计算提供了基础，未来可实现定制化遥测、统计和基于应用提示的内存优化功能。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [240] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 提出了一种多模态大语言模型助手(MLLMA)，不仅能预测布线拥塞，还能提供可解释的设计建议，通过结合自动特征生成和可解释的偏好学习框架来优化芯片物理设计。


<details>
  <summary>Details</summary>
Motivation: 现有的电子设计自动化(EDA)工具难以提供可解释的反馈或可操作的设计改进指导，需要一种能够理解多模态输入并提供人类可理解建议的智能助手。

Method: 采用多模态大语言模型引导的遗传提示进行自动特征生成，结合可解释的偏好学习框架，建模视觉、表格和文本输入中的拥塞相关权衡关系。

Result: 在CircuitNet基准测试中，该方法在准确性和可解释性方面均优于现有模型，设计建议案例研究和定性分析证实学习到的偏好与现实设计原则一致且对工程师具有可操作性。

Conclusion: 这项工作突显了多模态大语言模型作为交互式助手在可解释和上下文感知的物理设计优化中的潜力。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [241] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: 本文揭示了UPMEM PIM平台软件栈中的低效问题，通过汇编代码优化、位串行处理技术以及NUMA感知的内存分配，显著提升了整数运算和矩阵向量乘法的性能。


<details>
  <summary>Details</summary>
Motivation: PIM平台在数据管理和并行编程方面面临独特挑战，现有SDK仍有很大的性能优化空间，需要探索非标准编程技术来提升计算效率。

Method: 修改UPMEM编译器生成的汇编代码，采用位串行处理低精度数据，扩展支持NUMA架构的PIM分配API。

Result: 整数加法加速1.6-2倍，乘法加速1.4-5.9倍；INT4位串行点积计算加速2.7倍；主机-PIM数据传输吞吐量提升2.9倍；优化后的INT8 GEMV比基准快3.5倍，比双路CPU服务器快3倍以上。

Conclusion: 通过简单的软件栈优化和创新的编程技术，可以显著提升PIM平台的性能表现，特别是在低精度计算场景下具有明显优势。

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [242] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 3D集成技术随着光刻缩放挑战增加和微型通孔制造能力提升而受到广泛关注，它虽然能提高密度但带来更高功率密度挑战，需要在设计、制造和测试方面解决跨层协同设计、独立测试和组装良率等问题。


<details>
  <summary>Details</summary>
Motivation: 随着光刻缩放技术面临挑战，3D集成技术作为提高芯片密度的替代方案受到重视，但需要解决其带来的功率密度增加、跨层设计协同、独立测试和组装良率等新挑战。

Method: 通过分析3D集成系统的特性，阐述其在密度提升方面的优势，同时系统性地枚举和阐明3D系统在设计、组装和测试过程中面临的新挑战。

Result: 识别出3D集成需要在电路、通孔和宏单元的跨层协同设计、各功能层的独立测试能力、以及组装后的良率和测试等方面解决关键技术挑战。

Conclusion: 3D集成技术虽然能够加速芯片缩放，但需要在设计方法学、制造工艺和测试策略等方面进行系统性创新，才能充分发挥其潜力。

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [243] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink是一个创新的集体通信框架，通过聚合NVLink、PCIe和RDMA NICs等异构链路来解决大语言模型多节点部署中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多节点部署成为必需，但现有通信库如NCCL仅使用单一互连，导致性能瓶颈，其他硬件资源如PCIe和RDMA NICs在密集工作负载期间闲置。

Method: FlexLink采用两阶段自适应负载平衡策略，动态地将通信流量分配到所有可用链路上，确保较快的互连不会被较慢的互连拖慢。

Result: 在8-GPU H800服务器上，AllReduce和AllGather等集体操作符的带宽相比NCCL基线分别提高了26%和27%，通过将2-22%的总通信流量卸载到之前未充分利用的PCIe和RDMA NICs上实现。

Conclusion: FlexLink作为无损的即插即用替代方案，与NCCL API兼容，确保了易于采用，有效解决了异构链路利用率不足的问题。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [244] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: 提出全自动化需求驱动功能验证框架，降低可配置IP验证成本


<details>
  <summary>Details</summary>
Motivation: 半导体行业竞争加剧，需要在保持质量的同时降低芯片价格，而可配置IP的功能验证是开发成本的主要来源

Method: 开发全自动化框架，自动执行验证计划生成、测试平台创建、回归执行和需求管理工具报告等关键流程

Result: 大幅减少验证工作量，加速开发周期，减少人为错误，提高覆盖率

Conclusion: 该框架为验证可配置IP的挑战提供了可扩展且高效的解决方案

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [245] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: 本文提出了一种架构无关的测试方案，用于分析消费级NVIDIA GPU中矩阵乘法器的数值特性，包括舍入、归一化和累加器内部精度等特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么进行穷举搜索，要么依赖设备特定的硬编码常量，无法广泛适用于各种混合精度格式。需要开发一种更通用的测试方法来分析消费级GPU的数值特性。

Method: 采用架构无关的测试方案和测试向量生成方法，既不进行穷举搜索也不依赖设备特定常量，适用于广泛的混合精度格式。在RTX-3060和Ada RTX-1000显卡上应用该方案。

Result: 确定了RTX-3060消费级GPU的数值特性与数据中心GPU A100完全相同。该方法可扩展到更新的NVIDIA GPU架构和8位浮点格式。

Conclusion: 提出的测试方案具有通用性和可扩展性，能够分析各种NVIDIA GPU中矩阵乘法器的数值特性，且无需为未来架构修改代码。

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [246] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+是一个模拟消费级分区闪存存储的仿真器，通过添加块接口支持解决了原版ConZone无法挂载文件系统的问题，提升了可用性。


<details>
  <summary>Details</summary>
Motivation: 为了理解和高效改进消费级分区闪存存储的软硬件设计，需要能够模拟其资源约束和架构特征的仿真工具。

Method: 扩展ConZone仿真器，添加块接口支持，提供部署脚本和多项增强功能，使用户能够探索内部架构并集成系统软件优化。

Result: 通过与代表性硬件架构和最先进技术比较验证了ConZone+的准确性，并通过案例研究分析了分区存储设计和当前文件系统的不足。

Conclusion: ConZone+为消费级分区闪存存储提供了一个准确可用的仿真平台，有助于探索存储架构和文件系统优化。

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [247] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: BASIC_RV32s是一个开源的RISC-V RV32I架构实现框架，从单周期核心演进到5级流水线设计，包含完整的前向转发、动态分支预测和异常处理功能，在FPGA上实现并验证。


<details>
  <summary>Details</summary>
Motivation: 填补RISC-V架构理论知识与实际硬件实现之间的空白，为开源硬件生态系统提供可复现的教学路径。

Method: 采用经典的Patterson和Hennessy方法学，从基础单周期核心逐步演进到5级流水线设计，集成到带有UART通信的SoC中，并在Xilinx Artix-7 FPGA上进行验证。

Result: 在50MHz频率下实现1.09 DMIPS/MHz的性能，所有RTL源代码、逻辑框图和发展日志以MIT许可证在GitHub上开源。

Conclusion: BASIC_RV32s为RISC-V硬件实现提供了一个实用的微架构路线图和可复现的教学框架，促进了开源硬件生态系统的发展。

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [248] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: 提出了一种基于现有Load-Linked和Store-Conditional指令的硬件事务内存实现，无需修改缓存一致性协议，仅需修改L1数据缓存，适用于读写集不超过8个缓存行的应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有HTM实现硬件复杂度高，需要修改指令集架构和缓存一致性协议，限制了其广泛应用。本文旨在简化HTM实现，降低硬件复杂度。

Method: 扩展Load-Linked和Store-Conditional指令语义实现HTM，仅修改L1数据缓存，限制事务读写集大小，提供两种基于重试检测的前进保障机制。

Result: 在Gem5中模拟验证，实现了多个并发数据结构，显示8个字的读写集限制足够。在低竞争情况下性能优于TTS锁，跨节点竞争时中止率很低。

Conclusion: 提出的简化HTM设计在保持良好性能的同时显著降低了硬件实现复杂度，为HTM的广泛应用提供了可行方案。

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [249] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 本文探讨了3D集成光互连技术在AI工作负载扩展中的关键作用，通过光电共封装技术实现跨机架GPU互连，显著提升大规模模型训练性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的持续增长，传统半导体缩放放缓，需要新的扩展引擎。铜互连的最大传输距离限制（约1米）使得扩展域仅限于单个机架内，无法满足万亿参数模型训练需求。

Method: 采用3D堆叠光学和逻辑技术，通过光电共封装（CPO）实现高带宽、低延迟的GPU互连，构建跨越多个数据中心机架的大规模计算域。

Result: 3D CPO技术使扩展能力提升8倍，支持超过万亿参数的专家混合模型训练，训练时间减少2.7倍，实现前所未有的模型扩展。

Conclusion: 3D光电共封装技术是解决AI工作负载扩展瓶颈的关键创新，为大规模模型训练提供了新的扩展路径和性能突破。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [250] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: DiffPlace是一个基于条件去噪扩散过程的芯片布局框架，能够在不重新训练的情况下泛化到未见过的电路网表，实现可迁移的布局策略。


<details>
  <summary>Details</summary>
Motivation: 传统芯片布局方法（分析优化或强化学习）难以处理硬性布局约束或需要为每个新电路设计进行昂贵的在线训练。DiffPlace旨在解决这些限制，提供可泛化的布局解决方案。

Method: 将芯片布局建模为条件去噪扩散过程，利用扩散模型的生成能力探索布局空间，同时基于电路连接性和相对质量指标进行条件约束。结合能量引导采样和约束流形扩散确保布局合法性。

Result: 在所有实验场景中实现了极低的模块重叠率，弥合了基于优化和基于学习的布局方法之间的差距。

Conclusion: DiffPlace为现代VLSI设计提供了一条实现自动化、高质量芯片布局的实用路径，其源代码已公开。

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [251] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 提出意图驱动存储系统(IDSS)，利用大语言模型从非结构化信号中推断工作负载意图，指导存储系统自适应和跨层参数重配置，在FileBench测试中IOPS提升最高达2.45倍。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统缺乏对工作负载意图的可见性，无法适应现代大规模数据密集型应用的语义，导致脆弱的启发式方法和碎片化的优化。

Method: 提出IDSS架构，将LLMs集成到存储控制循环中，通过四个设计原则实现意图推断和参数重配置，在策略护栏内进行安全高效的决策。

Result: 在FileBench工作负载测试中，IDSS通过解释意图并为缓存和预取等存储组件生成可操作的配置，使IOPS提升最高达2.45倍。

Conclusion: 当受到护栏约束并嵌入结构化工作流时，LLMs可以作为高级语义优化器，弥合应用目标与低级系统控制之间的差距，指向存储系统更加自适应、自主和与动态工作负载需求对齐的未来。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [252] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: 提出了VeriPPA框架，使用LLMs进行芯片设计的PPA优化和Verilog代码生成，通过两阶段方法显著提升了代码正确性和性能优化效果。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型在芯片设计领域的潜力，特别是针对功耗-性能-面积(PPA)优化和准确Verilog代码生成的需求，以推动芯片设计自动化进程。

Method: 采用两阶段框架：第一阶段提升Verilog代码的功能和语法正确性，第二阶段优化Verilog代码以满足电路设计的PPA约束。

Result: 在RTLLM数据集上达到81.37%语法正确率和62.06%功能正确率；在VerilogEval数据集上达到99.56%语法正确率和43.79%功能正确率，均超越现有最佳方法。

Conclusion: LLMs在复杂技术领域具有巨大潜力，为芯片设计自动化带来了令人鼓舞的发展前景。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [253] [NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme](https://arxiv.org/abs/2510.15904)
*Subhradip Chakraborty,Ankur Singh,Xuming Chen,Gourav Datta,Akhilesh R. Jaiswal*

Main category: cs.AR

TL;DR: 提出了一种将RRAM集成到6T-SRAM单元的NVM-in-Cache架构，形成6T-2R混合单元，支持存内计算，在保持缓存数据的同时实现高能效并行计算。


<details>
  <summary>Details</summary>
Motivation: 随着DNN工作负载快速增长，大容量片上SRAM需求激增，SRAM阵列占据了芯片面积的很大比例。需要解决存储密度和计算效率的双重挑战。

Method: 将电阻式RAM器件集成到传统6T-SRAM单元中，形成紧凑的6T-2R位单元。利用6T-2R结构固有特性实现存内计算模式，在缓存电源线上直接执行大规模并行乘累加操作。

Result: 在GlobalFoundries 22nm FDSOI技术中，电路和阵列级仿真显示：吞吐量达0.4 TOPS，能效达491.78 TOPS/W。在128行并行操作下，Resnet-18网络在CIFAR-10分类任务中达到91.27%准确率。

Conclusion: NVM-in-Cache方法通过重新利用现有6T SRAM缓存架构，为下一代AI加速器和通用处理器提供了可扩展、高能效的计算解决方案。

Abstract: The rapid growth of deep neural network (DNN) workloads has significantly
increased the demand for large-capacity on-chip SRAM in machine learning (ML)
applications, with SRAM arrays now occupying a substantial fraction of the
total die area. To address the dual challenges of storage density and
computation efficiency, this paper proposes an NVM-in-Cache architecture that
integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell,
forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory
(PIM) mode, which performs massively parallel multiply-and-accumulate (MAC)
operations directly on cache power lines while preserving stored cache data. By
exploiting the intrinsic properties of the 6T-2R structure, the architecture
achieves additional storage capability, high computational throughput without
any bit-cell area overhead. Circuit- and array-level simulations in
GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design
achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel
operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18
neural network, achieving an accuracy of 91.27%. These results highlight the
potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient
computing method by re-purposing existing 6T SRAM cache architecture for
next-generation AI accelerators and general purpose processors.

</details>


### [254] [FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures](https://arxiv.org/abs/2510.15906)
*Yunsheng Bai,Ghaith Bany Hamad,Chia-Tung Ho,Syed Suhaib,Haoxing Ren*

Main category: cs.AR

TL;DR: FVDebug是一个自动化形式验证失败根因分析的系统，通过结合波形、RTL代码和设计规范，将失败跟踪转化为可操作的见解，并提供具体的RTL修复方案。


<details>
  <summary>Details</summary>
Motivation: 形式验证失败调试是现代硬件设计流程中最耗时的瓶颈之一，工程师需要手动分析复杂的多周期反例，现有解决方案无法处理设计意图与实现逻辑之间的复杂交互。

Method: 采用新颖的流水线方法：1) 因果图合成将失败跟踪构建为有向无环图；2) 图扫描器使用批量LLM分析和正反提示来识别可疑节点；3) 洞察漫游器利用代理叙事探索生成高级因果解释；4) 修复生成器提供具体RTL修复。

Result: 在开放基准测试中，FVDebug实现了高假设质量和强Pass@k修复率，并在两个专有的生产级形式验证反例上报告了结果，证明了从学术基准到工业设计的适用性。

Conclusion: FVDebug通过智能自动化根因分析，显著提升了形式验证失败调试的效率，能够处理从学术基准到工业设计的复杂验证问题。

Abstract: Debugging formal verification (FV) failures represents one of the most
time-consuming bottlenecks in modern hardware design workflows. When properties
fail, engineers must manually trace through complex counter-examples spanning
multiple cycles, analyze waveforms, and cross-reference design specifications
to identify root causes - a process that can consume hours or days per bug.
Existing solutions are largely limited to manual waveform viewers or simple
automated tools that cannot reason about the complex interplay between design
intent and implementation logic. We present FVDebug, an intelligent system that
automates root-cause analysis by combining multiple data sources - waveforms,
RTL code, design specifications - to transform failure traces into actionable
insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis
that structures failure traces into directed acyclic graphs, (2) Graph Scanner
using batched Large Language Model (LLM) analysis with for-and-against
prompting to identify suspicious nodes, and (3) Insight Rover leveraging
agentic narrative exploration to generate high-level causal explanations.
FVDebug further provides concrete RTL fixes through its Fix Generator.
Evaluated on open benchmarks, FVDebug attains high hypothesis quality and
strong Pass@k fix rates. We further report results on two proprietary,
production-scale FV counterexamples. These results demonstrate FVDebug's
applicability from academic benchmarks to industrial designs.

</details>


### [255] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: 提出了一种基于解析延迟公式的符号时序分析方法，能够计算电路内部信号转换时间的闭式解析表达式，支持无仿真的时序分析和灵敏度分析。


<details>
  <summary>Details</summary>
Motivation: 传统时序分析通常依赖数值仿真，无法提供解析依赖关系。本文旨在开发一种能够解析研究时序特性对输入信号和门参数依赖关系的方法。

Method: 基于Ferdowsi等人开发的2输入NOR、NAND和Muller-C门的解析延迟公式，在固定信号转换顺序下，计算所有内部信号转换时间的闭式解析表达式。

Result: 实现了基于SageMath计算机代数系统的框架，并在c17 slack基准电路的NOR门版本上进行了验证，能够进行符号时序分析和灵敏度分析。

Conclusion: 该方法为数字集成电路的符号时序分析提供了新途径，能够解析研究时序特性对电路参数的依赖关系，具有重要的理论和应用价值。

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [256] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: Belenos对生物力学有限元模拟进行工作负载分析，发现小规模工作负载存在前端停顿，大规模工作负载存在严重后端瓶颈，通过gem5敏感性研究确定了领域专用加速器的最优硬件配置。


<details>
  <summary>Details</summary>
Motivation: 当前生物力学有限元模拟在硬件和软件架构上存在效率限制，特别是在材料参数识别等迭代任务中，往往需要在精度和可处理性之间权衡。可重构硬件如FPGA提供了领域专用加速的潜力，但在生物力学中的应用尚未充分探索。

Method: 使用FEBio模拟器进行生物力学有限元模拟的工作负载表征，结合gem5敏感性研究和VTune性能分析，识别性能瓶颈和最优硬件配置。

Result: VTune分析显示小规模工作负载前端停顿约13.1%，大规模工作负载后端瓶颈占59.9%-82.2%。gem5敏感性研究发现不合理的流水线、内存或分支预测器设置会导致性能下降达37.1%。

Conclusion: 生物力学模拟工作负载需要架构感知的协同设计，以有效支持其计算需求，领域专用加速器可以显著提升性能。

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [257] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: SoCks是一个灵活可扩展的构建框架，通过将SoC镜像划分为高级单元（块）来降低复杂性，实现独立构建和最小化依赖，使构建速度比现有工具快3倍。


<details>
  <summary>Details</summary>
Motivation: 现代异构SoC设备集成先进组件，带来强大能力的同时也引入显著复杂性。开发工具日益复杂但缺乏足够支持，导致学习曲线陡峭和故障排除困难。

Method: 引入SoCks框架，将SoC镜像划分为称为块的高级单元，以封装方式独立构建每个固件和软件块，通过标准化接口实现块间信息交换，最小化依赖关系。

Result: 测量结果表明，SoCks可以比现有工具快3倍构建完整的SoC镜像，同时简化现有块实现的复用和版本间无缝替换。

Conclusion: SoCks通过模块化方法和标准化接口有效降低了SoC开发的复杂性，支持去中心化和部分自动化的开发流程，显著提升了构建效率和开发体验。

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [258] [VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts](https://arxiv.org/abs/2510.15914)
*Jiayu Zhao,Song Chen*

Main category: cs.AR

TL;DR: VeriGRAG框架通过图神经网络提取Verilog代码的结构图嵌入，结合多模态检索器选择相关嵌入，生成结构感知的软提示，显著提升LLM生成Verilog代码的正确性。


<details>
  <summary>Details</summary>
Motivation: Verilog代码编码了硬件电路的结构信息，但现有方法未能有效利用这些结构信息来增强LLM生成代码的功能和语法正确性。

Method: 使用GNN提取Verilog结构图嵌入，多模态检索器选择相关嵌入，VeriFormer模块对齐代码模态生成结构感知软提示。

Result: 在VerilogEval和RTLLM基准测试中达到最先进或更优性能，显著提升Verilog代码生成的正确性。

Conclusion: VeriGRAG通过有效利用Verilog代码的结构信息，成功提升了LLM生成硬件描述语言的正确性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
generating Verilog code from natural language descriptions. However, Verilog
code inherently encodes structural information of hardware circuits.
Effectively leveraging this structural information to enhance the functional
and syntactic correctness of LLM-generated Verilog code remains a significant
challenge. To address this challenge, we propose VeriGRAG , a novel framework
that extracts structural graph embeddings from Verilog code using graph neural
networks (GNNs). A multimodal retriever then selects the graph embeddings most
relevant to the given generation task, which are aligned with the code modality
through the VeriFormer module to generate structure-aware soft prompts. Our
experiments demonstrate that VeriGRAG substantially improves the correctness of
Verilog code generation, achieving state-of-the-art or superior performance
across both VerilogEval and RTLLM benchmarks.

</details>


### [259] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe是首个基于查表的1.58位三元LLM加速器，专为低功耗边缘FPGA设计，支持预填充和自回归解码，在5W功耗下实现高达25 tokens/s的解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和嵌入式系统的普及，在边缘平台部署大型语言模型成为迫切需求，但受限于高计算内存需求、有限片上资源和预填充阶段的长延迟。

Method: 采用表查找三元矩阵乘法引擎、细粒度URAM权重缓冲管理、流式数据流架构、反向重排序预填充注意力机制和资源高效解码注意力等创新技术。

Result: 在5W功耗预算下，实现25 tokens/s解码吞吐量，64-128 token提示的首次token生成时间为0.45-0.96秒。

Conclusion: TeLLMe显著提升了边缘FPGA上LLM推理的能效，为边缘设备部署LLM提供了可行的解决方案。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [260] [Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales](https://arxiv.org/abs/2510.15930)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 提出可配置卷积块库和方法论框架，用于优化FPGA上的CNN实现，通过资源预测模型帮助在硬件约束下部署卷积神经网络。


<details>
  <summary>Details</summary>
Motivation: FPGA实现CNN具有低延迟、高能效和灵活性优势，但开发复杂且设计周期长，难以在严格约束下进行资源优化和网络配置探索。

Method: 设计可配置卷积块库，并开发预测FPGA资源利用率的数学模型框架，通过参数相关性分析和误差指标验证方法。

Result: 设计的卷积块能够适应硬件约束，模型能准确预测资源消耗，为FPGA选择和CNN优化部署提供有用工具。

Conclusion: 该方法有效解决了FPGA上CNN实现的复杂性问题，支持在资源约束下进行快速网络配置探索和优化部署。

Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate
arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower
latency, greater power efficiency and greater flexibility. However, this
development remains complex due to the hardware knowledge required and the long
synthesis, placement and routing stages, which slow down design cycles and
prevent rapid exploration of network configurations, making resource
optimisation under severe constraints particularly challenging. This paper
proposes a library of configurable convolution Blocks designed to optimize FPGA
implementation and adapt to available resources. It also presents a
methodological framework for developing mathematical models that predict FPGA
resources utilization. The approach is validated by analyzing the correlation
between the parameters, followed by error metrics. The results show that the
designed blocks enable adaptation of convolution layers to hardware
constraints, and that the models accurately predict resource consumption,
providing a useful tool for FPGA selection and optimized CNN deployment.

</details>


### [261] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 提出Kelle软件硬件协同设计解决方案，在基于eDRAM的边缘设备上部署LLM，通过细粒度内存管理实现3.9倍加速和4.5倍能耗节省


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行LLM可降低延迟、提高实时处理能力并增强隐私保护，但KV缓存的内存占用和数据访问成本限制了部署

Method: 使用eDRAM作为主要存储，结合细粒度内存驱逐、重计算和刷新控制算法的软件硬件协同设计

Result: Kelle加速器相比现有基线解决方案实现3.9倍加速和4.5倍能耗节省

Conclusion: Kelle方案有效解决了边缘设备上LLM部署的KV缓存内存瓶颈问题

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [262] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS项目旨在设计和评估非常规硬件加速器（光电、易失/非易失性内存计算、神经形态）以解决AI的功耗、效率和可扩展性瓶颈，特别关注国防应用。本文介绍了该项目的系统架构、软件栈和仿真工具。


<details>
  <summary>Details</summary>
Motivation: 解决AI在功耗、效率和可扩展性方面的瓶颈，特别是针对国防应用场景如自动驾驶车辆、监控无人机、海上和太空平台的需求。

Method: 开发系统架构和软件栈来集成和支持光电、易失/非易失性内存计算、神经形态等非常规硬件加速器，并构建仿真软件用于早期原型设计。

Result: 提出了ARCHYTAS项目的整体架构方案，包括硬件加速器集成框架和相应的软件开发环境。

Conclusion: ARCHYTAS项目通过创新的硬件加速器设计和相应的软件生态系统，有望显著提升AI系统在国防应用中的性能和能效表现。

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [263] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: 开发适用于孟加拉国等发展中国家非车道化、异质交通的智能交通信号系统，使用RTSP视频流、Raspberry Pi处理和YOLO目标检测，结合NSGA-II多目标优化算法优化信号配时。


<details>
  <summary>Details</summary>
Motivation: 发展中国家城市如达卡面临严重交通拥堵，现有智能交通信号技术主要针对发达国家的结构化交通，无法适应非车道化、异质交通环境，需要开发适合当地交通特点的解决方案。

Method: 使用RTSP视频流采集交通数据，在Raspberry Pi 4B上运行基于YOLO的目标检测模型（使用NHT-1071数据集训练），结合NSGA-II多目标优化算法生成优化的信号配时方案。

Result: 在达卡Palashi的五路交叉口进行测试，证明该系统能显著改善交通管理，减少等待时间并提高车辆通行效率。

Conclusion: 该测试平台为具有复杂交通动态的发展中国家地区开发更符合实际情境的智能交通信号解决方案铺平了道路。

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [264] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: smaRTLy是一种针对RTL逻辑综合中多路复用器的优化技术，通过移除冗余多路复用器树并重构剩余结构，显著减少门数量。


<details>
  <summary>Details</summary>
Motivation: 传统工具如Yosys通过遍历多路复用器树并监控控制端口值来优化，但未能充分利用信号间的内在逻辑关系或结构优化潜力。

Method: 开发创新策略来移除冗余多路复用器树并重构剩余结构，采用逻辑推理和结构重建技术。

Result: 在IWLS-2005和RISC-V基准测试中，相比Yosys额外减少8.95%的AIG面积；在工业级百万门规模基准测试中，比Yosys多移除47.2%的AIG面积。

Conclusion: smaRTLy的逻辑推理和结构重建技术能有效增强RTL优化过程，实现更高效的硬件设计。

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>
