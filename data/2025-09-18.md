<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.LG](#cs.LG) [Total: 54]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A User-centric Kubernetes-based Architecture for Green Cloud Computing](https://arxiv.org/abs/2509.13325)
*Matteo Zanotto,Leonardo Vicentini,Redi Vreto,Francesco Lumpp,Diego Braga,Sandro Fiore*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的用户中心化绿色云计算架构，通过碳强度预测和绿色能源可用性调度工作负载，实现最多13%的碳排放减少


<details>
  <summary>Details</summary>
Motivation: 数据中心规模增长导致电力消耗和CO2排放增加，云提供商虽接近最优能效但缺乏精确的可持续性报告，需要在用户侧进一步改进

Method: 实现碳强度预测器，利用区域和时间变化调度工作负载，基于绿色能源可用性最小化排放，使用Kubernetes架构

Result: 在严格资源限制场景下，相比轮询调度基线可实现最多13%的碳排放减少

Conclusion: 用户中心的绿色云计算架构能有效利用时空变化减少碳排放，为云计算的可持续发展提供可行方案

Abstract: To meet the increasing demand for cloud computing services, the scale and
number of data centers keeps increasing worldwide. This growth comes at the
cost of increased electricity consumption, which directly correlates to CO2
emissions, the main driver of climate change. As such, researching ways to
reduce cloud computing emissions is more relevant than ever. However, although
cloud providers are reportedly already working near optimal power efficiency,
they fail in providing precise sustainability reporting. This calls for further
improvements on the cloud computing consumer's side. To this end, in this paper
we propose a user-centric, Kubernetes-based architecture for green cloud
computing. We implement a carbon intensity forecaster and we use it to schedule
workloads based on the availability of green energy, exploiting both regional
and temporal variations to minimize emissions. We evaluate our system using
real-world traces of cloud workloads execution comparing the achieved carbon
emission savings against a baseline round-robin scheduler. Our findings
indicate that our system can achieve up to a 13% reduction in emissions in a
strict scenario with heavy limitations on the available resources.

</details>


### [2] [Testing and benchmarking emerging supercomputers via the MFC flow solver](https://arxiv.org/abs/2509.13575)
*Benjamin Wilfong,Anand Radhakrishnan,Henry A. Le Berre,Tanush Prathi,Stephen Abbott,Spencer H. Bryngelson*

Main category: cs.DC

TL;DR: MFC是一个计算流体动力学代码，配有自动化工具链，用于评估不同硬件和编译器组合的性能和正确性，已在多代GPU和CPU架构上进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 部署新超级计算机需要通过应用程序代码进行测试和评估，需要便携、用户友好的工具来简化这一过程。

Method: 使用Multicomponent Flow Code (MFC) CFD代码及其自动化工具链，包括输入生成、编译、批处理作业提交、回归测试和基准测试功能。

Result: 测试了五代NVIDIA GPU、三代AMD GPU和各种CPU架构，使用Intel、Cray、NVIDIA、AMD和GNU编译器，发现了编译器错误和回归问题，已基准测试约50个计算设备和5个旗舰超级计算机。

Conclusion: MFC工具链能够有效评估编译器-硬件组合的正确性和性能，即使对于软件工程经验有限的用户也适用，为超级计算机部署提供了重要的测试和评估手段。

Abstract: Deploying new supercomputers requires testing and evaluation via application
codes. Portable, user-friendly tools enable evaluation, and the Multicomponent
Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this
need. MFC is adorned with a toolchain that automates input generation,
compilation, batch job submission, regression testing, and benchmarking. The
toolchain design enables users to evaluate compiler-hardware combinations for
correctness and performance with limited software engineering experience. As
with other PDE solvers, wall time per spatially discretized grid point serves
as a figure of merit. We present MFC benchmarking results for five generations
of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,
utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have
revealed compiler bugs and regressions on recent machines such as Frontier and
El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship
supercomputers.

</details>


### [3] [Modeling the Carbon Footprint of HPC: The Top 500 and EasyC](https://arxiv.org/abs/2509.13583)
*Varsha Rao,Andrew A. Chien*

Main category: cs.DC

TL;DR: 本文首次对Top 500超级计算机系统进行碳足迹评估，使用EasyC工具建模了391个系统的运营碳排放和283个系统的隐含碳排放，填补了HPC领域碳报告的空白。


<details>
  <summary>Details</summary>
Motivation: HPC系统缺乏统一的碳排放报告机制，即使最大的HPC站点也不进行GHG协议报告，需要开发可行的方法来评估HPC系统的碳足迹。

Method: 利用Top500.org公开数据和EasyC工具，通过有限数据建模碳排放，并利用插值法估算Top 500系统的碳足迹，数据覆盖率可提升至运营排放98%、隐含排放80.8%。

Result: Top 500 HPC系统的运营碳排放为13.937亿吨CO2e（1年），隐含碳排放为18.818亿吨CO2e，并预测了到2030年的增长趋势。

Conclusion: EasyC工具能够用少量数据指标有效建模HPC系统碳足迹，为HPC行业的碳报告提供了可行解决方案，填补了该领域的空白。

Abstract: Climate change is a critical concern for HPC systems, but GHG protocol
carbon-emission accounting methodologies are difficult for a single system, and
effectively infeasible for a collection of systems. As a result, there is no
HPC-wide carbon reporting, and even the largest HPC sites do not do GHG
protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The
key challenge lies in modeling the carbon footprint with limited data
availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able
to model the operational carbon of 391 HPC systems and the embodied carbon of
283 HPC systems. We further show how this coverage can be enhanced by
exploiting additional public information. With improved coverage, then
interpolation is used to produce the first carbon footprint estimates of the
Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1
Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top
500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few
data metrics. We explore availability of data and enhancement, showing that
coverage can be increased to 98% of Top 500 systems for operational and 80.8%
of the systems for embodied emissions.

</details>


### [4] [GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach](https://arxiv.org/abs/2509.13703)
*Sriram Srinivasan,Hamdan Alabsi,Rand Obeidat,Nithisha Ponnala,Azene Zenebe*

Main category: cs.DC

TL;DR: 开发了一门关于GPU架构和编程的专业课程，通过AWS云平台进行实践教学，显著提升了学生的技术能力和问题解决能力


<details>
  <summary>Details</summary>
Motivation: 为了满足现代计算密集型领域的需求，将并行计算整合到STEM教育中，为学生提供GPU编程和AI代理开发的实践技能

Method: 设计并实施面向本科和研究生GPU课程，从GPU/CPU硬件基础开始，逐步教授RAG开发和GPU优化，使用AWS云GPU实例进行实践操作，通过评估、课程评价和匿名调查来评估学习成果

Result: AWS作为经济有效的GPU编程平台，体验式学习显著提升了技术熟练度和参与度，课程通过TensorBoard和HPC分析器等工具增强了学生的问题解决和批判性思维能力

Conclusion: 研究强调了将并行计算整合到STEM教育的教学价值，主张在STEM课程中更广泛地采用类似选修课，以培养学生应对现代计算密集型领域需求的能力

Abstract: We present the design, implementation, and comprehensive evaluation of a
specialized course on GPU architecture, GPU programming, and how these are used
for developing AI agents. This course is offered to undergraduate and graduate
students during Fall 2024 and Spring 2025. The course began with foundational
concepts in GPU/CPU hardware and parallel computing and progressed to develop
RAG and optimizing them using GPUs. Students gained experience provisioning and
configuring cloud-based GPU instances, implementing parallel algorithms, and
deploying scalable AI solutions. We evaluated learning outcomes through
assessments, course evaluations, and anonymous surveys. The results reveal that
(1) AWS served as an effective and economical platform for practical GPU
programming, (2) experiential learning significantly enhanced technical
proficiency and engagement, and (3) the course strengthened students'
problem-solving and critical thinking skills through tools such as TensorBoard
and HPC profilers, which exposed performance bottlenecks and scaling issues.
Our findings underscore the pedagogical value of integrating parallel computing
into STEM education. We advocate for broader adoption of similar electives
across STEM curricula to prepare students for the demands of modern,
compute-intensive fields.

</details>


### [5] [LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology](https://arxiv.org/abs/2509.13978)
*Renan Souza,Timothy Poteet,Brian Etz,Daniel Rosendo,Amal Gueroudji,Woong Shin,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 提出了一种利用交互式大语言模型代理进行科学工作流数据运行时分析的方法，包括评估方法学、参考架构和开源实现，解决了大规模溯源数据复杂难分析的问题


<details>
  <summary>Details</summary>
Motivation: 现代科学发现依赖跨边缘、云和高性能计算的工作流，需要深入数据分析来验证假设、检测异常和确保可重现性，但现有溯源技术在规模扩大时变得复杂难分析，传统方法限制数据交互

Method: 采用轻量级元数据驱动设计，将自然语言转换为结构化溯源查询，使用模块化设计、提示调优和检索增强生成(RAG)技术，支持多种LLM模型(LLaMA、GPT、Gemini、Claude)

Result: 评估显示该方法能够生成准确且富有洞察力的响应，超越了记录的溯源数据范围，在真实化学工作流和多样化查询类别中表现良好

Conclusion: 交互式LLM代理为大规模科学工作流数据分析提供了有效解决方案，通过自然语言接口显著提升了数据交互和分析能力

Abstract: Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs](https://arxiv.org/abs/2509.13557)
*Zesong Jiang,Yuqi Sun,Qing Zhong,Mahathi Krishna,Deepak Patil,Cheng Tan,Sriram Krishnamoorthy,Jeff Zhang*

Main category: cs.AR

TL;DR: MACO是一个基于多智能体大语言模型的开源框架，用于粗粒度可重构阵列(CGRA)的硬件/软件协同设计，通过LLM推理自动生成和优化CGRA架构，显著减少人工设计工作量。


<details>
  <summary>Details</summary>
Motivation: CGRA设计面临设计空间巨大、架构参数独立、人工设计耗时等挑战，而大语言模型的快速发展为自动化这一过程提供了新的机遇。

Method: 采用多智能体LLM框架，通过四个阶段进行HW/SW协同设计：硬件/软件协同设计、设计错误修正、最佳设计选择、评估与反馈，并引入LLM自学习机制来选择最优CGRA。

Result: 实验结果表明，MACO能够高效生成高质量的CGRA架构，在性能、功耗和面积方面优于最先进的基于LLM的方法和人工CGRA设计。

Conclusion: 该框架展示了多智能体LLM在真实世界CGRA设计中的潜力，显著减少了人工设计工作量，为自动化硬件设计提供了有效解决方案。

Abstract: Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing
architecture that can deliver high-performance, energy-efficient acceleration
across diverse domains. By supporting reconfiguration at the functional unit
level, CGRAs efficiently adapt to varying computational patterns and optimize
resource utilization. However, designing CGRAs is highly challenging due to the
vast design space, independent architectural parameters, and the time-consuming
nature of manual design. Fortunately, the rapid advancement of large language
models (LLMs) presents new opportunities to automate this process.
  In this work, we propose MACO -- an open-source multi-agent LLM-based
framework for Hardware/Software (HW/SW) co-design of CGRAs. The framework
employs LLM reasoning to generate CGRAs across four stages: HW/SW co-design,
Design error correction, Best design selection, and Evaluation & Feedback.
Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent
reasoning and feedback to achieve higher PPA (that is, power, performance, and
area) design points for a given domain. In addition, we introduce an LLM
self-learning mechanism that employs LLM-driven decision making to select the
optimal CGRA to accelerate the design process.
  We evaluate the framework with state-of-the-art LLM-based methods and manual
CGRA design, in terms of performance, power consumption, and area. Experimental
results show that MACO efficiently generates high-quality CGRA architectures,
significantly reducing manual design effort and demonstrating the potential of
our framework for real-world CGRA design.

</details>


### [7] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP是一种软硬件协同设计方法，通过编译器分析代码温度（热/冷）并提供给硬件，优化指令缓存替换策略，减少热代码的驱逐率，在移动CPU上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代移动CPU软件由于复杂的运行时行为导致指令重用距离大，传统指令缓存替换策略不足。移动代码常出现CPU前端大量停顿，导致资源饥饿。应用复杂性和代码占用增长速度快于片上内存，传统硬件中心方法不够用。

Method: 提出TRRIP软硬件协同设计：编译器分析、分类和转换基于温度的代码，通过操作系统接口提供代码温度信息给硬件。轻量级硬件扩展利用代码温度属性优化指令缓存替换策略。

Result: TRRIP可将L2指令MPKI降低26.5%，在已使用PGO优化的移动代码上实现3.9%的几何平均加速比。

Conclusion: TRRIP是一种实用且可在真实移动系统中采用的方法，通过软硬件协同优化指令缓存管理，有效提升移动CPU性能。

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


### [8] [StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs](https://arxiv.org/abs/2509.13694)
*Hanchen Ye,Deming Chen*

Main category: cs.AR

TL;DR: StreamTensor是一个编译器框架，通过引入迭代张量类型系统自动构建和优化基于流的数据流加速器，在FPGA上实现比现有FPGA LLM加速器和GPU更低的延迟和更高的能效


<details>
  <summary>Details</summary>
Motivation: 解决深度学习工作负载在数据流架构上执行时的内存瓶颈问题，现有方法在处理内核间相关性、外部内存访问管理和缓冲区优化方面存在困难

Method: 提出StreamTensor编译器框架，引入新颖的迭代张量类型系统来显式编码流布局，通过系统探索张量分块、内核融合和资源分配三个层次设计空间

Result: 在FPGA上的大型语言模型评估中，相比最先进的FPGA LLM加速器延迟降低0.76倍，相比GPU延迟降低0.64倍，能效比GPU提高1.99倍

Conclusion: StreamTensor为可扩展的基于数据流的深度学习加速提供了一种有前景的方法

Abstract: Efficient execution of deep learning workloads on dataflow architectures is
crucial for overcoming memory bottlenecks and maximizing performance. While
streaming intermediate results between computation kernels can significantly
improve efficiency, existing approaches struggle with inter-kernel
correlations, external memory access management, and buffer optimization. In
this work, we propose StreamTensor, a compiler framework that automatically
constructs and optimizes stream-based dataflow accelerators. StreamTensor
introduces a novel iterative tensor type system to explicitly encode stream
layouts, enabling seamless kernel fusion, buffer allocation, and memory
optimization. By systematically exploring three hierarchical design spaces,
including tensor tiling, kernel fusion, and resource allocation, StreamTensor
balances computational intensity, memory efficiency, and data streaming to
maximize performance. Based on FPGA evaluations on Large Language Models (LLM),
StreamTensor achieves up to 0.76x and 0.64x lower latency compared to the
state-of-the-art FPGA LLM accelerators and GPUs, and up to 1.99x higher energy
efficiency compared to GPUs, making it a promising approach for scalable
dataflow-based deep learning acceleration.

</details>


### [9] [CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration](https://arxiv.org/abs/2509.13710)
*Hongyi Li,Songchen Ma,Huanyu Qu,Weihao Zhang,Jia Chen,Junfeng Lin,Fengbin Tu,Rong Zhao*

Main category: cs.AR

TL;DR: CompAir是一种新型混合PIM架构，结合DRAM-PIM和SRAM-PIM，通过混合键合技术实现高效线性计算和多粒度数据通路，并开发了具有嵌入式ALU的NoC来在数据移动时执行非线性操作，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和能耗需求巨大，内存墙问题成为关键瓶颈。现有PIM架构难以平衡LLM动态内存计算模式和算子多样性的灵活性、性能和成本效率需求。

Method: 提出CompAir混合PIM架构：1）集成DRAM-PIM和SRAM-PIM的混合键合技术；2）开发CompAir-NoC，在数据移动时执行非线性操作；3）设计分层指令集架构确保灵活性和可编程性。

Result: 相比最先进的全PIM架构，预填充性能提升1.83-7.98倍，解码性能提升1.95-6.28倍；相比混合A100和HBM-PIM系统，能耗降低3.52倍且吞吐量相当。

Conclusion: CompAir是首个系统探索混合DRAM-PIM和SRAM-PIM架构并具备网络内计算能力的工作，为LLM提供了高效解决方案，有效解决了内存墙问题并提升了能效。

Abstract: The rapid advancement of Large Language Models (LLMs) has revolutionized
various aspects of human life, yet their immense computational and energy
demands pose significant challenges for efficient inference. The memory wall,
the growing processor-memory speed disparity, remains a critical bottleneck for
LLM. Process-In-Memory (PIM) architectures overcome limitations by co-locating
compute units with memory, leveraging 5-20$\times$ higher internal bandwidth
and enabling greater energy efficiency than GPUs. However, existing PIMs
struggle to balance flexibility, performance, and cost-efficiency for LLMs'
dynamic memory-compute patterns and operator diversity. DRAM-PIM suffers from
inter-bank communication overhead despite its vector parallelism. SRAM-PIM
offers sub-10ns latency for matrix operation but is constrained by limited
capacity. This work introduces CompAir, a novel PIM architecture that
integrates DRAM-PIM and SRAM-PIM with hybrid bonding, enabling efficient linear
computations while unlocking multi-granularity data pathways. We further
develop CompAir-NoC, an advanced network-on-chip with an embedded arithmetic
logic unit that performs non-linear operations during data movement,
simultaneously reducing communication overhead and area cost. Finally, we
develop a hierarchical Instruction Set Architecture that ensures both
flexibility and programmability of the hybrid PIM. Experimental results
demonstrate that CompAir achieves 1.83-7.98$\times$ prefill and
1.95-6.28$\times$ decode improvement over the current state-of-the-art fully
PIM architecture. Compared to the hybrid A100 and HBM-PIM system, CompAir
achieves 3.52$\times$ energy consumption reduction with comparable throughput.
This work represents the first systematic exploration of hybrid DRAM-PIM and
SRAM-PIM architectures with in-network computation capabilities, offering a
high-efficiency solution for LLM.

</details>


### [10] [TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge](https://arxiv.org/abs/2509.13765)
*Zhirui Huang,Rui Ma,Shijie Cao,Ran Shu,Ian Wang,Ting Cao,Chixiao Chen,Yongqiang Xiong*

Main category: cs.AR

TL;DR: TENET是一个针对三元量化LLM推理的稀疏感知LUT中心架构，通过算法、计算和内存协同优化，在FPGA和ASIC平台上分别实现4.3倍和21.1倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 传统GPU平台无法充分利用三元量化的优势，缺乏对三元算术和内存专业化的原生支持，在低批量实时场景下利用率严重不足。

Method: 提出Sparse Ternary LUT (STL)核心优化三元混合精度GEMM，采用动态激活N:M稀疏性，设计基于LUT的64B:80B权重解压缩模块，构建异构可编程加速器。

Result: TENET-FPGA和TENET-ASIC相比A100 GPU分别实现4.3倍和21.1倍的能效提升，TENET-ASIC在端到端推理延迟上平均加速2.7倍。

Conclusion: TENET架构通过算法-计算-内存协同优化，有效解决了三元量化LLM推理在传统硬件上的效率问题，为实时推理部署提供了高效解决方案。

Abstract: Ternary quantization has emerged as a powerful technique for reducing both
computational and memory footprint of large language models (LLM), enabling
efficient real-time inference deployment without significantly compromising
model accuracy. Conventional LLM inference platforms (e.g GPUs) cannot
capitalize on its benefits, as they (i) lack native support for ternary
arithmetic and memory specialization and (ii) remain severely under-utilized in
low-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware
LUT-centric architecture that co-optimizes algorithm, compute, and memory for
ternary LLM inference. To maximize the efficiency of Ternary Linear layer,
TENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary
mixed-precision GEMM using a symmetric precompute lookup table. It also
features Dynamic Activation N:M Sparsity to exploit the sparsity within the
activation of each token. Additionally, we propose a LUT-based 64B:80B ternary
weight decompression module to fully exploit the memory efficiency of ternary
values. At the system level, we design a heterogeneous TENET accelerator with
full programmability that integrates STL cores with high-precision cores. An
associated Linear-Projection-aware Sparse Attention dataflow is introduced to
optimize memory access and hardware utilization. We implement TENET accelerator
prototype on both FPGA and ASIC platforms. Experiments across various model
sizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy
efficiency by 4.3$\times$ and 21.1$\times$, respectively, compared to the A100
GPU. Furthermore, TENET-ASIC achieves a 2.7$\times$ average speedup compared to
the A100 GPU in end-to-end inference latency.

</details>


### [11] [An RDMA-First Object Storage System with SmartNIC Offload](https://arxiv.org/abs/2509.13997)
*Yu Zhu,Aditya Dhakal,Pedro Bruel,Gourav Rattihalli,Yunming Xiao,Johann Lombardi,Dejan Milojicic*

Main category: cs.AR

TL;DR: ROS2是一个基于RDMA的对象存储系统，将DAOS客户端卸载到NVIDIA BlueField-3 SmartNIC上，通过分离控制平面和数据平面来优化AI训练中的I/O性能。


<details>
  <summary>Details</summary>
Motivation: AI训练和推理需要持续、细粒度的I/O操作，传统基于TCP的存储路径存在性能瓶颈，需要重新审视POSIX兼容的对象存储方案来支持GPU中心化流水线。

Method: 设计ROS2系统，采用RDMA优先的架构，将DAOS客户端卸载到SmartNIC，保持控制平面（gRPC）和数据平面（UCX/libfabric）分离，移除数据路径中的主机中介。

Result: RDMA在服务器级CPU上始终优于TCP，SmartNIC卸载的RDMA性能与主机相当，而TCP在SmartNIC上性能较差，证明RDMA对卸载部署的重要性。

Conclusion: RDMA优先、SmartNIC卸载的对象存储栈是现代LLM训练环境中扩展数据交付的实用基础，未来可集成GPU直接放置功能。

Abstract: AI training and inference impose sustained, fine-grain I/O that stresses
host-mediated, TCP-based storage paths. Motivated by kernel-bypass networking
and user-space storage stacks, we revisit POSIX-compatible object storage for
GPU-centric pipelines. We present ROS2, an RDMA-first object storage system
design that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while
leaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a
lightweight control plane (gRPC for namespace and capability exchange) from a
high-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host
mediation from the data path.
  Using FIO/DFS across local and remote configurations, we find that on
server-grade CPUs RDMA consistently outperforms TCP for both large sequential
and small random I/O. When the RDMA-driven DAOS client is offloaded to
BlueField-3, end-to-end performance is comparable to the host, demonstrating
that SmartNIC offload preserves RDMA efficiency while enabling DPU-resident
features such as multi-tenant isolation and inline services (e.g.,
encryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags
host performance, underscoring the importance of RDMA for offloaded
deployments.
  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded
object-storage stack is a practical foundation for scaling data delivery in
modern LLM training environments; integrating optional GPU-direct placement for
LLM tasks is left for future work.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Outperforming Dijkstra on Sparse Graphs: The Lightning Network Use Case](https://arxiv.org/abs/2509.13448)
*Danila Valko,Rohan Paranjpe,Jorge Marx Gómez*

Main category: cs.PF

TL;DR: BMSSP算法理论上比Dijkstra更快，但在实际闪电网络拓扑测试中性能提升有限，未达到理论预期


<details>
  <summary>Details</summary>
Motivation: 支付通道网络（如闪电网络）需要高效路由算法，虽然Dijkstra算法长期被认为是稀疏图上的最优选择，但新的BMSSP算法理论上具有更好的渐近复杂度

Method: 在Rust中实现BMSSP算法，使用真实的闪电网络拓扑数据进行测试，通过多次随机试验和统计检验比较BMSSP与Dijkstra的性能

Result: 当前BMSSP实现并未显著优于Dijkstra算法，速度提升比理论预测小，可能由于实现开销和常数因子影响

Conclusion: 研究首次提供了BMSSP加速闪电网络路由的实证证据，为未来PCN路径查找算法的优化提供了参考

Abstract: Efficient routing is critical for payment channel networks (PCNs) such as the
Lightning Network (LN), where most clients currently rely on Dijkstra-based
algorithms for payment pathfinding. While Dijkstra's algorithm has long been
regarded as optimal on sparse graphs, recent theoretical work challenges this
view. The new Bounded Multi-Source Shortest Path (BMSSP) algorithm by Duan et
al. theoretically achieves $O(m~log^{2/3}~n)$ runtime, which is asymptotically
faster than Dijkstra's $O(m + n~log~n)$ on sparse directed graphs. In this
paper, we implement BMSSP on Rust and compare its performance against
Dijkstra's using real LN topology data. Our evaluation, based on multiple
randomized trials and statistical tests, shows that current implementations of
BMSSP do not significantly outperform Dijkstra's in practice, and speedups are
smaller than what theory predicts, possibly due to implementation and constant
factor overheads. These results provide the first empirical evidence of BMSSP's
potential to accelerate LN routing and inform future optimizations of PCN
pathfinding algorithms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [13] [Oscillator Formulations of Many NP Problems](https://arxiv.org/abs/2509.13560)
*Wenxiao Cai,Zongru Li,Yu-Neng Wang,Sara Achour,Thomas H. Lee*

Main category: cs.ET

TL;DR: 本文提出了一种基于CMOS振荡器网络的NP问题优化方法，使用3态非对称加权振荡器优化器设计来优化一阶和多相Potts哈密顿量公式化的NP问题


<details>
  <summary>Details</summary>
Motivation: 高效优化NP问题在多个领域具有深远意义，CMOS振荡器网络已被证明能有效近似某些NP难问题，且计算复杂性理论保证任何NP问题都可归约到此类问题

Method: 使用一阶和多相Potts哈密顿量公式化多种NP问题，提出3态非对称加权振荡器优化器设计，基于现有CMOS设计知识构建算法

Result: 提出的算法为大规模NP问题优化提供了有前景的途径

Conclusion: CMOS振荡器网络方法为多项式时间内高效优化NP问题提供了可行的技术路线

Abstract: Efficiently optimizing Nondeterministic Polynomial time (NP) problems in
polynomial time has profound implications in many domains. CMOS oscillator
networks have been shown to be effective and efficient in approximating certain
NP-hard problems such as minimization of Potts Hamiltonian, and computational
complexity theory guarantees that any NP problem can be reduced to it. In this
paper, we formulate a variety of NP problems using first-order and multi-phase
Potts Hamiltonian. We also propose a 3-state asymmetrically weighted oscillator
optimizer design to optimize the problems. Building on existing knowledge in
CMOS design, our proposed algorithms offer a promising pathway for large-scale
optimization of NP problems.

</details>


### [14] [Analytical Modelling of the Transport in Analog Filamentary Conductive-Metal-Oxide/HfOx ReRAM Devices](https://arxiv.org/abs/2509.13964)
*Donato Francesco Falcone,Stephan Menzel,Tommaso Stecconi,Matteo Galetta,Antonio La Porta,Bert Jan Offrein,Valeria Bragaglia*

Main category: cs.ET

TL;DR: 提出了首个基于物理的TaOx/HfOx ReRAM器件电流传输和电阻切换分析模型，解释了陷阱间隧穿机制和缺陷密度调制过程。


<details>
  <summary>Details</summary>
Motivation: 尽管阻变存储器性能有所提升，但其电阻切换机制尚未完全理解，需要建立物理模型来深入理解并支持电路仿真。

Method: 通过求解3D有限元模型中的电热传输方程，分析局部温度和电场分布，建立基于陷阱间隧穿的电流传输模型和缺陷密度调制的电阻切换模型。

Result: 模型准确描述了模拟TaOx/HfOx ReRAM器件的实验切换特性，能够估算TaOx层中的缺陷迁移能量，并揭示了CMO层电热特性的作用。

Conclusion: 该分析模型增强了物理理解，提供了电路仿真所需的方程，为基于ReRAM的存内计算系统设计提供了重要理论基础。

Abstract: The recent co-optimization of memristive technologies and programming
algorithms enabled neural networks training with in-memory computing systems.
In this context, novel analog filamentary conductive-metal-oxide (CMO)/HfOx
redox-based resistive switching memory (ReRAM) represents a key technology.
Despite device performance enhancements reported in literature, the underlying
mechanism behind resistive switching is not fully understood. This work
presents the first physics-based analytical model of the current transport and
of the resistive switching in these devices. As a case study, analog TaOx/HfOx
ReRAM devices are considered. The current transport is explained by a
trap-to-trap tunneling process, and the resistive switching by a modulation of
the defect density within the sub-band of the TaOx that behaves as electric
field and temperature confinement layer. The local temperature and electric
field distributions are derived from the solution of the electric and heat
transport equations in a 3D finite element ReRAM model. The intermediate
resistive states are described as a gradual modulation of the TaOx defect
density, which results in a variation of its electrical conductivity. The
drift-dynamics of ions during the resistive switching is analytically
described, allowing the estimation of defect migration energies in the TaOx
layer. Moreover, the role of the electro-thermal properties of the CMO layer is
unveiled. The proposed analytical model accurately describes the experimental
switching characteristic of analog TaOx/HfOx ReRAM devices, increasing the
physical understanding and providing the equations necessary for circuit
simulations incorporating this technology.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: 提出了USPIL框架，将物理信息神经网络与守恒定律结合，统一建模捕食者-猎物系统的时空动力学，在精度和计算效率上显著优于传统数值方法。


<details>
  <summary>Details</summary>
Motivation: 生态系统的复杂多尺度动力学挑战传统建模方法，需要新方法来捕捉时间振荡和涌现的时空模式，同时遵守守恒原理。

Method: 使用物理信息神经网络(PINNs)和守恒定律结合的深度学习架构，通过自动微分强制执行物理约束，采用自适应损失加权平衡数据保真度和物理一致性。

Result: 在Lotka-Volterra系统中，1D时间动力学达到98.9%相关性，2D系统捕捉到复杂螺旋波，守恒定律遵守度在0.5%以内，推理速度比数值求解器快10-50倍。

Conclusion: USPIL为多尺度生态建模开辟了新途径，是生态预测、保护规划和理解生态系统恢复力的变革性工具，确立了物理信息深度学习作为强大且科学严谨的范式。

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [16] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 本文通过360次实验比较了8种优化器在能源效率方面的表现，发现AdamW和NAdam在环保和性能方面表现均衡，而SGD在复杂数据集上性能更优但碳排放更高


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型变得越来越复杂和计算密集，理解训练决策对环境的影响对于可持续AI发展变得至关重要

Method: 在三个基准数据集(MNIST, CIFAR-10, CIFAR-100)上使用8种流行优化器进行360次受控实验，使用CodeCarbon在Apple M1 Pro硬件上精确追踪能源消耗

Result: 发现训练速度、准确性和环境影响之间存在显著权衡，这些权衡因数据集和模型复杂度而异。AdamW和NAdam表现一致高效，而SGD在复杂数据集上性能更优但排放更高

Conclusion: 研究结果为从业者在机器学习工作流中平衡性能和可持续性提供了可行的见解

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [17] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS是一个10-800亿参数的像素级Swin扩散变换器，用于天气和气候预测，通过SWiPe并行化技术实现高效扩展，在Aurora超级计算机上达到10.21 ExaFLOPS性能，超越IFS ENS系统并保持90天季节尺度的稳定性。


<details>
  <summary>Details</summary>
Motivation: 生成式机器学习为理解复杂地球系统动力学提供了新机会。现有的扩散方法虽然解决了光谱偏差问题并改善了天气预报的集合校准，但在高分辨率下难以稳定扩展。

Method: 提出AERIS（1.3-80B参数像素级Swin扩散变换器）和SWiPe技术（将窗口并行化与序列和流水线并行化结合，在不增加通信成本或全局批次大小的情况下对基于窗口的变换器进行分片）。

Result: 在Aurora（10,080节点）上，AERIS在0.25° ERA5数据集上维持10.21 ExaFLOPS（混合精度）和11.21 ExaFLOPS峰值性能，弱扩展效率达95.5%，强扩展效率81.6%。超越IFS ENS并在90天季节尺度保持稳定。

Conclusion: 十亿参数扩散模型在天气和气候预测方面具有巨大潜力，AERIS展示了在高分辨率下稳定扩展的能力，为地球系统动力学建模开辟了新途径。

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [18] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet-Transolver框架，用于解决PET塑料瓶屈曲分析问题，能够同时预测节点位移场和时间相关的反作用力，在几何参数化设计上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的神经代理和算子网络方法在处理不同非参数几何域时泛化能力有限，而传统的有限元分析计算成本高昂，特别是在包装设计等需要快速评估的应用中。

Method: 采用混合DeepONet-Transolver框架，结合深度算子网络和变换求解器，在Abaqus非线性有限元模拟生成的254个独特设计数据上进行训练，处理两参数和四参数瓶型几何家族。

Result: 在四参数瓶型家族上，位移场的平均相对L²误差为2.5-13%，时间相关反作用力的误差约为2.4%。点误差分析显示绝对位移误差在10⁻⁴-10⁻³量级，最大差异局限于局部几何区域，模型能准确捕捉屈曲等关键物理现象。

Conclusion: 该框架展示了作为可扩展且计算高效的代理模型的潜力，特别适用于计算力学中的多任务预测和需要快速设计评估的应用场景。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [19] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis是一个并行保护框架，通过模型分割策略在联邦学习中实现隐私-效用-效率的灵活平衡控制。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中现有保护机制（如差分隐私和同态加密）在模型效用和计算效率之间强制刚性权衡的问题，缺乏灵活性阻碍了实际应用。

Method: 采用战略模型分割方案：对模型中不太关键的低范数部分应用轻量级差分隐私，其余部分使用同态加密保护，并通过分布式投票机制确保分割共识。

Result: 理论分析确认了在相同隐私保护下效率与效用之间的可调节性。实验结果表明通过调整超参数，该方法能够灵活地在模型精度和训练时间之间进行优先选择。

Conclusion: ParaAegis框架为联邦学习实践者提供了对隐私-效用-效率平衡的灵活控制，解决了现有保护机制的刚性权衡问题。

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [20] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 提出一种图正则化的分布式高斯混合模型学习方法，利用相似性图指导节点间参数共享，在异构小样本场景下优于集中式和本地训练方法


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中数据异构且样本有限的情况下，如何有效学习高斯混合模型而不需要传输原始数据的问题

Method: 利用提供的相似性图来指导节点间的参数共享，避免原始数据传输，实现邻居参数的灵活聚合

Result: 在异构、小样本场景下，该方法性能优于集中式训练和本地训练的GMM模型

Conclusion: 图正则化方法为分布式异构数据下的GMM学习提供了有效解决方案，通过图结构指导参数共享实现了更好的模型性能

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [21] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL是一种线性元学习算法，在保持可解释性的同时提高多个化学性质预测的准确性，通过共享相关任务间的模型参数来学习共同的功能流形


<details>
  <summary>Details</summary>
Motivation: 化学研究中高质量数据集有限，机器学习方法虽然提升了预测能力但增加了数据需求，同时需要满足可解释AI的需求，弥合预测准确性和人类可理解性之间的差距

Method: 采用元学习框架，识别相关任务间的共享模型参数（即使任务不共享数据），学习共同的功能流形作为新任务的更明智起点

Result: 性能比标准岭回归提升1.1-25倍，在不同数据集领域表现一致优于或匹配传统线性方法

Conclusion: LAMeL是化学性质预测中既准确又可解释的可靠工具，特别适用于需要准确性和可解释性都至关重要的场景

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [22] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: 提出WILF-Q方法解决无线联邦学习中的客户端选择问题，使用Q学习自适应学习Whittle指数来选择最优客户端，显著提升学习效率


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中客户端动态状态变化影响计算和通信效率，服务器无法直接观测这些状态，需要高效选择机制来减少达到目标精度所需时间

Method: 将客户端选择建模为多臂老虎机问题，提出WILF-Q方法：使用Q学习自适应学习和更新每个客户端的近似Whittle指数，选择指数最高的客户端

Result: 实验结果表明WILF-Q在学​习效率方面显著优于现有基线策略

Conclusion: WILF-Q不需要客户端状态转移或数据分布的显式知识，适用于实际联邦学习部署，提供了鲁棒高效的客户端选择方法

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [23] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: 研究发现GPT-4o mini存在"单模态瓶颈"安全架构缺陷，多模态推理被上下文无关的安全过滤器系统性地阻断，导致良性内容被错误拦截


<details>
  <summary>Details</summary>
Motivation: 随着大型多模态模型(LMMs)成为日常生活的重要组成部分，理解其安全架构对于AI对齐至关重要，需要系统分析全球部署模型在多模态仇恨言论检测任务中的表现

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段调查，分析模型的推理和失败模式，并对144个内容策略拒绝进行定量验证

Result: 实验识别出"单模态瓶颈"架构缺陷，50%的拒绝由视觉内容触发，50%由文本内容触发，安全系统脆弱，不仅阻止高风险图像，还阻止良性的常见meme格式

Conclusion: 这些发现揭示了最先进LMMs中能力与安全之间的根本张力，强调需要更集成、上下文感知的对齐策略，以确保AI系统能够安全有效地部署

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [24] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 提出基于语义嵌入和序列感知神经网络的自动化故障分析框架，通过处理EPICS控制系统实时事件日志来检测异常并识别系统故障前的关键事件序列


<details>
  <summary>Details</summary>
Motivation: 为了解决先进光源(ALS)等复杂系统中故障诊断的挑战，需要自动化工具来处理实时事件日志并快速识别可能导致系统故障的关键事件模式

Method: 将日志条目视为自然语言，使用语义嵌入技术转换为上下文向量表示，然后使用在正常操作数据上训练的序列感知神经网络为每个事件分配实时异常分数

Result: 该方法能够标记与基线行为的偏差，使操作员能够快速识别复杂系统故障前的关键事件序列

Conclusion: 该自动化故障分析框架为复杂控制系统提供了有效的实时异常检测和故障预警能力

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [25] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私的文本生成框架，通过聚合token级输出分布来生成高质量合成文本，在保护隐私的同时保持高实用性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在隐私泄露风险，攻击者可能从提示中提取敏感信息，需要开发既能生成高质量文本又能提供强隐私保证的方法

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token的输出分布，无需微调底层模型。还提出了混合私有和公共推理的简单操作来增强实用性

Result: 经验评估表明，该方法在上下文学习任务上优于先前的最先进方法，能够生成长且连贯的合成文本

Conclusion: 该方法为隐私保护文本生成提供了一个有前景的方向，在保持高实用性的同时提供强隐私保证

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [26] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: 提出DeepLogit模型，通过序列约束方法结合深度学习与离散选择模型，在保持参数可解释性的同时提升预测精度


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在规划和政策领域的应用受限于其黑盒特性，需要开发既能保持可解释性又能提高准确性的方法

Method: 采用序列约束方法：先估计线性参数的CNN模型（等价于多项logit模型），然后约束需要解释的参数值，引入高阶项或Transformer等先进架构

Result: 在真实世界的新加坡公交刷卡数据上验证，方法在保持选择参数可解释性的同时显著提高了模型准确性

Conclusion: 展示了理论驱动的离散选择模型与数据驱动的AI模型相结合的统一方法潜力，可在保持规划政策应用适用性的同时获得更准确的模型

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [27] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种结合数字孪生和零知识联邦学习的新型框架，用于优化无人机辅助联邦学习系统的能源效率、通信效率和安全性


<details>
  <summary>Details</summary>
Motivation: 解决无人机辅助联邦学习系统中存在的能源消耗过高、通信效率低下和安全漏洞等问题，确保系统的可靠运行

Method: 集成数字孪生技术实现实时系统监控和预测性维护，采用零知识证明增强安全性，引入动态分配策略优化无人机飞行路径、传输功率和处理速率，使用块坐标下降和凸优化技术

Result: 系统能耗相比传统联邦学习方法降低高达29.6%，学习性能、安全性和可扩展性均得到显著提升

Conclusion: 该框架为下一代无人机智能网络提供了一个有前景的解决方案，在能源效率、安全性和性能方面表现出色

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [28] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 将多模态生理信号（PPG、GSR、ACC）转换为2D图像矩阵，使用CNN进行压力检测的新方法，通过信号融合和图像化表示提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理多模态生理信号或依赖固定编码，无法有效捕捉信号间的时空依赖关系，需要一种能更好利用CNN优势的信号表示方法。

Method: 将PPG、GSR和ACC信号融合成结构化2D图像矩阵，采用多阶段训练管道系统性地重组信号格式，结合数据增强技术。

Result: 该方法显著提升了分类性能，改善了模型泛化能力和鲁棒性，为可穿戴设备提供更准确、个性化的实时健康监测。

Conclusion: 提出的图像化转换方法不仅适用于压力检测，可广泛应用于任何涉及多模态生理信号的领域，为健康监测技术开辟了新途径。

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [29] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个将交错图像-文本生成重新定义为工具使用问题的动态框架，通过强化学习训练LLM智能协调多种视觉工具，在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型存在"单一工具"瓶颈，仅限于合成图像生成，难以处理需要事实基础或程序化精度的任务，需要更灵活的框架来整合多种专业视觉工具。

Method: 设计了一个中央LLM/MLLM代理，通过强化学习框架协调在线图像搜索、扩散生成、代码执行和图像编辑等专业工具，采用结合规则逻辑和LLM评估的混合奖励系统。

Result: 在四个不同模型骨干上训练，在四个基准测试中实现了最先进的性能，大幅超越现有方法，并通过新颖的测试时缩放策略获得额外性能提升。

Conclusion: LLM-I框架成功解决了当前模型的局限性，通过工具使用范式实现了更灵活、准确的交错图像-文本生成，为多模态任务提供了新的解决方案。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [30] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了GenPAS框架，系统性地分析和优化生成式推荐中的数据增强策略，通过控制偏见的三个采样步骤来提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统中数据增强策略被简化处理，缺乏系统性理解，而不同增强策略会导致显著的性能差异

Method: 提出GenPAS框架，将数据增强建模为包含序列采样、目标采样和输入采样三个偏见控制步骤的随机采样过程

Result: 在基准和工业数据集上的实验表明，GenPAS在准确性、数据效率和参数效率方面均优于现有策略

Conclusion: GenPAS为生成式推荐中的训练数据构建提供了原则性指导，统一了广泛使用的增强策略并实现了对训练分布的灵活控制

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [31] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 提出CPT方法，通过多目标优化实现公平性与准确性的可控权衡，使用移动平均梯度稳定性和关键参数梯度剪枝来精确控制用户偏好的权衡比例。


<details>
  <summary>Details</summary>
Motivation: 现有工作只寻找单一"最优"解决方案，忽略了帕累托前沿上的多样性。需要根据用户偏好提供可控的权衡方案。

Method: 采用多目标优化(MOO)，使用移动平均梯度稳定公平性更新方向，并通过关键参数梯度剪枝来精确控制权衡。

Result: 在仇恨言论检测和职业分类任务上，CPT比基线方法获得更高质量的帕累托前沿解集，展现出更好的可控性并能精确跟随人工定义的参考向量。

Conclusion: CPT方法有效解决了公平性-准确性权衡的可控性问题，能够根据用户偏好生成高质量的多样化解决方案。

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [32] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM是一个基于辐射场的多域局部统计信道建模框架，通过物理感知的频率相关衰减模型和点云辅助环境增强方法，解决了传统方法在多细胞、多网格、多频段建模的局限性，显著提升了信道预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统局部统计信道建模方法局限于单细胞、单网格和单载波频率分析，无法捕捉复杂的跨域交互，限制了蜂窝网络优化的效果。

Method: 提出RF-LSCM框架，使用辐射场联合建模大尺度信号衰减和多径分量；引入物理感知的频率相关衰减模型实现跨频泛化；采用点云辅助环境增强方法支持多细胞多网格建模；利用低秩张量表示和分层张量角度建模算法提高计算效率。

Result: 在真实多细胞数据集上的实验表明，RF-LSCM显著优于现有方法，覆盖预测的平均绝对误差降低30%，通过有效融合多频数据实现22%的MAE改进。

Conclusion: RF-LSCM通过创新的多域建模方法和高效计算架构，成功解决了传统信道建模的局限性，为蜂窝网络优化提供了更准确可靠的预测能力。

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [33] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文提出了一个基于共形预测的分布无关不确定性量化框架，为物理信息神经网络(PINNs)提供具有严格统计保证的预测区间校准。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs的不确定性量化方法缺乏严格的统计保证，需要一种能够提供有限样本覆盖保证的分布无关方法。

Method: 采用分布无关的共形预测框架，通过在校准集上构建非共形性分数来校准预测区间，并引入局部共形分位数估计来处理空间异方差性。

Result: 在典型PDE系统上的系统评估表明，该框架实现了可靠的校准和局部自适应不确定性区间，在多个不确定性指标上一致优于启发式UQ方法。

Conclusion: 该工作通过将PINNs与分布无关UQ相结合，不仅提高了校准性和可靠性，还为复杂PDE系统的不确定性感知建模开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [34] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 本研究开发了一个基于智能手表的心率监测系统，结合机器学习方法，用于实时检测社交焦虑患者的瞬时焦虑状态，在数据集上达到60.4%的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑是一种常见心理健康问题，但现有研究很少测量和预测日常生活中的瞬时焦虑波动。捕捉这些日内动态对于设计实时个性化干预措施至关重要。

Method: 使用定制智能手表系统收集91名社交焦虑大学生的心率数据（平均9.03天），结合7次/日的生态瞬时评估。基于外部10,000+天心率数据预训练基础模型，迁移学习到本数据集并进行微调，生成概率预测，再与特质水平测量结合使用元学习器。

Result: 在本数据集上达到60.4%的平衡准确率；在TILES-18数据集的外部验证中达到59.1%的平衡准确率，比先前工作提升至少7%。

Conclusion: 该方法能够有效检测社交焦虑患者的瞬时焦虑状态，具有良好的泛化能力，为实时个性化干预提供了技术基础。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [35] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了DirGraphSSM，首个将状态空间模型系统扩展到有向图学习的架构，通过k-hop ego图序列化和消息传递机制，在保持高效训练的同时实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN和图Transformer在处理有向图时面临两个主要挑战：有效捕捉长距离因果依赖关系，以及在处理大规模图数据集时平衡准确性和训练效率。现有的图状态空间模型仅适用于无向图，限制了其性能。

Method: 提出DirEgo2Token方法通过k-hop ego图将有向图序列化，并在此基础上开发DirGraphSSM架构，通过消息传递机制在有向图上实现状态空间模型。

Result: 在三个代表性有向图学习任务上达到SOTA性能，在另外两个任务上获得竞争性性能，训练速度比现有SOTA模型快1.5-2倍。

Conclusion: DirGraphSSM成功将有向图学习与状态空间模型相结合，在保持高效训练的同时显著提升了有向图学习的性能，为有向图学习开辟了新方向。

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [36] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK是一个增强大语言模型捕捉时空依赖性的新框架，通过空间增强注意力和记忆检索前馈网络解决LLM在交通预测中的空间建模限制


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要针对序列标记处理设计，在交通预测中难以有效捕捉空间依赖性，特别是对图结构空间数据的建模存在架构不兼容问题

Method: 提出ST-LINK框架，包含空间增强注意力（SE-Attention）和记忆检索前馈网络（MRFFN）。SE-Attention扩展旋转位置编码，在注意力机制中直接整合空间相关性；MRFFN动态检索历史模式以捕捉复杂时间依赖性

Result: 在基准数据集上的综合实验表明，ST-LINK超越了传统深度学习和LLM方法，能有效捕捉常规交通模式和突变

Conclusion: ST-LINK成功解决了LLM在交通预测中的空间建模挑战，为智能交通系统中的时空依赖建模提供了有效解决方案

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [37] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 本文从因果视角分析多视图无监督特征选择，提出CAUSA方法通过因果正则化模块分离混淆变量并平衡分布，以减轻虚假相关性，从而选择因果信息特征。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法依赖特征与聚类标签间的相关性，但这些相关性可能因混淆变量而产生虚假关联，导致选择不相关特征。需要从因果角度解决这一问题。

Method: 提出CAUSA方法：1）使用广义无监督谱回归模型捕获特征与共识聚类标签的依赖关系；2）引入因果正则化模块自适应分离多视图数据中的混淆变量，并学习视图共享样本权重来平衡混淆变量分布。

Result: 综合实验表明CAUSA方法在多个数据集上优于现有最先进方法。

Conclusion: 这是首个在无监督设置下对因果多视图特征选择的深入研究，CAUSA方法能有效减轻虚假相关性，选择更具因果信息的特征。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [38] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: 提出了FHNN框架，通过物理结构化的神经网络预测可解释的水动力参数，结合解析运动方程，在涡流数据集上比Neural ODEs误差低一个数量级，并能处理耗散动力学。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒神经网络模型在流体-结构相互作用中缺乏可解释性且长期预测不稳定，需要一种既能保持物理可解释性又能有效处理耗散动力学的方法。

Method: 使用物理结构化的Floating-Body Hydrodynamic Neural Networks (FHNN)框架，预测方向附加质量、阻力系数和基于流函数的流动等水动力参数，并与解析运动方程耦合。

Result: 在合成涡流数据集上，FHNN比Neural ODEs误差低一个数量级，能够恢复物理一致的流场，比哈密顿和拉格朗日神经网络更有效地处理耗散动力学。

Conclusion: FHNN填补了黑盒学习和透明系统识别之间的差距，通过物理约束增强可解释性并稳定积分过程，为流体-结构相互作用建模提供了有效的解决方案。

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [39] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: 提出了通用物理变换器(GPhyT)，这是一个基于Transformer的物理基础模型，能够在多个物理领域实现零样本泛化和稳定长期预测，无需重新训练或知道底层物理方程。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的机器学习方法局限于单一狭窄领域且需要为每个新系统重新训练，缺乏像NLP中基础模型那样的"一次训练，随处部署"能力。物理基础模型(PFM)将能够 democratize 高保真模拟访问、加速科学发现并消除专业求解器开发需求。

Method: 使用Transformer架构，在1.8TB的多样化模拟数据上进行训练，关键洞察是Transformer能够从上下文中推断控制动力学，使单个模型能够模拟多种物理现象而无需被告知底层方程。

Result: GPhyT实现了三个突破：1)在多个物理域中性能优于专用架构达29倍；2)通过上下文学习实现零样本泛化到完全未见过的物理系统；3)通过50时间步展开实现稳定长期预测。

Conclusion: 这项工作证明单个模型可以从数据中学习可泛化的物理原理，为通向可能改变计算科学与工程的通用物理基础模型开辟了道路。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [40] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子-经典工作流，用于解决普惠金融中数据稀缺的少样本信用风险评估问题，通过经典机器学习进行特征工程，再用量子神经网络作为核心分类器，在真实数据集上取得了优于经典方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决普惠金融中由于数据稀缺和不平衡导致的信用风险评估挑战，传统方法在这种少样本场景下效果有限，需要探索量子机器学习的新范式。

Method: 设计混合量子-经典工作流：首先使用经典机器学习模型（逻辑回归、随机森林、XGBoost）进行智能特征工程和降维，然后使用通过参数偏移规则训练的量子神经网络作为核心分类器。

Result: 在279个样本的真实信用数据集上，量子神经网络在模拟中达到0.852±0.027的平均AUC，在Quafu量子云平台的ScQ-P21超导处理器上实验获得0.88的AUC，性能超越了一系列经典基准方法。

Conclusion: 该研究为NISQ时代量子计算在数据受限金融场景中的应用提供了实用蓝图，并为量子机器学习在高风险应用（如普惠金融）中的潜力提供了有价值的实证证据。

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [41] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出了一种端到端可微分的混合框架，将图神经网络嵌入孔隙网络模型中，用于多孔介质渗透率预测，避免了传统方法的理想化几何假设，同时保持了物理基础的流动计算。


<details>
  <summary>Details</summary>
Motivation: 传统纯数据驱动模型缺乏跨尺度泛化能力且不包含显式物理约束，而孔隙网络模型虽然基于物理但依赖理想化几何假设来估算孔尺度水力传导度，在复杂结构中精度有限。

Method: 开发了一个端到端可微分混合框架，用基于图神经网络的预测替代传统解析公式进行传导度计算，通过自动微分和离散伴随方法实现全耦合训练，仅需单一渗透率标量作为训练目标。

Result: 该模型实现了高精度和良好的跨尺度泛化能力，在准确性和泛化性方面均优于纯数据驱动和传统孔隙网络模型方法。

Conclusion: 该方法为复杂多孔介质中的渗透率预测提供了一个可扩展且物理信息丰富的框架，降低了模型不确定性并提高了准确性，梯度敏感性分析还增强了模型的可解释性。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [42] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，将掩码扩散模型解释为离散最优传输中的能量最小化问题，证明了三种能量公式的数学等价性，并通过Beta分布参数化插值调度，实现了高效的后训练调优。


<details>
  <summary>Details</summary>
Motivation: 统一掩码扩散模型的理论基础，澄清其数学本质，并为实际采样改进提供理论指导。

Method: 证明三种能量公式（动能、条件动能和测地能量）在MDMs结构下的数学等价性；通过Beta分布参数化插值调度，将调度设计空间简化为2D搜索。

Result: 实验证明，基于能量启发的调度在合成和真实世界基准测试中优于手工设计的基线，特别是在低步采样设置中表现突出。

Conclusion: 该框架不仅统一了MDMs的理论理解，还提供了实用的调度优化方法，显著提升了采样效率。

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [43] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG是一种基于随机采样的历史感知漂移对齐方法，通过维护客户端漂移记忆和基于参与率的门控机制，有效解决联邦学习中的非IID数据和部分参与导致的客户端漂移问题，显著提升收敛速度和测试准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非IID数据和部分客户端参与会导致客户端漂移和局部最优不一致，造成收敛不稳定和准确率下降，需要一种有效的方法来对齐局部和全局模型。

Method: FedSSG维护每个客户端的漂移记忆来累积局部模型差异作为历史梯度的轻量级草图，通过基于观察/期望参与比的门控函数来控制记忆更新和局部对齐项，该门控在采样噪声主导时保持弱平滑，在参与统计稳定后增强。

Result: 在CIFAR-10/100数据集上，100/500个客户端，2-15%参与率的情况下，FedSSG相比基线方法提升测试准确率约0.9点(CIFAR-10)和2.7点(CIFAR-100)，平均加速目标准确率收敛约4.5倍。

Conclusion: FedSSG证明采样统计可以转化为原则性的历史感知相位控制，有效稳定和加速联邦训练，仅需O(d)客户端内存和常数时间门控，在近IID或均匀采样情况下优雅退化为温和正则化器。

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [44] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter是一个轻量级适配器，无需微调即可为时间序列基础模型添加协变量信息，通过两阶段方法结合伪预测和TSFM预测，在真实数据集上比基础模型提升24-27%的性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型无法有效利用协变量信息，而协变量在许多应用中对于准确预测至关重要。需要一种轻量级方法来增强基础模型的协变量处理能力。

Method: 采用两阶段方法：1) 使用简单回归模型生成伪预测；2) 训练高斯过程回归器，结合伪预测、TSFM预测和协变量来优化预测结果。无需微调基础模型。

Result: 在真实世界数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，相比基础模型实现了24-27%的性能提升，且数据和计算开销极小。

Conclusion: 轻量级适配器有潜力弥合通用基础模型与领域特定预测需求之间的差距，TFMAdapter为时间序列基础模型有效整合协变量信息提供了可行方案。

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [45] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx是首个专门针对交叉公平性的框架，通过多目标优化处理多个敏感属性的组合偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法只处理单一属性，无法捕捉交叉属性（如种族、性别、年龄组合）产生的复杂、乘性偏差

Method: 结合自适应多目标优化器（动态切换Pareto锥投影、梯度加权和探索策略）、可微交叉公平性度量、理论收敛保证

Result: 在四个真实数据集上实验显示，APFEx能显著减少公平性违规，同时保持竞争力的准确率

Conclusion: APFEx填补了公平机器学习的重要空白，为交叉公平性提供了可扩展、模型无关的解决方案

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [46] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: 通过简单的置信度加权平均方法，无需重新训练即可组合多个最先进的深度学习模型，在车辆轨迹预测任务中实现了10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶领域不断涌现更大更先进的预测模型，如何在不进行昂贵重新训练的情况下结合这些大模型的优势成为一个重要挑战。

Method: 使用置信度加权平均方法，将多个最先进的深度学习模型进行组合，无需重新训练或微调。

Result: 在NuScenes和Argoverse数据集上，该方法比最佳单一模型性能提升10%，特别是在长尾指标上表现优异，且改进在整个数据分布上都保持一致。

Conclusion: 简单的模型集成方法能够有效提升轨迹预测性能，证明了无需复杂重新训练即可组合现有模型的可行性，相关代码已开源。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [47] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN是一种扩展物理信息神经网络框架，用于解决多裂纹断裂力学问题，通过能量损失函数、定制积分方案和域分解方法，结合XFEM思想在神经网络解空间中引入特殊函数来捕捉裂纹不连续性和奇异性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多裂纹断裂力学问题时面临挑战，需要开发一种能够有效捕捉裂纹不连续性和奇异性的新型计算框架。

Method: 提出X-PINN框架，采用能量损失函数、定制积分方案和域分解，借鉴XFEM思想在神经网络解空间中引入特殊函数来显式处理裂纹不连续性和奇异性，使用不同神经网络分别建模标准和增强解分量。

Result: 数值实验验证了该方法在1D和2D多裂纹问题中的有效性和鲁棒性，并具有良好的3D问题扩展性。

Conclusion: X-PINN为复杂多裂纹断裂力学问题提供了一个灵活有效的计算框架，具有处理不连续性和奇异性的优势。

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [48] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART是一个用于癫痫发作检测的持续学习框架，通过选择性保留高熵和预测为发作的样本，在有限内存和计算资源下实现个性化适应，在CHB-MIT数据集上F1分数提升21%


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断依赖专家分析脑电图，过程耗时且需要专业知识。现有深度学习模型存在灾难性遗忘问题，无法适应患者脑电图信号随时间演变的特点

Method: 提出EpiSMART持续学习框架，使用大小受限的重放缓冲区和智能样本选择策略，选择性保留高熵和预测为癫痫发作的样本，增量适应患者特异性脑电图信号

Result: 在CHB-MIT数据集验证，相比不更新的基线模型F1分数提升21%，平均每天仅需6.46分钟标记数据和6.28次更新，适合可穿戴系统实时部署

Conclusion: EpiSMART能够在资源受限条件下有效整合新数据而不损害已有知识，实现鲁棒的个性化癫痫发作检测，推动可穿戴医疗系统的实际应用

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [49] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 提出基于动态图回归的GNSS干扰抑制方法，使用异构图卷积LSTM网络实时预测并校正接收器水平偏差，在多种干扰场景下显著优于传统时间序列基线模型


<details>
  <summary>Details</summary>
Motivation: GNSS系统日益受到故意干扰的影响，导致在需要精确定位和计时时系统可用性下降，需要实时干扰抑制解决方案

Method: 将卫星接收器环境建模为异构星形图，使用单层异构图卷积LSTM（HeteroGCLSTM）聚合空间上下文和时间动态，输出2D偏差向量进行实时校正

Result: 在-45dBm强干扰下达到3.64-7.74cm的MAE，在-60至-70dBm时改善至1.65-2.08cm，混合模式下MAE为3.78-4.25cm，数据效率优异（仅10%训练数据仍优于基线）

Conclusion: 该方法能有效实时抑制GNSS干扰，在各种干扰类型和功率水平下均表现优异，具有显著的数据效率和鲁棒性

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [50] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 提出基于联邦学习和差分隐私的隐私保护流行病预测方法，在德国县级层面实现本地化预测，在保护隐私的同时保持预测准确性


<details>
  <summary>Details</summary>
Motivation: 在流行病期间需要快速反应，但本地化机器学习模型训练面临数据不足问题，而集中化数据又存在隐私敏感性问题，需要找到既能保护隐私又能提供详细情境数据的解决方案

Method: 使用联邦学习框架结合客户端级差分隐私，以德国县和社区作为客户端，通过多层感知机在滑动窗口上进行病例数预测，客户端只交换经过范数裁剪的更新，服务器聚合更新时添加差分隐私噪声

Result: 在严格隐私保护下预测不稳定，但在适度隐私级别下，差分隐私模型接近非隐私模型性能：2020年11月R²=0.94（对比0.95），MAPE=26%；2022年3月R²=0.88（对比0.93），MAPE=21%

Conclusion: 客户端级差分隐私联邦学习能够提供有用的县级预测并具有强隐私保证，可行的隐私预算取决于流行病阶段，允许卫生当局进行隐私合规的本地预测协作

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [51] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: 本文提出了一种使用小波变换将纳米孔电流信号转换为尺度图图像，然后利用机器学习算法进行蛋白质分类的新方法，在42种肽上达到了81%的分类准确率，为实时疾病诊断提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 开发能够在临床环境中实时分类蛋白质的设备，实现廉价快速的疾病诊断。纳米孔技术虽然具有潜力，但当前信号复杂性限制了其准确性。

Method: 将纳米孔电流信号通过小波变换转换为尺度图图像，捕获振幅、频率和时间信息，然后使用机器学习算法进行分类。

Result: 在42种肽上测试，分类准确率达到约81%，创造了该领域的新最先进水平。

Conclusion: 该方法为实时肽/蛋白质诊断在护理点的实际应用迈出了重要一步，并展示了模型迁移技术，为实时疾病诊断开辟了新途径。

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [52] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 提出基于环境传感器融合的轻量级蜂王检测系统，使用温度、湿度和压力差数据，在STM32微控制器上实现实时低功耗边缘计算，准确率超过99%


<details>
  <summary>Details</summary>
Motivation: 传统蜂王检测方法依赖人工检查，劳动强度大且干扰蜂群；现有音频方法功耗高、预处理复杂且易受环境噪声影响

Method: 使用环境传感器（温度、湿度、压力差）融合技术，在STM32微控制器上采用量化决策树推理进行实时边缘计算

Result: 仅使用环境输入即可实现超过99%的蜂王检测准确率，音频特征未带来显著性能提升

Conclusion: 提供了一种可扩展、可持续的非侵入式蜂巢监测解决方案，为使用现成节能硬件的自主精准养蜂铺平道路

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [53] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究强化学习中的贝叶斯风险规避方法，通过BRMDP处理模型参数不确定性，证明了贝叶斯风险价值函数与真实价值函数之间的渐近正态性差异，并提出了基于后验采样的在线RL和CMAB算法，获得了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中由于数据不足导致的认知不确定性，通过贝叶斯风险规避方法来处理未知基础模型的参数不确定性。

Method: 采用贝叶斯风险马尔可夫决策过程(BRMDP)，推导贝叶斯风险价值函数与真实价值函数之间的渐近正态性关系，提出基于后验采样的在线RL和CMAB算法。

Result: 贝叶斯风险规避方法会悲观地低估原始价值函数，这种差异随风险规避强度增加而增大，随数据量增加而减小。获得了RL和CMAB设置的次线性遗憾界。

Conclusion: 提出的贝叶斯风险规避方法能有效处理认知不确定性，数值实验验证了理论性质的有效性，为在线RL和CMAB问题提供了实用的风险感知解决方案。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [54] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 本研究比较了不同优化器和神经网络架构在EEG频段分类中的性能，发现Adagrad和RMSprop优化器表现最佳，CNN在空间特征提取方面表现优异，SHAP分析揭示了各频段对分类准确性的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同优化器和神经网络架构在EEG频段分类任务中的性能差异，以及如何有效预测左右脑半球的分类，为神经影像分类任务提供优化策略。

Method: 使用TensorFlow和PyTorch框架实现三种神经网络架构（深度密集网络、浅层三层网络、CNN），比较多种优化器（Adagrad、RMSprop、Adadelta、SGD、FTRL）在不同EEG频段的性能，并采用SHAP进行特征重要性分析。

Result: Adagrad和RMSprop优化器在各频段表现稳定，Adagrad在beta频段表现最佳，RMSprop在gamma频段最优。CNN模型准确率第二高，擅长捕捉EEG空间特征；深度密集网络在学习复杂模式方面有竞争力；浅层网络计算效率高但准确率较低。

Conclusion: 优化器选择、模型架构和EEG频段分析对分类器性能至关重要，研究为神经影像分类任务提供了重要的优化见解和特征重要性理解。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [55] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 提出了Quantile Neural Basis Model，将分位数广义可加模型的解释性原理融入神经网络框架，在保持预测性能的同时提供模型行为的可解释性洞察。


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络在多水平概率预测中取得了高精度，但理解特征条件输出的底层机制仍然是一个重大挑战，需要提高模型的可解释性。

Method: 利用共享基分解和权重分解，将Quantile Generalized Additive Models的可解释性原则整合到端到端神经网络训练框架中，避免参数分布假设。

Result: 在日前电价预测任务中验证了方法，预测性能与分布回归和分位数回归神经网络相当，同时通过学习到的从输入特征到输出预测的非线性映射提供有价值的模型行为洞察。

Conclusion: Quantile Neural Basis Model成功地在保持神经网络预测性能的同时，提供了类似可加模型的可解释性，为理解特征条件预测机制提供了有效途径。

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [56] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 该研究通过预测建模和实地试验，针对高风险再监禁人群进行心理健康干预，发现对最高风险群体的干预效果最显著，能改善心理健康服务使用、减少急救调度和司法介入。


<details>
  <summary>Details</summary>
Motivation: 监狱系统往往无法有效处理被监禁者的心理健康、药物依赖和无家可归等复杂需求，导致再犯罪和监禁循环，特别是加剧了种族不平等。需要创新方法来打破这一循环。

Method: 采用预测建模方法识别再监禁高风险人群，设计并实施实地试验进行针对性心理健康外展干预，评估模型预测能力和干预效果。

Result: 模型对新的监狱收监具有高度预测性，最高风险群体中超过一半在一年内重返监狱。干预对最高风险个体最有效，显著改善了心理健康服务使用、减少了急救调度和司法系统介入。

Conclusion: 针对性心理健康外展干预，特别是针对最高再监禁风险人群的干预，能够有效打破监禁循环，改善个体结局和公共安全。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [57] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: 该论文研究了核岭回归的组合变体，通过坐标重加权进行特征学习，证明了在噪声变量为高斯分布时，全局最小值和驻点都能有效消除噪声坐标，发现ℓ1型核能恢复非线性特征而高斯核只能恢复线性特征。


<details>
  <summary>Details</summary>
Motivation: 研究组合架构中的特征学习问题，为变量选择提供理论保证，探索不同核函数在特征恢复中的表现差异。

Method: 使用变分问题框架下的组合核岭回归模型，分析全局最小值和驻点的性质，比较ℓ1型核（如拉普拉斯核）和高斯核在特征恢复中的效果。

Result: 证明了在噪声变量高斯分布条件下，全局最小值和驻点都能成功消除噪声坐标；发现ℓ1型核能够恢复非线性效应的特征，而高斯核只能恢复线性特征。

Conclusion: 组合核岭回归为特征学习提供了有效的测试平台，ℓ1型核在非线性特征恢复方面优于高斯核，为变量选择和特征学习提供了理论指导。

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [58] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 提出了一种端到端框架，通过人工智能架构和新型数据生成策略PCDS，从稀疏的常规数据中非侵入性地估计青光眼等疾病中无法测量的关键参数（如小梁网渗透性），解决了缺乏真实数据和计算成本高的逆问题挑战。


<details>
  <summary>Details</summary>
Motivation: 解决医疗决策中关键参数无法测量的挑战，特别是在青光眼治疗中，眼内压的主要决定因素小梁网渗透性无法在体内测量，临床医生只能依赖间接替代指标。同时，缺乏真实数据和计算成本高阻碍了预测模型的发展。

Method: 采用多阶段人工智能架构功能分离问题；提出新颖的PCDS数据生成策略，避免需要数十万次昂贵模拟，将有效计算时间从数年缩短到数小时；使用贝叶斯引擎量化预测不确定性。

Result: 该框架仅从常规输入就将单次眼内压测量分解为其基本组成部分，获得了无法测量的组织渗透性和患者流出能力的估计。非侵入性估计的流出能力与最先进的眼压测量技术表现出极好的一致性，精度可与直接物理仪器相媲美。新推导的渗透性生物标志物在按疾病风险分层临床队列方面表现出高准确性。

Conclusion: 该框架为在数据稀缺、计算密集的其他领域中解决类似逆问题建立了可推广的蓝图，具有重要的诊断潜力。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [59] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing是一个端到端框架，通过图算法和LLM代理实现电路理解，并将知识整合到贝叶斯优化中，提高模拟电路设计效率


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号电路设计面临高质量数据短缺和领域知识难以嵌入自动化流程的挑战，传统黑盒优化缺乏电路理解，学习型方法成本高且需要重新训练

Method: 首先使用图算法将电路组织为层次化设备-模块-阶段表示，然后LLM代理执行假设-验证-精炼循环进行标注，最后将验证的洞察整合到贝叶斯优化中，通过LLM引导初始采样和停滞触发信任区域更新

Result: 提高了优化效率同时保持可行性

Conclusion: TopoSizing框架能够直接从原始网表实现稳健的电路理解，并将这些知识转化为优化收益，解决了传统方法的局限性

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [60] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO是一个离线强化学习框架，通过树形轨迹表示和过程奖励模型解决Web Agent训练中的信用分配、标注成本和奖励稀疏问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，使用大模型作为Web Agent进行自动化网页交互变得重要，但强化学习训练面临信用分配不当、标注成本高和奖励稀疏等关键挑战。

Method: 提出Tree-Guided Preference Optimization (TGPO)框架，采用树形轨迹表示合并语义相同的状态消除标签冲突，包含过程奖励模型自动生成细粒度奖励（通过子目标进度、冗余检测和动作验证），以及动态权重机制优先处理高影响力决策点。

Result: 在Online-Mind2Web和自建的C-WebShop数据集上实验表明，TGPO显著优于现有方法，以更少的冗余步骤实现更高的成功率。

Conclusion: TGPO框架有效解决了Web Agent训练中的关键问题，为自动化网页交互提供了更高效的解决方案。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [61] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign是一个轻量级的即插即用框架，通过简单的重构任务学习辅助特征来弥补时间序列输入历史与未来目标之间的分布差异，显著提升各种基础预测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习等表示学习技术在时间序列预测中表现不佳，但作者认为显式的表示对齐可以提供关键信息来弥补输入历史与未来目标之间的分布差距。

Method: 提出TimeAlign框架，通过重构任务学习辅助特征，然后将这些特征反馈给任何基础预测器。该方法是架构无关的，计算开销很小。

Result: 在八个基准测试上的广泛实验验证了其优越性能，增益主要来自纠正历史输入与未来输出之间的频率不匹配问题。理论分析表明TimeAlign能增加学习表示与预测目标之间的互信息。

Conclusion: TimeAlign可以作为现代深度学习时间序列预测系统的通用对齐模块，具有架构无关性和可忽略的开销优势。

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [62] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出了一个统一的变分框架来形式化基于残差的自适应策略，通过积分残差的凸变换将离散化选择与误差度量直接联系起来，为自适应方案提供了理论依据和系统设计方法。


<details>
  <summary>Details</summary>
Motivation: 基于残差的自适应策略在科学机器学习中广泛使用但缺乏理论依据，需要建立一个统一的理论框架来形式化这些方法并建立有原则的离散化和训练策略。

Method: 引入变分框架，通过积分残差的凸变换来形式化自适应策略，不同的变换对应不同的目标函数（指数权重对应均匀误差最小化，线性权重对应二次误差最小化），将自适应加权等价于选择优化原始目标的采样分布。

Result: 该框架实现了三个好处：系统设计跨范数的自适应方案、通过损失估计器的方差减少降低离散化误差、通过改善梯度信噪比增强学习动态。在算子学习中展示了显著的性能提升。

Conclusion: 为基于残差的自适应性提供了理论依据，建立了有原则的离散化和训练策略的基础，证明了该框架在不同优化器和架构上的有效性。

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [63] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一个开创性的Banach-Bregman随机优化框架，突破了传统Hilbert空间的限制，为下一代优化算法提供了统一的理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有的随机优化理论主要局限于Hilbert空间，无法有效处理非欧几里得设置，如单纯形上的镜像下降、稀疏学习的Bregman近端方法、信息几何中的自然梯度下降等实际问题。

Method: 提出了基于Bregman几何的统一框架，包括Bregman投影和Bregman-Fejer单调性模板，支持超松弛（λ>2）在非Hilbert设置中的应用，涵盖了随机逼近、镜像下降、自然梯度、自适应方法和镜像-近端等多种算法。

Result: 在机器学习（UCI基准）、深度学习（Transformer训练）、强化学习（actor-critic）和大语言模型（WikiText-2与distilGPT-2）等任务上，相比经典基线方法实现了高达20%的收敛加速、方差降低和精度提升。

Conclusion: Banach-Bregman几何成为统一优化理论和AI核心范式实践的基石，为下一代优化算法的发展奠定了重要基础。

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [64] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: 提出RKTV-INR去噪框架，结合隐式神经表示、龙格-库塔积分和全变分约束，从噪声观测中重建动力系统轨迹，并用于SINDy系统辨识


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的数据驱动建模常受测量噪声影响，需要有效的去噪方法来准确恢复系统动力学

Method: 使用隐式神经表示(INR)拟合噪声观测，施加龙格-库塔积分和全变分约束确保重建状态符合动力系统轨迹，通过自动微分获得精确导数，最后用SINDy识别控制方程

Result: 实验证明该方法能有效抑制噪声、精确估计导数，并可靠地进行系统辨识

Conclusion: RKTV-INR框架为噪声环境下的非线性动力系统建模提供了有效的解决方案，结合了神经网络表示和物理约束的优势

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [65] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 语言模型的激活值线性编码了训练过程中信息被学习的时间顺序，模型能够区分不同时间学习的信息


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否以及如何编码信息的学习时间顺序，这对于理解模型如何处理冲突数据和知识修改具有重要意义

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的命名实体数据集上，分析激活值的线性编码特性，使用线性探测和2D投影技术

Result: 发现激活值中心点在2D子空间中按训练顺序直线排列，线性探测能准确区分早期和晚期实体（~90%准确率），模型还能微调以报告未见实体的训练阶段（~80%准确率）

Conclusion: 语言模型确实能够按获取时间区分信息，这一发现对模型处理冲突数据和知识修改的方式具有重要启示意义

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [66] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 提出使用临界阻尼高阶朗之万动力学来防御扩散模型中的成员推理攻击，通过引入辅助变量和联合扩散过程来混合外部随机性，从而在扩散过程中早期破坏敏感输入数据


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用的数据安全担忧日益增长，扩散模型虽然比其他生成模型对成员推理攻击更具抵抗力，但仍存在脆弱性，需要有效的防御机制

Method: 采用临界阻尼高阶朗之万动力学，引入多个辅助变量和联合扩散过程，利用辅助变量混合外部随机性来在扩散过程早期破坏敏感输入数据

Result: 在玩具数据集和语音数据集上通过AUROC曲线和FID指标进行了理论研究和实验验证，证明了防御方法的有效性

Conclusion: 提出的基于高阶朗之万动力学的防御机制能够有效保护扩散模型免受成员推理攻击，增强了生成模型的数据安全性

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [67] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA是一种新颖的结构化剪枝方法，通过神经正切核理论指导的显著性准则和自适应稀疏分配机制，在保持零样本准确性的同时实现高效LLM压缩。


<details>
  <summary>Details</summary>
Motivation: 解决现有结构化剪枝方法在零样本设置下性能显著下降的问题，避免昂贵的恢复技术如监督微调或适配器插入。

Method: 使用基于Adam优化动态的神经正切核一阶显著性准则，结合跨层和模块的自适应稀疏分配机制，以及基于KL散度的校准数据选择策略。

Result: 在Llama3、Qwen和T5模型上的实验表明，NIRVANA在同等稀疏度约束下优于现有结构化剪枝方法。

Conclusion: NIRVANA提供了一种理论上有依据且实用的LLM压缩方法，平衡了零样本准确性保持和微调能力。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [68] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: CaT（Compute as Teacher）通过将模型在推理时的探索转化为无参考监督，利用并行rollout合成单一参考，并优化模型向该参考靠近，从而在没有真实标签的后训练中生成学习信号。


<details>
  <summary>Details</summary>
Motivation: 解决在没有真实标签的后训练中，如何生成有效的学习信号的问题。传统选择方法（如best-of-N、多数投票等）存在局限性，无法在所有rollout都错误时提供正确指导。

Method: 1. 当前策略生成一组并行rollout；2. 冻结的锚点策略（初始策略）协调冲突和遗漏，合成单一参考；3. 通过两种奖励机制：可验证任务使用程序等价性，不可验证任务使用自提议的评分标准并由独立LLM评判；4. 可作为测试时过程或强化学习训练。

Result: 在Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B上取得显著提升：MATH-500上最高+27%，HealthBench上+12%。使用强化学习（CaT-RL）后获得进一步增益（最高+33%和+30%），训练后的策略超越了初始教师信号。

Conclusion: CaT成功将推理时的额外计算转化为有效的监督信号，解决了无真实标签环境下的学习问题，其性能随rollout数量增加而提升，且合成方法相比传统选择方法具有独特优势。

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>
