<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 185]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.DC](#cs.DC) [Total: 36]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction](https://arxiv.org/abs/2601.11770)
*Voktho Das,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: NuRedact是一个非均匀eFPGA重写框架，通过架构非均匀性平衡安全性和效率，相比传统均匀结构实现高达9倍面积减少，同时保持强大抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于可重构的重写技术（如LUT和eFPGA方案）虽然提高了安全性，但带来了巨大开销。这些开销主要来自为阻碍逆向工程而人为增加的复杂性，而非真正的功能可重构需求，导致结构利用率低且安全成本过高。

Method: NuRedact采用三阶段方法：1）生成具有引脚映射不规则性的定制结构；2）修改VPR级工具以支持基于Python优化器的非均匀布局；3）进行重写感知的目标IP模块重配置和映射。该框架基于OpenFPGA基础设施构建。

Result: 实验结果显示，相比传统均匀结构实现高达9倍面积减少，在保持强大抗攻击能力的同时，达到与LUT基甚至晶体管级重写技术相竞争的效率。安全评估表明对SAT基、循环和时序变体等先进攻击模型具有增强的抵抗能力。

Conclusion: NuRedact通过引入架构非均匀性，在安全性和效率之间实现了更好的平衡，为集成电路供应链安全提供了更实用的重写解决方案，同时保持了可接受的设计开销。

Abstract: While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.

</details>


### [2] [Domain-specific Hardware Acceleration for Model Predictive Path Integral Control](https://arxiv.org/abs/2601.12089)
*Erwan Tanguy-Legac,Tommaso Belvedere,Gianluca Corsini,Marco Tognon,Marcello Traiola*

Main category: cs.AR

TL;DR: 提出了一种用于MPPI控制的硬件加速器，相比GPU实现能提供更精确的轨迹控制


<details>
  <summary>Details</summary>
Motivation: 机器人实时控制面临挑战，现有MPC方法在非线性系统上难以实现，MPPI方法计算负载重，GPU加速功耗过大不适合电池供电的自主系统，而FPGA定制设计能实现低功耗加速，但目前缺乏MPPI的专用加速器

Method: 设计并实现了一个MPPI控制的硬件加速器，通过模拟执行验证其性能

Result: MPPI定制加速器相比基于GPU的MPPI实现能够实现更精确的轨迹控制

Conclusion: 提出的MPPI硬件加速器解决了现有方法在功耗和性能方面的限制，为机器人实时控制提供了更优的解决方案

Abstract: Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.

</details>


### [3] [Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification](https://arxiv.org/abs/2601.12156)
*Debabrata Das,Yogeeth G. K.,Arnav Gupta*

Main category: cs.AR

TL;DR: 设计了一个基于SystemVerilog的精确周期硬件SNN核心，使用LIF神经元模型和定点运算，通过主动剪枝机制降低功耗，在数字分类任务上达到89%准确率


<details>
  <summary>Details</summary>
Motivation: 边缘AI部署受限于传统ANN的高功耗和延迟，而神经形态计算通过事件驱动处理模仿生物效率，提供了一种有吸引力的替代方案

Method: 设计并实现了一个精确周期的硬件导向SNN核心，使用LIF神经元模型、定点运算和位级原语（移位和加法），包含片上泊松编码器和新颖的主动剪枝机制

Result: 在数字分类任务上，设计在有限时间步内快速收敛达到89%准确率，同时相比传统密集架构显著减少了计算开销

Conclusion: 这项工作为FPGA和ASIC平台上可扩展、高能效的神经形态硬件提供了基础构建模块

Abstract: The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.

</details>


### [4] [CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device](https://arxiv.org/abs/2601.12298)
*Ye Lin,Chao Fang,Xiaoyong Song,Qi Wu,Anying Jiang,Yichuan Bai,Li Du*

Main category: cs.AR

TL;DR: CD-PIM是一种针对边缘低批次LLM部署的内存处理架构，通过高带宽计算效率模式、低批次交错模式和计算效率单元设计，解决现有PIM架构的带宽限制、组件利用不足和计算能力低的问题。


<details>
  <summary>Details</summary>
Motivation: 边缘部署低批次大语言模型面临内存带宽瓶颈，现有数字内存处理架构存在三个关键限制：带宽改进有限、混合工作负载下组件利用不足、计算单元计算能力低。

Method: 提出CD-PIM架构，包含四个创新：1）高带宽计算效率模式（HBCEM），通过分段全局位线将每个存储体分为四个伪存储体；2）低批次交错模式（LBIM），通过重叠GEMV和GEMM操作提高组件利用率；3）计算效率单元设计，以流水线方式串行输入权重数据；4）键缓存矩阵采用列向映射，值缓存矩阵采用行向映射。

Result: 在HBCEM模式下，相比GPU基线和最先进PIM设计，CD-PIM在单批次内分别实现11.42倍和4.25倍的平均加速。在低批次场景下，LBIM相比HBCEM实现1.12倍平均加速。

Conclusion: CD-PIM通过创新的架构设计有效解决了边缘低批次LLM部署中的内存带宽瓶颈问题，显著提升了GEMV操作的执行效率。

Abstract: Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.

</details>


### [5] [Best Practices for Large Load Interconnections: A North American Perspective on Data Centers](https://arxiv.org/abs/2601.12686)
*Rafi Zahedi,Amin Zamani,Rahul Anilkumar*

Main category: cs.AR

TL;DR: 该论文综述了北美大型负载（数据中心、加密货币挖矿、制氢设施、重型充电站）并网的最佳实践，分析了技术挑战并提出了实用指导建议。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心、加密货币挖矿、制氢设施和重型充电站等大型负载在北美快速扩张，特别是AI部署推动数据中心容量空前增长，这些负载的规模、工作周期和变流器主导的接口给输电并网带来了新的挑战，包括扰动行为、稳态性能和运行可见性等问题。

Method: 通过分析北美公用事业公司和系统运营商指南，结合手册和人工分析、跨公用事业公司比较以及欧洲发展方向展望，综合形成一套连贯的技术要求框架。

Result: 研究明确了电能质量、遥测、调试测试和保护协调等方面的要求，同时指出了在穿越规范、负载变化管理和扰动后恢复目标方面存在的差距。

Conclusion: 基于研究发现，论文为开发商和公用事业公司提出了实用的指导建议，以应对大型负载并网带来的技术挑战。

Abstract: Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.

</details>


### [6] [PRIMAL: Processing-In-Memory Based Low-Rank Adaptation for LLM Inference Accelerator](https://arxiv.org/abs/2601.13628)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: PRIMAL是一个基于存内计算(PIM)的大型语言模型推理加速器，采用低秩适应(LoRA)技术，通过异构PIM处理单元、2D网格互连网络和创新的SRAM重编程与功耗门控方案，实现了高效能推理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模不断扩大，传统GPU架构在推理时面临内存带宽瓶颈和能耗问题。存内计算技术有望解决这些挑战，但需要针对LoRA等参数高效微调方法进行专门优化。

Method: PRIMAL采用异构PIM处理单元通过2D网格互连网络连接，提出SRAM重编程与功耗门控方案实现流水线化的LoRA更新和亚线性功耗扩展，通过优化的空间映射和数据流编排最小化通信开销。

Result: 在Llama-13B模型上，PRIMAL相比NVIDIA H100 GPU，在LoRA秩为8(Q,V)时实现了1.5倍的吞吐量和25倍的能效提升。

Conclusion: PRIMAL展示了PIM架构在LLM推理加速方面的巨大潜力，特别是针对LoRA等参数高效微调方法，通过创新的硬件架构设计实现了显著的性能和能效优势。

Abstract: This paper presents PRIMAL, a processing-in-memory (PIM) based large language model (LLM) inference accelerator with low-rank adaptation (LoRA). PRIMAL integrates heterogeneous PIM processing elements (PEs), interconnected by 2D-mesh inter-PE computational network (IPCN). A novel SRAM reprogramming and power gating (SRPG) scheme enables pipelined LoRA updates and sub-linear power scaling by overlapping reconfiguration with computation and gating idle resources. PRIMAL employs optimized spatial mapping and dataflow orchestration to minimize communication overhead, and achieves $1.5\times$ throughput and $25\times$ energy efficiency over NVIDIA H100 with LoRA rank 8 (Q,V) on Llama-13B.

</details>


### [7] [The Non-Predictability of Mispredicted Branches using Timing Information](https://arxiv.org/abs/2601.13804)
*Ioannis Constantinou,Arthur Perais,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 研究探索使用微架构信息（分支时序）辅助传统分支历史预测，提出SBR方法，但实验显示对整体性能提升有限，仅在特定难预测分支上有效。


<details>
  <summary>Details</summary>
Motivation: 分支预测错误是现代处理器性能下降和能耗浪费的主要原因。现有预测器虽然表现良好，但在难预测分支上仍有较高误预测率。本研究探索是否可以通过结合微架构信息（除传统分支历史外）来提高预测准确性。

Method: 提出Speculative Branch Resolution (SBR)方法：在分支分配到ROB后N个周期，收集各种时序信息（包括ROB中较老分支、已提交分支以及相对于当前预测分支的较新分支的分支时序信息），并利用这些信息重新进行预测。使用gem5模拟器实现SBR，并基于TAGE-Like预测器进行极限研究。

Result: 实验表明，所使用的分配后时序信息未能带来超越无限制TAGE-SC的性能增益。但在两个难预测分支上，时序信息确实提供了优势，并深入分析了其中一个案例以理解原因。

Conclusion: 预测器可能受益于特定的微架构信息来提高特定难预测分支的准确性，后端覆盖预测可能带来性能优势，但需要进一步研究确定有效的信息向量。

Abstract: Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.

</details>


### [8] [From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs](https://arxiv.org/abs/2601.13815)
*Lukas Krupp,Matthew Venn,Norbert Wehn*

Main category: cs.AR

TL;DR: 本文提出一个基于LLM的芯片设计教育平台，通过AI助手简化流程，让初学者能在90分钟内完成从设计到流片的完整芯片设计。


<details>
  <summary>Details</summary>
Motivation: 传统芯片设计技术门槛高，初学者容易被技术复杂性吓退。需要一种能让新手轻松入门的教育平台，同时覆盖前端和后端设计全流程。

Method: 基于Tiny Tapeout生态系统构建浏览器工作流，集成LLM聊天助手，引导用户从设计想法到RTL代码生成，最终完成流片准备。

Result: 18名高中生参与案例研究，在90分钟内开发出8个功能完整的VGA芯片设计（130nm工艺）。所有小组都成功实现了流片准备项目。

Conclusion: LLM辅助的芯片设计具有可行性和教育价值，能吸引早期学习者，显著扩大芯片设计领域的受众范围。

Abstract: This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.

</details>


### [9] ['1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators](https://arxiv.org/abs/2601.14087)
*Ruichi Han,Yizhi Chen,Tong Lei,Jordi Altayo Gonzalez,Ahmed Hemani*

Main category: cs.AR

TL;DR: 提出一种针对CNN优化的免比较排序单元硬件实现，通过近似计算将人口计数分组到粗粒度桶中，在保持数据重排序功耗优势的同时减少硬件面积。


<details>
  <summary>Details</summary>
Motivation: 互连功耗是DNN加速器的主要瓶颈。虽然基于'1'位计数的数据排序可以通过减少切换活动来缓解此问题，但实用的硬件排序实现仍未被充分探索。

Method: 设计针对CNN优化的免比较排序单元硬件实现，利用近似计算将人口计数分组到粗粒度桶中，从而减少硬件面积。

Result: 近似排序单元实现了高达35.4%的面积减少，同时保持了19.50%的BT减少，与精确实现的20.42%相比表现良好。

Conclusion: 通过近似计算实现的硬件排序单元在保持互连功耗优势的同时，显著减少了硬件面积，为DNN加速器的互连功耗优化提供了实用的硬件解决方案。

Abstract: Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction compared to 20.42% of precise implementation.

</details>


### [10] [CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems](https://arxiv.org/abs/2601.14140)
*Tong Xie,Yijiahao Qi,Jinqi Wen,Zishen Wan,Yanchi Dong,Zihao Wang,Shaofei Cai,Yitao Liang,Tianyu Jia,Yuan Wang,Runsheng Wang,Meng Li*

Main category: cs.AR

TL;DR: CREATE提出了一种异构弹性设计原则，通过在电路层、模型层和应用层协同优化，实现具身AI系统的能效与可靠性协同提升，平均节省40.6%计算能耗。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统结合LLM规划器和RL控制器，但部署在电池供电设备上面临高计算能耗挑战。降低工作电压可提升能效，但会引入比特错误导致任务失败，需要解决能效与可靠性的权衡问题。

Method: 1) 对现代具身AI系统进行全面错误注入研究，发现异构容错特性；2) 电路层：异常检测与清除机制消除异常错误；3) 模型层：权重旋转增强规划算法提升LLM规划器容错性；4) 应用层：自主适应性电压缩放动态调整控制器工作电压；5) 协同设计电压缩放电路支持在线电压调整。

Result: 在不影响任务质量的前提下，CREATE相比标称电压基线平均节省40.6%计算能耗，相比现有技术节省35.0%。芯片级能耗节省29.5%-37.3%，电池寿命提升约15%-30%。

Conclusion: CREATE通过跨层异构弹性设计，成功实现具身AI系统的能效与可靠性协同优化，为电池供电设备上的具身AI部署提供了有效的解决方案。

Abstract: Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.

</details>


### [11] [The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization](https://arxiv.org/abs/2601.14148)
*Meng Li,Tong Xie,Zuodong Zhang,Runsheng Wang*

Main category: cs.AR

TL;DR: 本文提出了一种跨层可靠性感知的AI加速器设计方法，通过整合老化/变异感知时序分析、数据流优化和LLM弹性架构，解决传统防护带设计在性能和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着CMOS技术进入纳米尺度，老化效应和工艺变异日益显著，对AI加速器的可靠性构成严峻挑战。传统基于防护带的设计方法依赖悲观时序裕量，牺牲了大量性能和计算效率，无法满足高性能AI计算需求。当前可靠性感知的AI加速器设计面临两大核心挑战：缺乏系统性的跨层分析工具来捕捉设备、电路、架构和应用层之间的耦合可靠性效应；以及传统可靠性优化与计算效率之间的根本性权衡。

Method: 本文系统性地提出了一系列可靠性感知的加速器设计方法，包括：(1) 老化与变异感知的动态时序分析器；(2) 使用关键输入模式减少的加速器数据流优化；(3) 针对大语言模型(LLMs)的弹性特性表征和新颖架构设计。通过紧密整合跨层可靠性建模和AI工作负载特性，这些协同优化方法有效实现了可靠且高效的AI加速。

Result: 通过提出的跨层协同优化方法，能够有效解决传统防护带设计在性能和效率上的不足，实现可靠且高效的AI加速器设计。

Conclusion: 本文提出的可靠性感知AI加速器设计方法通过跨层协同优化，成功解决了纳米尺度CMOS技术下的可靠性挑战，在保证可靠性的同时显著提升了计算效率，为高性能AI计算提供了有效的解决方案。

Abstract: As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration](https://arxiv.org/abs/2601.11556)
*Boyang Wang,Yash Vishe,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: CSyMR-Bench是一个用于评估LLMs在符号音乐推理中组合性推理能力的基准，包含126个多选问题，并提出了基于music21工具增强的代理框架来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注孤立的知识或原子分析，缺乏对连接音乐结构的组合性推理能力的评估，而这是音乐理解和创作中的关键能力。

Method: 1) 创建CSyMR-Bench基准，包含126个从专家论坛和专业考试中提取的多选问题，每个问题需要组合多个原子分析；2) 提出基于music21库的工具增强代理框架，利用符号音乐分析工具来解决CSyMR-Bench中的挑战。

Result: CSyMR-Bench对现有模型构成了非平凡的挑战，而提出的工具增强代理框架在所有基线方法中表现最佳，实现了5-7%的绝对准确率提升。

Conclusion: 该研究强调了组合性音乐推理的重要性，提出的CSyMR-Bench基准和工具增强代理框架为评估和提升LLMs在符号音乐推理中的能力提供了有效工具。

Abstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.

</details>


### [13] [Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs](https://arxiv.org/abs/2601.12341)
*Rezky Kam,Coddy N. Siswanto*

Main category: cs.LG

TL;DR: 提出数据集和概念框架，让LLM通过物理信息神经网络模拟真实世界情感动态，实现可解释的对话建模


<details>
  <summary>Details</summary>
Motivation: 当前LLM在模拟真实世界情感动态方面存在局限，缺乏时间维度和可解释性，需要新的方法来捕捉情感随时间变化的复杂动态

Method: 结合物理信息神经网络（PINN）和上下文学习，构建能够模拟情感动态的概念框架，并创建相应数据集

Result: 开发了能够模拟真实世界情感动态的框架，为可解释的对话建模开辟了新可能性

Conclusion: 提出的方法为LLM理解复杂情感动态提供了新途径，增强了对话系统的可解释性和真实性

Abstract: This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.

</details>


### [14] [AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control](https://arxiv.org/abs/2601.11568)
*Quang-Hung Bui,Anh Son Ta*

Main category: cs.LG

TL;DR: AdaFRUGAL通过动态控制梯度分割的超参数（子空间比例ρ和更新频率T），自动优化内存使用和计算开销，在保持性能的同时显著减少LLM训练的内存和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练内存需求高，现有FRUGAL框架的静态超参数需要手动调优，成本高且适应性差，需要自动化解决方案。

Method: 提出AdaFRUGAL框架，引入两种动态控制机制：1) 子空间比例ρ的线性衰减策略，逐步减少内存占用；2) 基于损失感知的更新频率T调度，降低计算开销。

Result: 在英文C4、越南语VietVault的大规模预训练和GLUE微调实验中，AdaFRUGAL在保持与AdamW和静态FRUGAL相当性能的同时，显著减少了GPU内存使用和训练时间。

Conclusion: AdaFRUGAL为资源受限的LLM训练提供了更实用、自主的解决方案，实现了内存、时间和性能的良好平衡。

Abstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($ρ$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $ρ$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.

</details>


### [15] [NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness](https://arxiv.org/abs/2601.13162)
*Ali Shafiee Sarvestani,Jason Schmidt,Arman Roohi*

Main category: cs.LG

TL;DR: Neuro-symbolic框架DesignII通过符号规则监督增强神经网络的对抗鲁棒性和可解释性，在GTSRB数据集上相比标准对抗训练获得约3倍的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络存在对抗脆弱性和缺乏可解释性的关键限制，特别是在自动驾驶等安全敏感场景中，需要同时提升鲁棒性和可解释性。

Method: 提出DesignII神经符号框架，将领域知识编码为形状、颜色等外观属性的逻辑约束，通过语义和符号逻辑损失在训练中强制执行这些约束。

Result: 在GTSRB数据集上，FGSM-Neuro-Symbolic和PGD-Neuro-Symbolic模型相比对应对抗训练基线分别提升18.1%和17.35%的对抗精度，鲁棒性增益约为标准对抗训练的3倍，且不降低干净样本准确率。

Conclusion: 符号推理为构建鲁棒且可解释的AI提供了有效路径，神经符号方法能在轻量架构上实现与复杂Transformer防御相当或更好的鲁棒性。

Abstract: Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\% and 17.35\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.

</details>


### [16] [Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces](https://arxiv.org/abs/2601.11572)
*Timo Aukusti Laine*

Main category: cs.LG

TL;DR: 使用线性代数和哈密顿形式等数学工具分析LLM嵌入空间结构，发现L2归一化约束使嵌入空间适合哈密顿分析，并探索了量子力学类比


<details>
  <summary>Details</summary>
Motivation: 观察到LLM嵌入表现出离散的语义状态，表明存在离散语义表示，因此探索应用数学工具分析语义关系

Method: 应用线性代数和哈密顿形式分析LLM嵌入空间，推导余弦相似度与嵌入向量扰动的关系，探索直接和间接语义转换，并从量子力学角度推导零点能类比

Result: L2归一化约束使嵌入空间具有适合哈密顿分析的结构，建立了余弦相似度与嵌入扰动的关系，探索了量子力学类比（包括零点能和Koopman-von Neumann力学）

Conclusion: 这种方法为深入理解LLM提供了有前景的途径，可能有助于开发减少幻觉的新方法，但解释需要谨慎考虑

Abstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.

</details>


### [17] [GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment](https://arxiv.org/abs/2601.11574)
*Lukas Abrie Nel*

Main category: cs.LG

TL;DR: GRADE使用Gumbel-softmax重参数化和直通估计替代传统强化学习方法，实现了从奖励信号到模型参数的端到端梯度传播，在文本对齐任务中表现更优、更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统的基于人类反馈的强化学习方法（如PPO）存在梯度估计方差高、需要大量超参数调优和计算资源的问题，需要更简单稳定的替代方案。

Method: 提出GRADE方法，使用Gumbel-softmax重参数化和直通估计（GRADE-STE），通过离散token采样过程的可微分松弛实现直接反向传播，替代高方差的策略梯度估计。

Result: 在IMDB数据集的情感控制文本生成任务中，GRADE-STE获得0.763±0.344的测试奖励，优于PPO的0.510±0.313和REINFORCE的0.617±0.378，梯度方差比REINFORCE低14倍以上，训练动态更稳定。

Conclusion: GRADE为LLM对齐提供了一种更简单、更稳定、更有效的强化学习替代方案，具有更好的泛化特性。

Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.

</details>


### [18] [Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.11604)
*Jonaid Shianifar,Michael Schukat,Karl Mason*

Main category: cs.LG

TL;DR: Hindsight Preference Replay (HPR) 是一种简单的回放增强策略，通过重新标记存储的转移数据来利用其他偏好的离线数据，从而在多目标强化学习中提高性能。


<details>
  <summary>Details</summary>
Motivation: CAPQL方法在多目标强化学习中只使用特定偏好下收集的数据，导致其他偏好的离线数据未被利用，限制了学习效率。

Method: 提出Hindsight Preference Replay (HPR)，一种回放增强策略，通过追溯性地用替代偏好重新标记存储的转移数据，在不改变CAPQL架构或损失函数的情况下，增加偏好单纯形上的监督密度。

Result: 在6个MO-Gymnasium运动任务中，HPR-CAPQL在5个环境中提高了超体积(HV)，在4个环境中提高了期望效用(EUM)。例如在mo-humanoid-v5中，EUM从323±125提升到1613±464，HV从0.52M提升到9.63M。

Conclusion: HPR是一种简单有效的策略，能够显著提高多目标强化学习的性能，通过更好地利用离线数据来增强学习效率。

Abstract: Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\!\pm\!125$ to $1613\!\pm\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.

</details>


### [19] [A Multimodal Data Processing Pipeline for MIMIC-IV Dataset](https://arxiv.org/abs/2601.11606)
*Farzana Islam Adiba,Varsha Danduri,Fahmida Liza Piya,Ali Abbasi,Mehak Gupta,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 提出了一个针对MIMIC-IV数据集的多模态数据处理管道，能够自动化处理结构化数据、临床笔记、波形和影像数据，显著减少处理时间并提高研究可重复性。


<details>
  <summary>Details</summary>
Motivation: MIMIC-IV数据集包含多种模态的医疗数据，但现有处理工具要么只针对少数模态，要么不支持任意的下游应用。处理这些分散的模态需要大量人工预处理和对齐工作。

Method: 扩展了先前流行的单模态管道，开发了一个全面的可定制多模态管道。该系统能够自动进行队列选择、跨模态时间对齐，并生成适用于任意静态和时间序列下游应用的标准多模态输出格式。

Result: 开发了一个完整的处理管道，包括代码、简单UI和Python包，支持选择性集成（含嵌入）。该管道能显著减少多模态处理时间，增强基于MIMIC研究的可重复性。

Conclusion: 提出的多模态管道解决了MIMIC-IV数据处理中的关键挑战，为临床机器学习研究提供了高效、可重复的工具，有望促进该领域的研究发展。

Abstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.

</details>


### [20] [Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction](https://arxiv.org/abs/2601.11609)
*Weinuo Ou*

Main category: cs.LG

TL;DR: 提出ApCM模型，一种神经记忆存储架构，解决LLMs缺乏有效运行时记忆机制的问题


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型缺乏有效的运行时记忆机制，难以适应动态和个性化的交互需求

Method: 提出辅助预测压缩记忆模型（ApCM模型），一种新颖的神经记忆存储架构

Result: 论文仅提供了方法介绍，未给出具体实验结果

Conclusion: ApCM模型旨在解决LLMs的记忆机制问题，提升其对动态个性化交互的适应能力

Abstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).

</details>


### [21] [Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home](https://arxiv.org/abs/2601.11611)
*Marina Vicini,Martin Rudorfer,Zhuangzhuang Dai,Luis J. Manso*

Main category: cs.LG

TL;DR: 提出一种结合时间聚类和循环时间特征的人体活动识别方法，在智能家居环境中通过被动传感器监测老年人日常活动，在低数据场景下显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口老龄化，需要让老年人能够独立安全地在家中生活。使用被动红外传感器和门传感器等普遍存在的传感器来监测日常活动并促进预防性医疗干预变得越来越重要。然而，现有方法在有效利用时间信息方面存在挑战。

Method: 将活动按早晨、下午和晚上进行聚类，并将这些时间信息编码到特征加权方法中，计算不同的互信息矩阵。进一步扩展特征向量，加入一天中的时间和一周中的天数作为循环时间特征，并添加用户位置跟踪特征。

Result: 在四个真实世界数据集中的三个上，相比现有最先进方法，准确率和F1分数都有所提高，在低数据场景下获得最高增益。

Conclusion: 该方法展示了开发有效智能家居解决方案以支持老年人居家养老的潜力，通过更好地利用时间信息提升了人体活动识别的性能。

Abstract: With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.

</details>


### [22] [A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia](https://arxiv.org/abs/2601.11615)
*Beyza Cinar,Louisa van den Boom,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本文综述了基于连续血糖监测数据的机器学习模型在1型糖尿病低血糖预测中的应用，比较了不同预测时间窗口下的模型性能，发现1小时内的短期预测效果最佳，传统ML方法在分类任务中表现更好，而深度学习在回归任务中更优。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病患者需要终身胰岛素治疗，但胰岛素治疗有导致低血糖的副作用。低血糖（血糖低于70 mg/dL）会增加死亡风险，因此需要准确预测低血糖事件以改善糖尿病管理。

Method: 综述分析了基于连续血糖监测数据的机器学习模型，包括回归模型（预测血糖水平）和分类模型（识别低血糖事件）。比较了短期（15-120分钟）和长期（3-24小时以上）预测时间窗口下的模型性能，并探讨了四个关键问题：预测时间窗口、最佳模型类型、影响因素以及个性化模型的效果。

Result: 1) 1小时内的预测时间窗口效果最佳；2) 传统机器学习方法在分类任务中表现最好，深度学习在回归任务中更优，单个模型无法在多个预测时间窗口上都有良好表现；3) 多变量数据集和输入序列长度影响模型性能；4) 个性化数据能提升性能，但由于数据质量限制，基于人群的模型更受青睐。

Conclusion: 机器学习模型能够有效预测1型糖尿病患者的低血糖事件，短期预测（1小时内）效果最佳。模型选择应根据具体任务（分类或回归）而定，且需要考虑多变量数据和输入序列长度的影响。虽然个性化模型有潜力，但目前基于人群的模型在实际应用中更为可行。

Abstract: Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.

</details>


### [23] [Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective](https://arxiv.org/abs/2601.11616)
*Feilong Liu*

Main category: cs.LG

TL;DR: MoE架构通过路由机制将表示空间软划分为重叠的局部区域，降低了局部敏感性，提高了表示的有效秩，并将变换分解为专家特定的低重叠子空间。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构对学习函数和表示几何结构的影响，理解路由机制如何塑造函数空间和表示空间的几何特性。

Method: 引入双重雅可比-PCA谱几何探针，通过雅可比奇异值谱分析局部函数几何，通过加权PCA分析路由隐藏状态的表示几何。在可控的MLP-MoE设置中比较密集、Top-k和全软路由架构。

Result: MoE路由一致降低局部敏感性，专家局部雅可比矩阵具有更小的主导奇异值和更快的谱衰减。加权PCA显示专家局部表示在更多主方向上分布方差，表明在相同输入分布下具有更高的有效秩。平均专家雅可比矩阵接近正交，表明变换被分解为低重叠的专家特定子空间。

Conclusion: MoE可被几何解释为函数空间的软划分，它平坦化局部曲率同时重新分配表示方差。Top-k路由产生更低秩、更集中的专家局部结构，而全软路由产生更广泛、更高秩的表示。

Abstract: Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local function geometry via Jacobian singular-value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting that permits exact Jacobian computation, we compare dense, Top-k, and fully-soft routing architectures under matched capacity. Across random seeds, we observe that MoE routing consistently reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay than dense baselines. At the same time, weighted PCA reveals that expert-local representations distribute variance across a larger number of principal directions, indicating higher effective rank under identical input distributions. We further find that average expert Jacobians are nearly orthogonal, suggesting a decomposition of the transformation into low-overlap expert-specific subspaces rather than scaled variants of a shared map. We analyze how routing sharpness modulates these effects, showing that Top-k routing produces lower-rank, more concentrated expert-local structure, while fully-soft routing yields broader, higher-rank representations. Together, these results support a geometric interpretation of MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance.

</details>


### [24] [Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention](https://arxiv.org/abs/2601.11618)
*Luis Rosario Freytes*

Main category: cs.LG

TL;DR: 几何注意力(GA)通过四个独立输入定义注意力层：载体、证据核规则、探针族和锚定/更新规则，将不变结构与建模选择分离，为注意力机制提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制缺乏统一的数学框架，难以进行系统比较和扩展。本文旨在建立几何注意力理论，分离注意力机制的不变结构与具体建模选择，为注意力机制提供理论基础。

Method: 提出几何注意力框架，通过四个组件定义：有限载体（可寻址索引）、证据核规则（掩码原分数和链接产生非负权重）、探针族（可观测量的集合）、锚定/更新规则（核选择和应用方式）。探针族诱导核上的操作等价关系和规范，锚定选择相对于探针的代表核。

Result: 在标量关系工作表示和证据乘法组合律下，可容许链接族是指数形式，产生吉布斯权重；行锚定包含softmax核族作为子机制。商掉一元行/列分数场后，剩余交互分量具有规范秩r正规形式（Eckart-Young/SVD）；点积分数图实现相应的低秩交互机制。

Conclusion: 几何注意力框架将注意力机制的不变结构与具体建模选择分离，支持多头/混合核、基于计划的锚定（如熵最优传输/Sinkhorn）和一元算子等机制，为注意力机制和基于注意力的架构提供了系统比较和扩展的理论基础。

Abstract: Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.

</details>


### [25] [NoiseFormer -- Noise Diffused Symmetric Attention Transformer](https://arxiv.org/abs/2601.11619)
*Phani Kumar,Nyshadham,Jyothendra Varma,Polisetty V R K,Aditya Rathore*

Main category: cs.LG

TL;DR: 提出一种新型的噪声扩散对称注意力Transformer架构，在保持对称注意力内存优势的同时，通过微小参数和计算开销提升模型性能，在GPT2基础模型上验证了在GLUE基准任务上的准确率提升和显著模型大小缩减。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型规模急剧增大，内存占用导致难以在单设备上部署，需要多设备计算从而增加成本。稀疏注意力技术成为模型压缩的有效途径，但现有对称注意力方法在性能上仍有提升空间。

Method: 提出噪声扩散对称注意力Transformer架构，在对称点积注意力基础上引入噪声扩散机制，保持对称注意力的内存优势同时增强模型表达能力，仅增加微小参数和计算开销。

Result: 在GPT2基础模型上验证，在多个GLUE基准任务上，准确率介于原始对称注意力和GPT2基础模型之间，同时实现了显著的模型大小缩减。

Conclusion: 噪声扩散对称注意力Transformer在保持内存效率的同时有效提升了模型性能，为大规模语言模型的效率优化提供了新的解决方案。

Abstract: Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.

</details>


### [26] [Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition](https://arxiv.org/abs/2601.13953)
*Gorgi Pavlov*

Main category: cs.LG

TL;DR: 提出分层谱组合架构，通过选择布尔傅里叶基的谱系数，结合Sinkhorn约束路由和列符号调制，实现精确布尔逻辑学习，支持硬件高效的神经符号逻辑合成。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络通过梯度下降学习布尔逻辑时，通常收敛到"模糊"近似，在量化下性能下降。需要一种能够学习精确布尔逻辑的架构，支持硬件高效实现。

Method: 分层谱组合架构：从冻结的布尔傅里叶基中选择谱系数，通过Sinkhorn约束路由进行组合，并加入列符号调制实现布尔否定。基于流形约束超连接框架，将路由矩阵投影到Birkhoff多面体上。

Result: 1) n=2时，梯度下降达到100%准确率；2) n=3时，梯度下降76%准确率，但枚举证明存在最优三元掩码(100%准确率，39%稀疏度)；3) n=4时，谱合成方法达到100%准确率。GPU上实现单周期组合逻辑推理，速度达10,959 MOps/s。

Conclusion: 证明了所有测试函数都存在三元多项式阈值表示，但随着维度增加，需要超越纯梯度下降的方法。该方法支持硬件高效的神经符号逻辑合成，在GPU上实现高速推理。

Abstract: Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to "fuzzy" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.
  We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.

</details>


### [27] [Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System](https://arxiv.org/abs/2601.11638)
*Josafat Ribeiro Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 该论文提出使用Fisher信息（g_F^C）作为评估PINNs学习物理系统完整动力学行为的定量框架，通过比较PINN学习模型与原始解析模型的Fisher信息景观来验证PINN的保真度。


<details>
  <summary>Details</summary>
Motivation: 当前PINNs在解决微分方程和建模物理系统方面表现出色，但缺乏量化PINN是否完整捕捉系统动力学行为（而不仅仅是轨迹预测）的严格方法。需要一种能够评估PINN是否准确学习到系统内在不确定性、相空间曲率和稳定性等关键几何特性的评估框架。

Method: 提出使用可微动力系统的Fisher信息（g_F^C）作为评估指标，该信息不同于统计Fisher信息，用于测量确定性系统中的内在不确定性（如对初始条件的敏感性）。通过计算PINN学习到的运动方程与原始解析模型的Jacobian矩阵，比较两者的Fisher信息景观，以定量评估PINN的保真度。实验方法采用汽车动力学模型作为案例研究。

Result: 论文提出了完整的实验方法论，但未提供具体实验结果。该方法理论上能够通过比较PINN学习模型与原始解析模型的Fisher信息景观，定量评估PINN是否准确捕捉了系统的完整动力学特性，包括相空间曲率和稳定性等几何性质。

Conclusion: 提出的基于Fisher信息（g_F^C）的评估框架为PINNs的保真度评估提供了新的定量方法，能够验证PINN是否不仅准确预测状态演化，还完整捕捉了系统的关键几何和稳定性特性，这对于确保PINN在实际物理系统建模中的可靠性具有重要意义。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.

</details>


### [28] [Global Optimization By Gradient from Hierarchical Score-Matching Spaces](https://arxiv.org/abs/2601.11639)
*Ming Li*

Main category: cs.LG

TL;DR: 该论文提出了一种通过分数匹配获取梯度，将带复杂约束的优化问题统一为无约束分层优化目标的新方法，首次实现了使用严格梯度的确定性全局优化，并揭示了全局优化与基于扩散的生成建模之间的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 梯度下降法作为最常用的优化方法，存在局部最优性限制，且仅适用于具有简单凸约束的连续可微问题。需要突破这些限制，处理具有各种复杂约束的优化问题。

Method: 通过分数匹配获取梯度，将所有具有复杂约束的优化问题统一为无约束的一般分层优化目标，使用确定性方法进行全局优化。

Result: 首次实现了使用严格梯度的确定性全局优化，并通过简单构造和复杂实际实验验证了方法的有效性。

Conclusion: 该方法不仅突破了传统梯度下降的局限性，更重要的是揭示了全局优化与基于扩散的生成建模之间的深刻联系，为优化领域提供了新的理论视角。

Abstract: Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.

</details>


### [29] [Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning](https://arxiv.org/abs/2601.11657)
*Jack T. Beerman,Shobhan Roy,H. S. Udaykumar,Stephen S. Baek*

Main category: cs.LG

TL;DR: D-PARC（可变形物理感知循环卷积）通过受混合拉格朗日-欧拉方法启发的可变形卷积核，在小型网络中实现了比大型网络更优的物理模拟精度，展示了物理直觉架构设计优于参数扩展。


<details>
  <summary>Details</summary>
Motivation: 当前CNN架构在处理高度非线性流体时存在困难，而单纯扩大模型规模对物理建模效果有限。需要借鉴计算力学中的自适应方法，设计更符合物理直觉的神经网络架构。

Method: 提出D-PARC（可变形物理感知循环卷积），受混合拉格朗日-欧拉数值方法启发，使用可变形卷积核替代传统CNN的刚性卷积核，使网络能够自适应地调整感受野。

Result: 在Burgers方程、Navier-Stokes方程和反应流等多个物理系统中，D-PARC在较小网络规模下实现了比大型架构更优的模拟精度。分析显示卷积核表现出反聚集行为，形成了独特的"主动过滤"策略。

Conclusion: 物理直觉的架构设计优于参数扩展，精简网络的策略性学习为物理感知深度学习提供了更有效的路径，D-PARC能够自主地在高应变区域集中计算资源，类似于计算力学中的自适应细化。

Abstract: Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. While scaling model size addresses complexity in broader AI, this approach yields diminishing returns for physics modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs. Across Burgers' equation, Navier-Stokes, and reactive flows, D-PARC achieves superior fidelity compared to substantially larger architectures. Analysis reveals that kernels display anti-clustering behavior, evolving into a learned "active filtration" strategy distinct from traditional h- or p-adaptivity. Effective receptive field analysis confirms that D-PARC autonomously concentrates resources in high-strain regions while coarsening focus elsewhere, mirroring adaptive refinement in computational mechanics. This demonstrates that physically intuitive architectural design can outperform parameter scaling, establishing that strategic learning in lean networks offers a more effective path forward for PADL than indiscriminate network expansion.

</details>


### [30] [Machine learning model for predicting surface wettability in laser-textured metal alloys](https://arxiv.org/abs/2601.11661)
*Mohammad Mohammadzadeh Sanandaji,Danial Ebrahimzadeh,Mohammad Ikram Haider,Yaser Mike Banad,Aleksandar Poleksic,Hongtao Ding*

Main category: cs.LG

TL;DR: 机器学习框架通过形态和化学特征准确预测激光纹理金属合金的润湿性，R²达0.942，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表面润湿性由形貌和化学性质共同决定，在传热、润滑、微流控等领域至关重要。传统方法难以准确预测复杂表面特性对润湿性的影响，需要数据驱动的方法来理解这些复杂相互作用。

Method: 使用纳秒激光纹理化结合化学浸渍处理在AA6061和AISI 4130合金上制备超亲水和超疏水表面。通过Laws纹理能量法和轮廓测量量化表面形貌，通过XPS表征表面化学性质，提取功能基团极性、分子体积、峰面积分数等特征。使用包含残差连接、批归一化和dropout正则化的集成神经网络模型进行训练。

Result: 模型达到高预测精度（R² = 0.942，RMSE = 13.896），优于先前方法。特征重要性分析显示表面化学对接触角预测影响最大，形貌特征也有显著贡献。

Conclusion: 该研究展示了人工智能通过捕捉表面特性的复杂相互作用来建模和预测润湿行为的潜力，为设计定制功能表面提供了数据驱动的途径。

Abstract: Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing followed by chemical immersion treatments. Surface morphology was quantified using the Laws texture energy method and profilometry, while surface chemistry was characterized through X-ray photoelectron spectroscopy (XPS), extracting features such as functional group polarity, molecular volume, and peak area fraction. These features were used to train an ensemble neural network model incorporating residual connections, batch normalization, and dropout regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE = 13.896), outperforming previous approaches. Feature importance analysis revealed that surface chemistry had the strongest influence on contact angle prediction, with topographical features also contributing significantly. This work demonstrates the potential of artificial intelligence to model and predict wetting behavior by capturing the complex interplay of surface characteristics, offering a data-driven pathway for designing tailored functional surfaces.

</details>


### [31] [Activation Sensitivity as a Unifying Principle for Post-Training Quantization](https://arxiv.org/abs/2601.11663)
*Bruce Changlong Xu*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架来理解后训练量化方法，通过形式化激活敏感度概念，将AWQ和GPTQ等现有方法解释为对敏感度的不同近似。


<details>
  <summary>Details</summary>
Motivation: 当前后训练量化方法（如AWQ和GPTQ）依赖于启发式方法，但缺乏统一的理论基础。这些方法概念上分散，不清楚它们近似的是什么底层量。需要建立一个理论框架来统一理解这些方法。

Method: 通过一阶泰勒展开，形式化定义激活敏感度（通道扰动对损失的期望影响），将其表示为梯度加权激活的平方范数。在此框架下分析AWQ和GPTQ作为敏感度在不同简化假设下的近似。

Result: 建立了后训练量化的统一理论框架，将激活敏感度作为通道重要性的原则性度量。展示了AWQ和GPTQ可以解释为互补的近似方法，并连接了梯度显著性、Fisher信息和Hessian准则。

Conclusion: 该工作为理解后训练量化方法提供了概念基础，通过敏感度视角统一了现有方法，澄清了它们与经典剪枝方法的关系，为量化方法的设计和比较提供了理论指导。

Abstract: Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, these approaches remain conceptually fragmented, and it is unclear what underlying quantity they are approximating. In this work, we present a unified theoretical framework for PTQ by formalizing activation sensitivity, defined as the expected impact of channel-wise perturbations on the loss. Using a first-order Taylor expansion, we show that sensitivity naturally arises as the squared norm of gradient-weighted activations, yielding a principled measure of channel importance that captures both activation magnitude and downstream error propagation. Within this framework, AWQ and GPTQ can be interpreted as complementary approximations that recover sensitivity under distinct simplifying assumptions. We analyze the design space of sensitivity metrics, connect gradient-based saliency, Fisher information, and Hessian-based criteria, and clarify their relationships to classical pruning methods such as Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new quantization algorithm, this work provides a conceptual foundation for understanding and comparing post-training quantization methods through the lens of sensitivity.

</details>


### [32] [Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction](https://arxiv.org/abs/2601.11667)
*Xiaojie Xia,Huigang Zhang,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.LG

TL;DR: 提出一种高效方法将预训练全注意力Transformer转换为混合注意力模型，通过权重迁移和贪心层替换策略，在保持性能的同时实现线性复杂度


<details>
  <summary>Details</summary>
Motivation: 全注意力Transformer具有二次复杂度，限制了实际部署；线性注意力虽然高效但性能下降；混合模型需要昂贵训练且手动设计困难

Method: 1) 通过块级局部蒸馏将预训练全注意力权重迁移到线性注意力模块；2) 引入贪心层替换策略，迭代替换全注意力块为线性注意力块，同时监控验证性能

Result: 该方法能在单次高效过程中获得任务特定的混合模型，无需昂贵重新训练或神经架构搜索，适用于各种预训练全注意力骨干和下游任务

Conclusion: 提出的方法有效解决了混合注意力模型训练昂贵和设计困难的问题，实现了效率与表达能力的平衡，具有广泛适用性

Abstract: Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: training such hybrid models from scratch is computationally expensive, and manually designing the optimal placement of attention types is highly nontrivial. We address both issues by first transferring weights from the pretrained full-attention modules to its linear attention counterparts through blockwise local distillation, and second, introducing a greedy layer replacement strategy that iteratively substitutes full attention blocks with linear ones while monitoring validation performance on the target task. This yields a task-specific hybrid model in a single efficient pass, without costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for diverse downstream tasks.

</details>


### [33] [IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning](https://arxiv.org/abs/2601.11669)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Xiaofeng Yang,Qingchao Jiang,Xuefeng Yan*

Main category: cs.LG

TL;DR: IPEC是一种测试时方法，通过利用先前查询样本的信息优化原型估计，在少样本分类任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于度量的少样本方法在测试时遵循批次独立性假设，无法利用先前批次积累的宝贵知识，限制了性能提升。

Method: 提出增量原型增强分类器(IPEC)，维护动态辅助集，通过双重过滤机制选择高置信度查询样本，与支持集聚合构建更稳定的原型，采用贝叶斯解释和"预热-测试"两阶段推理协议。

Result: 在多个少样本分类任务上的广泛实验验证了IPEC的优越性能。

Conclusion: IPEC通过测试时利用先前查询样本信息，有效减少了对初始支持集的依赖，构建了更稳定和具有代表性的原型，提升了少样本分类性能。

Abstract: Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical "warm-up and test" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.

</details>


### [34] [A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning](https://arxiv.org/abs/2601.11670)
*Jinshi Liu,Pan Liu*

Main category: cs.LG

TL;DR: 提出CoVar理论框架，通过结合最大置信度和残差类别方差来改进半监督学习中的伪标签选择，避免传统固定置信度阈值导致的过自信问题。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习中的伪标签选择策略依赖固定置信度阈值，假设预测置信度能可靠指示正确性。但实际上深度网络经常过自信：高置信度预测可能错误，而有信息量的低置信度样本（靠近决策边界）却被丢弃。

Method: 从熵最小化原则出发，推导出结合最大置信度(MC)和残差类别方差(RCV)的可靠性度量。将伪标签选择建模为置信度-方差特征空间中的谱松弛问题，设计无阈值选择机制来区分高可靠性和低可靠性预测。

Result: 在PASCAL VOC 2012、Cityscapes、CIFAR-10和Mini-ImageNet数据集上，使用不同标签比例和骨干网络，CoVar作为插件模块能持续改进强基线方法。

Conclusion: 结合置信度和残差类别方差比固定置信度阈值提供了更可靠的伪标签选择基础，能有效纠正过自信但不稳定的预测。

Abstract: Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)

</details>


### [35] [Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis](https://arxiv.org/abs/2601.11686)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 提出结合预测模型与LLM的混合框架，为野火风险管理生成结构化可操作报告


<details>
  <summary>Details</summary>
Motivation: 当前野火风险评估方法忽视实际运营需求，无法满足应急响应和消防服务的实用价值，需要多维度分析而非单一预测指标

Method: 开发混合框架：结合各风险维度（气象危险、着火活动、干预复杂性、资源调动）的预测模型，使用大语言模型（LLMs）将异构输出合成为结构化可操作报告

Result: 论文提出的是概念验证（proof of concept），具体结果未在摘要中说明

Conclusion: 需要开发结合预测模型和LLM的混合框架，为野火风险管理提供多维度、可操作的综合分析，提升应急响应实用性

Abstract: Current state-of-the-art approaches to wildfire risk assessment often overlook operational needs, limiting their practical value for first responders and firefighting services. Effective wildfire management requires a multi-target analysis that captures the diverse dimensions of wildfire risk, including meteorological danger, ignition activity, intervention complexity, and resource mobilization, rather than relying on a single predictive indicator. In this proof of concept, we propose the development of a hybrid framework that combines predictive models for each risk dimension with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports.

</details>


### [36] [jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation](https://arxiv.org/abs/2601.11719)
*Ho Fung Tsoi,Dylan Rankin*

Main category: cs.LG

TL;DR: jBOT是一种基于自蒸馏的预训练方法，用于CERN大型强子对撞机的喷注数据，结合局部粒子级和全局喷注级蒸馏学习喷注表示，支持异常检测和分类等下游任务。


<details>
  <summary>Details</summary>
Motivation: 自监督学习是一种无需标签即可学习特征表示的强大预训练方法，通常能从数据中捕获通用的底层语义，并可在下游任务中进行微调。本研究旨在为CERN大型强子对撞机的喷注数据开发有效的预训练方法。

Method: 提出jBOT预训练方法，基于自蒸馏技术，结合局部粒子级蒸馏和全局喷注级蒸馏来学习喷注表示。该方法在无标签喷注上进行预训练，在表示空间中产生语义类别聚类。

Result: 预训练导致表示空间中出现语义类别聚类。当仅在背景喷注上预训练时，冻结嵌入中的聚类可通过简单的基于距离的度量实现异常检测。学习到的嵌入可微调用于分类，相比从头训练的监督模型性能有所提升。

Conclusion: jBOT是一种有效的喷注数据预训练方法，通过自蒸馏学习到的表示支持异常检测和分类任务，在表示空间中产生语义聚类，为粒子物理数据分析提供了有价值的工具。

Abstract: Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.

</details>


### [37] [Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis](https://arxiv.org/abs/2601.11789)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Minhak Song,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文研究了SGD在病态优化中的"可疑对齐"现象，发现梯度与主导子空间的对齐呈现先下降、后上升、最终稳定的三阶段行为，并揭示了步长选择如何影响这一现象。


<details>
  <summary>Details</summary>
Motivation: 研究SGD在病态优化问题中出现的"可疑对齐"现象，即梯度与Hessian主导子空间的对齐行为。这种对齐被称为"可疑"的，因为尽管梯度与主导子空间高度对齐，但沿该方向的更新却无法有效降低损失。需要深入理解步长选择如何产生这种现象。

Method: 在高维二次设置中进行细粒度分析。提出了步长条件，揭示了在低对齐区域存在自适应临界步长η_t^*，区分了对齐下降和对齐上升的步长区间。在高对齐区域，对齐具有自校正特性。进一步证明了在足够病态条件下，存在步长区间使得向bulk子空间投影能降低损失，而向主导子空间投影反而增加损失。

Result: 建立了自适应步长理论，证明了在常数步长和大初始化条件下，SGD表现出明显的两阶段行为：初始对齐下降阶段，随后稳定在高对齐阶段。这解释了最近实证观察到的现象：将梯度更新投影到主导子空间是无效的。

Conclusion: SGD在病态优化中的"可疑对齐"现象可以通过步长选择理论得到解释。在低对齐区域，步长相对于临界步长的位置决定了对齐变化方向；在高对齐区域，对齐具有自校正特性。这一理论为理解SGD在病态问题中的行为提供了新的视角。

Abstract: This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $η_t^*$ separates alignment-decreasing ($η_t < η_t^*$) from alignment-increasing ($η_t > η_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.

</details>


### [38] [Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing](https://arxiv.org/abs/2601.11794)
*Abdelrahman Ramadan,Zahra Dorbeigi Namaghi,Emily Taylor,Lucas Edwards,Xan Giuliani,David S. McLagan,Sidney Givigi,Melissa Greeff*

Main category: cs.LG

TL;DR: PC²DAE：一种物理约束的降噪自编码器，用于解决无人机野火监测中低成本传感器数据稀缺和物理不可信的问题，通过架构设计而非损失函数来保证物理合理性。


<details>
  <summary>Details</summary>
Motivation: 无人机野火监测需要高分辨率大气测量，但低成本传感器存在基线漂移、交叉敏感性和响应滞后等问题，传统深度学习方法需要大量数据，而无人机飞行活动数据有限。

Method: 提出PC²DAE物理约束降噪自编码器，通过softplus激活函数确保浓度估计非负，物理合理的时间平滑处理，采用分层解码器头处理不同传感器家族，提供PC²DAE-Lean（21k参数）和PC²DAE-Wide（204k参数）两个变体。

Result: 在仅7,894个样本（约2.2小时飞行数据）上评估，PC²DAE-Lean实现67.3%平滑度提升和90.7%高频噪声降低，零物理违规，而五个基线模型产生15-23%负输出。精简版优于宽版（+5.6%平滑度），训练时间仅65秒。

Conclusion: PC²DAE通过将物理约束嵌入网络架构而非损失函数，在数据稀缺情况下有效解决传感器降噪问题，精简架构配合强归纳偏置可防止过拟合，适合边缘部署和离线处理。

Abstract: Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\% smoothness improvement and 90.7\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\% negative outputs. The lean variant outperforms wide (+5.6\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.

</details>


### [39] [Shapelets-Enriched Selective Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2601.11821)
*Shivani Tomar,Seshu Tirupathi,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 提出基于shapelets的选择性预测框架，识别时间序列关键区域，选择性丢弃不可靠预测，提升零样本和全样本微调模型的预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在零样本预测中平均表现良好，但在某些关键数据区域的预测不可靠，限制了在实际应用中的可用性，特别是当数据呈现独特趋势时。

Method: 使用shapelets构建选择性预测框架：在目标域验证集上通过平移不变字典学习学习shapelets，利用基于距离的相似度识别关键时间序列段，选择性丢弃不可靠预测。

Result: 在多样化基准数据集上，该方法使零样本模型整体误差平均降低22.17%，全样本微调模型降低22.62%；相比随机选择方法，在某个数据集上分别提升21.41%和21.43%。

Conclusion: 基于shapelets的选择性预测框架能有效识别时间序列关键区域，提升预测可靠性，为用户提供更现实的模型能力评估，增强时间序列基础模型在实际应用中的可用性。

Abstract: Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.

</details>


### [40] [MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization](https://arxiv.org/abs/2601.11827)
*Andrea Rubbi,Amir Akbarnejad,Mohammad Vali Sanian,Aryan Yazdan Parast,Hesam Asadollahzadeh,Arian Amani,Naveed Akhtar,Sarah Cooper,Andrew Bassett,Pietro Liò,Lassi Paavolainen,Sattar Vakili,Mo Lotfollahi*

Main category: cs.LG

TL;DR: MixFlow是一个条件流匹配框架，通过联合学习描述符条件的基础分布和流场，实现更好的分布外泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有条件流方法在分布偏移下泛化能力不足，难以在训练条件之外进行外推，这是条件生成建模中的核心挑战

Method: 提出MixFlow框架，通过最短路径流匹配联合学习描述符条件的基础分布和流场，将基础分布建模为可学习的描述符依赖混合分布

Result: 在单细胞转录组数据和高通量显微镜药物筛选等多个领域，MixFlow相比标准条件流匹配基线有显著提升，能更好地预测未见扰动响应

Conclusion: MixFlow提供了一个简单而强大的方法，能在异构领域实现鲁棒、可泛化且可控的生成建模

Abstract: Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.

</details>


### [41] [AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training](https://arxiv.org/abs/2601.11864)
*Zhiyuan Li,Yuan Wu,Yi Chang*

Main category: cs.LG

TL;DR: 提出自适应分组梯度裁剪(AGGC)，通过按功能类型分组并基于历史行为自适应调节，解决传统全局梯度裁剪中的梯度异质性问题，在多个LLM上优于LoRA和全微调。


<details>
  <summary>Details</summary>
Motivation: 传统全局梯度裁剪假设所有参数梯度同质，但实际上不同功能模块梯度行为差异很大，导致"溢出效应"——不稳定参数迫使稳定参数被不必要地缩放，影响训练稳定性。

Method: AGGC将参数按功能类型分组，使用指数移动平均(EMA)跟踪每组历史梯度行为，构建自适应区间同时缓解梯度爆炸和消失，并采用时间相关调度机制平衡探索与收敛。

Result: 在LLaMA 2-7B、Mistral-7B、Gemma-7B上，AGGC持续优于LoRA并经常超越全微调。GSM8K基准上，Mistral-7B+AGGC达到72.93%准确率，优于LoRA的69.5%。AGGC还能稳定RLVR训练，提升Qwen 2.5和Llama 3.2的逻辑推理能力。

Conclusion: AGGC通过模块化自适应裁剪策略有效解决了传统梯度裁剪方法的局限性，特别是梯度异质性问题。其轻量级设计可无缝集成到现有后训练流程中，开销可忽略。

Abstract: To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse "spill-over" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.

</details>


### [42] [TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures](https://arxiv.org/abs/2601.11880)
*Yingxiao Zhang,Jiaxin Duan,Junfu Zhang,Ke Feng*

Main category: cs.LG

TL;DR: TF-CoDiT：首个用于语言控制国债期货合成的扩散Transformer框架，通过DWT变换和U形VAE处理低数据量，引入FinMAP标准化市场描述，在国债期货数据合成上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer在股票价格和订单流等金融时间序列数据合成上表现良好，但在国债期货数据合成方面仍缺乏探索。国债期货数据具有低交易量、市场依赖性强、多变量间存在分组相关性等特点，需要专门的方法来处理这些挑战。

Method: 提出TF-CoDiT框架：1）将多通道1-D时间序列转换为离散小波变换系数矩阵以促进低数据学习；2）设计U形VAE分层编码跨通道依赖关系，并通过解码桥接潜在空间和DWT空间，实现潜在扩散生成；3）引入金融市场属性协议（FinMAP）作为多级描述系统，从7/8个视角识别17/23个经济指标来标准化市场动态描述。

Result: 收集2015-2025年四种国债期货数据，定义从一周到四个月不同时长的合成任务。实验表明TF-CoDiT能生成高度真实的数据，与真实数据的MSE误差最大为0.433，MAE误差最大为0.453。进一步研究证明TF-CoDiT在不同合约和时间跨度上具有鲁棒性。

Conclusion: TF-CoDiT是首个专门针对国债期货数据合成的语言控制扩散Transformer框架，通过结合DWT变换、U形VAE和FinMAP描述系统，有效解决了国债期货数据的低交易量、市场依赖和分组相关性等挑战，为金融时间序列合成提供了新方法。

Abstract: Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.

</details>


### [43] [Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach](https://arxiv.org/abs/2601.11883)
*Chaoqi Jia,Longkun Guo,Kewen Liao,Zhigang Lu,Chao Chen,Jason Xue*

Main category: cs.LG

TL;DR: 本文提出了一种基于支配匹配集问题转换的新型局部搜索框架，用于解决带实例级不能链接(CL)和必须链接(ML)约束的k-center聚类问题，达到了最佳可能的2近似比。


<details>
  <summary>Details</summary>
Motivation: 传统的k-center问题具有最佳近似比2，而带约束的聚类问题在实际应用中很重要。虽然不相交的CL约束集允许常数因子近似，但局部搜索是否能达到这样的保证仍是一个开放问题。

Method: 提出了一种新颖的局部搜索框架，通过将约束k-center聚类问题转换为支配匹配集问题，然后应用局部搜索算法来求解。

Result: 算法达到了最佳可能的2近似比，在真实世界和合成数据集上的实验结果表明，该算法在解质量上优于基线方法。

Conclusion: 本文解决了约束k-center聚类中局部搜索近似保证的开放问题，提出的框架达到了理论上的最优近似比，并在实践中表现出色。

Abstract: Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - ε would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.

</details>


### [44] [From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs](https://arxiv.org/abs/2601.11890)
*Xihe Gu,Urbashi Mitra,Tara Javidi*

Main category: cs.LG

TL;DR: 本文提出了一种加权参数化的凹覆盖目标族U_ρ，用于指导奖励无关MDP中的定向探索，该框架统一了多种现有覆盖目标，并开发了基于梯度的算法主动引导占用分布。


<details>
  <summary>Details</summary>
Motivation: 在奖励无关的马尔可夫决策过程中，不同状态-动作对具有不同的重要性或难度，需要主动且显式地构建到控制探索策略中。现有方法缺乏统一的框架来平衡探索的优先级。

Method: 提出基于状态-动作占用测度的加权参数化凹覆盖目标族U_ρ，该族统一了基于散度的边缘匹配、加权平均覆盖和最坏情况覆盖等目标。利用U_ρ的凹性和梯度闭式解，开发基于梯度的算法主动引导占用分布。

Result: U_ρ框架成功统一了多种覆盖目标，其梯度闭式解使得能够显式控制优先探索未充分探索的状态-动作对。随着ρ增大，探索策略越来越强调最少探索的状态-动作对，在极限情况下恢复最坏情况覆盖行为。

Conclusion: 提出的加权参数化凹覆盖目标族U_ρ为奖励无关MDP中的定向探索提供了统一框架，能够通过梯度算法主动引导探索策略，平衡不同状态-动作对的探索优先级。

Abstract: Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_ρ$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_ρ$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_ρ$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $ρ$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.

</details>


### [45] [DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models](https://arxiv.org/abs/2601.11895)
*Pareesa Ameneh Golnari,Adarsh Kumarappan,Wen Wen,Xiaoyu Liu,Gabriel Ryan,Yuting Sun,Shengyu Fu,Elsie Nallipogu*

Main category: cs.LG

TL;DR: DevBench是一个基于真实开发者遥测数据的代码补全基准测试，包含1800个实例，覆盖6种编程语言和6个任务类别，旨在评估LLM在实际开发场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全基准测试缺乏生态效度（与实际开发场景脱节），存在训练数据污染问题，且无法提供详细的诊断信息。需要创建一个更贴近真实开发环境、能避免数据污染并提供可操作见解的基准测试。

Method: 基于真实开发者遥测数据构建包含1800个评估实例的基准测试，覆盖6种编程语言和6个任务类别（如API使用、代码目的理解）。采用功能正确性、基于相似度的指标和LLM评估者（关注有用性和上下文相关性）相结合的综合评估方法。

Result: 评估了9个最先进的模型，揭示了它们在语法精度、语义推理和实际效用方面的差异。基准测试提供了传统基准测试所缺乏的详细诊断信息，为模型选择和改进提供了可操作的见解。

Conclusion: DevBench通过强调生态效度、避免数据污染和提供详细诊断，填补了现有代码补全基准测试的空白，为实际部署和针对性模型开发提供了重要指导。

Abstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.

</details>


### [46] [Task-tailored Pre-processing: Fair Downstream Supervised Learning](https://arxiv.org/abs/2601.11897)
*Jinwon Sohn,Guang Lin,Qifan Song*

Main category: cs.LG

TL;DR: 提出一种针对监督学习的公平性预处理框架，通过HGR相关性分析发现现有数据公平方法正则化过强，设计任务定制化预处理方法平衡公平性与效用，并理论分析下游模型性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有公平性预处理方法分为两类：数据公平方法（独立于下游模型）和任务定制化方法。研究发现数据公平方法从HGR相关性角度看施加了过强的正则化，需要设计更适合监督学习的预处理方法。

Method: 提出新颖的监督学习定制化预处理框架，在获取预处理映射时考虑公平性与效用的权衡，理论分析任意下游监督模型在变换数据上的行为，找到保证公平性改进和效用保持的充分条件。

Result: 在表格和图像数据集上的对比实验显示，该方法相比现有竞争模型具有优越性，能保持多个下游模型间一致的权衡。特别在计算机视觉数据中，仅改变与核心机器学习任务相关的必要语义特征来实现公平性。

Conclusion: 该工作为监督学习公平性预处理提供了理论保证，首次在任务定制化方法分支中理论分析预处理数据对下游模型的保证，实现了公平性与效用的更好平衡。

Abstract: Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.

</details>


### [47] [Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits](https://arxiv.org/abs/2601.11924)
*Ming Shi*

Main category: cs.LG

TL;DR: 研究多智能体协作随机多臂老虎机问题，考虑向量奖励、对抗性腐败和有限验证，分析通信协议如何影响腐败放大效应


<details>
  <summary>Details</summary>
Motivation: 研究多智能体协作学习中的通信-腐败耦合问题，探索不同通信协议（原始样本共享、统计摘要共享、推荐共享）如何影响对抗性腐败的放大效应，为实际部署提供理论指导

Method: 提出协议诱导的多重性函数形式化通信-腐败耦合，证明不同通信协议下的遗憾界，建立信息论下界，分析验证观测如何恢复可学习性

Result: 原始样本共享可能导致N倍腐败放大，而摘要共享和推荐共享保持O(Γ)腐败项并达到中心化速率；高腐败Γ=Θ(NT)下无干净信息无法获得次线性遗憾；验证观测ν超过识别阈值可恢复学习能力

Conclusion: 通信协议选择对腐败放大有决定性影响，验证在极端腐败下是必要的，认证共享可使团队遗憾独立于腐败预算Γ

Abstract: We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $Γ$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $φ$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $Γ$ can translate into an effective corruption level ranging from $Γ$ to $NΓ$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(Γ)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $Ω(Γ)$ penalty and a high-corruption regime $Γ=Θ(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $ν$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $Γ$.

</details>


### [48] [Trainability-Oriented Hybrid Quantum Regression via Geometric Preconditioning and Curriculum Optimization](https://arxiv.org/abs/2601.11942)
*Qingyu Meng,Yangshuai Wang*

Main category: cs.LG

TL;DR: 提出混合量子-经典回归框架，通过经典嵌入层作为几何预处理器改善量子神经网络的训练稳定性和收敛性


<details>
  <summary>Details</summary>
Motivation: 量子神经网络在回归任务中面临梯度噪声大、优化条件差的问题，需要提高训练稳定性和收敛性

Method: 1) 设计混合架构：轻量级经典嵌入层作为可学习的几何预处理器；2) 课程优化协议：渐进增加电路深度，从SPSA随机探索过渡到Adam梯度微调

Result: 在PDE回归基准和标准数据集上，该框架在固定训练预算下比纯QNN基线表现更好，收敛更稳定，在数据有限情况下尤其有效，减少了与振荡分量相关的结构化误差

Conclusion: 几何预处理结合课程训练是稳定量子回归的实用方法，能改善量子神经网络在科学机器学习中的训练瓶颈

Abstract: Quantum neural networks (QNNs) have attracted growing interest for scientific machine learning, yet in regression settings they often suffer from limited trainability under noisy gradients and ill-conditioned optimization. We propose a hybrid quantum-classical regression framework designed to mitigate these bottlenecks. Our model prepends a lightweight classical embedding that acts as a learnable geometric preconditioner, reshaping the input representation to better condition a downstream variational quantum circuit. Building on this architecture, we introduce a curriculum optimization protocol that progressively increases circuit depth and transitions from SPSA-based stochastic exploration to Adam-based gradient fine-tuning. We evaluate the approach on PDE-informed regression benchmarks and standard regression datasets under a fixed training budget in a simulator setting. Empirically, the proposed framework consistently improves over pure QNN baselines and yields more stable convergence in data-limited regimes. We further observe reduced structured errors that are visually correlated with oscillatory components on several scientific benchmarks, suggesting that geometric preconditioning combined with curriculum training is a practical approach for stabilizing quantum regression.

</details>


### [49] [Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration](https://arxiv.org/abs/2601.11953)
*Shiqing Gao,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: 提出MICE方法解决约束强化学习中成本函数低估问题，通过记忆模块存储危险状态并引入内在成本，显著减少约束违反同时保持策略性能


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习算法在训练过程中经常出现严重的约束违反，限制了其在安全关键场景中的应用。作者发现成本价值函数的低估是导致这些违反的关键因素。

Method: 提出记忆驱动的内在成本估计（MICE）方法：1）构建记忆模块存储先前探索的不安全状态以识别高风险区域；2）将内在成本定义为当前状态访问这些风险区域的伪计数；3）提出结合内在成本的外在-内在成本价值函数，采用偏差校正策略；4）在信任区域内制定优化目标及相应优化方法。

Result: 实验表明MICE显著减少了约束违反，同时保持了与基线相当的策略性能。理论上提供了成本价值函数的收敛保证，并建立了MICE更新的最坏情况约束违反界限。

Conclusion: MICE通过解决成本价值函数低估问题，有效提升了约束强化学习的安全性，使其更适合安全关键应用场景。

Abstract: Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.

</details>


### [50] [Data-centric Prompt Tuning for Dynamic Graphs](https://arxiv.org/abs/2601.11954)
*Yufei Peng,Cheng Yang,Zhengjie Fan,Chuan Shi*

Main category: cs.LG

TL;DR: DDGPrompt是一个面向动态图的数据中心化提示框架，通过调整预训练节点嵌入来适应不同下游任务，在少样本和冷启动场景下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统动态图方法通常通过动态链接预测预训练模型，然后将节点时间嵌入直接应用于下游任务。但由于下游任务差异大，尤其在少样本设置下性能下降明显。现有提示方法通常与特定模型架构或预训练任务强耦合，难以适应新模型设计，且只关注修改节点或时间特征而忽略空间结构信息，导致表达能力有限。

Method: 提出DDGPrompt框架：1）定义统一的节点表达特征矩阵，聚合每个节点的所有相关时间和结构信息；2）引入三个提示矩阵（时间偏置、边权重和特征掩码）来完全调整特征矩阵，实现节点嵌入的任务特定适应。

Result: 在四个公共动态图数据集上的严格少样本设置下评估，实验结果表明该方法在标签有限和冷启动条件下显著优于传统方法和现有提示方法。

Conclusion: DDGPrompt是一个有效的数据中心化提示框架，能够通过调整预训练节点嵌入来适应多样下游任务，在少样本和冷启动场景下表现出色。

Abstract: Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.

</details>


### [51] [CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction](https://arxiv.org/abs/2601.12917)
*He Sun,Jinrui Zhou,Li Li,Mingjun Xiao*

Main category: cs.LG

TL;DR: CooperLLM：云辅助的边端协同联邦微调框架，结合移动设备上的零阶优化和云端梯度校正，显著降低内存占用、加速收敛并提升精度


<details>
  <summary>Details</summary>
Motivation: 大语言模型在移动设备上微调面临高内存和计算成本挑战，而现有联邦学习方法要么依赖内存密集型反向传播，要么使用收敛慢、精度低的零阶优化方法

Method: 提出CooperLLM框架：移动客户端在私有数据上执行轻量级零阶优化更新，云端在辅助公共数据上使用反向传播微调并注入引导扰动来校正本地更新，同时采用流水线调度和自适应压缩技术优化系统性能

Result: 在多个Transformer模型和数据集上的实验表明，CooperLLM将设备内存降低高达86.4%，加速收敛8.8倍，相比最先进的零阶优化基线方法精度提升高达10个百分点

Conclusion: CooperLLM通过云辅助的边端协同设计，有效解决了移动设备上大语言模型联邦微调的内存、收敛和精度问题，实现了隐私保护下的高效个性化

Abstract: Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\%$, accelerates convergence by $8.8 \times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.

</details>


### [52] [R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning](https://arxiv.org/abs/2601.11960)
*Jingchu Wang,Bingbing Xu,Yige Yuan,Bin Xie,Xiaoqian Sun,Huawei Shen*

Main category: cs.LG

TL;DR: 提出R²PO方法，通过引入轻量级残差Rollout-Head解耦训练轨迹与推理响应，解决强化学习中探索不足的问题，提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用单一策略同时生成推理响应和训练优化轨迹，导致生成稳定推理响应与多样化训练轨迹之间的目标冲突，造成探索不足，损害推理能力

Method: 提出R²PO（残差Rollout策略优化），在策略之上引入轻量级残差Rollout-Head，将训练轨迹与推理响应解耦，在训练期间实现受控的轨迹多样化，同时保持推理生成的稳定性

Result: 在多个基准测试中一致优于基线方法，在MATH-500上平均准确率提升3.1%，在APPS上提升2.4%，同时减少格式错误并缓解长度偏差以实现稳定优化

Conclusion: R²PO通过解耦训练轨迹与推理响应，有效解决了强化学习中的探索不足问题，显著提升了LLM的推理能力，为LLM推理优化提供了新思路

Abstract: Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.

</details>


### [53] [Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay](https://arxiv.org/abs/2601.13456)
*Sahasra Kokkula,Daniel David,Aaditya Baruah*

Main category: cs.LG

TL;DR: 该论文提出客户端经验回放方法，通过每个客户端维护少量历史样本缓冲区，有效解决联邦学习中的时间概念漂移问题，防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在时间概念漂移（客户端数据分布随时间变化）下表现不佳，标准FedAvg方法在季节性漂移场景中会出现灾难性遗忘问题，准确率大幅下降。

Method: 提出客户端经验回放方法：每个客户端维护一个小的历史样本缓冲区，在本地训练时将过去样本与当前数据混合使用。该方法无需改变服务器聚合机制，简单易行。

Result: 在Fashion-MNIST数据集上，标准FedAvg准确率从74%降至28%；而使用每类50个样本的缓冲区后，性能恢复至78-82%，有效防止了遗忘。消融研究显示缓冲区大小与准确性之间存在明确的权衡关系。

Conclusion: 客户端经验回放是一种简单有效的解决方案，能够显著缓解联邦学习中的时间概念漂移问题，防止灾难性遗忘，且无需修改服务器端架构。

Abstract: Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.

</details>


### [54] [One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints](https://arxiv.org/abs/2601.11977)
*Ren He,Yinliang Xu,Jinfeng Wang,Jeremy Watson,Jian Song*

Main category: cs.LG

TL;DR: 提出MoE-Encoder模块，通过稀疏混合专家层增强预训练时序模型，解决电力系统多变量预测中的复杂依赖和隐私约束问题


<details>
  <summary>Details</summary>
Motivation: 电力系统预测面临多变量复杂依赖、跨区域隐私约束、传统方法需要大量专家知识且泛化能力有限、预训练模型零样本性能不足等挑战

Method: 在预训练预测模型的标记化和编码层之间注入稀疏混合专家层，将多变量预测转化为专家引导的单变量任务，支持联邦学习中的本地化训练和轻量参数共享

Result: 在公开多变量数据集上显著提升预测精度，联邦环境模拟显示仅传输MoE-Encoder参数即可高效适应新区域，性能下降最小

Conclusion: MoE-Encoder为时序基础模型提供了可扩展且隐私感知的扩展方案

Abstract: Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.

</details>


### [55] [Extreme Value Policy Optimization for Safe Reinforcement Learning](https://arxiv.org/abs/2601.12008)
*Shiqing Gao,Yihang Zhou,Shuai Shao,Haoyu Luo,Yiheng Bing,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: 提出EVO算法，利用极值理论处理强化学习中的极端风险事件，减少约束违反


<details>
  <summary>Details</summary>
Motivation: 传统约束强化学习基于期望约束，忽略了尾部分布中的罕见但高影响极端事件（如黑天鹅事件），可能导致严重约束违反

Method: 提出EVO算法：1）引入极端分位数优化目标捕捉成本尾部分布的极端样本；2）提出极端优先重放机制，放大罕见但高影响极端样本的学习信号；3）基于极值理论建模和利用极端奖励和成本样本

Result: 理论上建立了策略更新期间预期约束违反的上界，保证在零违反分位数水平的严格约束满足；实验表明EVO显著减少训练期间的约束违反，同时保持与基线相当的策略性能

Conclusion: EVO算法能有效处理强化学习中的极端风险事件，比基于期望的方法具有更低的约束违反概率，比分位数回归方法具有更低的方差

Abstract: Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.

</details>


### [56] [Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features](https://arxiv.org/abs/2601.12011)
*Yize Zhao,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 损失重加权在过参数化DNN中无法改变最终学习阶段，但在训练早期显著有益。作者引入小规模模型(SSM)来透明分析这一现象，发现标准ERM早期优先学习多数类特征而延迟少数类学习，而重加权能恢复平衡学习动态。


<details>
  <summary>Details</summary>
Motivation: 虽然损失重加权在现代深度学习中呈现复杂图景（无法改变过参数化DNN的最终学习阶段但早期训练有益），但缺乏对这一现象的透明分析和理解。需要建立简化模型来抽象DNN架构和输入数据的复杂性，同时保持类别不平衡的关键结构信息。

Method: 引入小规模模型(SSM)，该模型专门设计来抽象DNN架构和输入数据的固有复杂性，同时在其频谱分量中保持类别不平衡结构的关键信息。通过SSM分析标准经验风险最小化(ERM)与重加权方法的学习动态差异。

Result: SSM揭示：1) 标准ERM在训练早期优先学习区分多数类而非少数类，从而延迟少数类学习；2) 与此形成鲜明对比，重加权能恢复平衡学习动态，使与多数类和少数类相关的特征能够同时学习。

Conclusion: 损失重加权在训练早期通过恢复平衡学习动态而有益，使多数类和少数类特征能够同时学习，而不是像标准ERM那样优先学习多数类特征。SSM为理解这一现象提供了透明分析框架。

Abstract: The application of loss reweighting in modern deep learning presents a nuanced picture. While it fails to alter the terminal learning phase in overparameterized deep neural networks (DNNs) trained on high-dimensional datasets, empirical evidence consistently shows it offers significant benefits early in training. To transparently demonstrate and analyze this phenomenon, we introduce a small-scale model (SSM). This model is specifically designed to abstract the inherent complexities of both the DNN architecture and the input data, while maintaining key information about the structure of imbalance within its spectral components. On the one hand, the SSM reveals how vanilla empirical risk minimization preferentially learns to distinguish majority classes over minorities early in training, consequently delaying minority learning. In stark contrast, reweighting restores balanced learning dynamics, enabling the simultaneous learning of features associated with both majorities and minorities.

</details>


### [57] [Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models](https://arxiv.org/abs/2601.12083)
*Siru Zhong,Junjie Qiu,Yangyu Wu,Yiqiu Liu,Yuanpeng He,Zhongwen Rao,Bin Yang,Chenjuan Guo,Hao Xu,Yuxuan Liang*

Main category: cs.LG

TL;DR: FactoST-v2是一个增强的因子化时空基础模型框架，通过分离通用时间学习和领域特定空间适应，实现全权重转移和任意长度泛化，在零样本和少样本场景中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时空基础模型虽然承诺跨数据集泛化能力，但联合时空预训练计算成本高昂，且难以处理领域特定空间模式的异质性。需要一种更实用、可扩展的方法来构建真正通用的时空基础模型。

Method: 采用两阶段因子化框架：第一阶段使用随机序列掩码预训练简约的仅编码器骨干网络，捕捉不变的时间动态，实现跨可变时间范围的概率分位数预测；第二阶段通过精简的适配器，利用元自适应学习和提示快速注入空间感知能力。

Result: FactoST-v2在多个领域评估中实现了最先进的准确性，具有线性效率，在零样本和少样本场景中显著优于现有基础模型，并能与领域特定专家基线相媲美。

Conclusion: 这种因子化范式为构建真正通用的时空基础模型提供了一条实用、可扩展的路径，解决了联合预训练的计算成本和领域异质性挑战。

Abstract: Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.

</details>


### [58] [Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate](https://arxiv.org/abs/2601.12091)
*Qian Tan,Lei Jiang,Yuting Zeng,Shuoyang Ding,Xiaohua Xu*

Main category: cs.LG

TL;DR: 本文针对LLM存在的西方中心偏见问题，提出了CEBiasBench中英双语基准和Multi-Agent Cultural Debate框架，发现中文提示仅将偏见转向东亚视角而非消除，而MACD通过赋予代理不同文化角色并采用"求同存异"策略，显著提升了无偏见率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在系统性西方中心偏见，但使用非西方语言（如中文）提示能否缓解这种偏见尚未充分研究。现有方法存在两个不足：评估方法强制输出到预定义的文化类别且缺乏中立选项；缓解方法依赖昂贵的多文化语料库或使用缺乏明确文化表征的功能角色代理框架。

Method: 1) 提出CEBiasBench中英双语基准；2) 引入Multi-Agent Vote(MAV)支持明确的"无偏见"判断；3) 提出Multi-Agent Cultural Debate(MACD)训练免费框架，为代理分配不同的文化角色，并通过"求同存异"策略协调辩论。

Result: 实验发现中文提示仅将偏见转向东亚视角而非消除偏见。MACD在CEBiasBench上通过LLM-as-judge评估达到57.6%平均无偏见率，通过MAV评估达到86.0%（使用GPT-4o作为骨干的基线分别为47.6%和69.0%）。MACD还能推广到阿拉伯语CAMeL基准。

Conclusion: 在代理框架中明确的文化表征对于跨文化公平至关重要。MACD框架通过文化角色分配和"求同存异"策略，有效缓解了LLM的文化偏见，且无需额外训练。

Abstract: Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a "Seeking Common Ground while Reserving Differences" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.

</details>


### [59] [PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems](https://arxiv.org/abs/2601.12093)
*Duarte Alexandrino,Ben Moseley,Pavlos Protopapas*

Main category: cs.LG

TL;DR: 提出PTL-PINN框架，结合微扰理论和迁移学习，快速求解非线性微分方程，速度比传统方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在建模非线性动力学时存在泛化能力有限和训练时间长的问题，需要更高效的求解方法。

Method: 提出PTL-PINN框架，将微扰理论与迁移学习结合，通过闭式表达式求解近似线性微扰系统，计算复杂度仅为矩阵向量乘法。

Result: PTL-PINN精度与多种Runge-Kutta方法相当，计算速度比传统方法快一个数量级，成功求解了非线性振荡器、Lotka-Volterra系统、KPP-Fisher方程和波动方程等问题。

Conclusion: 该工作将长期存在的微扰方法与PINN连接起来，展示了微扰理论如何指导基础模型以接近经典求解器的速度解决非线性系统。

Abstract: Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.

</details>


### [60] [Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding](https://arxiv.org/abs/2601.12095)
*Hamidreza Sadeghi,Saeedeh Momtazi,Reza Safabakhsh*

Main category: cs.LG

TL;DR: 提出固定长度数字嵌入向量，通过神经同构场保持有理数代数运算性质，加法表现优异（>95%准确率），乘法有待改进（53-73%准确率）


<details>
  <summary>Details</summary>
Motivation: 神经网络处理极小数或极大数时面临溢出、下溢和不稳定输出变化等问题，需要一种能保持代数性质同时避免数值不稳定的数字表示方法

Method: 提出固定长度数字嵌入向量，首次实现保持有理数代数运算（加法、乘法、比较）的嵌入表示；引入神经同构场作为群、域等代数结构的神经抽象，其元素为保持代数结构的嵌入向量

Result: 加法表现优异，在恒等性、封闭性、结合性等关键代数测试中准确率超过95%；乘法表现较差，在不同代数性质测试中准确率在53%到73%之间

Conclusion: 模型在保持加法代数性质方面表现出色，但在处理乘法方面需要进一步改进；神经同构场为数字嵌入提供了有前景的框架，但需要增强乘法运算的代数保持能力

Abstract: Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.

</details>


### [61] [SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data](https://arxiv.org/abs/2601.12124)
*Bing Hu,Yixin Li,Asma Bahamyirou,Helen Chen*

Main category: cs.LG

TL;DR: SynQP是一个用于评估合成数据隐私风险的开放框架，使用模拟敏感数据来保护原始数据隐私，并提出新的身份披露风险度量方法。


<details>
  <summary>Details</summary>
Motivation: 健康应用中合成数据的使用引发隐私担忧，但缺乏开放的隐私评估框架阻碍了其采用。主要挑战是难以获取敏感数据来构建可访问的基准数据集。

Method: 引入SynQP开放框架，使用模拟敏感数据进行合成数据生成的隐私基准测试，确保原始数据保密。提出新的身份披露风险度量方法，更准确地评估隐私风险。

Result: 在质量评估中，非隐私模型实现了接近完美的机器学习效能（≥0.97）。隐私评估显示差分隐私（DP）持续降低身份披露风险和成员推理攻击风险，所有DP增强模型都保持在0.09监管阈值以下。

Conclusion: SynQP为提高隐私评估的透明度和可靠性提供了关键工具，使合成数据在健康相关应用中的使用更加安全。

Abstract: The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \(\ge0.97\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP

</details>


### [62] [SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics](https://arxiv.org/abs/2601.12131)
*Santosh Chapagain,MohammadReza EskandariNasab,Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: SolarGPT-QA是基于LLaMA-3构建的领域适应大语言模型问答系统，专门用于空间天气和太阳物理学教育，通过科学文献和GPT-4生成的大规模问答数据进行训练，在零样本设置下优于通用模型，在教育解释方面与指令调优模型竞争。


<details>
  <summary>Details</summary>
Motivation: 太阳活动（太阳耀斑、日冕物质抛射、地磁暴）对卫星、航空、电网等关键基础设施有重大影响，极端事件可能造成巨大经济损失。当前大语言模型缺乏领域专业知识和教学能力，无法清晰解释复杂的空间科学概念，需要专门的教育工具。

Method: 基于LLaMA-3基础模型构建SolarGPT-QA问答系统，使用科学文献和GPT-4生成的大规模问答数据进行训练，并通过Grok-3以学生友好的故事风格进行精炼。结合领域自适应预训练和教学微调，平衡科学准确性和教育效果。

Result: 人类成对评估显示，SolarGPT-QA在零样本设置下优于通用模型，在教育解释方面与指令调优模型竞争相当。小型试点学生理解研究表明生成解释的清晰度和可访问性有所改善。消融实验表明领域自适应预训练与教学微调的结合对平衡科学准确性和教育效果很重要。

Conclusion: 这项工作代表了向更广泛的SolarGPT框架迈出的第一步，该框架旨在支持空间科学教育和预报。领域自适应预训练与教学微调相结合的方法在保持科学准确性的同时提高了教育效果，为专业领域的教育工具开发提供了参考。

Abstract: Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.
  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.

</details>


### [63] [EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.12137)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.LG

TL;DR: 提出EMoE架构，通过基于正交特征基的路由机制解决MoE中的负载不均和专家同质化问题，无需额外平衡损失函数。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能提高效率，但存在"富者愈富"的负载不均问题和专家学习冗余表示的同质化问题。现有解决方案通常使用辅助负载平衡损失，但这往往以牺牲专业化为代价加剧同质化。

Method: 引入Eigen-Mixture-of-Experts (EMoE)架构，使用基于学习到的正交特征基的路由机制。将输入token投影到这个共享的特征基上，根据它们在特征空间主成分上的对齐程度进行路由。

Result: EMoE通过几何数据划分内在地促进了平衡的专家利用和多样化、专业化专家的开发，无需冲突的辅助损失函数。

Conclusion: EMoE提供了一种原则性的解决方案，同时解决了MoE架构中的负载不均和专家同质化问题，为高效深度学习模型提供了有前景的路径。

Abstract: The relentless scaling of deep learning models has led to unsustainable computational demands, positioning Mixture-of-Experts (MoE) architectures as a promising path towards greater efficiency. However, MoE models are plagued by two fundamental challenges: 1) a load imbalance problem known as the``rich get richer" phenomenon, where a few experts are over-utilized, and 2) an expert homogeneity problem, where experts learn redundant representations, negating their purpose. Current solutions typically employ an auxiliary load-balancing loss that, while mitigating imbalance, often exacerbates homogeneity by enforcing uniform routing at the expense of specialization. To resolve this, we introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE projects input tokens onto this shared eigenbasis and routes them based on their alignment with the principal components of the feature space. This principled, geometric partitioning of data intrinsically promotes both balanced expert utilization and the development of diverse, specialized experts, all without the need for a conflicting auxiliary loss function. Our code is publicly available at https://github.com/Belis0811/EMoE.

</details>


### [64] [Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling](https://arxiv.org/abs/2601.12145)
*Xingyue Huang,Xueying Ding,Mingxuan Ju,Yozen Liu,Neil Shah,Tong Zhao*

Main category: cs.LG

TL;DR: TDA是一种无注意力下沉的稀疏注意力机制，通过阈值差分方法解决Softmax在长上下文中的结构限制问题，实现超稀疏性和更好的长序列鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Softmax注意力在长上下文中存在结构限制：严格的归一化约束导致注意力下沉到无关token上，且随着序列长度增加，概率质量会分散，影响长上下文处理能力。

Method: 提出阈值差分注意力(TDA)：采用行级极值阈值化配合长度相关的门控机制，只保留超过阈值的部分；同时借鉴差分transformer思想，减去抑制视图以增强表达能力。

Result: 理论上证明TDA能将每行虚假幸存者的期望数控制在O(1)，且独立视图间的虚假匹配会随上下文增长而消失；实证上TDA产生>99%的精确零值，消除注意力下沉，在标准和长上下文基准上保持竞争力。

Conclusion: TDA通过阈值差分方法有效解决了Softmax在长上下文中的结构限制，实现了无注意力下沉的超稀疏注意力机制，在保持性能的同时显著提升长序列处理能力。

Abstract: Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.

</details>


### [65] [Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses](https://arxiv.org/abs/2601.12178)
*Fallou Niakh*

Main category: cs.LG

TL;DR: 提出一个联邦学习框架，用于在异构可再生能源生产损失下校准参数化保险指数，无需共享原始数据。


<details>
  <summary>Details</summary>
Motivation: 传统参数化保险指数校准需要共享敏感的生产损失数据，存在隐私问题。在可再生能源领域，不同生产者的损失分布具有异质性，需要一种既能保护隐私又能处理异质性的校准方法。

Method: 使用联邦学习框架，生产者本地使用Tweedie广义线性模型建模损失，通过联邦优化学习共同指数。支持方差和链接函数的异质性，在分布式设置中直接最小化全局偏差目标。比较了FedAvg、FedProx和FedOpt算法，并与现有基于近似的聚合方法进行基准测试。

Result: 在德国太阳能发电生产的实证应用中，联邦学习在中等异质性下能够恢复可比较的指数系数，同时提供了一个更通用和可扩展的框架。

Conclusion: 联邦学习为参数化保险指数校准提供了一个有效的隐私保护解决方案，能够处理数据异质性，为可再生能源保险提供了更灵活和可扩展的方法。

Abstract: We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.

</details>


### [66] [Speculative Sampling with Reinforcement Learning](https://arxiv.org/abs/2601.12212)
*Chenan Wang,Daniel H. Shi,Haipeng Chen*

Main category: cs.LG

TL;DR: Re-SpS：首个基于强化学习的推测采样框架，通过动态调整草稿树超参数实现实时优化，相比EAGLE-3提升生成速度达1.12倍


<details>
  <summary>Details</summary>
Motivation: 当前推测采样方法（如EAGLE-3）使用静态超参数控制树结构，限制了在不同上下文和领域中的灵活性和效率。需要动态调整机制来平衡推测激进性和计算开销。

Method: 提出基于强化学习的推测采样框架（Re-SpS），动态实时调整草稿树超参数。利用目标模型隐藏状态构建高效状态表示，引入多步动作持久化以改进上下文建模。

Result: 在五个多样化基准测试中，相比骨干LLM实现最高5.45倍加速，相比SOTA方法EAGLE-3实现最高1.12倍加速，且输出保真度无损失。

Conclusion: Re-SpS通过强化学习动态优化草稿树超参数，显著提升LLM推理速度，为实时应用提供了更高效的推测采样解决方案。

Abstract: Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\times$ speedup over the backbone LLM and up to 1.12$\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.

</details>


### [67] [One-Sided Matrix Completion from Ultra-Sparse Samples](https://arxiv.org/abs/2601.12213)
*Hongyang R. Zhang,Zhenshuo Zhang,Huy L. Nguyen,Guanghui Lan*

Main category: cs.LG

TL;DR: 本文研究超稀疏采样下的矩阵补全问题，提出一种无偏估计器来恢复矩阵的行空间或二阶矩矩阵，在行数远大于列数的稀疏面板数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵补全方法在超稀疏采样（每个条目独立观测概率p=C/d，C≥2）场景下失效，特别是当每行只有C个观测值（少于矩阵秩）时，无法准确补全矩阵。需要研究如何在这种极端稀疏情况下估计矩阵的行空间或二阶矩矩阵。

Method: 提出无偏估计器：首先对二阶矩矩阵的非零条目按观测频率进行归一化处理，然后使用梯度下降法补全二阶矩矩阵的缺失条目。归一化过程将n个二项随机变量的加权和除以总观测数。

Result: 理论证明：当矩阵行向量来自满足不相干条件的秩r因子模型时，若n≥O(dr⁵ε⁻²C⁻²log d)，梯度下降的任意局部最小值都近似全局最优，能以误差至多ε²恢复二阶矩矩阵T。实验验证：在MovieLens数据集上偏差减少88%，在稀疏度10⁻⁷的Amazon评论数据集上，T的恢复误差减少59%，M的恢复误差减少38%。

Conclusion: 本文提出的方法在超稀疏采样环境下有效解决了矩阵行空间和二阶矩矩阵的估计问题，理论分析和实验验证均表明该方法在稀疏面板数据应用中具有显著优势。

Abstract: Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\times d$ matrix $M$ (with $n \ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\top} M / n$.
  The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \ge O({d r^5 ε^{-2} C^{-2} \log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $ε^2$.
  Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\%$ and $M$ by $38\%$ compared to baseline methods.

</details>


### [68] [Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models](https://arxiv.org/abs/2601.12215)
*Megha Thukral,Cyrus Tanade,Simon A. Lee,Juhyeon Lee,Hao Zhou,Keum San Chun,Migyeong Gwak,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Mehrab Bin Morshed,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出MMR自监督预训练框架，通过小波多尺度分解重建任务学习PPG信号的层次化时频特征，在17个健康相关任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴基础模型大多忽略PPG信号的频谱结构，而许多下游健康任务需要从细粒度波形形态到全局节律动态的多分辨率特征。

Method: 提出掩码多尺度重建（MMR）自监督预训练框架，使用小波多分辨率分解PPG信号，随机掩码系数后让Transformer编码器重建，强制模型整合跨时频尺度的信息。

Result: 在32,000名智能手表用户的约1700万个10秒PPG片段上预训练，在19个健康相关任务中的17个上优于或匹配现有最佳PPG基础模型、时间序列基础模型和其他自监督基线。

Conclusion: MMR展示了构建通用PPG基础模型的潜力，小波表示能捕获稳健且具有生理基础的特征，为可穿戴健康监测提供了有效框架。

Abstract: Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.

</details>


### [69] [Learning Longitudinal Health Representations from EHR and Wearable Data](https://arxiv.org/abs/2601.12227)
*Yuanyun Zhang,Han Zhou,Li Feng,Yilin Hong,Shi Li*

Main category: cs.LG

TL;DR: 提出一种多模态基础模型，联合表示电子健康记录和可穿戴设备数据作为连续时间潜在过程，在生理预测和风险建模任务中优于单模态基线


<details>
  <summary>Details</summary>
Motivation: 电子健康记录模型受限于稀疏和不规则的文档，可穿戴设备提供密集连续的生理信号但缺乏语义基础，现有方法通常单独建模或通过后期融合结合这些数据源

Method: 使用模态特定编码器和共享时间骨干网络，通过自监督和跨模态目标进行预训练，将电子健康记录和可穿戴数据联合表示为连续时间潜在过程

Result: 在预测生理和风险建模任务中，模型优于仅使用电子健康记录或仅使用可穿戴设备的强基线，特别是在长时程预测和数据缺失情况下表现更好

Conclusion: 联合电子健康记录和可穿戴设备预训练能够产生更准确的纵向健康表示，证明多模态融合在临床预测中的价值

Abstract: Foundation models trained on electronic health records show strong performance on many clinical prediction tasks but are limited by sparse and irregular documentation. Wearable devices provide dense continuous physiological signals but lack semantic grounding. Existing methods usually model these data sources separately or combine them through late fusion. We propose a multimodal foundation model that jointly represents electronic health records and wearable data as a continuous time latent process. The model uses modality specific encoders and a shared temporal backbone pretrained with self supervised and cross modal objectives. This design produces representations that are temporally coherent and clinically grounded. Across forecasting physiological and risk modeling tasks the model outperforms strong electronic health record only and wearable only baselines especially at long horizons and under missing data. These results show that joint electronic health record and wearable pretraining yields more faithful representations of longitudinal health.

</details>


### [70] [Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention](https://arxiv.org/abs/2601.12231)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Shijie Xu,Guanggang Geng*

Main category: cs.LG

TL;DR: 提出一种结合小波感知调制、多分辨率小波分解和分辨率自适应注意力的框架，用于企业安全中的内部威胁检测，在CERT基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 企业内部威胁检测面临多通道、非平稳的用户活动日志，且异常事件稀少，使得传统异常检测方法难以有效识别复杂的行为模式。

Method: 1) 偏差感知调制方案抑制常规行为并放大异常偏差；2) 离散小波变换将日志信号分解为多分辨率表示；3) 可学习的注意力机制动态重新加权最具判别性的频带。

Result: 在CERT r4.2基准测试中，该方法在精度、召回率和F1分数上始终优于现有基线方法，适用于不同时间粒度和场景。

Conclusion: 提出的集成小波感知调制、多分辨率分解和自适应注意力的框架能够有效处理复杂的企业安全日志，显著提升内部威胁检测性能。

Abstract: Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.

</details>


### [71] [TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization](https://arxiv.org/abs/2601.12288)
*Lei Liu,Tengyuan Liu,Hongwei Zhao,Jiahui Huang,Ruibo Guo,Bin Li*

Main category: cs.LG

TL;DR: TimeGMM：基于高斯混合模型的概率时间序列预测框架，通过GRIN模块动态适应时间-概率分布偏移，在单次前向传播中捕获复杂未来分布


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法依赖计算昂贵的采样或限制性参数假设，限制了预测性能并导致分布不匹配。需要一种能高效捕获复杂未来分布的方法。

Method: 提出TimeGMM框架，基于高斯混合模型在单次前向传播中捕获复杂分布。核心包括GRIN模块（适应时间-概率分布偏移）、时间编码器（TE-Module）和条件时间-概率解码器（CTPD-Module）联合捕获时间依赖性和混合分布参数。

Result: 在广泛实验中，TimeGMM始终优于最先进方法，在CRPS指标上最大提升22.48%，在NMAE指标上最大提升21.23%。

Conclusion: TimeGMM通过高斯混合模型和GRIN模块有效解决了现有概率预测方法的局限性，在单次前向传播中实现了对复杂未来分布的高效准确建模。

Abstract: Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\% in CRPS and 21.23\% in NMAE.

</details>


### [72] [Distribution Shift Is Key to Learning Invariant Prediction](https://arxiv.org/abs/2601.12296)
*Hong Zheng,Fei Teng*

Main category: cs.LG

TL;DR: 研究发现训练数据中的分布偏移程度越大，ERM方法越能接近不变预测模型的性能，甚至有时优于专门设计的OOD方法


<details>
  <summary>Details</summary>
Motivation: 观察到ERM有时优于专门为OOD任务设计的方法，这促使研究者探究算法设计之外的原因，特别是训练域间的分布偏移对模型学习的影响

Method: 通过理论推导上界分析和实证验证相结合的方法，研究分布偏移程度对模型预测能力的影响，证明在特定数据条件下ERM解能达到不变预测模型的性能

Result: 理论分析表明分布偏移程度直接影响模型预测能力，偏移越大模型能力越强，越接近不变预测模型；实证验证显示训练数据分布偏移增加时，学习模型的预测接近Oracle或Optimal模型

Conclusion: 分布偏移在模型学习中起关键作用，大的分布偏移有助于学习不变预测，ERM在特定条件下能达到与专门设计方法相当的性能，这为理解ERM在OOD任务中的表现提供了新视角

Abstract: An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.

</details>


### [73] [Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments](https://arxiv.org/abs/2601.12305)
*Deepak Kanneganti,Sajib Mistry,Sheik Fattah,Joshua Boland,Aneesh Krishna*

Main category: cs.LG

TL;DR: 提出MLaaS数据集生成器(MDG)框架，用于创建可配置、可复现的数据集来评估MLaaS服务选择与组合，通过模拟真实MLaaS行为生成大规模基准数据集。


<details>
  <summary>Details</summary>
Motivation: 需要系统评估机器学习即服务(MLaaS)的选择和组合性能，但缺乏可配置、可复现的数据集来支持这一研究领域。

Method: MDG框架通过训练和评估多种模型家族，在多个真实数据集和数据分布设置下模拟MLaaS行为，记录功能属性、服务质量指标和组合特定指标，并内置组合机制模拟物联网条件下的服务交互。

Result: 生成了超过一万个MLaaS服务实例，构建了大规模基准数据集，实验表明MDG生成的数据集相比现有基线提高了选择准确性和组合质量。

Conclusion: MDG为推进MLaaS选择和组合的数据驱动研究提供了实用且可扩展的基础，能够生成高质量、可复现的评估数据集。

Abstract: We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition

</details>


### [74] [Explanova: Automatically Discover Data Insights in N \times M Table via XAI Combined LLM Workflow](https://arxiv.org/abs/2601.12317)
*Yiming Huang*

Main category: cs.LG

TL;DR: Explanova是一个基于预设AutoML式工作流的自动数据分析框架，使用本地小LLM降低成本


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理框架（如DeepAnalyze、DataSage、Datawise）虽然强大，但依赖工具调用能力。作者探索是否可以通过预设的AutoML式工作流来实现自动化数据分析，并降低成本

Method: 采用预设的AutoML式工作流，遍历所有可能的探索路径：变量自身统计、变量间关系、变量与所有其他变量的关系，最后进行解释。使用本地小型LLM来降低计算成本

Result: Explanova框架实现了基于预设工作流的自动化数据分析，相比现有代理框架更便宜（使用本地小LLM）

Conclusion: 预设的AutoML式工作流可以作为LLM代理框架的替代方案，实现自动化数据分析，同时通过本地小LLM降低成本

Abstract: Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.

</details>


### [75] [Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays](https://arxiv.org/abs/2601.12322)
*Chang-Wei Shi,Shi-Shang Wang,Wu-Jun Li*

Main category: cs.LG

TL;DR: OrLoMo是首个实现带本地更新的异步分布式动量SGD的方法，通过按全局迭代顺序聚合本地动量来加速异构集群训练。


<details>
  <summary>Details</summary>
Motivation: 动量SGD是深度学习训练的基础优化器，而异步分布式学习对于训练大规模模型至关重要，特别是在工作节点计算能力异构的情况下。目前如何实现带本地更新的异步分布式动量SGD仍未被探索。

Method: 提出OrLoMo方法：每个工作节点本地运行动量SGD，服务器根据全局迭代索引顺序聚合来自各工作节点的本地动量。

Result: 证明了OrLoMo在任意延迟下的非凸问题收敛性，实验验证OrLoMo能超越其同步版本和其他异步方法。

Conclusion: OrLoMo是首个实现带本地更新的异步分布式动量SGD的方法，能有效处理异构集群训练，减少通信频率，加速收敛。

Abstract: Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \underline{or}dered \underline{lo}cal \underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.

</details>


### [76] [IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning](https://arxiv.org/abs/2601.12330)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Ayesha Kanwal,Sidra Sultana,Nazia Perwaiz*

Main category: cs.LG

TL;DR: 提出IceWatch深度學習框架，結合空間視覺與時間序列分析，用於預測冰川湖潰決洪水(GLOF)，提升預警系統的自動化與準確性。


<details>
  <summary>Details</summary>
Motivation: 傳統GLOF預測方法依賴水文模型、閾值監測和人工衛星影像分析，存在更新慢、依賴人工、雲層干擾和現場數據不足等問題，需要更自動化、準確的解決方案。

Method: IceWatch包含兩個核心組件：RiskFlow使用CNN分析Sentinel-2多光譜衛星影像的空間模式；TerraFlow和TempFlow分別從ITS_LIVE時間序列建模冰川速度、從MODIS LST記錄預測近地表溫度，通過協調預處理實現多模態物理信息融合。

Result: 系統提供交叉驗證，提升GLOF檢測的可靠性和可解釋性，確保強預測性能、快速數據處理以支持實時應用，並對噪聲和缺失信息具有魯棒性。

Conclusion: IceWatch為自動化、可擴展的GLOF預警系統鋪平道路，具有整合多種傳感器輸入和全球冰川監測活動的潛力。

Abstract: Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.

</details>


### [77] [LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH](https://arxiv.org/abs/2601.12355)
*Beicheng Xu,Weitong Qian,Lingching Tung,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: LB-MCTS：结合LLM和贝叶斯优化的AutoML框架，通过蒙特卡洛树搜索解决CASH问题，在104个数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 降低机器学习门槛需要解决CASH问题（算法选择和超参数调优）。传统贝叶斯优化存在冷启动问题，而现有LLM方法难以处理高维结构化CASH空间，需要结合两者优势

Method: 提出LB-MCTS框架，将LLM和贝叶斯优化结合在蒙特卡洛树搜索结构中。使用选择性调优记忆（STM）最大化LLM推理能力，实现显式的探索-利用权衡。随着数据积累，动态从LLM驱动转向BO驱动

Result: 在104个AMLB数据集上的实验表明，LB-MCTS优于现有基线方法

Conclusion: LB-MCTS成功结合了LLM的语义先验和贝叶斯优化的数据驱动优势，有效解决了高维结构化CASH问题，为AutoML提供了更强大的解决方案

Abstract: To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.

</details>


### [78] [Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems](https://arxiv.org/abs/2601.12362)
*Natthapong Promsricha,Chotirawee Chatpattanasiri,Nuttavut Kerdgongsup,Stavroula Balabani*

Main category: cs.LG

TL;DR: 基于机器学习的控制阀粘滞故障早期预测框架，使用常规过程信号（控制器输出和过程变量），LSTM模型在真实炼油厂数据上实现最高精度，可提前4小时预测粘滞故障。


<details>
  <summary>Details</summary>
Motivation: 控制阀粘滞是工业过程中的常见故障，会导致系统不稳定、设备磨损和维护成本增加。许多工厂仍使用缺乏实时监控的传统阀门，使得早期预测变得困难。

Method: 开发了三种深度学习模型：卷积神经网络（CNN）、CNN-SVM混合模型和长短期记忆网络（LSTM）。使用基于斜率比分析的数据驱动标注方法，在真实油气炼油厂数据集上进行训练。

Result: LSTM模型取得了最高精度，能够提前4小时预测控制阀粘滞故障。这是首个基于真实工业数据展示机器学习早期预测控制阀粘滞的研究。

Conclusion: 提出的框架可集成到现有控制系统中，支持预测性维护，减少停机时间，避免不必要的硬件更换。

Abstract: Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.

</details>


### [79] [Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation](https://arxiv.org/abs/2601.12380)
*Ou Deng,Shoji Nishimura,Atsushi Ogihara,Qun Jin*

Main category: cs.LG

TL;DR: SNI是一种可解释的混合类型插补框架，通过可控先验特征注意力模块结合统计先验与神经网络注意力，提供内在依赖诊断和统计-神经权衡参数。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据库常结合连续测量和分类记录，但缺失值普遍存在且会扭曲下游分析。现有方法要么缺乏可解释性，要么无法有效处理混合数据类型。

Method: 提出统计-神经交互(SNI)框架，包含可控先验特征注意力(CPFA)模块，学习头级先验强度系数λ_h，软正则化注意力朝向统计先验，同时允许数据驱动的非线性模式偏差。

Result: 在6个数据集上评估，30%MCAR/strict-MAR缺失下，SNI在连续变量指标上具有竞争力，但在分类变量上常被MissForest、MIWAE等精度优先基线超越；提供内在依赖诊断和显式权衡参数。

Conclusion: SNI在精度与可解释性间取得平衡，特别适用于需要依赖诊断和统计-神经权衡透明度的部署场景，但在严重不平衡分类目标上存在局限。

Abstract: Real-world tabular databases routinely combine continuous measurements and categorical records, yet missing entries are pervasive and can distort downstream analysis. We propose Statistical-Neural Interaction (SNI), an interpretable mixed-type imputation framework that couples correlation-derived statistical priors with neural feature attention through a Controllable-Prior Feature Attention (CPFA) module. CPFA learns head-wise prior-strength coefficients $\{λ_h\}$ that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present in the data. Beyond imputation, SNI aggregates attention maps into a directed feature-dependency matrix that summarizes which variables the imputer relied on, without requiring post-hoc explainers. We evaluate SNI against six baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets spanning ICU monitoring, population surveys, socio-economic statistics, and engineering applications. Under MCAR/strict-MAR at 30\% missingness, SNI is generally competitive on continuous metrics but is often outperformed by accuracy-first baselines (MissForest, MIWAE) on categorical variables; in return, it provides intrinsic dependency diagnostics and explicit statistical-neural trade-off parameters. We additionally report MNAR stress tests (with a mask-aware variant) and discuss computational cost, limitations -- particularly for severely imbalanced categorical targets -- and deployment scenarios where interpretability may justify the trade-off.

</details>


### [80] [Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation](https://arxiv.org/abs/2601.12401)
*Jinmei Liu,Haoru Li,Zhenhong Sun,Chaofeng Chen,Yatao Bian,Bo Wang,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: DRIFT框架通过采样、提示和优化三个角度解决RL微调中的多样性崩溃问题，在保持任务对齐的同时显著提升生成多样性


<details>
  <summary>Details</summary>
Motivation: 强化学习在微调生成模型时存在多样性崩溃问题，即优化过程会使策略收敛到狄拉克分布，导致生成结果缺乏多样性，限制了模型在需要多样化候选生成的应用中的实用性

Method: 提出DRIFT框架，从三个角度激励多样性：1) 采样奖励集中的子集过滤异常值防止过早崩溃；2) 使用随机变体提示扩展条件空间；3) 通过基于势能的奖励塑造机制优化组内多样性

Result: DRIFT在任务对齐和生成多样性方面实现了帕累托优势，在相同对齐水平下多样性提升9.08%~43.46%，在相同多样性水平下对齐度提升59.65%~65.86%

Conclusion: DRIFT框架有效解决了RL微调中的多样性崩溃问题，实现了任务对齐与生成多样性的平衡，增强了生成模型在需要多样化候选生成的应用中的实用性

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \textbf{DRIFT} (\textbf{D}ive\textbf{R}sity-\textbf{I}ncentivized Reinforcement \textbf{F}ine-\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\%\!\sim\! 43.46\%$ increase in diversity at equivalent alignment levels and a $ 59.65\% \!\sim\! 65.86\%$ increase in alignment at equivalent levels of diversity.

</details>


### [81] [Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants](https://arxiv.org/abs/2601.12405)
*Manasi Kanade,Abhi Thakkar,Gabriela Fernandes*

Main category: cs.LG

TL;DR: 开发了一个可解释的机器学习框架用于儿童牙科风险分层，强调可解释性和伦理部署而非最大预测精度，模型AUC为0.61，年龄和收入贫困比是最重要的风险因素。


<details>
  <summary>Details</summary>
Motivation: 儿童牙科疾病是全球最普遍且不平等的慢性健康问题之一。尽管流行病学证据显示口腔健康结果与社会经济和人口因素相关，但现有AI牙科应用多依赖图像诊断和黑盒预测模型，缺乏透明度，限制了在儿童人群中的伦理适用性。

Method: 使用人口层面的儿童数据（包括年龄、收入贫困比、种族/民族、性别和医疗史）训练监督机器学习模型。通过ROC分析和校准曲线评估性能，使用SHAP方法实现可解释性，提供全局和个体层面的预测解释。

Result: 模型实现了中等区分度（AUC=0.61），校准保守，在高概率水平下低估风险。SHAP分析显示年龄和收入贫困比是最强的风险预测因素，其次是种族/民族和性别。

Conclusion: 可解释的机器学习能够实现透明的、预防导向的儿童牙科风险分层，支持人群筛查和公平资源分配，而非用于诊断决策。

Abstract: Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.
  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.
  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.
  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.
  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.

</details>


### [82] [Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF](https://arxiv.org/abs/2601.12415)
*Wang Zixian*

Main category: cs.LG

TL;DR: 该论文提出了正交化策略优化（OPO）框架，将对齐方法中的采样几何与优化几何解耦，解决了KL散度带来的数值不稳定问题，为现有对齐方法提供了统一视角。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法（如PPO、DPO、IPO）往往隐含地将两个独立的设计选择混为一谈：采样几何（决定哪些样本主导梯度信号）和优化几何（决定如何惩罚价值偏差）。KL散度对无界价值信号施加指数惩罚，导致数值不稳定和高置信度下的梯度消失问题。

Method: 提出正交化策略优化（OPO）框架，通过α加权重要性采样和卡方诱导的二次正则化（在比率坐标中）将采样几何与优化几何显式解耦。该框架产生简单且条件良好的目标函数，具有线性梯度动态。

Result: OPO在保持峰值寻求行为的同时实现了稳定优化，即使模型置信度高时也能避免梯度饱和。该框架为现有对齐方法提供了统一视角，并为鲁棒的推理导向训练奠定了理论基础。

Conclusion: 通过将采样几何与优化几何正交化，OPO解决了传统对齐方法中的数值不稳定问题，为大规模语言模型对齐提供了更稳健的优化框架，并为未来的对齐研究提供了新的理论基础。

Abstract: Recent alignment methods for large language models, including PPO, DPO, and IPO, are often presented as distinct algorithms. In this work, we show that many of these approaches implicitly conflate two fundamental and independent design choices: (i) the sampling geometry, which determines which samples dominate the gradient signal, and (ii) the optimization geometry, which determines how deviations in value are penalized. We formalize this observation by expressing alignment as the minimization of a generalized distance between policy energy and target energy, parameterized by an alpha-divergence-based sampling weight and a Bregman-divergence-based value metric. We demonstrate that the commonly used KL divergence induces an exponential penalty on unbounded value signals, leading to numerical instability and vanishing gradients in high-confidence regimes. To address this issue, we propose Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples sampling geometry from optimization geometry. By combining alpha-weighted importance sampling with a chi-square-induced quadratic regularization in ratio coordinates, OPO yields a simple and well-conditioned objective with linear gradient dynamics. This formulation maintains stable optimization while preserving peak-seeking behavior and avoids gradient saturation even when model confidence is high. Our analysis positions OPO as a unifying perspective on existing alignment methods and provides a principled foundation for robust reasoning-oriented training.

</details>


### [83] [Graph Attention Networks with Physical Constraints for Anomaly Detection](https://arxiv.org/abs/2601.12426)
*Mohammadhossein Homaei,Iman Khazrak,Ruben Molano,Andres Caro,Mar Avila*

Main category: cs.LG

TL;DR: 提出一种基于水力感知的图注意力网络，利用归一化守恒定律违规作为特征，结合图注意力和双向LSTM学习时空模式，在BATADAL数据集上达到F1=0.979，比现有方法提升3.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 供水系统面临日益增长的网络物理风险，需要可靠的异常检测。现有数据驱动模型忽略网络拓扑且难以解释，而基于模型的方法严重依赖参数精度。

Method: 提出水力感知图注意力网络，使用归一化守恒定律违规（质量和能量平衡残差）作为特征，结合图注意力机制和双向LSTM学习时空模式，并采用多尺度模块从节点级到网络级聚合检测分数。

Result: 在BATADAL数据集上达到F1分数0.979，比现有方法提升3.3个百分点，在15%参数噪声下表现出高鲁棒性。

Conclusion: 该方法有效结合了物理知识和深度学习，在供水系统异常检测中实现了高性能和高鲁棒性，为网络物理系统安全提供了有前景的解决方案。

Abstract: Water distribution systems (WDSs) face increasing cyber-physical risks, which make reliable anomaly detection essential. Many data-driven models ignore network topology and are hard to interpret, while model-based ones depend strongly on parameter accuracy. This work proposes a hydraulic-aware graph attention network using normalized conservation law violations as features. It combines mass and energy balance residuals with graph attention and bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module aggregates detection scores from node to network level. On the BATADAL dataset, it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\%$ parameter noise.

</details>


### [84] [Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery](https://arxiv.org/abs/2601.12442)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: CANUF框架统一贝叶斯深度学习与可微符号推理，通过约束提取、概率神经骨干和可微约束满足层，在科学AI中实现可信不确定性估计与领域约束满足。


<details>
  <summary>Details</summary>
Motivation: 科学AI应用需要既提供可信不确定性估计又尊重领域约束的模型。现有不确定性量化方法缺乏整合符号科学知识的机制，而神经符号方法在确定性操作中缺乏原则性的不确定性建模。

Method: 提出约束感知神经符号不确定性框架(CANUF)，包含三个组件：1)从科学文献自动提取约束；2)具有变分推理的概率神经骨干；3)确保物理一致性的可微约束满足层。

Result: 在Materials Project(14万+材料)、QM9分子性质和气候基准测试中，CANUF相比贝叶斯神经网络将预期校准误差降低34.7%，同时保持99.2%的约束满足率。约束引导的重新校准贡献18.3%性能提升，约束提取精度达91.4%。

Conclusion: CANUF提供了首个端到端可微管道，同时解决科学预测中的不确定性量化、约束满足和可解释性解释问题，统一了贝叶斯深度学习与可微符号推理。

Abstract: Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.

</details>


### [85] [Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting](https://arxiv.org/abs/2601.12467)
*Saurish Nagrath*

Main category: cs.LG

TL;DR: 提出两阶段预测框架，将局部时间表示学习与全局依赖建模分离，通过CNN提取局部动态特征，再用Transformer建模全局依赖，在合成多元时间序列数据上取得竞争性性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列预测中表现良好，但其效果严重依赖于从原始多元时间序列数据中提取的输入表示的质量和结构。现有方法需要更好地处理局部时间动态与全局依赖之间的关系。

Method: 两阶段框架：第一阶段使用CNN在固定长度的时间片段上提取短程时间动态和非线性特征交互，生成紧凑的片段级token嵌入，并通过token级自注意力精炼这些嵌入；第二阶段使用Transformer编码器处理token序列，建模片段间的时间依赖并生成每个片段的预测。

Result: 在具有受控静态和动态因素的合成多元时间序列数据上的实验表明，所提出的基于片段的token化策略相比卷积和基于片段的Transformer基线实现了竞争性的预测性能。

Conclusion: 结构化时间表示的重要性得到验证，将局部时间编码与基于注意力的全局建模解耦能够产生更有效和稳定的时间序列预测。

Abstract: Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.

</details>


### [86] [Semidefinite Programming for Quantum Channel Learning](https://arxiv.org/abs/2601.12502)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: cs.LG

TL;DR: 该论文提出使用半定规划（SDP）从经典数据重建量子通道的方法，适用于保真度可表示为两个二次型比值的情况，并发现重建的量子通道通常具有较小的Kraus秩。


<details>
  <summary>Details</summary>
Motivation: 研究如何从经典实验数据重建量子通道的问题，这对于量子信息处理和实验验证量子系统行为具有重要意义。

Method: 采用半定规划（SDP）方法，将保真度优化问题转化为关于Choi矩阵的凸优化问题，利用商业SDP求解器进行数值求解。

Result: 成功重建了多种形式的量子通道，发现获得的量子通道的Kraus秩通常小于其最大可能值的几个百分点，表明较小的Kraus秩量子通道足以描述实验观测数据。

Conclusion: SDP是重建量子通道的有效方法，较小的Kraus秩表明实验数据通常可由相对简单的量子通道描述，该方法还可应用于重建投影算子，并讨论了基于量子通道变换的经典计算模型。

Abstract: The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.

</details>


### [87] [Cooperative Multi-agent RL with Communication Constraints](https://arxiv.org/abs/2601.12518)
*Nuoya Xiong,Aarti Singh*

Main category: cs.LG

TL;DR: 提出基策略预测技术，通过利用旧梯度预测策略更新并收集基策略序列样本，减少基策略与当前策略差距，在有限通信下实现有效学习。


<details>
  <summary>Details</summary>
Motivation: 传统协作多智能体强化学习需要频繁访问全局信息，但在去中心化系统中通信成本高。当通信受限时，智能体只能依赖过时信息更新策略，而重要性采样在通信受限时（缺失数据概率高）会变得不稳定，因为基策略已过时。

Method: 提出基策略预测技术：利用旧梯度预测策略更新，为一系列基策略收集样本，减少基策略与当前策略之间的差距。这种方法能在一次通信轮次内收集预测基策略的样本，显著减少通信轮次。

Result: 理论上证明算法在势博弈中收敛到ε-纳什均衡，仅需O(ε^{-3/4})通信轮次和O(poly(max_i|A_i|)ε^{-11/4})样本，改进了通信成本和样本复杂度，避免了对联合动作空间大小的指数依赖。在一般马尔可夫协作博弈中也能找到智能体局部最优。

Conclusion: 基策略预测算法在有限通信下能有效学习，显著减少通信轮次，在模拟游戏和复杂环境MAPPO中表现良好，解决了重要性采样在通信受限时的不稳定问题。

Abstract: Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\varepsilon$-Nash equilibrium in potential games with only $O(\varepsilon^{-3/4})$ communication rounds and $O(poly(\max_i |A_i|)\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.

</details>


### [88] [Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks](https://arxiv.org/abs/2601.12519)
*Abdullah Umut Hamzaogullari,Arkadas Ozakin*

Main category: cs.LG

TL;DR: 本文提出改进拉格朗日神经网络（LNNs）的训练稳定性方法，包括Hessian正则化、专用激活函数和物理感知坐标缩放，成功应用于复杂系统（如三摆）和相对论场景，显著提升了LNNs的实用性和科学发现能力。


<details>
  <summary>Details</summary>
Motivation: 拉格朗日神经网络虽然能从轨迹数据学习任意拉格朗日量，但其不寻常的优化目标导致严重的训练不稳定性，限制了在复杂系统中的应用。需要解决这些基本挑战以扩展LNNs的实际应用范围。

Method: 提出三种改进技术：1）Hessian正则化方案，惩罚拉格朗日量对速度二阶导数中的非物理特征；2）更适合学习拉格朗日量的激活函数；3）物理感知坐标缩放以改善稳定性。同时扩展正则化以惩罚洛伦兹特征违反，处理相对论场景。

Result: 改进架构成功训练前所未有的复杂系统（包括三摆），在双摆系统中验证损失降低96.6%，稳定性提升90.68%。首次从轨迹数据预测AdS₄时空度量下的测地线拉格朗日量，展示了从测地线轨迹提取时空度量张量分量的能力。

Conclusion: 虽然仍保留原始LNN框架的一些限制（特别是可逆Hessian要求），但提出的改进显著扩展了LNNs在科学发现任务中的实际适用性，为物理中几何结构的自动发现开辟了新可能性。

Abstract: Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\% lower validation loss value and 90.68\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.

</details>


### [89] [Approximating splits for decision trees quickly in sparse data streams](https://arxiv.org/abs/2601.12525)
*Nikolaj Tatti*

Main category: cs.LG

TL;DR: 提出针对稀疏二值特征和二值分类的决策树分裂优化算法，在近似最优信息增益或基尼指数下实现高效计算，特别适用于稀疏数据场景。


<details>
  <summary>Details</summary>
Motivation: 传统决策树学习算法在处理数据流时，需要在每个叶子节点维护计数器以确定最优分裂点，当面对稀疏二值特征和二值分类问题时，寻找最优分裂的计算复杂度为O(d)，其中d是特征数量。对于稀疏数据（m << d），需要更高效的算法来加速分裂搜索过程。

Method: 提出近似算法，针对条件熵和基尼指数两种情况：1) 对于条件熵，实现(1+α)近似，摊销时间复杂度为O(α⁻¹(1+m log d) log log n)；2) 对于基尼指数，同样实现(1+α)近似，摊销时间复杂度为O(α⁻¹ + m log d)。其中m是数据点中1的数量，n是数据点数量。

Result: 算法在稀疏数据场景下显著提升分裂搜索效率，实验结果表明能够高效找到近似最优分裂点，性能优于基线方法，且实际表现优于理论近似保证。

Conclusion: 该算法为稀疏二值特征和二值分类的决策树学习提供了高效的分裂搜索方法，特别适用于数据流场景，能够在保证近似最优性的同时大幅降低计算复杂度。

Abstract: Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + α)$ approximation when using conditional entropy in amortized $O(α^{-1}(1 + m\log d) \log \log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + α)$ approximation in amortized $O(α^{-1} + m \log d)$ time. Our approach is beneficial for sparse data where $m \ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.

</details>


### [90] [Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem](https://arxiv.org/abs/2601.12543)
*Alireza Ghahtarani,Martin Cousineau,Amir-massoud Farahmand,Jorge E. Mendoza*

Main category: cs.LG

TL;DR: 论文提出将电动汽车在线集中充电调度问题（OCCSP）游戏化，通过图像到动作模型和DAgger训练，在负载均衡和经济效益上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 解决实时动态到达的电动汽车集中充电调度问题，需要在容量限制下平衡负载，传统方法面临模型复杂和泛化能力不足的挑战

Method: 将充电调度问题游戏化建模为网格上的约束放置问题，设计启发式策略，使用专家演示训练学习代理，并通过数据集聚合（DAgger）进行改进

Result: 游戏化方法降低了模型复杂度并获得更紧的泛化界，DAgger训练的模型在多种EV到达模式下均优于启发式基线、向量方法和监督学习代理，在蒙特利尔案例中年节省数千万美元

Conclusion: 游戏化学习框架有效解决了OCCSP问题，不仅提升负载均衡性能，还带来显著经济效益，并具有延缓电网升级的潜力

Abstract: We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montréal Area (Québec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.

</details>


### [91] [Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory](https://arxiv.org/abs/2601.12557)
*Mark Moussa,Amber V. Young,Brianna Isola,Vasuda Trehan,Michael D. Himes,Nicholas Wogan,Giada Arney*

Main category: cs.LG

TL;DR: 开发了两种机器学习架构（BCNN和SQuAT）用于从系外行星反射光谱预测生物特征物种通量，为HWO等旗舰任务提供观测目标筛选和优化工具。


<details>
  <summary>Details</summary>
Motivation: 未来直接成像旗舰任务（如NASA的HWO）面临严格的时间和资源限制，需要高效筛选观测目标。需要开发能够从反射光谱预测生物特征物种通量的工具，以加速目标筛选和优化观测计划。

Method: 提出了两种机器学习架构：1）贝叶斯卷积神经网络（BCNN），用于量化认知和随机不确定性；2）光谱查询自适应变换器（SQuAT），采用查询驱动注意力机制增强可解释性，将光谱特征与特定生物特征物种关联。

Result: 两种模型在涵盖广泛系外行星条件的增强数据集上都实现了较高的预测准确性。BCNN在不确定性量化方面表现优异，SQuAT在光谱可解释性方面具有优势。

Conclusion: 这些方法有望成为加速目标筛选、优化观测计划和最大化科学回报的有前景工具，特别适用于HWO等即将到来的旗舰任务。

Abstract: Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.

</details>


### [92] [Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization](https://arxiv.org/abs/2601.12598)
*Younes Bouhadjar,Maxime Fabre,Felix Schmidt,Emre Neftci*

Main category: cs.LG

TL;DR: 提出SelectivBench基准测试，用于系统评估线性循环模型的选择性能力，揭示关键架构特征的作用


<details>
  <summary>Details</summary>
Motivation: 线性循环神经网络作为Transformer的高效替代方案，但现有模型架构复杂化且缺乏系统性比较，现有基准任务要么过于简单要么资源消耗过大

Method: 提出线性循环模型的细化分类法，并设计SelectivBench基准测试集，使用基于规则的语法生成可调节复杂度的序列，包含故意违反转换规则的不规则间隔

Result: SelectivBench评估显示线性循环模型的性能模式与大规模语言任务结果一致，揭示了关键架构特征的作用：门控和快速遗忘机制促进召回，通道混合对选择性不必要但对泛化关键，softmax注意力因内存容量随序列长度扩展而保持优势

Conclusion: SelectivBench为线性循环模型提供了有针对性的高效探索工具，并为研究大规模评估中观察到的行为提供了受控环境

Abstract: Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench

</details>


### [93] [Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization](https://arxiv.org/abs/2601.12604)
*Safwan Labbi,Daniil Tiapkin,Paul Mangold,Eric Moulines*

Main category: cs.LG

TL;DR: 提出基于f-softargmax的新策略参数化方法，配合f-散度正则化，无需预条件即可获得非渐近收敛保证，显著改善策略梯度方法的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 传统softmax参数化会导致优化景观病态和指数级慢收敛，虽然预条件可以缓解但计算成本高，需要更有效的替代方案。

Method: 使用广义f-softargmax替代softmax作为策略参数化，配合相应f-散度的正则化器，改善优化景观并确保满足Polyak-Lojasiewicz不等式。

Result: 首次为有限MDP的随机策略梯度方法建立了无需预条件的显式非渐近最后迭代收敛保证；对于无正则化问题推导了样本复杂度界限，其中Tsallis散度的f-PG实现多项式样本复杂度，而标准softmax需要指数复杂度。

Conclusion: f-softargmax参数化配合f-散度正则化是解决策略梯度方法收敛问题的有效方案，显著优于传统softmax参数化，为策略优化提供了理论保证。

Abstract: Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.

</details>


### [94] [What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes](https://arxiv.org/abs/2601.12612)
*Piyush Sao*

Main category: cs.LG

TL;DR: 该论文提出了一种基于矩阵迹幂的新方法，用于计算大型对称正定矩阵的对数行列式，通过矩生成函数变换和插值技术，同时提供可证明的误差界限。


<details>
  <summary>Details</summary>
Motivation: 高斯过程推断和贝叶斯模型比较中需要计算大型对称正定矩阵的对数行列式。传统方法结合矩阵向量乘积和多项式逼近，但本文研究当矩阵幂可用时，通过迹幂 $p_k = \tr(A^k)$ 来估计对数行列式的不同模型。

Method: 提出基于矩生成函数 $M(t) = \E[X^t]$ 的方法，其中 $X = λ/\AM$ 是归一化特征值。通过对数变换 $K(t) = \log M(t)$ 压缩范围，在 $m+1$ 个连续整数点插值 $K$ 并微分估计 $K'(0)$。同时从相同迹信息推导出 $(\det A)^{1/n}$ 的上界，给定谱下界 $r \leq λ_{\min}$ 时获得矩约束下界，从而提供对数行列式的可证明区间。

Result: 证明了使用有限正矩的连续估计器在无界条件数下无法达到一致精度的基本极限。所有估计器和界限的计算成本为 $O(m)$，当 $m \in \{4, \ldots, 8\}$ 时，实际上是常数时间。

Conclusion: 该方法通过矩生成函数变换和插值技术，为大型矩阵对数行列式计算提供了新的有效方法，同时通过可证明的误差界限和间隙诊断，能够指示何时信任点估计、何时报告界限，增强了计算结果的可靠性。

Abstract: Computing $\log\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \tr(A^k)$, natural when matrix powers are available.
  Classical moment-based approximations Taylor-expand $\log(λ)$ around the arithmetic mean. This requires $|λ- \AM| < \AM$ and diverges when $κ> 4$. We work instead with the moment-generating function $M(t) = \E[X^t]$ for normalized eigenvalues $X = λ/\AM$. Since $M'(0) = \E[\log X]$, the log-determinant becomes $\log\det(A) = n(\log \AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \log M(t)$ compresses this range. Normalization by $\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features.
  We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \E[\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\det A)^{1/n}$. Given a spectral floor $r \leq λ_{\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\log\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \in \{4, \ldots, 8\}$, this is effectively constant time.

</details>


### [95] [Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach](https://arxiv.org/abs/2601.12624)
*Shiqi Wang,Mahdi Khosravy,Neeraj Gupta,Olaf Witkowski*

Main category: cs.LG

TL;DR: 提出一种基于浮点编码、惩罚驱动的单目标进化框架，用于生成通用对抗扰动，在降低可见性的同时提高攻击成功率


<details>
  <summary>Details</summary>
Motivation: 通用对抗扰动能够用单一噪声模式破坏多个输入的深度神经网络，进化算法因其在非凸、无梯度环境中的导航能力而成为有前景的生成方法

Method: 采用浮点编码、惩罚驱动的单目标进化框架，利用连续基因表示适应深度学习规模，结合动态进化算子与自适应调度，采用模块化PyTorch实现，通过跨模型测试和周期性批次切换确保通用性

Result: 在ImageNet数据集上的实验表明，该框架相比现有进化方法能产生更小范数、更高误分类效果、更快收敛的扰动

Conclusion: 该方法展现了生成通用对抗扰动方面的鲁棒性和可扩展性，适用于各种深度学习架构

Abstract: Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.

</details>


### [96] [Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.12637)
*Long D. Nguyen,Kelin Xia,Binh P. Nguyen*

Main category: cs.LG

TL;DR: MI-MoE：一种多尺度专家混合模型，通过拓扑感知路由机制自适应建模分子3D几何中的短、中、长程相互作用，可即插即用提升现有3D分子图神经网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D分子图神经网络使用固定的距离截断和邻居限制等启发式方法定义消息传递邻域，导致刚性的、数据无关的交互预算，无法自适应建模不同几何区域（短程、中程、长程）的相互作用。

Method: 提出多尺度交互专家混合模型(MI-MoE)：1) 距离截断专家集合，显式捕获短、中、长程相互作用；2) 拓扑门控编码器，使用基于过滤的描述符（包括持久同调特征）将输入路由到不同专家；3) 作为即插即用模块，可应用于多种3D分子骨干网络。

Result: MI-MoE在多个分子和聚合物性质预测基准数据集（包括回归和分类任务）上，一致提升了多种强3D分子骨干网络的性能。

Conclusion: 拓扑感知的多尺度路由是3D分子图学习的有效原则，MI-MoE通过自适应建模不同几何区域的相互作用，克服了传统固定邻域启发式方法的局限性。

Abstract: Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning.

</details>


### [97] [Explanation Multiplicity in SHAP: Characterization and Assessment](https://arxiv.org/abs/2601.12654)
*Hyunseung Hwang,Seungeun Lee,Lucas Rosenblatt,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: SHAP解释存在多重性：同一预测可产生多个内部有效但实质不同的特征归因解释，即使输入、任务和模型固定不变。


<details>
  <summary>Details</summary>
Motivation: SHAP常被视为可靠的特征归因解释工具，但实际应用中存在解释多重性问题，即同一预测会产生多个不同的有效解释，这影响了解释的可信度和实际应用价值。

Method: 提出量化解释多重性的方法，区分模型训练/选择与解释流程内在随机性的来源；比较基于幅度和基于排名的度量指标；推导随机基线值作为参考基准。

Result: 发现解释多重性普遍存在，即使在高置信度预测中也是如此；幅度距离可能接近零而排名度量显示显著变化；需要与解释用途相匹配的度量和基线。

Conclusion: SHAP解释存在固有的多重性问题，需要开发匹配实际使用场景的评估指标和基线，以提高特征归因解释的可靠性和实用性。

Abstract: Post-hoc explanations are widely used to justify, contest, and audit automated decisions in high-stakes domains. SHAP, in particular, is often treated as a reliable account of which features drove an individual prediction. Yet SHAP explanations can vary substantially across repeated runs even when the input, task, and trained model are held fixed. We term this phenomenon explanation multiplicity: multiple internally valid but substantively different explanations for the same decision. We present a methodology to characterize multiplicity in feature-attribution explanations and to disentangle sources due to model training/selection from stochasticity intrinsic to the explanation pipeline. We further show that apparent stability depends on the metric: magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in the identity and ordering of top features. To contextualize observed disagreement, we derive randomized baseline values under plausible null models. Across datasets, model classes, and confidence regimes, we find explanation multiplicity is pervasive and persists even for high-confidence predictions, highlighting the need for metrics and baselines that match the intended use of explanations.

</details>


### [98] [Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks](https://arxiv.org/abs/2601.12662)
*Xingran Chen,Navid NaderiAlizadeh,Alejandro Ribeiro,Shirin Saeedi Bidokhti*

Main category: cs.LG

TL;DR: 提出基于图多智能体强化学习的框架，用于优化多跳无线网络中自回归马尔可夫源的实时采样与估计策略，该策略具有可迁移性且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多跳无线网络中，节点通过缓存其他节点的样本并在无线碰撞信道上通信，以最小化时间平均估计误差。由于动作空间维度高和网络拓扑复杂，解析推导最优策略不可行。

Method: 提出图多智能体强化学习框架进行策略优化，理论证明所提策略具有可迁移性，可在结构相似的图上有效应用。

Result: 数值实验表明：(1) 所提策略优于现有基线；(2) 训练策略可迁移到更大网络，性能增益随智能体数量增加而增加；(3) 图训练过程能抵抗非平稳性；(4) 循环机制在独立学习和集中训练分散执行中至关重要，提高了对非平稳性的鲁棒性。

Conclusion: 图多智能体强化学习框架能有效解决复杂无线网络中的实时采样与估计问题，所提策略具有可迁移性、鲁棒性和优越性能。

Abstract: We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.

</details>


### [99] [MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning](https://arxiv.org/abs/2601.12680)
*Zheng Fang,Wolfgang Mayer,Zeyu Zhang,Jian Wang,Hong-Yu Zhang,Wanli Li,Zaiwen Feng*

Main category: cs.LG

TL;DR: 提出MetaToolAgent（MTA）元学习方法，通过包含155个工具、9,377个问答对的跨7个领域数据集，提升大语言模型在未见工具上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具选择方法通常局限于有限工具集，难以泛化到实际部署中遇到的新工具，需要解决大语言模型在动态工具协调中的泛化问题。

Method: 提出MetaToolAgent（MTA）元学习方法，构建包含7个领域、155个工具、9,377个问答对的综合数据集，模拟真实集成场景，通过元学习提升跨工具泛化能力。

Result: 实验结果显示MTA在未见工具上显著优于基线方法，证明其构建灵活可扩展动态工具协调系统的潜力。

Conclusion: MTA通过元学习方法有效提升了大语言模型在工具选择中的跨工具泛化能力，为构建灵活可扩展的动态工具协调系统提供了有前景的解决方案。

Abstract: Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.

</details>


### [100] [Resource-Conscious RL Algorithms for Deep Brain Stimulation](https://arxiv.org/abs/2601.12699)
*Arkaprava Gupta,Nicholas Carter,William Zellers,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Parasara Sridhar Duggirala,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出T3P MAB强化学习方法用于帕金森病深部脑刺激，能够同时调节频率和振幅，轻量级适合植入设备，无需离线训练，功耗低。


<details>
  <summary>Details</summary>
Motivation: 传统DBS使用固定频率和振幅，存在副作用和电池寿命短的问题。现有RL方法计算复杂，无法在体内训练，且大多只调节单一参数。

Method: 提出时间与阈值触发的多臂老虎机(T3P MAB)强化学习方法，能够同时调节DBS的频率和振幅，轻量级设计适合在微控制器上部署。

Result: T3P MAB算法比现有方法更有效，样本效率更高，收敛时间短，功耗低，适合资源受限平台，首次在硬件上实现并报告能耗测量。

Conclusion: T3P MAB RL方法为DBS提供了一种轻量级、自适应、可同时调节多个参数的解决方案，适合植入设备部署，解决了现有方法的计算复杂性和功耗问题。

Abstract: Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson's Disease (PD). DBS involves stimulating specific regions of the brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources.
  We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.

</details>


### [101] [Towards Spectroscopy: Susceptibility Clusters in Language Models](https://arxiv.org/abs/2601.12703)
*Andrew Gordon,Garrett Baker,George Wang,William Snell,Stan van Wingerden,Daniel Murfet*

Main category: cs.LG

TL;DR: 提出一种基于谱分析的神经网络内部结构探测方法，通过扰动数据分布测量模型响应，识别出510个可解释的聚类模式


<details>
  <summary>Details</summary>
Motivation: 借鉴物理学中的谱分析原理，通过测量系统对扰动的响应来推断内部结构，将这一思想应用于神经网络，旨在理解模型内部组件如何响应数据分布的变化

Method: 通过上加权特定token来扰动数据分布，使用随机梯度Langevin动力学(SGLD)计算敏感性χ_xy，开发基于传导度的聚类算法分析模型响应模式

Result: 在Pythia-14M模型上识别出510个可解释的聚类，涵盖语法模式、代码结构、数学符号等多个领域，与稀疏自编码器有50%的匹配度

Conclusion: 该方法成功将谱分析原理应用于神经网络，提供了一种系统性的内部结构探测工具，验证了不同方法在恢复模型结构方面的一致性

Abstract: Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $χ_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts "for similar reasons" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.

</details>


### [102] [Adaptively trained Physics-informed Radial Basis Function Neural Networks for Solving Multi-asset Option Pricing Problems](https://arxiv.org/abs/2601.12704)
*Yan Ma,Yumeng Ren*

Main category: cs.LG

TL;DR: 提出基于径向基函数神经网络的物理信息机器学习算法，用于求解多资产期权定价的Black-Scholes偏微分方程


<details>
  <summary>Details</summary>
Motivation: 需要有效求解多资产期权定价的Black-Scholes偏微分方程，特别是处理非光滑支付条件的情况

Method: 开发物理信息径向基函数神经网络（PIRBFNN），结合传统径向基函数配置法和物理信息神经网络方法，采用基于PDE残差的技术自适应优化隐藏神经元分布

Result: 通过单资产欧式看跌期权、双资产交换期权和四资产篮子看涨期权的实验验证了方法的有效性

Conclusion: PIRBFNN能够准确高效地处理多维期权定价模型，特别是具有非光滑支付条件的情况

Abstract: The present study investigates the numerical solution of Black-Scholes partial differential equation (PDE) for option valuation with multiple underlying assets. We develop a physics-informed (PI) machine learning algorithm based on a radial basis function neural network (RBFNN) that concurrently optimizes the network architecture and predicts the target option price. The physics-informed radial basis function neural network (PIRBFNN) combines the strengths of the traditional radial basis function collocation method and the physics-informed neural network machine learning approach to effectively solve PDE problems in the financial context. By employing a PDE residual-based technique to adaptively refine the distribution of hidden neurons during the training process, the PIRBFNN facilitates accurate and efficient handling of multidimensional option pricing models featuring non-smooth payoff conditions. The validity of the proposed method is demonstrated through a set of experiments encompassing a single-asset European put option, a double-asset exchange option, and a four-asset basket call option.

</details>


### [103] [Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting](https://arxiv.org/abs/2601.12706)
*Sina Kazemdehbashi*

Main category: cs.LG

TL;DR: 论文提出TATS模型，将时间序列预测重构为趋势预测和数值预测两部分，通过趋势调整提升预测精度，在金融时间序列上优于传统LSTM模型。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法（包括传统统计模型和神经网络如LSTM）在预测精度上仍有提升空间，特别是对于波动性强的金融时间序列。同时，常用的MSE和MAE指标不能全面评估模型性能，需要加入趋势检测准确率。

Method: 将时间序列预测重构为两部分任务：1) 使用二元分类器预测下一时间步的趋势（方向性运动）；2) 使用LSTM或Bi-LSTM预测定量数值。提出TATS模型，基于二元分类器预测的趋势调整预测值。

Result: 在每日黄金价格这一波动性金融时间序列上的实验表明，TATS模型相比标准LSTM和Bi-LSTM模型显著降低了预测误差。同时发现仅使用MSE和MAE指标不足，需要结合趋势检测准确率来全面评估模型性能。

Conclusion: TATS模型通过将时间序列预测分解为趋势预测和数值预测两部分，并基于预测趋势调整数值，能有效提升预测精度。趋势检测准确率应作为时间序列模型评估的重要补充指标。

Abstract: Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.

</details>


### [104] [Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization](https://arxiv.org/abs/2601.12707)
*Junyi Liao,Zihan Zhu,Ethan Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出一个统一框架，用于从观察到的玩家策略和行动中恢复两人零和矩阵博弈和马尔可夫博弈中的未知奖励函数，利用熵正则化和量化响应均衡解决逆问题的模糊性和数据覆盖限制。


<details>
  <summary>Details</summary>
Motivation: 在逆强化学习和博弈论中，估计驱动智能体行为的未知奖励函数是核心问题。然而，由于逆问题的固有模糊性、可行奖励的非唯一性以及有限观测数据覆盖，这项任务具有挑战性。

Method: 建立基于量化响应均衡的奖励函数可识别性理论框架，提出从观察行动中学习奖励函数的新算法，适用于静态和动态设置，可结合最大似然估计等方法，并提供理论保证。

Result: 算法在理论和数值研究中都表现出可靠性和样本效率，为竞争环境中的决策制定提供了新见解，证明了在熵正则化条件下奖励函数恢复的可行性。

Conclusion: 该研究为两人零和博弈中的奖励函数恢复提供了一个统一的理论和算法框架，通过熵正则化和量化响应均衡解决了逆问题的挑战，为理解竞争环境中的决策行为提供了有效工具。

Abstract: Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.

</details>


### [105] [Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off](https://arxiv.org/abs/2601.12730)
*Zhaochun Li,Chen Wang,Jionghao Bai,Shisheng Cui,Ge Lan,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: DCPO提出了一种分布中心的强化学习方法，通过分布级正则化控制策略熵，解决GRPO训练中探索不足的问题，相比样本中心方法实现更可控的探索和更强的探索-利用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法（如GRPO）在LLM训练中倾向于过度利用，导致熵单调下降、样本收敛、探索消失。现有解决方案多为样本中心的启发式方法，依赖"幸运"样本，缺乏对策略的原则性控制，效果有限且不稳定。

Method: 提出分布中心视角，将探索视为由"更好"的目标分布引导，策略抵抗熵崩溃的能力由分布本身而非个体样本决定。基于此提出DCPO，将熵调节重新表述为分布级正则化，完全在策略内实现可控熵，无需从外部分布采样。

Result: 在多个模型和七个基准测试中，DCPO相比GRPO平均提升约20%。实现了高效探索的同时保持训练稳定性，提供理论依据更强、更灵活的探索控制框架。

Conclusion: DCPO用分布级原则替代样本级启发式方法，为可控探索和更强的探索-利用权衡提供了理论基础和灵活框架，解决了RL训练中的探索不足问题。

Abstract: The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the "luck" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \textbf{distribution-centric} perspective for RL, in which exploration is always guided by a "better" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.

</details>


### [106] [A Graph Prompt Fine-Tuning Method for WSN Spatio-Temporal Correlation Anomaly Detection](https://arxiv.org/abs/2601.12745)
*Miao Ye,Jing Cui,Yuan huang,Qian He,Yong Wang,Jiwen Zhang*

Main category: cs.LG

TL;DR: 提出一种结合时空相关特征的图神经网络异常检测骨干网络，采用"预训练-图提示-微调"的多任务自监督训练策略，用于WSN多时序模态数据的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有多时序模态数据异常检测方法存在时空相关特征提取不足、异常样本标注成本高、异常样本不平衡等问题，需要针对WSN图结构数据特点设计更有效的解决方案。

Method: 1) 改进Mamba模型，结合多尺度策略和模态间融合方法，与变分图卷积模块结合，构建异常检测骨干网络；2) 设计包含无负对比学习、预测和重构的三子任务预训练方法；3) 采用"图提示-微调"机制指导预训练模型完成参数微调。

Result: 在公开数据集和实际采集数据集上的F1指标分别达到91.30%和92.31%，相比现有方法具有更好的检测性能和泛化能力。

Conclusion: 该方法能充分提取WSN多节点多时序模态场景中的时空相关特征，降低训练成本，提升检测泛化性能，为WSN可靠运行提供重要保障。

Abstract: Anomaly detection of multi-temporal modal data in Wireless Sensor Network (WSN) can provide an important guarantee for reliable network operation. Existing anomaly detection methods in multi-temporal modal data scenarios have the problems of insufficient extraction of spatio-temporal correlation features, high cost of anomaly sample category annotation, and imbalance of anomaly samples. In this paper, a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy of "pre-training - graph prompting - fine-tuning" are designed for the characteristics of WSN graph structure data. First, the anomaly detection backbone network is designed by improving the Mamba model based on a multi-scale strategy and inter-modal fusion method, and combining it with a variational graph convolution module, which is capable of fully extracting spatio-temporal correlation features in the multi-node, multi-temporal modal scenarios of WSNs. Secondly, we design a three-subtask learning "pre-training" method with no-negative comparative learning, prediction, and reconstruction to learn generic features of WSN data samples from unlabeled data, and design a "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning. The model is fine-tuned through the "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning model to complete the parameter fine-tuning, thereby reducing the training cost and enhancing the detection generalization performance. The F1 metrics obtained from experiments on the public dataset and the actual collected dataset are up to 91.30% and 92.31%, respectively, which provides better detection performance and generalization ability than existing methods designed by the method.

</details>


### [107] [A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining](https://arxiv.org/abs/2601.12751)
*Manjish Pal*

Main category: cs.LG

TL;DR: 提出基于布尔函数理论的GNN表达能力新框架，引入子群体布尔同构(SBI)概念，超越现有表达能力度量，识别傅里叶度、电路类和影响力为公平性GNN表达能力的关键障碍，设计能处理高复杂度布尔函数定义子群体的公平算法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络表达能力分析框架（如WL、双连通性、同态等）无法精细分析GNN捕捉复杂子群体结构的能力，特别是在公平性场景下处理由高复杂度布尔函数定义的交叉子群体时存在局限。

Method: 基于布尔函数理论构建GNN表达能力新框架，引入子群体布尔同构(SBI)概念，识别表达能力的关键障碍（傅里叶度、电路类、影响力），设计基于电路遍历的公平算法处理高复杂度布尔函数定义的子群体。

Result: SBI严格包含现有表达能力度量；在真实世界图上，新方法在现有方法失败的交叉子群体上实现低公平性差距，首次为公平性量身定制了GNN表达能力的理论处理。

Conclusion: 该工作为GNN表达能力提供了基于布尔函数理论的新框架，首次系统处理公平性场景下的表达能力问题，提出的SBI概念和电路遍历算法为处理复杂子群体结构提供了理论保证和实践方案。

Abstract: We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.

</details>


### [108] [Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural Networks](https://arxiv.org/abs/2601.12775)
*Yuta Hirabayashi,Daisuke Matusoka,Konobu Kimura*

Main category: cs.LG

TL;DR: 该研究提出了一种基于多尺度图神经网络的海洋模型，用于10天全球预报，通过双分辨率球面网格和大气变量输入，提高了短期预测精度和多尺度海洋变率的表征能力。


<details>
  <summary>Details</summary>
Motivation: 虽然数据驱动的海洋模型研究进展迅速，但在全球涡旋分辨海洋预报中的应用仍然有限。准确表征广泛空间尺度上的海洋动力学是此类应用的主要挑战。

Method: 采用编码器-处理器-解码器架构，使用两种不同分辨率的球面网格来捕捉海洋动力学的多尺度特性。模型将表面大气变量与海洋状态变量一起作为节点输入，以通过表示大气强迫来提高短期预测精度。

Result: 通过表面动能谱和案例研究评估表明，模型准确表征了广泛的空间尺度范围，均方根误差比较显示短期预测技能有所提高。

Conclusion: 所提出的模型提供了更准确的短期预报和改进的多尺度海洋动力学表征，突显了其在推进数据驱动、涡旋分辨全球海洋预报方面的潜力。

Abstract: Research on data-driven ocean models has progressed rapidly in recent years; however, the application of these models to global eddy-resolving ocean forecasting remains limited. The accurate representation of ocean dynamics across a wide range of spatial scales remains a major challenge in such applications. This study proposes a multi-scale graph neural network-based ocean model for 10-day global forecasting that improves short-term prediction skill and enhances the representation of multi-scale ocean variability. The model employs an encoder-processor-decoder architecture and uses two spherical meshes with different resolutions to better capture the multi-scale nature of ocean dynamics. In addition, the model incorporates surface atmospheric variables along with ocean state variables as node inputs to improve short-term prediction accuracy by representing atmospheric forcing. Evaluation using surface kinetic energy spectra and case studies shows that the model accurately represents a broad range of spatial scales, while root mean square error comparisons demonstrate improved skill in short-term predictions. These results indicate that the proposed model delivers more accurate short-term forecasts and improved representation of multi-scale ocean dynamics, thereby highlighting its potential to advance data-driven, eddy-resolving global ocean forecasting.

</details>


### [109] [Distilling Time Series Foundation Models for Efficient Forecasting](https://arxiv.org/abs/2601.12785)
*Yuqi Li,Kuiye Ding,Chuanguang Yang,Szu-Yu Chen,Yingli Tian*

Main category: cs.LG

TL;DR: DistilTS是首个专门为时间序列基础模型设计的知识蒸馏框架，通过解决任务难度差异和架构差异问题，实现模型压缩，在保持预测性能的同时大幅减少参数和加速推理。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型虽然预测性能强，但参数量大导致部署成本高。现有的通用知识蒸馏技术无法直接应用于时间序列预测，因为存在任务难度差异（短期和长期预测难度不同）和架构差异等独特挑战。

Method: 提出DistilTS框架：1) 引入水平加权目标函数，平衡不同预测时间跨度的学习；2) 设计时间对齐策略，减少师生模型间的架构不匹配。这些方法专门针对时间序列预测的特点设计。

Result: 在多个基准测试中，DistilTS实现了与完整大小TSFMs相当的预测性能，同时将参数减少高达1/150，推理速度提升高达6000倍。

Conclusion: DistilTS是首个专门为时间序列基础模型设计的蒸馏框架，有效解决了任务难度差异和架构差异问题，为高效部署时间序列预测模型提供了实用解决方案。

Abstract: Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. While knowledge distillation offers a natural and effective approach for model compression, techniques developed for general machine learning tasks are not directly applicable to time series forecasting due to the unique characteristics. To address this, we present DistilTS, the first distillation framework specifically designed for TSFMs. DistilTS addresses two key challenges: (1) task difficulty discrepancy, specific to forecasting, where uniform weighting makes optimization dominated by easier short-term horizons, while long-term horizons receive weaker supervision; and (2) architecture discrepancy, a general challenge in distillation, for which we design an alignment mechanism in the time series forecasting. To overcome these issues, DistilTS introduces horizon-weighted objectives to balance learning across horizons, and a temporal alignment strategy that reduces architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000x. Code is available at: https://github.com/itsnotacie/DistilTS-ICASSP2026.

</details>


### [110] [Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs](https://arxiv.org/abs/2601.12807)
*Zixing Song,Irwin King*

Main category: cs.LG

TL;DR: SIT-Graph：一种用于图学习的半监督指令调优框架，通过自训练利用未标记节点提升大语言模型在图学习任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有图指令调优方法需要大量标注节点数据，这在社交领域等敏感或快速变化的内容中成本高昂且耗时。同时，这些方法未能利用未标记节点中因边连接而产生的潜在相关性，而这些相关性对下游预测有益。

Method: 提出SIT-Graph半监督指令调优流水线，采用迭代自训练过程：1）首先仅使用标记节点的指令对进行微调；2）为未标记节点生成置信度过滤的伪响应，策略性地扩充数据集；3）通过迭代优化逐步使LLM与底层节点相关性对齐。该方法模型无关，可集成到任何使用LLM作为预测器的图指令调优方法中。

Result: 实验表明，将SIT-Graph集成到最先进的图指令调优方法中，在文本属性图基准测试上显著提升性能，在低标签比例设置下实现超过20%的改进。

Conclusion: SIT-Graph有效解决了图学习中标注数据稀缺的问题，通过半监督学习利用未标记节点信息，显著提升了LLM在图学习任务上的性能，特别是在低标签比例场景下表现出色。

Abstract: The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.

</details>


### [111] [Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning](https://arxiv.org/abs/2601.12816)
*Ishir Garg,Neel Kolhe,Andy Peng,Rohan Gopalam*

Main category: cs.LG

TL;DR: 提出FOPNG优化器，通过Fisher正交约束保护旧任务性能，在信息几何框架中统一自然梯度下降和正交梯度方法


<details>
  <summary>Details</summary>
Motivation: 持续学习需要神经网络在顺序任务中学习新知识，但关键挑战是学习新任务时不会灾难性遗忘旧任务。现有方法在欧几里得参数空间中操作，缺乏信息几何框架的统一性。

Method: 提出Fisher-正交投影自然梯度下降(FOPNG)优化器，将梯度投影到先前任务梯度的Fisher正交补空间上。该方法在信息几何框架中统一自然梯度下降和正交梯度方法，更新方向在重新参数化下不变，保证在Fisher度量中下降。

Result: 在标准持续学习基准测试中表现出色，包括Permuted-MNIST、Split-MNIST、Rotated-MNIST、Split-CIFAR10和Split-CIFAR100。

Conclusion: FOPNG通过Fisher正交约束有效缓解灾难性遗忘，在信息几何框架中提供理论保证和实际可行的实现方法，为持续学习提供了新的优化器解决方案。

Abstract: Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. The resulting update direction is invariant under reparameterization, guarantees descent in the Fisher metric, and helps preserve prior task outputs. We provide theoretical analysis establishing the properties of the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.

</details>


### [112] [Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations](https://arxiv.org/abs/2601.12839)
*Gyuyeon Na,Minjung Park,Soyoun Kim,Jungbin Shin,Sangmi Chai*

Main category: cs.LG

TL;DR: 提出RDLI框架，通过将专家启发式逻辑嵌入表示学习，结合检索式上下文模块，在极端标签稀缺下显著提升加密货币异常交易检测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 去中心化加密货币网络中异常轨迹检测面临极端标签稀缺和恶意行为者自适应规避策略的挑战。传统GNN虽然能捕捉局部结构模式，但难以内化多跳、逻辑驱动的资金分散和分层等洗钱特征，限制了其在FATF旅行规则等监管要求下的法证可问责性。

Method: 提出关系域逻辑集成(RDLI)框架，将专家推导的启发式方法作为可微分、逻辑感知的潜在信号嵌入表示学习。不同于静态规则方法，RDLI能检测规避标准消息传递的复杂交易流。还包含检索式上下文(RGC)模块，将异常评分基于监管和宏观经济上下文，减少良性制度变化导致的误报。

Result: 在极端标签稀缺(0.01%)条件下，RDLI在F1分数上比最先进的GNN基线方法高出28.9%。微观专家用户研究进一步证实，RDLI的路径级解释在可信度、感知有用性和清晰度方面显著优于现有方法。

Conclusion: RDLI框架展示了将领域逻辑与上下文基础相结合对于提升加密货币异常检测准确性和可解释性的重要性，特别是在极端标签稀缺和复杂洗钱模式的情况下。

Abstract: Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.

</details>


### [113] [Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates](https://arxiv.org/abs/2601.12859)
*Luca Schaufelberger,Aline Hartgers,Kjell Jorner*

Main category: cs.LG

TL;DR: PuckerFlow是一个生成机器学习模型，通过Cremer-Pople空间进行流匹配，专门用于生成环状分子的构象，在构象生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 环状分子在化学和生物学应用中广泛存在，其受限的构象柔性对药物发现和催化功能至关重要，但可靠采样环系统的构象集合仍然具有挑战性。

Method: 使用流匹配技术在Cremer-Pople空间（捕获环相关自由度的低维内部坐标系）上进行生成建模，确保生成有效的闭合环结构。

Result: PuckerFlow在几乎所有定量指标上都优于其他构象生成方法，能够生成既多样又精确的构象，特别适用于催化和药物发现相关的环系统。

Conclusion: 该工作实现了环状结构的高效可靠构象生成，为建模结构-性质关系和跨化学与生物学应用的属性引导环生成铺平了道路。

Abstract: Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.

</details>


### [114] [Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition](https://arxiv.org/abs/2601.12879)
*Mohammed Mudassir Uddin,Shahnawaz Alam,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: HAGD框架通过分层抽象和可微分搜索，将电路发现复杂度从指数级降至多项式级，在多种大语言模型上实现高行为保持率


<details>
  <summary>Details</summary>
Motivation: 当前机制可解释性面临两大挑战：从数十亿参数语言模型中提取稀疏计算电路的指数搜索复杂度，以及普遍存在的多义性问题

Method: 提出分层归因图分解框架，包含跨层转码器提取单义特征、图神经网络元学习预测拓扑结构、因果干预协议验证

Result: 在模块算术任务上达到91%行为保持率，跨架构迁移实验显示电路结构相似度平均67%，表明存在共享计算模式

Conclusion: 为更大规模模型的可解释性提供初步基础，同时指出当前归因方法的显著局限性需要未来改进

Abstract: Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks. On modular arithmetic tasks, the framework achieves up to 91% behavioral preservation ($\pm$2.3\% across runs) while maintaining interpretable subgraph sizes. Cross-architecture transfer experiments suggest that discovered circuits exhibit moderate structural similarity (averaging 67%) across model families, indicating potential shared computational patterns. These results provide preliminary foundations for interpretability at larger model scales while identifying significant limitations in current attribution methodologies that require future advances.

</details>


### [115] [AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs](https://arxiv.org/abs/2601.12893)
*Ting Dang,Soumyajit Chatterjee,Hong Jia,Yu Wu,Flora Salim,Fahim Kawsar*

Main category: cs.LG

TL;DR: AdaNODEs：一种针对时间序列预测任务的源无关测试时自适应方法，利用神经常微分方程处理分布偏移，仅需更新少量参数即可显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法主要针对独立数据设计，忽视了时间序列数据的特性，且很少处理预测任务。时间序列数据中的分布偏移具有独特特征，需要专门的自适应方法。

Method: 基于神经常微分方程构建自适应框架，提出新的损失函数专门处理预测任务的自适应，仅更新有限模型参数以减少内存使用，同时保持捕捉时间依赖关系的能力。

Result: 在一维和高维数据上的广泛实验表明，AdaNODEs相比最先进基线分别带来5.88%和28.4%的相对改进，在更高严重程度的分布偏移下表现出更强的鲁棒性。

Conclusion: AdaNODEs为时间序列预测任务提供了一种有效的源无关测试时自适应解决方案，能够处理时间序列特有的分布偏移，在保持高效内存使用的同时显著提升预测性能。

Abstract: Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\% and 28.4\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.

</details>


### [116] [Supervised Learning for the (s,S) Inventory Model with General Interarrival Demands and General Lead Times](https://arxiv.org/abs/2601.12900)
*Eliran Sherzer,Yonit Barron*

Main category: cs.LG

TL;DR: 使用神经网络监督学习框架近似(s,S)库存系统的稳态性能指标，替代昂贵的仿真计算


<details>
  <summary>Details</summary>
Motivation: 传统的(s,S)库存模型在非马尔可夫系统中分析困难，通常依赖昂贵的仿真来计算长期性能指标

Method: 提出监督学习框架：先用仿真生成训练标签，然后用神经网络训练，仅需少量低阶矩作为输入即可准确预测系统指标

Result: 神经网络能快速准确地预测库存水平稳态分布、期望周期时间、缺货概率等指标，在广泛参数范围内表现良好

Conclusion: 该框架有效替代重复昂贵的仿真运行，可轻松扩展到其他库存模型，为分析复杂随机系统提供高效快速方案

Abstract: The continuous-review (s,S) inventory model is a cornerstone of stochastic inventory theory, yet its analysis becomes analytically intractable when dealing with non-Markovian systems. In such systems, evaluating long-run performance measures typically relies on costly simulation.
  This paper proposes a supervised learning framework via a neural network model for approximating stationary performance measures of (s,S) inventory systems with general distributions for the interarrival time between demands and lead times under lost sales. Simulations are first used to generate training labels, after which the neural network is trained. After training, the neural network provides almost instantaneous predictions of various metrics of the system, such as the stationary distribution of inventory levels, the expected cycle time, and the probability of lost sales. We find that using a small number of low-order moments of the distributions as input is sufficient to train the neural networks and to accurately capture the steady-state distribution. Extensive numerical experiments demonstrate high accuracy over a wide range of system parameters. As such, it effectively replaces repeated and costly simulation runs. Our framework is easily extendable to other inventory models, offering an efficient and fast alternative for analyzing complex stochastic systems.

</details>


### [117] [Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets](https://arxiv.org/abs/2601.12903)
*Meng Liu,Ke Liang,Siwei Wang,Xingchen Hu,Sihang Zhou,Xinwang Liu*

Main category: cs.LG

TL;DR: 提出了BenchTGC基准，用于解决时序图聚类任务中的技术不适用和数据集不适用两大挑战，通过改进现有聚类技术和开发专用数据集来推动时序图聚类的发展。


<details>
  <summary>Details</summary>
Motivation: 时序图聚类是一个新兴但关注度低的任务，相比静态图聚类，它能通过基于交互序列的批处理模式找到时间与空间需求的平衡。然而，现有聚类技术不适用和缺乏合适数据集两大挑战阻碍了该领域发展。

Method: 提出了BenchTGC基准框架，包括：1）设计BenchTGC框架说明时序图聚类的范式；2）改进现有聚类技术以适应时序图；3）分析公开时序图数据集问题并开发适用于TGC任务的BenchTGC数据集。

Result: 通过大量实验验证了BenchTGC的优势，并证明了时序图聚类任务的必要性和重要性。研究表明现实世界中动态变化和复杂场景是时序图聚类的基础。

Conclusion: BenchTGC基准成功解决了时序图聚类面临的主要挑战，为这一新兴领域提供了技术框架和数据集支持，推动了时序图聚类的发展和应用。

Abstract: Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.

</details>


### [118] [An efficient heuristic for geometric analysis of cell deformations](https://arxiv.org/abs/2601.12928)
*Yaima Paz Soto,Silena Herold Garcia,Ximo Gual-Arnau,Antoni Jaume-i-Capó,Manuel González-Hidalgo*

Main category: cs.LG

TL;DR: 提出基于形状空间和弹性距离的镰状细胞自动分类方法，通过固定参数化和模板对齐简化计算，达到96.03%准确率


<details>
  <summary>Details</summary>
Motivation: 镰状细胞病在全球尤其是资源有限地区造成严重医疗负担，需要自动化分类来减少专家工作量、避免人工错误，并准确评估病情严重程度

Method: 将红细胞建模为形状空间中的闭合平面曲线，使用弹性距离（对旋转、平移、缩放和重参数化不变）。改进方法包括：(1) 基于细胞长轴使用固定参数化计算距离，(2) 在计算距离前使用该参数化将每个细胞与两个模板对齐，简化计算

Result: 在监督分类和无监督聚类中都达到96.03%的准确率，在保持或提高形状空间模型准确性的同时显著降低计算成本

Conclusion: 该方法实现了高效的红细胞分类，通过固定参数化和模板对齐策略简化了基于形状空间的分类计算，为镰状细胞病的自动化诊断提供了实用解决方案

Abstract: Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.

</details>


### [119] [Online Continual Learning for Time Series: a Natural Score-driven Approach](https://arxiv.org/abs/2601.12931)
*Edoardo Urettini,Daniele Atzeni,Ioanna-Yvonni Tsaknaki,Antonio Carta*

Main category: cs.LG

TL;DR: 该论文将在线持续学习应用于在线时间序列预测，提出NatSR方法，通过自然梯度下降与重放缓冲结合，在保持长期记忆的同时实现快速适应，在预测性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线时间序列预测需要同时具备快速适应新数据和保持长期记忆的能力，这与在线持续学习的目标高度一致。现有方法在理论联系和实践效果上仍有不足，需要建立更紧密的理论框架和更有效的算法。

Method: 1) 将神经网络优化重构为参数滤波问题，证明自然梯度下降是一种得分驱动方法并证明其信息理论最优性；2) 使用Student's t似然与自然梯度结合，实现有界更新以提高对异常值的鲁棒性；3) 提出NatSR方法，结合鲁棒优化器、重放缓冲和动态尺度启发式策略，提升在状态漂移时的快速适应能力。

Result: NatSR在预测性能上超越了更复杂的现有最先进方法，证明了所提方法的有效性。

Conclusion: 该研究强化了时间序列方法与在线持续学习之间的理论和实践联系，提出的NatSR方法在在线时间序列预测中实现了更好的快速适应和长期记忆平衡，具有实际应用价值。

Abstract: Online continual learning (OCL) methods adapt to changing environments without forgetting past knowledge. Similarly, online time series forecasting (OTSF) is a real-world problem where data evolve in time and success depends on both rapid adaptation and long-term memory. Indeed, time-varying and regime-switching forecasting models have been extensively studied, offering a strong justification for the use of OCL in these settings. Building on recent work that applies OCL to OTSF, this paper aims to strengthen the theoretical and practical connections between time series methods and OCL. First, we reframe neural network optimization as a parameter filtering problem, showing that natural gradient descent is a score-driven method and proving its information-theoretic optimality. Then, we show that using a Student's t likelihood in addition to natural gradient induces a bounded update, which improves robustness to outliers. Finally, we introduce Natural Score-driven Replay (NatSR), which combines our robust optimizer with a replay buffer and a dynamic scale heuristic that improves fast adaptation at regime drifts. Empirical results demonstrate that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.

</details>


### [120] [Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning](https://arxiv.org/abs/2601.12965)
*Doheon Kim*

Main category: cs.LG

TL;DR: 本文解释了为什么使用乘性噪声条件的扩散模型尽管不能完全学习正确的分数函数，在实践中仍能生成良好样本的理论原因


<details>
  <summary>Details</summary>
Motivation: Song和Ermon（2020）的工作表明，使用乘性噪声条件的神经网络虽然不能完全学习正确的分数函数，但仍能生成令人满意的样本。这种理论与实践的差距需要理论解释。

Method: 通过研究相关微分方程的确定性动力学，分析乘性噪声条件模型的运行机制，为其实践有效性提供理论解释。

Result: 为乘性噪声条件扩散模型在实际中表现良好的现象提供了理论解释，揭示了尽管模型不能完全学习正确分数函数，但通过确定性动力学机制仍能有效生成样本。

Conclusion: 即使乘性噪声条件模型在理论上不能完全学习正确的分数函数，但通过研究其确定性动力学，可以解释为什么这些模型在实践中仍能有效工作。

Abstract: Score-based diffusion models generate new samples by learning the score function associated with a diffusion process. While the effectiveness of these models can be theoretically explained using differential equations related to the sampling process, previous work by Song and Ermon (2020) demonstrated that neural networks using multiplicative noise conditioning can still generate satisfactory samples. In this setup, the model is expressed as the product of two functions: one depending on the spatial variable and the other on the noise magnitude. This structure limits the model's ability to represent a more general relationship between the spatial variable and the noise, indicating that it cannot fully learn the correct score. Despite this limitation, the models perform well in practice. In this work, we provide a theoretical explanation for this phenomenon by studying the deterministic dynamics of the associated differential equations, offering insight into how the model operates.

</details>


### [121] [Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients](https://arxiv.org/abs/2601.12971)
*Pancheng Niu,Jun Guo,Qiaolin He,Yongming Chen,Yanchao Shi*

Main category: cs.LG

TL;DR: 提出ACR-PINN方法，通过架构-优化协同设计解决PINNs中的表示能力不足和梯度冲突问题，显著提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)在实际应用中面临两个主要问题：1) 表示能力有限；2) 由于竞争性物理约束和冲突梯度导致的优化困难。这些问题限制了PINNs的性能和可靠性。

Method: 提出架构-优化协同设计方法：1) 提出层间动态注意力机制(LDA-PINN)增强表示灵活性；2) 将PINN训练重新表述为多任务学习问题，引入梯度冲突解决策略(GC-PINN)；3) 整合两者形成ACR-PINN，结合注意力表示和冲突感知优化，同时保持标准PINN损失形式。

Result: 在Burgers、Helmholtz、Klein-Gordon和腔流等基准PDE问题上进行广泛实验，ACR-PINN相比标准PINNs实现了更快的收敛速度和显著更低的相对L2和L∞误差。

Conclusion: 架构-优化协同设计能有效提升PINN求解器的鲁棒性和准确性，ACR-PINN通过注意力表示和冲突感知优化的结合，为解决PINNs的表示和优化问题提供了有效方案。

Abstract: Physics-Informed Neural Networks (PINNs) provide a learning-based framework for solving partial differential equations (PDEs) by embedding governing physical laws into neural network training. In practice, however, their performance is often hindered by limited representational capacity and optimization difficulties caused by competing physical constraints and conflicting gradients. In this work, we study PINN training from a unified architecture-optimization perspective. We first propose a layer-wise dynamic attention mechanism to enhance representational flexibility, resulting in the Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training as a multi-task learning problem and introduce a conflict-resolved gradient update strategy to alleviate gradient interference, leading to the Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components, we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines attentive representations with conflict-aware optimization while preserving the standard PINN loss formulation. Extensive experiments on benchmark PDEs, including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow problems, demonstrate that ACR-PINN achieves faster convergence and significantly lower relative $L_2$ and $L_\infty$ errors than standard PINNs. These results highlight the effectiveness of architecture-optimization co-design for improving the robustness and accuracy of PINN-based solvers.

</details>


### [122] [PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient](https://arxiv.org/abs/2601.12988)
*Zijian Wang,Tiancheng Huang,Hanqi Li,Da Ma,Lu Chen,Kai Yu*

Main category: cs.LG

TL;DR: PaperCompass：基于认知科学启发的框架，通过分离高层规划与细粒度执行，使用DFPO训练方法，在科学论文问答任务中实现高效且性能相当的自主阅读代理。


<details>
  <summary>Details</summary>
Motivation: 科学文献快速增长使得研究人员难以通过手动阅读跟踪新进展。现有基于LLM的自主阅读代理要么依赖大量工程化提示，要么使用传统SFT-RL训练流程，都容易导致过度探索和低效。需要更高效的框架来缩小LLM的"知行差距"。

Method: 提出PaperCompass框架，分离高层规划与细粒度执行：先制定明确计划草案（规划行动序列），再进行详细推理（实例化每个步骤，选择函数调用参数）。引入Draft-and-Follow Policy Optimization (DFPO)训练方法，联合优化计划草案和最终解决方案，这是一种轻量级的分层强化学习方法。

Result: 在Paper-QA基准测试中，PaperCompass在保持性能的同时提高了效率，取得了与更大模型相当的结果。理论分析表明DFPO具有有利的优化特性，支持稳定可靠的训练过程。

Conclusion: PaperCompass通过认知启发的规划-执行分离框架和DFPO训练方法，有效解决了现有自主阅读代理的过度探索问题，在科学论文问答任务中实现了高效且性能相当的解决方案。

Abstract: The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.

</details>


### [123] [HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads](https://arxiv.org/abs/2601.13013)
*Xiaohui Zhao,Xinjian Zhao,Jiahui Zhang,Guoyu Liu,Houzhi Wang,Shu Wu*

Main category: cs.LG

TL;DR: 提出HT-GNN模型解决新闻流广告LTV预测中的用户群体异质性和动态行为序列挑战，通过超图监督模块、自适应Transformer编码器和任务自适应专家混合实现多时间跨度预测


<details>
  <summary>Details</summary>
Motivation: 新闻流广告中的LTV预测面临两大挑战：1）基于人口统计的目标定位导致不同用户群体间LTV分布差异巨大；2）动态营销策略产生不规则行为序列，用户参与模式快速变化

Method: 提出超时序图神经网络（HT-GNN），包含三个关键组件：1）超图监督模块捕捉跨用户群体关系；2）基于Transformer的时序编码器带自适应权重；3）任务自适应专家混合与动态预测塔用于多时间跨度LTV预测

Result: 在百度广告平台的1500万用户数据集上实验表明，HT-GNN在所有指标和预测时间跨度上均优于现有最先进方法

Conclusion: HT-GNN通过联合建模人口统计异质性和时序动态，有效解决了LTV预测中的关键挑战，为新闻流广告的长期收入增长优化提供了有效解决方案

Abstract: Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: (1) demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups; and (2) dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models demographic heterogeneity and temporal dynamics through three key components: (i) a hypergraph-supervised module capturing inter-segment relationships; (ii) a transformer-based temporal encoder with adaptive weighting; and (iii) a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on \textit{Baidu Ads} with 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.

</details>


### [124] [PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning](https://arxiv.org/abs/2601.13020)
*Zhiyan Hou,Haiyun Guo,Haokai Ma,Yandu Sun,Yonghui Yang,Jinqiao Wang*

Main category: cs.LG

TL;DR: 提出PASs（通路激活子空间）方法解决多模态大语言模型持续指令调优中的专家责任模糊和遗忘问题，通过通路激活信号校准路由并稳定重要秩方向，在不增加参数的情况下提升准确性和抗遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的MoE方法在持续指令调优中，路由器和专家会共同漂移，导致专家责任模糊和遗忘加剧。这种现象被称为"错位共同漂移"，需要一种能力对齐的坐标系统来指导路由和保留。

Method: 提出PASs（通路激活子空间）概念，基于LoRA诱导的子空间反映输入在每个专家中激活的低秩通路方向。在此基础上开发PASs-based MoE-LoRA方法，包含两个组件：1) PAS引导的重新加权，使用专家通路激活信号校准路由；2) PAS感知的秩稳定，选择性稳定对先前任务重要的秩方向。

Result: 在持续指令调优基准测试中，该方法在准确性和抗遗忘方面一致优于传统持续学习基线和各种MoE-LoRA变体，且不增加额外参数。

Conclusion: PASs提供了一种能力对齐的坐标系统，能够有效解决持续指令调优中的错位共同漂移问题，通过校准路由和稳定重要秩方向来提升模型性能并减少遗忘。

Abstract: Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.

</details>


### [125] [Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis](https://arxiv.org/abs/2601.13021)
*Nataša Petrović,Gabriel Moyà-Alcover,Antoni Jaume-i-Capó,Jose Maria Buades Rubio*

Main category: cs.LG

TL;DR: 提出一种基于集成学习的镰状细胞病诊断支持系统，通过特征选择和模型优化实现更好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 开发一个能够提供镰状细胞病诊断支持的系统，重点关注模型的泛化能力和可解释性，以辅助基于外周血涂片图像的诊断

Method: 对血细胞图像进行预处理和分割，提取高质量特征；采用集成机器学习方法（随机森林和极端随机树）进行分类；设计特征重要性分析方法以降低模型复杂度和提高可解释性；使用新数据集验证泛化能力

Result: 集成模型获得F1分数90.71%和SDS分数93.33%，相比梯度提升分类器的87.32%和89.51%有明显提升；在新数据集上表现优于现有最优模型

Conclusion: 提出的集成学习方法在镰状细胞病诊断支持中表现出优异的泛化能力和性能，通过特征选择和可解释性分析增强了模型的实用性和可靠性

Abstract: This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\% and SDS-score 89.51\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.

</details>


### [126] [Analysis of Long Range Dependency Understanding in State Space Models](https://arxiv.org/abs/2601.13048)
*Srividya Ravikumar,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.LG

TL;DR: 本文首次对在真实世界任务（源代码漏洞检测）上训练的S4D模型进行了系统的核可解释性研究，揭示了S4D的长程建模能力在不同架构下差异显著，其核可表现为低通、带通或高通滤波器。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型在长序列基准测试中表现出色，但现有研究大多关注预测准确性而非可解释性。本文旨在填补这一空白，对S4D模型进行系统的核可解释性分析。

Method: 通过对在源代码漏洞检测任务上训练的S4D模型进行时域和频域分析，研究其核特性。分析S4D核在不同模型架构下的行为模式。

Result: 研究发现S4D的长程建模能力在不同架构下差异显著，直接影响模型性能。具体而言，S4D核可表现为低通、带通或高通滤波器，这取决于具体的架构设计。

Conclusion: 本文的分析结果为未来设计更好的基于S4D的模型提供了指导，强调了架构选择对模型长程建模能力和滤波器特性的重要影响。

Abstract: Although state-space models (SSMs) have demonstrated strong performance on long-sequence benchmarks, most research has emphasized predictive accuracy rather than interpretability. In this work, we present the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, we show that the long-range modeling capability of S4D varies significantly under different model architectures, affecting model performance. For instance, we show that the depending on the architecture, S4D kernel can behave as low-pass, band-pass or high-pass filter. The insights from our analysis can guide future work in designing better S4D-based models.

</details>


### [127] [TinyML-Enabled IoT for Sustainable Precision Irrigation](https://arxiv.org/abs/2601.13054)
*Kamogelo Taueatsoala,Caitlyn Daniels,Angelina J. Ramsunar,Petrus Bronkhorst,Absalom E. Ezugwu*

Main category: cs.LG

TL;DR: 提出一个基于边缘计算和TinyML的离线智能灌溉系统，使用低成本硬件实现高精度灌溉预测，显著减少水资源消耗


<details>
  <summary>Details</summary>
Motivation: 解决小规模农业社区面临的水资源短缺、气候模式不稳定以及缺乏先进、经济实惠的农业技术的问题，特别是在网络连接有限的农村地区

Method: 设计四层边缘优先物联网架构，使用ESP32微控制器作为边缘推理节点，Raspberry Pi作为本地边缘服务器，集成多种环境传感器，通过比较集成模型选择梯度提升算法，并将其转换为轻量级TinyML推理引擎部署在ESP32上

Result: 梯度提升模型表现最佳，R²得分0.9973，MAPE为0.99%，优于随机森林模型（R²=0.9916，MAPE=1.81%）。部署后系统预测灌溉需求的MAPE<1%，在受控环境中显著减少水资源使用

Conclusion: 该系统为资源受限的农村环境提供了实用、经济高效的解决方案，通过设备端人工智能增强水资源利用效率，具有可持续性和可扩展性

Abstract: Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.

</details>


### [128] [METIS: Mentoring Engine for Thoughtful Inquiry & Solutions](https://arxiv.org/abs/2601.13075)
*Abhinav Rajeev Kumar,Dhruv Trehan,Paras Chopra*

Main category: cs.LG

TL;DR: METIS是一个AI导师系统，通过工具增强、阶段感知的辅助帮助学生从想法到论文写作，在多个评估中优于GPT-5和Claude Sonnet 4.5。


<details>
  <summary>Details</summary>
Motivation: 许多学生缺乏专家研究指导，需要AI导师帮助学生从研究想法发展到完整论文。

Method: 构建METIS系统，包含文献搜索、指导指南、方法检查、记忆功能，采用工具增强和阶段感知架构。通过LLM作为裁判的成对偏好、学生角色评分、多轮辅导、证据/合规检查进行评估。

Result: 在90个单轮提示中，LLM裁判偏好METIS超过Claude Sonnet 4.5（71%）和GPT-5（54%）。学生评分在清晰度/可操作性/约束适应性方面更高。多轮会话中METIS最终质量略高于GPT-5。优势集中在文档基础阶段。

Conclusion: METIS作为AI研究导师有效，特别在文档基础阶段表现优异，但存在工具路由过早、基础浅薄、阶段分类错误等失败模式。

Abstract: Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.

</details>


### [129] [Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement](https://arxiv.org/abs/2601.13100)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出了一个递归元蒸馏的公理化和算子理论框架，将迭代知识蒸馏形式化为一系列概率分布算子，证明了在温和假设下锚定递归蒸馏能诱导KL散度收缩，收敛到基础教师分布。


<details>
  <summary>Details</summary>
Motivation: 虽然概率域知识蒸馏在温度缩放、多教师聚合和单阶段设置的偏差-方差权衡方面已有公理框架，但递归或多代蒸馏的数学行为理解不足，先前方法主要依赖经验启发式。

Method: 引入递归元蒸馏的公理化和算子理论框架，将迭代知识蒸馏形式化为概率分布算子序列，定义有效元教师构建的结构公理，证明满足这些公理的非平凡算子族存在性。

Result: 在温和可实现性和凸性假设下，锚定递归蒸馏诱导KL散度收缩，产生几何收敛到基础教师分布和唯一的全局吸引不动点。

Conclusion: 该框架是基础性的而非算法性的，刻画了递归蒸馏何时在数学上是适定和收敛的，而非误差累积的，独立于模型架构、优化细节或特定算子实例化，为理解迭代和多教师蒸馏的稳定性、偏差-方差行为和失效模式提供了理论基础。

Abstract: Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers.
  We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point.
  The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.

</details>


### [130] [FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference](https://arxiv.org/abs/2601.13143)
*Chaeyoung Jung,Youngjoon Jang,Seungwoo Lee,Joon Son Chung*

Main category: cs.LG

TL;DR: FastAV是首个为音频-视觉大语言模型设计的token剪枝框架，通过两阶段剪枝策略减少40%以上计算量，同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然token剪枝在标准LLM和视觉-语言模型中已被广泛研究，但在音频-视觉LLM中却鲜有探索，而多模态融合显著增加了这些模型的token需求，需要专门的剪枝方法。

Method: 提出基于注意力权重的剪枝策略：1）在中间层进行全局剪枝，移除广泛影响力较小的token；2）在后续层进行精细剪枝，考虑对下一个token生成的影响。该方法不依赖完整注意力图，与FlashAttention等高效注意力机制完全兼容。

Result: 在两种代表性AV-LLM上的实验表明，FastAV能够减少40%以上的FLOPs，同时保持甚至提升模型性能。

Conclusion: FastAV为音频-视觉大语言模型提供了首个有效的token剪枝框架，显著降低计算成本而不损害性能，填补了该领域的研究空白。

Abstract: In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.

</details>


### [131] [Training instability in deep learning follows low-dimensional dynamical principles](https://arxiv.org/abs/2601.13160)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 论文提出将训练稳定性视为学习系统的内在动态属性，通过四个维度（优化、环境/数据、参数、学习信号）进行统一分析，并通过受控扰动审计识别训练稳定性与最终性能脱钩等规律。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统虽然取得了显著的实证性能，但训练过程本身的稳定性仍然理解不足。训练是一个高维动态系统，微小的扰动可能导致突然且不可逆的崩溃，这破坏了可重复性和可扩展性。

Method: 提出统一的动态视角，将训练稳定性组织为四个相互作用的维度：优化稳定性、环境/数据稳定性、参数稳定性和学习信号稳定性。通过受控扰动审计训练轨迹来操作化这一视角，在不修改学习算法的情况下探测学习动态对结构化扰动的响应。

Result: 在强化学习和大型语言模型训练中识别出三个重复出现的规律：1）高最终性能经常与训练稳定性脱钩；2）受控随机性在不同范式中持续缓冲学习动态；3）低维潜在元状态的偏差系统地先于可观察的性能崩溃。

Conclusion: 这些发现确立了训练稳定性作为学习系统可测量和可比较的动态属性，为超越最终性能结果研究学习动态提供了描述性基础。

Abstract: Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.
  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.
  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.

</details>


### [132] [LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations](https://arxiv.org/abs/2601.13190)
*Vittoria De Pellegrini,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: LAViG-FLOW：一种用于地下多相流体流动建模的潜在自回归视频生成扩散框架，可快速生成饱和度和压力场，比传统数值求解器快几个数量级。


<details>
  <summary>Details</summary>
Motivation: 地下多相流体流动建模对于地质CO2封存和地热生产等应用至关重要，但高保真模拟器在需要多次前向运行进行反演和不确定性量化时成本过高。

Method: 提出LAViG-FLOW框架，使用专用2D自编码器压缩每个状态变量，通过视频扩散变换器(VDiT)建模它们在时间上的耦合分布，先训练模型学习耦合关系，然后自回归微调以进行时间外推。

Result: 在开源CO2封存数据集上评估，LAViG-FLOW生成的饱和度和压力场在时间上保持一致，运行速度比传统数值求解器快几个数量级。

Conclusion: LAViG-FLOW为地下多相流体流动建模提供了一种高效替代方案，显著降低了计算成本，适用于需要大量模拟的应用场景。

Abstract: Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.

</details>


### [133] [A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms](https://arxiv.org/abs/2601.13243)
*Yapeng Li,Jiakuo Yu,Zhixin Liu,Xinnan Liu,Jing Yu,Songze Li,Tonghua Su*

Main category: cs.LG

TL;DR: 该研究对LLM推理范式（直接生成、思维链、多智能体系统）进行了统一评估，发现结构复杂性并不总是提升推理性能，并提出了新的开放式基准MIMeBench来评估语义能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为推理系统部署时，不同推理范式（如思维链和多智能体系统）的相对有效性、成本-准确率权衡缺乏系统理解，需要全面评估以指导实践选择。

Method: 采用统一评估框架，比较三种推理范式：直接单模型生成、思维链增强的单模型推理、代表性多智能体系统工作流；使用多样化封闭式基准测试，进行角色隔离分析，并引入新的开放式基准MIMeBench评估语义抽象和对比辨别能力。

Result: 研究发现结构复杂性并不总是带来推理性能提升，其效果高度依赖于推理范式本身的特性和适用性；某些多智能体工作流在成本-准确率权衡上表现良好，而另一些则成本过高但收益有限。

Conclusion: 需要根据具体任务特性选择合适的推理范式，而不是盲目追求结构复杂性；MIMeBench为评估LLM语义能力提供了新维度，代码已开源供进一步研究。

Abstract: Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.

</details>


### [134] [Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks](https://arxiv.org/abs/2601.13244)
*Prateek Munjal,Clement Christophe,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.LG

TL;DR: 指令微调并不总是提升推理能力，反而可能让模型过度依赖特定提示模式而非真正的推理能力


<details>
  <summary>Details</summary>
Motivation: 研究指令微调是否真正提升LLM的推理能力，还是仅仅诱导表面模式匹配

Method: 在标准数学基准测试、结构扰动变体和领域转移任务上评估基础模型和指令微调模型

Result: 1. 在零-shot CoT设置中，基础模型优于指令微调模型（Llama3-70B下降32.67%）；2. 指令微调模型仅在提供few-shot示例时表现相当；3. 在领域转移任务（MedCalc）中，基础模型表现更好；4. 指令微调模型对提示结构扰动敏感

Conclusion: 指令微调的性能优势不稳定且脆弱，模型可能过度依赖特定提示模式而非发展内在推理能力

Abstract: Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.

</details>


### [135] [Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification](https://arxiv.org/abs/2601.13272)
*Aaron Pim,Tristan Pryer*

Main category: cs.LG

TL;DR: 提出基于蒙特卡洛dropout的多层蒙特卡洛框架，通过重用dropout掩码构建耦合粗-细估计器，降低方差并保持无偏性


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛dropout是量化神经网络不确定性的常用方法，但需要大量前向传播计算预测矩，计算成本高。需要开发更高效的方差缩减技术来降低计算负担。

Method: 将dropout掩码作为认知随机源，通过不同数量的随机前向传播定义保真度层次结构。通过跨保真度重用dropout掩码构建耦合的粗-细估计器，形成用于预测均值和预测方差的伸缩式多层蒙特卡洛估计器。

Result: 推导了明确的偏差、方差和有效成本表达式，以及跨层级的样本分配规则。在正向和逆向PINNs-Uzawa基准测试中验证了预测的方差率，并在相同计算成本下展示了相对于单层MC-dropout的效率提升。

Conclusion: 提出的多层蒙特卡洛框架能够有效减少蒙特卡洛dropout的方差，在保持无偏性的同时显著提高计算效率，为神经网络不确定性量化提供了更实用的工具。

Abstract: We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.

</details>


### [136] [Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning](https://arxiv.org/abs/2601.13284)
*Duygu Nur Yaldiz,Evangelia Spiliopoulou,Zheng Qi,Siddharth Varia,Srikanth Doss,Nikolaos Pappas*

Main category: cs.LG

TL;DR: RLVR微调使LLM过度自信，SFT校准更好但性能提升较小；作者提出校准感知的RL方法，在保持性能的同时改善校准


<details>
  <summary>Details</summary>
Motivation: LLM在决策任务中需要可靠的置信度估计，以便下游系统决定何时信任模型、何时使用备用机制。当前RLVR微调虽然提升任务性能，但导致模型过度自信，而SFT校准更好但性能提升有限。

Method: 系统研究SFT和RLVR两种微调范式的校准特性；通过实验诊断RLVR失败原因，发现决策标记在推理轨迹中仅作为决策提取步骤，不携带置信信息；提出校准感知的强化学习公式，直接调整决策标记概率。

Result: RLVR显著提高任务性能但产生极度过度自信的模型；SFT校准效果更好，即使在分布偏移下也保持良好，但性能提升较小；提出的校准感知RL方法在保持RLVR准确率水平的同时缓解过度自信，将ECE分数降低多达9个点。

Conclusion: RLVR微调导致LLM过度自信，而SFT提供更好的校准但性能有限；通过理解决策标记在推理中的作用，可以设计校准感知的强化学习方法，在保持性能的同时改善置信度校准。

Abstract: Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.

</details>


### [137] [CooperBench: Why Coding Agents Cannot be Your Teammates Yet](https://arxiv.org/abs/2601.13295)
*Arpandeep Khatua,Hao Zhu,Peter Tran,Arya Prabhudesai,Frederic Sadrieh,Johann K. Lieberwirth,Xinkai Yu,Yicheng Fu,Michael J. Ryan,Jiaxin Pei,Diyi Yang*

Main category: cs.LG

TL;DR: 论文提出CooperBench基准测试，评估AI代理在协作编程中的协调能力，发现当前代理存在"协调诅咒"——协作成功率比单独执行低30%，揭示了沟通、承诺和预期理解三大问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在复杂工作中日益协作，它们需要发展协调能力以成为有效的团队成员。然而，作者假设当前代理缺乏这些能力，需要建立评估框架来验证这一假设。

Method: 引入CooperBench基准测试，包含600多个协作编程任务，覆盖12个库和4种编程语言。每个任务分配两个代理不同的功能特性，这些特性可以独立实现，但缺乏协调可能产生冲突。任务基于真实开源仓库和专家编写的测试。

Result: 观察到"协调诅咒"现象：代理协作时的平均成功率比单独执行两个任务低30%。这与人类团队通常通过增加成员提高生产力的模式形成鲜明对比。分析揭示三大关键问题：沟通渠道堵塞、承诺偏离、对他人计划和沟通的错误预期。同时观察到罕见的涌现协调行为，如角色分工、资源分配和协商。

Conclusion: 研究提出了新颖的协作编程基准测试，呼吁从追求个体代理能力转向发展社会智能。当前AI代理在团队协作方面存在显著缺陷，需要新的研究方向来提升协调能力。

Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.

</details>


### [138] [Verifying Local Robustness of Pruned Safety-Critical Networks](https://arxiv.org/abs/2601.13303)
*Minh Le,Phuong Cao*

Main category: cs.LG

TL;DR: 研究发现剪枝对神经网络形式化验证有非线性影响：轻量剪枝（40%）在MNIST上、重度剪枝（70%-90%）在NASA JPL数据集上能提升可验证性，使模型在L∞鲁棒性证明上优于未剪枝基线。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的形式化验证对安全关键应用至关重要，但大规模模型的验证计算成本高昂，阻碍了实际应用。需要探索模型压缩（如剪枝）如何影响形式化验证效果。

Method: 使用最先进的α,β-CROWN验证器，在不同剪枝比例下评估ResNet4模型，在MNIST和NASA JPL火星霜冻识别数据集上进行实验，分析剪枝对形式化局部鲁棒性证书的影响。

Result: 发现剪枝与可验证性之间存在非线性关系：MNIST数据集上轻量剪枝（40%）效果最佳，而NASA JPL数据集上重度剪枝（70%-90%）反而能提升可验证性，使模型在L∞鲁棒性证明上超越未剪枝基线。

Conclusion: 减少网络连接简化了形式化求解器的搜索空间，最优剪枝比例因数据集差异显著。研究为在高风险环境中部署高效且经过形式化验证的DNN提供了关键见解。

Abstract: Formal verification of Deep Neural Networks (DNNs) is essential for safety-critical applications, ranging from surgical robotics to NASA JPL autonomous systems. However, the computational cost of verifying large-scale models remains a significant barrier to adoption. This paper investigates the impact of pruning on formal local robustness certificates with different ratios. Using the state-of-the-art $α,β$-CROWN verifier, we evaluate ResNet4 models across varying pruning ratios on MNIST and, more importantly, on the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a non-linear relationship: light pruning (40%) in MNIST and heavy pruning (70%-90%) in JPL improve verifiability, allowing models to outperform unpruned baselines in proven $L_\infty$ robustness properties. This suggests that reduced connectivity simplifies the search space for formal solvers and that the optimal pruning ratio varies significantly between datasets. This research highlights the complex nature of model compression, offering critical insights into selecting the optimal pruning ratio for deploying efficient, yet formally verified, DNNs in high-stakes environments where reliability is non-negotiable.

</details>


### [139] [Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans](https://arxiv.org/abs/2601.13350)
*Abdel Djalil Sad Saoud,Fred Maurice Ngolè Mboula,Hanane Slimani*

Main category: cs.LG

TL;DR: 提出一种基于谱嵌入的领域不变表示学习方法，将平滑传输计划解释为二分图邻接矩阵，用于解决训练和推理数据分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 训练和推理数据之间的分布偏移是机器学习中的核心挑战，会导致性能下降。现有的基于最优传输的无监督领域自适应方法依赖于使用传输计划近似Monge映射，但这种方法对正则化策略和超参数敏感，可能导致领域对齐偏差。

Method: 将平滑传输计划解释为连接源域和目标域的二分图邻接矩阵，通过谱嵌入推导领域不变的样本表示。

Result: 在音乐流派识别、音乐-语音判别以及不同诊断设置下使用时域反射的电缆缺陷检测和分类任务上进行了评估，取得了整体强劲的性能。

Conclusion: 提出的谱嵌入方法能够有效学习领域不变表示，在多个声学适应基准测试中表现出色，为解决分布偏移问题提供了新的解决方案。

Abstract: Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.

</details>


### [140] [On the Relation of State Space Models and Hidden Markov Models](https://arxiv.org/abs/2601.13357)
*Aydin Ghojogh,M. Hadi Sepanj,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 本文系统比较了HMMs、线性高斯状态空间模型、卡尔曼滤波和现代NLP状态空间模型，分析它们在概率图模型框架下的相似性和差异，并探讨了经典概率模型与现代深度学习模型之间的关系。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）和隐马尔可夫模型（HMMs）是序列数据建模的基础框架，广泛应用于信号处理、控制理论和机器学习。尽管它们具有相似的时间结构，但在潜在状态性质、概率假设、推理过程和训练范式上存在根本差异。近年来，确定性状态空间模型通过S4和Mamba等架构在自然语言处理中重新出现，引发了关于经典概率SSMs、HMMs和现代神经序列模型之间关系的新问题。

Method: 通过概率图模型的视角分析这些模型的公式化表达，研究它们的推理算法（包括前向-后向推理和卡尔曼滤波），并对比通过期望最大化（EM）和基于梯度的优化的学习过程。系统比较HMMs、线性高斯状态空间模型、卡尔曼滤波和当代NLP状态空间模型。

Result: 阐明了这些模型在何时等价、何时根本不同，以及现代NLP状态空间模型如何与经典概率模型相关联。通过突出结构相似性和语义差异，为理解这些模型提供了清晰的框架。

Conclusion: 本文的分析连接了控制理论、概率建模和现代深度学习的视角，为理解经典概率模型与现代深度学习模型之间的关系提供了系统框架，有助于澄清这些模型在理论和实践中的联系与区别。

Abstract: State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models.
  In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.

</details>


### [141] [CausationEntropy: Pythonic Optimal Causation Entropy](https://arxiv.org/abs/2601.13365)
*Kevin Slote,Jeremie Fish,Erik Bollt*

Main category: cs.LG

TL;DR: CausationEntropy v1.1是一个Python包，实现了最优因果熵（oCSE）算法及其优化扩展，用于从动态系统中发现因果网络，包含多种熵估计器和数据生成工具。


<details>
  <summary>Details</summary>
Motivation: 为因果网络发现提供一个标准化、易用的工具包，特别是针对复杂动态系统中的因果推断问题，填补现有工具在oCSE算法实现方面的空白。

Method: 基于最优因果熵（oCSE）理论，实现多种熵估计方法：高斯估计、k近邻、几何k近邻、核密度估计和泊松熵估计，并包含合成数据生成和可视化工具。

Result: 发布了CausationEntropy v1.1，包含完整文档、代码示例，支持PyPi安装，采用MIT开源许可，为复杂动态系统的因果发现提供基准工具。

Conclusion: 该软件包将成为复杂动态系统中因果发现的基准工具，其模块化设计支持未来扩展，有望促进因果网络建模领域的研究和应用。

Abstract: Optimal Causation Entropy (oCSE) is a robust causal network modeling technique that reveals causal networks from dynamical systems and coupled oscillators, distinguishing direct from indirect paths. CausationEntropy is a Python package that implements oCSE and several of its significant optimizations and methodological extensions. In this paper, we introduce the version 1.1 release of CausationEntropy, which includes new synthetic data generators, plotting tools, and several advanced information-theoretical causal network discovery algorithms with criteria for estimating Gaussian, k-nearest neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density (KDE) and Poisson entropic estimators. The package is easy to install from the PyPi software repository, is thoroughly documented, supplemented with extensive code examples, and is modularly structured to support future additions. The entire codebase is released under the MIT license and is available on GitHub and through PyPi Repository. We expect this package to serve as a benchmark tool for causal discovery in complex dynamical systems.

</details>


### [142] [Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility](https://arxiv.org/abs/2601.13398)
*Nickil Maveli,Antonio Vergari,Shay B. Cohen*

Main category: cs.LG

TL;DR: RTCE基准测试揭示LLMs在代码双向执行一致性上的缺陷，现有方法无法解决这一根本问题


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码基准测试中表现良好，但在双向代码执行中缺乏一致性推理能力，现有基准无法全面评估这种一致性

Method: 提出RoundTripCodeEval(RTCE)基准，包含四种代码执行推理任务，通过无执行、精确匹配的方式评估双向映射保真度

Result: 零样本提示、监督微调和自反思机制都只能带来有限改进，无法解决根本问题，表明当前LLMs缺乏真正的双向执行一致性

Conclusion: RTCE揭示了现有基准未捕捉到的新见解，表明LLMs缺乏可信代码推理所需的内在一致性，代码和数据集将开源

Abstract: LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.

</details>


### [143] [TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction](https://arxiv.org/abs/2601.13422)
*Dahai Yu,Rongchao Xu,Dingyi Zhuang,Yuheng Bu,Shenhao Wang,Guang Wang*

Main category: cs.LG

TL;DR: TrustEnergy是一个用于准确可靠用户级能源使用预测的统一框架，包含分层时空表示模块和顺序符合分位数回归模块，在预测准确性和不确定性量化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有能源使用预测方法大多忽视家庭间的空间相关性，难以扩展到个体化预测，且缺乏对动态不确定性的量化，无法满足准确细粒度的用户级预测需求。

Method: 提出TrustEnergy框架：1) 分层时空表示模块使用记忆增强时空图神经网络捕获宏观和微观能源使用模式；2) 顺序符合分位数回归模块动态调整不确定性边界，无需对数据分布做强假设。

Result: 与佛罗里达电力供应商合作评估，TrustEnergy相比最先进基线方法，预测准确性提升5.4%，不确定性量化改进5.7%。

Conclusion: TrustEnergy框架通过有效捕捉时空相关性和动态不确定性量化，实现了准确可靠的用户级能源使用预测，为电网管理、基础设施规划和灾害响应等应用提供了有效解决方案。

Abstract: Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.

</details>


### [144] [A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization](https://arxiv.org/abs/2601.13435)
*Shuozhe Li,Du Cheng,Leqi Liu*

Main category: cs.LG

TL;DR: WaveLSFormer：一种可学习的小波变换长短期Transformer模型，用于日内交易策略学习，通过端到端训练的多尺度分解和风险控制，在多个行业组上显著超越传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列的日内交易策略学习面临三大挑战：高噪声、非平稳性和资产间的强横截面依赖性。传统方法难以有效处理这些复杂特征，需要一种能够联合进行多尺度分解和收益导向决策学习的新方法。

Method: 提出WaveLSFormer模型：1) 可学习小波前端通过端到端训练的滤波器组生成低/高频分量，使用频谱正则化器确保稳定且分离良好的频带；2) 低引导高频注入(LGHI)模块融合多尺度信息，用高频线索细化低频表示并控制训练稳定性；3) 输出长/短头寸组合，按固定风险预算重新缩放，直接通过交易目标和风险感知正则化进行优化。

Result: 在5年每小时数据、6个行业组、10个随机种子的广泛实验中，WaveLSFormer始终优于MLP、LSTM和Transformer基线（无论是否使用固定离散小波前端）。在所有行业平均中，WaveLSFormer实现累计总策略收益0.607±0.045，夏普比率2.157±0.166，在盈利能力和风险调整收益方面显著超越最强基线。

Conclusion: WaveLSFormer通过可学习的小波分解和Transformer架构，有效解决了金融时间序列交易中的多尺度特征提取和风险控制问题，为日内交易策略学习提供了一种强大且稳健的解决方案。

Abstract: Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \pm 0.045$ and a Sharpe ratio of $2.157 \pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.

</details>


### [145] [BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions](https://arxiv.org/abs/2601.13445)
*Ashish S. Nair,Sandipp Krishnan Ravi,Itzel Salgado,Changjie Sun,Sayan Ghosh,Liping Wang*

Main category: cs.LG

TL;DR: 提出基于DeepSDF的涡轮叶片专用隐式生成框架，实现性能感知的可制造设计生成，通过连续SDF表示和可解释潜空间实现高精度重建与控制合成。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI在工程设计中缺乏性能感知建模和可制造设计生成能力，传统2D引导或非约束3D管道无法满足涡轮叶片设计的特定需求。

Method: 采用DeepSDF框架，使用连续符号距离函数表示几何形状，建立与叶片参数对齐的近高斯潜空间，通过神经网络将工程描述符映射到潜码，实现性能引导的几何生成。

Result: 实现高重建保真度，表面距离误差控制在最大叶片尺寸的1%以内，对未见设计具有鲁棒泛化能力，支持约束、目标和性能指标集成。

Conclusion: 该框架超越了传统设计管道，为数据驱动的涡轮叶片建模和概念生成提供了实用且可解释的解决方案，推进了性能感知的工程几何生成。

Abstract: Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.

</details>


### [146] [Fairness-informed Pareto Optimization : An Efficient Bilevel Framework](https://arxiv.org/abs/2601.13448)
*Sofiane Tanji,Samuel Vaiter,Yassine Laguel*

Main category: cs.LG

TL;DR: BADR是一个双层自适应重标量化框架，可为任意公平性指标恢复最优帕累托效率模型，解决了现有公平机器学习方法常产生帕累托无效模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习方法常产生帕累托无效模型，某些群体的性能可以在不损害其他群体的情况下得到改进。传统处理方法（如公平性正则化）经常出现此问题，而现有帕累托效率方法偏向特定公平视角，无法适应文献中广泛研究的各种公平性指标。

Method: 提出BADR（Bilevel Adaptive Rescalarisation）框架：下层是加权经验风险最小化任务，权重是各群体的凸组合；上层优化选定的公平性目标。开发了两种新颖的大规模单循环算法BADR-GD和BADR-SGD，并建立了收敛保证。

Result: 发布了badr开源Python工具箱，支持多种学习任务和公平性指标。广泛的数值实验表明BADR优于现有的帕累托效率公平性方法。

Conclusion: BADR提供了一个简单框架，能够为任意公平性指标恢复最优帕累托效率模型，解决了现有方法的局限性，并通过开源工具和收敛保证算法实现了实用化。

Abstract: Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.

</details>


### [147] [Quantum Qualifiers for Neural Network Model Selection in Hadronic Physics](https://arxiv.org/abs/2601.13463)
*Brandon B. Le,D. Keller*

Main category: cs.LG

TL;DR: 论文提出了一种量子机器学习诊断框架，通过量子限定器指导在强子物理问题中选择经典或量子神经网络模型，并应用于康普顿形状因子提取。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习架构的成熟，关键挑战不再是构建这些架构，而是识别它们在哪些场景下能提供相对于经典方法的实际优势。特别是在数据驱动的强子物理问题中，需要系统的方法来指导模型选择。

Method: 开发了一个诊断框架，核心是定量的量子限定器，基于数据的内在特性指导经典与量子深度神经网络之间的模型选择。通过受控的分类和回归研究，分析模型性能随复杂性、噪声和维度的系统趋势，并将这些趋势提炼为预测准则。

Result: 研究表明相对模型性能遵循复杂性、噪声和维度的系统趋势，这些趋势可以提炼为预测准则。在深度虚拟康普顿散射的康普顿形状因子提取应用中，量子限定器成功识别出有利于量子模型的运动学区域。

Conclusion: 这些结果为在精密强子物理中部署量子机器学习工具建立了一个原则性框架，为量子优势的实际应用提供了系统指导。

Abstract: As quantum machine-learning architectures mature, a central challenge is no longer their construction, but identifying the regimes in which they offer practical advantages over classical approaches. In this work, we introduce a framework for addressing this question in data-driven hadronic physics problems by developing diagnostic tools - centered on a quantitative quantum qualifier - that guide model selection between classical and quantum deep neural networks based on intrinsic properties of the data. Using controlled classification and regression studies, we show how relative model performance follows systematic trends in complexity, noise, and dimensionality, and how these trends can be distilled into a predictive criterion. We then demonstrate the utility of this approach through an application to Compton form factor extraction from deeply virtual Compton scattering, where the quantum qualifier identifies kinematic regimes favorable to quantum models. Together, these results establish a principled framework for deploying quantum machine-learning tools in precision hadronic physics.

</details>


### [148] [Preconditioning Benefits of Spectral Orthogonalization in Muon](https://arxiv.org/abs/2601.13474)
*Jianhao Ma,Yu Huang,Yuejie Chi,Yuxin Chen*

Main category: cs.LG

TL;DR: 论文分析了Muon优化器的简化变体，证明了在矩阵分解和线性Transformer的上下文学习中，该算法具有独立于条件数的线性收敛性，优于梯度下降和Adam。


<details>
  <summary>Details</summary>
Motivation: Muon优化器作为利用梯度谱正交化的大语言模型预训练里程碑算法，其底层机制特别是梯度正交化的作用仍不明确，缺乏端到端的理论分析来解释其在具体应用中的优势。

Method: 通过两个案例研究简化版Muon的有效性：矩阵分解和线性Transformer的上下文学习。在谱域中将Muon动态解耦为独立的标量序列，分析其收敛行为。

Result: 证明了简化Muon在这两个问题上都具有独立于相关条件数的线性收敛性，理论上优于梯度下降和Adam。揭示了谱正交化诱导的预处理效应。

Conclusion: 理论形式化了谱正交化带来的预处理效应，为Muon在矩阵优化问题中的有效性提供了理论解释，并可能推广到更广泛的应用场景。

Abstract: The Muon optimizer, a matrix-structured algorithm that leverages spectral orthogonalization of gradients, is a milestone in the pretraining of large language models. However, the underlying mechanisms of Muon -- particularly the role of gradient orthogonalization -- remain poorly understood, with very few works providing end-to-end analyses that rigorously explain its advantages in concrete applications. We take a step by studying the effectiveness of a simplified variant of Muon through two case studies: matrix factorization, and in-context learning of linear transformers. For both problems, we prove that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, provably outperforming gradient descent and Adam. Our analysis reveals that the Muon dynamics decouple into a collection of independent scalar sequences in the spectral domain, each exhibiting similar convergence behavior. Our theory formalizes the preconditioning effect induced by spectral orthogonalization, offering insight into Muon's effectiveness in these matrix optimization problems and potentially beyond.

</details>


### [149] [A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model](https://arxiv.org/abs/2601.13476)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: PRAIM：基于大语言模型和检索增强记忆的概率变分插补框架，用于解决电动汽车充电数据缺失问题，显著提升插补精度和下游预测性能


<details>
  <summary>Details</summary>
Motivation: 电动汽车基础设施中数据驱动应用的可靠性依赖于完整、高质量的充电数据。然而，现实世界中的EV数据集经常存在缺失记录，现有插补方法无法处理充电数据的复杂多模态特性，且通常采用"一站一模型"的局限范式，忽略了有价值的站间相关性。

Method: 开发了PRAIM框架：使用预训练语言模型编码异构数据（时间序列需求、日历特征、地理空间上下文）为统一语义表示；通过检索增强记忆从整个充电网络中检索相关示例；采用变分神经架构构建统一的插补模型来克服数据稀疏性。

Result: 在四个公共数据集上的广泛实验表明，PRAIM在插补精度和保持原始数据统计分布方面显著优于现有基线方法，并大幅提升了下游预测性能。

Conclusion: PRAIM通过结合大语言模型和检索增强记忆，有效解决了电动汽车充电数据缺失问题，为数据驱动的EV基础设施应用提供了可靠的完整数据支持。

Abstract: The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.

</details>


### [150] [StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing](https://arxiv.org/abs/2601.13522)
*Shuang Li*

Main category: cs.LG

TL;DR: 提出一种基于Tucker分解的随机交替最小化算法，直接在核心张量和因子矩阵上操作，避免重复张量投影，实现低维张量因子的高效小批量更新。


<details>
  <summary>Details</summary>
Motivation: 低Tucker秩张量感知是信号处理和机器学习中的基本问题，现有方法要么在全张量变量上操作且需要昂贵的张量投影，要么采用因子化公式但仍依赖全梯度计算，而大多数随机因子化方法仅限于张量分解设置。

Method: 提出一种随机交替最小化算法，直接在Tucker分解下的核心张量和因子矩阵上操作，避免重复张量投影，实现对低维张量因子的高效小批量更新。

Result: 在合成张量感知的数值实验中，与代表性随机张量恢复基线相比，所提算法在运行时间上表现出更优的收敛行为。

Conclusion: 该方法为低Tucker秩张量感知提供了一种高效的随机优化方案，避免了现有方法的计算瓶颈，在运行时间上具有优势。

Abstract: Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.

</details>


### [151] [MN-TSG:Continuous Time Series Generation with Irregular Observations](https://arxiv.org/abs/2601.13534)
*Xu Zhang,Junwei Deng,Chang Xu,Hao Li,Jiang Bian*

Main category: cs.LG

TL;DR: MN-TSG是一个基于专家混合神经控制微分方程的时间序列生成框架，专门处理不规则采样和稀疏观测数据，支持连续高分辨率生成。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法大多假设规则采样和固定输出分辨率，与现实世界中不规则采样、稀疏观测的场景不匹配，特别是在临床监测等应用中需要支持连续高分辨率生成。

Method: 提出MN-TSG框架，采用专家混合神经控制微分方程架构，包含动态参数化的专家函数和解耦设计，并利用现有TSG模型学习专家混合与生成时间序列的联合分布。

Result: 在10个公共和合成数据集上的实验表明，MN-TSG在从不规则到规则和不规则到连续生成任务上均优于强基线方法。

Conclusion: MN-TSG通过专家混合NCDE架构有效解决了不规则时间序列的连续生成问题，能够为每个样本生成适当的专家配置，支持精细的连续时间序列生成。

Abstract: Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series.
  Neural Controlled Differential Equations (NCDEs) have shown strong potential for modeling irregular time series, yet they still face challenges in capturing complex dynamic temporal patterns and supporting continuous TSG. To address these limitations, we propose MN-TSG, a novel framework that explores Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG models for irregular and continuous generation tasks.
  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically parameterized expert functions and a decoupled design that facilitates more effective optimization of MoE dynamics. Furthermore, we leverage existing TSG models to learn the joint distribution over the mixture of experts and the generated time series. This enables the framework not only to generate new samples, but also to produce appropriate expert configurations tailored to each sample, thereby supporting refined continuous TSG.
  Extensive experiments on ten public and synthetic datasets demonstrate the effectiveness of MN-TSG, consistently outperforming strong TSG baselines on both irregular-to-regular and irregular-to-continuous generation tasks.

</details>


### [152] [Patterning: The Dual of Interpretability](https://arxiv.org/abs/2601.13548)
*George Wang,Daniel Murfet*

Main category: cs.LG

TL;DR: 提出"模式化"作为机制可解释性的对偶问题：给定期望的泛化形式，确定产生它的训练数据。通过可逆性框架，可以从内部结构反推训练数据干预。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性研究神经网络如何泛化到训练数据之外，但缺乏从期望泛化形式反推所需训练数据的系统方法。本文旨在建立这种对偶关系，实现从"读取"内部结构到"写入"内部结构的转变。

Method: 基于敏感性概念，测量可观测量后验期望值对数据分布微小变化的响应。通过逆线性响应关系，确定引导模型达到目标内部配置的数据干预。具体包括：1) 沿主敏感性方向重新加权训练数据；2) 在完美训练准确度的任务中，通过目标化局部学习系数选择特定算法。

Result: 在小语言模型中，通过重新加权训练数据可以加速或延迟结构形成（如归纳电路）。在括号平衡任务中，模式化可以选择模型学习的具体算法，即使多个算法都能达到完美训练准确度。

Conclusion: 建立了用于读取内部结构的数学框架的可逆性，实现了从"读取"到"写入"的转变。模式化作为机制可解释性的对偶问题，为控制模型学习特定泛化模式提供了系统方法。

Abstract: Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the data intervention that steers the model toward a target internal configuration. We demonstrate patterning in a small language model, showing that re-weighting training data along principal susceptibility directions can accelerate or delay the formation of structure, such as the induction circuit. In a synthetic parentheses balancing task where multiple algorithms achieve perfect training accuracy, we show that patterning can select which algorithm the model learns by targeting the local learning coefficient of each solution. These results establish that the same mathematical framework used to read internal structure can be inverted to write it.

</details>


### [153] [ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits](https://arxiv.org/abs/2601.13563)
*Aryan Karmore*

Main category: cs.LG

TL;DR: ButterflyMoE通过将专家视为共享量化基质的几何重定向，而非独立权重矩阵，实现了专家数量的亚线性内存扩展，在256个专家时达到150倍内存压缩。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法需要存储N个独立的专家权重矩阵，内存复杂度为O(N·d²)，超过了边缘设备的存储预算。现有的压缩方法如量化、剪枝和低秩分解只能减少常数因子，无法解决线性扩展瓶颈。

Method: 提出ButterflyMoE方法，将专家视为共享量化基质的几何重定向。通过将学习到的旋转应用于共享的三元原型，每个专家从不同角度利用共享容量，实现O(d² + N·d log d)的内存复杂度。

Result: 在语言建模基准测试中，ButterflyMoE在256个专家时实现了150倍的内存压缩，精度损失可忽略。这使得64个专家可以部署在4GB设备上，而标准MoE只能部署8个专家。

Conclusion: 几何参数化打破了MoE模型的线性内存扩展瓶颈，通过共享容量和量化旋转实现了专家数量的亚线性内存增长，为边缘设备部署大规模MoE模型提供了可行方案。

Abstract: Linear memory scaling stores $N$ independent expert weight matrices requiring $\mathcal{O}(N \cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. Diversity among experts arises from viewing different angles of shared capacity, not from redundant storage. By applying learned rotations to a shared ternary prototype, each expert yields $\mathcal{O}(d^2 + N \cdot d \log d)$ memory -- sub-linear in the number of experts. The key insight: training these rotations with quantization reduces activation outliers and stabilizes extreme low bit training, where static methods collapse. Across language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction at 256 experts with negligible accuracy loss. This allows 64 experts to fit on 4GB devices compared to standard MoE's 8 experts, showing geometric parametrization breaks linear scaling.

</details>


### [154] [Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework](https://arxiv.org/abs/2601.13564)
*Yanheng Li,Zhichen Pu,Lijiang Yang,Zehao Zhou,Yi Qin Gao*

Main category: cs.LG

TL;DR: LUMOS是一个数据与物理双驱动的荧光分子逆设计框架，通过结合生成器与预测器在共享潜在表示中，实现从规格到分子的直接设计，并在多目标优化中表现出色。


<details>
  <summary>Details</summary>
Motivation: 设计具有特定光学和物理化学性质的荧光小分子需要探索巨大的化学空间，同时满足多个目标和约束。传统的生成-评分-筛选方法在现实设计场景下效率低下，机器学习预测的泛化能力不可靠，量子化学计算成本过高。

Method: LUMOS将生成器和预测器耦合在共享潜在表示中，实现直接规格到分子设计。结合神经网络与快速TD-DFT计算流程，构建具有不同速度-精度-泛化性权衡的互补预测器。采用属性引导的扩散模型与多目标进化算法集成，支持多目标和约束下的从头设计和分子优化。

Result: 在综合基准测试中，LUMOS在荧光性质预测的准确性、泛化性和物理合理性方面持续优于基线模型，在多目标骨架和片段级分子优化中表现优异。通过TD-DFT和MD模拟验证，LUMOS能够生成满足各种目标规格的有效荧光团。

Conclusion: LUMOS建立了一个数据与物理双驱动的通用荧光团逆设计框架，为荧光分子的高效设计提供了有效的解决方案。

Abstract: Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.

</details>


### [155] [Self-Improvement as Coherence Optimization: A Theoretical Account](https://arxiv.org/abs/2601.13566)
*Tianyi Qiu,Ahmed Hani Ismail,Zhonghao He,Shi Feng*

Main category: cs.LG

TL;DR: 该论文提出"一致性优化"理论框架，解释语言模型无需外部监督即可自我提升的现象，证明辩论、引导和内部一致性最大化等方法都是该框架的特例，并建立了与描述长度正则化的等价关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法如辩论、引导和内部一致性最大化能让语言模型在无外部监督下提升准确性，甚至匹配有监督微调的性能，但这些方法为何有效缺乏理论解释。

Method: 提出"一致性优化"理论框架：寻找最可压缩且联合可预测的上下文到行为的映射。证明该方法等价于描述长度正则化，并推导出在预训练模型下该正则化方案在半监督学习中的最优性。

Result: 理论分析表明一致性优化能解释无反馈自我提升的工作原理，并预测其成功或失败的条件。初步实验支持理论结论。

Conclusion: 一致性优化为语言模型的无监督自我提升提供了统一的理论框架，解释了现有方法的有效性，并指导未来无监督学习方法的开发。

Abstract: Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.

</details>


### [156] [DRGW: Learning Disentangled Representations for Robust Graph Watermarking](https://arxiv.org/abs/2601.13569)
*Jiasen Li,Yanwei Liu,Zhuoyi Shang,Xiaoyan Gu,Weiping Wang*

Main category: cs.LG

TL;DR: DRGW是首个基于解耦表示学习的图水印框架，通过分离图结构和数字表示来解决现有方法在透明度和鲁棒性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图水印方法主要在图结构或纠缠的图表示上操作，由于图表示中的信息耦合以及连续数值表示转换为图结构时的不可控离散化，导致水印的透明度和鲁棒性受损。

Method: 1) 设计对抗训练的编码器学习对扰动的结构不变表示，并推导统计独立的水印载体；2) 设计图感知的可逆神经网络提供无损的水印嵌入和提取通道；3) 开发结构感知编辑器将潜在修改解析为离散图编辑。

Result: 在多个基准数据集上的实验表明DRGW具有优越的有效性。

Conclusion: DRGW通过解耦表示学习解决了图水印中的透明度和鲁棒性问题，为图数据知识产权保护提供了有效解决方案。

Abstract: Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.

</details>


### [157] [GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds](https://arxiv.org/abs/2601.13570)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: cs.LG

TL;DR: GeoDynamics是一个几何状态空间神经网络，直接在对称正定流形上建模脑功能连接动态，用于追踪任务驱动状态变化和疾病早期标志物


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型通常将大脑视为松散连接区域或施加过度简化的网络先验，缺乏真正的整体自组织动态系统视角。脑功能连接矩阵位于黎曼流形而非欧几里得空间，需要几何感知的方法来捕捉其动态轨迹

Method: GeoDynamics将每个连接矩阵嵌入到流形感知的循环框架中，在高维对称正定流形上直接学习平滑且尊重几何的转换，揭示任务驱动状态变化和疾病标志物

Result: 模型成功揭示了任务驱动的状态变化，以及阿尔茨海默病、帕金森病和自闭症的早期标志物。在人类动作识别基准测试（UTKinect、Florence、HDM05）上也验证了其可扩展性和鲁棒性

Conclusion: GeoDynamics提供了一种在几何流形上直接建模脑动态的新方法，不仅适用于神经科学，还能扩展到其他复杂时空动态建模领域，展示了跨领域应用的潜力

Abstract: State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieved powerful fits to functional neuroimaging data. However, most existing approaches still view the brain as a set of loosely connected regions or impose oversimplified network priors, falling short of a truly holistic and self-organized dynamical system perspective. Brain functional connectivity (FC) at each time point naturally forms a symmetric positive definite (SPD) matrix, which resides on a curved Riemannian manifold rather than in Euclidean space. Capturing the trajectories of these SPD matrices is key to understanding how coordinated networks support cognition and behavior. To this end, we introduce GeoDynamics, a geometric state-space neural network that tracks latent brain-state trajectories directly on the high-dimensional SPD manifold. GeoDynamics embeds each connectivity matrix into a manifold-aware recurrent framework, learning smooth and geometry-respecting transitions that reveal task-driven state changes and early markers of Alzheimer's disease, Parkinson's disease, and autism. Beyond neuroscience, we validate GeoDynamics on human action recognition benchmarks (UTKinect, Florence, HDM05), demonstrating its scalability and robustness in modeling complex spatiotemporal dynamics across diverse domains.

</details>


### [158] [Behavior Knowledge Merge in Reinforced Agentic Models](https://arxiv.org/abs/2601.13572)
*Xiangchi Yuan,Dachuan Shi,Chunhui Zhang,Zheyuan Liu,Shenglong Yao,Soroush Vosoughi,Wenke Lee*

Main category: cs.LG

TL;DR: RAM：针对RL训练智能体模型的分布感知合并框架，通过分离共享和任务特定参数更新，解决传统合并方法在RL智能体上的性能损失问题


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要针对监督微调设计，不适用于RL训练的智能体模型。RL产生的任务向量稀疏且异构，而SFT合并假设密集且全局可比，导致标准全局平均会稀释RL任务特定的关键行为参数更新

Method: RAM（Reinforced Agent Merging）框架：1）解耦共享和任务特定的独特参数更新；2）对共享组件进行平均；3）选择性保留并重新缩放独特组件以抵消参数更新稀释

Result: 在多个智能体领域和模型架构上的实验表明，RAM不仅超越了现有合并基线，还能解锁智能体间的协同潜力，在某些领域甚至优于专门训练的智能体

Conclusion: RAM是针对RL训练智能体模型的专门合并框架，解决了任务向量不匹配问题，能够有效整合多个RL智能体，实现优于专门智能体的性能

Abstract: Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

</details>


### [159] [FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning](https://arxiv.org/abs/2601.13578)
*Qian Feng,JiaHang Tu,Mintong Kang,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.LG

TL;DR: 提出FG-OrIU框架，通过特征和梯度的双重正交约束实现深度遗忘，解决增量遗忘中的表面遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有增量遗忘方法主要在参数层面抑制或混淆知识，缺乏对特征和梯度层面的显式约束，导致"表面遗忘"——残留信息仍可恢复。这种不完全遗忘存在安全风险，并破坏保留平衡，特别是在增量遗忘场景中。

Method: 提出FG-OrIU框架：1) 使用SVD分解特征空间，将遗忘类和保留类特征分离到不同子空间；2) 实施双重正交约束：特征正交投影确保遗忘类和保留类特征分离，梯度正交投影防止更新过程中重新引入遗忘知识并干扰保留类；3) 动态子空间适应机制，合并新遗忘子空间并收缩保留子空间，确保序列遗忘任务中的稳定平衡。

Result: 大量实验证明了该方法的有效性，能够实现深度遗忘（遗忘效果不可逆），解决了表面遗忘问题。

Conclusion: FG-OrIU是首个统一特征和梯度层面正交约束的框架，通过双重正交约束和动态子空间适应，实现了深度遗忘，在序列遗忘任务中保持了移除和保留的稳定平衡。

Abstract: Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\textbf{F}eature-\textbf{G}radient \textbf{Or}thogonality for \textbf{I}ncremental \textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.

</details>


### [160] [Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models](https://arxiv.org/abs/2601.13580)
*Ahmad Al-Zuraiqi*

Main category: cs.LG

TL;DR: Neural Organ Transplantation (NOT) 是一种模块化适配框架，可将训练好的Transformer层作为可重用检查点进行跨模型移植，显著优于LoRA等现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法将训练参数与特定模型实例和训练数据紧密耦合，缺乏模块化和可重用性。NOT旨在实现隐私保护的专家知识共享，通过检查点分发实现高效领域适配。

Method: 从预训练模型中提取连续层子集（"捐赠器官"），在领域特定数据上独立训练，保存为独立检查点文件，然后移植到兼容的接收模型中，无需原始训练数据。

Result: 在124M到20B参数的三种仅解码器Transformer架构上，捐赠移植显著优于现有适配方法，困惑度比LoRA提升一个数量级，训练速度更快。早期插入位置效果最佳，十亿参数规模的跨领域转移显示出意外的正则化效益。

Conclusion: Transformer中间层支持仅解码器架构的高效模块化转移，可通过检查点分发实现隐私保护的专家知识共享。该方法目前仅限于仅解码器模型，在编码器架构上效果有限。

Abstract: We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets ("donor organs") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.

</details>


### [161] [Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments](https://arxiv.org/abs/2601.13592)
*Hao Jing,Sa Xiao,Haoyu Li,Huadong Xiao,Wei Xue*

Main category: cs.LG

TL;DR: 使用残差卷积神经网络替代RRTMG辐射方案，通过离线训练在线耦合方法，在保持精度的同时将计算速度提升约8倍，实现10天稳定预报。


<details>
  <summary>Details</summary>
Motivation: 辐射过程是数值模型中最耗时的物理过程，需要提高计算效率。研究关注混合预报框架中嵌入深度神经网络的两个关键限制：耦合兼容性和长期积分稳定性。

Method: 采用残差卷积神经网络近似RRTMG辐射方案，使用离线训练在线耦合方法。通过模型模拟生成包含有云和无云大气柱的完整数据集，采用经验回放增强数据稳定性，并基于物理意义添加输出约束。使用基于LibTorch的耦合方法进行实时计算。

Result: 混合模型能够完成10天积分预报需求。两个月的业务回算实验表明，机器学习模拟器达到与传统物理方案相当的精度，同时将计算速度提升约8倍。

Conclusion: 成功开发了能够稳定运行10天预报的混合模型，证明了机器学习方法在替代传统辐射方案方面的可行性和效率优势，为业务数值预报系统提供了有效的加速方案。

Abstract: Radiation is typically the most time-consuming physical process in numerical models. One solution is to use machine learning methods to simulate the radiation process to improve computational efficiency. From an operational standpoint, this study investigates critical limitations inherent to hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, with a specific focus on two fundamental bottlenecks: coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration. We adopted an offline training and online coupling approach. First, a comprehensive dataset is generated through model simulations, encompassing all atmospheric columns both with and without cloud cover. To ensure the stability of the hybrid model, the dataset is enhanced via experience replay, and additional output constraints based on physical significance are imposed. Meanwhile, a LibTorch-based coupling method is utilized, which is more suitable for real-time operational computations. The hybrid model is capable of performing ten-day integrated forecasts as required. A two-month operational reforecast experiment demonstrates that the machine learning emulator achieves accuracy comparable to that of the traditional physical scheme, while accelerating the computation speed by approximately eightfold.

</details>


### [162] [Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models](https://arxiv.org/abs/2601.13599)
*Linrui Ma,Yufei Cui,Kai Han,Yunhe Wang*

Main category: cs.LG

TL;DR: 提出Diffusion in Diffusion框架，通过"草稿-精炼"两阶段解决块扩散语言模型的不可逆性和短视问题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 块扩散语言模型结合了自回归和扩散模型的优势，但其严格的单向块依赖导致不可逆性和牺牲了扩散模型的全局规划能力

Method: 采用"草稿-精炼"框架：1) 用小块进行块扩散生成快速草稿；2) 用更大的双向感受野进行全局双向扩散精炼；使用快照置信度重掩码识别需要修改的关键token，并采用混合尺度训练扩展块扩散模型的全局能力

Result: 在OpenWebText数据集上为离散扩散模型设定了新基准，仅使用基线模型26%的微调预算，将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距

Conclusion: Diffusion in Diffusion框架有效解决了块扩散模型的局限性，通过两阶段方法实现了更好的全局规划和更高效的生成质量

Abstract: Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.

</details>


### [163] [Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2601.13608)
*Zhipeng Chang,Ting He,Wenrui Hao*

Main category: cs.LG

TL;DR: 提出FIPA方法，使用Fisher信息矩阵进行参数级加权聚合，解决非IID数据下的客户端漂移问题


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg）对所有参数使用相同的标量权重，在非IID数据下会导致客户端更新严重错位，造成客户端漂移和全局模型性能下降

Method: 提出Fisher信息矩阵参数化聚合（FIPA），用参数特定的Fisher信息矩阵权重替代客户端级标量权重，实现真正的参数级缩放，捕捉每个客户端数据对不同参数的独特影响，并通过低秩近似保持通信和计算效率

Result: 在非线性函数回归、PDE学习和图像分类任务中，FIPA始终优于基于平均的聚合方法，并能有效结合最先进的客户端优化算法进一步提升图像分类准确率

Conclusion: FIPA方法在异构数据分布下的联邦学习中具有显著优势，通过参数级加权聚合改善了模型性能

Abstract: Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.

</details>


### [164] [Quadratic Upper Bound for Boosting Robustness](https://arxiv.org/abs/2601.13645)
*Euijin You,Hyang-Won Lee*

Main category: cs.LG

TL;DR: 提出一种二次上界损失函数来改进快速对抗训练，通过平滑损失景观提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 快速对抗训练(FAT)虽然减少了训练时间，但往往因对抗空间探索不足而导致鲁棒性下降，需要解决这一权衡问题

Method: 推导对抗训练损失函数的二次上界(QUB)，并将该上界损失与现有FAT方法结合使用

Result: 将QUB损失应用于现有方法显著提升了鲁棒性，并通过多种指标证明这种改进源于所得模型的平滑损失景观

Conclusion: 二次上界损失函数能有效缓解快速对抗训练中的鲁棒性下降问题，通过平滑损失景观实现更好的鲁棒性-效率权衡

Abstract: Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.

</details>


### [165] [TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation](https://arxiv.org/abs/2601.13653)
*Xingjian Wu,Junkai Lu,Zhengyu Li,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: TimeART：一个融合强效现成工具分析能力和大语言模型推理能力的框架，作为全自动数据科学家处理时间序列问答任务


<details>
  <summary>Details</summary>
Motivation: 当前时间序列数据分析主要依赖人工数据科学家，成本高昂且缺乏自动化，需要开发自动化解决方案来处理时间序列问答任务

Method: 1) 提出TimeART框架，融合现成工具分析能力和LLM推理能力；2) 收集10万专家轨迹语料库TimeToolBench；3) 设计四阶段训练策略，通过早期经验和自我反思增强模型泛化能力

Result: 训练了一个8B参数的时间序列推理模型，在多个时间序列问答任务上取得了最先进的性能

Conclusion: TimeART框架开创了代理式时间序列推理的新方法，实现了全自动的时间序列数据分析

Abstract: Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.

</details>


### [166] [Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery](https://arxiv.org/abs/2601.13676)
*Fabian Greifeneder,Wolfgang Fenz,Benedikt Alkin,Johannes Brandstetter,Michael Giretzlehner,Philipp Moser*

Main category: cs.LG

TL;DR: 提出基于深度学习的脑组织变形模拟替代模型，使用Universal Physics Transformers处理大规模网格数据，通过随机教师强制策略减少误差累积，实现实时神经外科手术模拟。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器难以满足神经外科手术模拟器的实时性能要求，需要开发能够高效模拟手术器械与脑组织连续交互引起的瞬态变形的替代模型。

Method: 基于Universal Physics Transformers构建深度学习替代模型，直接处理大规模网格数据；使用非线性有限元模拟生成的大规模数据集进行训练；提出随机教师强制策略，在训练中逐步减少真实输入比例，增加模型预测输入。

Result: 模型能够准确预测多种瞬态脑变形场景，支持多达15万个节点的网格；随机教师强制策略将最大预测误差从6.7mm降低到3.5mm；在消费级硬件上实现每步模拟时间低于10ms，可集成到交互式神经外科模拟环境中。

Conclusion: 提出的深度学习框架能够实现快速、平滑且准确的动态脑组织生物力学模拟，为真实的手术训练环境奠定了基础。

Abstract: Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.

</details>


### [167] [Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation](https://arxiv.org/abs/2601.13698)
*Arjun Nichani,Hsiang Hsu,Chun-Fu,Chen,Haewon Jeong*

Main category: cs.LG

TL;DR: 本文使用信息论工具Chernoff信息分析公平性、隐私性和准确性三者之间的关系，提出Noisy Chernoff Difference作为分析工具，揭示了这种关系的分布依赖性，并在合成和真实数据上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 公平性和隐私性是可信机器学习的两个重要支柱，但公平性与隐私性之间的关系研究相对较少。本文旨在通过信息论方法同时分析公平性、隐私性和准确性三者之间的关系，揭示其数据依赖性。

Method: 使用信息论度量Chernoff信息，定义Noisy Chernoff Difference作为分析工具。首先在合成数据上分析该值在不同数据分布下的三种行为模式，然后提出从未知分布数据中估计Chernoff信息的方法，并在真实数据集上应用该框架。

Result: 研究发现Noisy Chernoff Difference在合成数据中表现出三种不同的行为模式（取决于数据分布），揭示了公平性和隐私性的不同含义。该值可作为公平性-准确性曲线陡峭度的代理指标。在真实数据集上的分析进一步验证了该框架的有效性。

Conclusion: 本文通过信息论方法统一理解公平性、隐私性和准确性之间的关系，强调了这种关系的分布依赖性。提出的分析框架为同时研究这三个重要维度提供了工具，有助于构建更全面的可信机器学习理论。

Abstract: Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.

</details>


### [168] [Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction](https://arxiv.org/abs/2601.13710)
*Sayeed Shafayet Chowdhury,Snehasis Mukhopadhyay,Shiaofen Fang,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 研究比较监督式机器学习与生成式AI在预测慢性鼻窦炎手术效果上的表现，发现ML模型（MLP）在准确率、校准和临床决策效益上优于GenAI，建议采用ML为主、GenAI为辅的工作流程。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医学影像领域已有广泛应用，但在临床数据上进行前瞻性决策支持的应用仍然有限。本研究旨在探索术前预测慢性鼻窦炎手术效果的可行性，特别是识别那些手术效果不佳、本应避免手术的患者。

Method: 在前瞻性收集的队列中（所有患者均接受手术），使用相同的结构化术前临床数据，比较监督式机器学习（逻辑回归、树集成、自研MLP）与生成式AI（ChatGPT、Claude、Gemini、Perplexity）的表现。约束输出为二元推荐及置信度，并建立可重复的表格数据到GenAI的评估协议。

Result: 最佳ML模型（MLP）达到85%准确率，具有更优的校准和决策曲线净效益。生成式AI在判别和校准方面表现较差。值得注意的是，GenAI的推理依据与临床医生启发式方法和MLP特征重要性一致，都强调基线SNOT-22评分、CT/内镜严重程度、息肉表型以及心理/疼痛共病。

Conclusion: 研究支持ML优先、GenAI增强的工作流程：部署校准的ML模型进行手术候选者的初步筛查，使用GenAI作为解释工具以增强透明度和共同决策制定。

Abstract: Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.

</details>


### [169] [EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory](https://arxiv.org/abs/2601.13748)
*Tien-Dat Pham,Xuan-The Tran*

Main category: cs.LG

TL;DR: EEG-Titans：一种双分支架构，结合滑动窗口注意力和循环记忆通路，用于癫痫发作预测，在CHB-MIT数据集上达到99.46%的平均段级敏感度


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测面临挑战，因为发作前动态可能跨越长时间范围，而临床相关特征可能微妙且短暂。现有深度学习模型在捕获局部时空模式和保持长距离上下文信息之间存在权衡。

Method: 提出EEG-Titans双分支架构，结合现代神经记忆机制进行长上下文建模。使用滑动窗口注意力捕获短期异常，同时采用循环记忆通路总结随时间推移的缓慢渐进趋势。

Result: 在CHB-MIT头皮EEG数据集上，按时间顺序保留协议评估，EEG-Titans在18名受试者中达到99.46%的平均段级敏感度。通过分层上下文策略扩展高噪声受试者的感受野，可显著减少误报（极端异常情况下降至0.00 FPR/h）而不牺牲敏感度。

Conclusion: 记忆增强的长上下文建模可以在临床约束评估下提供稳健的癫痫发作预测，表明这种方法能够有效平衡局部特征捕获和长距离上下文建模的需求。

Abstract: Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation

</details>


### [170] [vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.13768)
*Wenzhen Yue,Ruohao Guo,Ji Shi,Zihan Hao,Shiyu Hu,Xianghua Ying*

Main category: cs.LG

TL;DR: vLinear是一种基于线性运算的高效多元时间序列预测器，包含vecTrans模块和WFMLoss目标函数，在保持高性能的同时将复杂度从O(N²)降低到O(N)


<details>
  <summary>Details</summary>
Motivation: 现有最先进的预测器通常依赖自注意力或其变体来捕捉多元相关性，但计算复杂度为O(N²)，随着变量数量增加计算开销巨大。需要设计更高效的模型来降低复杂度同时保持性能。

Method: 提出两个核心组件：1) vecTrans模块，使用可学习向量建模多元相关性，将复杂度降至O(N)，可无缝集成到Transformer预测器中；2) WFMLoss目标函数，采用最终序列导向的加权流匹配损失，包含路径和水平加权策略，专注于更可靠的路径和预测水平。

Result: 在22个基准测试和124个预测设置中达到最先进性能；vecTrans可提供高达5倍的推理加速和一致的性能提升；WFMLoss作为即插即用目标函数，能持续改进现有预测器。

Conclusion: vLinear通过vecTrans模块和WFMLoss目标函数，实现了高效且高性能的多元时间序列预测，在降低计算复杂度的同时保持甚至提升预测准确性，为实际应用提供了实用解决方案。

Abstract: In this paper, we present \textbf{vLinear}, an effective yet efficient \textbf{linear}-based multivariate time series forecaster featuring two components: the \textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \textbf{velocity-oriented} flow matching objectives, we demonstrate that a \textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.

</details>


### [171] [Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks](https://arxiv.org/abs/2601.13776)
*Thibaut Boissin,Franck Mamalet,Valentin Lafargue,Mathieu Serrurier*

Main category: cs.LG

TL;DR: Orthogonium是一个统一的PyTorch库，提供正交和1-Lipschitz神经网络层，解决了现有实现分散、有限且计算成本高的问题，支持标准卷积功能并保持严格数学保证。


<details>
  <summary>Details</summary>
Motivation: 正交和1-Lipschitz神经网络层对于认证对抗鲁棒性、稳定生成模型和可靠循环网络至关重要，但现有实现分散、有限且计算成本高，需要标准化可靠工具。

Method: 开发Orthogonium库，提供统一高效的PyTorch实现，支持标准卷积功能（步长、扩张、分组、转置），同时保持严格数学保证，并优化大规模基准测试性能。

Result: 库的优化实现减少了ImageNet等大规模基准测试的开销，并通过严格测试发现了现有实现的关键错误，显著降低了采用障碍。

Conclusion: Orthogonium为需要正交性和鲁棒Lipschitz约束的多样化应用提供了可扩展的实验和集成平台，是标准化可靠工具的重要贡献。

Abstract: Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at https://github.com/deel-ai/orthogonium.

</details>


### [172] [Principled Latent Diffusion for Graphs via Laplacian Autoencoders](https://arxiv.org/abs/2601.13780)
*Antoine Siraudin,Christopher Morris*

Main category: cs.LG

TL;DR: LG-Flow：一种潜在图扩散框架，通过将图压缩到低维潜在空间进行扩散，解决了传统图扩散模型二次复杂度和稀疏图建模效率低的问题，实现了近无损重建和1000倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统图扩散模型存在两个主要问题：1）节点数量的二次复杂度限制了模型规模；2）在稀疏图中大量计算资源被浪费在建模不存在的边上。虽然其他模态已成功应用潜在扩散，但图生成需要近乎无损的重建，因为邻接矩阵中的单个错误就可能导致整个样本无效。

Method: 提出LG-Flow框架：1）使用置换等变自编码器将每个节点映射到固定维度的嵌入，从该嵌入中可以证明地完全恢复完整邻接矩阵，实现无向图和有向无环图的近无损重建；2）潜在表示维度与节点数量线性相关，消除二次瓶颈；3）在潜在空间中训练基于流匹配的扩散变换器，实现高效且表达力强的图生成。

Result: LG-Flow在性能上与最先进的图扩散模型竞争，同时实现了高达1000倍的加速。潜在表示的线性缩放使得训练更大、更具表达力的模型成为可能。

Conclusion: LG-Flow成功解决了图潜在扩散中的关键挑战，通过置换等变自编码器实现近无损重建，消除了传统图扩散模型的二次复杂度瓶颈，为高效、可扩展的图生成提供了新途径。

Abstract: Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\times$ speed-up.

</details>


### [173] [PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles](https://arxiv.org/abs/2601.13793)
*ByeoungDo Kim,JunYeop Na,Kyungwook Tak,JunTae Kim,DongHyeon Kim,Duckky Kim*

Main category: cs.LG

TL;DR: 提出一种基于注意力机制的ETA预测模型，通过历史道路速度模式实现高效准确的到达时间估计


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和智能交通系统普及，准确可靠的ETA预测在导航、出行规划和交通管理中变得至关重要。然而，由于交通流的动态复杂性，传统方法要么简单组合实时和历史数据，要么依赖复杂规则，而现有深度学习模型计算成本高且未能有效捕捉时空模式。

Method: 提出基于注意力机制的ETA模型，利用注意力机制提取和利用沿路线每个时空点累积的时空特征。该架构能够高效准确地估计ETA，同时保持模型轻量化和可扩展性。

Result: 使用真实世界驾驶数据集验证，该方法优于现有基线模型，能够有效整合道路特征、实时交通条件和历史速度模式。

Conclusion: 提出的基于注意力机制的ETA模型能够有效解决时空因果关系，实现高效准确的到达时间预测，为导航和交通管理提供可靠解决方案。

Abstract: In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.

</details>


### [174] [ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks](https://arxiv.org/abs/2601.13824)
*Xiaohong Yang,Tong Xie,Minghui Liwang,Chikai Shang,Yang Lu,Zhenzhen Jiao,Liqun Fu,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: ELSA框架结合分割学习和分层联邦学习，通过客户端聚类、模型分割和轻量通信方案，在资源受限的边缘网络中高效微调大语言模型。


<details>
  <summary>Details</summary>
Motivation: 边缘设备训练大语言模型面临三大挑战：设备资源限制、数据异构性和隐私风险。现有方法难以同时解决这些问题，需要一种新的框架来支持资源受限边缘网络中的分布式LLM微调。

Method: 1. 任务无关的行为感知客户端聚类机制，使用公共探针输入和对称KL散度构建语义指纹，结合预测一致性信任评分和延迟感知边缘分配；2. 将LLM分割为三部分分布在客户端和边缘服务器，云端仅用于适配器聚合；3. 基于计算草图结合语义子空间正交扰动的轻量通信方案。

Result: 在多种NLP任务上的实验表明，ELSA在适应性、收敛行为和鲁棒性方面持续优于最先进方法，为资源受限环境下的边缘侧LLM微调提供了可扩展且隐私感知的解决方案。

Conclusion: ELSA通过系统集成分割学习和分层联邦学习，有效解决了边缘LLM训练的资源约束、数据异构和隐私问题，为边缘智能中的大语言模型部署提供了实用框架。

Abstract: Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.

</details>


### [175] [Optimal L2 Regularization in High-dimensional Continual Linear Regression](https://arxiv.org/abs/2601.13844)
*Gilad Karpel,Edward Moroshko,Ran Levinstein,Ron Meir,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 研究过参数化持续线性回归中的泛化问题，推导出高维情况下任意线性教师的闭式解，发现各向同性正则化能缓解标签噪声，最优正则强度随任务数T以T/lnT缩放


<details>
  <summary>Details</summary>
Motivation: 研究持续学习中的泛化问题，特别是在过参数化线性回归设置下，探索正则化如何影响模型在序列任务上的泛化性能，解决现有工作在处理多教师时要么不使用正则化、要么使用内存密集型方法的问题

Method: 采用过参数化持续线性回归框架，使用L2（各向同性）正则化，推导高维状态下任意线性教师的闭式表达式，分析单教师和多独立同分布教师设置，证明最优正则强度与任务数的关系

Result: 各向同性正则化能有效缓解标签噪声，最优固定正则强度随任务数T以T/lnT的比例缩放，这是持续学习理论中的首个此类结果，在线性回归和神经网络实验中验证了理论发现

Conclusion: 各向同性正则化是持续学习中缓解标签噪声的有效方法，最优正则强度的T/lnT缩放律为持续学习系统设计提供了实用指导，理论结果在真实模型中得到验证

Abstract: We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.

</details>


### [176] [Inverting Self-Organizing Maps: A Unified Activation-Based Framework](https://arxiv.org/abs/2601.13851)
*Alessandro Londei,Matteo Benati,Denise Lanzieri,Vittorio Loreto*

Main category: cs.LG

TL;DR: 提出MUSIC方法，基于自组织映射(SOM)的原型距离几何特性实现精确输入恢复和可控语义轨迹生成，无需采样、先验或编码器-解码器架构。


<details>
  <summary>Details</summary>
Motivation: 传统SOM主要用于可视化和聚类，但其激活模式（到原型的平方距离）包含足够信息来精确恢复输入。利用欧氏距离几何原理，探索如何基于SOM原型实现数据恢复和可控语义操作。

Method: 基于D+1个仿射独立参考点的距离唯一确定D维点的几何原理，推导线性系统并分析反演条件。提出MUSIC更新规则，通过修改选定原型的平方距离同时保持其他距离，实现确定性几何流。使用Tikhonov正则化稳定更新规则。

Result: 在合成高斯混合、MNIST和Faces in the Wild数据集上验证，MUSIC能生成平滑、可解释的轨迹，揭示学习流形的底层几何结构，实现精确输入恢复和可控语义变化。

Conclusion: SOM激活模式包含足够信息用于精确输入恢复和可控语义操作，MUSIC为基于原型几何的数据增强和潜在探索提供了新视角，展示了SOM反演相比无监督聚类的优势。

Abstract: Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.

</details>


### [177] [Multi-Objective Hierarchical Optimization with Large Language Models](https://arxiv.org/abs/2601.13892)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Frank Hutter,Arber Zela*

Main category: cs.LG

TL;DR: LLM作为替代模型和候选采样器，在分层搜索策略中用于多目标优化，通过自适应分区和局部推理实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理能力方面表现出色，但尚未成为多目标优化的现成选择。传统方法在数值处理和探索-利用平衡方面表现更好，因此需要将LLM的优势与结构化搜索策略结合。

Method: 提出分层搜索策略：1）将输入空间自适应划分为不相交的超矩形区域；2）使用复合评分函数对区域排序；3）将LLM的生成过程限制在具有高潜力的子空间内，使其仅需进行局部推理而非全局推理。

Result: 在标准正则性假设下，算法生成的候选解在Hausdorff距离上收敛到真实Pareto集。实证表明，该方法持续优于基于LLM的全局多目标优化器，并在合成和真实世界基准测试中与标准进化和贝叶斯优化算法相当。

Conclusion: 通过将LLM作为替代模型和候选采样器集成到结构化分层搜索策略中，成功弥补了LLM在多目标优化领域的应用空白，实现了局部推理与全局优化的有效结合。

Abstract: Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.

</details>


### [178] [TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography](https://arxiv.org/abs/2601.13897)
*Ankita Joshi,Ashutosh Sharma,Anoushkrit Goel,Ranjeet Ranjan Jha,Chirag Ahuja,Arnav Bhavsar,Aditya Nigam*

Main category: cs.LG

TL;DR: 提出TractRLFusion，一种基于GPT的策略融合框架，通过数据驱动的融合策略整合多个RL策略，以提升白质纤维束重建的准确性和解剖可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统纤维束成像方法存在准确重建白质纤维束同时最小化虚假连接的挑战，需要改进现有深度学习和深度强化学习方法。

Method: 采用基于GPT的策略融合框架，包含两阶段训练数据选择过程进行有效策略融合，然后进行多critic微调阶段以增强鲁棒性和泛化能力。

Result: 在HCP、ISMRM和TractoInferno数据集上的实验表明，TractRLFusion在准确性和解剖可靠性方面优于单个RL策略以及最先进的经典和DRL方法。

Conclusion: TractRLFusion通过整合多个RL策略的GPT-based融合框架，显著提升了纤维束成像的性能，为脑连接研究和神经外科规划提供了更可靠的工具。

Abstract: Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.

</details>


### [179] [RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning](https://arxiv.org/abs/2601.13964)
*Cheol-Hui Lee,Hwa-Yeon Lee,Dong-Joo Kim*

Main category: cs.LG

TL;DR: RL-BioAug：一种利用强化学习自动选择最优数据增强策略的框架，用于EEG对比学习，仅需10%标签数据指导，在睡眠分期和癫痫检测任务上显著优于随机增强策略。


<details>
  <summary>Details</summary>
Motivation: EEG信号具有非平稳性，传统静态或随机数据增强策略难以保留内在信息，影响对比学习性能。需要一种能自动适应EEG信号特性的增强策略选择方法。

Method: 提出RL-BioAug框架，使用标签高效的强化学习代理自主确定最优增强策略。仅用10%标签数据指导代理策略，编码器以严格自监督方式学习鲁棒表示。

Result: 在Sleep-EDFX和CHB-MIT数据集上，RL-BioAug相比随机选择策略分别提升Macro-F1分数9.69%和8.80%。代理针对不同任务选择不同最优策略：睡眠分期62%概率选择时间掩码，癫痫检测77%概率选择裁剪与调整大小。

Conclusion: RL-BioAug有潜力替代传统基于启发式的增强方法，建立数据增强的自主范式。框架展示了强化学习在生物信号处理中自动选择增强策略的有效性。

Abstract: The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\% probability for sleep stage classification and Crop \& Resize with a 77\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.

</details>


### [180] [A universal linearized subspace refinement framework for neural networks](https://arxiv.org/abs/2601.13989)
*Wenbo Cao,Weiwei Zhang*

Main category: cs.LG

TL;DR: LSR是一种架构无关的框架，利用固定训练网络状态的Jacobian诱导线性残差模型，通过求解子空间中的直接最小二乘问题，显著提升梯度训练模型的精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络通常使用基于梯度的方法训练，但在许多应用中，其最终预测精度远未达到模型表达能力可达到的水平。梯度训练往往无法达到可获得的精度，即使局部线性化产生凸问题。

Method: LSR利用固定训练网络状态的Jacobian诱导线性残差模型，在子空间中求解简化直接最小二乘问题，计算线性化残差模型的子空间最优解。对于具有复合损失结构的算子约束问题，进一步引入迭代LSR，交替进行一次性LSR和监督非线性对齐。

Result: LSR系统性地暴露了梯度训练未充分利用的精度水平，经常实现数量级的误差减少。在监督函数逼近、数据驱动算子学习和物理信息算子微调等任务中表现优异。

Conclusion: LSR通过将非线性神经表示与固定线性化点的降阶线性求解器连接，为监督学习、算子学习和科学计算提供了一个数值基础广泛适用的精炼框架，表明损失诱导的数值病态性而非非凸性或模型表达能力是主要瓶颈。

Abstract: Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.

</details>


### [181] [Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment](https://arxiv.org/abs/2601.14022)
*Rodrigo Pereira David,Luciano Araujo Dourado Filho,Daniel Marques da Silva,João Alfredo Cal-Braz*

Main category: cs.LG

TL;DR: 提出基于机器学习的框架，在相同真实驾驶条件下公平比较内燃机车和电动车的CO2排放


<details>
  <summary>Details</summary>
Motivation: 道路运输脱碳需要一致透明的方法来比较不同车辆技术的CO2排放，现有方法难以在相同条件下公平比较内燃机车和电动车

Method: 使用循环神经网络独立训练两个模型，学习从驾驶变量（速度、加速度、温度）到内部执行变量（扭矩、油门）和瞬时CO2当量排放率的映射，构建反事实场景进行对比

Result: 建立了一个可扩展的框架，能够在统一瞬时排放指标下公平、可重复地评估动力总成技术性能

Conclusion: 该框架为基于数据的车辆碳性能评估提供了可信、可扩展的基础，支持在真实操作条件下进行公平的动力总成技术比较

Abstract: Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.

</details>


### [182] [Universal Approximation Theorem for Input-Connected Multilayer Perceptrons](https://arxiv.org/abs/2601.14026)
*Vugar Ismailov*

Main category: cs.LG

TL;DR: 提出IC-MLP架构，其中隐藏神经元除了接收前一层输出外，还直接接收原始输入的仿射连接。证明了深度IC-MLP在激活函数非线性时能够通用逼近闭区间上的连续函数。


<details>
  <summary>Details</summary>
Motivation: 研究一种改进的神经网络架构，通过为隐藏神经元添加直接输入连接来增强网络表达能力，探索这种架构的理论性质。

Method: 引入输入连接多层感知机(IC-MLP)，每个隐藏神经元接收前一层输出和原始输入的仿射连接。首先在单变量设置下分析，给出任意有限隐藏层的显式描述和迭代公式。然后扩展到向量值输入。

Result: 证明了深度IC-MLP的通用逼近定理：当且仅当激活函数非线性时，IC-MLP能够逼近闭区间上的任何连续函数。该结果进一步扩展到紧致子集上的连续函数。

Conclusion: IC-MLP架构具有强大的逼近能力，其通用逼近性质与标准MLP类似，但通过直接输入连接可能提供更好的表达能力和训练特性。

Abstract: We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\mathbb{R}^n$.

</details>


### [183] [PAC-Private Responses with Adversarial Composition](https://arxiv.org/abs/2601.14033)
*Xiaochen Zhu,Mayuri Sridhar,Srinivas Devadas*

Main category: cs.LG

TL;DR: 该论文提出了一种基于PAC隐私的API部署机器学习模型隐私保护方法，通过在模型输出层面而非权重层面实施隐私保护，实现了高效用和强隐私保证的平衡。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型越来越多地通过API部署，这使得传统的权重隐私保护方法（如DP-SGD）在保持效用方面产生不必要的噪声。模型权重在不同训练数据集间变化很大，但模型对特定输入的反应维度更低且更稳定，这促使直接在模型输出层面实施隐私保护。

Method: 采用PAC隐私框架，通过控制互信息（MI）为任意黑盒函数提供基于实例的隐私保证。引入新算法通过自适应噪声校准实现对抗性组合，证明了在自适应和对抗性查询下互信息保证线性累积。

Result: 在表格、视觉和NLP任务上的实验表明，该方法在极小的每查询隐私预算下实现高效用。在CIFAR-10上达到87.79%准确率，每步MI预算为2^{-32}。能够服务100万次查询，同时可证明地将成员推理攻击成功率限制在51.08%——相当于(0.04, 10^{-5})-DP保证。通过私有响应标记公共数据蒸馏出的模型在CIFAR-10上达到91.86%准确率，MIA成功率上限为50.49%，相当于(0.02,10^{-5})-DP。

Conclusion: 该方法通过在模型输出层面实施PAC隐私保护，解决了API部署场景下的隐私保护挑战，实现了效用和隐私的优异平衡，支持大规模查询服务同时提供强隐私保证，并能蒸馏出可发布的隐私保护模型。

Abstract: Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.
  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.
  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.

</details>


### [184] [LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems](https://arxiv.org/abs/2601.14053)
*Badri N. Patro,Vijay S. Agneeswaran*

Main category: cs.LG

TL;DR: LLMOrbit 是一个全面的大语言模型分类体系（2019-2025），分析了50多个模型，识别了数据稀缺、成本增长和能耗三大危机，并提出了六种突破扩展瓶颈的范式。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能从基础Transformer架构发展到接近人类水平的推理系统，需要系统性地梳理大语言模型的发展脉络，识别当前面临的扩展瓶颈（数据、成本、能耗），并探索突破这些限制的技术路径。

Method: 提出了LLMOrbit圆形分类法，通过八个相互关联的轨道维度分析50多个模型和15个组织。采用系统调查方法，记录架构创新、训练方法和效率模式，识别三大危机和六种突破范式。

Result: 识别了数据稀缺（2026-2028年耗尽9-27T tokens）、成本指数增长（5年内从300万美元到3亿美元以上）和能耗不可持续（增加22倍）三大危机。发现了六种突破扩展瓶颈的范式：测试时计算、量化、分布式边缘计算、模型融合、高效训练和小型专用模型。

Conclusion: 大语言模型发展面临扩展瓶颈，但通过测试时计算、效率革命和民主化三大范式转变，结合后训练增益技术（RLHF等）和从被动生成到工具使用代理的演进，可以突破当前限制，实现更可持续的发展。

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.

</details>


### [185] [Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.14092)
*Babacar Toure,Dimitrios Tsilimantos,Omid Esrafilian,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出基于注意力的多目标强化学习架构，用于无人机在未知信道条件下的数据收集与能耗权衡路径规划，实现单模型适应多种偏好和动态场景。


<details>
  <summary>Details</summary>
Motivation: 无人机在无线网络服务中日益重要，但现有AI方法存在训练数据有限、忽略多目标本质、无法适应动态环境等问题，需要更鲁棒的解决方案。

Method: 采用基于注意力的多目标强化学习架构，无需先验信道知识，通过单模型处理数据收集与能耗的权衡，适应不同偏好和动态参数，无需微调或重新训练。

Result: 仿真实验显示，该方法在性能、模型紧凑性、样本效率和泛化能力方面显著优于现有RL解决方案，特别是在未见过的场景中表现优异。

Conclusion: 提出的注意力多目标强化学习架构有效解决了无人机路径规划中的数据收集与能耗权衡问题，实现了更好的适应性、泛化性和实际部署潜力。

Abstract: Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.

</details>


### [186] [Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping](https://arxiv.org/abs/2601.14099)
*Shi-Shun Chen,Xiao-Yang Li,Enrico Zio*

Main category: cs.LG

TL;DR: 提出基于时滞交叉映射的因果特征选择框架，解决工业过程中变量间时滞和相互依赖问题，提升软测量模型性能


<details>
  <summary>Details</summary>
Motivation: 现有因果特征选择方法忽略工业过程的两个关键特性：1) 变量间的因果关系存在时滞，而现有方法在同一时间维度分析因果关系；2) 工业过程变量相互依赖，与传统因果推断方法的去相关假设矛盾，导致软测量模型精度和稳定性不足

Method: 提出基于时滞交叉映射的因果特征选择框架：1) 使用时滞收敛交叉映射(TDCCM)进行总因果推断；2) 使用时滞偏交叉映射(TDPCM)进行直接因果推断；3) 提出客观特征选择策略，基于验证集模型性能自动确定因果阈值并选择因果特征

Result: 两个真实案例研究表明：TDCCM实现了最高的平均性能，而TDPCM在最差情况下提升了软测量的稳定性和性能

Conclusion: 提出的时滞交叉映射框架能有效处理工业过程中的时滞和变量相互依赖问题，提升软测量模型的准确性和稳定性，代码已开源

Abstract: Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.

</details>


### [187] [Riemannian Liquid Spatio-Temporal Graph Network](https://arxiv.org/abs/2601.14115)
*Liangsi Lu,Jingchao Wang,Zhaorong Dai,Hanqian Liu,Yang Shi*

Main category: cs.LG

TL;DR: RLSTG将连续时间液体动力学与黎曼流形几何归纳偏置相结合，在弯曲流形上建模图演化，克服了传统LTC网络局限于欧几里得空间的限制，能更好地捕捉具有复杂结构图的本质几何特性。


<details>
  <summary>Details</summary>
Motivation: 现有Liquid Time-Constant网络（LTCs）虽然擅长建模不规则采样的动态系统，但局限于欧几里得空间，在处理具有固有非欧几里得结构（如层次结构和循环）的真实世界图时会产生几何失真，降低表示质量。

Method: 提出黎曼液体时空图网络（RLSTG），将连续时间液体动力学与黎曼流形几何归纳偏置统一起来。该方法直接在弯曲流形上构建常微分方程（ODE）来建模图演化，能够忠实捕捉静态和动态时空图的内在几何结构。

Result: 在真实世界基准测试中，RLSTG通过结合先进的时间动力学和黎曼空间表示，在具有复杂结构的图上实现了优越的性能。同时提供了严格的理论保证，将LTC的稳定性定理扩展到黎曼域，并通过状态轨迹分析量化了其表达能力。

Conclusion: RLSTG成功克服了传统LTC网络的欧几里得空间限制，通过将连续时间液体动力学与黎曼几何相结合，为建模具有复杂非欧几里得结构的时空图提供了更强大的框架。

Abstract: Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io

</details>


### [188] [Penalizing Localized Dirichlet Energies in Low Rank Tensor Products](https://arxiv.org/abs/2601.14173)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: TPBS模型在回归任务中表现出色，通过局部Dirichlet能量正则化解决全局正则化失效问题，在过拟合情况下优于神经网络。


<details>
  <summary>Details</summary>
Motivation: 研究低秩张量积B样条模型在回归任务中的应用，探索Dirichlet能量作为平滑度度量，发现全局Dirichlet能量正则化在某些情况下会失效，需要新的正则化策略。

Method: 提出基于训练点周围小超立方体定义的局部Dirichlet能量正则化策略，并利用预训练的TPBS模型引入两种从不完整样本进行推断的估计器。

Result: TPBS模型在大多数数据集的过拟合情况下优于神经网络，在其他情况下保持竞争力。TPBS模型对过拟合更鲁棒且能持续受益于正则化，而神经网络对过拟合更敏感且利用正则化的效果较差。

Conclusion: TPBS模型是回归任务中有效的替代方案，特别是在过拟合情况下表现优异，局部Dirichlet能量正则化策略有效解决了全局正则化失效的问题。

Abstract: We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.

</details>


### [189] [A model of errors in transformers](https://arxiv.org/abs/2601.14175)
*Suvrat Raju,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 论文研究LLM在确定性任务（如算术）上的错误率，提出基于注意力机制误差累积的定量模型，通过两个参数预测准确率，并进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在需要确定性输出的任务（如算术）上为何会出错，挑战现有观点（如"推理崩溃"或无法表达组合函数），旨在建立定量预测模型。

Method: 提出基于注意力机制误差累积的理论模型，将LLM的众多参数重组为两个关键参数：基本噪声率和可能错误预测的token数量。使用Gemini 2.5 Flash、Gemini 2.5 Pro和DeepSeek R1进行广泛的实证测试。

Result: 理论模型与多种任务的观测准确率高度一致，但也发现某些情况下的偏差。模型提供了替代现有解释的框架，并展示了如何通过提示工程降低错误率。

Conclusion: LLM在确定性任务上的错误可以通过注意力机制的小误差累积来解释，提出的两参数模型能有效预测错误率，挑战了"推理崩溃"等观点，并为改进LLM性能提供了新思路。

Abstract: We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.

</details>


### [190] [Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery](https://arxiv.org/abs/2601.14196)
*Albina Galiullina,Wouter van Heeswijk,Tom van Woensel*

Main category: cs.LG

TL;DR: 提出差异化取货点提供策略，通过为每位顾客推荐单个取货点而非无限制选择，联合减少配送卡车和顾客出行的碳排放，在动态随机环境中使用强化学习方法优化推荐策略。


<details>
  <summary>Details</summary>
Motivation: 取货点作为家庭配送的可持续替代方案，通过订单整合可以缩短配送路线并提高首次投递成功率。然而，当顾客驾车取货时，这些环境效益可能被抵消。需要一种策略来同时减少配送卡车路线和顾客出行的碳排放。

Method: 提出差异化取货点提供策略：为每位到达的顾客推荐单个取货点（而非无限制选择所有位置），同时保留家庭配送选项。在动态随机环境中，使用基于强化学习的方法设计策略，考虑顾客与取货点之间的空间关系及其对未来路线整合的影响。

Result: 计算实验表明，差异化取货点提供策略能显著减少总碳排放。相对于纯家庭配送，总排放最多减少9%；相对于替代策略（包括无限制取货点选择和最近取货点分配），平均减少2%。在密集城市环境中，当取货点多且距离短时，差异化提供特别有效。

Conclusion: 差异化取货点提供策略能有效减少总碳排放，特别是在密集城市环境中。当顾客不太倾向于选择取货点配送而非家庭配送时，明确考虑顾客到达和选择的动态特性尤为重要。

Abstract: Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.

</details>


### [191] [InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning](https://arxiv.org/abs/2601.14209)
*Matthew Y. R. Yang,Hao Bai,Ian Wu,Gene Yang,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出Intervention Training (InT)方法，通过让模型自我修正推理轨迹中的错误，实现细粒度信用分配，显著提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在最终答案层面分配信用，会惩罚整个推理轨迹（即使中间步骤正确），或强化所有步骤（即使包含错误步骤）。这种粗粒度的信用分配限制了推理能力的提升。

Method: InT训练范式：模型在自身推理轨迹上提出简短、有针对性的修正，引导轨迹获得更高奖励。利用数学推理数据集中可用的参考解，模型识别推理中的第一个错误，提出单步干预将轨迹导向正确解，然后对错误点之前的轨迹加上干预进行监督微调。

Result: 在IMO-AnswerBench上，经过InT和后续RL微调后，4B参数基础模型的准确率提升近14%，优于gpt-oss-20b等更大的开源模型。

Conclusion: InT通过让模型自我修正推理轨迹，实现了细粒度信用分配，为后续RL训练提供了更好的初始化，显著提升了数学推理能力。

Abstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.

</details>


### [192] [Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment](https://arxiv.org/abs/2601.14228)
*Punit Kumar,Vaibhav Saran,Divyesh Patel,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: 提出一个可解释的脓毒症决策支持框架，包含患者分层、数据增强、离线强化学习和理由生成四个模块，在MIMIC-III和eICU数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是ICU主要死因之一，及时准确的治疗决策对患者预后至关重要。现有决策支持系统往往缺乏可解释性，难以获得临床医生信任。

Method: 1) 基于聚类的患者风险分层模块；2) 使用VAE和扩散模型的数据增强管道；3) 基于AWR的离线强化学习代理，配备轻量注意力编码器和集成模型；4) 多模态LLM驱动的理由生成模块。

Result: 在MIMIC-III和eICU数据集上评估，该方法实现了高治疗准确性，同时为临床医生提供可解释且稳健的策略建议。

Conclusion: 该框架通过结合患者分层、数据增强、强化学习和自然语言解释，为脓毒症治疗提供了准确且可解释的决策支持，有助于提高临床决策质量。

Abstract: Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.

</details>


### [193] [KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning](https://arxiv.org/abs/2601.14232)
*Egor Cherepanov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: KAGE-Env是一个JAX原生的2D平台游戏环境，能够将视觉观察过程分解为独立可控的视觉轴，同时保持底层控制问题不变，用于系统研究像素策略在视觉分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试通常将多种视觉偏移源混杂在一起，难以进行系统分析。像素强化学习代理在纯视觉分布偏移下经常失败，即使潜在动态和奖励保持不变，需要一种能够分解视觉因素的环境来研究这一问题。

Method: 开发了KAGE-Env，一个JAX原生的2D平台游戏环境，将观察过程分解为独立可控的视觉轴。基于此构建了KAGE-Bench基准测试，包含6个已知轴套件和34个训练-评估配置对，用于隔离单个视觉偏移。使用标准的PPO-CNN基线进行实验。

Result: 观察到强烈的轴依赖性失败：背景和光度偏移通常导致任务完全失败，而代理外观偏移相对温和。某些偏移保留了前进运动但破坏了任务完成，表明仅靠回报可能掩盖泛化失败。JAX实现支持单GPU上每秒3300万环境步的快速实验。

Conclusion: KAGE-Env和KAGE-Bench提供了一个干净的抽象框架，用于系统研究像素强化学习代理在视觉分布偏移下的泛化问题，揭示了不同视觉因素对性能的差异化影响，并为快速、可重复的实验提供了高效平台。

Abstract: Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.

</details>


### [194] [Q-learning with Adjoint Matching](https://arxiv.org/abs/2601.14234)
*Qiyang Li,Sergey Levine*

Main category: cs.LG

TL;DR: QAM是一种新的强化学习算法，通过伴随匹配技术解决连续动作RL中扩散/流匹配策略优化难题，避免不稳定反向传播，在稀疏奖励任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 连续动作强化学习中，如何高效优化表达性强的扩散或流匹配策略一直是个长期挑战。现有方法要么丢弃梯度信息，要么牺牲策略表达能力或引入偏差。

Method: QAM利用伴随匹配技术，将critic的动作梯度转换为步进式目标函数，避免通过多步去噪过程进行不稳定反向传播，同时结合时间差分备份进行critic学习。

Result: QAM在离线RL和离线到在线RL的困难稀疏奖励任务上，持续优于现有方法。

Conclusion: QAM通过伴随匹配技术有效解决了连续动作RL中扩散策略优化的数值稳定性问题，为表达性策略优化提供了无偏且高效的解决方案。

Abstract: We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

</details>


### [195] [Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression](https://arxiv.org/abs/2601.14238)
*Shaurya Mathur,Shreyas Bellary Manjunath,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: FireCastRL是一个结合深度学习预测和强化学习灭火策略的AI框架，用于主动式野火管理


<details>
  <summary>Details</summary>
Motivation: 传统野火管理主要是被动的，只在火灾发生后进行应对。野火频率和强度不断增加，造成巨大生态和经济损失，需要更主动的预防和应对方法。

Method: 1. 使用深度时空模型预测野火起火点；2. 对高风险预测，部署预训练的强化学习代理在物理信息3D模拟中执行直升机灭火战术；3. 生成威胁评估报告帮助应急响应资源分配。

Result: 开发了FireCastRL框架，并公开了一个包含950万个环境变量样本的大规模时空数据集用于野火预测。

Conclusion: 深度学习与强化学习相结合可以同时支持野火预测和战术响应，为主动式野火管理提供了新方法。

Abstract: Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.

</details>


### [196] [Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow](https://arxiv.org/abs/2601.14243)
*Haocheng Xi,Charlie Ruan,Peiyuan Liao,Yujun Lin,Han Cai,Yilong Zhao,Shuo Yang,Kurt Keutzer,Song Han,Ligeng Zhu*

Main category: cs.LG

TL;DR: Jet-RL：首个全面的FP8强化学习训练框架，通过统一的FP8精度流解决现有BF16训练+FP8 rollout策略的不稳定性问题，实现高达33%的rollout加速和16%端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有RL训练管道计算效率低下，rollout阶段占训练时间70%以上。虽然FP8量化训练有望缓解这一瓶颈，但常用的BF16训练+FP8 rollout策略在长序列和复杂任务中会出现严重训练不稳定和精度崩溃问题。

Method: 提出Jet-RL框架，采用统一的FP8精度流同时用于训练和rollout，最小化数值差异，消除低效的跨步校准需求，实现稳定优化的FP8 RL训练。

Result: 实验验证Jet-RL有效性：rollout阶段加速33%，训练阶段加速41%，端到端加速16%（相比BF16训练），在所有设置中保持稳定收敛，精度损失可忽略。

Conclusion: Jet-RL是首个全面的FP8 RL训练框架，通过统一精度流解决了现有混合精度策略的稳定性问题，显著加速RL训练同时保持性能，为高效RL训练提供了实用解决方案。

Abstract: Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [197] [A Proof of Concept for a Digital Twin of an Ultrasonic Fermentation System](https://arxiv.org/abs/2601.11723)
*Francesco Saverio Sconocchia Pisoni,Andrea Vitaletti,Davide Appolloni,Federico Ortenzi,Blasco Morozzo della Rocca,Mariano José Guillén,Alessandro Contaldo*

Main category: cs.ET

TL;DR: 开发用于超声波增强啤酒发酵系统的数字孪生概念验证，实现智能监测、预测和控制


<details>
  <summary>Details</summary>
Motivation: 传统发酵过程缺乏智能化监控和优化能力，需要开发能够实时监测、预测并主动调控酵母生长环境的系统，以加速发酵过程并提高效率

Method: 在传统发酵罐中安装压电换能器产生超声波刺激酵母生长，构建数字孪生系统，采用并改进Palacios等人的预测模型，利用温度、超声频率和占空比作为输入参数处理有限训练样本

Result: 模型性能评估表明所提方法具有可行性，能够有效预测酵母培养密度随时间的变化

Conclusion: 超声波增强啤酒发酵系统的数字孪生概念验证成功，为智能化发酵监控和优化提供了可行方案

Abstract: This paper presents the design and implementation of a proof of concept digital twin for an innovative ultrasonic-enhanced beer-fermentation system, developed to enable intelligent monitoring, prediction, and actuation in yeast-growth environments. A traditional fermentation tank is equipped with a piezoelectric transducer able to irradiate the tank with ultrasonic waves, providing an external abiotic stimulus to enhance the growth of yeast and accelerate the fermentation process. At its core, the digital twin incorporates a predictive model that estimates yeast's culture density over time based on the surrounding environmental conditions. To this end, we implement, tailor and extend the model proposed in Palacios et al., allowing us to effectively handle the limited number of available training samples by using temperature, ultrasonic frequency, and duty cycle as inputs. The results obtained along with the assessment of model performance demonstrate the feasibility of the proposed approach.

</details>


### [198] [COVERT: Trojan Detection in COTS Hardware via Statistical Activation of Microarchitectural Events](https://arxiv.org/abs/2601.11939)
*Mahmudul Hasan,Sudipta Paria,Swarup Bhunia,Tamzidul Hoque*

Main category: cs.ET

TL;DR: 提出COVERT框架，无需黄金模型即可检测COTS微处理器中的硬件木马，通过LLM生成测试程序触发罕见微架构事件来激活潜在木马触发器


<details>
  <summary>Details</summary>
Motivation: COTS硬件广泛使用但供应链不可信，现有木马检测方法不适用于黑盒COTS组件，需要无需黄金模型、可扩展的解决方案

Method: 基于LLM自动生成测试程序触发罕见微架构事件，从公开RTL实现中推导事件知识，将激活知识迁移到不同处理器设计

Result: 在RISC-V COTS微处理器上验证有效，能高覆盖率激活组合和时序木马触发器，对最罕见5%事件实现超过80%触发覆盖率

Conclusion: COVERT提供高效、无需黄金模型的COTS微处理器信任验证框架，能检测多种硬件木马，具有实际应用价值

Abstract: Commercial Off-The-Shelf (COTS) hardware, such as microprocessors, are widely adopted in system design due to their ability to reduce development time and cost compared to custom solutions. However, supply chain entities involved in the design and fabrication of COTS components are considered untrusted from the consumer's standpoint due to the potential insertion of hidden malicious logic or hardware Trojans (HTs). Existing solutions to detect Trojans are largely inapplicable for COTS components due to their black-box nature and lack of access to a golden model. A few studies that apply require expensive equipment, lack scalability, and apply to a limited class of Trojans. In this work, we present a novel golden-free trust verification framework, COVERT for COTS microprocessors, which can efficiently test the presence of hardware Trojan implants by identifying microarchitectural rare events and transferring activation knowledge from existing processor designs to trigger highly susceptible internal nodes. COVERT leverages Large Language Models to automatically generate test programs that trigger rare microarchitectural events, which may be exploited to develop Trojan trigger conditions. By deriving these events from publicly available Register Transfer Level implementations, COVERT can verify a wide variety of COTS microprocessors that inherit the same Instruction Set Architecture. We have evaluated the proposed framework on open-source RISC-V COTS microprocessors and demonstrated its effectiveness in activating combinational and sequential Trojan triggers with high coverage, highlighting the efficiency of the trust verification. By pruning rare microarchitectural events from mor1kx Cappuccino OpenRISC processor design, COVERT has been able to achieve more than 80% trigger coverage for the rarest 5% of events in or1k Marocchino and PicoRV32 as COTS processors.

</details>


### [199] [AlphaSyndrome: Tackling the Syndrome Measurement Circuit Scheduling Problem for QEC Codes](https://arxiv.org/abs/2601.12509)
*Yuhao Liu,Shuohao Ping,Junyu Zhou,Ethan Decker,Justin Kalloor,Mathias Weiden,Kean Chen,Yunong Shi,Ali Javadi-Abhari,Costin Iancu,Gushu Li*

Main category: cs.ET

TL;DR: AlphaSyndrome：一个自动化框架，用于优化量子纠错码中的症状测量调度，通过蒙特卡洛树搜索减少逻辑错误率


<details>
  <summary>Details</summary>
Motivation: 量子纠错中重复的症状测量循环占据了主要的时空和硬件成本。虽然稳定子可交换并允许多种执行顺序，但不同调度在真实噪声下会导致不同的错误传播路径，从而造成逻辑错误率的巨大差异。除了表面码外，有效的症状测量调度研究较少。

Method: AlphaSyndrome将调度问题形式化为优化问题，通过蒙特卡洛树搜索探索排序和并行性。该框架利用代码结构和解码器反馈，旨在使错误传播避免接近逻辑算子的模式，并保持在解码器可纠正区域内。

Result: 在多种代码族、尺寸和解码器中，AlphaSyndrome相对于深度最优基线平均减少80.6%的逻辑错误率（最高达96.2%），与Google手工设计的表面码调度相匹配，并优于IBM的双变量自行车码调度。

Conclusion: AlphaSyndrome为一般可交换稳定子码提供了一种有效的自动化症状测量调度框架，显著降低了逻辑错误率，为量子纠错的实际应用提供了重要工具。

Abstract: Quantum error correction (QEC) is essential for scalable quantum computing, yet repeated syndrome-measurement cycles dominate its spacetime and hardware cost. Although stabilizers commute and admit many valid execution orders, different schedules induce distinct error-propagation paths under realistic noise, leading to large variations in logical error rate. Outside of surface codes, effective syndrome-measurement scheduling remains largely unexplored. We present AlphaSyndrome, an automated synthesis framework for scheduling syndrome-measurement circuits in general commuting-stabilizer codes under minimal assumptions: mutually commuting stabilizers and a heuristic decoder. AlphaSyndrome formulates scheduling as an optimization problem that shapes error propagation to (i) avoid patterns close to logical operators and (ii) remain within the decoder's correctable region. The framework uses Monte Carlo Tree Search (MCTS) to explore ordering and parallelism, guided by code structure and decoder feedback. Across diverse code families, sizes, and decoders, AlphaSyndrome reduces logical error rates by 80.6% on average (up to 96.2%) relative to depth-optimal baselines, matches Google's hand-crafted surface-code schedules, and outperforms IBM's schedule for the Bivariate Bicycle code.

</details>


### [200] [Bounded Minds, Generative Machines: Envisioning Conversational AI that Works with Human Heuristics and Reduces Bias Risk](https://arxiv.org/abs/2601.13376)
*Jiqun Liu*

Main category: cs.ET

TL;DR: 本文提出基于有限理性理论设计对话AI的研究路径，主张AI应配合而非对抗人类启发式思维，关注认知脆弱性检测、不确定性判断支持，以及超越事实准确性的决策质量和认知鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI系统大多假设理想化用户，但实际人类推理受限于注意力有限、知识不均和依赖易产生偏见的启发式方法。需要设计能适应人类真实认知局限的对话AI。

Method: 基于有限理性理论框架，提出研究路径：1) 检测认知脆弱性；2) 支持不确定性下的判断；3) 超越事实准确性，评估对话系统的决策质量和认知鲁棒性。

Result: 提出了对话AI设计的新研究方向，强调系统应配合人类启发式思维而非对抗，关注认知脆弱性检测、不确定性判断支持，以及更全面的系统评估标准。

Conclusion: 对话AI设计应基于有限理性理论，配合人类启发式思维，关注认知脆弱性检测、不确定性判断支持，并采用超越事实准确性的评估标准，以提升决策质量和认知鲁棒性。

Abstract: Conversational AI is rapidly becoming a primary interface for information seeking and decision making, yet most systems still assume idealized users. In practice, human reasoning is bounded by limited attention, uneven knowledge, and reliance on heuristics that are adaptive but bias-prone. This article outlines a research pathway grounded in bounded rationality, and argues that conversational AI should be designed to work with human heuristics rather than against them. It identifies key directions for detecting cognitive vulnerability, supporting judgment under uncertainty, and evaluating conversational systems beyond factual accuracy, toward decision quality and cognitive robustness.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [201] [PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices](https://arxiv.org/abs/2601.11553)
*Kaiwei Liu,Liekang Zeng,Lilin Xu,Bufang Yang,Zhenyu Yan*

Main category: cs.DC

TL;DR: PerCache：面向移动RAG应用的层次化缓存系统，通过渐进式查询匹配和QKV缓存重用，结合预测性缓存填充和自适应配置，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的检索增强生成(RAG)应用（如个人助理）面临高延迟问题，现有云环境缓存方案（如KV缓存重用、语义缓存重用）在移动场景下表现不佳，需要专门针对移动RAG特性的缓存解决方案。

Method: PerCache采用层次化架构，在不同计算阶段渐进式匹配相似查询和QKV缓存以最大化中间结果重用；使用预测方法填充可能出现的未来查询；自适应调整配置以适应动态系统负载，在最小资源消耗下最大化缓存效用。

Result: 在现有移动LLM推理引擎上实现，经广泛评估，PerCache相比最佳基线方法能减少34.4%的延迟，在各种应用中保持最优延迟性能，并能适应动态资源变化。

Conclusion: PerCache是针对移动RAG应用的有效缓存解决方案，通过层次化架构、预测性缓存和自适应配置，显著降低了端到端延迟，解决了移动设备资源约束下的RAG性能问题。

Abstract: Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.
  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.

</details>


### [202] [Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments](https://arxiv.org/abs/2601.11584)
*Jody Almaida Putra*

Main category: cs.DC

TL;DR: 研究表明，将日志保留期从90天缩短至14天可降低78%存储成本，同时保留97%以上有操作价值的日志，为小型云团队提供成本效益分析框架。


<details>
  <summary>Details</summary>
Motivation: 早期云部署中，日志保留策略常默认设为90天或更长，未考虑财务和性能影响，导致过度保留成为隐藏的重复成本。需要从成本意识角度分析日志保留窗口选择的影响。

Method: 使用反映真实世界日志量和访问模式变化的合成日志数据集，评估7、14、30和90天保留窗口。分析三个指标：存储成本、操作有用日志比例、每个有用日志的成本。操作有用性定义为模拟调试和事件分析任务期间访问的日志数据。

Result: 将日志保留从90天减少到14天可降低日志存储成本高达78%，同时保留超过97%的操作有用日志。更长的保留窗口提供递减的操作回报，同时不成比例地增加存储成本和查询开销。

Conclusion: 适度的配置更改可在不影响系统可靠性的情况下产生显著成本节约。本研究提供了一个轻量级、易访问的框架，帮助小型工程团队从成本效益角度思考日志保留策略，鼓励在资源受限的云环境中进行更审慎的可观测性配置。

Abstract: Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.
  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.
  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.
  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.

</details>


### [203] [Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure](https://arxiv.org/abs/2601.11577)
*Yuankai Fan,Qizhen Weng,Xuelong Li*

Main category: cs.DC

TL;DR: AI Trinity提出计算-带宽-内存三要素权衡的统一框架，通过动态资源分配优化AI系统性能，解决单一资源瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型发展面临硬件限制，计算、带宽和内存三个维度相互制约，单一优化往往导致其他瓶颈，需要系统性的平衡方案。

Method: 提出AI Trinity框架，将计算、带宽、内存视为同等重要的三大支柱，识别三者之间的三个基本权衡关系：更多计算→更少带宽、更多带宽→更少内存、更多内存→更少计算，实现动态资源分配。

Result: 通过边缘-云通信、大规模分布式训练和模型推理等代表性系统设计展示了AI Trinity的有效性，能够缓解单一资源瓶颈，适应多样化场景。

Conclusion: AI Trinity为可扩展AI基础设施提供了新的范式，既提供了概念基础，又为广泛的应用场景提供了实践指导。

Abstract: Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.

</details>


### [204] [Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud](https://arxiv.org/abs/2601.12266)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 研究在满足平均延迟约束下，使用竞价型和按需型云实例调度延迟敏感作业以最小化平均成本的问题，提出了基于排队论和优化的分析方法及自适应算法。


<details>
  <summary>Details</summary>
Motivation: 云计算中，竞价型实例成本低但不稳定，按需型实例稳定但成本高。延迟敏感作业需要在成本与延迟之间权衡，现有研究缺乏对这一调度问题的理论分析。

Method: 使用排队论、随机过程和优化工具进行理论分析：推导一般策略的成本表达式，证明低目标延迟时队列长度为1最优，刻画最优等待时间分布；高目标延迟时识别背包结构并设计调度策略，提出自适应算法充分利用允许延迟。

Result: 理论分析揭示了不同延迟约束下的最优调度特性，设计的自适应算法在实证中表现出接近最优的性能，能够有效平衡成本与延迟约束。

Conclusion: 该研究首次对云实例调度问题提供了理论分析框架，揭示了最优调度策略的结构特性，提出的自适应算法在实际中表现优异，为云资源调度提供了理论指导和实用工具。

Abstract: We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.

</details>


### [205] [Towards Scalable Federated Container Orchestration: The CODECO Approach](https://arxiv.org/abs/2601.13351)
*Rute C. Sofia,Josh Salomon,Ray Carrol,Luis Garcés-Erice,Peter Urbanetz,Jürgen Gesswein,Rizkallah Touma,Alejandro Espinosa,Luis M. Contreras,Vasileios Theodorou,George Papathanail,Georgios Koukis,Vassilis Tsaoussidis,Alberto del Rio,David Jimenez,Efterpi Paraskevoulakou,Panagiotis Karamolegkos,John Soldatos,Borja Dorado Nogales,Alejandro Tjaarda*

Main category: cs.DC

TL;DR: CODECO是一个面向Kubernetes的联邦编排框架，采用数据-计算-网络协同编排方法，支持异构基础设施、移动性和多云提供商操作，通过语义应用模型、分区联邦和AI辅助决策实现上下文感知的应用部署。


<details>
  <summary>Details</summary>
Motivation: 解决云中心化部署的局限性，支持在异构基础设施、移动环境和多云提供商场景下的应用编排需求，特别是在边缘计算和联邦环境中的挑战。

Method: 扩展Kubernetes，引入语义应用模型、分区联邦机制和AI辅助决策支持；采用混合治理模型，结合集中式策略执行与分布式执行学习；提供软件实验框架进行可重复评估。

Result: CODECO实现了上下文感知的应用和微服务在联邦环境中的自适应管理，支持边缘自主性同时保持全局一致性，为联邦边缘云基础设施环境提供了可重复的实验评估框架。

Conclusion: CODECO通过数据-计算-网络协同编排方法有效解决了云中心化部署的局限性，为异构联邦边缘云环境提供了灵活、自适应的编排解决方案，支持移动性和多云操作。

Abstract: This paper presents CODECO, a federated orchestration framework for Kubernetes that addresses the limitations of cloud-centric deployment. CODECO adopts a data-compute-network co-orchestration approach to support heterogeneous infrastructures, mobility, and multi-provider operation.
  CODECO extends Kubernetes with semantic application models, partition-based federation, and AI-assisted decision support, enabling context-aware placement and adaptive management of applications and their micro-services across federated environments. A hybrid governance model combines centralized policy enforcement with decentralized execution and learning to preserve global coherence while supporting far Edge autonomy. The paper describes the architecture and core components of CODECO, outlines representative orchestration workflows, and introduces a software-based experimentation framework for reproducible evaluation in federated Edge-Cloud infrastructure environments.

</details>


### [206] [PLA-Serve: A Prefill-Length-Aware LLM Serving System](https://arxiv.org/abs/2601.11589)
*Jianshu She,Zonghang Li,Hongchao Du,Shangyu Wu,Wenhao Zheng,Eric Xing,Zhengzhong Liu,Huaxiu Yao,Jason Xue,Qirong Ho*

Main category: cs.DC

TL;DR: PLA-Serve通过基于提示长度的请求分离和智能批处理优化LLM服务，减少TTFT延迟，在异构工作负载下提升性能


<details>
  <summary>Details</summary>
Motivation: 现有系统虽然解耦了预填充和解码阶段，但仍采用统一调度策略，无法适应提示长度变化带来的异构工作负载特性，导致性能瓶颈

Method: 1) 基于提示长度分离长预填充和短预填充请求；2) 双队列设计支持时间分离或空间分离；3) 短预填充采用批处理等待窗口和CUDA Graph聚类；4) 智能批处理机制减少异构计算干扰

Result: 相比SGLang，预填充延迟降低30%以上；多实例部署中SLO违规减少28%；多GPU设置下比SGLang路由器进一步降低12% SLO违规；高并发混合场景下预填充实例吞吐量提升35%

Conclusion: PLA-Serve通过长度感知的请求分离和智能批处理，有效优化了异构LLM服务负载，显著降低了延迟并提升了系统吞吐量

Abstract: PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.

</details>


### [207] [EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend](https://arxiv.org/abs/2601.11590)
*Fan Bai,Pai Peng,Zhengzhi Tang,Zhe Wang,Gong Chen,Xiang Lu,Yinuo Li,Huan Lin,Weizhe Lin,Yaoyuan Wang,Xiaosong Li*

Main category: cs.DC

TL;DR: EPD-Serve：一种面向多模态大模型的阶段级解耦推理服务系统，通过将推理流程解耦为独立的编码、预填充和解码阶段，实现资源高效利用和系统吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理系统采用单体架构，将编码、预填充和解码阶段紧密耦合在同质硬件上，忽视了各阶段的异构计算特性，导致资源利用效率低下和系统吞吐量受限。

Method: 提出EPD-Serve系统：1）将推理管道解耦为独立的编码、预填充和解码阶段；2）利用昇腾互联拓扑实现异步特征预取和分层分组KV缓存传输；3）采用多路由调度、实例级负载均衡和多阶段硬件协同定位等技术。

Result: 在高并发场景下，EPD-Serve相比PD解耦部署将端到端吞吐量提升了57.37-69.48%，同时满足严格的SLO约束：TTFT低于2000ms，TPOT低于50ms。

Conclusion: 阶段级解耦是优化多模态大模型推理系统的有效方法，EPD-Serve通过解耦架构和优化技术显著提升了系统性能和资源利用率。

Abstract: With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.

</details>


### [208] [Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration](https://arxiv.org/abs/2601.11595)
*Meenakshi Amulya Jayanti,X. Y. Han*

Main category: cs.DC

TL;DR: CA-MCP在传统MCP基础上引入共享上下文存储，通过减少LLM调用次数和响应失败率，提升多智能体系统的效率和协作能力。


<details>
  <summary>Details</summary>
Motivation: 传统MCP框架中智能体、模型和服务器都是无状态的，缺乏全局上下文，导致多智能体协作中存在冗余和知识传递困难。在需要LLM驱动的协调任务中，共享上下文存储可以显著提升工作流的效率和一致性。

Method: 设计上下文感知的MCP（CA-MCP），将执行逻辑卸载到专门的MCP服务器，这些服务器能够读写共享上下文内存，实现实时自主协调。上下文管理作为核心机制，通过跟踪中间状态和共享变量来维持任务执行的连续性。

Result: 在TravelPlanner和REALM-Bench基准数据集上的实验表明，CA-MCP相比传统MCP能显著减少复杂任务所需的LLM调用次数，降低任务条件不满足时的响应失败频率，从而提升整体效率和响应性。

Conclusion: 通过CA-MCP引入共享上下文存储，能够在LLM驱动的多智能体系统中实现更高效、更连贯的协作，减少冗余并促进知识传递，具有实际应用潜力。

Abstract: The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.

</details>


### [209] [Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores](https://arxiv.org/abs/2601.11608)
*Ganesh Bikshandi*

Main category: cs.DC

TL;DR: 提出一种硬件感知的CNN计算重构方法，通过重写规则在不修改网络权重的情况下满足硬件对齐要求，提高部署效率。


<details>
  <summary>Details</summary>
Motivation: 现代CNN性能受硬件约束限制，如NVIDIA Tensor Core要求输入通道数为8或512的倍数，传统零填充方法效率低下，需要一种更优的硬件对齐解决方案。

Method: 使用重写规则对CNN计算进行硬件感知重构，在训练后完全满足硬件对齐要求，不修改网络权重，当前实现专注于Tensor Core的单一变换。

Result: 该方法为语义调优奠定了基础，是一种系统化的硬件感知优化策略，可推广到CPU和其他加速器的更多变换。

Conclusion: 这是迈向语义调优的初步探索，为CNN模型在专用AI硬件上的高效部署提供了新的优化方向。

Abstract: Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.

</details>


### [210] [Radio Labeling of Strong Prismatic Network With Star](https://arxiv.org/abs/2601.11624)
*Liming Wang,Feng Li,Linlin Cui*

Main category: cs.DC

TL;DR: 本文研究星形强棱柱网络的无线频谱分配问题，通过图论中的无线电标号建模，提出相关定理、示例和并行算法以提高大规模网络计算效率。


<details>
  <summary>Details</summary>
Motivation: 无线通信快速发展使得高效频谱分配成为提升网络性能的关键因素。无线电标号作为信道分配的组合优化模型是NP难问题，研究特定图类的无线电标号对无线网络最优信道分配设计具有重要意义。

Method: 将频谱分配问题转化为图的无线电标号问题，研究星形强棱柱网络的无线电标号。提出相关定理和示例，并设计并行算法以提高大规模网络场景下的计算效率。

Result: 讨论了星形强棱柱网络的无线电标号，提出了相关定理和示例，并开发了并行算法来提升大规模网络的计算效率。

Conclusion: 强积是构建规则网络的重要工具，研究其无线电标号对无线网络最优信道分配设计是必要的。本文的工作为星形强棱柱网络的频谱分配提供了理论支持和高效计算方法。

Abstract: The rapid development of wireless communication has made efficient spectrum assignment a crucial factor in enhancing network performance. As a combinatorial optimization model for channel assignment, the radio labeling is recognized as an NP-hard problem. Therefore, converting the spectrum assignment problem into the radio labeling of graphs and studying the radio labeling of specific graph classes is of great significance. For $G$, a radio labeling $\varphi: V(G) \to \{0, 1, 2, \ldots\}$ is required to satisfy $|\varphi(u) - \varphi(v)| \geq \text{diam}(G) + 1 -d_G(u, v)$, where ${diam(G)}$ and $d_G(u, v)$ are diameter and distance between $u$ and $v$. For a radio labeling $\varphi$, its $\text{span}$ is defined as the largest integer assigned by $\varphi$ to the vertices of $G$; the radio labeling specifically denotes the labeling with the minimal span among possible radio labeling. The strong product is a crucial tool for constructing regular networks, and studying its radio labeling is necessary for the design of optimal channel assignment in wireless networks. Within this manuscript, we discuss the radio labeling of strong prismatic network with star, present the relevant theorems and examples, and propose a parallel algorithm to improve computational efficiency in large-scale network scenarios.

</details>


### [211] [A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects](https://arxiv.org/abs/2601.11646)
*Chao Wang,Ruijia Li,Yang Zhou,Peng Wu,Yi Lv,Jianwei Liao,Jim Woodcock,Zhiming Liu*

Main category: cs.DC

TL;DR: 论文系统研究了线性化对象与向前模拟之间的关系，证明了在不同活性约束下线性化对象集合构成有界半格或格结构，并提出了线性化的等价特征化方法。


<details>
  <summary>Details</summary>
Motivation: 研究线性化对象与向前模拟之间的理论联系，为并发对象的验证提供新的理论框架和工具。

Method: 通过数学证明建立线性化对象集合在向前模拟关系下的格结构，提出通过对象$\mathcal{U}_{Spec}$将线性化检查转化为向前模拟检查的等价特征化方法。

Result: 证明了不同活性约束下的线性化对象集合构成有界半格或格；证明了强线性化对象之间相互模拟；证明了时间戳队列模拟Herlihy-Wing队列；证明了Herlihy-Wing队列被$\mathcal{U}_{Spec}$模拟。

Conclusion: 线性化与向前模拟之间存在深刻的数学结构联系，提出的等价特征化方法可用于线性化的验证，为并发对象验证提供了新的理论工具。

Abstract: In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.

</details>


### [212] [WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching](https://arxiv.org/abs/2601.11652)
*Xiangchen Li,Jiakun Fan,Qingyuan Wang,Dimitrios Spatharakis,Saeid Ghafouri,Hans Vandierendonck,Deepu John,Bo Ji,Ali R. Butt,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: WISP是一个高效的分布式LLM推理系统，通过智能推测控制器、验证时间估计器和验证批调度器解决分布式推测解码中的瓶颈，显著提升系统容量和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在边缘设备的普及，大量推理请求集中在中心GPU集群，导致数据中心压力巨大而边缘设备利用率不足，造成网络资源不平衡和效率低下。

Method: 提出WISP系统，包含三个核心组件：智能推测控制器优化草稿生成效率，验证时间估计器预测验证时间，验证批调度器优化服务器上的验证请求调度，共同解决"浪费的草稿时间"和"验证干扰"两个瓶颈。

Result: 相比集中式服务和SLED，WISP将系统容量分别提升至2.1倍和4.1倍，系统吞吐量分别提升至1.94倍和3.7倍。

Conclusion: WISP通过有效整合边缘设备到LLM推理过程中，解决了分布式推测解码的关键瓶颈，实现了高效的SLO感知分布式LLM推理服务。

Abstract: As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.

</details>


### [213] [HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network](https://arxiv.org/abs/2601.11676)
*Peirong Zheng,Wenchao Xu,Haozhao Wang,Jinyu Chen,Xuemin Shen*

Main category: cs.DC

TL;DR: HALO是一个用于边缘网络中分布式LLM推理的框架，通过松弛同步和智能神经元组分配来应对不可靠网络条件，实现3.41倍端到端加速。


<details>
  <summary>Details</summary>
Motivation: 边缘部署LLM推理能提高响应速度并保护隐私，但单个边缘节点资源有限。现有分布式推理方法需要严格同步，这在不可靠的边缘网络条件下不可行。

Method: 提出HALO框架：1) 语义感知预测器在激活前评估神经元组重要性；2) 模型推理期间并行执行神经元组加载；3) 负载均衡调度器协调异构设备资源。

Result: 在树莓派集群上的实验表明，HALO在不可靠网络条件下为LLaMA系列LLM实现3.41倍端到端加速，性能接近最优条件，显著优于现有方法。

Conclusion: HALO通过松弛同步和智能资源分配，有效解决了边缘网络中分布式LLM推理的挑战，为资源受限环境中的高效推理提供了可行方案。

Abstract: The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.

</details>


### [214] [CPU-less parallel execution of lambda calculus in digital logic](https://arxiv.org/abs/2601.13040)
*Harry Fitchett,Charles Fox*

Main category: cs.DC

TL;DR: 将纯函数式语言直接编译为数字逻辑电路，绕过传统CPU和冯·诺依曼瓶颈，实现并行计算


<details>
  <summary>Details</summary>
Motivation: 晶体管密度持续增加但时钟速度停滞，需要寻找新的并行架构。传统CPU存在冯·诺依曼瓶颈，而纯函数式语言具有内在并行性，但现有编译器仍依赖传统CPU

Method: 使用λ演算作为源语言，编译为数字逻辑电路。采用基于树的表示方法，节点对应λ语法形式，通过总线进行消息传递，并行执行β归约

Result: 实现了概念验证系统，通过模拟展示了λ表达式的成功执行，测试套件验证了方法的可行性

Conclusion: 该方法为无CPU函数式计算提供了新模型基础，可扩展到更大的函数式语言，展示了直接编译函数式语言到硬件逻辑的潜力

Abstract: While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.

</details>


### [215] [RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation](https://arxiv.org/abs/2601.11822)
*Amna Masood,Pratishtha Gaur,Nuwan Jayasena*

Main category: cs.DC

TL;DR: RAPID-Serve：一种在相同GPU上并发执行预填充和解码的技术，在满足延迟SLO的同时保持高吞吐量和资源利用率，相比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理服务系统采用混合批处理和分离式服务两种技术，但各有局限：混合批处理提高资源利用率但增加延迟；分离式服务优化SLO但导致资源利用不足和KV缓存传输开销。需要一种能同时满足延迟SLO、高吞吐和高效资源利用的方案。

Method: 提出RAPID-Serve技术，在相同GPU上并发执行预填充和解码阶段；采用自适应资源管理进行运行时计算资源分配；可选利用AMD Instinct™ GPU上的CU掩码（细粒度计算单元分区功能）。

Result: RAPID-Serve提供高达4.1倍（平均1.7倍）的无约束吞吐量提升，在SLO约束下提供32倍及以上（平均4.9倍）的吞吐量提升，特别是在资源受限环境中表现优异。

Conclusion: RAPID-Serve是一种有效的策略，相比最先进方法，能在满足延迟SLO的同时保持高吞吐量和高效资源利用率，特别适合资源受限环境。

Abstract: Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.

</details>


### [216] [Big Data Workload Profiling for Energy-Aware Cloud Resource Management](https://arxiv.org/abs/2601.11935)
*Milan Parikh,Aniket Abhishek Soni,Sneja Mitinbhai Shah,Ayush Raj Jha*

Main category: cs.DC

TL;DR: 提出基于工作负载感知的节能调度框架，通过分析CPU、内存、存储IO等资源使用模式，优化虚拟机放置决策，在保证SLA的同时实现15-20%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 随着大数据工作负载规模和复杂度的增长，云数据中心面临降低运营能耗的压力。传统调度器缺乏对工作负载特性的深入理解，导致能源效率低下。

Method: 开发工作负载感知的节能调度框架，通过分析CPU利用率、内存需求和存储IO行为来指导虚拟机放置决策。结合历史执行日志和实时遥测数据，预测候选放置方案的能耗和性能影响，实现自适应整合同时保持SLA合规性。

Result: 在多节点云测试平台上使用Hadoop MapReduce、Spark MLlib和ETL工作负载进行评估，相比基线调度器实现15-20%的持续能耗节省，性能下降可忽略不计。

Conclusion: 工作负载分析是提高基于云的大数据处理环境可持续性的实用且可扩展策略，能够显著降低能耗而不影响服务质量。

Abstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.

</details>


### [217] [Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale](https://arxiv.org/abs/2601.14159)
*Panagiotis-Eleftherios Eleftherakis,George Anagnostopoulos,Anastassis Kapetanakis,Mohammad Umair,Jean-Yves Vet,Konstantinos Iliakis,Jonathan Vincent,Jing Gong,Akshay Patil,Clara García-Sánchez,Gerardo Zampino,Ricardo Vinuesa,Sotirios Xydis*

Main category: cs.DC

TL;DR: 该论文分析了SOD2D CFD框架在AMD和NVIDIA GPU架构上的性能可移植性，发现内存访问优化对加速效果影响显著（0.69×-3.91×差异），多GPU扩展时存在性能波动，强调需要多层次调优。


<details>
  <summary>Details</summary>
Motivation: 随着异构超级计算架构在HPC中日益重要，CFD作为典型HPC负载需要高效利用GPU硬件。性能可移植性（在不同加速器上保持近最优性能）是HPC代码的关键挑战。REFMAP项目针对城市气流预测的可扩展多保真度CFD，需要分析SOD2D框架在AMD和NVIDIA GPU上的性能可移植性。

Method: 首先讨论SOD2D的物理和数值模型，识别计算热点。然后以多层次方式分析性能和可扩展性：定义跨越应用、软件和硬件基础设施参数的完整设计空间。在服务器级NVIDIA和AMD GPU架构及厂商特定编译器栈上进行单GPU性能表征，评估内存访问优化的影响。在LUMI多GPU集群上进一步分析扩展性能，通过性能剖析揭示吞吐量变化。

Result: 单GPU性能表征显示内存访问优化对加速效果有显著影响，加速比偏差达0.69×-3.91×。在LUMI多GPU集群上的扩展性能分析显示类似的吞吐量变化，表明性能预测存在局限性。不同GPU架构和编译器栈表现出不同的性能特征。

Conclusion: SOD2D在AMD和NVIDIA GPU架构上表现出性能可移植性挑战，内存访问优化对性能影响显著且多样化。多GPU扩展时存在性能波动，强调需要基于多层次信息的调优策略，而不能依赖简单的性能预测。这为异构HPC架构上的CFD代码优化提供了重要见解。

Abstract: As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\times$ - 3.91$\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.

</details>


### [218] [DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia](https://arxiv.org/abs/2601.12209)
*Sana Taghipour Anvari,Julian Samaroo,Matin Raayai Ardakani,David Kaeli*

Main category: cs.DC

TL;DR: DaggerFFT：基于Julia的动态任务调度分布式FFT框架，在CPU和GPU集群上分别实现2.6倍和1.35倍加速


<details>
  <summary>Details</summary>
Motivation: 随着科学模拟向百亿亿次计算系统发展，需要能有效利用现代异构高性能计算系统的分布式FFT算法。传统FFT算法在异构平台上存在性能瓶颈，现有分布式FFT方法依赖静态任务分配和同步屏障，限制了可扩展性和资源利用率。

Method: 提出DaggerFFT分布式FFT框架，将高度并行的FFT计算视为动态调度的任务图。每个FFT阶段操作在单独定义的分布式数组上，FFT操作表示为在铅笔或平板分区DArray上运行的DTasks。运行时使用Dagger的动态调度器（支持工作窃取）在设备间分配任务。

Result: DaggerFFT的动态调度器在CPU和GPU后端上均优于最先进的分布式FFT库，在CPU集群上实现最高2.6倍加速，在GPU集群上实现最高1.35倍加速。已成功集成到地球物理流体动力学框架Oceananigans.jl中。

Conclusion: 基于任务的运行时（如DaggerFFT）能够在大规模实际模拟中同时提供卓越性能和模块化，展示了高级任务调度框架在异构HPC系统上的优势。

Abstract: The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.

</details>


### [219] [Power Aware Dynamic Reallocation For Inference](https://arxiv.org/abs/2601.12241)
*Yiwei Jiang,Sangeeta Chowdhary,Nathaniel Morris,Rutwik Jain,Srilatha Manne,Sam Bayliss*

Main category: cs.DC

TL;DR: RAPID：一种面向大语言模型推理的功率感知解耦框架，通过联合管理GPU角色和功率预算，在严格功率限制下提升性能


<details>
  <summary>Details</summary>
Motivation: 随着模型和集群规模扩大，功率而非计算已成为限制整体性能和成本效率的主要因素。现有解耦方案虽能提升硬件利用率，但未充分考虑功率限制下的性能优化。

Method: RAPID采用静态和动态功率重分配结合GPU重分配策略，在固定功率限制下联合管理GPU角色和功率预算，以维持良好的应用性能一致性。

Result: 相比静态分配方案，RAPID在峰值负载下可将SLO达成率提升高达2倍，且不增加系统复杂度或成本。

Conclusion: 功率感知的解耦推理框架RAPID能有效应对功率限制挑战，在固定功率约束下显著提升大语言模型推理的性能和应用一致性。

Abstract: Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.

</details>


### [220] [RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs](https://arxiv.org/abs/2601.12347)
*Pranjal Naman,Parv Agarwal,Hrishikesh Haritas,Yogesh Simmhan*

Main category: cs.DC

TL;DR: RIPPLE++：针对动态图的流式GNN推理框架，通过增量更新机制高效处理图结构和特征变化，相比现有方法显著提升吞吐量和降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图是动态变化的，顶点和边的属性不断更新，这对图神经网络的高效推理提出了挑战。现有的顶点级和层级推理方法不适合动态图，会导致冗余计算、大邻域遍历和高通信成本。采样方法虽然可以近似最终层嵌入，但在关键应用中因非确定性而不被青睐，这些限制阻碍了实时应用所需的低延迟推理。

Method: 提出RIPPLE++框架，引入广义增量编程模型，捕捉GNN聚合函数的语义，并增量地将更新传播到受影响的邻域。该框架支持所有常见的图更新操作（顶点/边的添加/删除、顶点特征更新），并支持单机和分布式部署。

Result: 在单机环境下，在稀疏图Arxiv上达到56K更新/秒，在稠密图Products上达到7.6K更新/秒，延迟为0.06-960ms，吞吐量比现有最优方法提升2.2-24倍。在分布式环境下，提供约25倍更高的吞吐量和20倍更低的通信成本。

Conclusion: RIPPLE++能够高效准确地处理动态图的流式GNN推理，显著提升性能并降低通信开销，适用于需要实时推理的应用场景。

Abstract: Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\approx25\times$ higher throughput and $20\times$ lower communication costs compared to recomputing baselines.

</details>


### [221] [ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment](https://arxiv.org/abs/2601.12434)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: cs.DC

TL;DR: ASAS-BridgeAMM提出了一种桥接耦合的自动做市商，通过"受控降级"机制在检测到攻击信号时优雅降低功能，而非完全崩溃，显著降低了跨链桥的系统性风险。


<details>
  <summary>Details</summary>
Motivation: 跨链桥是DeFi中最大的系统性风险来源，已造成超过28亿美元损失。现有桥接安全模型存在根本缺陷：要么完全运行，要么灾难性崩溃，缺乏中间状态来应对部分故障。

Method: 将跨链消息延迟量化为执行风险，协议动态调整抵押品折扣、滑点边界和提款限制。采用桥接耦合的自动做市商设计，引入"受控降级"操作状态，在检测到对抗信号时优雅降低功能。

Result: 在以太坊和两条辅助链上18个月的历史回放测试中，相比基础的铸币-销毁架构，ASAS-BridgeAMM将最坏情况下的桥接引发破产减少了73%，在压力期间保留了104.5%的交易量。在延迟最终性、预言机操纵和流动性攻击等对抗模拟中，协议保持偿付能力的概率>0.9999，每周期坏账限制在总抵押品的<0.2%。

Conclusion: ASAS-BridgeAMM通过受控降级机制显著提高了跨链桥的安全性，提供了形式化证明的安全边界、活跃性和抗操纵性，为DeFi跨链基础设施提供了更稳健的设计范式。

Abstract: Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.

</details>


### [222] [SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception](https://arxiv.org/abs/2601.12524)
*Zechuan Gong,Hui Zhang,Yuquan Yang,Wenyu Lu*

Main category: cs.DC

TL;DR: 提出完全去中心化的协作感知框架，通过两阶段博弈论方法让车辆自组织成协作群组，在有限通信带宽下实现高效感知共享


<details>
  <summary>Details</summary>
Motivation: 在密集交通中，协作感知能提高自动驾驶安全性，但大规模部署面临通信带宽有限和缺乏路边基础设施的挑战

Method: 两阶段博弈论方法：第一阶段车辆基于感知互补性和运动一致性形成稳定集群并选举协调者；第二阶段协调者引导成员通过非合作势博弈选择性地传输点云片段，最后通过交换紧凑检测消息实现全局场景理解

Result: 在CARLA-OpenCDA-NS3联合仿真平台上验证，相比现有基线方法，显著降低通信开销，同时提供更高的感知精度和更广的有效覆盖范围

Conclusion: 提出的完全去中心化框架能在有限通信带宽下实现高效协作感知，通过车辆自组织和选择性数据传输平衡了通信效率与感知性能

Abstract: Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.

</details>


### [223] [Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications](https://arxiv.org/abs/2601.12713)
*Luke Marzen,Junhyung Shim,Ali Jannesari*

Main category: cs.DC

TL;DR: 提出OMPDataPerf工具，使用动态分析技术检测异构计算中的数据移动瓶颈，通过OpenMP Tools Interface实现低开销（5%）的性能分析


<details>
  <summary>Details</summary>
Motivation: 随着异构计算的普及，CPU与加速器结合提升了性能和能效，但设备间的数据移动成为主要瓶颈，现有性能工具需要大量人工干预来诊断数据传输效率问题

Method: 提出动态分析技术来检测和剖析异构应用中的低效数据传输和分配模式，实现为OMPDataPerf工具，利用OpenMP Tools Interface (OMPT)进行监控

Result: OMPDataPerf能够提供详细的问题数据映射追踪、源代码归因和优化潜力评估，仅产生5%的几何平均运行时开销

Conclusion: OMPDataPerf通过低开销的动态分析有效解决了异构计算中数据移动瓶颈的诊断问题，简化了应用开发中的性能优化过程

Abstract: With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.

</details>


### [224] [Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization](https://arxiv.org/abs/2601.12749)
*Hui Zhang,Yuquan Yang,Zechuan Gong,Xiaohua Xu,Dan Keun Sung*

Main category: cs.DC

TL;DR: 提出LGCP框架，通过分区协作感知减少通信开销，实现通信和计算高效的自动驾驶协同感知


<details>
  <summary>Details</summary>
Motivation: 协同感知能提高自动驾驶准确性，但存在通信开销大和计算延迟高的问题，需要更高效的协作框架

Method: 将道路划分为非重叠区域，每个区域分配专用CAV组进行本地感知，组长融合数据上传至RSU，RSU聚合全局视图广播给所有CAV

Result: LGCP框架平均减少44倍数据传输量，同时保持甚至提高整体协同感知性能

Conclusion: LGCP框架通过本地到全局的协作方式，有效解决了协同感知中的通信和计算效率问题

Abstract: Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.

</details>


### [225] [Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination](https://arxiv.org/abs/2601.12784)
*Haoyang Li,Sheng Lin,Fangcheng Fu,Yuming Zhou,Xiaodong Ji,Yanfeng Zhao,Lefeng Wang,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: StaleFlow：一个解决强化学习后训练中数据陈旧性和偏斜性问题的系统，通过全局一致性协议和重构架构实现高性能且不损害收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有完全解耦架构的RL系统面临两个关键数据问题：异步执行导致轨迹数据陈旧（影响收敛），以及轨迹长度变化导致数据偏斜（影响系统性能）。现有系统无法统一解决这两个问题，必须在收敛和性能之间做出权衡。

Method: 1. 引入全局一致性协议跟踪每个轨迹的全生命周期以控制数据陈旧性；2. 重构RL系统架构，构建轨迹和参数数据服务器实现灵活的rollout协调；3. 开发一套面向吞吐量的陈旧性感知策略来提升系统性能。

Result: StaleFlow相比最先进系统实现了1.42-2.68倍（平均1.17-2.01倍）的吞吐量提升，且不损害收敛性。

Conclusion: StaleFlow成功解决了RL后训练中的数据陈旧性和偏斜性问题，实现了收敛性和系统性能的统一优化，为大规模RL训练提供了高效解决方案。

Abstract: Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.
  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\times$ (1.17-2.01$\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.

</details>


### [226] [From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation](https://arxiv.org/abs/2601.12830)
*Om Mishra,Jayesh Patil,Sathwik Narkedimilli,G Srikantha Sharma,Ananda S,Manjunath K Vanahalli*

Main category: cs.DC

TL;DR: 提出一种新型轨道碎片清除架构，结合机械夹持系统、太阳能推进器和自主导航协议，实现高效、燃料依赖度低的碎片清除方案


<details>
  <summary>Details</summary>
Motivation: 轨道碎片日益增多威胁空间操作可持续性，当前燃料依赖方法存在局限性，需要开发更高效、可持续的清除方案

Method: 整合机械夹持系统用于安全捕获碎片，采用高效太阳能NASA进化氙推进器(NEXT)，结合自主导航协议和雷达扩展卡尔曼滤波导航，使用延迟/中断容忍网络协议进行数据传输

Result: 高保真仿真验证成功实现从800公里到100公里的逆行离轨，位置均方根误差小于10米，延迟/中断容忍网络协议在1秒内实现93%的数据传输效率

Conclusion: 该架构显著推进轨道管理，建立了可再生太阳能推进的基准，减少对传统燃料的依赖，延长多目标清除任务寿命

Abstract: The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.

</details>


### [227] [On Resilient and Efficient Linear Secure Aggregation in Hierarchical Federated Learning](https://arxiv.org/abs/2601.12853)
*Shudi Weng,Xiang Zhang,Yizhou Zhao,Giuseppe Caire,Ming Xiao,Mikael Skoglund*

Main category: cs.DC

TL;DR: 研究不可靠通信下分层安全聚合的基本极限，提出最优协议并证明其最优性，同时改进问题表述以弥合理论与实际联邦学习的差距。


<details>
  <summary>Details</summary>
Motivation: 现有安全聚合协议通常假设可靠通信，但实际联邦学习系统中客户端到中继和中继到服务器的链路都可能间歇性中断，需要研究不可靠通信下的基本极限。

Method: 建立分层网络模型，客户端连接多个中继，链路不可靠；通过信息论方法刻画最小通信和随机性成本；设计最优协议并给出匹配的逆证明；改进问题表述以更好地贴合实际联邦学习场景。

Result: 确定了分层安全聚合在不可靠通信下的基本极限，提出了达到这些极限的最优协议，并通过逆证明确立了协议的最优性；改进了问题表述框架。

Conclusion: 该工作为不可靠通信环境下的分层安全聚合建立了理论基础，提供了最优解决方案，并弥合了信息论协议与实际联邦学习应用之间的差距。

Abstract: In this paper, we study the fundamental limits of hierarchical secure aggregation under unreliable communication. We consider a hierarchical network where each client connects to multiple relays, and both client-to-relay and relay-to-server links are intermittent. Under this setting, we characterize the minimum communication and randomness costs required to achieve robust secure aggregation. We then propose an optimal protocol that attains these minimum costs, and establish its optimality through a matching converse proof. In addition, we introduce an improved problem formulation that bridges the gap between existing information-theoretic secure aggregation protocols and practical real-world federated learning problems.

</details>


### [228] [Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference](https://arxiv.org/abs/2601.12967)
*Anish Biswas,Kanishk Goel,Jayashree Mohan,Alind Khare,Anjaly Parayil,Ramachandran Ramjee,Chetan Bansal*

Main category: cs.DC

TL;DR: SUTRADHARA是一个协同设计的智能体推理系统，通过整合编排与LLM服务，优化工具调用与LLM执行的并行性，降低智能体应用的延迟。


<details>
  <summary>Details</summary>
Motivation: 智能体应用中工具调用占最终答案首次渲染延迟的30-80%，KV缓存命中率下降，顺序编排浪费了潜在的并行性，现有编排器和LLM引擎作为解耦的黑盒设计阻碍了跨层优化。

Method: 提出SUTRADHARA系统，通过薄API整合编排与LLM服务，实现三种优化：1) 工具感知的提示分割，重叠工具执行与后续LLM预填充；2) 流式工具执行，在解码过程中增量调度工具；3) 编排器感知的缓存管理，利用语义提示提高命中率。

Result: 在vLLM上实现，在A100 GPU上，SUTRADHARA将中位数首次渲染延迟降低15%，端到端延迟降低10%。

Conclusion: 协同设计可以系统性地降低智能体系统中的延迟，解决工具调用、缓存管理和并行执行等关键瓶颈。

Abstract: Agentic applications are LLMs that iteratively invoke external tools to accomplish complex tasks. Such tool-based agents are rapidly becoming the dominant paradigm for deploying language models in production. Unlike traditional single-turn inference, agentic workloads chain together multiple LLM calls and tool executions before producing a final response, creating a new performance bottleneck that manifests as increased latency in First Token Rendered (FTR) of the final answer. Through analysis of synthetic requests at production scale, we reveal three critical challenges: tool calls account for 30-80% of FTR latency, KV cache hit rates collapse despite substantial context reuse across iterations, and sequential orchestration wastes potential intra-request parallelism by sequentially executing LLM calls and tools. These bottlenecks stem from a design gap in which orchestrators and LLM engines operate as decoupled black boxes, preventing cross-layer optimizations. We present SUTRADHARA, a co-designed agentic inference system that integrates orchestration with LLM serving through a thin API enabling three optimizations: overlap tool execution with subsequent LLM prefill using tool-aware prompt splitting, streaming tool execution to dispatch tools incrementally during decode rather than waiting for complete output, and orchestrator-aware cache management that uses semantic hints to improve hit rates and reduce thrashing. Implemented on vLLM, SUTRADHARA reduces median FTR latency by 15% and end-to-end latency by 10% across workloads on A100 GPUs, demonstrating that co-design can systematically tame latency in agentic systems.

</details>


### [229] [Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value](https://arxiv.org/abs/2601.12989)
*Yitian Wang,Yebo Feng,Yingjiu Li,Jiahua Xu*

Main category: cs.DC

TL;DR: ePBS虽然分离了区块构建和提议，但显著加剧了利润集中化，Gini系数从0.1749升至0.8358，少数高效构建者通过MEV拍卖获取大部分价值，95.4%区块价值流向提议者，需要新机制设计平衡去中心化、公平性和MEV缓解。


<details>
  <summary>Details</summary>
Motivation: PoS共识机制中交易处理的公平性对维护去中心化和用户信任至关重要，但MEV的出现加剧了经济集中化和内容操纵的担忧。虽然ePBS通过分离区块构建和提议来应对这些漏洞，但其实际效果需要评估。

Method: 开发了一个结合数学分析和基于代理模拟的形式化框架，特别关注MEV动态，评估ePBS的拍卖式区块构建机制。

Result: ePBS显著加剧了利润和内容集中化：利润的Gini系数从标准PoS的0.1749上升到ePBS下的0.8358，表明少数高效构建者通过MEV驱动的拍卖获取大部分价值。95.4%的区块价值奖励给提议者，尽管他们在区块组装中作用有限。

Conclusion: ePBS加剧了构建者采用激进MEV策略的激励，突显了未来需要研究更好的机制设计，以平衡去中心化、公平性和MEV缓解。

Abstract: In blockchain systems operating under the Proof-of-Stake (PoS) consensus mechanism, fairness in transaction processing is essential to preserving decentralization and maintaining user trust. However, with the emergence of Maximal Extractable Value (MEV), concerns about economic centralization and content manipulation have intensified. To address these vulnerabilities, the Ethereum community has introduced Proposer Builder Separation (PBS), which separates block construction from block proposal. Later, enshrined Proposer Builder Separation (ePBS) was also proposed in EIP-7732, which embeds PBS directly into the Ethereum consensus layer.
  Our work identifies key limitations of ePBS by developing a formal framework that combines mathematical analysis and agent-based simulations to evaluate its auction-based block-building mechanism, with particular emphasis on MEV dynamics. Our results reveal that, although ePBS redistributes responsibilities between builders and proposers, it significantly amplifies profit and content centralization: the Gini coefficient for profits rises from 0.1749 under standard PoS without ePBS to 0.8358 under ePBS. This sharp increase indicates that a small number of efficient builders capture most value via MEV-driven auctions. Moreover, 95.4% of the block value is rewarded to proposers in ePBS, revealing a strong economic bias despite their limited role in block assembly. These findings highlight that ePBS exacerbates incentives for builders to adopt aggressive MEV strategies, suggesting the need for future research into mechanism designs that better balance decentralization, fairness, and MEV mitigation.

</details>


### [230] [Exploration on Highly Dynamic Graphs](https://arxiv.org/abs/2601.13047)
*Ashish Saxena,Kaushik Mondal*

Main category: cs.DC

TL;DR: 该论文研究了动态图中移动代理的探索问题，在1-Interval Connectivity和Connectivity Time两种模型下，改进了不可能性结果，并提出了新的探索算法。


<details>
  <summary>Details</summary>
Motivation: 研究动态图中移动代理的探索问题，特别是在两种重要模型（1-Interval Connectivity和Connectivity Time）下，改进现有的不可能性结果并设计有效的探索算法。

Method: 首先加强了1-Interval Connectivity模型的现有不可能性结果；然后证明在Connectivity Time动态图中，即使代理具有完全系统参数知识、全局通信、完全可见性和无限内存，使用(n-1)(n-2)/2个代理也无法完成探索；最后提出使用(n-1)(n-2)/2+1个代理的探索算法，假设全局通信、1跳可见性和O(log n)内存。

Result: 1. 加强了1-Interval Connectivity模型的不可能性结果；2. 证明在Connectivity Time模型中，使用(n-1)(n-2)/2个代理无法完成探索，显著改进了先前已知的n个代理的界限；3. 证明要使用(n-1)(n-2)/2+1个代理解决探索问题，1跳可见性是必要的；4. 提出了使用(n-1)(n-2)/2+1个代理的探索算法。

Conclusion: 该研究在动态图探索问题上取得了重要进展，显著改进了Connectivity Time模型中的不可能性界限，并设计了接近最优的探索算法，为动态网络中的移动代理探索提供了新的理论结果。

Abstract: We study the exploration problem by mobile agents in two prominent models of dynamic graphs: $1$-Interval Connectivity and Connectivity Time. The $1$-Interval Connectivity model was introduced by Kuhn et al.~[STOC 2010], and the Connectivity Time model was proposed by Michail et al.~[JPDC 2014]. Recently, Saxena et al.~[TCS 2025] investigated the exploration problem under both models. In this work, we first strengthen the existing impossibility results for the $1$-Interval Connectivity model. We then show that, in Connectivity Time dynamic graphs, exploration is impossible with $\frac{(n-1)(n-2)}{2}$ mobile agents, even when the agents have full knowledge of all system parameters, global communication, full visibility, and infinite memory. This significantly improves the previously known bound of $n$. Moreover, we prove that to solve exploration with $\frac{(n-1)(n-2)}{2}+1$ agents, $1$-hop visibility is necessary. Finally, we present an exploration algorithm that uses $\frac{(n-1)(n-2)}{2}+1$ agents, assuming global communication, $1$-hop visibility, and $O(\log n)$ memory per agent.

</details>


### [231] [OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC](https://arxiv.org/abs/2601.13146)
*Nicolas Nicolaou,Kishori M. Konwar,Moritz Grundei,Aleksandr Bezobchuk,Muriel Médard,Sriram Vishwanath*

Main category: cs.DC

TL;DR: OPTIMUM-DERAM是一个去中心化、可重构的原子读写共享内存系统，通过RLNC编码、一致性哈希环和区块链预言机实现高性能、可扩展和安全的分布式共享内存。


<details>
  <summary>Details</summary>
Motivation: 传统分布式共享内存系统通过多线程单对象内存实例实现多对象支持，理论上可行但资源需求过大，在实际系统中成本过高。需要一种更高效、可扩展的解决方案。

Method: 1) 使用随机线性网络编码(RLNC)提升性能和存储可扩展性；2) 基于一致性哈希环的对象放置和发现机制支持更多原子对象；3) 利用区块链预言机作为注册服务支持动态节点加入/离开；4) 容忍拜占庭故障确保安全性。

Result: 在全球分布式节点上的实验表明，OPTIMUM-DERAM相比传统分布式共享内存解决方案(如ABD算法)具有显著的性能和可扩展性优势。

Conclusion: OPTIMUM-DERAM提供了一种高度一致、可扩展、安全且去中心化的共享内存解决方案，解决了传统分布式共享内存系统的资源消耗问题，在实际系统中具有更好的可行性。

Abstract: This paper introduces OPTIMUM-DERAM, a highly consistent, scalable, secure, and decentralized shared memory solution. Traditional distributed shared memory implementations offer multi-object support by multi-threading a single object memory instance over the same set of data hosts. While theoretically sound, the amount of resources required made such solutions prohibitively expensive in practical systems. OPTIMUM-DERAM proposes a decentralized, reconfigurable, atomic read/write shared memory (DeRAM) that: (i) achieves improved performance and storage scalability by leveraging Random Linear Network Codes (RLNC); (ii) scales in the number of supported atomic objects by introducing a new object placement and discovery approach based on a consistent hashing ring; (iii) scales in the number of participants by allowing dynamic joins and departures leveraging a blockchain oracle to serve as a registry service; and (iv) is secure against malicious behavior by tolerating Byzantine failures.
  Experimental results over a globally distributed set of nodes, help us realize the performance and scalability gains of OPTIMUM-DERAM over previous distributed shared memory solutions (i.e., the ABD algorithm [3])

</details>


### [232] [Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies](https://arxiv.org/abs/2601.13424)
*Alexander Martinez Mendez,Antonio J. Rubio-Montero,Carlos J. Barrios H.,Hernán Asorey,Rafael Mayo-García,Luis A. Núñez*

Main category: cs.DC

TL;DR: 本文分析LAGO项目在高性能计算资源使用效率方面的问题，通过历史作业数据分析识别主要工作负载类型和效率瓶颈，提出优化建议以提高科学产出。


<details>
  <summary>Details</summary>
Motivation: LAGO项目依赖大量高性能计算资源进行复杂的宇宙粒子物理模拟，资源使用效率对科学产出和可持续性至关重要。需要量化并改进HPC资源利用效率，特别是在LAGO特有的计算环境中。

Method: 分析EGI FedCloud平台的历史作业记账数据，识别主要工作负载类别（蒙特卡洛模拟、数据处理、用户分析/测试），使用关键效率指标（CPU利用率、运行时间利用率、I/O模式）评估性能。

Result: 分析显示显著模式：单个模拟任务具有高CPU效率，但短期测试作业对聚合指标产生扭曲影响。识别出具体低效率问题，为LAGO的HPC使用提供数据驱动的见解。

Conclusion: 研究结果为优化资源请求、改进工作流管理策略提供直接建议，指导未来提升计算吞吐量的努力，最终最大化LAGO的HPC投资科学回报。

Abstract: The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.

</details>


### [233] [RASC: Enhancing Observability & Programmability in Smart Spaces](https://arxiv.org/abs/2601.13496)
*Anna Karanika,Kai-Siang Wang,Han-Ting Liang,Shalni Sundram,Indranil Gupta*

Main category: cs.DC

TL;DR: 论文提出RASC抽象，为物联网设备操作提供请求-确认-开始-完成四阶段模型，提升物联网系统的可观察性和可编程性。


<details>
  <summary>Details</summary>
Motivation: 传统RPC虽然构成系统栈基础，但面对智能家居、仓库、办公楼等用户面向的物联网设备集合时，需要更富表达力的抽象。现有工作主要关注物联网通信可靠性，而本文专注于提升物联网操作的可观察性和可编程性。

Method: 提出RASC（请求-确认-开始-完成）抽象，在物联网设备操作启动后的关键节点提供确认机制。RASC设计为在现有RPC机制之上实现而非替代。将RASC集成到开源物联网框架Home Assistant中。

Result: 基于trace的评估显示RASC满足延迟SLO，特别是对于持续数分钟的长时间操作。家庭自动化调度策略比现有最优方案提升10%-55%性能。

Conclusion: RASC抽象更适合物联网操作的自然时空变化特性，能够准确预测操作完成时间、快速检测操作失败、支持细粒度依赖编程和调度，显著提升物联网系统的可观察性和可编程性。

Abstract: While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all "user-facing"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.

</details>


### [234] [A Kubernetes custom scheduler based on reinforcement learning for compute-intensive pods](https://arxiv.org/abs/2601.13579)
*Hanlin Zhou,Huah Yong Chan,Shun Yao Zhang,Meie Lin,Jingfei Ni*

Main category: cs.DC

TL;DR: 提出两种基于强化学习的Kubernetes调度器SDQN和SDQN-n，在计算密集型工作负载下比默认调度器性能更好，能降低集群节点CPU利用率10-20%。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和轻量级容器技术的发展，Docker和Kubernetes成为主流部署技术，但默认Kubernetes调度器在处理计算密集型工作负载（特别是容器化机器学习训练）时无法实现最优调度。

Method: 提出两种基于深度Q网络（DQN）框架的自定义强化学习调度器：SDQN和SDQN-n。SDQN-n采用将pod整合到更少节点上的策略。

Result: 在计算密集型场景中，SDQN和SDQN-n优于默认Kubernetes调度器以及基于Transformer和LSTM的替代方案，平均降低集群节点CPU利用率10%，SDQN-n可降低超过20%。

Conclusion: pod调度需要根据不同场景采用定制策略以获得更好性能。提出的SDQN和SDQN-n架构中的强化学习组件可通过调整参数轻松调优，适应未来各种场景需求，有助于推进更绿色、更节能的数据中心。

Abstract: With the rise of cloud computing and lightweight containers, Docker has emerged as a leading technology for rapid service deployment, with Kubernetes responsible for pod orchestration. However, for compute-intensive workloads-particularly web services executing containerized machine-learning training-the default Kubernetes scheduler does not always achieve optimal placement. To address this, we propose two custom, reinforcement-learning-based schedulers, SDQN and SDQN-n, both built on the Deep Q-Network (DQN) framework. In compute-intensive scenarios, these models outperform the default Kubernetes scheduler as well as Transformer-and LSTM-based alternatives, reducing average CPU utilization per cluster node by 10%, and by over 20% when using SDQN-n. Moreover, our results show that SDQN-n approach of consolidating pods onto fewer nodes further amplifies resource savings and helps advance greener, more energy-efficient data centers.Therefore, pod scheduling must employ different strategies tailored to each scenario in order to achieve better performance.Since the reinforcement-learning components of the SDQN and SDQN-n architectures proposed in this paper can be easily tuned by adjusting their parameters, they can accommodate the requirements of various future scenarios.

</details>


### [235] [Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2601.13817)
*Haitao Zhao,Xiaoyu Tang,Bo Xu,Jinlong Sun,Linghao Zhang*

Main category: cs.DC

TL;DR: 提出分层分割联邦学习框架，通过联合优化设备关联、模型分割层选择和资源分配，平衡SAGIN中联邦学习的训练效率和模型精度


<details>
  <summary>Details</summary>
Motivation: 6G支持在空天地一体化网络中部署联邦学习，但面临资源受限和数据分布不平衡的挑战，需要平衡训练效率和模型精度

Method: 提出分层分割联邦学习框架，推导损失函数上界，将联合优化问题分解为子问题，设计基于暴力搜索分割点的迭代优化算法

Result: 仿真结果表明，所提算法能有效平衡SAGIN中联邦学习的训练效率和模型精度

Conclusion: HSFL框架和优化算法为解决SAGIN中联邦学习的资源受限和数据不平衡问题提供了有效方案

Abstract: 6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.

</details>


### [236] [torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch](https://arxiv.org/abs/2601.13994)
*Mingyuan Chi*

Main category: cs.DC

TL;DR: TorchSLA是一个开源的PyTorch库，提供GPU加速、可扩展且可微分的稀疏线性代数运算，支持大规模科学计算。


<details>
  <summary>Details</summary>
Motivation: 工业科学计算主要使用稀疏矩阵表示非结构化数据（有限元网格、图、点云），但现有工具在GPU加速、多GPU扩展和可微分性方面存在限制。

Method: 开发TorchSLA库，解决三个核心挑战：1) GPU加速稀疏线性求解、非线性求解和特征值计算；2) 通过域分解和halo交换实现多GPU扩展；3) 基于伴随方法的微分，实现O(1)计算图节点和O(nnz)内存消耗。

Result: 在3个GPU上实现了4亿自由度线性求解，支持多种后端（SciPy、cuDSS、PyTorch-native），并与PyTorch autograd无缝集成。

Conclusion: TorchSLA为大规模科学计算提供了高效、可扩展且可微分的稀疏线性代数解决方案，填补了现有工具的空白。

Abstract: Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\mathcal{O}(1)$ computational graph nodes (for autograd) and $\mathcal{O}(\text{nnz})$ memory -- independent of solver iterations. \torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.

</details>
