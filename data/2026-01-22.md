<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 73]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [End-to-End Transformer Acceleration Through Processing-in-Memory Architectures](https://arxiv.org/abs/2601.14260)
*Xiaoxuan Yang,Peilin Chen,Tergel Molom-Ochir,Yiran Chen*

Main category: cs.AR

TL;DR: 该论文针对Transformer模型部署中的三大挑战（注意力机制计算开销大、KV缓存内存瓶颈、注意力复杂度高），提出了内存内处理解决方案，通过重构计算、动态压缩缓存和重新解释注意力为关联内存操作，显著提升了能效和延迟性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在大规模部署时面临三个主要挑战：1）注意力机制需要大量矩阵乘法和中间结果频繁在内存与计算单元间移动，导致高延迟和高能耗；2）长上下文推理中KV缓存可能超过模型权重大小，造成内存和带宽瓶颈；3）注意力机制相对于序列长度的二次复杂度放大了数据移动和计算开销。

Method: 提出了内存内处理解决方案：1）重构注意力和前馈计算以最小化片外数据传输；2）动态压缩和剪枝KV缓存以管理内存增长；3）将注意力重新解释为关联内存操作以降低复杂度和硬件占用。同时评估了该内存内处理设计与最先进加速器和通用GPU的性能对比。

Result: 评估显示，与最先进的加速器和通用GPU相比，该内存内处理设计在能效和延迟方面都有显著改进，有效解决了计算开销、内存可扩展性和注意力复杂度问题。

Conclusion: 通过内存内处理解决方案，该工作实现了对Transformer模型的高效端到端加速，解决了大规模部署中的关键瓶颈问题，为进一步提升Transformer模型的推理效率提供了有效途径。

Abstract: Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.

</details>


### [2] [Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability](https://arxiv.org/abs/2601.14347)
*George Rafael Gourdoumanis,Fotoini Oikonomou,Maria Pantazi-Kypraiou,Pavlos Stoikos,Olympia Axelou,Athanasios Tziouvaras,Georgios Karakonstantis,Tahani Aladwani,Christos Anagnostopoulos,Yixian Shen,Anuj Pathania,Alberto Garcia-Ortiz,George Floros*

Main category: cs.AR

TL;DR: COIN-3D项目旨在通过开发开源EDA工具，提升2.5D/3D VLSI系统的可靠性分析能力，以应对半导体制造向亚纳米工艺和GAAFETs转型带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造从3nm工艺向亚纳米工艺发展，并从FinFETs转向GAAFETs，制造复杂性和挑战不断增加。3D chiplet方法成为解决这些限制并利用扩展设计空间的关键技术，但需要更好的可靠性评估工具。

Method: 通过Horizon Europe Twinning项目COIN-3D，在欧洲领先机构间建立合作，开发新颖的开源电子设计自动化（EDA）工具，集成先进的物理层和系统层可靠性分析算法。

Result: 该项目旨在提供用于3D系统可靠性评估的开源EDA工具，通过协作提升欧洲在2.5D/3D VLSI系统可靠性方面的研究卓越性。

Conclusion: COIN-3D项目通过开发开源可靠性分析工具，为应对先进半导体制造挑战提供了重要解决方案，支持3D chiplet异构系统的可靠设计。

Abstract: As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.

</details>


### [3] [Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA](https://arxiv.org/abs/2601.15151)
*Jean Bruant,Pierre-Henri Horrein,Olivier Muller,Frédéric Pétrot*

Main category: cs.AR

TL;DR: PAF是一个基于Chisel的开源架构参数化框架，用于自动化FPGA流水线设计，解决网络基础设施中FPGA硬件设计过程缓慢、难以适应快速变化需求的问题。


<details>
  <summary>Details</summary>
Motivation: 在云服务提供商部署可扩展基础设施的背景下，FPGA作为网络基础设施的一部分，虽然能保证低延迟和高吞吐量的数据包处理，但硬件设计过程缓慢，难以成为敏捷基础设施的一部分。同时，在不同FPGA目标上部署和维护网络功能需要对硬件设计进行精细调整。

Method: 提出PAF（Pipeline Automation Framework），这是一个基于流水线导向设计方法的开源架构参数化框架。PAF基于Chisel（Scala嵌入式硬件构造语言）实现，利用其接口进行电路细化。应用于工业网络数据包分类系统，通过参数化实现同一流水线设计在不同FPGA上的重用和优化。

Result: PAF展示了高效的参数化能力，能够在多个FPGA上重用和优化相同的流水线设计。同时，PAF将流水线描述集中在架构意图上，减少了表达复杂功能所需的代码行数。自动化并未导致架构控制力的损失，实现了与等效详尽描述实现相当的性能和资源使用效率。

Conclusion: PAF框架成功解决了FPGA硬件设计过程缓慢的问题，通过架构参数化和自动化实现了敏捷的网络基础设施部署，同时保持了架构控制力，为云服务提供商提供了可扩展的FPGA解决方案。

Abstract: In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Call2Instruct: Automated Pipeline for Generating Q&A Datasets from Call Center Recordings for LLM Fine-Tuning](https://arxiv.org/abs/2601.14263)
*Alex Echeverria,Sávio Salvarino Teles de Oliveira,Fernando Marques Federson*

Main category: cs.LG

TL;DR: 本文提出了一個端到端自動化流程，從客服中心音訊錄音生成問答式教學資料集，用於大型語言模型的領域適應微調。


<details>
  <summary>Details</summary>
Motivation: 大型語言模型在特定領域的適應需要高品質的微調資料集，特別是問答格式的教學資料。然而從客服中心音訊錄音等非結構化來源生成這些資料集面臨挑戰，因為數據雜亂且組織性差。

Method: 開發了一個端到端自動化流程，包含音訊處理（說話人分離、降噪、自動轉錄）、文本處理（清理、標準化、匿名化）、使用向量嵌入進行語義提取客戶需求與客服回應，以及通過語義搜索匹配形成最終的問答對。

Result: 成功實現完整流程，生成專門用於教學微調的資料集。通過成功微調基於Llama 2 7B的LLM模型，證明了生成資料集的實用價值和可行性。

Conclusion: 提出的方法可行，能將客服中心的非結構化對話數據轉換為訓練大型語言模型的寶貴資源，有潛力為客服領域的問答任務創建更有效的AI系統。開發的代碼已公開以促進可重現性和未來研究。

Abstract: The adaptation of Large-Scale Language Models (LLMs) to specific domains depends on high-quality fine-tuning datasets, particularly in instructional format (e.g., Question-Answer - Q&A). However, generating these datasets, particularly from unstructured sources such as call center audio recordings, poses a significant challenge due to the noisy and disorganized nature of the data. This paper presents a solution to this challenge by offering an end-to-end automated pipeline for generating Q&A instructional datasets from such recordings. The methodology developed comprises sequential steps of audio processing (including diarization, noise removal and automatic transcription), textual processing (cleaning, normalization, and anonymization), semantic extraction of customer demands and attendant responses using vector embeddings, and matching via semantic search to form the final Q&A pairs. As a result, the complete pipeline was successfully implemented, generating a dataset specifically formatted for Instruct Fine Tuning. The practical value and feasibility of the generated dataset were substantiated and functionally demonstrated through the successful fine-tuning of an LLM model (based on Llama 2 7B). The conclusion of the paper states that the proposed approach is viable for converting unstructured conversational data from call centers into valuable resources for training LLMs. This development has the potential to open up avenues for creating more effective AI systems for Q&A tasks in the customer service domain. The developed codes have been made publicly available to promote reproducibility and future research.

</details>


### [5] [GCG Attack On A Diffusion LLM](https://arxiv.org/abs/2601.14266)
*Ruben Neyroud,Sam Corley*

Main category: cs.LG

TL;DR: 探索GCG风格对抗性提示攻击在扩散语言模型LLaDA上的应用，评估不同攻击变体在有害提示上的效果，为扩散语言模型的鲁棒性研究提供初步见解。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数LLM是自回归的，但基于扩散的LLM最近成为生成任务的替代方法。GCG攻击对自回归模型有效，但其在扩散语言模型上的适用性尚未充分探索。

Method: 对开源扩散LLM LLaDA进行GCG风格对抗性提示攻击的探索性研究，评估多种攻击变体（包括前缀扰动和后缀对抗生成），使用AdvBench数据集中的有害提示进行评估。

Result: 研究提供了扩散语言模型鲁棒性和攻击面的初步见解，揭示了GCG攻击在扩散模型上的适用性。

Conclusion: 这项工作为扩散语言模型的对抗性分析提供了基础，并推动了在该场景下开发替代优化和评估策略的需求。

Abstract: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

</details>


### [6] [Divide and Refine: Enhancing Multimodal Representation and Explainability for Emotion Recognition in Conversation](https://arxiv.org/abs/2601.14274)
*Anh-Tuan Mai,Cam-Van Thi Nguyen,Duc-Trong Le*

Main category: cs.LG

TL;DR: 提出DnR框架，通过分解模态为独有、冗余、协同三部分并分别优化，提升多模态对话情感识别性能


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别需要有效整合模态特有信号、跨模态共享信息和模态组合产生的交互作用，但现有方法在数据准备阶段容易模糊这些组件的边界

Method: 提出两阶段DnR框架：Divide阶段将每个模态明确分解为独有性、成对冗余性和协同性；Refine阶段通过定制目标增强这些组件的信息量同时保持其不同角色

Result: 在IEMOCAP和MELD数据集上的广泛实验表明，该框架在多种MERC骨干网络上带来一致改进

Conclusion: 明确分解、优化和重组多模态表示是推进情感识别的有效策略，提出的表示具有即插即用兼容性

Abstract: Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \emph{unique}, \emph{redundant}, and \emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \emph{\textbf{D}ivide and \textbf{R}efine} (\textbf{DnR}). In the \textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026

</details>


### [7] [Report for NSF Workshop on AI for Electronic Design Automation](https://arxiv.org/abs/2601.14541)
*Deming Chen,Vijay Ganesh,Weikai Li,Yingyan,Lin,Yong Liu,Subhasish Mitra,David Z. Pan,Ruchir Puri,Jason Cong,Yizhou Sun*

Main category: cs.LG

TL;DR: NSF AI for EDA研讨会报告总结了AI在电子设计自动化中的应用前景，包括物理设计、高层次综合、优化工具和测试验证四个主题，并提出促进AI/EDA合作、投资基础设施和人才培养等建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，特别是大语言模型、图神经网络等技术的成熟，AI有望显著提升电子设计自动化（EDA）的效率和质量，缩短硬件设计周期。然而，AI与EDA领域的交叉合作尚不充分，需要系统性的研讨和规划。

Method: 通过举办NSF研讨会，汇集机器学习与EDA领域的专家，围绕四个核心主题进行深入讨论：1) AI在物理设计与制造中的应用；2) AI在高层次与逻辑综合中的应用；3) AI优化工具开发；4) AI在测试与验证中的应用。

Result: 研讨会明确了AI在EDA各环节的具体应用场景，识别了技术挑战和机遇，并形成了系统性的建议：促进跨领域合作、投资基础AI研究、建设数据基础设施、发展计算资源、加强人才培养，以推动硬件设计的民主化和下一代硬件系统发展。

Conclusion: AI技术为EDA领域带来了革命性机遇，通过系统性的跨领域合作、基础设施投资和人才培养，可以显著提升硬件设计效率，缩短设计周期，推动硬件创新和民主化设计。

Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.

</details>


### [8] [Quality or Quantity? Error-Informed Selective Online Learning with Gaussian Processes in Multi-Agent Systems: Extended Version](https://arxiv.org/abs/2601.14275)
*Zewen Yang,Xiaobing Dai,Jiajun Cheng,Yulong Huang,Peng Shi*

Main category: cs.LG

TL;DR: 本文提出首个选择性在线学习框架用于分布式高斯过程回归，通过误差信息选择高质量模型，优先质量而非数量，实现高效合作学习。


<details>
  <summary>Details</summary>
Motivation: 在分布式多智能体系统中，盲目包含所有模型进行联合预测是不合理的，需要优先考虑模型质量而非数量。现有方法缺乏对合作者质量的评估和选择机制。

Method: 提出分布式误差信息高斯过程（EIGP）框架，包含选择函数评估邻居模型质量，选择误差较小的高质量模型。还提出贪婪算法（gEIGP）加速预测和自适应算法（aEIGP）提高精度，结合误差信息量化项迭代和数据删除策略实现实时学习。

Result: 数值模拟验证了方法的有效性，展示了其相对于现有分布式GP方法的优越性，在不同基准测试中表现更佳。

Conclusion: 选择性合作学习框架通过优先质量而非数量，实现了更高效的分布式高斯过程回归，为多智能体系统提供了有效的在线学习解决方案。

Abstract: Effective cooperation is pivotal in distributed learning for multi-agent systems, where the interplay between the quantity and quality of the machine learning models is crucial. This paper reveals the irrationality of indiscriminate inclusion of all models on agents for joint prediction, highlighting the imperative to prioritize quality over quantity in cooperative learning. Specifically, we present the first selective online learning framework for distributed Gaussian process (GP) regression, namely distributed error-informed GP (EIGP), that enables each agent to assess its neighboring collaborators, using the proposed selection function to choose the higher quality GP models with less prediction errors. Moreover, algorithmic enhancements are embedded within the EIGP, including a greedy algorithm (gEIGP) for accelerating prediction and an adaptive algorithm (aEIGP) for improving prediction accuracy. In addition, approaches for fast prediction and model update are introduced in conjunction with the error-informed quantification term iteration and a data deletion strategy to achieve real-time learning operations. Numerical simulations are performed to demonstrate the effectiveness of the developed methodology, showcasing its superiority over the state-of-the-art distributed GP methods with different benchmarks.

</details>


### [9] [Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct](https://arxiv.org/abs/2601.14277)
*Uygar Kurt*

Main category: cs.LG

TL;DR: 对llama.cpp量化格式的统一实证研究，评估Llama-3.1-8B-Instruct在不同比特数K-quant和传统格式下的性能表现，为实际应用提供量化方案选择指南


<details>
  <summary>Details</summary>
Motivation: 量化技术能降低大语言模型的部署门槛，但现有量化格式评估不一致，难以选择合适的量化方案，特别是对于在本地硬件上运行模型的用户

Method: 对Llama-3.1-8B-Instruct模型进行统一的实证研究，覆盖3-8位K-quant和传统量化格式，评估下游任务性能（推理、知识、指令遵循、真实性）、困惑度、CPU吞吐量、模型大小、压缩率和量化时间

Result: 提供了不同量化格式在性能、效率和资源消耗方面的详细对比数据，帮助用户根据具体使用场景和资源预算做出明智选择

Conclusion: 本研究为选择llama.cpp量化方案提供了实用的指导，帮助用户根据预期用途和资源预算做出信息充分、情境感知的决策

Abstract: Quantization is a practical technique for making large language models easier to deploy by reducing the precision used to store and operate on model weights. This can lower memory use and improve runtime feasibility on constrained hardware, which is especially relevant for users running models locally. Quantization in llama.cpp enables large language models to run on commodity hardware, but available formats are often evaluated inconsistently, making it hard to choose among schemes. We present a unified empirical study of the llama.cpp quantization on a single modern model, Llama-3.1-8B-Instruct (FP16, GGUF), covering 3-8 bit K-quant and legacy formats. We evaluate downstream task performance across standard reasoning, knowledge, instruction-following, and truthfulness benchmarks, and also measure perplexity and CPU throughput (prefill/decoding) alongside model size, compression, and quantization time. Ultimately, this work is a practical guide for choosing a llama.cpp quantization scheme, helping readers make informed, context-aware decisions for their intended use and resource budget.

</details>


### [10] [On the Limits of Learned Importance Scoring for KV Cache Compression](https://arxiv.org/abs/2601.14279)
*Brady Steele*

Main category: cs.LG

TL;DR: 论文研究了通过推测重要性预测（SIP）进行KV缓存压缩，发现1.7M参数的SIP模型在多个任务和设置下未能超越简单基线方法，包括随机选择和位置启发式方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索通过学习方法来压缩KV缓存，以提升推理效率。KV缓存是Transformer推理中的主要内存瓶颈，传统方法如窗口缓存或动态剪枝需要手动设计启发式规则，作者希望开发数据驱动的方法来预测token重要性。

Method: 提出了Speculative Importance Prediction (SIP)方法，这是一个1.7M参数的非查询感知评分器，仅从KV表示中预测token重要性。SIP采用了复杂的架构设计，包括多视野前瞻和交叉注意力机制。

Result: 实验结果显示SIP未能超越简单基线方法：1）位置启发式方法（保留前4个token+最后N个token）表现相当或更好；2）预填充注意力提供了与复杂学习评分器相当的信号；3）KV表示中除了位置和预填充注意力之外的边际信息对重要性预测有限。

Conclusion: 研究发现学习KV缓存压缩具有挑战性，简单的位置启发式方法已足够有效。作者假设未来查询与生成轨迹之间的循环依赖关系导致了学习方法的困难，KV表示中用于重要性预测的信息可能主要限于位置和预填充注意力信号。

Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.

</details>


### [11] [Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design](https://arxiv.org/abs/2601.14283)
*Kangyu Zheng,Kai Zhang,Jiale Tan,Xuehan Chen,Yingzhou Lu,Zaixi Zhang,Lichao Sun,Marinka Zitnik,Tianfan Fu,Zhiding Liang*

Main category: cs.LG

TL;DR: 该研究建立了首个跨算法类别的结构药物设计基准，评估了15种不同算法模型在药物性质、对接亲和力和构象方面的表现，揭示了各类算法的独特优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 当前结构药物设计领域主要有搜索算法、深度生成模型和强化学习三类算法，但现有研究通常只比较同一算法类别内的模型，缺乏跨算法类别的系统比较。本研究旨在填补这一空白。

Method: 建立了一个基准测试框架，评估15个不同算法基础的模型，通过评估生成分子的药物性质、与指定靶蛋白的对接亲和力和构象来比较性能。特别强调了1D/2D配体中心方法可以通过将对接函数视为黑盒预言机应用于SBDD。

Result: 评估揭示了不同模型类别的独特模式：3D结构模型在结合亲和力方面表现出色，但在化学有效性和构象质量方面存在不一致性；1D模型在标准分子指标上表现可靠，但很少达到最佳结合亲和力；2D模型提供平衡性能，保持高化学有效性同时获得中等结合分数。

Conclusion: 通过多蛋白靶点的详细分析，识别了每个模型类别的关键改进领域，为研究人员提供了结合不同方法优势并解决其局限性的见解。所有基准测试代码已开源。

Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark

</details>


### [12] [A Comparison of Polynomial-Based Tree Clustering Methods](https://arxiv.org/abs/2601.14285)
*Pengyu Liu,Mariel Vázquez,Nataša Jonoska*

Main category: cs.LG

TL;DR: 本文比较了基于树多项式距离的聚类方法性能，发现基于条目级归一化距离的方法具有最高的聚类准确率。


<details>
  <summary>Details</summary>
Motivation: 生命科学中树结构数据（如RNA二级结构、系统发育树）日益增多，需要新的树结构数据分析方法。树多项式提供了一种高效、可解释的树结构编码方式，但需要评估不同距离度量在聚类中的性能。

Method: 1. 比较基于树多项式不同距离度量的聚类方法性能；2. 实现两种基本的自编码器模型用于树聚类；3. 使用树区分多项式作为基础表示。

Result: 基于条目级归一化距离的方法在所有比较方法中表现出最高的聚类准确率。

Conclusion: 对于树结构数据的聚类分析，基于树多项式的条目级归一化距离方法是最有效的选择，为生命科学中的树结构数据分析提供了实用工具。

Abstract: Tree structures appear in many fields of the life sciences, including phylogenetics, developmental biology and nucleic acid structures. Trees can be used to represent RNA secondary structures, which directly relate to the function of non-coding RNAs. Recent developments in sequencing technology and artificial intelligence have yielded numerous biological data that can be represented with tree structures. This requires novel methods for tree structure data analytics. Tree polynomials provide a computationally efficient, interpretable and comprehensive way to encode tree structures as matrices, which are compatible with most data analytics tools. Machine learning methods based on the Canberra distance between tree polynomials have been introduced to analyze phylogenies and nucleic acid structures. In this paper, we compare the performance of different distances in tree clustering methods based on a tree distinguishing polynomial. We also implement two basic autoencoder models for clustering trees using the polynomial. We find that the distance based methods with entry-level normalized distances have the highest clustering accuracy among the compared methods.

</details>


### [13] [Beyond Denial-of-Service: The Puppeteer's Attack for Fine-Grained Control in Ranking-Based Federated Learning](https://arxiv.org/abs/2601.14687)
*Zhihao Chen,Zirui Gong,Jianting Ning,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.LG

TL;DR: FRL虽能抵抗传统模型投毒攻击，但仍面临新型细粒度控制攻击ECA的威胁，该攻击能精确控制目标模型准确率且难以检测。


<details>
  <summary>Details</summary>
Motivation: 联邦排序学习(FRL)因其离散的排序更新机制被认为对模型投毒攻击具有鲁棒性，但作者发现FRL仍存在安全漏洞，需要研究新型攻击方法以揭示其脆弱性。

Method: 提出边缘控制攻击(ECA)：1)识别并操纵升序和降序边缘，使全局模型与目标模型对齐；2)扩大选择边界间隙，稳定全局模型在目标准确率水平。

Result: 在7个基准数据集和9种拜占庭鲁棒聚合规则上的实验表明，ECA能实现细粒度准确率控制，平均误差仅0.224%，比基线方法提升高达17倍。

Conclusion: FRL虽然减少了攻击面，但仍易受新型细粒度控制攻击，需要开发更强的防御机制来应对高级投毒攻击。

Abstract: Federated Rank Learning (FRL) is a promising Federated Learning (FL) paradigm designed to be resilient against model poisoning attacks due to its discrete, ranking-based update mechanism. Unlike traditional FL methods that rely on model updates, FRL leverages discrete rankings as a communication parameter between clients and the server. This approach significantly reduces communication costs and limits an adversary's ability to scale or optimize malicious updates in the continuous space, thereby enhancing its robustness. This makes FRL particularly appealing for applications where system security and data privacy are crucial, such as web-based auction and bidding platforms. While FRL substantially reduces the attack surface, we demonstrate that it remains vulnerable to a new class of local model poisoning attack, i.e., fine-grained control attacks. We introduce the Edge Control Attack (ECA), the first fine-grained control attack tailored to ranking-based FL frameworks. Unlike conventional denial-of-service (DoS) attacks that cause conspicuous disruptions, ECA enables an adversary to precisely degrade a competitor's accuracy to any target level while maintaining a normal-looking convergence trajectory, thereby avoiding detection. ECA operates in two stages: (i) identifying and manipulating Ascending and Descending Edges to align the global model with the target model, and (ii) widening the selection boundary gap to stabilize the global model at the target accuracy. Extensive experiments across seven benchmark datasets and nine Byzantine-robust aggregation rules (AGRs) show that ECA achieves fine-grained accuracy control with an average error of only 0.224%, outperforming the baseline by up to 17x. Our findings highlight the need for stronger defenses against advanced poisoning attacks. Our code is available at: https://github.com/Chenzh0205/ECA

</details>


### [14] [Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents](https://arxiv.org/abs/2601.14287)
*Xiucheng Xu,Bingbing Xu,Xueyun Tian,Zihe Huang,Rongxin Chen,Yunfan Li,Huawei Shen*

Main category: cs.LG

TL;DR: CoM框架通过轻量级内存构建和链式记忆机制，显著提升LLM代理在长序列任务中的性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有外部内存系统存在两个根本问题：复杂的内存构建成本高但性能提升有限；简单的上下文拼接无法弥补检索召回与推理准确性之间的差距。

Method: 提出CoM框架，采用轻量级构建结合复杂利用的范式转变。引入Chain-of-Memory机制，通过动态演化将检索到的片段组织成连贯的推理路径，并使用自适应截断来修剪无关噪声。

Result: 在LongMemEval和LoCoMo基准测试中，CoM相比强基线准确率提升7.5%-10.4%，同时计算开销大幅降低至复杂内存架构的约2.7%令牌消耗和6.0%延迟。

Conclusion: CoM通过轻量级构建和复杂利用的平衡，为LLM外部内存系统提供了更高效实用的解决方案，在保持高性能的同时显著降低计算成本。

Abstract: External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.

</details>


### [15] [RadixMLP -- Intra-batch Deduplication for Causal Transformers](https://arxiv.org/abs/2601.15013)
*Michael Feil,Julius Lipp*

Main category: cs.LG

TL;DR: RadixMLP通过前缀树压缩共享前缀，消除批量推理中MLP等组件的冗余计算，提升因果Transformer模型的推理速度


<details>
  <summary>Details</summary>
Motivation: 因果Transformer模型批量推理时经常处理具有共同前缀的序列（如系统提示、少样本示例），标准推理引擎独立处理每个序列，导致相同MLP激活被重复计算

Method: RadixMLP利用MLP、LayerNorm、线性投影和嵌入的位置特性，将批次映射到前缀树中，将共享段压缩为紧凑表示进行位置计算，仅在注意力边界处散射结果

Result: 在MS~MARCO v1.1的Qwen3模型（0.6B到8B参数）上，RadixMLP在重排序任务中实现1.44-1.59倍加速，在具有更长共享前缀的合成基准测试中达到5倍加速

Conclusion: RadixMLP是一种无状态、单前向传播的技术，能有效消除因果Transformer批量推理中的冗余计算，显著提升推理效率

Abstract: Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\times$ speedups in realistic reranking workloads, with up to $5\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.

</details>


### [16] [Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity](https://arxiv.org/abs/2601.14300)
*Jun Liu,Leo Yu Zhang,Fengpeng Li,Isao Echizen,Jiantao Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种新的硬标签黑盒攻击框架，通过零查询频域初始化和模式驱动优化策略，在仅能观察到top-1预测标签的受限设置下，实现了比现有方法更高的攻击成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 硬标签黑盒设置（仅能观察top-1预测标签）是理解模型行为的重要反馈模型，但面临从离散响应中恢复梯度信息的核心挑战。现有硬标签攻击方法多为启发式搜索，缺乏理论指导。

Method: 提出统一理论视角，将现有硬标签攻击解释为隐式近似真实损失梯度的符号。基于此提出新攻击框架：1）零查询频域初始化，在温和假设下获得更高期望余弦相似度；2）模式驱动优化策略，显著降低查询复杂度。

Result: 在CIFAR-10、ImageNet、ObjectNet等数据集上，涵盖标准模型、对抗训练模型、商业API和CLIP模型，新方法在攻击成功率和查询效率上均超越SOTA硬标签攻击，尤其在低查询区域表现优异。还能有效规避Blacklight防御（0%检测率），并泛化到损坏数据、生物医学数据集和密集预测任务。

Conclusion: 该工作为硬标签黑盒攻击提供了统一的理论视角，提出的新框架在理论和实验上均表现出色，不仅提升了攻击性能，还展示了良好的泛化能力和防御规避能力，为理解模型在受限反馈下的行为提供了新工具。

Abstract: Hard-label black-box settings, where only top-1 predicted labels are observable, pose a fundamentally constrained yet practically important feedback model for understanding model behavior. A central challenge in this regime is whether meaningful gradient information can be recovered from such discrete responses. In this work, we develop a unified theoretical perspective showing that a wide range of existing sign-flipping hard-label attacks can be interpreted as implicitly approximating the sign of the true loss gradient. This observation reframes hard-label attacks from heuristic search procedures into instances of gradient sign recovery under extremely limited feedback. Motivated by this first-principles understanding, we propose a new attack framework that combines a zero-query frequency-domain initialization with a Pattern-Driven Optimization (PDO) strategy. We establish theoretical guarantees demonstrating that, under mild assumptions, our initialization achieves higher expected cosine similarity to the true gradient sign compared to random baselines, while the proposed PDO procedure attains substantially lower query complexity than existing structured search approaches. We empirically validate our framework through extensive experiments on CIFAR-10, ImageNet, and ObjectNet, covering standard and adversarially trained models, commercial APIs, and CLIP-based models. The results show that our method consistently surpasses SOTA hard-label attacks in both attack success rate and query efficiency, particularly in low-query regimes. Beyond image classification, our approach generalizes effectively to corrupted data, biomedical datasets, and dense prediction tasks. Notably, it also successfully circumvents Blacklight, a SOTA stateful defense, resulting in a $0\%$ detection rate. Our code will be released publicly soon at https://github.com/csjunjun/DPAttack.git.

</details>


### [17] [DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search](https://arxiv.org/abs/2601.15127)
*Bostan Khan,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: DeepFedNAS提出了一种新的联邦神经架构搜索框架，通过帕累托最优超网训练和预测器免费搜索方法，显著提升了联邦学习中的模型设计效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦神经架构搜索面临两个关键瓶颈：无指导的超网训练导致次优模型，以及后训练子网发现需要多小时的昂贵流程。需要一种更高效、更智能的联邦NAS方法。

Method: 提出两阶段框架：1）联邦帕累托最优超网训练，使用预计算的帕累托最优架构缓存作为智能课程来优化共享超网权重；2）预测器免费搜索方法，利用多目标适应度函数作为零成本精度代理，实现秒级子网发现。

Result: 在CIFAR-100上实现最高1.21%的绝对精度提升，具有优越的参数和通信效率，后训练搜索流程时间加速约61倍，从20多小时减少到约20分钟，单个子网搜索仅需20秒。

Conclusion: DeepFedNAS通过智能的超网训练和高效的搜索方法，使硬件感知的联邦学习部署变得即时实用，显著提升了联邦NAS的效率和性能。

Abstract: Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS

</details>


### [18] [Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2601.14327)
*YuanLab. ai,Shawn Wu,Jiangang Luo,Tong Yu,Darcy Chen,Sean Wang,Xudong Zhao,Louie Li,Claire Wang,Hunter He,Carol Wang,Allen Wang*

Main category: cs.LG

TL;DR: 提出LAEP算法，在MoE LLM预训练阶段自适应剪枝未充分利用的专家并重新组织专家分布，显著提升训练效率并减少参数


<details>
  <summary>Details</summary>
Motivation: MoE LLMs虽然通过减少激活参数实现了更好的准确性，但其预训练阶段存在计算瓶颈，主要原因是专家利用率不足和训练效率有限

Method: 提出层自适应专家剪枝（LAEP）算法，在预训练阶段根据token分布统计选择性剪枝未充分利用的专家，并跨计算设备重新组织专家分布

Result: LAEP有效减少模型大小并显著提升预训练效率。在从头预训练1010B Base模型时，训练效率提升48.3%，参数减少33.3%，同时在多个领域保持优异性能

Conclusion: LAEP算法成功解决了MoE LLMs预训练阶段的效率瓶颈问题，为大规模MoE模型的训练提供了有效的优化方案

Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.

</details>


### [19] [Hierarchical Contextual Uplift Bandits for Catalog Personalization](https://arxiv.org/abs/2601.14333)
*Anupam Agrawal,Rajesh Mohanty,Shamik Bhattacharjee,Abhimanyu Mittal*

Main category: cs.LG

TL;DR: 提出分层上下文提升赌博机框架，通过动态调整上下文粒度解决幻想体育动态环境中的个性化推荐问题，显著提升收入和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 传统上下文赌博机算法在幻想体育等动态环境中表现不佳，用户行为快速变化，奖励分布因外部影响剧烈波动，需要频繁重新训练。

Method: 提出分层上下文提升赌博机框架，动态调整上下文粒度（从系统级洞察到用户级细节），利用上下文相似性促进策略迁移，缓解冷启动问题，并整合提升建模原理。

Result: 在Dream11幻想体育平台的大规模A/B测试中，方法显著提升推荐质量，实现0.4%收入增长并改善用户满意度指标。2025年5月部署到生产环境后，进一步获得0.5%收入提升。

Conclusion: 分层上下文提升赌博机框架有效解决了动态环境中的个性化推荐挑战，在幻想体育平台上实现了显著的业务指标提升，并已成功部署为默认目录个性化系统。

Abstract: Contextual Bandit (CB) algorithms are widely adopted for personalized recommendations but often struggle in dynamic environments typical of fantasy sports, where rapid changes in user behavior and dramatic shifts in reward distributions due to external influences necessitate frequent retraining. To address these challenges, we propose a Hierarchical Contextual Uplift Bandit framework. Our framework dynamically adjusts contextual granularity from broad, system-wide insights to detailed, user-specific contexts, using contextual similarity to facilitate effective policy transfer and mitigate cold-start issues. Additionally, we integrate uplift modeling principles into our approach. Results from large-scale A/B testing on the Dream11 fantasy sports platform show that our method significantly enhances recommendation quality, achieving a 0.4% revenue improvement while also improving user satisfaction metrics compared to the current production system. We subsequently deployed this system to production as the default catalog personalization system in May 2025 and observed a further 0.5% revenue improvement.

</details>


### [20] [Log anomaly detection via Meta Learning and Prototypical Networks for Cross domain generalization](https://arxiv.org/abs/2601.14336)
*Krishna Sharma,Vivek Yelleti*

Main category: cs.LG

TL;DR: 提出基于元学习的日志异常检测框架，通过动态漂移标注和语义匹配实现跨域适应，使用MAML和原型网络快速适应新领域，结合SMOTE处理类别不平衡，在跨域设置中取得最佳F1分数。


<details>
  <summary>Details</summary>
Motivation: 日志异常检测面临类别不平衡和跨域适应两大挑战。传统模型在新领域泛化能力差，且目标域缺乏标注异常数据。需要开发能够快速适应不同日志系统（如HDFS和Linux）的检测方法。

Method: 1. 数据准备：使用Drain3日志解析和动态漂移标注技术，通过语义和模糊匹配将源域异常知识迁移到目标域；2. 特征处理：获取BERT语义嵌入并进行特征选择降维；3. 元学习模型：采用MAML和原型网络进行快速有效适应；4. 不平衡处理：使用SMOTE过采样方法。

Result: 采用留一源方法评估，提出的元学习驱动方法获得了最高的平均F1分数，在跨域设置中被证明是有效的。

Conclusion: 提出的基于元学习的框架成功解决了日志异常检测中的跨域适应和类别不平衡问题，为不同日志系统的异常检测提供了有效的解决方案。

Abstract: Log anomaly detection is essential for system reliability, but it is extremely challenging to do considering it involves class imbalance. Additionally, the models trained in one domain are not applicable to other domains, necessitating the need for cross-domain adaptation (such as HDFS and Linux). Traditional detection models often fail to generalize due to significant data drift and the inherent absence of labeled anomalies in new target domains. To handle the above challenges, we proposed a new end-to-end framework based on a meta-learning approach. Our methodology first gets the data ready by combining a Drain3 log parsing mechanism with a dynamic drift-based labeling technique that uses semantic and fuzzy matching to move existing anomaly knowledge from one source to another. BERT-based semantic embeddings are obtained, and the feature selection is invoked to reduce the dimensionality. Later, Model Agnostic Meta-Learning (MAML) and Prototypical Networks models are trained to adapt quickly and effectively. The SMOTE oversampling method is employed to handle imbalances in the data. All the results are obtained by employing the leave-one-out source method, and the corresponding mean F1 scores are reported. Our empirical findings validate that the proposed meta-learning-driven approach yielded the highest mean F1 score and proved to be effective for cross-domain settings.

</details>


### [21] [DiSPA: Differential Substructure-Pathway Attention for Drug Response Prediction](https://arxiv.org/abs/2601.14346)
*Yewon Han,Sunghyun Kim,Eunyi Jeong,Sungkyung Lee,Seokwoo Yun,Sangsoo Lim*

Main category: cs.LG

TL;DR: DiSPA是一个通过双向条件化化学亚结构与通路级基因表达来解耦结构驱动和上下文驱动药物响应机制的表征学习框架，在GDSC基准测试中达到SOTA性能，并具有零样本迁移到空间转录组学的能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型通常独立处理化学和转录组学模态，或仅在后期结合它们，限制了捕捉药物作用的细粒度、上下文依赖机制的能力。标准注意力机制对高维生物网络中的噪声和稀疏性敏感，阻碍了泛化性和可解释性。

Method: 提出DiSPA框架，通过化学亚结构与通路级基因表达之间的双向条件化，明确解耦结构驱动和上下文驱动的药物响应机制。引入差分交叉注意力模块，抑制虚假的通路-亚结构关联，同时放大上下文相关的相互作用。

Result: 在GDSC基准测试的多个评估设置中达到最先进性能，特别是在评估未见药物-细胞组合泛化性的不相交集设置中表现出显著改进。学习到的注意力模式恢复了已知的药效团，区分了结构驱动和上下文依赖的化合物，并在生物通路中表现出连贯的组织。仅使用bulk RNA-seq数据训练的DiSPA能够零样本迁移到空间转录组学，无需重新训练即可揭示区域特异性药物敏感性模式。

Conclusion: DiSPA建立了一个稳健且可解释的整合药物基因组学建模框架，能够超越事后解释，对药物响应机制进行原则性分析，为精准医学中的药物响应预测提供了新方法。

Abstract: Accurate prediction of drug response in precision medicine requires models that capture how specific chemical substructures interact with cellular pathway states. However, most existing deep learning approaches treat chemical and transcriptomic modalities independently or combine them only at late stages, limiting their ability to model fine-grained, context-dependent mechanisms of drug action. In addition, standard attention mechanisms are often sensitive to noise and sparsity in high-dimensional biological networks, hindering both generalization and interpretability. We present DiSPA, a representation learning framework that explicitly disentangles structure-driven and context-driven mechanisms of drug response through bidirectional conditioning between chemical substructures and pathway-level gene expression. DiSPA introduces a differential cross-attention module that suppresses spurious pathway-substructure associations while amplifying contextually relevant interactions. Across multiple evaluation settings on the GDSC benchmark, DiSPA achieves state-of-the-art performance, with particularly strong improvements in the disjoint-set setting, which assesses generalization to unseen drug-cell combinations. Beyond predictive accuracy, DiSPA yields mechanistically informative representations: learned attention patterns recover known pharmacophores, distinguish structure-driven from context-dependent compounds, and exhibit coherent organization across biological pathways. Furthermore, we demonstrate that DiSPA trained solely on bulk RNA-seq data enables zero-shot transfer to spatial transcriptomics, revealing region-specific drug sensitivity patterns without retraining. Together, these results establish DiSPA as a robust and interpretable framework for integrative pharmacogenomic modeling, enabling principled analysis of drug response mechanisms beyond post hoc interpretation.

</details>


### [22] [VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models](https://arxiv.org/abs/2601.14354)
*Yongchao Huang*

Main category: cs.LG

TL;DR: VJEPA是一种概率化的联合嵌入预测架构，通过变分目标学习未来潜在状态的预测分布，统一了表示学习与预测状态表示和贝叶斯滤波，为高维噪声环境中的可扩展鲁棒规划提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用确定性回归目标，掩盖了概率语义，限制了在随机控制中的应用。需要一种概率化扩展来支持不确定性估计和鲁棒规划。

Method: 提出变分JEPA（VJEPA），通过变分目标学习未来潜在状态的预测分布；进一步提出贝叶斯JEPA（BJEPA），将预测信念分解为学习动态专家和模块化先验专家的乘积，支持零样本任务迁移和约束满足。

Result: VJEPA和BJEPA能成功过滤高方差干扰因素，避免生成基线中的表示坍塌；提供原则性不确定性估计（如通过采样构建可信区间），同时保持对观测的似然无关性。

Conclusion: VJEPA为高维噪声环境中的可扩展、鲁棒、不确定性感知规划提供了基础框架，统一了表示学习与预测状态表示，证明了序列建模不需要自回归观测似然。

Abstract: Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.

</details>


### [23] [Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation](https://arxiv.org/abs/2601.14473)
*Danny Butvinik,Nana Boateng,Achi Hackmon*

Main category: cs.LG

TL;DR: 提出一种在线自适应核密度估计方法，将风险评分流转换为审查队列，无需标签且支持多队列路由，在满足容量约束的同时减少阈值抖动。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用top-K或手动调整阈值来将风险评分流转换为审查队列，但这些方法在明确的容量约束下不够灵活，且可能产生阈值抖动问题。

Method: 对评分流拟合在线自适应核密度估计，将密度转换为尾部质量曲线以满足容量要求，并将结果"捕捉"到跨带宽检测到的持久密度谷值，支持滑动窗口或指数遗忘的实时操作。

Result: 在合成、漂移、多模态流数据上，该方法在保持竞争性容量依从性的同时减少了阈值抖动，每个事件更新成本为O(G)，每个活动占用恒定内存。

Conclusion: 该方法提供了一种无标签、支持多队列路由的实时解决方案，能够有效处理风险评分流转换问题，在满足容量约束的同时保持阈值稳定性。

Abstract: We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity

</details>


### [24] [GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling](https://arxiv.org/abs/2601.14476)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.LG

TL;DR: 该论文发现p-bit设备变异性不仅能降低计算性能，还能通过利用时序变异性增强算法性能，并开发了GPU加速的模拟退火框架，在MAX-CUT问题上实现了两个数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索使用磁性隧道结等新兴设备实现概率比特时产生的设备变异性对计算性能的影响。传统观点认为设备变异性会负面影响性能，但本研究旨在全面分析这种影响，并开发能够准确模拟真实设备行为的计算框架。

Method: 方法包括：1) 开发基于CUDA的GPU加速开源模拟退火框架；2) 建模三种关键设备变异性因素：时序、强度和偏移变异性；3) 在MAX-CUT基准问题上进行测试，问题规模从800到20,000个节点；4) 比较GPU实现与CPU实现的性能差异。

Result: 主要结果：1) 发现设备变异性不仅能降低性能，还能通过利用时序变异性增强算法性能；2) GPU加速框架相比CPU实现实现了两个数量级的速度提升；3) 框架能够有效模拟真实设备行为，为概率计算研究提供了可扩展且易用的工具。

Conclusion: 结论表明设备变异性在概率计算中具有双重作用，既可能降低性能，也可能通过适当利用时序变异性来增强性能。开发的GPU加速框架为概率计算研究提供了强大工具，能够推动优化算法在多个领域的应用，并促进新兴设备在计算系统中的实际部署。

Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -- timing, intensity, and offset -- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.

</details>


### [25] [Stabilizing autoregressive forecasts in chaotic systems via multi-rate latent recurrence](https://arxiv.org/abs/2601.14487)
*Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: MSR-HINE：一种用于混沌动力系统长期预测的分层隐式预测器，通过多尺度潜在先验和多速率循环模块，显著提升了长期预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 混沌动力系统的长期自回归预测面临挑战：小的一步误差会迅速放大，导致物理不一致的预测和大尺度统计特征崩溃。现有方法难以同时保持长期上下文和快速尺度变异性。

Method: 提出MSR-HINE分层隐式预测器，结合多尺度潜在先验和多速率循环模块。采用粗到细的循环状态生成潜在先验，隐式一步预测器通过多尺度潜在注入细化状态，门控融合与后验潜在确保尺度一致性更新，轻量级隐藏状态校正对齐循环记忆与融合潜在。

Result: 在两个基准测试中表现优异：在Kuramoto-Sivashinsky系统上，H=400时端到端RMSE降低62.8%，ACC从-0.155提升到0.828；在Lorenz-96系统上，H=100时RMSE降低27.0%，ACC从0.144提升到0.545。两个系统的ACC≥0.5可预测范围分别从241步扩展到400步和从58步扩展到100步。

Conclusion: MSR-HINE通过分层多尺度架构有效缓解了混沌系统中的误差累积问题，在保持长期慢流形上下文的同时保留了快速尺度变异性，显著提升了长期预测的准确性和稳定性。

Abstract: Long-horizon autoregressive forecasting of chaotic dynamical systems remains challenging due to rapid error amplification and distribution shift: small one-step inaccuracies compound into physically inconsistent rollouts and collapse of large-scale statistics. We introduce MSR-HINE, a hierarchical implicit forecaster that augments multiscale latent priors with multi-rate recurrent modules operating at distinct temporal scales. At each step, coarse-to-fine recurrent states generate latent priors, an implicit one-step predictor refines the state with multiscale latent injections, and a gated fusion with posterior latents enforces scale-consistent updates; a lightweight hidden-state correction further aligns recurrent memories with fused latents. The resulting architecture maintains long-term context on slow manifolds while preserving fast-scale variability, mitigating error accumulation in chaotic rollouts. Across two canonical benchmarks, MSR-HINE yields substantial gains over a U-Net autoregressive baseline: on Kuramoto-Sivashinsky it reduces end-horizon RMSE by 62.8% at H=400 and improves end-horizon ACC by +0.983 (from -0.155 to 0.828), extending the ACC >= 0.5 predictability horizon from 241 to 400 steps; on Lorenz-96 it reduces RMSE by 27.0% at H=100 and improves end horizon ACC by +0.402 (from 0.144 to 0.545), extending the ACC >= 0.5 horizon from 58 to 100 steps.

</details>


### [26] [Learning PDE Solvers with Physics and Data: A Unifying View of Physics-Informed Neural Networks and Neural Operators](https://arxiv.org/abs/2601.14517)
*Yilong Dai,Shengyu Chen,Ziyi Wang,Xiaowei Jia,Yiqun Xie,Vipin Kumar,Runlong Yu*

Main category: cs.LG

TL;DR: 该论文提出一个统一框架来分析物理信息神经网络（PINNs）和神经算子（NOs）这两种主流PDE学习方法，从三个维度组织现有方法：学习内容、物理结构整合方式、计算负载分摊机制。


<details>
  <summary>Details</summary>
Motivation: 当前科学建模中基于学习的PDE求解方法日益重要，但缺乏统一视角来理解不同方法（如PINNs和NOs）之间的关系、局限性和在科学工作流中的适用角色。

Method: 提出一个统一的设计空间框架，从三个基本维度组织现有方法：1) 学习什么（what is learned），2) 如何将物理结构整合到学习过程中（how physical structures are integrated），3) 如何在不同问题实例间分摊计算负载（how computational load is amortized）。

Result: 通过这个统一视角，可以更好地理解现有方法的挑战和局限性，这些挑战往往是学习PDE结构特性的结果。该框架为分析现有进展提供了系统化的工具。

Conclusion: 该统一视角有助于开发可靠的基于学习的PDE求解器，促进物理学与数据科学的融合，为领域发展提供系统化的分析框架。

Abstract: Partial differential equations (PDEs) are central to scientific modeling. Modern workflows increasingly rely on learning-based components to support model reuse, inference, and integration across large computational processes. Despite the emergence of various physics-aware data-driven approaches, the field still lacks a unified perspective to uncover their relationships, limitations, and appropriate roles in scientific workflows. To this end, we propose a unifying perspective to place two dominant paradigms: Physics-Informed Neural Networks (PINNs) and Neural Operators (NOs), within a shared design space. We organize existing methods from three fundamental dimensions: what is learned, how physical structures are integrated into the learning process, and how the computational load is amortized across problem instances. In this way, many challenges can be best understood as consequences of these structural properties of learning PDEs. By analyzing advances through this unifying view, our survey aims to facilitate the development of reliable learning-based PDE solvers and catalyze a synthesis of physics and data.

</details>


### [27] [How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness](https://arxiv.org/abs/2601.14519)
*Giulio Rossolini*

Main category: cs.LG

TL;DR: 论文研究了对抗性攻击作为随机扰动鲁棒性评估代理的有效性，提出了一个概率框架来量化方向性偏置扰动分布下的噪声风险，并通过实验验证对抗性攻击在多大程度上能反映真实的噪声风险。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击被广泛用于评估模型鲁棒性，但其作为随机扰动鲁棒性代理的有效性仍存在争议。作者质疑对抗性扰动是否能代表相同幅度随机噪声下的鲁棒性估计，还是仅仅反映了非典型的极端情况。

Method: 引入了一个概率度量框架，通过方向性偏置扰动分布来量化噪声风险，使用浓度因子κ在等向噪声和对抗方向之间插值。提出了在统计上更接近均匀噪声的对抗攻击策略，并在ImageNet和CIFAR-10上系统性地评估了广泛使用的攻击方法。

Result: 实验系统性地评估了广泛使用的对抗攻击方法，揭示了对抗性成功在何时能有效反映噪声风险，何时会失效，为安全导向的评估提供了指导。

Conclusion: 对抗性攻击作为随机扰动鲁棒性代理的有效性是有条件的，论文提出的框架能帮助识别对抗性攻击何时能提供有意义的噪声风险估计，何时会误导评估，从而为安全导向的模型评估提供更可靠的指导。

Abstract: Adversarial attacks are widely used to evaluate model robustness, yet their validity as proxies for robustness to random perturbations remains debated. We ask whether an adversarial perturbation provides a representative estimate of robustness under random noise of the same magnitude, or instead reflects an atypical worst-case event. To this end, we introduce a probabilistic metric that quantifies noisy risk with respect to directionally biased perturbation distributions, parameterized by a concentration factor $κ$ that interpolates between isotropic noise and adversarial direction. Using this framework, we study the limits of adversarial perturbations as estimators of noisy risk by proposing an attack strategy designed to operate in regimes statistically closer to uniform noise. Experiments on ImageNet and CIFAR-10 systematically benchmark widely used attacks, highlighting when adversarial success meaningfully reflects noisy risk and when it fails, thereby informing their use in safety-oriented evaluation.

</details>


### [28] [On the Runway Cascade of Transformers for Language Modeling](https://arxiv.org/abs/2601.14522)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 论文提出runway-aware rewiring机制，通过显式整合runway上下文到注意力机制中，解决因果transformer中直接路径与间接路径信息传播不匹配的问题，提升语言建模、信息检索和外推能力。


<details>
  <summary>Details</summary>
Motivation: 因果transformer中，因果掩码创建的计算图通过直接路径注意力和中间令牌形成的间接路径传播信息。研究发现这两种信息传播模式的不匹配会导致冗余和无关信息级联到令牌表示中，即使注意力模式已经充分学习，这解释了最近工作中观察到的因果transformer的某些失败模式。

Method: 提出runway-aware rewiring机制，根据每个令牌的runway景观摘要重新连接注意力模式。该方法将runway上下文直接整合到每个令牌的直接路径注意力中，使模型能够意识到累积的表征影响，实现更平衡的信息传播。该方法不引入额外参数，可无缝集成到标准注意力机制中。

Result: 实验表明，经过rewiring的transformer在通用语言建模方面获得稳定改进，同时在信息检索和外推能力方面相比标准transformer表现出明显更强的性能。

Conclusion: 通过显式整合runway上下文到注意力机制中，可以有效解决因果transformer中直接路径与间接路径信息传播不匹配的问题，提升模型在各种任务上的性能，且无需额外参数。

Abstract: In decoder-only (causal) transformers, the computation graph created by causal masking routes information through both direct-path attention and indirect paths formed by intermediate tokens. We denote these indirect paths between token pairs as their runways. We argue that certain failure modes of causal transformers as observed by a growing body of recent works are likely exacerbated by a misalignment between these two information propagation modes. We formalize runway cascade as a phenomenon whereby this misalignment results in redundancies and irrelevant information cascading to token representations despite adequately learned attention patterns. As a solution, we propose runway-aware rewiring as a more explicit way of incorporating runway context directly into each token's direct-path attention. This mechanism re-wires the attention pattern for each token based on a summary of its runway landscape, enabling awareness of accumulating representational influences and allowing for more balanced information propagation. Our proposed methodology introduces no additional parameters and can seamlessly be integrated into standard attention mechanism. Empirically, our rewired transformer results in steady improvements in general language modeling as well as noticeably stronger information retrieval and extrapolation abilities compared to standard transformers.

</details>


### [29] [Search over Self-Edit Strategies for LLM Adaptation](https://arxiv.org/abs/2601.14532)
*Alistair Cheong,Haolin Cong,Tyler Yang,Dustin Miao*

Main category: cs.LG

TL;DR: 研究探索LLM能否利用任务反馈自主决定权重更新策略，在SEAL框架中放宽固定模板限制，让模型生成自编辑模板以控制训练数据和超参数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM开放搜索系统通常冻结基础模型，这可能限制长期进步。虽然已有研究探索在测试时更新提议模型，但更新策略仍需人工指定。因此本研究旨在探索LLM能否利用任务反馈自主决定如何更新权重。

Method: 在SEAL框架中放宽固定人类模板约束，允许模型生成自编辑模板，从而控制训练数据和NTP超参数。研究两种变体：无存档版本和基于轻量级历史模板存档的条件生成版本。在Qwen3-8B模型和SQuAD数据集上进行实验。

Result: 无存档变体表现与较弱的"Implications"基线相当，存档变体优于"Implications"基线并接近最强的人工设计"Rewrite"基线但未超越。分析显示朴素存档能提供短期鲁棒性，但也可能加速同质化。

Conclusion: LLM能够利用任务反馈自主决定权重更新策略，但需要显式的新颖性压力才能持续超越精心优化的人工策略。朴素存档虽能提供短期鲁棒性，但可能加速同质化。

Abstract: Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker "Implications" baseline, while the archive variant outperformed "Implications" and approached the strongest human-designed "Rewrite" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .

</details>


### [30] [engGNN: A Dual-Graph Neural Network for Omics-Based Disease Classification and Feature Selection](https://arxiv.org/abs/2601.14536)
*Tiantian Yang,Yuxuan Wang,Zhenwei Zhou,Ching-Ti Liu*

Main category: cs.LG

TL;DR: engGNN提出了一种双图神经网络框架，同时利用外部生物网络和基于数据生成的图，以提高高维组学数据的预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 组学数据（如转录组学、蛋白质组学）具有高维性、小样本量和复杂生物网络的特点，现有方法通常只依赖外部知识图或数据驱动生成的图，无法充分利用互补信息。

Method: engGNN采用双图框架：1）从已知生物网络数据库构建生物学信息无向特征图；2）从树集成模型生成有向特征图。通过联合利用这两种图结构生成更全面的嵌入表示。

Result: 在模拟和真实基因表达数据应用中，engGNN持续优于最先进的基线方法。除了分类性能提升，还能提供可解释的特征重要性评分，支持生物学意义的发现（如通路富集分析）。

Conclusion: engGNN是一个稳健、灵活且可解释的框架，适用于高维组学环境中的疾病分类和生物标志物发现，通过整合外部知识和数据驱动信息实现了性能提升。

Abstract: Omics data, such as transcriptomics, proteomics, and metabolomics, provide critical insights into disease mechanisms and clinical outcomes. However, their high dimensionality, small sample sizes, and intricate biological networks pose major challenges for reliable prediction and meaningful interpretation. Graph Neural Networks (GNNs) offer a promising way to integrate prior knowledge by encoding feature relationships as graphs. Yet, existing methods typically rely solely on either an externally curated feature graph or a data-driven generated one, which limits their ability to capture complementary information. To address this, we propose the external and generated Graph Neural Network (engGNN), a dual-graph framework that jointly leverages both external known biological networks and data-driven generated graphs. Specifically, engGNN constructs a biologically informed undirected feature graph from established network databases and complements it with a directed feature graph derived from tree-ensemble models. This dual-graph design produces more comprehensive embeddings, thereby improving predictive performance and interpretability. Through extensive simulations and real-world applications to gene expression data, engGNN consistently outperforms state-of-the-art baselines. Beyond classification, engGNN provides interpretable feature importance scores that facilitate biologically meaningful discoveries, such as pathway enrichment analysis. Taken together, these results highlight engGNN as a robust, flexible, and interpretable framework for disease classification and biomarker discovery in high-dimensional omics contexts.

</details>


### [31] [QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design](https://arxiv.org/abs/2601.14549)
*Nilesh Prasad Pandey,Jangseon Park,Onat Gungor,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: QMC是一种无需重新训练的量化方法，结合异构内存架构，将SLM中的正常权重存储在紧凑的ReRAM中，关键异常值存储在高精度MRAM中，显著降低内存使用、数据传输、能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在边缘平台上部署小型语言模型面临内存、延迟和能耗限制。现有量化方法受设备噪声影响，传统内存层次结构效率有限：SRAM密度低，DRAM需同时处理静态权重和动态KV缓存导致带宽争用，Flash主要用于初始化而在推理期间闲置。

Method: 提出QMC方法：1）识别SLM中的正常权重和异常值权重；2）将正常权重存储在紧凑的多级ReRAM中；3）将关键异常值保存在高精度片上MRAM中；4）无需重新训练的量化与异构内存架构协同设计。

Result: 在语言建模和推理基准测试中，QMC优于或匹配使用先进算法和混合数据格式的最先进量化方法。在最新边缘AI平台上，相比FP16，内存使用减少6.3-7.3倍，外部数据传输减少7.6倍，能耗降低11.7倍，延迟减少12.5倍。

Conclusion: QMC是一种可扩展、可部署的协同设计方法，通过异常值感知量化与异构内存架构的结合，有效解决了边缘设备上SLM推理的内存、带宽和能耗限制问题。

Abstract: Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.

</details>


### [32] [Constructing Multi-label Hierarchical Classification Models for MITRE ATT&CK Text Tagging](https://arxiv.org/abs/2601.14556)
*Andrew Crossman,Jonah Dodd,Viralam Ramamurthy Chaithanya Kumar,Riyaz Mohammed,Andrew R. Plummer,Chandra Sekharudu,Deepak Warrier,Mohammad Yekrangian*

Main category: cs.LG

TL;DR: 该论文提出了一种分层任务空间框架，用于自动化MITRE ATT&CK文本标注任务，并构建了基于经典机器学习的多标签分层分类模型，在战术层面达到94%准确率，技术层面达到82%准确率，优于GPT-4o等复杂方法。


<details>
  <summary>Details</summary>
Motivation: 当前MITRE ATT&CK知识库的标注工作主要依赖人工完成，效率低下且难以扩展。需要自动化解决方案来加速网络安全情报分析，但现有方法缺乏系统化的任务框架。

Method: 提出分层"任务空间"框架来组织ATT&CK标注任务，采用多标签分层分类方法，使用经典机器学习技术（而非LLM或复杂分层方法），构建可共享的计算工具和公开模型。

Result: 模型在战术层面达到约94%准确率，技术层面达到约82%准确率，超越GPT-4o（约60%准确率）并达到或超过最先进水平。模型已应用于金融威胁场景分析。

Conclusion: 该研究证明了经典机器学习方法在ATT&CK文本标注任务中的有效性，提供了系统化任务框架和开源工具，为网络安全社区提供了实用的自动化解决方案。

Abstract: MITRE ATT&CK is a cybersecurity knowledge base that organizes threat actor and cyber-attack information into a set of tactics describing the reasons and goals threat actors have for carrying out attacks, with each tactic having a set of techniques that describe the potential methods used in these attacks. One major application of ATT&CK is the use of its tactic and technique hierarchy by security specialists as a framework for annotating cyber-threat intelligence reports, vulnerability descriptions, threat scenarios, inter alia, to facilitate downstream analyses. To date, the tagging process is still largely done manually. In this technical note, we provide a stratified "task space" characterization of the MITRE ATT&CK text tagging task for organizing previous efforts toward automation using AIML methods, while also clarifying pathways for constructing new methods. To illustrate one of the pathways, we use the task space strata to stage-wise construct our own multi-label hierarchical classification models for the text tagging task via experimentation over general cyber-threat intelligence text -- using shareable computational tools and publicly releasing the models to the security community (via https://github.com/jpmorganchase/MITRE_models). Our multi-label hierarchical approach yields accuracy scores of roughly 94% at the tactic level, as well as accuracy scores of roughly 82% at the technique level. The models also meet or surpass state-of-the-art performance while relying only on classical machine learning methods -- removing any dependence on LLMs, RAG, agents, or more complex hierarchical approaches. Moreover, we show that GPT-4o model performance at the tactic level is significantly lower (roughly 60% accuracy) than our own approach. We also extend our baseline model to a corpus of threat scenarios for financial applications produced by subject matter experts.

</details>


### [33] [Place with Intention: An Empirical Attendance Predictive Study of Expo 2025 Osaka, Kansai, Japan](https://arxiv.org/abs/2601.14570)
*Xiaojie Yang,Dizhi Huang,Hangli Ge,Masahiro Sano,Takeaki Ohdake,Kazuma Hatano,Noboru Koshizuka*

Main category: cs.LG

TL;DR: 提出基于Transformer的框架，利用预约动态（门票预订和更新）作为参观者出席意图的代理，用于大规模国际活动的出席预测，避免多源外部数据依赖。


<details>
  <summary>Details</summary>
Motivation: 大规模国际活动（如世博会）的准确出席预测对交通、人流和服务管理至关重要。现有方法依赖多源外部数据（天气、交通、社交媒体），但在历史数据不足时不可靠。

Method: 提出Transformer框架，利用预约动态（门票预订和更新）作为参观者出席意图的代理，假设这些意图最终反映在预约模式中。构建包含入场记录和预约动态的数据集，在单通道（总出席）和双通道（东/西门分开）设置下评估模型。

Result: 结果显示，分开建模东门和西门能持续提高预测准确性，特别是对短期和中期预测。消融研究证实编码器-解码器结构、逆风格嵌入和自适应融合模块的重要性。

Conclusion: 预约动态为大规模国际活动的出席预测提供了实用且信息丰富的基础，避免了多源数据整合的复杂性，同时通过预约模式隐式捕获了外部影响因素。

Abstract: Accurate forecasting of daily attendance is vital for managing transportation, crowd flows, and services at large-scale international events such as Expo 2025 Osaka, Kansai, Japan. However, existing approaches often rely on multi-source external data (such as weather, traffic, and social media) to improve accuracy, which can lead to unreliable results when historical data are insufficient. To address these challenges, we propose a Transformer-based framework that leverages reservation dynamics, i.e., ticket bookings and subsequent updates within a time window, as a proxy for visitors' attendance intentions, under the assumption that such intentions are eventually reflected in reservation patterns. This design avoids the complexity of multi-source integration while still capturing external influences like weather and promotions implicitly embedded in reservation dynamics. We construct a dataset combining entrance records and reservation dynamics and evaluate the model under both single-channel (total attendance) and two-channel (separated by East and West gates) settings. Results show that separately modeling East and West gates consistently improves accuracy, particularly for short- and medium-term horizons. Ablation studies further confirm the importance of the encoder-decoder structure, inverse-style embedding, and adaptive fusion module. Overall, our findings indicate that reservation dynamics offer a practical and informative foundation for attendance forecasting in large-scale international events.

</details>


### [34] [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 该研究评估了使用大语言模型（GPT-4、BioMistral-7B、LLaMA-3.1-8B）生成反事实解释的效果，在临床数据集上验证了其在干预质量和数据增强方面的有效性，优于传统优化方法。


<details>
  <summary>Details</summary>
Motivation: 反事实解释能为机器学习预测提供可操作的干预建议，并可作为数据增强提升模型鲁棒性。传统优化方法生成的CFs在临床场景中缺乏语义连贯性和可操作性，需要探索LLM生成CFs的潜力。

Method: 使用多模态AI-READI临床数据集，评估GPT-4（零样本和少样本）、BioMistral-7B和LLaMA-3.1-8B（预训练和微调）生成CFs的效果。从干预质量、特征多样性和增强效果三个维度评估，并与DiCE、CFNOW、NICE等优化基线比较。

Result: 微调后的LLaMA-3.1-8B生成CFs的合理性达99%，有效性达0.99，特征调整现实且可行为修改。在标签稀缺场景下，LLM生成的CFs作为数据增强可平均恢复20%的F1分数，优于传统优化方法。

Conclusion: LLM驱动的反事实解释在可解释干预设计和数据高效模型训练方面具有潜力，SenseCF框架通过微调LLM生成有效的CFs并补充不平衡数据集，提升了模型鲁棒性和预测性能。

Abstract: Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.
  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance

</details>


### [35] [Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective](https://arxiv.org/abs/2601.14599)
*Xiao Hu,Hong Xie,Tao Tan,Defu Lian,Jianyu Han*

Main category: cs.LG

TL;DR: 该论文提出了一种自下而上的实验框架来分析强化学习微调LLM中的各种优化选择，通过最小化配置连接多臂老虎机理论，逐步扩展实验设计来理解每个选择的作用和瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习微调领域存在大量启发式方法，但各种主张相互矛盾，缺乏对两个基本问题的清晰理解：1）每个优化选择的作用是什么？2）哪些是瓶颈？

Method: 提出自下而上的实验流程：底层是最小化配置（单一训练数据、每轮单次rollout、直接使用奖励作为学习信号），连接多臂老虎机理论；然后逐层扩展配置，检验每个设计选择的作用。

Result: 在三个LLM和两个推理数据集上的实验结果不仅揭示了设计选择的新理解，还为该领域提供了重要见解。

Conclusion: 该研究通过系统化的实验框架澄清了LLM强化学习微调中的优化选择作用，为领域发展提供了理论基础和实践指导。

Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.

</details>


### [36] [Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum](https://arxiv.org/abs/2601.14603)
*Jingru Li,Yibo Fan,Huan Li*

Main category: cs.LG

TL;DR: Muon通过正交动量更新加速LLM预训练，提出了Muon-NSR和Muon-VS两种变体，在GPT-2和LLaMA预训练中比AdamW和Muon基准收敛更快、验证损失更低。


<details>
  <summary>Details</summary>
Motivation: LLM预训练计算成本高昂，优化器效率成为重要实际考虑。现有Adam优化器可视为方差自适应符号更新算法，但仍有改进空间。

Method: 提出Muon优化器，通过正交动量更新作为元素级符号算子的矩阵模拟。进一步提出Muon-NSR（应用噪声信号比调制）和Muon-VS（执行方差缩放）两种变体，在正交化前对动量应用方差自适应归一化。

Result: 在GPT-2和LLaMA预训练实验中，Muon-NSR和Muon-VS加速收敛，始终比调优良好的AdamW和Muon基准获得更低的验证损失。例如在LLaMA-1.2B模型上，达到目标验证损失所需的迭代次数减少了1.36倍。

Conclusion: Muon-NSR和Muon-VS通过方差自适应归一化正交动量更新，显著提高了LLM预训练效率，为大规模语言模型训练提供了更高效的优化器选择。

Abstract: Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\times$ relative to the well-tuned Muon following the recent benchmark.

</details>


### [37] [Relational Graph Modeling for Credit Default Prediction: Heterogeneous GNNs and Hybrid Ensemble Learning](https://arxiv.org/abs/2601.14633)
*Yvonne Yang,Eranki Vasistha*

Main category: cs.LG

TL;DR: 该研究构建了一个大规模异构图（3100万节点，5000万边）来建模信用违约风险，评估了异构图神经网络（GNN）与表格模型的性能，发现GNN单独使用提升有限，但将GNN嵌入与表格特征结合的混合集成方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 信用违约风险涉及借款人、金融机构和交易行为的复杂交互。虽然表格模型在信用评分中表现良好，但难以显式捕捉多表金融历史中的跨实体依赖关系。

Method: 构建大规模异构图（3100万节点，5000万边），整合借款人属性和细粒度交易实体（分期付款、POS余额、信用卡历史）。评估异构图神经网络（异构图SAGE和关系感知注意力异构图GNN），并与梯度提升树等表格基线对比，还探索了对比预训练和混合集成方法。

Result: 独立GNN相比梯度提升树基线提升有限，但将GNN生成的客户嵌入与表格特征结合的混合集成方法在ROC-AUC和PR-AUC上表现最佳。对比预训练能改善优化稳定性，但在通用图增强下下游增益有限。通过可解释性和公平性分析揭示了关系信号对子群行为的影响。

Conclusion: 在信用违约风险建模中，异构图神经网络单独使用效果有限，但作为表格模型的补充特征提取器时能显著提升性能。关系感知建模需要结合传统特征工程方法，同时需要考虑模型的可解释性和公平性影响。

Abstract: Credit default risk arises from complex interactions among borrowers, financial institutions, and transaction-level behaviors. While strong tabular models remain highly competitive in credit scoring, they may fail to explicitly capture cross-entity dependencies embedded in multi-table financial histories. In this work, we construct a massive-scale heterogeneous graph containing over 31 million nodes and more than 50 million edges, integrating borrower attributes with granular transaction-level entities such as installment payments, POS cash balances, and credit card histories.
  We evaluate heterogeneous graph neural networks (GNNs), including heterogeneous GraphSAGE and a relation-aware attentive heterogeneous GNN, against strong tabular baselines. We find that standalone GNNs provide limited lift over a competitive gradient-boosted tree baseline, while a hybrid ensemble that augments tabular features with GNN-derived customer embeddings achieves the best overall performance, improving both ROC-AUC and PR-AUC. We further observe that contrastive pretraining can improve optimization stability but yields limited downstream gains under generic graph augmentations. Finally, we conduct structured explainability and fairness analyses to characterize how relational signals affect subgroup behavior and screening-oriented outcomes.

</details>


### [38] [Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport](https://arxiv.org/abs/2601.14653)
*Yuyu Liu,Jiannan Yang,Ziyang Yu,Weishen Pan,Fei Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: CROT是一种基于最优传输的插补算法，专门处理表格数据中的块状缺失数据，在保持高精度的同时显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序数据中的缺失数据给生物学分析带来挑战，现有插补方法假设数据均匀完整，难以处理大块缺失数据的情况。

Method: 提出CROT算法，基于最优传输理论设计，专门处理表格格式中的块状缺失数据，能够有效捕捉存在显著缺失情况下的底层数据结构。

Result: CROT实现了优越的插补精度，同时显著减少了运行时间，展示了其在大规模数据集上的可扩展性和效率。

Conclusion: 该工作为异质高维数据集中结构化数据缺失的插补提供了鲁棒解决方案，解决了生物和临床数据分析中的关键挑战。

Abstract: Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github.

</details>


### [39] [Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2601.14693)
*Jianwen Sun,Xinrui Li,Fuqing Li,Xiaoxuan Shen*

Main category: cs.LG

TL;DR: 提出EGRL-SR框架，使用经验驱动的目标条件强化学习进行符号回归，通过历史轨迹和动作价值网络主动引导搜索，相比传统误差驱动方法在恢复率和鲁棒性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统基于搜索的符号回归方法主要依赖拟合误差来指导搜索，但在庞大的表达式空间中，许多候选表达式可能有相似的误差值但结构差异很大，导致搜索方向模糊，难以收敛到真实函数。

Method: 将符号回归建模为目标条件强化学习问题，结合后见经验回放，让动作价值网络从多样化的输入-输出对中学习通用映射模式。设计了全点满足二元奖励函数，使网络关注结构模式而非低误差表达式，并提出结构引导的启发式探索策略增强搜索多样性和空间覆盖。

Result: 在公共基准测试中，EGRL-SR在恢复率和鲁棒性方面持续优于最先进方法，在相同搜索预算下能恢复更复杂的表达式。消融实验验证了动作价值网络能有效指导搜索，奖励函数和探索策略都起到关键作用。

Conclusion: EGRL-SR通过经验驱动的目标条件强化学习框架，解决了传统误差驱动方法在符号回归中的搜索方向模糊问题，实现了更鲁棒的表达式搜索，能够发现更复杂的数学表达式。

Abstract: Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.

</details>


### [40] [Re-understanding Graph Unlearning through Memorization](https://arxiv.org/abs/2601.14694)
*Pengfei Ding,Yan Wang,Guanfeng Liu*

Main category: cs.LG

TL;DR: 提出MGU框架，基于GNN记忆化视角解决图遗忘的三个核心问题：准确评估遗忘难度、自适应调整遗忘策略、建立全面评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有图遗忘方法存在三个根本局限：1) 遗忘难度评估不准确且不实用（需要测试访问和无效假设）；2) 对难以遗忘的任务效果不佳；3) 评估协议错位，过度强调简单任务而未能捕捉真实遗忘能力。

Method: 提出MGU（Memorization-guided Graph Unlearning）框架：1) 建立GNN记忆化作为理解图遗忘的新视角；2) 提供准确实用的难度评估；3) 开发基于难度水平的自适应策略动态调整遗忘目标；4) 建立符合实际需求的全面评估协议。

Result: 在10个真实世界图数据上的广泛实验表明，MGU在遗忘质量、计算效率和效用保持方面持续优于最先进的基线方法。

Conclusion: MGU通过记忆化视角解决了图遗忘的关键挑战，提供了更准确、实用和全面的解决方案，为图神经网络的安全应用提供了重要支持。

Abstract: Graph unlearning (GU), which removes nodes, edges, or features from trained graph neural networks (GNNs), is crucial in Web applications where graph data may contain sensitive, mislabeled, or malicious information. However, existing GU methods lack a clear understanding of the key factors that determine unlearning effectiveness, leading to three fundamental limitations: (1) impractical and inaccurate GU difficulty assessment due to test-access requirements and invalid assumptions, (2) ineffectiveness on hard-to-unlearn tasks, and (3) misaligned evaluation protocols that overemphasize easy tasks and fail to capture true forgetting capability. To address these issues, we establish GNN memorization as a new perspective for understanding graph unlearning and propose MGU, a Memorization-guided Graph Unlearning framework. MGU achieves three key advances: it provides accurate and practical difficulty assessment across different GU tasks, develops an adaptive strategy that dynamically adjusts unlearning objectives based on difficulty levels, and establishes a comprehensive evaluation protocol that aligns with practical requirements. Extensive experiments on ten real-world graphs demonstrate that MGU consistently outperforms state-of-the-art baselines in forgetting quality, computational efficiency, and utility preservation.

</details>


### [41] [CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation](https://arxiv.org/abs/2601.14695)
*Yutong Chen,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 提出CoScale-RL方法，通过扩大解决方案而非数据集来提升大型推理模型的训练稳定性和效率


<details>
  <summary>Details</summary>
Motivation: 训练大型推理模型通常不稳定且难以预测，特别是在处理困难问题或基础模型较弱时。当前的后训练扩展策略在这些情况下仍有改进空间。

Method: CoScale-RL包含三个核心部分：1) 扩展解决方案 - 为每个问题收集多个解决方案而非简单扩大数据集；2) 扩展rollout计算以稳定强化学习；3) 使用Re-distillation模型合并技术维持计算效率

Result: 在四个基准测试上平均获得3.76倍的准确率提升，显著提高了数据和计算效率，能够在不依赖大量监督微调数据集的情况下提升LRM的能力边界

Conclusion: CoScale-RL为提升大型推理模型的推理能力提供了新的扩展方向，通过更好的数据和计算效率解决了训练不稳定问题

Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.

</details>


### [42] [Case-Guided Sequential Assay Planning in Drug Discovery](https://arxiv.org/abs/2601.14710)
*Tianchi Chen,Jan Bima,Sean L. Wu,Otto Ritter,Bingjia Yang,Xiang Yu*

Main category: cs.LG

TL;DR: IBMDP框架利用历史数据构建隐式贝叶斯模型，通过集成MCTS规划在无模拟器的药物发现实验中实现资源高效决策


<details>
  <summary>Details</summary>
Motivation: 药物发现中的实验顺序规划面临严重不确定性和资源约束，标准强化学习缺乏环境模拟器或转移数据，需要仅依赖静态历史数据库进行规划

Method: 提出隐式贝叶斯马尔可夫决策过程(IBMDP)，通过相似历史结果构建非参数信念分布，实现贝叶斯信念更新，并采用集成蒙特卡洛树搜索规划生成稳定策略

Result: 在真实CNS药物发现任务中，IBMDP比现有启发式方法减少92%资源消耗；在合成环境中，比确定性值迭代方法更接近最优策略

Conclusion: IBMDP为数据丰富但模拟器稀缺领域的顺序实验设计提供了实用解决方案，展示了集成规划器的优越性

Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.

</details>


### [43] [PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning](https://arxiv.org/abs/2601.14716)
*Yao Lu,Dengdong Fan,Jianzheng Nie,Fan Xu,Jie Chen,Bin Zhou,Yonghong Tian*

Main category: cs.LG

TL;DR: PCL-Reasoner-V1.5是基于Qwen2.5-32B构建的320亿参数数学推理大模型，采用监督微调和强化学习训练，提出离线RL方法提升训练稳定性和效率，在AIME竞赛中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发更稳定高效的强化学习方法用于大语言模型的数学推理能力提升，解决标准在线RL方法（如GRPO）存在的训练不稳定和效率问题。

Method: 基于Qwen2.5-32B构建320亿参数模型，采用监督微调（SFT）后接强化学习（RL）的两阶段训练策略，核心创新是提出的离线RL方法，相比在线RL方法（如GRPO）提供更好的训练稳定性和效率。

Result: 在AIME 2024上达到90.9%的平均准确率，在AIME 2025上达到85.6%的平均准确率，在基于Qwen2.5-32B后训练的模型中达到最先进的性能水平。

Conclusion: 离线RL方法为推进大语言模型推理能力提供了一种稳定高效的训练范式，所有实验均在华为昇腾910C NPU上完成。

Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.

</details>


### [44] [FSX: Message Flow Sensitivity Enhanced Structural Explainer for Graph Neural Networks](https://arxiv.org/abs/2601.14730)
*Bizu Feng,Zhimu Yang,Shaode Yu,Zixin Hu*

Main category: cs.LG

TL;DR: FSX是一个新颖的GNN可解释性框架，通过结合内部消息流分析和合作博弈方法，高效生成高保真度的结构解释。


<details>
  <summary>Details</summary>
Motivation: 现有GNN可解释方法存在权衡：基于梯度的方法计算高效但忽略结构交互，而博弈论方法能捕捉交互但计算开销大且可能偏离模型真实推理路径。需要一种能兼顾效率和准确性的方法。

Method: 提出FSX混合框架：1) 通过流敏感度分析识别关键消息流（单次前向传播中模拟局部节点扰动）；2) 将敏感度排名的流投影到输入图定义紧凑子图；3) 在子图中进行流感知合作博弈，通过类似Shapley值的指标公平评估节点贡献。

Result: 在多个数据集和GNN架构上的广泛评估表明，FSX在显著减少运行时间的同时实现了优越的解释保真度，并能提供前所未有的结构逻辑洞察。

Conclusion: FSX成功解决了现有方法的权衡问题，通过结合内部消息流和外部图数据，提供了高效、准确且具有结构洞察力的GNN解释框架。

Abstract: Despite the widespread success of Graph Neural Networks (GNNs), understanding the reasons behind their specific predictions remains challenging. Existing explainability methods face a trade-off that gradient-based approaches are computationally efficient but often ignore structural interactions, while game-theoretic techniques capture interactions at the cost of high computational overhead and potential deviation from the model's true reasoning path. To address this gap, we propose FSX (Message Flow Sensitivity Enhanced Structural Explainer), a novel hybrid framework that synergistically combines the internal message flows of the model with a cooperative game approach applied to the external graph data. FSX first identifies critical message flows via a novel flow-sensitivity analysis: during a single forward pass, it simulates localized node perturbations and measures the resulting changes in message flow intensities. These sensitivity-ranked flows are then projected onto the input graph to define compact, semantically meaningful subgraphs. Within each subgraph, a flow-aware cooperative game is conducted, where node contributions are evaluated fairly through a Shapley-like value that incorporates both node-feature importance and their roles in sustaining or destabilizing the identified critical flows. Extensive evaluation across multiple datasets and GNN architectures demonstrates that FSX achieves superior explanation fidelity with significantly reduced runtime, while providing unprecedented insights into the structural logic underlying model predictions--specifically, how important sub-structures exert influence by governing the stability of key internal computational pathways.

</details>


### [45] [RefProtoFL: Communication-Efficient Federated Learning via External-Referenced Prototype Alignment](https://arxiv.org/abs/2601.14746)
*Hongyue Wu,Hangyu Li,Guodong Fan,Haoran Zhu,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: RefProtoFL：一种通信高效的联邦学习框架，通过外部参考原型对齐和自适应概率更新丢弃，在有限带宽和异构数据下实现更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘环境中面临通信带宽有限和客户端数据分布异构的挑战。现有的基于原型的FL方法虽然通过交换类特征原型而非完整模型参数来缓解通信问题，但在严重通信约束下仍存在泛化性能不佳的问题。

Method: 1. 将模型分解为私有骨干网络和轻量级共享适配器，仅对适配器参数进行联邦通信；2. 自适应概率更新丢弃（APUD）：基于幅度的Top-K稀疏化，仅传输最重要的适配器更新；3. 外部参考原型对齐（ERPA）：利用服务器持有的少量公共数据集构建外部参考原型作为共享语义锚点，对于公共数据覆盖的类别直接对齐到公共诱导原型，未覆盖类别则通过加权平均使用服务器聚合的全局参考原型。

Result: 在标准基准测试上的广泛实验表明，RefProtoFL相比最先进的基于原型的FL基线方法获得了更高的分类准确率。

Conclusion: RefProtoFL通过结合通信高效的APUD和解决表示不一致性的ERPA，在通信受限的异构联邦学习环境中实现了更好的性能平衡。

Abstract: Federated learning (FL) enables collaborative model training without sharing raw data in edge environments, but is constrained by limited communication bandwidth and heterogeneous client data distributions. Prototype-based FL mitigates this issue by exchanging class-wise feature prototypes instead of full model parameters; however, existing methods still suffer from suboptimal generalization under severe communication constraints. In this paper, we propose RefProtoFL, a communication-efficient FL framework that integrates External-Referenced Prototype Alignment (ERPA) for representation consistency with Adaptive Probabilistic Update Dropping (APUD) for communication efficiency. Specifically, we decompose the model into a private backbone and a lightweight shared adapter, and restrict federated communication to the adapter parameters only. To further reduce uplink cost, APUD performs magnitude-aware Top-K sparsification, transmitting only the most significant adapter updates for server-side aggregation. To address representation inconsistency across heterogeneous clients, ERPA leverages a small server-held public dataset to construct external reference prototypes that serve as shared semantic anchors. For classes covered by public data, clients directly align local representations to public-induced prototypes, whereas for uncovered classes, alignment relies on server-aggregated global reference prototypes via weighted averaging. Extensive experiments on standard benchmarks demonstrate that RefProtoFL attains higher classification accuracy than state-of-the-art prototype-based FL baselines.

</details>


### [46] [Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models](https://arxiv.org/abs/2601.14758)
*Injin Kong,Hyoungjoon Lee,Yohan Jo*

Main category: cs.LG

TL;DR: 该研究通过电路分析比较了自回归模型（ARMs）与其对应的掩码扩散模型（MDMs），发现扩散后训练不仅调整参数，还从根本上重组了内部计算机制，支持非顺序的全局规划。


<details>
  <summary>Details</summary>
Motivation: 将预训练的自回归模型（ARMs）后训练成掩码扩散模型（MDMs）是一种经济高效的策略，但尚不清楚这种范式转变是否真正赋予了双向推理能力，还是仅仅重新包装了自回归启发式方法。需要探究这种后训练引发的内部算法转变。

Method: 对ARMs及其MDM对应模型进行对比电路分析，研究它们在结构和语义层面的机制变化。分析任务的结构性质如何影响机制转变。

Result: 发现系统性的"机制转变"：1）结构上，对于局部因果依赖任务，MDMs基本保留自回归电路；对于全局规划任务，MDMs放弃初始化路径，表现出早期层处理增强的重新布线。2）语义上，从ARMs的尖锐局部专业化转变为MDMs的分布式集成。

Conclusion: 扩散后训练不仅仅是调整模型参数，而是从根本上重组内部计算，以支持非顺序的全局规划能力，表明MDMs获得了真正的双向推理能力而非简单重新包装。

Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic "mechanism shift" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.

</details>


### [47] [Anytime Optimal Decision Tree Learning with Continuous Features](https://arxiv.org/abs/2601.14765)
*Harold Kiossou,Pierre Schaus,Siegfried Nijssen*

Main category: cs.LG

TL;DR: 提出一种基于有限差异搜索的随时完整算法，用于学习具有连续特征的最优决策树，改善了现有深度优先搜索方法的随时性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习连续特征最优决策树的深度优先搜索方法虽然能找到最优解，但计算时间随深度急剧增加，且中断时找到的树往往高度不平衡且次优，导致随时性能差。

Method: 采用有限差异搜索策略，将计算努力更均匀地分配到整个树结构中，而不是像深度优先搜索那样完全优化左子树后再探索右子树。

Result: 实验结果表明，该方法在随时性能方面优于现有方法，能够在任何中断点提供高质量的决策树。

Conclusion: 提出的基于有限差异搜索的随时完整方法有效解决了现有最优决策树学习算法在连续特征上的随时性能问题，提供了更好的中断时解决方案质量。

Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.

</details>


### [48] [Robustness of Mixtures of Experts to Feature Noise](https://arxiv.org/abs/2601.14792)
*Dong Sun,Rahul Nittala,Rebekka Burkholz*

Main category: cs.LG

TL;DR: MoE模型通过稀疏专家激活作为噪声过滤器，在特征噪声下比密集网络有更好的泛化误差、鲁棒性和收敛速度


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型在实践中很成功，但除了参数规模扩展外，为什么它们能超越密集网络仍不清楚。本文研究在输入具有潜在模块化结构但受到特征噪声污染的情况下，MoE的优势机制

Method: 在等参数设置下，研究输入具有潜在模块化结构但受特征噪声污染的情况。理论分析MoE的稀疏专家激活如何作为噪声过滤器，并与密集估计器进行比较。在合成数据和真实语言任务上进行实证验证

Result: 相比密集估计器，MoE在特征噪声下获得更低的泛化误差、更好的扰动鲁棒性和更快的收敛速度。合成数据和真实语言任务的实证结果证实了理论洞察

Conclusion: 稀疏模块化计算通过专家激活的噪声过滤机制，为MoE模型提供了超越密集网络的鲁棒性和效率优势，这解释了MoE在参数规模之外的成功原因

Abstract: Despite their practical success, it remains unclear why Mixture of Experts (MoE) models can outperform dense networks beyond sheer parameter scaling. We study an iso-parameter regime where inputs exhibit latent modular structure but are corrupted by feature noise, a proxy for noisy internal activations. We show that sparse expert activation acts as a noise filter: compared to a dense estimator, MoEs achieve lower generalization error under feature noise, improved robustness to perturbations, and faster convergence speed. Empirical results on synthetic data and real-world language tasks corroborate the theoretical insights, demonstrating consistent robustness and efficiency gains from sparse modular computation.

</details>


### [49] [Reflecting in the Reflection: Integrating a Socratic Questioning Framework into Automated AI-Based Question Generation](https://arxiv.org/abs/2601.14798)
*Ondřej Holub,Essi Ryymin,Rodrigo Alves*

Main category: cs.LG

TL;DR: 本文提出一个"反思中的反思"框架，使用双智能体对话（学生-教师和教师-教育者）迭代生成反思问题，在中学ICT教学中评估显示优于单次生成基线。


<details>
  <summary>Details</summary>
Motivation: 设计高质量的反思问题对教学很重要，但耗时且教师支持不均衡。需要自动化工具来帮助教师生成有效的反思问题。

Method: 采用反思中的反思框架，协调两个角色专门化的智能体：学生-教师（提出候选问题）和教师-教育者（评估问题质量），通过苏格拉底式多轮对话迭代精炼问题。使用GPT-4o-mini作为骨干模型，GPT-4级模型作为外部评估器。

Result: 动态停止机制结合上下文信息（学生水平和教学材料）优于固定5或10步迭代；双智能体协议生成的问题在相关性、深度和整体质量上显著优于单次生成基线。

Conclusion: 反思中的反思框架能有效生成高质量的反思问题，动态停止和上下文信息是关键因素，为教师提供了实用的自动化支持工具。

Abstract: Designing good reflection questions is pedagogically important but time-consuming and unevenly supported across teachers. This paper introduces a reflection-in-reflection framework for automated generation of reflection questions with large language models (LLMs). Our approach coordinates two role-specialized agents, a Student-Teacher and a Teacher-Educator, that engage in a Socratic multi-turn dialogue to iteratively refine a single question given a teacher-specified topic, key concepts, student level, and optional instructional materials. The Student-Teacher proposes candidate questions with brief rationales, while the Teacher-Educator evaluates them along clarity, depth, relevance, engagement, and conceptual interconnections, responding only with targeted coaching questions or a fixed signal to stop the dialogue. We evaluate the framework in an authentic lower-secondary ICT setting on the topic, using GPT-4o-mini as the backbone model and a stronger GPT- 4-class LLM as an external evaluator in pairwise comparisons of clarity, relevance, depth, and overall quality. First, we study how interaction design and context (dynamic vs.fixed iteration counts; presence or absence of student level and materials) affect question quality. Dynamic stopping combined with contextual information consistently outperforms fixed 5- or 10-step refinement, with very long dialogues prone to drift or over-complication. Second, we show that our two-agent protocol produces questions that are judged substantially more relevant and deeper, and better overall, than a one-shot baseline using the same backbone model.

</details>


### [50] [Statistical Learning Theory for Distributional Classification](https://arxiv.org/abs/2601.14818)
*Christian Fiedler*

Main category: cs.LG

TL;DR: 该论文研究监督学习中分布输入的核方法理论分析，特别关注两阶段采样设置下的SVM分类问题，建立了新的oracle不等式、一致性结果和学习率分析。


<details>
  <summary>Details</summary>
Motivation: 在基于学习的医学筛查或因果学习等应用中，输入是概率分布，但在学习阶段只能获得分布的样本而非分布本身。现有方法使用核均值嵌入将分布嵌入希尔伯特空间，然后应用SVM等核方法，但缺乏充分的理论分析。

Method: 采用核均值嵌入将分布或样本嵌入希尔伯特空间，然后应用支持向量机进行分类。建立了新的理论分析框架，包括oracle不等式、一致性证明和学习率推导。特别针对高斯核和铰链损失，提出了新的噪声假设变体。

Result: 建立了新的oracle不等式，证明了方法的收敛一致性，推导了学习率。针对高斯核和铰链损失的SVM，在新噪声假设下获得了具体的学习率。还开发了高斯核在希尔伯特空间上的新特征空间表示等独立有用的技术工具。

Conclusion: 该工作为分布输入的两阶段采样监督学习提供了系统的理论分析框架，特别在SVM分类方面取得了理论进展。提出的技术工具如高斯核在希尔伯特空间的新特征空间表示具有独立价值，为后续研究奠定了基础。

Abstract: In supervised learning with distributional inputs in the two-stage sampling setup, relevant to applications like learning-based medical screening or causal learning, the inputs (which are probability distributions) are not accessible in the learning phase, but only samples thereof. This problem is particularly amenable to kernel-based learning methods, where the distributions or samples are first embedded into a Hilbert space, often using kernel mean embeddings (KMEs), and then a standard kernel method like Support Vector Machines (SVMs) is applied, using a kernel defined on the embedding Hilbert space. In this work, we contribute to the theoretical analysis of this latter approach, with a particular focus on classification with distributional inputs using SVMs. We establish a new oracle inequality and derive consistency and learning rate results. Furthermore, for SVMs using the hinge loss and Gaussian kernels, we formulate a novel variant of an established noise assumption from the binary classification literature, under which we can establish learning rates. Finally, some of our technical tools like a new feature space for Gaussian kernels on Hilbert spaces are of independent interest.

</details>


### [51] [From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps](https://arxiv.org/abs/2601.14848)
*Mohamed Abouras,Catherine M. Elias*

Main category: cs.LG

TL;DR: 该论文研究高速公路匝道区域的车辆行为预测，使用多层LSTM架构在ExiD无人机数据集上训练模型，在4秒预测范围内取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 高速公路匝道区域（上下匝道）是研究不足的路段，这些区域引入了更高水平的交通交互变化。预测这些区域的车辆行为可以减少不确定性影响并提高道路安全性。

Method: 使用多层LSTM架构在ExiD无人机数据集上训练匝道区域模型，比较了匝道区域与直线高速公路路段的差异，测试了不同的预测时间范围和模型工作流程。

Result: 在4秒预测范围内取得了良好效果：匝道区域预测准确率从约76%开始，一般高速公路场景在最大预测范围内达到94%的准确率。

Conclusion: 研究结果表明多层LSTM架构在高速公路匝道区域车辆行为预测方面具有良好潜力，特别是在4秒预测范围内，这有助于提高道路安全和减少交通不确定性。

Abstract: On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.

</details>


### [52] [Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference](https://arxiv.org/abs/2601.14855)
*Baojun Che,Yifan Chen,Daniel Zhengyu Huang,Xinying Mao,Weijie Wang*

Main category: cs.LG

TL;DR: 提出一种结合仿射不变预处理、指数积分器和自适应时间步长的稳定高效黑盒变分推断框架，用于高斯混合族逼近复杂后验分布。


<details>
  <summary>Details</summary>
Motivation: 标准数值优化方法在黑盒变分推断中经常存在不稳定和低效的问题，特别是当使用高斯混合族逼近复杂后验分布时，需要更稳定高效的优化框架。

Method: 结合三个关键组件：1) 通过自然梯度公式实现仿射不变预处理；2) 无条件保持协方差矩阵正定性的指数积分器；3) 确保稳定性并适应不同阶段的自适应时间步长。

Result: 对于高斯后验，证明了在无噪声情况下的指数收敛性和蒙特卡洛估计下的几乎必然收敛性。数值实验在多种分布、Neal的多尺度漏斗和基于PDE的贝叶斯反问题中验证了方法的有效性。

Conclusion: 提出的框架为黑盒变分推断提供了一个稳定高效的优化方法，具有与流形优化和镜像下降的自然联系，并通过理论分析和实验验证了其优越性。

Abstract: Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.

</details>


### [53] [Strategic Doctrine Language Models (sdLM): A Learning-System Framework for Doctrinal Consistency and Geopolitical Forecasting](https://arxiv.org/abs/2601.14862)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Derya Umut Kulali*

Main category: cs.LG

TL;DR: sdLM是一个用于多文档战略推理的学习系统框架，通过结合多文档注意力、时间编码和教义一致性层，在长期预测和计划合理性方面表现优于通用LLM基线，并与人类专家在长期判断上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有通用LLM在多文档战略推理中缺乏教义一致性约束和校准不确定性，导致长期预测不准确、计划不合理，以及严重的教义违反问题。

Method: 结合多文档注意力机制、时间编码和教义一致性层，构建Strategic Doctrine Language Models (sdLM)框架，确保战略推理的教义一致性和不确定性校准。

Result: 在三个基准测试中表现优异：(1)专家小组对战略场景评分(N=47)；(2)336份教义出版物(12,847条声明)的教义一致性；(3)127个历史反事实(1945-2020)的12-60个月地缘政治预测。sdLM在战略质量和校准方面优于通用LLM基线，在长期判断上与人类专家竞争力相当。

Conclusion: sdLM框架通过教义一致性约束和校准不确定性，显著提升了多文档战略推理的质量，为长期战略预测和计划评估提供了有效的工具，在操作环境中具有良好的性能/延迟特性。

Abstract: We introduce Strategic Doctrine Language Models (sdLM), a learning-system framework for multi-document strategic reasoning with doctrinal consistency constraints and calibrated uncertainty. The approach combines multi-document attention, temporal encoding, and a doctrine-consistency layer to improve long-horizon forecasting and plan plausibility while reducing severe doctrinal violations. We evaluate sdLM using (i) expert-panel scoring of strategic scenarios (N=47), (ii) doctrine consistency on 336 doctrine publications (12,847 statements), and (iii) geopolitical forecasting on 127 historical counterfactuals (1945-2020) across 12-60 month horizons. Across these benchmarks, sdLM achieves higher strategic quality and better calibration than strong general-purpose LLM baselines, and remains competitive with human experts on long-horizon judgments. We further report ablations, scaling trends, and deployment-oriented performance/latency characteristics to clarify which components drive improvements and how they translate to operational settings.

</details>


### [54] [What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study](https://arxiv.org/abs/2601.14888)
*Keyu Lv,Manyi Zhang,Xiaobo Xia,Jingchen Ni,Shannan Yan,Xianzhi Yu,Lu Hou,Chun Yuan,Haoli Bai*

Main category: cs.LG

TL;DR: 本文系统研究了推理模型的量化感知训练(QAT)，提出了一种优化的推理-QAT工作流程，在低比特设置下显著提升推理模型的量化性能。


<details>
  <summary>Details</summary>
Motivation: 推理模型在复杂任务上表现出色，但其推理速度慢且token效率低。后训练量化(PTQ)通常会导致精度大幅下降，尤其是在低比特设置下的推理任务中，因此需要研究更有效的量化方法。

Method: 通过系统实证研究量化感知训练(QAT)在推理模型中的应用，包括：使用知识蒸馏作为监督微调或强化学习的鲁棒目标；利用PTQ作为QAT的强初始化；探索强化学习在量化模型中的可行性；对齐PTQ校准域与QAT训练域。最终整合这些发现形成优化的推理-QAT工作流程。

Result: 推理-QAT在多个LLM骨干网络和推理数据集上持续优于最先进的PTQ方法。例如，在Qwen3-0.6B上，比GPTQ在MATH-500上提升了44.53%，并在2比特设置下持续恢复性能。

Conclusion: 量化感知训练是提升推理模型量化效率的有效方法，通过系统优化的工作流程可以在低比特设置下显著提升性能，为高效推理模型的部署提供了实用解决方案。

Abstract: Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.

</details>


### [55] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的个性化血糖预测方法，通过利用患者特定数据提高预测准确性，相比传统通用模型能更好地处理个体差异，为糖尿病管理提供更精准的决策支持。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴血糖监测设备和移动健康应用的普及，准确的血糖预测对于增强自动化胰岛素输送和决策支持系统至关重要。传统通用模型无法充分处理个体差异，需要开发能够适应患者特定特征的个性化预测方法。

Method: 采用深度学习框架进行个性化血糖预测，比较留一受试者交叉验证与微调策略来评估患者特定动态建模能力。使用多模态患者特定数据（与传统仅使用CGM数据的方法对比），并进行消融研究以确定有效个性化所需的最小训练数据量。

Result: 个性化模型显著改善不良事件的预测能力，能够实现更精准和及时的干预。多模态患者特定方法优于传统CGM-only方法，消融研究确定了有效个性化所需的最小数据量，这对于实际应用中数据收集受限的情况具有重要意义。

Conclusion: 自适应、个性化的血糖预测模型在推进下一代糖尿病管理方面具有巨大潜力，特别是在可穿戴和移动健康平台中，能够增强面向消费者的糖尿病护理解决方案。该方法为实际应用提供了可行的个性化策略，即使在数据有限的情况下也能实现有效预测。

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [56] [Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning](https://arxiv.org/abs/2601.14942)
*Hang Zhao,Hongru Li,Dongfang Xu,Shenghui Song,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出三阶段通信感知分布式学习框架，用于多模态边缘推理，通过自监督学习、证据融合和不确定性引导反馈，在减少通信开销的同时保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态边缘推理面临两大挑战：1) 多模态特性导致带宽受限无线链路上的通信开销巨大；2) 信道变化和噪声多模态输入下的鲁棒性有限。需要一种既能提高训练推理效率又能保持无线信道鲁棒性的解决方案。

Method: 三阶段框架：阶段I - 本地多模态自监督学习，无需设备-服务器交换，获得共享和模态特定编码器；阶段II - 分布式微调与集中式证据融合，校准每个模态的不确定性并可靠聚合受噪声或信道衰落影响的特征；阶段III - 不确定性引导反馈机制，为不确定样本选择性请求额外特征，优化通信-准确率权衡。

Result: 在RGB-深度室内场景分类实验中，该框架以更少的训练通信轮次获得更高准确率，对模态退化或信道变化保持鲁棒性，优于现有的自监督和全监督基线方法。

Conclusion: 提出的三阶段通信感知分布式学习框架有效解决了多模态边缘推理中的通信效率和鲁棒性问题，通过自监督学习减少通信开销，证据融合提高鲁棒性，不确定性反馈优化通信-准确率权衡，为分布式边缘智能提供了实用解决方案。

Abstract: Semantic communication is emerging as a key enabler for distributed edge intelligence due to its capability to convey task-relevant meaning. However, achieving communication-efficient training and robust inference over wireless links remains challenging. This challenge is further exacerbated for multi-modal edge inference (MMEI) by two factors: 1) prohibitive communication overhead for distributed learning over bandwidth-limited wireless links, due to the \emph{multi-modal} nature of the system; and 2) limited robustness under varying channels and noisy multi-modal inputs. In this paper, we propose a three-stage communication-aware distributed learning framework to improve training and inference efficiency while maintaining robustness over wireless channels. In Stage~I, devices perform local multi-modal self-supervised learning to obtain shared and modality-specific encoders without device--server exchange, thereby reducing the communication cost. In Stage~II, distributed fine-tuning with centralized evidential fusion calibrates per-modality uncertainty and reliably aggregates features distorted by noise or channel fading. In Stage~III, an uncertainty-guided feedback mechanism selectively requests additional features for uncertain samples, optimizing the communication--accuracy tradeoff in the distributed setting. Experiments on RGB--depth indoor scene classification show that the proposed framework attains higher accuracy with far fewer training communication rounds and remains robust to modality degradation or channel variation, outperforming existing self-supervised and fully supervised baselines.

</details>


### [57] [Multimodal Rumor Detection Enhanced by External Evidence and Forgery Features](https://arxiv.org/abs/2601.14954)
*Han Li,Hua Sun*

Main category: cs.LG

TL;DR: 提出一种结合外部证据和伪造特征的多模态谣言检测模型，通过双对比学习和门控自适应特征缩放融合机制，在微博和Twitter数据集上优于主流基线方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中图文混合帖子传播信息，但谣言常利用细微不一致和伪造内容，仅基于帖子内容检测困难。深度语义不匹配谣言（图像和文本表面一致）尤其具有挑战性，威胁网络舆论。现有多模态谣言检测方法存在特征提取有限、噪声对齐、融合策略不灵活等问题，且忽略验证复杂谣言所需的外部事实证据。

Method: 1) 使用ResNet34视觉编码器和BERT文本编码器；2) 伪造特征模块通过傅里叶变换提取频域痕迹和压缩伪影；3) 使用BLIP生成图像描述桥接图像和文本语义空间；4) 双对比学习模块计算文本-图像和文本-描述对的对比损失，改善语义不一致检测；5) 门控自适应特征缩放融合机制动态调整多模态融合并减少冗余。

Result: 在微博和Twitter数据集上的实验表明，该模型在宏观准确率、召回率和F1分数上优于主流基线方法。

Conclusion: 提出的结合外部证据和伪造特征的多模态谣言检测模型能有效检测深度语义不匹配谣言，通过双对比学习和自适应融合机制提升了检测性能。

Abstract: Social media increasingly disseminates information through mixed image text posts, but rumors often exploit subtle inconsistencies and forged content, making detection based solely on post content difficult. Deep semantic mismatch rumors, which superficially align images and texts, pose particular challenges and threaten online public opinion. Existing multimodal rumor detection methods improve cross modal modeling but suffer from limited feature extraction, noisy alignment, and inflexible fusion strategies, while ignoring external factual evidence necessary for verifying complex rumors. To address these limitations, we propose a multimodal rumor detection model enhanced with external evidence and forgery features. The model uses a ResNet34 visual encoder, a BERT text encoder, and a forgery feature module extracting frequency-domain traces and compression artifacts via Fourier transformation. BLIP-generated image descriptions bridge image and text semantic spaces. A dual contrastive learning module computes contrastive losses between text image and text description pairs, improving detection of semantic inconsistencies. A gated adaptive feature-scaling fusion mechanism dynamically adjusts multimodal fusion and reduces redundancy. Experiments on Weibo and Twitter datasets demonstrate that our model outperforms mainstream baselines in macro accuracy, recall, and F1 score.

</details>


### [58] [Improving Regret Approximation for Unsupervised Dynamic Environment Generation](https://arxiv.org/abs/2601.14957)
*Harry Mead,Bruno Lacerda,Jakob Foerster,Nick Hawes*

Main category: cs.LG

TL;DR: 本文提出DEGen方法改善无监督环境设计，通过动态环境生成提供更密集的奖励信号，并引入MNA作为更好的遗憾近似度量，显著提升了RL代理的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前无监督环境设计方法面临两个主要问题：1）在环境参数空间中，少数参数配置会导致策略复杂度急剧增加，现有方法难以识别这些挑战性环境；2）随着环境规模增大，信用分配问题变得更加困难，现有遗憾近似方法效果有限。

Method: 提出DEGen（动态环境生成）方法，通过更密集的生成器奖励信号缓解信用分配问题；同时引入MNA（最大化负优势）作为新的遗憾近似度量，能更准确地识别挑战性环境配置。

Result: 实验表明：1）MNA在识别挑战性环境方面优于现有遗憾近似方法；2）DEGen与MNA结合后，特别是在大规模环境中，性能显著超越现有方法。

Conclusion: DEGen和MNA的组合有效解决了UED中的信用分配和挑战性环境识别问题，使无监督环境设计能够扩展到更大规模的环境，提升了RL代理的泛化能力。

Abstract: Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

</details>


### [59] [InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement](https://arxiv.org/abs/2601.14968)
*Mingyue Cheng,Xiaoyu Tao,Huajian Zhang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: InstructTime++：将时间序列分类重构为多模态生成任务，通过离散化时间序列、跨模态对齐和隐式特征建模，利用语言模型生成文本标签，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法采用判别式范式，直接将输入序列映射到one-hot编码的类别标签。这种方法难以融入上下文特征，也无法捕捉类别间的语义关系。

Method: 1. 将时间序列分类重构为多模态生成任务：连续数值序列、上下文文本特征和任务指令作为多模态输入，类别标签作为语言模型生成的文本输出
2. 引入时间序列离散化模块：将连续序列转换为离散时间标记
3. 设计对齐投影层和生成式自监督预训练策略：增强跨模态表示对齐
4. InstructTime++扩展：加入隐式特征建模，使用统计特征提取和视觉语言图像描述等工具包挖掘原始时间序列中的信息模式，转换为文本描述进行集成

Result: 在多个基准数据集上的广泛实验表明，InstructTime++表现出优越的性能。

Conclusion: 通过将时间序列分类重构为生成式多模态任务，并引入隐式特征建模，InstructTime++能够更好地利用上下文信息和类别语义关系，显著提升了时间序列分类的效果。

Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.

</details>


### [60] [Fine-Grained Traceability for Transparent ML Pipelines](https://arxiv.org/abs/2601.14971)
*Liping Chen,Mujie Liu,Haytham Fayek*

Main category: cs.LG

TL;DR: FG-Trac是一个模型无关的框架，为机器学习流水线建立可验证的细粒度样本级可追溯性，通过加密承诺记录样本生命周期事件，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统多为多阶段流水线，但现有透明度机制仅停留在模型层面，缺乏样本级的可追溯性，无法验证特定样本是否被使用、何时处理、记录是否完整。

Method: FG-Trac定义明确的机制来捕获和验证预处理和训练中的样本生命周期事件，基于训练检查点计算贡献分数，并通过防篡改加密承诺锚定这些追踪记录。框架无需修改模型架构或训练目标。

Result: 在卷积神经网络和多模态图学习流水线上的实验表明，FG-Trac在保持预测性能的同时，使机器学习系统能够提供关于单个样本在模型执行过程中如何被使用和传播的可验证证据。

Conclusion: FG-Trac填补了机器学习流水线中样本级可追溯性的空白，为实践者和用户提供了可验证的数据使用历史，具有实际的计算开销，增强了机器学习系统的透明度和可信度。

Abstract: Modern machine learning systems are increasingly realised as multistage pipelines, yet existing transparency mechanisms typically operate at a model level: they describe what a system is and why it behaves as it does, but not how individual data samples are operationally recorded, tracked, and verified as they traverse the pipeline. This absence of verifiable, sample-level traceability leaves practitioners and users unable to determine whether a specific sample was used, when it was processed, or whether the corresponding records remain intact over time. We introduce FG-Trac, a model-agnostic framework that establishes verifiable, fine-grained sample-level traceability throughout machine learning pipelines. FG-Trac defines an explicit mechanism for capturing and verifying sample lifecycle events across preprocessing and training, computes contribution scores explicitly grounded in training checkpoints, and anchors these traces to tamper-evident cryptographic commitments. The framework integrates without modifying model architectures or training objectives, reconstructing complete and auditable data-usage histories with practical computational overhead. Experiments on a canonical convolutional neural network and a multimodal graph learning pipeline demonstrate that FG-Trac preserves predictive performance while enabling machine learning systems to furnish verifiable evidence of how individual samples were used and propagated during model execution.

</details>


### [61] [Lineup Regularized Adjusted Plus-Minus (L-RAPM): Basketball Lineup Ratings with Informed Priors](https://arxiv.org/abs/2601.15000)
*Christos Petridis,Konstantinos Pelechrinis*

Main category: cs.LG

TL;DR: 提出L-RAPM回归方法，通过控制对手阵容并利用球员信息，解决篮球阵容数据稀疏导致的统计噪声问题，提高小样本阵容表现预测能力。


<details>
  <summary>Details</summary>
Motivation: 篮球等运动中识别表现良好的阵容组合是体育分析的重要任务，但频繁换人导致数据高度稀疏（NBA球队每赛季使用600多个阵容，每个阵容平均仅25-30次进攻回合），现有统计数据噪声大、预测价值低，且目前没有公开研究解决此问题。

Method: 提出基于回归的方法L-RAPM，该方法控制每个阵容面对的对手阵容影响，同时利用组成阵容的球员信息进行建模。

Result: 实验表明L-RAPM比当前使用的基线方法具有更好的预测能力，且随着阵容样本量变小，改进效果更加明显。

Conclusion: L-RAPM方法能有效解决阵容数据稀疏问题，提高阵容表现预测的准确性，特别是在小样本情况下表现更优。

Abstract: Identifying combinations of players (that is, lineups) in basketball - and other sports - that perform well when they play together is one of the most important tasks in sports analytics. One of the main challenges associated with this task is the frequent substitutions that occur during a game, which results in highly sparse data. In particular, a National Basketball Association (NBA) team will use more than 600 lineups during a season, which translates to an average lineup having seen the court in approximately 25-30 possessions. Inevitably, any statistics that one collects for these lineups are going to be noisy, with low predictive value. Yet, there is no existing work (in the public at least) that addresses this problem. In this work, we propose a regression-based approach that controls for the opposition faced by each lineup, while it also utilizes information about the players making up the lineups. Our experiments show that L-RAPM provides improved predictive power than the currently used baseline, and this improvement increases as the sample size for the lineups gets smaller.

</details>


### [62] [Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control](https://arxiv.org/abs/2601.15015)
*Jannis Becktepe,Aleksandra Franz,Nils Thuerey,Sebastian Peitz*

Main category: cs.LG

TL;DR: FluidGym：首个独立、完全可微的强化学习主动流控制基准套件，基于PyTorch和GPU加速的PICT求解器，无需外部CFD软件，提供标准化评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在主动流控制中的研究难以评估进展，因为依赖异构的观测和执行方案、数值设置和评估协议。当前基准依赖外部CFD求解器、不完全可微、且3D和多智能体支持有限。

Method: 基于PyTorch和GPU加速的PICT求解器构建完全独立的基准套件，在单一Python栈中运行，无需外部CFD软件，提供标准化评估协议，支持3D和多智能体场景。

Result: 提供了PPO和SAC的基线结果，发布了所有环境、数据集和训练模型作为公共资源，建立了可扩展的学习型流控制研究基础。

Conclusion: FluidGym解决了现有基准的局限性，实现了控制方法的系统比较，为学习型流控制的未来研究提供了可扩展的基础设施。

Abstract: Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.

</details>


### [63] [Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization](https://arxiv.org/abs/2601.15021)
*Adam Rokah,Daniel Veress,Caleb Caulk,Sourav Sharan*

Main category: cs.LG

TL;DR: 该研究在图像分类任务中比较了Dense、SoftMoE和SparseMoE三种架构的性能，发现MoE变体在CIFAR10上略优于密集基线，但条件路由在小型模型上未实现推理加速。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在图像分类中的行为，而非传统的大语言模型扩展场景，重点关注预测性能、专家利用率和泛化特性。

Method: 在CIFAR10数据集上，在可比模型容量下比较Dense、SoftMoE和SparseMoE分类头；使用正则化防止专家崩溃；通过Hessian矩阵的谱分析和迹评估锐度；进行损失曲面扰动分析。

Result: 两种MoE变体验证准确率略高于密集基线，专家利用率保持平衡；SoftMoE显示更高的锐度指标，但所有模型泛化性能相当；条件路由在当前硬件规模下未实现推理加速。

Conclusion: MoE在图像分类中能获得轻微性能提升，但理论效率与实际硬件实现存在差距；损失曲面分析揭示了密集和MoE模型之间的定性差异，有助于解释曲率测量结果。

Abstract: Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive performance, expert utilization, and generalization. We compare dense, SoftMoE, and SparseMoE classifier heads on the CIFAR10 dataset under comparable model capacity. Both MoE variants achieve slightly higher validation accuracy than the dense baseline while maintaining balanced expert utilization through regularization, avoiding expert collapse. To analyze generalization, we compute Hessian-based sharpness metrics at convergence, including the largest eigenvalue and trace of the loss Hessian, evaluated on both training and test data. We find that SoftMoE exhibits higher sharpness by these metrics, while Dense and SparseMoE lie in a similar curvature regime, despite all models achieving comparable generalization performance. Complementary loss surface perturbation analyses reveal qualitative differences in non-local behavior under finite parameter perturbations between dense and MoE models, which help contextualize curvature-based measurements without directly explaining validation accuracy. We further evaluate empirical inference efficiency and show that naively implemented conditional routing does not yield inference speedups on modern hardware at this scale, highlighting the gap between theoretical and realized efficiency in sparse MoE models.

</details>


### [64] [Factorizable joint shift revisited](https://arxiv.org/abs/2601.15036)
*Dirk Tasche*

Main category: cs.LG

TL;DR: 论文提出了一个分析一般标签空间分布偏移的框架，将因子化联合偏移（FJS）推广到分类和回归任务，并扩展了EM算法用于类别先验概率估计。


<details>
  <summary>Details</summary>
Motivation: 现有因子化联合偏移（FJS）研究仅限于分类标签空间，需要扩展到一般标签空间以覆盖分类和回归模型，并重新审视广义标签偏移（GLS）。

Method: 提出一个分析一般标签空间分布偏移的理论框架，将FJS结果推广到一般标签空间，扩展EM算法用于类别先验概率估计，并重新分析广义标签偏移。

Result: 建立了适用于分类和回归任务的一般标签空间分布偏移分析框架，成功推广了FJS理论结果，并提出了扩展的EM算法。

Conclusion: 该框架为一般标签空间的分布偏移分析提供了统一的理论基础，扩展了现有FJS和GLS理论，为分类和回归模型的分布偏移处理提供了新工具。

Abstract: Factorizable joint shift (FJS) was proposed as a type of distribution shift (or dataset shift) that comprises both covariate and label shift. Recently, it has been observed that FJS actually arises from consecutive label and covariate (or vice versa) shifts. Research into FJS so far has been confined to the case of categorical label spaces. We propose a framework for analysing distribution shift in the case of general label spaces, thus covering both classification and regression models. Based on the framework, we generalise existing results on FJS to general label spaces and propose a related extension of the expectation maximisation (EM) algorithm for class prior probabilities. We also take a fresh look at generalized label shift (GLS) in the case of general label spaces.

</details>


### [65] [A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2601.15038)
*Mertcan Daysalilar,Fuat Uyguroglu,Gabriel Nicolosi,Adam Meyers*

Main category: cs.LG

TL;DR: 提出基于课程学习的深度强化学习框架CB-DRL，通过三阶段渐进式训练解决电动汽车路径规划问题的训练不稳定问题，在小规模实例训练后能泛化到大规模未见实例。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在解决带时间窗的电动汽车路径规划问题(EVRPTW)时面临训练不稳定问题，难以在约束密集的情况下收敛和泛化，需要更稳定的学习框架。

Method: 采用三阶段课程学习：阶段A学习距离和车队优化，阶段B加入电池管理，阶段C学习完整EVRPTW问题。使用改进的近端策略优化算法，结合异构图注意力编码器，采用全局-局部注意力和特征线性调制技术。

Result: 仅在N=10的小规模实例上训练，模型能泛化到N=5到N=100的未见实例，在中规模问题上显著优于基线方法，在分布外实例上实现高可行率和竞争性解质量。

Conclusion: 课程学习引导的深度强化学习框架能有效解决EVRPTW问题的训练不稳定问题，在神经网络的快速求解和操作可靠性之间架起桥梁，实现从简单到复杂问题的稳定学习和泛化。

Abstract: The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.

</details>


### [66] [HyperNet-Adaptation for Diffusion-Based Test Case Generation](https://arxiv.org/abs/2601.15041)
*Oliver Weißl,Vincenzo Riccio,Severin Kacianka,Andrea Stocco*

Main category: cs.LG

TL;DR: HyNeA是一种基于扩散模型的生成式测试方法，通过超网络实现无数据集可控性，能够高效生成真实故障案例，用于深度学习系统的可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击主要评估鲁棒性而非功能行为，生成式测试方法受限于简单数据集或有限输入域。扩散模型虽然能合成高保真图像，但计算成本高且可控性有限，难以应用于大规模测试。

Method: HyNeA使用超网络提供无数据集可控性，通过独特的训练策略支持实例级调优，无需依赖包含类似故障示例的数据集，能够直接高效控制扩散生成过程。

Result: 实验结果表明，HyNeA相比现有生成式测试生成器提高了可控性和测试多样性，能够泛化到没有故障标签训练数据的领域，且计算成本显著低于基于搜索的方法。

Conclusion: HyNeA为深度学习系统的可靠性评估提供了一种高效可控的生成式测试方法，能够在没有故障标签数据的情况下生成真实故障案例，具有实际应用价值。

Abstract: The increasing deployment of deep learning systems requires systematic evaluation of their reliability in real-world scenarios. Traditional gradient-based adversarial attacks introduce small perturbations that rarely correspond to realistic failures and mainly assess robustness rather than functional behavior. Generative test generation methods offer an alternative but are often limited to simple datasets or constrained input domains. Although diffusion models enable high-fidelity image synthesis, their computational cost and limited controllability restrict their applicability to large-scale testing. We present HyNeA, a generative testing method that enables direct and efficient control over diffusion-based generation. HyNeA provides dataset-free controllability through hypernetworks, allowing targeted manipulation of the generative process without relying on architecture-specific conditioning mechanisms or dataset-driven adaptations such as fine-tuning. HyNeA employs a distinct training strategy that supports instance-level tuning to identify failure-inducing test cases without requiring datasets that explicitly contain examples of similar failures. This approach enables the targeted generation of realistic failure cases at substantially lower computational cost than search-based methods. Experimental results show that HyNeA improves controllability and test diversity compared to existing generative test generators and generalizes to domains where failure-labeled training data is unavailable.

</details>


### [67] [LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training](https://arxiv.org/abs/2601.15079)
*Chenyu Liu,Haige Li,Luca Rossi*

Main category: cs.LG

TL;DR: 提出LoRAP方法，通过低秩聚合提示优化GNN量化训练，在低比特量化下提升性能且计算开销小


<details>
  <summary>Details</summary>
Motivation: GNN量化能减少模型大小、加速推理，但现有方法在量化聚合操作上存在性能损失问题。与LLM量化不同，GNN量化更关注图特征的量化，需要优化量化聚合结果

Method: 提出低秩聚合提示（LoRAP），在量化感知训练中为每个聚合特征注入轻量级、输入相关的提示，优化量化聚合结果。相比仅提示节点特征的方法，LoRAP能更全面地优化量化聚合

Result: 在4个主流QAT框架和9个图数据集上的广泛评估表明，LoRAP能持续提升低比特量化GNN的性能，同时引入的计算开销极小

Conclusion: LoRAP是一种有效的GNN量化优化方法，通过低秩聚合提示机制解决了量化聚合的性能损失问题，为资源受限环境下的高效GNN部署提供了新思路

Abstract: Graph Neural Networks (GNNs) are neural networks that aim to process graph data, capturing the relationships and interactions between nodes using the message-passing mechanism. GNN quantization has emerged as a promising approach for reducing model size and accelerating inference in resource-constrained environments. Compared to quantization in LLMs, quantizing graph features is more emphasized in GNNs. Inspired by the above, we propose to leverage prompt learning, which manipulates the input data, to improve the performance of quantization-aware training (QAT) for GNNs. To mitigate the issue that prompting the node features alone can only make part of the quantized aggregation result optimal, we introduce Low-Rank Aggregation Prompting (LoRAP), which injects lightweight, input-dependent prompts into each aggregated feature to optimize the results of quantized aggregations. Extensive evaluations on 4 leading QAT frameworks over 9 graph datasets demonstrate that LoRAP consistently enhances the performance of low-bit quantized GNNs while introducing a minimal computational overhead.

</details>


### [68] [Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning](https://arxiv.org/abs/2601.15086)
*Oleg Shchendrigin,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 论文提出一个评估强化学习智能体记忆重写能力的基准测试，发现传统循环模型在记忆更新任务上优于现代结构化记忆和Transformer架构


<details>
  <summary>Details</summary>
Motivation: 现实世界决策需要记忆既稳定又适应性强，但现有RL基准和记忆增强智能体主要关注记忆保留，忽视了记忆重写这一同等重要的能力

Method: 引入一个在部分可观测性下测试持续记忆更新的基准，比较循环模型、Transformer架构和结构化记忆在记忆重写任务上的表现

Result: 经典循环模型在记忆重写任务上表现出更大的灵活性和鲁棒性，而结构化记忆仅在狭窄条件下成功，Transformer智能体在非平凡保留情况下经常失败

Conclusion: 当前方法存在根本性局限，需要平衡稳定保留和适应性更新的记忆机制，强调了设计具有明确可训练遗忘机制的未来RL智能体的必要性

Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/

</details>


### [69] [Field-Space Autoencoder for Scalable Climate Emulators](https://arxiv.org/abs/2601.15102)
*Johannes Meuer,Maximilian Witte,Étiénne Plésiat,Thomas Ludwig,Christopher Kadow*

Main category: cs.LG

TL;DR: 提出Field-Space Autoencoder，一种基于球面压缩模型的可扩展气候模拟框架，通过Field-Space Attention处理原生气候模型输出，避免几何失真，并支持零样本超分辨率，结合扩散模型学习多分辨率数据。


<details>
  <summary>Details</summary>
Motivation: 千米尺度地球系统模型计算成本高且输出数据量巨大（PB级），限制了其在概率风险评估等应用中的实用性，需要开发更高效的气候模拟框架。

Method: 提出Field-Space Autoencoder框架，采用球面压缩模型和Field-Space Attention直接在原生气候模型输出上操作，避免将球面数据强制映射到欧几里得网格造成的几何失真。该方法生成结构化压缩场，作为下游生成式模拟的良好基线，并支持零样本超分辨率。在此基础上训练生成扩散模型，同时从丰富的低分辨率数据学习内部变异性，从稀疏的高分辨率数据学习精细尺度物理。

Result: 该方法比卷积基线显著更好地保持物理结构，能够桥接低分辨率集合统计的高数据量和高分辨率物理细节的稀缺性之间的差距。

Conclusion: Field-Space Autoencoder提供了一个可扩展的气候模拟框架，通过球面压缩和注意力机制有效处理原生气候数据，支持多分辨率数据融合，为气候风险评估等应用提供了更实用的解决方案。

Abstract: Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencoder, a scalable climate emulation framework based on a spherical compression model that overcomes these challenges. By utilizing Field-Space Attention, the model efficiently operates on native climate model output and therefore avoids geometric distortions caused by forcing spherical data onto Euclidean grids. This approach preserves physical structures significantly better than convolutional baselines. By producing a structured compressed field, it serves as a good baseline for downstream generative emulation. In addition, the model can perform zero-shot super-resolution that maps low-resolution large ensembles and scarce high-resolution data into a shared representation. We train a generative diffusion model on these compressed fields. The model can simultaneously learn internal variability from abundant low-resolution data and fine-scale physics from sparse high-resolution data. Our work bridges the gap between the high volume of low-resolution ensemble statistics and the scarcity of high-resolution physical detail.

</details>


### [70] [Auditing Language Model Unlearning via Information Decomposition](https://arxiv.org/abs/2601.15111)
*Anmol Goel,Alan Ritter,Iryna Gurevych*

Main category: cs.LG

TL;DR: 当前机器学习遗忘方法存在关键局限：尽管遗忘算法表面成功，但被遗忘数据的信息仍能从内部表示中线性解码。论文引入基于部分信息分解的信息论框架来审计遗忘效果，发现冗余信息构成残留知识，并提出基于表示的风险评分来缓解隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 揭示当前语言模型机器学习遗忘方法的局限性：尽管遗忘算法看似成功，但被遗忘数据的信息仍能从模型内部表示中解码出来，存在隐私泄露风险。需要建立系统化的审计框架来评估遗忘效果。

Method: 引入基于部分信息分解（PID）的信息论框架，比较遗忘前后模型的内部表示，将与被遗忘数据的互信息分解为不同组件，形式化定义遗忘知识和残留知识的概念。提出基于表示的风险评分来指导推理时对敏感输入的弃权。

Result: 分析发现冗余信息（两个模型共享的部分）构成残留知识，在遗忘后仍然存在，并与已知对抗性重建攻击的易感性相关。提出的表示风险评分能够有效指导敏感输入的弃权决策。

Conclusion: 该工作提出了一个原则性的、表示层面的遗忘审计框架，为语言模型的安全部署提供了理论洞见和实用工具，能够缓解隐私泄露风险。

Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.

</details>


### [71] [Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.15124)
*Haonan Yuan,Qingyun Sun,Jiacheng Tao,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: RAG-GFM提出了一种检索增强的图基础模型，通过将知识从参数中卸载到外部存储，解决了现有图基础模型的内存瓶颈和知识压缩问题，在跨领域节点和图分类任务上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型(GFMs)存在内存瓶颈问题：它们试图将知识编码到模型参数中，这限制了语义容量，引入了严重的损失压缩和冲突，并且将图表示与知识纠缠在一起，阻碍了高效适应，影响了可扩展性和可解释性。

Method: 提出RAG-GFM，采用检索增强生成框架，将知识从参数中卸载。构建双模态统一检索模块：基于前缀结构文本的语义存储和基于中心性基元的结构存储。设计双视图对齐目标来保留异构信息，通过上下文增强在下游任务中利用检索到的文本和基元作为上下文证据。

Result: 在五个基准图数据集上的实验表明，RAG-GFM在跨领域节点和图分类任务上持续优于13个最先进的基线方法，实现了优越的有效性和效率。

Conclusion: RAG-GFM通过检索增强生成方法成功解决了图基础模型的内存瓶颈问题，通过外部化图知识实现了更好的可扩展性、适应性和性能表现。

Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.

</details>


### [72] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: CLEANER提出了一种利用LLM内在自校正能力的轨迹净化方法，通过相似性感知自适应回滚机制构建干净轨迹，解决了参数受限模型在强化学习中因执行失败导致的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 参数受限的LLM（4B-7B）在强化学习探索阶段频繁出现执行失败，产生噪声轨迹，导致信用分配问题——错误动作与成功结果同时被强化。现有方法面临困境：密集奖励易引发奖励黑客行为，而超采样则计算成本过高。

Method: CLEANER利用模型内在自校正能力，在数据收集过程中直接消除错误污染。核心是相似性感知自适应回滚（SAAR）机制，通过回顾性地用成功的自校正替换失败来构建干净的净化轨迹。基于语义相似性，SAAR自适应调节替换粒度，从浅层执行修复到深层推理替换。

Result: 在AIME24/25、GPQA和LiveCodeBench上的实验结果显示，相比基线平均准确率分别提升6%、3%和5%。特别值得注意的是，CLEANER仅使用三分之一训练步骤就达到了最先进性能，证明了轨迹净化作为高效智能体强化学习的可扩展解决方案。

Conclusion: CLEANER通过轨迹净化有效解决了参数受限LLM在强化学习中的信用分配问题，利用模型自校正能力构建干净轨迹，使模型内化正确推理模式而非错误恢复循环，实现了高效且可扩展的智能体强化学习。

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>


### [73] [Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data](https://arxiv.org/abs/2601.15158)
*Yuval Ran-Milo,Yotam Alexander,Shahar Mendel,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型在稀疏奖励强化学习下如何自发产生链式推理能力，通过理论分析和实验验证了简单样本在梯度下降中促使模型学习可泛化推理策略的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管基于结果的强化学习训练能让Transformer自发产生链式推理能力，但稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不清楚。研究者希望理解这一过程，特别是简单样本在促进可泛化推理策略学习中的作用。

Method: 采用理论分析和实验验证相结合的方法：1）在合成图遍历任务上分析单层Transformer的梯度流动力学；2）证明仅基于最终答案正确性训练时，梯度流会驱动模型收敛到结构化、可解释的迭代算法；3）识别分布特性要求，特别是"简单样本"（需要较少推理步骤的实例）的关键作用；4）在合成数据和真实语言模型的数学推理任务上进行实验验证。

Result: 研究发现：1）梯度流确实能驱动模型学习迭代遍历图的算法；2）训练分布中简单样本的质量对学习成功至关重要——当简单样本足够多时，模型能学习可泛化到更长链的遍历策略；3）当简单样本消失时，基于梯度的学习变得不可行；4）理论发现在实际语言模型的数学推理任务中得到验证。

Conclusion: 稀疏奖励强化学习能够促使Transformer自发产生链式推理能力，关键在于训练分布中简单样本的存在。这些简单样本为梯度下降提供了必要的学习信号，使模型能够发现可泛化的推理策略，这一机制在理论和实践中都得到了验证。

Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of "simple examples": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.

</details>


### [74] [ZENITH: Automated Gradient Norm Informed Stochastic Optimization](https://arxiv.org/abs/2601.15212)
*Dhrubo Saha*

Main category: cs.LG

TL;DR: ZENITH优化器通过梯度范数的时间演化自适应调整学习率，在图像分类、目标检测等任务中实现了更高的精度和更快的训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有自适应优化器存在计算内存开销大、与正则化不兼容、学习率选择次优等问题，需要手动调整学习率计划。

Method: 提出ZENITH优化器，利用梯度范数的时间演化信息来自适应调整学习率，实现零开销的自动学习率调度。

Result: 在6种CNN架构和6个基准测试中，ZENITH在更短的时间内获得更高的测试精度；在MS COCO的目标检测、关键点检测和实例分割任务中取得更好的mAP；与正则化兼容性良好。

Conclusion: ZENITH优化器通过梯度范数演化自适应调整学习率，实现了高效、兼容正则化的自动学习率调度，在多个视觉任务中表现优异。

Abstract: Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.

</details>


### [75] [Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism](https://arxiv.org/abs/2601.15249)
*Garrett G. Wen,Buxin Su,Natalie Collina,Zhun Deng,Weijie Su*

Main category: cs.LG

TL;DR: 作者提出了一种基于等渗机制的作者辅助最佳论文评选方法，通过让作者对自己的论文进行排名评估，结合原始评审分数来更准确地估计论文的真实质量。


<details>
  <summary>Details</summary>
Motivation: NeurIPS、ICML等AI会议每年收到数万篇投稿，给同行评审过程带来巨大挑战，特别是最佳论文奖的评选日益引发争议。现有评审机制难以保证评选质量的一致性。

Method: 采用等渗机制让作者对自己的投稿进行排名评估，然后将作者排名与原始评审分数结合，通过优化调整来估计论文的真实质量。该方法还扩展到处理作者重叠的情况。

Result: 理论证明当作者的效用函数是凸可加函数时，作者有动机如实报告；在作者只有一个提名配额的特殊情况下，即使效用函数只是非递减可加函数，真实性仍然成立。使用ICLR（2019-2023）和NeurIPS（2021-2023）的公开评审数据进行验证，模拟结果显示该方法显著提高了获奖论文的质量。

Conclusion: 作者辅助的等渗机制为大规模会议的最佳论文评选提供了一种有效的解决方案，能够在放松先前工作假设的同时，激励作者真实报告，显著提升评选质量。

Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.

</details>


### [76] [MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs](https://arxiv.org/abs/2601.15279)
*Christoph Bartmann,Johannes Schimunek,Mykyta Ielanskyi,Philipp Seidl,Günter Klambauer,Sohvi Luukkonen*

Main category: cs.LG

TL;DR: MolecularIQ是一个专门用于评估大语言模型对分子结构推理能力的基准测试，专注于符号可验证的任务，以揭示模型在分子图理解方面的具体能力模式。


<details>
  <summary>Details</summary>
Motivation: 现有化学基准测试大多侧重于一般化学知识，依赖文献或代理标签（存在泄露或偏见风险），或将评估简化为多项选择题。需要专门的基准来评估LLMs对分子图结构的推理能力。

Method: 引入MolecularIQ基准，专注于符号可验证的任务，能够对分子图推理进行细粒度评估，将模型失败定位到特定任务和分子结构。

Result: MolecularIQ能够揭示当前化学LLMs的能力模式，将模型失败定位到特定任务和分子结构，提供对模型优势和局限性的可操作见解。

Conclusion: MolecularIQ为评估化学LLMs的分子结构推理能力提供了专门工具，能够指导开发能够忠实推理分子结构的模型。

Abstract: A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [77] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: JAXMg为JAX提供多GPU稠密线性代数支持，通过集成cuSOLVERMg实现跨GPU的Cholesky分解和对称特征值分解，解决单GPU内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 大规模稠密线性系统和特征值问题在科学计算中至关重要，但现有框架难以在单GPU之外扩展，且多GPU求解器库难以集成到可组合的JIT编译Python工作流中。

Method: 通过XLA外部函数接口将JAX与NVIDIA的cuSOLVERMg连接，将分布式GPU求解器暴露为JIT兼容的JAX原语，保持与JAX变换的可组合性。

Result: 实现了支持多GPU的稠密线性代数操作，使矩阵能够超出单GPU内存限制，允许在端到端科学工作流中进行多GPU执行。

Conclusion: JAXMg成功将可扩展的线性代数直接嵌入JAX程序中，为大规模科学计算提供了与JIT编译工作流兼容的多GPU解决方案。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [78] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: 该研究将Itoyori和ItoyoriFBC两种异步多任务运行时集成到Task Bench框架中，与MPI和HPX进行性能和生产力的综合评估，揭示了不同系统在效率和编程复杂度上的权衡。


<details>
  <summary>Details</summary>
Motivation: 异步多任务运行时作为MPI的替代方案具有生产力优势，但多样化的AMT生态系统使得公平比较变得困难。需要系统性地评估不同并行编程系统的性能和程序员生产力。

Method: 使用Task Bench参数化框架集成Itoyori（基于PGAS和RDMA工作窃取）和ItoyoriFBC（扩展了基于future的同步），与MPI和HPX进行对比。性能评估包括计算密集型内核、弱扩展、负载不平衡和通信密集型模式，使用应用效率和最小有效任务粒度指标。生产力评估使用代码行数和库构造数量。

Result: MPI在规则、通信轻量级工作负载中效率最高但代码冗长；HPX在不同节点数下负载不平衡时保持稳定效率，但生产力指标最差；Itoyori在通信密集型配置中效率最高且生产力领先；ItoyoriFBC效率略低于Itoyori，但基于future的同步为不规则工作负载提供了表达潜力。

Conclusion: 不同并行编程系统存在明显的权衡：MPI在规则工作负载中性能最优但编程复杂；AMT系统不一定比MPI更具生产力优势；Itoyori在通信密集型场景中实现了性能和生产力的最佳平衡；ItoyoriFBC的future机制为不规则工作负载提供了灵活性。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [79] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 提出一种随机调度算法ROSS，在混合云环境中实现最优竞争比√K，相比现有方法成本节省达30%


<details>
  <summary>Details</summary>
Motivation: 解决混合云环境中具有硬截止时间的作业调度问题，现有确定性策略在最坏情况下竞争比为Ω(K)，存在显著改进空间

Method: 提出随机调度算法ROSS，在合理的截止时间约束下，通过随机化策略在成本效益但不可靠的spot实例和昂贵但可靠的on-demand实例之间进行调度决策

Result: ROSS算法实现了理论最优竞争比√K，在Azure和AWS真实轨迹数据评估中，相比现有最优方法成本节省高达30%

Conclusion: ROSS算法在混合云环境中有效平衡了成本优化和截止时间保证，显著提升了调度性能，适用于多样化的spot市场条件

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [80] [Specifying and Verifying RDMA Synchronisation (Extended Version)](https://arxiv.org/abs/2601.14642)
*Guillaume Ambal,Max Stupple,Brijesh Dongol,Azalea Raad*

Main category: cs.DC

TL;DR: 该论文扩展了RDMA语义，增加了远程读-修改-写(RMW)操作的形式化模型，并基于此构建了可组合的同步抽象库和三类远程锁验证。


<details>
  <summary>Details</summary>
Motivation: 现有的RDMA^TSO语义缺乏远程同步的形式化描述，导致无法验证锁等常见抽象的正确性。需要填补这一空白，为远程RMW指令提供形式化语义。

Method: 提出RDMA^TSO_RMW语义，描述远程RMW指令在TSO架构下的行为；构建RDMA^WAIT_RMW库作为同步抽象基础；在此基础上实现并验证三类远程锁；提出强RDMA模型RDMA^SC_RMW。

Result: 发现远程RMW操作较弱，仅保证对其他远程RMW操作的原子性；成功构建了可组合的同步抽象库；实现了三类适用于不同场景的远程锁验证；确保与现有高性能LOCO库兼容。

Conclusion: 填补了RDMA语义中远程同步的形式化空白，为RDMA程序的正确性验证提供了理论基础和实用工具，支持高性能分布式系统的可靠开发。

Abstract: Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\text{RDMA}^\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\text{RDMA}^{\text{TSO}}_{\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$ library. Underpinned by $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\text{RDMA}^{\text{SC}}_{\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.

</details>


### [81] [Optimizing FaaS Platforms for MCP-enabled Agentic Workflows](https://arxiv.org/abs/2601.14735)
*Varad Kulkarni,Vaibhav Jha,Nikhil Reddy,Yogesh Simmhan*

Main category: cs.DC

TL;DR: FAME是一个基于FaaS的架构，用于编排支持MCP的智能体工作流，通过将智能体模式分解为可组合的FaaS函数，解决了云部署扩展性和状态管理问题，实现了显著的延迟降低、成本节约和工作流完成率提升。


<details>
  <summary>Details</summary>
Motivation: 基于LLM和MCP服务器的自主AI智能体工作流正在快速增长，但面临云部署扩展性和状态管理的挑战。传统的VM托管方式资源密集且缺乏弹性，而FaaS平台虽然提供模块化、自动扩展和成本效益，但本质上是无状态的。

Method: FAME将智能体模式（如ReAct）分解为可组合的智能体：Planner、Actor和Evaluator，每个都是使用LangGraph构建的FaaS函数，并作为FaaS工作流进行编排。通过DynamoDB实现智能体内存持久化和注入，使用AWS Lambda包装器优化MCP服务器部署，在S3中缓存工具输出，并提出函数融合策略。

Result: 在两个代表性应用（研究论文摘要和日志分析）上的评估显示，FAME实现了高达13倍的延迟降低、88%的输入令牌减少和66%的成本节约，同时提高了工作流完成率。

Conclusion: FAME证明了无服务器平台在托管复杂、多智能体AI工作流方面的可行性，能够实现大规模部署，解决了传统方法的扩展性和状态管理问题。

Abstract: Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.

</details>


### [82] [AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems](https://arxiv.org/abs/2601.14912)
*Guangba Yu,Genting Mai,Rui Wang,Ruipeng Li,Pengfei Chen,Long Pan,Ruijie Xu*

Main category: cs.DC

TL;DR: AlertGuardian框架结合LLM和图模型优化云系统告警生命周期管理，通过去噪、摘要和规则优化三阶段显著减少告警疲劳并提升诊断效率


<details>
  <summary>Details</summary>
Motivation: 大规模云系统产生海量告警导致告警疲劳，现有系统告警生命周期管理效率低下，影响运维效率和系统可靠性

Method: 提出AlertGuardian框架，结合大语言模型和轻量级图模型：1) Alert Denoise使用带虚拟噪声的图学习模型过滤噪声；2) Alert Summary采用检索增强生成(RAG)创建可操作摘要；3) Alert Rule Refinement利用多智能体迭代反馈改进告警规则质量

Result: 在四个真实数据集上评估：告警减少率94.8%，故障诊断准确率90.5%，改进1,174条告警规则，其中375条被SRE接受（32%接受率）

Conclusion: AlertGuardian有效缓解云系统告警疲劳问题，提升运维效率，分享了在Company-X部署后的成功经验和教训

Abstract: Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\% alert reduction ratios) and accelerates fault diagnosis (90.5\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.

</details>


### [83] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 本文提出一个应用级可观测性框架，用于现代边云系统，通过开发者驱动的仪器化和SLO感知反馈实现自主适应，结合多种开源工具实现实时监控和自适应控制。


<details>
  <summary>Details</summary>
Motivation: 现代边云系统需要在异构动态环境中确保自适应行为和性能目标合规性，这需要细粒度的可观测性来监控和调整系统行为。

Method: 集成OpenTelemetry、Prometheus、K3s和Chaos Mesh，构建应用级可观测性框架，结合开发者驱动的仪器化和SLO感知反馈机制，实现实时监控和自适应控制。

Result: 通过视频处理用例展示框架能根据应用级指标自动调整以维持目标帧率、延迟和检测精度，初步结果显示提高了可扩展性、容错性和响应性。

Conclusion: 该框架为自适应、符合SLO的边云应用提供了实用基础，通过应用级可观测性实现了在可变工作负载和注入故障下的自主适应能力。

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


### [84] [Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks](https://arxiv.org/abs/2601.14980)
*Mengchun Xia,Zhicheng Dong,Donghong Cai,Fang Fang,Lisheng Fan,Pingzhi Fan*

Main category: cs.DC

TL;DR: 提出3P-ADMM-PC2算法，结合并行协作ADMM、Paillier同态加密和GPU加速，解决边缘网络分布式计算中的隐私保护和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 边缘网络分布式计算面临单节点计算能力有限、协作计算导致信息泄露和通信开销过大等问题，需要设计既能保护隐私又能高效计算的解决方案。

Method: 设计并行协作分布式ADMM算法，引入Paillier同态加密保护数据隐私，采用量化方法将实数映射到正整数区间，通过GPU加速实现长密钥的并行加密解密计算。

Result: 3P-ADMM-PC2算法在均方误差性能上接近无隐私保护的分布式ADMM，相比CPU实现的集中式和分布式ADMM，获得了显著的速度提升。

Conclusion: 提出的GPU加速3P-ADMM-PC2算法有效解决了边缘网络分布式计算中的隐私保护和计算效率问题，具有良好的实际应用价值。

Abstract: Distributed computing has been widely applied in distributed edge networks for reducing the processing burden of high-dimensional data centralization, where a high-dimensional computational task is decomposed into multiple low-dimensional collaborative processing tasks or multiple edge nodes use distributed data to train a global model. However, the computing power of a single-edge node is limited, and collaborative computing will cause information leakage and excessive communication overhead. In this paper, we design a parallel collaborative distributed alternating direction method of multipliers (ADMM) and propose a three-phase parallel collaborative ADMM privacy computing (3P-ADMM-PC2) algorithm for distributed computing in edge networks, where the Paillier homomorphic encryption is utilized to protect data privacy during interactions. Especially, a quantization method is introduced, which maps the real numbers to a positive integer interval without affecting the homomorphic operations. To address the architectural mismatch between large-integer and Graphics Processing Unit (GPU) computing, we transform high-bitwidth computations into low-bitwidth matrix and vector operations. Thus the GPU can be utilized to implement parallel encryption and decryption computations with long keys. Finally, a GPU-accelerated 3P-ADMM-PC2 is proposed to optimize the collaborative computing tasks. Meanwhile, large-scale computational tasks are conducted in network topologies with varying numbers of edge nodes. Experimental results demonstrate that the proposed 3P-ADMM-PC2 has excellent mean square error performance, which is close to that of distributed ADMM without privacy-preserving. Compared to centralized ADMM and distributed ADMM implemented with Central Processing Unit (CPU) computation, the proposed scheme demonstrates a significant speedup ratio.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [85] [SynPerf: A Hybrid Analytical-ML Framework for GPU Performance Prediction](https://arxiv.org/abs/2601.14910)
*Kaixuan Zhang,Yunfan Cui,Shuhao Zhang,Chutong Ding,Shiyou Qian,Luping Wang,Jian Cao,Guangtao Xue,Cheng Huang,Guodong Yang,Liping Zhang*

Main category: cs.PF

TL;DR: SyncPerf是一个统一的GPU性能建模框架，通过结合分析模型和机器学习模型，实现了高精度、强泛化能力的GPU性能预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer大语言模型的快速发展，对高性能GPU的需求急剧增加，需要快速、准确且泛化能力强的GPU性能模型来支持下一代硬件选择和系统级探索。然而，当前数据驱动方法泛化能力差，无法有效建模现代推理堆栈中的复杂生产级内核。

Method: SyncPerf采用统一框架：首先使用分析模型量化内核对GPU异构指令流水线的需求，然后将这些分析特征输入机器学习模型，捕捉复杂的跨流水线交互和资源依赖关系，实现高保真性能预测。

Result: 在4代主要架构的11种GPU类型和两个广泛使用的服务系统上评估，SyncPerf在核级预测平均误差仅6.1%，端到端推理误差8.5%，比最先进方法分别降低6.7倍和4.4倍。通过性能上限诊断优化生产级融合MoE Triton内核，实现最高1.7倍加速。

Conclusion: SyncPerf提供了一个高保真、强泛化的GPU性能建模框架，不仅显著优于现有方法，还能在实际应用中诊断实现缺陷并指导优化，具有超越模拟的实际价值。

Abstract: The rapid expansion of Transformer-based large language models has dramatically increased the need for high-performance GPUs. As a result, there is growing demand for fast, accurate, and widely generalizable GPU performance models to support next-generation hardware selection and system-level exploration. However, current data-driven methods are limited, exhibiting poor generalization across hardware and inadequate modeling of complex production-level kernels common in modern inference stacks. To address these issues, we present SyncPerf, a unified GPU modeling framework. This approach first employs an analytical model to quantify a given kernel's demands on the GPU's heterogeneous instruction pipelines. These analytical features are then fed into a machine learning (ML) model to capture complex cross-pipeline interactions and resource dependencies, enabling high-fidelity performance prediction. Our evaluation across 11 GPU types from four generations of major architectures on two widely-used serving systems demonstrates that SyncPerf delivers high fidelity and strong generalizability. It achieves accurate predictions, with only 6.1% average error at the kernel level and 8.5% for end-to-end inference -- reducing the error of state-of-the-art methods by 6.7x and 4.4x, respectively. We also demonstrate SynPerf's value "beyond simulation" by utilizing its performance ceiling to diagnose implementation shortcomings and guide the optimization of a production fused MoE Triton kernel, achieving up to 1.7x speedup.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [86] [GNN-based Path-aware multi-view Circuit Learning for Technology Mapping](https://arxiv.org/abs/2601.14286)
*Wentao Jiang,Jingxin Wang,Zhang Hu,Zhengyuan Shi,Chengyu Ma,Qiang Xu,Weikang Qian,Zhufei Chu*

Main category: cs.ET

TL;DR: 提出GPA框架，通过融合电路结构的三个互补视图，使用GNN学习精确的数据驱动延迟预测，显著减少传统技术映射中的延迟估计误差。


<details>
  <summary>Details</summary>
Motivation: 传统技术映射依赖抽象的技术无关延迟模型，存在系统性不准确问题，无法捕捉实际映射后电路的细微时序行为。

Method: 引入GPA框架，融合三个互补的电路结构视图：AIG功能编码、映射后技术视图和关键时序路径视图，使用GNN学习精确延迟预测，训练数据来自工业级映射后网表的关键路径单元延迟。

Result: 在19个EPFL组合基准测试中，GPA相比传统启发式方法（techmap、MCH）平均延迟减少19.9%和2.1%，相比先前最先进的ML方法SLAP减少4.1%，且不牺牲面积效率。

Conclusion: GPA框架通过数据驱动的延迟预测显著改善了技术映射的质量，为电路优化提供了更准确的时序分析工具。

Abstract: Traditional technology mapping suffers from systemic inaccuracies in delay estimation due to its reliance on abstract, technology-agnostic delay models that fail to capture the nuanced timing behavior behavior of real post-mapping circuits. To address this fundamental limitation, we introduce GPA(graph neural network (GNN)-based Path-Aware multi-view circuit learning), a novel GNN framework that learns precise, data-driven delay predictions by synergistically fusing three complementary views of circuit structure: And-Inverter Graphs (AIGs)-based functional encoding, post-mapping technology emphasizes critical timing paths. Trained exclusively on real cell delays extracted from critical paths of industrial-grade post-mapping netlists, GPA learns to classify cut delays with unprecedented accuracy, directly informing smarter mapping decisions. Evaluated on the 19 EPFL combinational benchmarks, GPA achieves 19.9%, 2.1% and 4.1% average delay reduction over the conventional heuristics methods (techmap, MCH) and the prior state-of-the-art ML-based approach SLAP, respectively-without compromising area efficiency.

</details>


### [87] [Analog-to-Stochastic Converter Using Magnetic Tunnel Junction Devices for Vision Chips](https://arxiv.org/abs/2601.14640)
*Naoya Onizawa,Daisaku Katagiri,Warren J. Gross,Takahiro Hanyu*

Main category: cs.ET

TL;DR: 提出使用磁性隧道结(MTJ)器件实现模拟信号到随机信号的一步转换，用于基于随机计算的视觉芯片，以降低功耗和面积开销。


<details>
  <summary>Details</summary>
Motivation: 随机计算已被用于面积高效的硬件实现（如LDPC解码器和图像处理器），但需要功耗和面积大的两步转换（模拟-数字和数字-随机）。需要实现一步转换来降低信号转换开销。

Method: 利用MTJ器件固有的概率性开关行为，将模拟信号直接转换为随机信号。理论上描述了转换特性，考虑MTJ电阻变异性进行补偿，在90nm CMOS和100nm MTJ技术中设计转换器，并使用NS-SPICE仿真器验证。

Result: 成功设计并验证了基于MTJ的模拟到随机转换器，实现了面积高效的一步转换，降低了传统两步转换的功耗和面积开销。

Conclusion: MTJ器件可用于实现高效的一步模拟到随机信号转换，为基于随机计算的视觉芯片提供了可行的解决方案，降低了信号转换的开销。

Abstract: This paper introduces an analog-to-stochastic converter using a magnetic tunnel junction (MTJ) device for vision chips based on stochastic computation. Stochastic computation has been recently exploited for area-efficient hardware implementation, such as low-density parity-check (LDPC) decoders and image processors. However, power-and-area hungry two-step (analog-to-digital and digital-to-stochastic) converters are required for the analog to stochastic signal conversion. To realize a one-step conversion, an MTJ device is used as it inherently exhibits a probabilistic switching behavior between two resistance states. Exploiting the device-based probabilistic behavior, analog signals can be directly and area-efficiently converted to stochastic signals to mitigate the signal-conversion overhead. The analog-to-stochastic signal conversion is theoretically described and the conversion characteristic is evaluated using device and circuit parameters. In addition, the resistance variability of the MTJ device is considered in order to compensate the variability effect on the signal conversion. Based on the theoretical analysis, the analog-to-stochastic converter is designed in 90nm CMOS and 100nm MTJ technologies and is verified using a SPICE simulator (NS-SPICE) that handles both transistors and MTJ devices.

</details>
