<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 134]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 4]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [A-Graph: A Unified Graph Representation for At-Will Simulation across System Stacks](https://arxiv.org/abs/2602.04847)
*Daniel Price,Prabhu Vellaisamy,Patricia Gonzalez,George Michelogiannakis,John P. Shen,Di Wu*

Main category: cs.PF

TL;DR: 提出Architecture-Graph (Agraph)统一系统表示和Archx框架，实现技术、架构、应用无关的灵活设计空间探索，提升可编程性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统在技术、架构、应用等方面的多样化，设计空间变得更大更复杂。现有EDA工具虽然准确但不进行设计空间探索，而现有仿真方法多为领域特定、灵活性有限且需要高领域专业知识。

Method: 提出Architecture-Graph (Agraph)作为统一系统表示图，能够围绕任意应用、软件、架构和电路进行建模。进一步开发Archx框架，提供易用编程接口自动生成设计点，并采用基于范围的度量检索来分析设计点。

Result: 通过案例研究展示了Agraph在技术、架构、应用方面的泛化能力，并实现了高仿真精度。Agraph和Archx为按需模拟性能和成本提供了基础。

Conclusion: Agraph和Archx通过统一的系统表示和用户友好的框架，实现了跨系统栈的灵活设计空间探索，解决了现有方法的局限性，为系统仿真提供了新的基础。

Abstract: As computer systems continue to diversify across technologies, architectures, applications, and beyond, the relevant design space has become larger and more complex. Given such trends, design space exploration (DSE) at early stages is critical to ensure agile development towards optimal performance and cost. Industry-grade EDA tools directly take in RTL code and report accurate results, but do not perform DSE. Recent works have attempted to explore the design space via simulation. However, most of these works are domain-specific and constrain the space that users are allowed to explore, offering limited flexibility between technologies, architecture, and applications. Moreover, they often demand high domain expertise to ensure high accuracy. To enable simulation that is agnostic to technology, architecture, and application at any granularity, we introduce Architecture-Graph (Agraph), a graph that unifies the system representation surrounding any arbitrary application, software, architecture, and circuit. Such a unified representation distinguishes Agraph from prior works, which focus on a single stack, allowing users to freely explore the design space across system stacks. To fully unleash the potential of Agraph, we further present Archx, a framework that implements Agraph. Archx is user-friendly in two ways. First, Archx has an easy-to-use programming interface to automatically generate and sweep design points under user constraints, boosting the programmability. Second, Archx adopts scope-based metric retrieval to analyze and understand each design point at any user-preferred hierarchy, enhancing the explainability. We conduct case studies that demonstrate Agraph's generalization across technologies, architecture, and applications with high simulation accuracy. Overall, we argue that Agraph and Archx serve as a foundation to simulate both performance and cost at will.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data](https://arxiv.org/abs/2602.03872)
*Jiaming Zhang,Huanyi Xie,Meng Ding,Shaopeng Fu,Jinyan Liu,Di Wang*

Main category: cs.LG

TL;DR: DP-SGD在长尾数据上表现不佳，因为梯度裁剪和噪声注入共同损害了模型记忆信息丰富但代表性不足样本的能力。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型通过记忆训练样本来实现高预测精度，这引发了隐私担忧，促使广泛采用DP-SGD等差分隐私训练算法。然而，经验研究表明DP-SGD在长尾数据上泛化性能较差，但这一现象缺乏理论理解。

Method: 开发了首个从特征学习角度分析DP-SGD在长尾数据上的理论框架，分析了DP-SGD的训练动态，展示了梯度裁剪和噪声注入如何共同影响模型记忆能力。

Result: 理论分析表明，DP-SGD训练模型在长尾子群体上的测试误差显著大于在整个数据集上的总体测试误差。梯度裁剪和噪声注入共同损害了模型记忆信息丰富但代表性不足样本的能力。

Conclusion: 该研究首次从理论角度解释了DP-SGD在长尾数据上表现不佳的现象，为理解差分隐私训练算法的局限性提供了理论框架，并通过实验验证了理论发现。

Abstract: Recent research shows that modern deep learning models achieve high predictive accuracy partly by memorizing individual training samples. Such memorization raises serious privacy concerns, motivating the widespread adoption of differentially private training algorithms such as DP-SGD. However, a growing body of empirical work shows that DP-SGD often leads to suboptimal generalization performance, particularly on long-tailed data that contain a large number of rare or atypical samples. Despite these observations, a theoretical understanding of this phenomenon remains largely unexplored, and existing differential privacy analysis are difficult to extend to the nonconvex and nonsmooth neural networks commonly used in practice. In this work, we develop the first theoretical framework for analyzing DP-SGD on long-tailed data from a feature learning perspective. We show that the test error of DP-SGD-trained models on the long-tailed subpopulation is significantly larger than the overall test error over the entire dataset. Our analysis further characterizes the training dynamics of DP-SGD, demonstrating how gradient clipping and noise injection jointly adversely affect the model's ability to memorize informative but underrepresented samples. Finally, we validate our theoretical findings through extensive experiments on both synthetic and real-world datasets.

</details>


### [3] [Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra](https://arxiv.org/abs/2602.03875)
*Stefan Kuhn,Vandana Dwarka,Przemyslaw Karol Grenda,Eero Vainikko*

Main category: cs.LG

TL;DR: 提出一种用于13C NMR的可逆深度学习模型，使用单一条件可逆神经网络实现分子结构与光谱之间的双向映射


<details>
  <summary>Details</summary>
Motivation: 传统方法需要分别训练正向（结构到光谱）和逆向（光谱到结构）模型，无法统一处理光谱预测和不确定性感知的候选结构生成

Method: 基于i-RevNet风格的双射块构建条件可逆神经网络，训练时从图结构编码预测128位分箱光谱代码，剩余潜在维度捕获残差变异性

Result: 在过滤子集上，模型在训练示例上数值可逆，光谱代码预测优于随机，在验证光谱上逆向生成粗糙但有意义的结构信号

Conclusion: 可逆架构可以在单一端到端模型中统一光谱预测和不确定性感知的候选结构生成

Abstract: We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.

</details>


### [4] [GOPO: Policy Optimization using Ranked Rewards](https://arxiv.org/abs/2602.03876)
*Kyuseong Choi,Dwaipayan Saha,Woojeong Kim,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: GOPO是一种新的策略优化方法，仅使用奖励排名而忽略其幅度，在非可验证奖励场景中相比GRPO表现更优


<details>
  <summary>Details</summary>
Motivation: 传统RLHF使用奖励模型的绝对幅度进行策略优化，但在非可验证奖励场景（如摘要、指令遵循、聊天完成）中，这种不匹配会导致次优性能

Method: 提出Group Ordinal Policy Optimization (GOPO)，仅使用奖励排名而丢弃幅度信息，通过基于排名的奖励转换来优化策略

Result: 相比GRPO，GOPO在非可验证奖励场景中：1) 训练/验证奖励轨迹更高；2) LLM-as-judge评估在大多数训练步骤中表现更好；3) 用更少训练步骤达到相当质量的策略

Conclusion: GOPO在多种任务和模型规模上均表现出一致改进，证明了在非可验证奖励场景中使用排名而非幅度进行策略优化的有效性

Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.

</details>


### [5] [Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems](https://arxiv.org/abs/2602.04120)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出XaaS架构，将可解释性作为独立服务，解耦推理与解释生成，降低边缘AI系统延迟38%


<details>
  <summary>Details</summary>
Motivation: 当前XAI在边缘和IoT系统中的集成通常是临时且低效的，现有方法将解释生成与模型推理耦合，导致冗余计算、高延迟和可扩展性差

Method: 提出Explainability-as-a-Service (XaaS)分布式架构，包含三个创新：1) 基于语义相似性的分布式解释缓存；2) 轻量级验证协议确保解释保真度；3) 自适应解释引擎根据设备能力和用户需求选择解释方法

Result: 在三个真实边缘AI用例（制造质量控制、自动驾驶感知、医疗诊断）上评估，XaaS将延迟降低38%，同时保持高质量的解释

Conclusion: XaaS实现了透明和可问责AI在大规模异构IoT系统中的部署，弥合了XAI研究与边缘实践之间的差距

Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.

</details>


### [6] [NeuroPareto: Calibrated Acquisition for Costly Many-Goal Search in Vast Parameter Spaces](https://arxiv.org/abs/2602.03901)
*Rong Fu,Wenxin Zhang,Chunlei Meng,Youjin Wang,Haoyu Zhao,Jiaxuan Lu,Kun Liu,JiaBao Dou,Simon James Fong*

Main category: cs.LG

TL;DR: NeuroPareto：一种集成排序过滤、不确定性解耦和历史条件获取策略的架构，用于在计算约束下高效进行多目标优化。


<details>
  <summary>Details</summary>
Motivation: 在高维搜索空间中，在严格计算约束下寻求最优权衡是多目标优化的核心挑战。现有方法在处理复杂目标景观时效率不足，需要更高效的优化框架。

Method: 1. 排序中心过滤：使用校准贝叶斯分类器估计非支配层级的不确定性
2. 不确定性解耦：深度高斯过程代理将预测不确定性分解为可减少和不可减少部分
3. 历史条件获取：轻量级获取网络从历史超体积改进中在线学习，指导昂贵评估
4. 分层筛选和摊销代理更新：保持准确性同时降低计算开销

Result: 在DTLZ和ZDT测试套件以及地下能源提取任务中，NeuroPareto在Pareto接近度和超体积指标上持续优于分类器增强和代理辅助的基线方法。

Conclusion: NeuroPareto通过集成排序过滤、不确定性解耦和历史条件获取策略，能够在计算约束下高效导航复杂目标景观，生成高质量候选解，为多目标优化提供了有效的解决方案。

Abstract: The pursuit of optimal trade-offs in high-dimensional search spaces under stringent computational constraints poses a fundamental challenge for contemporary multi-objective optimization. We develop NeuroPareto, a cohesive architecture that integrates rank-centric filtering, uncertainty disentanglement, and history-conditioned acquisition strategies to navigate complex objective landscapes. A calibrated Bayesian classifier estimates epistemic uncertainty across non-domination tiers, enabling rapid generation of high-quality candidates with minimal evaluation cost. Deep Gaussian Process surrogates further separate predictive uncertainty into reducible and irreducible components, providing refined predictive means and risk-aware signals for downstream selection. A lightweight acquisition network, trained online from historical hypervolume improvements, guides expensive evaluations toward regions balancing convergence and diversity. With hierarchical screening and amortized surrogate updates, the method maintains accuracy while keeping computational overhead low. Experiments on DTLZ and ZDT suites and a subsurface energy extraction task show that NeuroPareto consistently outperforms classifier-enhanced and surrogate-assisted baselines in Pareto proximity and hypervolume.

</details>


### [7] [GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression](https://arxiv.org/abs/2602.03906)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.LG

TL;DR: 本文提出GeoIB（几何信息瓶颈），通过信息几何视角重新审视信息瓶颈问题，避免了传统方法中互信息估计的偏差问题，实现了更直接的信息压缩控制。


<details>
  <summary>Details</summary>
Motivation: 传统信息瓶颈方法在深度学习中通常使用变分界或神经互信息估计器等可处理的替代方法，而不是直接控制互信息I(X;Z)。这些方法的松弛性和估计器依赖性偏差使得信息压缩只能间接控制，优化过程脆弱不稳定。

Method: 从信息几何角度重新审视IB问题，提出GeoIB方法。该方法将I(X;Z)和I(Z;Y)表示为从联合分布到各自独立流形的最小KL距离的精确投影形式。GeoIB通过两个互补项控制信息压缩：(1)分布层面的Fisher-Rao差异，匹配KL散度到二阶且重参数化不变；(2)几何层面的Jacobian-Frobenius项，通过惩罚编码器的拉回体积扩张来提供I(Z;X)的局部容量型上界。还推导了与FR度量一致的自然梯度优化器。

Result: 在多个流行数据集上的实验表明，GeoIB在信息平面上实现了比主流IB基线更好的预测精度和压缩率权衡。GeoIB通过将分布和几何正则化统一在单个瓶颈乘子下，提高了不变性和优化稳定性。

Conclusion: GeoIB通过信息几何视角提供了一种避免互信息估计偏差的新方法，实现了更直接和稳定的信息瓶颈控制，在信息平面上获得了更好的权衡性能。

Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB "compression" only indirectly controlled and optimization fragile.
  We revisit the IB problem through the lens of information geometry and propose a \textbf{Geo}metric \textbf{I}nformation \textbf{B}ottleneck (\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at "https://anonymous.4open.science/r/G-IB-0569".

</details>


### [8] [The Role of Target Update Frequencies in Q-Learning](https://arxiv.org/abs/2602.03911)
*Simon Weissmann,Tilman Aach,Benedikt Wille,Sebastian Kassing,Leif Döring*

Main category: cs.LG

TL;DR: 本文从近似动态规划角度理论分析了Q学习中的目标网络更新频率，证明了恒定更新频率是次优的，而几何增长的适应性调度能避免对数级样本复杂度开销。


<details>
  <summary>Details</summary>
Motivation: 目标网络更新频率是深度Q学习中的核心稳定机制，但当前对其选择缺乏理论理解，通常仅作为可调超参数而非原则性设计决策。本文旨在提供目标网络固定的理论分析，阐明其内在机制。

Method: 将周期性目标更新建模为嵌套优化方案：外层迭代应用不精确的Bellman最优算子，内层循环用通用优化器近似。在异步采样设置下进行有限时间收敛分析，特别以内层使用随机梯度下降为例。

Result: 理论分析明确刻画了目标更新周期引起的偏差-方差权衡，揭示了如何最优设置这一关键超参数。证明了恒定目标更新调度是次优的，会产生可避免的对数级样本复杂度开销。

Conclusion: 最优目标更新频率应在学习过程中几何增长，而非保持恒定。这为Q学习中的目标网络更新提供了理论指导，表明适应性调度能显著提升样本效率。

Abstract: The target network update frequency (TUF) is a central stabilization mechanism in (deep) Q-learning. However, their selection remains poorly understood and is often treated merely as another tunable hyperparameter rather than as a principled design decision. This work provides a theoretical analysis of target fixing in tabular Q-learning through the lens of approximate dynamic programming. We formulate periodic target updates as a nested optimization scheme in which each outer iteration applies an inexact Bellman optimality operator, approximated by a generic inner loop optimizer. Rigorous theory yields a finite-time convergence analysis for the asynchronous sampling setting, specializing to stochastic gradient descent in the inner loop. Our results deliver an explicit characterization of the bias-variance trade-off induced by the target update period, showing how to optimally set this critical hyperparameter. We prove that constant target update schedules are suboptimal, incurring a logarithmic overhead in sample complexity that is entirely avoidable with adaptive schedules. Our analysis shows that the optimal target update frequency increases geometrically over the course of the learning process.

</details>


### [9] [Echo State Networks for Time Series Forecasting: Hyperparameter Sweep and Benchmarking](https://arxiv.org/abs/2602.03912)
*Alexander Häußer*

Main category: cs.LG

TL;DR: ESN在M4数据集上的研究表明，这种纯反馈驱动的网络在月度数据上与ARIMA/TBATS相当，在季度数据上表现最佳，且计算成本更低，为自动化时间序列预测提供了实用选择。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估完全自动化的纯反馈驱动回声状态网络(ESN)是否能作为广泛使用的统计预测方法的竞争性替代方案，特别是在月度/季度时间序列预测中。

Method: 采用严格的两阶段评估：使用参数数据集进行超参数扫描（泄漏率、谱半径、储备池大小、正则化），超过400万次ESN拟合；然后使用不相交的预测数据集进行样本外精度评估，以MASE和sMAPE为指标，与漂移、季节性朴素、ARIMA、ETS、TBATS等基准对比。

Result: 超参数分析显示一致可解释模式：月度序列偏好中等持续性储备池，季度序列偏好收缩性动态；高泄漏率普遍优选。样本外评估中，ESN在月度数据上与ARIMA/TBATS相当，在季度数据上获得最低平均MASE，且计算成本低于复杂统计模型。

Conclusion: ESN在预测精度、鲁棒性和计算效率之间提供了引人注目的平衡，使其成为自动化时间序列预测的实用选择。

Abstract: This paper investigates the forecasting performance of Echo State Networks (ESNs) for univariate time series forecasting using a subset of the M4 Forecasting Competition dataset. Focusing on monthly and quarterly time series with at most 20 years of historical data, we evaluate whether a fully automatic, purely feedback-driven ESN can serve as a competitive alternative to widely used statistical forecasting methods. The study adopts a rigorous two-stage evaluation approach: a Parameter dataset is used to conduct an extensive hyperparameter sweep covering leakage rate, spectral radius, reservoir size, and information criteria for regularization, resulting in over four million ESN model fits; a disjoint Forecast dataset is then used for out-of-sample accuracy assessment. Forecast accuracy is measured using MASE and sMAPE and benchmarked against simple benchmarks like drift and seasonal naive and statistical models like ARIMA, ETS, and TBATS. The hyperparameter analysis reveals consistent and interpretable patterns, with monthly series favoring moderately persistent reservoirs and quarterly series favoring more contractive dynamics. Across both frequencies, high leakage rates are preferred, while optimal spectral radii and reservoir sizes vary with temporal resolution. In the out-of-sample evaluation, the ESN performs on par with ARIMA and TBATS for monthly data and achieves the lowest mean MASE for quarterly data, while requiring lower computational cost than the more complex statistical models. Overall, the results demonstrate that ESNs offer a compelling balance between predictive accuracy, robustness, and computational efficiency, positioning them as a practical option for automated time series forecasting.

</details>


### [10] [Causal Discovery for Cross-Sectional Data Based on Super-Structure and Divide-and-Conquer](https://arxiv.org/abs/2602.03914)
*Wenyu Wang,Yaping Wan*

Main category: cs.LG

TL;DR: 提出一个轻量级框架，通过放宽对超级结构构建的严格要求，在保持分治算法优势的同时大幅降低条件独立性测试的计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决基于超级结构的分治因果发现中的关键瓶颈：当条件独立性测试成本高昂且领域知识不可用时，构建准确超级结构的高计算成本问题。

Method: 提出一个新颖的轻量级框架，将弱约束的超级结构与高效的图划分和合并策略相结合，实例化为具体的因果发现算法。

Result: 在合成数据（高斯贝叶斯网络）上，方法在结构准确性上匹配或接近PC和FCI算法，同时大幅减少CI测试数量；在真实世界CHARLS数据集上验证了实际适用性。

Conclusion: 即使在初始超级结构假设最小的情况下，也能实现准确、可扩展的因果发现，为大规模、知识稀缺领域（如生物医学和社会科学研究）应用分治方法开辟了新途径。

Abstract: This paper tackles a critical bottleneck in Super-Structure-based divide-and-conquer causal discovery: the high computational cost of constructing accurate Super-Structures--particularly when conditional independence (CI) tests are expensive and domain knowledge is unavailable. We propose a novel, lightweight framework that relaxes the strict requirements on Super-Structure construction while preserving the algorithmic benefits of divide-and-conquer. By integrating weakly constrained Super-Structures with efficient graph partitioning and merging strategies, our approach substantially lowers CI test overhead without sacrificing accuracy. We instantiate the framework in a concrete causal discovery algorithm and rigorously evaluate its components on synthetic data. Comprehensive experiments on Gaussian Bayesian networks, including magic-NIAB, ECOLI70, and magic-IRRI, demonstrate that our method matches or closely approximates the structural accuracy of PC and FCI while drastically reducing the number of CI tests. Further validation on the real-world China Health and Retirement Longitudinal Study (CHARLS) dataset confirms its practical applicability. Our results establish that accurate, scalable causal discovery is achievable even under minimal assumptions about the initial Super-Structure, opening new avenues for applying divide-and-conquer methods to large-scale, knowledge-scarce domains such as biomedical and social science research.

</details>


### [11] [SpecMD: A Comprehensive Study On Speculative Expert Prefetching](https://arxiv.org/abs/2602.03921)
*Duc Hoang,Ajay Jaiswal,Mohammad Samragh,Minsik Cho*

Main category: cs.LG

TL;DR: SpecMD框架系统评估MoE专家缓存策略，发现传统缓存假设不适用，提出Least-Stale策略显著提升命中率和推理速度


<details>
  <summary>Details</summary>
Motivation: MoE模型通过稀疏专家激活实现高效推理，但需要缓存机制来发挥性能优势。现有硬件中心化缓存策略在不同硬件配置下的交互效果不明确，缺乏标准化评估框架

Method: 开发SpecMD标准化框架，在多种硬件配置下系统评估MoE缓存策略。基于实验发现专家访问不符合时间局部性假设，提出Least-Stale驱逐策略，利用MoE可预测的专家访问模式

Result: Least-Stale策略相比LRU减少85倍碰撞缺失，在仅5% VRAM缓存容量下实现88%命中率，TTFT减少34.7%。SpecMD框架成功复现并扩展了先前方法

Conclusion: MoE专家访问模式具有独特特性，传统缓存假设不适用。Least-Stale策略能有效利用这些模式，显著提升缓存效率。SpecMD为MoE缓存策略评估提供了标准化基准

Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\times$ over LRU. With such gains, we achieve over $88\%$ hit rates with up to $34.7\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\%$ or $0.6GB$ of VRAM cache capacity.

</details>


### [12] [Online Vector Quantized Attention](https://arxiv.org/abs/2602.03922)
*Nick Alonso,Tomas Figliolia,Beren Millidge*

Main category: cs.LG

TL;DR: OVQ-attention是一种新型序列混合层，在线向量量化注意力机制，在保持线性计算和常数内存的同时，通过稀疏内存更新提升长上下文处理能力，性能接近自注意力但内存消耗大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有序列混合层在效率和性能之间存在权衡：自注意力在长上下文任务中表现良好但计算和内存成本高（二次计算和线性内存），而线性注意力和SSM虽然计算和内存成本低（线性计算和常数内存），但在长上下文处理上表现不佳。需要找到更好的折中方案。

Method: 提出在线向量量化注意力（OVQ-attention），基于高斯混合回归理论，采用稀疏内存更新机制，允许大幅增加内存状态大小和内存容量，同时保持线性计算成本和常数内存需求。

Result: 在多种合成长上下文任务和长上下文语言建模中，OVQ-attention相比线性注意力基线和原始VQ-attention有显著改进，在64k序列长度内与强自注意力基线表现相当甚至相同，但仅使用自注意力的一小部分内存。

Conclusion: OVQ-attention在内存计算成本和长上下文处理之间找到了更好的平衡，提供了一种高效且有效的序列混合层替代方案，特别适合长序列处理任务。

Abstract: Standard sequence mixing layers used in language models struggle to balance efficiency and performance. Self-attention performs well on long context tasks but has expensive quadratic compute and linear memory costs, while linear attention and SSMs use only linear compute and constant memory but struggle with long context processing. In this paper, we develop a sequence mixing layer that aims to find a better compromise between memory-compute costs and long-context processing, which we call online vector-quantized (OVQ) attention. OVQ-attention requires linear compute costs and constant memory, but, unlike linear attention and SSMs, it uses a sparse memory update that allows it to greatly increase the size of its memory state and, consequently, memory capacity. We develop a theoretical basis for OVQ-attention based on Gaussian mixture regression, and we test it on a variety of synthetic long context tasks and on long context language modeling. OVQ-attention shows significant improvements over linear attention baselines and the original VQ-attention, on which OVQ-attention was inspired. It demonstrates competitive, and sometimes identical, performance to strong self-attention baselines up 64k sequence length, despite using a small fraction of the memory of full self-attention.

</details>


### [13] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: WIND是一个统一的天气气候基础模型，通过自监督视频重建预训练，无需任务特定微调即可解决多种天气气候问题


<details>
  <summary>Details</summary>
Motivation: 当前天气气候深度学习模型高度专业化且分散，需要统一的预训练基础模型来替代各种专门化基线

Method: 使用无条件视频扩散模型进行自监督视频重建预训练，通过后验采样将各种任务统一为逆问题求解

Result: WIND能够处理概率预报、时空降尺度、稀疏重建、守恒定律执行等任务，并能生成全球变暖情景下的极端天气反事实叙事

Conclusion: WIND通过生成视频建模与逆问题求解的结合，为AI大气建模提供了计算高效的新范式

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [14] [Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints](https://arxiv.org/abs/2602.03940)
*Olaf Yunus Laitinen Imanov,Duygu Erisken,Derya Umut Kulali,Taner Yilmaz,Rana Irem Turhan*

Main category: cs.LG

TL;DR: AURA是一个基于分层多智能体强化学习的系统，用于在严格监管约束下实时选择经济适用房用地，显著提高效率和优化多目标权衡。


<details>
  <summary>Details</summary>
Motivation: 全球数十亿人面临经济适用房短缺问题，而土地稀缺和监管限制使得选址过程缓慢。传统方法难以在复杂的监管约束下实时优化多个社会目标。

Method: 将任务建模为约束多目标马尔可夫决策过程，使用监管感知的状态编码（127个联邦和地方约束）、帕累托约束策略梯度（具有可行性保证）和奖励分解（分离即时成本与长期社会结果）。

Result: 在8个美国大都市数据集（47,392个候选地块）上，AURA达到94.3%的监管合规率，帕累托超体积比基线提高37.2%。在纽约市2026年案例研究中，将选址时间从18个月缩短至72小时，识别出多23%的可行地块，所选地块的交通便利性比专家选择高31%，环境影响低19%。

Conclusion: AURA系统能够有效解决经济适用房选址中的监管约束和多目标优化问题，显著提高决策效率和结果质量，为城市资源分配提供了一种创新的自动化解决方案。

Abstract: Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.

</details>


### [15] [Grables: Tabular Learning Beyond Independent Rows](https://arxiv.org/abs/2602.03945)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: 论文提出grables框架，将表格转换为图结构进行预测，解决了传统行级预测无法处理行间依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 传统表格学习主要使用行级预测器，假设各行独立同分布，但在事务性、时序性和关系性表格中，标签往往依赖于其他行。行级预测无法处理全局计数、重叠和关系模式等自然目标。

Method: 提出grables框架：模块化接口将表格提升为图（构造器）与在图上的预测计算（节点预测器）分离。通过消息传递捕获行间依赖关系，并探索混合方法，显式提取行间结构后输入给强大的表格学习器。

Result: 在合成任务、交易数据和RelBench临床试验数据集上的实验证实：消息传递能捕获行级模型忽略的行间依赖关系；显式提取行间结构并输入给表格学习器的混合方法能带来一致的性能提升。

Conclusion: 表格学习需要超越行级预测，考虑行间依赖关系。grables框架为理解不同架构如何利用表格结构提供了精确工具，混合方法在关系性表格任务中表现优异。

Abstract: Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make "using structure" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.

</details>


### [16] [Representation Geometry as a Diagnostic for Out-of-Distribution Robustness](https://arxiv.org/abs/2602.03951)
*Ali Zia,Farid Hazratian*

Main category: cs.LG

TL;DR: 提出基于几何的诊断框架，通过构建类别条件互k近邻图，提取全局谱复杂度和局部平滑度两个几何不变量，用于无标签预测模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏目标域标签的情况下，监控和优化分布偏移下的鲁棒泛化仍然困难。现有方法主要关注训练时正则化和低阶表示统计量，但缺乏对学习嵌入几何结构是否提供可靠后验鲁棒性信号的研究。

Method: 提出几何诊断框架：1）从分布内嵌入构建类别条件互k近邻图；2）提取两个互补不变量：基于归一化拉普拉斯矩阵约化对数行列式的全局谱复杂度代理，以及基于Ollivier-Ricci曲率的局部平滑度度量。

Result: 在多种架构、训练机制和腐败基准测试中，较低的谱复杂度和较高的平均曲率一致预测更强的分布外准确率。受控扰动和拓扑分析表明这些信号反映了有意义的表示结构而非表面嵌入统计。

Conclusion: 表示几何能够实现可解释、无标签的鲁棒性诊断，并支持在分布偏移下进行可靠的无人监督检查点选择。

Abstract: Robust generalization under distribution shift remains difficult to monitor and optimize in the absence of target-domain labels, as models with similar in-distribution accuracy can exhibit markedly different out-of-distribution (OOD) performance. While prior work has focused on training-time regularization and low-order representation statistics, little is known about whether the geometric structure of learned embeddings provides reliable post-hoc signals of robustness. We propose a geometry-based diagnostic framework that constructs class-conditional mutual k-nearest-neighbor graphs from in-distribution embeddings and extracts two complementary invariants: a global spectral complexity proxy based on the reduced log-determinant of the normalized Laplacian, and a local smoothness measure based on Ollivier--Ricci curvature. Across multiple architectures, training regimes, and corruption benchmarks, we find that lower spectral complexity and higher mean curvature consistently predict stronger OOD accuracy across checkpoints. Controlled perturbations and topological analyses further show that these signals reflect meaningful representation structure rather than superficial embedding statistics. Our results demonstrate that representation geometry enables interpretable, label-free robustness diagnosis and supports reliable unsupervised checkpoint selection under distribution shift.

</details>


### [17] [Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study](https://arxiv.org/abs/2602.03957)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 使用遗传算法神经架构搜索开发儿童死亡率预测模型，在时序验证中优于XGBoost，并通过公平性审计发现模型在贫困地区表现更好，识别出更多高风险儿童。


<details>
  <summary>Details</summary>
Motivation: 现有儿童死亡率预测模型存在前瞻性偏差，在应用于未来人群时准确性下降，需要开发更稳健的时序验证方法。

Method: 使用孟加拉国2011-2022年DHS数据，采用时序划分训练(2011-2014)、验证(2017)、测试(2022)。通过遗传算法神经架构搜索找到最优单层神经网络结构(64个单元)，并与XGBoost比较。使用SHAP值和Platt校准进行验证。

Result: 神经网络(AUROC=0.76)显著优于XGBoost(0.73)。公平性审计显示区域贫困水平与模型AUC呈负相关(r=-0.62)，模型在最贫困地区表现最好(AUC 0.74)，在最富裕地区最差(0.66)。在10%筛查阈值下，每年可比梯度提升模型多识别1300名高风险儿童。

Conclusion: 该方法提供了稳健、可部署的计算表型，能更准确地识别需要干预的高风险儿童，特别是在最需要帮助的贫困地区，为针对性妇幼健康干预提供了有效工具。

Abstract: The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p < 0.01). Additionally, through a detailed fairness audit, we identified an overall "Socioeconomic Predictive Gradient," with a positive correlation between regional poverty level (r = -0.62) and the algorithm's AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.

</details>


### [18] [Non-linear PCA via Evolution Strategies: a Novel Objective Function](https://arxiv.org/abs/2602.03967)
*Thomas Uriot,Elise Chung*

Main category: cs.LG

TL;DR: 提出一种结合PCA可解释性与神经网络灵活性的非线性PCA框架，使用进化策略优化，在保持可解释性的同时显著提升方差解释能力


<details>
  <summary>Details</summary>
Motivation: 传统PCA是线性方法，无法捕捉现实数据的复杂结构；核PCA虽能处理非线性但牺牲了可解释性且超参数选择困难。需要一种既保持PCA可解释性又能处理非线性的方法。

Method: 使用神经网络参数化变量变换，通过进化策略(ES)优化处理特征分解的不可微性；提出细粒度目标函数，最大化每个变量的个体方差贡献而非全局方差；原生支持分类和有序变量，避免独热编码的维度爆炸问题。

Result: 在合成和真实数据集上，该方法在解释方差方面显著优于线性PCA和核PCA；同时保持了PCA的可解释性，可以使用双标图等标准工具进行特征贡献的可视化和分析。

Conclusion: 该方法成功统一了PCA的直观可解释性与神经网络的非线性建模能力，提供了一种强大且实用的非线性降维解决方案。

Abstract: Principal Component Analysis (PCA) is a powerful and popular dimensionality reduction technique. However, due to its linear nature, it often fails to capture the complex underlying structure of real-world data. While Kernel PCA (kPCA) addresses non-linearity, it sacrifices interpretability and struggles with hyperparameter selection. In this paper, we propose a robust non-linear PCA framework that unifies the interpretability of PCA with the flexibility of neural networks. Our method parametrizes variable transformations via neural networks, optimized using Evolution Strategies (ES) to handle the non-differentiability of eigendecomposition. We introduce a novel, granular objective function that maximizes the individual variance contribution of each variable providing a stronger learning signal than global variance maximization. This approach natively handles categorical and ordinal variables without the dimensional explosion associated with one-hot encoding. We demonstrate that our method significantly outperforms both linear PCA and kPCA in explained variance across synthetic and real-world datasets. At the same time, it preserves PCA's interpretability, enabling visualization and analysis of feature contributions using standard tools such as biplots. The code can be found on GitHub.

</details>


### [19] [DeXposure-FM: A Time-series, Graph Foundation Model for Credit Exposures and Stability on Decentralized Financial Networks](https://arxiv.org/abs/2602.03981)
*Aijie Shu,Wenbin Wu,Gbenga Ibikunle,Fengxiang He*

Main category: cs.LG

TL;DR: DeXposure-FM是首个用于测量和预测DeFi网络协议间信用风险敞口的时间序列图基础模型，通过图-表格编码器和多任务头，在包含4300+协议、24300+代币的大规模数据集上训练，在风险预测和金融经济工具方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi中的信用风险敞口通常是隐性的、代币中介的，形成了密集的跨协议依赖网络。单一代币的冲击可能导致严重的传染效应。随着DeFi与传统金融基础设施（如稳定币）日益关联，这种动态风险需要更强大的量化工具来评估。

Method: 提出DeXposure-FM模型：1）采用图-表格编码器，使用预训练权重初始化；2）配备多个任务特定头；3）在DeXposure数据集上训练（4370万数据条目，覆盖602条区块链上的4300+协议和24300+独特代币）；4）训练目标为信用风险敞口预测，包括协议级资金流以及信用风险敞口链接的拓扑和权重联合动态预测。

Result: 1）在两个机器学习基准测试中持续优于最先进方法（包括图基础模型和时序图神经网络）；2）提供金融经济工具，支持宏观审慎监控和基于场景的DeFi压力测试，包括协议级系统重要性评分、部门级溢出和集中度测量；3）实证验证完全支持金融经济工具的有效性。

Conclusion: DeXposure-FM是首个用于DeFi网络协议间信用风险敞口测量和预测的时间序列图基础模型，在预测性能和金融经济工具方面表现出色，为DeFi风险管理和监控提供了强大工具。模型和代码已公开。

Abstract: Credit exposure in Decentralized Finance (DeFi) is often implicit and token-mediated, creating a dense web of inter-protocol dependencies. Thus, a shock to one token may result in significant and uncontrolled contagion effects. As the DeFi ecosystem becomes increasingly linked with traditional financial infrastructure through instruments, such as stablecoins, the risk posed by this dynamic demands more powerful quantification tools. We introduce DeXposure-FM, the first time-series, graph foundation model for measuring and forecasting inter-protocol credit exposure on DeFi networks, to the best of our knowledge. Employing a graph-tabular encoder, with pre-trained weight initialization, and multiple task-specific heads, DeXposure-FM is trained on the DeXposure dataset that has 43.7 million data entries, across 4,300+ protocols on 602 blockchains, covering 24,300+ unique tokens. The training is operationalized for credit-exposure forecasting, predicting the joint dynamics of (1) protocol-level flows, and (2) the topology and weights of credit-exposure links. The DeXposure-FM is empirically validated on two machine learning benchmarks; it consistently outperforms the state-of-the-art approaches, including a graph foundation model and temporal graph neural networks. DeXposure-FM further produces financial economics tools that support macroprudential monitoring and scenario-based DeFi stress testing, by enabling protocol-level systemic-importance scores, sector-level spillover and concentration measures via a forecast-then-measure pipeline. Empirical verification fully supports our financial economics tools. The model and code have been publicly available. Model: https://huggingface.co/EVIEHub/DeXposure-FM.
Code: https://github.com/EVIEHub/DeXposure-FM.

</details>


### [20] [eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models](https://arxiv.org/abs/2602.03986)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: cs.LG

TL;DR: 研究如何通过群对称化预训练模型来改进共形预测，利用几何信息分布非共形质量，减少不确定性区域


<details>
  <summary>Details</summary>
Motivation: 共形预测在长时任务中不确定性区域会显著增大，导致统计保证变得不具信息性，需要改进

Method: 通过群平均预训练预测器，将非共形质量分布在轨道上，每个样本作为轨道的代表，利用对称群元素缓解不确定性

Result: 理论上证明能产生收缩的非共形分数，改善指数尾界，在期望上获得更尖锐的共形预测集，特别是在高置信水平下

Conclusion: 提出了一种利用群对称化改进共形预测的方法，并通过行人轨迹预测实验设计验证理论主张

Abstract: We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.

</details>


### [21] [When Chains of Thought Don't Matter: Causal Bypass in Large Language Models](https://arxiv.org/abs/2602.03994)
*Anish Sathyanarayanan,Aditya Nagarsekar,Aarush Rathore*

Main category: cs.LG

TL;DR: 研究发现CoT提示中的推理内容与模型最终答案之间缺乏因果依赖，即使推理看起来合理，模型答案往往通过绕过推理的"旁路电路"产生。


<details>
  <summary>Details</summary>
Motivation: 验证CoT提示是否真正暴露模型推理过程并提高透明度，检验表面合规的推理是否保证因果依赖。

Method: 提出诊断框架：包含(i)可解释行为模块评分CoT文本中的操纵信号，(ii)因果探针通过隐藏状态修补测量CoT介导影响(CMI)，计算旁路分数量化答案独立于推理的程度。

Result: 即使审计感知提示增加可检测操纵信号，因果探针显示任务依赖性中介：许多QA项目显示近乎完全旁路(CMI≈0)，而某些逻辑问题显示较强中介(CMI高达0.56)。层分析显示即使平均CMI低，也存在狭窄且任务依赖的"推理窗口"。

Conclusion: CoT提示中的推理内容与模型答案之间缺乏因果依赖，表面合规的推理不能保证模型真正依赖推理内容，需要更严格的因果验证方法。

Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.

</details>


### [22] [Rational ANOVA Networks](https://arxiv.org/abs/2602.04006)
*Jusheng Zhang,Ningyuan Liu,Qinhan Lyu,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: RAN提出了一种基于函数ANOVA分解和Padé有理逼近的新架构，将非线性建模为可学习的理性单元，通过严格正分母避免数值不稳定，在保持参数效率的同时实现更好的外推和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络将非线性视为固定原语（如ReLU），限制了可解释性和对函数类的精细控制。现有加性模型（如KANs）使用样条但存在计算效率低和边界不稳定问题。

Method: 基于函数ANOVA分解和Padé有理逼近，将f(x)建模为主效应和稀疏成对交互作用的组合，每个组件由稳定可学习的有理单元参数化，强制严格正分母避免极点和数值不稳定。

Result: 在控制函数基准和视觉分类任务（如CIFAR-10）中，在匹配参数和计算预算下，RAN达到或超过参数匹配的MLP和可学习激活基线，具有更好的稳定性和吞吐量。

Conclusion: RAN通过ANOVA结构提供显式低阶交互偏置以提高数据效率和可解释性，有理参数化显著改善外推能力，为神经网络设计提供了新的基础架构。

Abstract: Deep neural networks typically treat nonlinearities as fixed primitives (e.g., ReLU), limiting both interpretability and the granularity of control over the induced function class. While recent additive models (like KANs) attempt to address this using splines, they often suffer from computational inefficiency and boundary instability. We propose the Rational-ANOVA Network (RAN), a foundational architecture grounded in functional ANOVA decomposition and Padé-style rational approximation. RAN models f(x) as a composition of main effects and sparse pairwise interactions, where each component is parameterized by a stable, learnable rational unit. Crucially, we enforce a strictly positive denominator, which avoids poles and numerical instability while capturing sharp transitions and near-singular behaviors more efficiently than polynomial bases. This ANOVA structure provides an explicit low-order interaction bias for data efficiency and interpretability, while the rational parameterization significantly improves extrapolation. Across controlled function benchmarks and vision classification tasks (e.g., CIFAR-10) under matched parameter and compute budgets, RAN matches or surpasses parameter-matched MLPs and learnable-activation baselines, with better stability and throughput. Code is available at https://github.com/jushengzhang/Rational-ANOVA-Networks.git.

</details>


### [23] [PromptSplit: Revealing Prompt-Level Disagreement in Generative Models](https://arxiv.org/abs/2602.04009)
*Mehdi Lotfian,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出PromptSplit框架，通过核方法检测生成模型间的提示依赖分歧，识别导致不同模型行为的提示类型。


<details>
  <summary>Details</summary>
Motivation: 随着提示引导生成AI模型在视觉和语言领域的快速发展，不同模型在训练数据和架构上的差异导致对相同提示产生不同响应。需要系统方法来识别哪些提示类型会导致模型行为差异。

Method: 提出PromptSplit框架：为每对模型构建提示-输出的张量积嵌入表示，计算核协方差矩阵，通过加权差矩阵的特征空间识别行为差异的主要方向。采用随机投影近似降低计算复杂度至O(nr² + r³)。

Result: 理论分析显示随机投影近似的特征结构估计与全维结果的期望偏差有界为O(1/r²)。在文生图、文生文和图像描述任务上的实验表明，PromptSplit能准确检测真实行为差异并定位相关提示。

Conclusion: PromptSplit提供了一个可解释的工具，用于检测生成模型在哪些提示上存在分歧，有助于理解不同模型的行为差异模式。

Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.

</details>


### [24] [Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.04019)
*Yichen Xu,Yuyang Liang,Shan Dai,Tianyang Hu,Tsz Nam Chan,Chenhao Ma*

Main category: cs.LG

TL;DR: 该论文提出了Layer Card诊断工具，用于指导参数高效微调（PEFT）中的层选择策略，通过分析残差范数、激活能量和层耦合等指标，实现性能与成本的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，全参数微调成本过高，参数高效微调成为主流。然而当前PEFT通常均匀应用于所有层，缺乏对层选择的深入理解和有效利用，无法在推理延迟、微调成本和性能之间取得最佳平衡。

Method: 提出统一的投影残差视角分析PEFT，在局部二次近似下，通过三个关键量分析层适应：投影残差范数（衡量可纠正偏差）、激活能量（决定特征条件）、层耦合（量化层间残差交互）。基于此开发Layer Card诊断工具，总结每层的残差信号强度、计算成本和性能。

Result: 在Qwen3-8B模型上，Layer Card指导的层选择策略能够在保持接近全层LoRA性能的同时，显著降低微调成本，减少推理时适配器增强的层数，提供更具成本效益的替代方案。

Conclusion: Layer Card为PEFT层选择提供了系统化的诊断框架，使研究人员能够根据性能、成本等不同目标灵活优化层适应策略，为参数高效微调提供了更智能、更经济的方法。

Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.

</details>


### [25] [Group Contrastive Learning for Weakly Paired Multimodal Data](https://arxiv.org/abs/2602.04021)
*Aditya Gorla,Hugues Van Assel,Jan-Christian Huetter,Heming Yao,Kyunghyun Cho,Aviv Regev,Russell Littman*

Main category: cs.LG

TL;DR: GROOVE是一种半监督多模态表示学习方法，针对弱配对的高通量扰动数据，通过GroupCLIP损失函数和动态回译自编码器框架实现跨模态纠缠表示学习。


<details>
  <summary>Details</summary>
Motivation: 解决高通量扰动数据中样本跨模态仅通过共享扰动标签弱配对（缺乏直接对应关系）的问题，填补弱配对设置下对比学习的空白。

Method: 提出GroupCLIP（组级对比损失）结合动态回译自编码器框架，在共享潜在空间中鼓励跨模态纠缠表示同时保持组级一致性；引入组合评估框架系统评估表示学习方法。

Result: 在模拟数据和两个真实单细胞遗传扰动数据集上，GROOVE在下游跨模态匹配和插补任务中表现相当或优于现有方法；GroupCLIP是性能提升的关键组件。

Conclusion: 在仅弱配对可用的场景中，利用组级约束对于有效的多模态表示学习至关重要；目前尚无对齐器在所有设置或模态对中普遍占优。

Abstract: We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.

</details>


### [26] [A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs](https://arxiv.org/abs/2602.04027)
*Pratyush Uppuluri,Shilpa Noushad,Sajan Kumar*

Main category: cs.LG

TL;DR: 提出基于共识的贝叶斯框架，通过建模目录为话题、用户为多级交互图中的智能体，使用影响力加权意见动力学模拟访问演化，检测企业目录访问图中的恶意用户行为。


<details>
  <summary>Details</summary>
Motivation: 企业目录访问图中恶意用户行为的检测需要能够识别违反结构规范的逻辑不一致性，现有方法可能无法有效捕捉动态访问模式中的异常。

Method: 1) 将目录建模为话题，用户建模为多级交互图中的智能体；2) 使用影响力加权意见动力学模拟访问演化；3) 通过动态矩阵Ci编码用户间逻辑依赖关系；4) 通过共享影响力矩阵W捕获目录相似性；5) 恶意行为被建模为违反强连通组件结构规范的跨组件逻辑扰动；6) 应用意见动力学理论保证话题收敛；7) 通过缩放意见方差检测异常；8) 引入随时间演化的贝叶斯异常评分机制，使用静态和在线先验。

Result: 在合成访问图上的仿真验证了该方法，证明其对逻辑不一致性的敏感性以及在动态扰动下的鲁棒性。

Conclusion: 该框架通过结合共识动力学和贝叶斯推理，能够有效检测企业目录访问图中的恶意用户行为，特别擅长识别违反结构规范的逻辑异常。

Abstract: This work presents a consensus-based Bayesian framework to detect malicious user behavior in enterprise directory access graphs. By modeling directories as topics and users as agents within a multi-level interaction graph, we simulate access evolution using influence-weighted opinion dynamics. Logical dependencies between users are encoded in dynamic matrices Ci, and directory similarity is captured via a shared influence matrix W. Malicious behavior is injected as cross-component logical perturbations that violate structural norms of strongly connected components(SCCs). We apply theoretical guarantees from opinion dynamics literature to determine topic convergence and detect anomaly via scaled opinion variance. To quantify uncertainty, we introduce a Bayesian anomaly scoring mechanism that evolves over time, using both static and online priors. Simulations over synthetic access graphs validate our method, demonstrating its sensitivity to logical inconsistencies and robustness under dynamic perturbation.

</details>


### [27] [The Illusion of Generalization: Re-examining Tabular Language Model Evaluation](https://arxiv.org/abs/2602.04031)
*Aditya Gorla,Ratish Puduppully*

Main category: cs.LG

TL;DR: 该研究对Tabula-8B进行系统重评估，发现其声称的泛化能力主要源于评估缺陷而非真正的表格推理能力


<details>
  <summary>Details</summary>
Motivation: 重新评估表格语言模型（TLMs）声称的涌现泛化能力，验证这些模型是否真正学会了表格推理，还是仅仅是评估方法的问题

Method: 使用UniPredict基准的165个数据集对Tabula-8B进行系统评估，分析其在不同类型分类任务上的表现，检查数据污染问题，并进行指令调优实验

Result: 1. 二元和分类分类的中位数提升接近零，强性能完全由四分位数分类任务驱动；2. 表现最佳的数据集存在普遍的数据污染，包括完整的训练-测试重叠和任务级泄漏；3. 无表格暴露的指令调优恢复了92.2%的标准分类性能，格式熟悉度弥补了71.3%的性能差距

Conclusion: TLMs声称的泛化能力可能反映了评估伪影而非学习的表格推理，需要加强TLM评估方法以避免数据污染和任务泄漏问题

Abstract: Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.

</details>


### [28] [DADP: Domain Adaptive Diffusion Policy](https://arxiv.org/abs/2602.04037)
*Pengcheng Wang,Qinghang Liu,Haotian Lin,Yiheng Li,Guojian Zhan,Masayoshi Tomizuka,Yixiao Wang*

Main category: cs.LG

TL;DR: DADP通过滞后上下文动态预测实现无监督解耦，将静态域信息与动态特性分离，并通过扩散注入实现鲁棒的零样本域适应


<details>
  <summary>Details</summary>
Motivation: 现有域表示学习方法在捕获域特定信息时，会将静态域信息与变化的动态特性纠缠在一起，这会混淆条件策略，限制零样本适应能力

Method: 提出DADP：1) 滞后上下文动态预测 - 使用历史偏移上下文进行未来状态估计，通过增加时间间隔无监督解耦静态域表示；2) 域感知扩散注入 - 将学习到的域表示直接集成到生成过程中，通过偏置先验分布和重新制定扩散目标

Result: 在运动和操作等挑战性基准测试中表现出优越性能，相比先前方法具有更好的泛化能力

Conclusion: DADP通过解耦静态域表示和域感知扩散注入，实现了鲁棒的零样本域适应，在复杂控制任务中优于现有方法

Abstract: Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.

</details>


### [29] [Partition Trees: Conditional Density Estimation over General Outcome Spaces](https://arxiv.org/abs/2602.04042)
*Felipe Angelim,Alessandro Leite*

Main category: cs.LG

TL;DR: 提出Partition Trees，一种基于树的框架，用于一般结果空间上的条件密度估计，支持连续和分类变量，通过最小化条件负对数似然学习数据自适应划分上的分段常数密度。


<details>
  <summary>Details</summary>
Motivation: 现有概率树方法通常对目标分布做出参数假设，缺乏一个统一的框架来处理连续和分类变量的条件密度估计。需要一种可扩展的非参数替代方案。

Method: 将条件分布建模为数据自适应划分上的分段常数密度，通过直接最小化条件负对数似然来学习树结构。进一步提出Partition Forests，通过平均条件密度实现集成扩展。

Result: 实验表明，该方法在概率预测方面优于CART风格树，与最先进的概率树方法和随机森林相比具有竞争性或更优性能，同时对冗余特征和异方差噪声具有鲁棒性。

Conclusion: Partition Trees提供了一个统一的、非参数的树框架，用于一般结果空间的条件密度估计，支持连续和分类变量，在概率预测方面表现出色且具有鲁棒性。

Abstract: We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.

</details>


### [30] [SEIS: Subspace-based Equivariance and Invariance Scores for Neural Representations](https://arxiv.org/abs/2602.04054)
*Huahua Lin,Katayoun Farrahi,Xiaohao Cai*

Main category: cs.LG

TL;DR: SEIS是一种基于子空间的度量方法，用于分析神经网络在几何变换下的特征表示，能够区分等变性和不变性，无需标签或变换知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过比较变换输入下的模型输出来评估鲁棒性，但无法深入了解几何信息在内部表示中的组织方式，也无法区分信息丢失与重新编码。

Method: 提出SEIS（基于子空间的等变性和不变性评分），这是一种子空间度量方法，用于分析层间特征表示在几何变换下的行为，能够解耦等变性和不变性。

Result: 合成验证表明SEIS能正确恢复已知变换；应用于训练好的分类网络显示：早期层呈现等变性，深层呈现不变性；数据增强增加不变性同时保持等变性；多任务学习在共享编码器中协同提升两种属性；跳跃连接恢复解码过程中丢失的等变性。

Conclusion: SEIS提供了一种无需监督的方法来分析神经网络表示中的几何结构，揭示了网络如何处理几何信息，并为理解表示学习提供了新工具。

Abstract: Understanding how neural representations respond to geometric transformations is essential for evaluating whether learned features preserve meaningful spatial structure. Existing approaches primarily assess robustness by comparing model outputs under transformed inputs, offering limited insight into how geometric information is organized within internal representations and failing to distinguish between information loss and re-encoding. In this work, we introduce SEIS (Subspace-based Equivariance and Invariance Scores), a subspace metric for analyzing layer-wise feature representations under geometric transformations, disentangling equivariance from invariance without requiring labels or explicit knowledge of the transformation. Synthetic validation confirms that SEIS correctly recovers known transformations. Applied to trained classification networks, SEIS reveals a transition from equivariance in early layers to invariance in deeper layers, and that data augmentation increases invariance while preserving equivariance. We further show that multi-task learning induces synergistic gains in both properties at the shared encoder, and skip connections restore equivariance lost during decoding.

</details>


### [31] [An Empirical Survey and Benchmark of Learned Distance Indexes for Road Networks](https://arxiv.org/abs/2602.04068)
*Gautam Choudhary,Libin Zhou,Yeasir Rayhan,Walid G. Aref*

Main category: cs.LG

TL;DR: 该论文首次对基于机器学习的道路网络距离索引进行系统性实证评估，比较了10种ML技术与经典非ML方法在训练时间、查询延迟、存储和准确性四个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 道路网络中最短路径距离计算是导航系统、位置服务和空间分析的核心操作。虽然经典算法（如Dijkstra算法）能提供精确答案，但其延迟对于现代实时大规模部署来说过高。近年来，随着机器学习的发展，研究人员设计了基于ML的距离索引来高效回答近似最短路径和距离查询，但缺乏对这些ML方法的全面系统性评估。

Method: 使用七个真实世界道路网络和基于轨迹数据的工作负载驱动查询数据集，对十种代表性ML技术进行基准测试，并与强大的经典非ML基线方法进行比较。评估沿四个关键维度进行：训练时间、查询延迟、存储和准确性。

Result: 论文提供了对ML距离索引的全面实证评估，揭示了关键见解和实际权衡。作者发布了统一的开源代码库，支持可重复性和未来对学习距离索引的研究。

Conclusion: 这是首次对基于机器学习的道路网络距离索引进行系统性实证调查，填补了该领域的研究空白，为研究人员和实践者提供了实用的评估框架和基准。

Abstract: The calculation of shortest-path distances in road networks is a core operation in navigation systems, location-based services, and spatial analytics. Although classical algorithms, e.g., Dijkstra's algorithm, provide exact answers, their latency is prohibitive for modern real-time, large-scale deployments. Over the past two decades, numerous distance indexes have been proposed to speed up query processing for shortest distance queries. More recently, with the advancement in machine learning (ML), researchers have designed and proposed ML-based distance indexes to answer approximate shortest path and distance queries efficiently. However, a comprehensive and systematic evaluation of these ML-based approaches is lacking. This paper presents the first empirical survey of ML-based distance indexes on road networks, evaluating them along four key dimensions: Training time, query latency, storage, and accuracy. Using seven real-world road networks and workload-driven query datasets derived from trajectory data, we benchmark ten representative ML techniques and compare them against strong classical non-ML baselines, highlighting key insights and practical trade-offs. We release a unified open-source codebase to support reproducibility and future research on learned distance indexes.

</details>


### [32] [Agentic AI-Empowered Dynamic Survey Framework](https://arxiv.org/abs/2602.04071)
*Furkan Mumcu,Lokman Bekit,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 提出动态调查框架，将综述论文视为活文档而非一次性生成任务，通过智能代理持续更新现有综述，整合新研究同时保持结构完整性。


<details>
  <summary>Details</summary>
Motivation: 研究产出快速增长导致综述论文迅速过时，造成文献冗余和碎片化。传统综述作为一次性任务无法跟上研究发展速度，需要重新思考综述的维护模式。

Method: 提出基于智能代理的动态调查框架，将综述视为活文档，支持现有综述的持续更新，增量整合新研究，同时保持综述结构和最小化不必要干扰。

Result: 通过回顾性实验验证，该框架能有效识别和整合新兴研究，同时保持现有综述的连贯性和结构完整性。

Conclusion: 综述写作应重新定义为长期维护问题而非一次性生成任务，动态调查框架为解决综述过时问题提供了可行方案，支持综述与所描述研究同步演进。

Abstract: Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.

</details>


### [33] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: BLUM框架利用脑损伤-症状映射作为外部验证标准，通过比较LLM扰动后的错误模式与中风失语症患者的脑损伤模式，发现两者具有显著相似性，为LLM可解释性提供了新的神经科学验证方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM可解释性方法主要依赖内部指标，缺乏外部验证。研究者希望建立一种能够因果验证LLM组件功能必要性的方法，借鉴临床神经科学中已确立的脑损伤-症状映射这一金标准。

Method: 提出BLUM框架：1) 使用410名中风后失语症患者数据训练症状-损伤模型；2) 对transformer层进行系统性扰动；3) 对扰动后的LLM和人类患者进行相同的临床评估；4) 将LLM错误模式投影到人类脑损伤空间进行比较。

Result: LLM错误模式与人类错误模式足够相似，在67%的图片命名条件和68.3%的句子完成条件下，预测的脑损伤位置与错误匹配的人类实际损伤位置显著相关（p<10^{-23}和p<10^{-61}）。语义主导错误映射到腹侧通路损伤模式，音位主导错误映射到背侧通路模式。

Conclusion: 该研究为LLM可解释性开辟了新方法学途径，临床神经科学提供了外部验证，建立了人类脑损伤-症状映射作为评估人工语言系统的参考框架，并推动研究行为对齐是否反映共享的计算原理。

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [34] [Principles of Lipschitz continuity in neural networks](https://arxiv.org/abs/2602.04078)
*Róisín Luo*

Main category: cs.LG

TL;DR: 该论文探讨了深度学习中鲁棒性和泛化性的理论基础，重点研究Lipschitz连续性在神经网络中的原理性理解，从训练动态和频率信号传播两个互补视角进行分析。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习取得了显著成功，但在面对输入扰动和分布外数据时仍存在鲁棒性和泛化性挑战。现有研究多关注基于Lipschitz约束的经验正则化方法，而对其底层原理探索不足，需要从更基础的理论层面理解Lipschitz连续性如何影响神经网络性能。

Method: 采用两个互补的研究视角：1）内部视角：分析神经网络训练过程中Lipschitz连续性的时间演化（训练动态）；2）外部视角：研究Lipschitz连续性如何调节神经网络对输入数据特征的行为，特别是其在控制频率信号传播中的作用。

Result: 论文未提供具体实验结果，但从摘要可推断该研究将建立Lipschitz连续性与神经网络鲁棒性、泛化性之间的理论联系，揭示训练过程中Lipschitz特性的演化规律，以及其对频率信号传播的调制机制。

Conclusion: 通过从内部训练动态和外部频率信号传播两个角度系统研究Lipschitz连续性，该论文旨在为深度学习的鲁棒性和泛化性提供更深入的理论基础，推动从经验方法向原理性理解的转变。

Abstract: Deep learning has achieved remarkable success across a wide range of domains, significantly expanding the frontiers of what is achievable in artificial intelligence. Yet, despite these advances, critical challenges remain -- most notably, ensuring robustness to small input perturbations and generalization to out-of-distribution data. These critical challenges underscore the need to understand the underlying fundamental principles that govern robustness and generalization. Among the theoretical tools available, Lipschitz continuity plays a pivotal role in governing the fundamental properties of neural networks related to robustness and generalization. It quantifies the worst-case sensitivity of network's outputs to small input perturbations. While its importance is widely acknowledged, prior research has predominantly focused on empirical regularization approaches based on Lipschitz constraints, leaving the underlying principles less explored. This thesis seeks to advance a principled understanding of the principles of Lipschitz continuity in neural networks within the paradigm of machine learning, examined from two complementary perspectives: an internal perspective -- focusing on the temporal evolution of Lipschitz continuity in neural networks during training (i.e., training dynamics); and an external perspective -- investigating how Lipschitz continuity modulates the behavior of neural networks with respect to features in the input data, particularly its role in governing frequency signal propagation (i.e., modulation of frequency signal propagation).

</details>


### [35] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: 提出概率神经算子框架，使用基于分数的条件扩散算子解决高频波现象近似问题，相比确定性方法在L2、H1和能量范数上表现更优，并能捕捉输入不确定性。


<details>
  <summary>Details</summary>
Motivation: 确定性神经算子在处理高频波现象时存在困难，因为输入到输出的强敏感性使算子学习具有挑战性，且谱偏差会模糊振荡。需要一种概率方法来近似高频波。

Method: 采用概率框架，使用基于分数的条件扩散算子。首先对Helmholtz算子进行稳定性分析，然后通过数值实验验证方法有效性。

Result: 概率神经算子在L2、H1和能量范数上产生最稳健的预测和最低误差。相比其他确定性方法，能有效捕捉输入声速图传播到解场的不确定性。

Conclusion: 概率算子学习是解决Helmholtz等复杂PDE在高频挑战性区域的有原则且有效的方法，为高频波现象提供了更好的近似框架。

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [36] [Federated Concept-Based Models: Interpretable models with distributed supervision](https://arxiv.org/abs/2602.04093)
*Dario Fenoglio,Arianna Casanova,Francesco De Santis,Mohan Li,Gabriele Dominici,Johannes Schneider,Martin Gjoreski,Marc Langheinrich,Pietro Barbiero,Giovanni De Felice*

Main category: cs.LG

TL;DR: 提出Federated Concept-based Models (F-CMs)，将概念模型与联邦学习结合，解决跨机构概念标注稀缺和异构性问题，实现隐私保护下的可解释深度学习


<details>
  <summary>Details</summary>
Motivation: 概念模型能提升深度学习可解释性，但概念标注昂贵且单一数据源难以大规模获取。联邦学习可跨机构利用分布式概念标注，但缺乏可解释建模范式。现有概念模型假设固定概念空间和架构，与真实世界联邦学习的异构性、非平稳性（机构动态加入、新监督信号）不兼容

Method: 提出F-CMs方法：1）跨机构聚合概念层信息；2）根据可用概念监督动态高效调整模型架构；3）保护机构隐私。实现概念模型在演化联邦学习环境中的部署

Result: F-CMs在保持全概念监督训练准确性和干预有效性的同时，优于非自适应联邦基线。关键创新：使机构能对自身未标注的概念进行可解释推理

Conclusion: F-CMs成功将概念模型整合到联邦学习中，解决了概念标注稀缺和异构性问题，实现了隐私保护下的可解释深度学习，为跨机构协作提供新范式

Abstract: Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.

</details>


### [37] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: CoRe框架通过探测token对上下文扰动的敏感性来识别和修正不稳定的token，在推理时进行训练无关的修订，显著提升MDM在推理和代码任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型的标准解码存在上下文僵化问题：token基于瞬时的置信度被保留，但早期预测缺乏完整上下文，导致初始不一致会误导后续生成。现有的修订策略依赖静态置信度分数，但这些信号本质上是短视的，不一致的token可能对模型本身显得很自信。

Method: 提出Context-Robust Remasking (CoRe)，一个无需训练的推理时修订框架。不依赖静态token概率，而是通过探测token对目标掩码上下文扰动的敏感性来识别上下文脆弱的token。将修订形式化为对上下文变化的鲁棒优化目标，并高效近似该目标以优先修订不稳定token。

Result: 在LLaDA-8B-Base模型上，CoRe在推理和代码基准测试中带来一致改进，优于计算匹配的基线方法，并将MBPP性能提升高达9.2个百分点。

Conclusion: CoRe通过动态评估token对上下文扰动的敏感性来解决MDM中的上下文僵化问题，提供了一种有效的训练无关推理时修订方法，显著提升了模型性能。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [38] [Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs](https://arxiv.org/abs/2602.04099)
*Letian Cheng,Junyan Wang,Yan Gao,Elliott Wen,Ting Dang,Hong Jia*

Main category: cs.LG

TL;DR: 论文提出LengthBenchmark框架，系统研究输入长度对LLM困惑度评估的影响，发现长度偏差普遍存在且影响跨模型公平比较


<details>
  <summary>Details</summary>
Motivation: 困惑度作为LLM评估的常用指标，在处理长输入时不可靠，但输入长度对困惑度的影响尚未从系统角度系统研究，也很少被视为影响公平性和效率的首要系统变量

Method: 提出LengthBenchmark框架，将输入长度、评估协议设计和系统级成本明确整合，在两种评分协议（直接累积和固定窗口滑动）下评估代表性LLM在不同上下文长度下的表现，并测量延迟、内存占用和评估成本

Result: 发现两个关键观察：(i)滑动窗口评估在短输入上持续夸大性能，(ii)全精度和量化模型随着评估片段长度增加都表现出性能提升，长度偏差是普遍现象

Conclusion: 长度偏差是普遍现象，会破坏跨模型公平比较，需要系统级评估框架来考虑输入长度对评估公平性和效率的影响

Abstract: Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.

</details>


### [39] [Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.04107)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 提出基于有限块长分析和有损压缩框架的信息论泛化理论，将训练数据采样视为编码、模型构建视为解码，推导出样本复杂度和泛化误差下界


<details>
  <summary>Details</summary>
Motivation: 现有泛化理论框架未能明确分离过拟合程度与归纳偏置-任务不匹配这两个关键因素，需要一个新的理论框架来统一信息论界和稳定性理论

Method: 将有损压缩和有限块长分析应用于机器学习泛化问题，将训练数据采样形式化为编码过程，模型构建形式化为解码过程，推导出样本复杂度和泛化误差下界

Result: 推导出的下界明确分离了学习算法的过拟合程度和归纳偏置-任务不匹配两个独立项，并将过拟合项分解为与现有信息论界和稳定性理论度量的理论联系

Conclusion: 提出的信息论框架为泛化分析提供了新的视角，统一了现有理论，明确分离了影响泛化的关键因素，具有超越现有框架的显著优势

Abstract: This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.

</details>


### [40] [Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery](https://arxiv.org/abs/2602.04110)
*Raymond Chu,Jaewoong Choi,Dohyun Kwon*

Main category: cs.LG

TL;DR: 论文分析了半对偶神经最优传输在低维流形数据上的伪解问题，提出了基于加性噪声平滑的解决方案，并推导出最优统计速率的可计算终端噪声水平。


<details>
  <summary>Details</summary>
Motivation: 半对偶神经最优传输通过最大最小目标学习传输映射，但训练可能收敛到错误或退化解。当数据集中在低维流形时，目标在数据流形外约束不足，而在流形上的传输信号仍可识别，需要解决这一问题。

Method: 采用加性噪声平滑作为补救措施，通过理论分析（i）最优计划的定量稳定性，（ii）平滑引起的偏差，以及（iii）有限样本误差，推导出由数据内在维度m控制的统计速率。

Result: 提出了可计算的终端噪声水平ε_stat(N)，达到最优统计速率，其缩放由数据内在维度m控制。同时发现简化半对偶目标在ε↓0时条件数变差，提供了原则性停止规则。

Conclusion: 加性噪声平滑能有效恢复低维流形数据上的传输映射，但需要合理选择噪声水平。低于ε_stat(N)的退火会恶化优化条件而不改善统计精度，因此ε_stat(N)提供了最优停止点。

Abstract: Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\varepsilon_{\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\varepsilon \downarrow 0$. This provides a principled stopping rule: annealing below $\varepsilon_{\mathrm{stat}}(N)$ can $\textit{worsen}$ optimization conditioning without improving statistical accuracy.

</details>


### [41] [Turning mechanistic models into forecasters by using machine learning](https://arxiv.org/abs/2602.04114)
*Amit K. Chakraborty,Hao Wang,Pouria Ramazi*

Main category: cs.LG

TL;DR: 提出一种数据驱动方法，通过引入时变参数来发现复杂系统的微分方程，并将其转化为预测模型，在多个数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法通常假设参数时不变，无法捕捉系统动态演化。当系统机制未知且参数随时间变化时，现有方法存在局限性。

Method: 允许部分参数随时间变化，直接从数据中学习其时变演化，推断包含常参数和时变参数的系统方程。然后将此框架转化为预测模型，通过预测时变参数并代入学习到的方程进行预测。

Result: 在SIR、消费者-资源、温室气体浓度和蓝藻细胞计数数据集上验证，学习时间序列的平均绝对误差低于3%，提前一个月预测的误差低于6%。相比CNN-LSTM和梯度提升机，在大多数数据集上表现更优。

Conclusion: 将时变参数整合到数据驱动的微分方程发现中，能同时提高建模精度和预测性能，更好地适应系统动态变化。

Abstract: The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\% for learning a time series and below 6\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.

</details>


### [42] [Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach](https://arxiv.org/abs/2602.04116)
*Sicheng Liu,Xunkai Li,Daohan Su,Ru Zhang,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: PLANET提出了一种新的多模态图基础模型框架，通过分治策略解决现有模型在模态交互和模态对齐方面的不足，显著提升了多模态图任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型主要关注文本属性图，而多模态属性图尚未充分开发。当前多模态图基础模型存在两个根本性局限：1)未能显式建模模态交互，无法捕捉复杂的跨模态语义；2)模态对齐效果不佳，难以弥合不同模态空间间的语义鸿沟。

Method: PLANET采用分治策略，在不同粒度上解耦模态交互和对齐：1)在嵌入粒度上，通过嵌入级域门控进行局部语义增强，自适应注入拓扑感知的跨模态上下文；2)在节点粒度上，通过节点级离散化检索构建离散语义表示空间，确保全局模态对齐。

Result: 大量实验表明，PLANET在多种图中心和生成式多模态任务上显著优于现有最先进的基线方法。

Conclusion: PLANET通过创新的分治策略有效解决了多模态图基础模型中的模态交互和对齐问题，为多模态属性图的建模提供了强大框架，扩展了图基础模型的应用范围。

Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.

</details>


### [43] [Learning to Reason in 13 Parameters](https://arxiv.org/abs/2602.04118)
*John X. Morris,Niloofar Mireshghallah,Mark Ibrahim,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: TinyLoRA：一种只需训练13个参数就能让8B模型在GSM8K上达到91%准确率的超低秩适配器方法，比传统LoRA参数少1000倍，且RL训练效果远优于SFT。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA无法扩展到低于模型维度的秩，作者质疑是否真的需要rank=1的LoRA来学习推理，因此探索更极端的低秩参数化方法。

Method: 提出TinyLoRA方法，将低秩适配器扩展到小至单个参数的规模，在bf16精度下仅用13个训练参数（26字节）就能训练8B参数的Qwen2.5模型。

Result: 在GSM8K上达到91%准确率；在AIME、AMC、MATH500等更难的学习推理基准上，仅用1000倍更少的参数就能恢复90%的性能提升；RL训练效果显著优于SFT。

Conclusion: TinyLoRA展示了极端低秩参数化的可行性，挑战了传统LoRA的规模限制，为高效模型微调提供了新方向，特别是RL在极低参数训练中的优势。

Abstract: Recent research has shown that language models can learn to \textit{reason}, often via reinforcement learning. Some work even trains low-rank parameterizations for reasoning, but conventional LoRA cannot scale below the model dimension. We question whether even rank=1 LoRA is necessary for learning to reason and propose TinyLoRA, a method for scaling low-rank adapters to sizes as small as one parameter. Within our new parameterization, we are able to train the 8B parameter size of Qwen2.5 to 91\% accuracy on GSM8K with only 13 trained parameters in bf16 (26 total bytes). We find this trend holds in general: we are able to recover 90\% of performance improvements while training $1000x$ fewer parameters across a suite of more difficult learning-to-reason benchmarks such as AIME, AMC, and MATH500. Notably, we are only able to achieve such strong performance with RL: models trained using SFT require $100-1000x$ larger updates to reach the same performance.

</details>


### [44] [Synthesizable Molecular Generation via Soft-constrained GFlowNets with Rich Chemical Priors](https://arxiv.org/abs/2602.04119)
*Hyeonah Kim,Minsu Kim,Celine Roget,Dionessa Biton,Louis Vaillancourt,Yves V. Brun,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: S3-GFN：通过序列式GFlowNet的简单软正则化生成可合成SMILES分子，无需硬性合成约束，实现高可合成性（≥95%）和高奖励分子生成


<details>
  <summary>Details</summary>
Motivation: 生成模型在实验性药物发现中的应用受到严重限制，因为设计实际可合成的全新分子非常困难。现有基于硬合成约束（预定义反应模板和构建块）的GFlowNet方法缺乏灵活性和可扩展性。

Method: 提出S3-GFN方法，通过序列式GFlowNet的简单软正则化生成可合成SMILES分子。利用从大规模SMILES语料库学习到的丰富分子先验，引导分子生成朝向高奖励、可合成的化学空间。通过基于可合成和不可合成样本缓冲区的对比学习信号进行离策略回放训练来施加约束。

Result: 实验表明S3-GFN能够学习生成高可合成性分子（≥95%），在多样化任务中获得更高奖励。

Conclusion: S3-GFN提供了一种灵活可扩展的替代方案，通过软正则化而非硬约束实现可合成分子生成，克服了现有方法的局限性。

Abstract: The application of generative models for experimental drug discovery campaigns is severely limited by the difficulty of designing molecules de novo that can be synthesized in practice. Previous works have leveraged Generative Flow Networks (GFlowNets) to impose hard synthesizability constraints through the design of state and action spaces based on predefined reaction templates and building blocks. Despite the promising prospects of this approach, it currently lacks flexibility and scalability. As an alternative, we propose S3-GFN, which generates synthesizable SMILES molecules via simple soft regularization of a sequence-based GFlowNet. Our approach leverages rich molecular priors learned from large-scale SMILES corpora to steer molecular generation towards high-reward, synthesizable chemical spaces. The model induces constraints through off-policy replay training with a contrastive learning signal based on separate buffers of synthesizable and unsynthesizable samples. Our experiments show that S3-GFN learns to generate synthesizable molecules ($\geq 95\%$) with higher rewards in diverse tasks.

</details>


### [45] [Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting](https://arxiv.org/abs/2602.04131)
*Mehrdad Moghimi,Anthony Coache,Hyejin Ku*

Main category: cs.LG

TL;DR: 提出了一种支持灵活折扣和风险度量的分布强化学习新框架，解决了传统指数折扣无法充分捕捉时间偏好的问题


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，折扣因子通常被忽视或作为固定参数处理，但折扣函数对表征智能体时间偏好至关重要，传统指数折扣无法完全捕捉这些偏好

Method: 提出了支持未来奖励灵活折扣和风险度量优化的分布强化学习新框架，包含多时间范围扩展以解决现有方法的问题

Result: 通过广泛实验验证了方法的鲁棒性，证明多时间范围扩展能解决现有方法的问题，折扣是决策问题中捕捉更丰富时间和风险偏好的关键

Conclusion: 折扣是决策问题中捕捉更丰富时间和风险偏好的基石，对现实世界安全关键应用具有潜在影响

Abstract: Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.

</details>


### [46] [Generative Neural Operators through Diffusion Last Layer](https://arxiv.org/abs/2602.04139)
*Sungwon Park,Anthony Zhou,Hongjoong Kim,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出扩散最后一层（DLL）作为神经算子的轻量级概率头部，用于建模预测不确定性，在随机PDE算子学习中提升泛化能力和不确定性感知预测。


<details>
  <summary>Details</summary>
Motivation: 许多实际系统本质上是随机的，需要可靠的不确定性量化。现有神经算子缺乏对不确定性的建模能力，而PDE解分布通常具有相对平滑和低维结构的特点，这为高效的不确定性建模提供了机会。

Method: 提出扩散最后一层（DLL），这是一个轻量级概率头部，可附加到任意神经算子骨干上。DLL通过低秩Karhunen-Loève展开直接在函数空间中参数化条件输出分布，实现高效且表达力强的不确定性建模。

Result: 在随机PDE算子学习基准测试中，DLL提升了泛化能力和不确定性感知预测。即使在确定性长时程推演设置中，DLL也增强了推演稳定性，并为骨干神经算子提供了有意义的本体不确定性估计。

Conclusion: DLL是一种简单有效的附加模块，能够为神经算子提供概率建模能力，在随机系统和确定性系统中都能提升性能并提供可靠的不确定性量化。

Abstract: Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.

</details>


### [47] [Training Data Efficiency in Multimodal Process Reward Models](https://arxiv.org/abs/2602.04145)
*Jinyuan Li,Chengsong Huang,Langlin Huang,Shaoyang Xu,Haolin Liu,Wenxuan Zhang,Jiaxin Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为平衡信息分数（BIS）的数据选择方法，用于高效训练多模态过程奖励模型（MPRMs），仅需10%的训练数据即可达到全数据性能。


<details>
  <summary>Details</summary>
Motivation: 训练多模态过程奖励模型（MPRMs）通常需要大规模蒙特卡洛（MC）标注数据集，成本高昂。研究发现MPRM训练在随机子采样下很快饱和，表明现有MC标注数据集中存在大量冗余，需要提高数据效率。

Method: 首先通过理论框架分析发现，信息梯度更新取决于两个因素：正负步骤的标签混合和标签可靠性（正步骤的平均MC分数）。基于此提出平衡信息分数（BIS），在rollout级别基于现有MC信号同时优先考虑混合性和可靠性，无需额外成本。

Result: 在两个骨干模型（InternVL2.5-8B和Qwen2.5-VL-7B）上，在VisualProcessBench上，BIS选择的子集在很小比例下就能匹配甚至超越全数据性能。BIS子集仅使用10%的训练数据就达到全数据性能，相对随机子采样提升4.1%。

Conclusion: BIS方法显著提高了MPRM训练的数据效率，通过智能数据选择减少了对大规模MC标注数据集的依赖，为视觉推理中的步骤级监督提供了更高效的训练方案。

Abstract: Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.

</details>


### [48] [Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework](https://arxiv.org/abs/2602.04153)
*Zihao Jing,Yuxi Long,Ganlin Feng*

Main category: cs.LG

TL;DR: TL-GPSTGN：一种面向迁移学习的时空图神经网络，通过选择性剪枝非优化图上下文来提升数据稀缺和跨域转移下的多变量时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型在数据稀缺和跨域分布偏移下性能下降，需要提升样本效率和分布外泛化能力。

Method: 采用信息论和相关性准则提取结构信息子图和特征，构建紧凑语义表示，然后集成到时态卷积架构中捕获复杂多变量动态。

Result: 在大规模交通基准测试中，TL-GPSTGN在低数据迁移场景下持续优于基线方法。

Conclusion: 显式上下文剪枝可作为强大的归纳偏置，提升基于图的预测模型的鲁棒性。

Abstract: Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context selection. We propose TL-GPSTGN, a transfer-oriented spatiotemporal framework that enhances sample efficiency and out-of-distribution generalization by selectively pruning non-optimized graph context. Specifically, our method employs information-theoretic and correlation-based criteria to extract structurally informative subgraphs and features, resulting in a compact, semantically grounded representation. This optimized context is subsequently integrated into a spatiotemporal convolutional architecture to capture complex multivariate dynamics. Evaluations on large-scale traffic benchmarks demonstrate that TL-GPSTGN consistently outperforms baselines in low-data transfer scenarios. Our findings suggest that explicit context pruning serves as a powerful inductive bias for improving the robustness of graph-based forecasting models.

</details>


### [49] [BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models](https://arxiv.org/abs/2602.04163)
*Junyu Chen,Jungang Li,Jing Xiong,Wenjie Wang,Qingyao Yang,He Xiao,Zhen Li,Taiqiang Wu,Mengzhao Chen,Zhen Peng,Chaofan Tao,Long Shi,Hongxia Yang,Ngai Wong*

Main category: cs.LG

TL;DR: BPDQ是一种新型量化方法，通过位平面分解和可变量化网格，在2-3位低比特量化中保持LLM推理精度，使Qwen2.5-72B能在单张RTX 3090上运行。


<details>
  <summary>Details</summary>
Motivation: LLM推理在资源受限环境中受限于内存占用和带宽，量化是高效服务的关键技术。现有后训练量化方法在4位表现良好，但在2-3位精度严重下降，主要因为固定均匀量化网格限制了误差最小化的可行解空间。

Method: 提出位平面分解量化(BPDQ)：通过位平面和标量系数构建可变量化网格，使用近似二阶信息迭代优化，逐步补偿量化误差以最小化输出差异。

Result: 在2位量化下，BPDQ使Qwen2.5-72B能在单张RTX 3090上运行，GSM8K准确率达到83.85%（16位为90.83%）。理论分析表明可变网格扩展了可行解空间，量化过程与Hessian诱导几何中的优化目标一致。

Conclusion: BPDQ通过可变量化网格解决了低比特量化的精度问题，为资源受限环境中的LLM高效服务提供了有效解决方案，并提供了理论保证。

Abstract: Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ.

</details>


### [50] [Topology-Aware Revival for Efficient Sparse Training](https://arxiv.org/abs/2602.04166)
*Meiling Jin,Fei Wang,Xiaoyun Yuan,Chen Qian,Yuan Cheng*

Main category: cs.LG

TL;DR: 提出TAR方法，在静态稀疏训练后进行一次性的连接复活，改善深度强化学习中静态稀疏网络的鲁棒性和性能


<details>
  <summary>Details</summary>
Motivation: 静态稀疏训练虽然高效，但固定的掩码模式会降低网络鲁棒性。早期剪枝决策可能将网络锁定在脆弱结构中，特别是在深度强化学习中，策略的不断演化会持续改变训练分布，使得网络难以适应

Method: TAR是一种轻量级的一次性后剪枝过程，包含三个步骤：1）根据拓扑需求在各层分配小规模预算；2）在各层内随机均匀地重新激活少量先前被剪枝的连接；3）保持结果连接性固定进行后续训练

Result: 在多个连续控制任务中，TAR相比静态稀疏基线将最终回报提高了最高37.9%，相比动态稀疏训练基线也有中位数13.5%的性能提升

Conclusion: TAR通过简单的连接复活机制有效改善了静态稀疏训练在深度强化学习中的性能，无需复杂的动态重连，为高效稀疏学习提供了实用解决方案

Abstract: Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%.

</details>


### [51] [Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving](https://arxiv.org/abs/2602.04189)
*Xiaoyu Qiu,Taewon Yang,Zhanhao Liu,Guanyang Wang,Liyue Shen*

Main category: cs.LG

TL;DR: 该论文系统评估了即插即用扩散先验（PnPDP）求解器在逆问题中的不确定性量化能力，提出了基于不确定性的分类方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前PnPDP求解器的评估主要关注单样本点估计精度，忽略了逆问题的随机性和内在不确定性，这与科学任务中需要后验分布输出的需求存在根本性不匹配。

Method: 设计了严谨的玩具模型模拟来评估不同PnPDP求解器的不确定性行为，提出了基于不确定性量化的分类方法，并在玩具模拟和真实科学逆问题上进行了广泛实验。

Result: 实验观察到的不确定性行为与提出的分类方法和理论解释一致，为理解和评估PnPDP的不确定性提供了新的见解。

Conclusion: 该研究填补了PnPDP求解器不确定性量化评估的空白，提出的分类框架为科学逆问题中的不确定性分析提供了系统方法。

Abstract: Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.

</details>


### [52] [LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data](https://arxiv.org/abs/2602.04192)
*Vivek Anand,Alec Helbling,Mark Davenport,Gordon Berman,Sankar Alagapan,Christopher Rozell*

Main category: cs.LG

TL;DR: LORE是一个可扩展框架，通过非凸Schatten-p拟范数正则化，从三元组比较中联合学习内在维度和序数嵌入，无需预先设定维度。


<details>
  <summary>Details</summary>
Motivation: 从序数数据（如"是否A比C更相似于B"的三元组比较）中学习主观感知空间（如味觉、嗅觉、美学）的内在维度是一个具有挑战性的问题。现有方法需要预先设定嵌入维度，这限制了其在实际应用中的灵活性。

Method: 提出LORE框架，使用非凸Schatten-p拟范数正则化，通过迭代重加权算法优化联合目标函数，同时学习序数嵌入和其内在维度。该方法无需预先指定嵌入维度。

Result: 在合成数据集、模拟感知空间和真实世界众包序数判断上的广泛实验表明，LORE能够学习到紧凑、可解释且高度准确的低维嵌入，恢复主观感知的潜在几何结构。

Conclusion: LORE通过同时推断内在维度和序数嵌入，为心理物理学提供了更可解释和数据高效的感知建模方法，并为机器学习中从序数数据发现低维结构开辟了新方向。

Abstract: Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, "Is A more similar to B than C?". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.

</details>


### [53] [From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction](https://arxiv.org/abs/2602.04201)
*Yanjie Tong,Peng Chen*

Main category: cs.LG

TL;DR: STRIDE：两阶段框架，通过时间编码器将传感器测量映射到潜在状态，再通过调制隐式神经表示解码器在任意位置重建场，在稀疏传感下优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在不同轨迹和参数设置间泛化，或依赖于与离散化绑定的解码器，无法自然跨网格和分辨率迁移。需要一种能从稀疏点传感器测量重建高维时空场的方法。

Method: 提出STRIDE框架：1）时间编码器将短窗口传感器测量映射到潜在状态；2）使用调制隐式神经表示（INR）解码器在任意查询位置重建场。采用FMMNN作为INR骨干，比基于正弦的INR能更好表示复杂空间场且优化更稳定。

Result: 在四个挑战性基准测试（混沌动力学和波传播）中，STRIDE在极端稀疏传感下优于强基线方法，支持超分辨率，并对噪声保持鲁棒性。

Conclusion: STRIDE提供了一种有效的两阶段框架，能从稀疏传感器测量重建高维时空场，具有良好的泛化能力和跨分辨率适应性，并有条件理论依据支持其架构合理性。

Abstract: Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.

</details>


### [54] [RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning](https://arxiv.org/abs/2602.04224)
*Zeming Wei,Qiaosheng Zhang,Xia Hu,Xingcheng Xu*

Main category: cs.LG

TL;DR: RAPO框架通过风险感知偏好优化，让大型推理模型能够自适应识别和处理安全风险，提升对复杂越狱攻击的防御能力


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然具备链式推理能力，但仍面临类似基础语言模型的安全问题。现有的安全拒绝机制在面对多样化和复杂的越狱攻击时泛化能力不足，需要更充分的安全推理过程来防御高级攻击提示。

Method: 提出风险感知偏好优化（RAPO）框架，使LRM能够自适应地识别安全风险，并在其思考内容中以适当的粒度处理这些风险。该方法通过理论分析和实证证据支持更充分的安全推理过程的必要性。

Result: 大量实验表明，RAPO成功地在多种LRM上实现了安全推理的自适应泛化，能够有效防御多样化的攻击提示，同时保持模型的一般效用。

Conclusion: RAPO为LRM安全提供了一种鲁棒的对齐技术，通过风险感知的偏好优化框架，显著提升了模型对复杂越狱攻击的防御能力，同时不损害模型的一般性能。

Abstract: Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.

</details>


### [55] [Cascading Robustness Verification: Toward Efficient Model-Agnostic Certification](https://arxiv.org/abs/2602.04236)
*Mohammadreza Maleki,Rushendra Sidibomma,Arman Adibi,Reza Samavi*

Main category: cs.LG

TL;DR: CRV提出级联鲁棒性验证框架，通过多验证器级联提升验证可靠性和效率，相比单验证器可提高验证精度并减少90%计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络对抗样本鲁棒性验证存在挑战：完全验证方法计算成本高，不完全验证方法虽高效但可能因近似松弛或与训练方法不匹配而低估鲁棒性。依赖单一验证器存在局限性。

Method: 提出级联鲁棒性验证(CRV)框架：1) 模型无关验证器；2) 多验证器级联策略，从最廉价方法开始，一旦验证通过即停止；3) 针对昂贵方法引入逐步松弛算法(SR)，增量添加约束避免不必要计算。

Result: 理论分析表明CRV能达到或超过级联中强大但计算昂贵的验证器的验证精度，同时显著降低验证开销。实验证实CRV验证的输入数量至少与基准方法相当，同时运行效率提升高达90%。

Conclusion: CRV框架通过多验证器级联策略，在保持高验证可靠性的同时大幅提升效率，解决了现有鲁棒性验证方法的局限性，为神经网络对抗鲁棒性验证提供了更实用的解决方案。

Abstract: Certifying neural network robustness against adversarial examples is challenging, as formal guarantees often require solving non-convex problems. Hence, incomplete verifiers are widely used because they scale efficiently and substantially reduce the cost of robustness verification compared to complete methods. However, relying on a single verifier can underestimate robustness because of loose approximations or misalignment with training methods. In this work, we propose Cascading Robustness Verification (CRV), which goes beyond an engineering improvement by exposing fundamental limitations of existing robustness metric and introducing a framework that enhances both reliability and efficiency. CRV is a model-agnostic verifier, meaning that its robustness guarantees are independent of the model's training process. The key insight behind the CRV framework is that, when using multiple verification methods, an input is certifiably robust if at least one method certifies it as robust. Rather than relying solely on a single verifier with a fixed constraint set, CRV progressively applies multiple verifiers to balance the tightness of the bound and computational cost. Starting with the least expensive method, CRV halts as soon as an input is certified as robust; otherwise, it proceeds to more expensive methods. For computationally expensive methods, we introduce a Stepwise Relaxation Algorithm (SR) that incrementally adds constraints and checks for certification at each step, thereby avoiding unnecessary computation. Our theoretical analysis demonstrates that CRV achieves equal or higher verified accuracy compared to powerful but computationally expensive incomplete verifiers in the cascade, while significantly reducing verification overhead. Empirical results confirm that CRV certifies at least as many inputs as benchmark approaches, while improving runtime efficiency by up to ~90%.

</details>


### [56] [Training A Foundation Model to Represent Graphs as Vectors](https://arxiv.org/abs/2602.04244)
*Qi Feng,Jicong Fan*

Main category: cs.LG

TL;DR: 该论文提出了一种图基础模型，通过多图特征对齐和密度最大化均值对齐算法，结合对比学习和多层参考分布模块，实现跨域泛化的图表示学习。


<details>
  <summary>Details</summary>
Motivation: 训练一个能够将任意图表示为向量的图基础模型，保留结构和语义信息，用于下游图级任务（如图分类和图聚类），同时保持对新领域的强泛化能力。

Method: 1. 多图特征对齐方法：利用每个数据集中所有节点的属性构建加权图，生成一致的节点嵌入
2. 密度最大化均值对齐算法：增强不同数据集特征的一致性，保证收敛性
3. 图神经网络结合对比学习：将原始图和生成的节点嵌入输入GNN，获得判别性图表示
4. 多层参考分布模块：不使用池化操作，增强从节点级表示到图级表示的信息保留
5. 提供理论泛化边界证明模型有效性

Result: 在小样本图分类和图聚类实验中，该模型优于强基线方法。

Conclusion: 提出的图基础模型能够有效学习跨域图表示，在少样本图分类和图聚类任务中表现出色，具有理论保证和实际应用价值。

Abstract: This paper aims to train a graph foundation model that is able to represent any graph as a vector preserving structural and semantic information useful for downstream graph-level tasks such as graph classification and graph clustering. To learn the features of graphs from diverse domains while maintaining strong generalization ability to new domains, we propose a multi-graph-based feature alignment method, which constructs weighted graphs using the attributes of all nodes in each dataset and then generates consistent node embeddings. To enhance the consistency of the features from different datasets, we propose a density maximization mean alignment algorithm with guaranteed convergence. The original graphs and generated node embeddings are fed into a graph neural network to achieve discriminative graph representations in contrastive learning. More importantly, to enhance the information preservation from node-level representations to the graph-level representation, we construct a multi-layer reference distribution module without using any pooling operation. We also provide a theoretical generalization bound to support the effectiveness of the proposed model. The experimental results of few-shot graph classification and graph clustering show that our model outperforms strong baselines.

</details>


### [57] [From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution](https://arxiv.org/abs/2602.04255)
*Hanlin Pan,Yuhao Tang,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出基于部分可观察马尔可夫决策过程的PML框架，联合建模标签消歧和特征选择，通过强化学习生成高质量伪标签和可解释特征排序。


<details>
  <summary>Details</summary>
Motivation: 在部分多标签学习中，真实标签不可观察，标签消歧困难且容易将错误传播到下游任务如特征工程中。

Method: 将消歧和特征选择联合建模为POMDP，将PML风险最小化转化为期望回报最大化。第一阶段通过强化学习训练transformer策略生成高质量硬伪标签；第二阶段将特征选择描述为序列强化学习问题，逐步选择特征并输出可解释的全局排序。

Result: 提供了PML-POMDP对应关系的理论分析和超额风险界限，将误差分解为伪标签质量项和样本量项。在多个指标和数据集上的实验验证了框架优势。

Conclusion: 提出的POMDP框架有效解决了PML中的标签消歧和特征选择问题，通过强化学习生成高质量伪标签和可解释特征排序，理论和实验均验证了其有效性。

Abstract: In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.

</details>


### [58] [From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers](https://arxiv.org/abs/2602.04264)
*Ibrahim Albool,Malak Gamal El-Din,Salma Elmalaki,Yasser Shoukry*

Main category: cs.LG

TL;DR: Deep Bernstein Networks 使用 Bernstein 多项式作为激活函数，无需残差连接即可解决梯度消失问题，同时提升表示能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统残差连接虽然缓解了梯度消失问题，但存在结构限制，且无法解决分段线性激活函数的固有低效问题。作者希望找到一种既能优化训练性又能提升表示能力的残差无关架构。

Method: 提出 Deep Bernstein Networks，使用 Bernstein 多项式作为激活函数。从理论上推导了局部导数的下界，证明其严格远离零值，从而解决梯度停滞问题。同时证明了基于 Bernstein 的网络近似误差随深度呈指数衰减。

Result: 实验表明，该架构将"死亡"神经元比例从标准深度网络的90%降至5%以下，优于 ReLU、Leaky ReLU、SeLU 和 GeLU。在 HIGGS 和 MNIST 数据集上实现了无需跳跃连接的高性能训练。

Conclusion: Bernstein 激活函数为函数近似和信号流提供了更优机制，为构建具有增强表达能力的深度残差无关架构提供了理论依据。

Abstract: Residual connections are the de facto standard for mitigating vanishing gradients, yet they impose structural constraints and fail to address the inherent inefficiencies of piecewise linear activations. We show that Deep Bernstein Networks (which utilizes Bernstein polynomials as activation functions) can act as residual-free architecture while simultaneously optimize trainability and representation power. We provide a two-fold theoretical foundation for our approach. First, we derive a theoretical lower bound on the local derivative, proving it remains strictly bounded away from zero. This directly addresses the root cause of gradient stagnation; empirically, our architecture reduces ``dead'' neurons from 90\% in standard deep networks to less than 5\%, outperforming ReLU, Leaky ReLU, SeLU, and GeLU. Second, we establish that the approximation error for Bernstein-based networks decays exponentially with depth, a significant improvement over the polynomial rates of ReLU-based architectures. By unifying these results, we demonstrate that Bernstein activations provide a superior mechanism for function approximation and signal flow. Our experiments on HIGGS and MNIST confirm that Deep Bernstein Networks achieve high-performance training without skip-connections, offering a principled path toward deep, residual-free architectures with enhanced expressive capacity.

</details>


### [59] [Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning](https://arxiv.org/abs/2602.04265)
*Wenze Lin,Zhen Yang,Xitai Jiang,Pony Ma,Gao Huang*

Main category: cs.LG

TL;DR: T2T(Thickening-to-Thinning)是一个动态奖励框架，通过双阶段机制解决RLVR中的熵崩溃、冗长和探索不足问题：错误时激励"增厚"（更长轨迹）以扩大搜索空间，正确时转为"减薄"施加长度惩罚以减少冗余。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励(RLVR)方法存在熵崩溃、过度冗长和对难题探索不足的问题，且现有奖励方案无法区分问题解决过程中的广泛搜索需求与已掌握知识所需的效率。

Method: 提出T2T动态奖励框架，受人类学习过程启发，采用双阶段机制：1) 错误尝试时激励"增厚"（更长推理轨迹）以扩大搜索空间和探索新解路径；2) 达到正确性后转为"减薄"，施加长度惩罚以减少冗余，培养模型信心并固化推理能力。

Result: 在数学基准测试(MATH-500, AIME, AMC)上对Qwen系列和Deepseek模型进行广泛实验，T2T显著优于标准GRPO和近期基线方法，实现了优越性能。

Conclusion: T2T框架通过动态调整奖励策略，有效解决了RLVR中的关键挑战，在增强大语言模型推理能力方面表现出色，为强化学习与可验证奖励提供了新的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

</details>


### [60] [Multi-Integration of Labels across Categories for Component Identification (MILCCI)](https://arxiv.org/abs/2602.04270)
*Noga Mudrik,Yuxi Chen,Gal Mishne,Adam S. Charles*

Main category: cs.LG

TL;DR: MILCCI是一种新颖的数据驱动方法，用于分析带有多类别标签的大规模时间序列数据，能够识别可解释的组件、捕捉跨试验变异性，并整合标签信息来理解每个类别在数据中的表示。


<details>
  <summary>Details</summary>
Motivation: 许多领域通过重复测量收集大规模时间序列数据，每个试验都带有跨多个类别的元数据变量标签。关键挑战是理解这些标签如何在多试验观测中被编码，并区分每个标签条目在不同类别中的不同影响。

Method: MILCCI扩展了稀疏的每试验分解方法，利用每个类别内的标签相似性来实现细微的、标签驱动的跨试验组件组成调整，并区分每个类别的贡献。同时学习每个组件对应的时间轨迹，这些轨迹在每个试验内随时间演化，并在不同试验间灵活变化。

Result: 通过合成数据和真实世界示例（包括投票模式、在线页面浏览趋势和神经元记录）展示了MILCCI的性能。

Conclusion: MILCCI提供了一种有效的方法来分析带有多类别标签的大规模时间序列数据，能够识别可解释的组件、捕捉跨试验变异性，并整合标签信息来理解每个类别在数据中的表示。

Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.

</details>


### [61] [Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms](https://arxiv.org/abs/2602.04277)
*Priyankkumar Dhrangdhariya,Soumyadipta Maiti,Venkataramana Runkana*

Main category: cs.LG

TL;DR: 提出集成生成式设计和机器学习的框架，优化UPTIS型非充气轮胎辐条几何结构，实现刚度可调性53%、耐久性提升50%、振动降低43%。


<details>
  <summary>Details</summary>
Motivation: 非充气轮胎是充气轮胎的有前景替代品，但其不连续的辐条结构在刚度调节、耐久性和高速振动方面存在挑战，需要系统化的优化方法。

Method: 采用高阶多项式参数化辐条轮廓，通过PCHIP几何变化生成约250个设计；使用KRR预测刚度、XGBoost预测耐久性和振动；结合粒子群优化和贝叶斯优化进行多目标性能优化。

Result: 优化后的设计相比基准实现了53%的刚度可调性、最高50%的耐久性提升和43%的振动降低；PSO提供快速收敛，贝叶斯优化有效探索多目标权衡。

Conclusion: 提出的集成框架能够系统化开发高性能下一代UPTIS辐条结构，减少对计算密集型FEM模拟的依赖，实现多目标性能优化。

Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.

</details>


### [62] [Convolution Operator Network for Forward and Inverse Problems (FI-Conv): Application to Plasma Turbulence Simulations](https://arxiv.org/abs/2602.04287)
*Xingzhuo Chen,Anthony Poole,Ionut-Gabriel Farcas,David R. Hatch,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: FI-Conv是一个基于U-Net架构的卷积算子网络，用于预测复杂时空动力学系统的演化和参数估计，特别针对湍流等离子体场的Hasegawa-Wakatani方程进行验证。


<details>
  <summary>Details</summary>
Motivation: 解决复杂时空动力学系统（如湍流）的长期预测和参数估计问题，为现有物理信息机器学习方法提供有效替代方案。

Method: 基于U-Net架构，用ConvNeXt V2块替换大部分卷积层，输入初始状态、PDE参数和演化时间，采用自回归预测过程进行前向预测，并开发基于梯度下降的逆估计方法进行参数推断。

Result: 在Hasegawa-Wakatani方程上，FI-Conv实现了短期（t~3）的准确等离子体状态演化预测，并能在长期（t~100）捕捉物理量的统计特性；逆估计方法能准确从演化数据推断PDE参数。

Conclusion: FI-Conv是处理复杂时空动力学系统前向和逆问题的有效替代方法，无需修改训练模型权重即可实现准确预测和参数估计。

Abstract: We propose the Convolutional Operator Network for Forward and Inverse Problems (FI-Conv), a framework capable of predicting system evolution and estimating parameters in complex spatio-temporal dynamics, such as turbulence. FI-Conv is built on a U-Net architecture, in which most convolutional layers are replaced by ConvNeXt V2 blocks. This design preserves U-Net performance on inputs with high-frequency variations while maintaining low computational complexity. FI-Conv uses an initial state, PDE parameters, and evolution time as input to predict the system future state. As a representative example of a system exhibiting complex dynamics, we evaluate the performance of FI-Conv on the task of predicting turbulent plasma fields governed by the Hasegawa-Wakatani (HW) equations. The HW system models two-dimensional electrostatic drift-wave turbulence and exhibits strongly nonlinear behavior, making accurate approximation and long-term prediction particularly challenging. Using an autoregressive forecasting procedure, FI-Conv achieves accurate forward prediction of the plasma state evolution over short times (t ~ 3) and captures the statistic properties of derived physical quantities of interest over longer times (t ~ 100). Moreover, we develop a gradient-descent-based inverse estimation method that accurately infers PDE parameters from plasma state evolution data, without modifying the trained model weights. Collectively, our results demonstrate that FI-Conv can be an effective alternative to existing physics-informed machine learning methods for systems with complex spatio-temporal dynamics.

</details>


### [63] [Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration](https://arxiv.org/abs/2602.04291)
*Sudipto Ghosh,Sujoy Nath,Sunny Manchanda,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: INFORM是一个可解释性分析框架，将多专家LLM系统的编排视为可分析的显式计算，揭示了路由主导性与功能必要性之间的差异，以及关系重要性与内在因果重要性之间的分歧。


<details>
  <summary>Details</summary>
Motivation: 多专家LLM系统在复杂任务中表现出色，但其编排策略（专家交互和排序）通常不透明，缺乏对编排过程的可解释性分析。

Method: INFORM框架将编排视为显式计算，解耦专家交互结构、执行顺序和因果归因。使用同质（10个指令调优专家）和异质（1B-7B参数模型）专家联盟，在GSM8K、HumanEval和MMLU任务上评估编排器。

Result: 路由主导性不能代表功能必要性；关系重要性（路由质量和交互拓扑）与内在重要性（基于梯度的因果归因）存在分歧；频繁选择的专家常作为交互枢纽但因果影响有限，而稀疏路由的专家可能结构关键；编排行为异步出现，专家集中化先于稳定路由置信度。

Conclusion: INFORM揭示了编排系统中超越准确性指标的因果和结构依赖关系，表明频繁路由的专家不一定是功能必要的，而稀疏路由的专家可能对系统结构至关重要。

Abstract: Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an interpretability analysis that treats orchestration as an explicit, analyzable computation, enabling the decoupling of expert interaction structure, execution order, and causal attribution. We use INFORM to evaluate an orchestrator on GSM8K, HumanEval, and MMLU using a homogeneous consortium of ten instruction-tuned experts drawn from LLaMA-3.1 8B, Qwen-3 8B, and DeepSeek-R1 8B, with controlled decoding-temperature variation, and a secondary heterogeneous consortium spanning 1B-7B parameter models. Across tasks, routing dominance is a poor proxy for functional necessity. We reveal a divergence between relational importance, captured by routing mass and interaction topology, and intrinsic importance, measured via gradient-based causal attribution: frequently selected experts often act as interaction hubs with limited causal influence, while sparsely routed experts can be structurally critical. Orchestration behaviors emerge asynchronously, with expert centralization preceding stable routing confidence and expert ordering remaining non-deterministic. Targeted ablations show that masking intrinsically important experts induces disproportionate collapse in interaction structure compared to masking frequent peers, confirming that INFORM exposes causal and structural dependencies beyond accuracy metrics alone.

</details>


### [64] [Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling](https://arxiv.org/abs/2602.04323)
*Dian Jin,Yancheng Yuan,Xiaoming Tao*

Main category: cs.LG

TL;DR: CEITNet：一种高效预测高阶晶体张量性质的方法，通过构建多通道笛卡尔局部环境张量并在通道空间进行学习，超越现有方法


<details>
  <summary>Details</summary>
Motivation: 端到端预测高阶晶体张量性质具有挑战性：虽然球谐函数等变模型表达能力强，但其Clebsch-Gordan张量积在高阶目标上计算和内存成本过高

Method: 提出笛卡尔环境交互张量网络（CEITNet），为每个原子构建多通道笛卡尔局部环境张量，通过可学习的通道空间交互进行灵活的多体混合，在通道空间学习并使用笛卡尔张量基组装等变输出

Result: 在二阶介电、三阶压电和四阶弹性张量预测的基准数据集上，CEITNet在关键精度指标上超越了先前的高阶预测方法，同时提供高计算效率

Conclusion: CEITNet通过笛卡尔张量表示和通道空间学习，实现了高效的高阶晶体张量性质预测，为材料科学中的张量性质预测提供了新的解决方案

Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.

</details>


### [65] [RISE: Interactive Visual Diagnosis of Fairness in Machine Learning Models](https://arxiv.org/abs/2602.04339)
*Ray Chen,Christan Grant*

Main category: cs.LG

TL;DR: RISE是一个可视化工具，通过排序残差分析来诊断模型公平性问题，特别是在领域偏移下的局部差异检测


<details>
  <summary>Details</summary>
Motivation: 现有的公平性评估指标通常是标量度量，难以揭示差异的具体来源和位置，特别是在领域偏移情况下，无法有效诊断局部差异和隐藏的公平性问题

Method: RISE（Residual Inspection through Sorted Evaluation）是一个交互式可视化工具，将排序后的残差转换为可解释的模式，将残差曲线结构与形式化公平概念联系起来

Result: RISE能够实现局部差异诊断、跨环境子组比较、检测隐藏的公平性问题，并通过事后分析揭示聚合统计量遗漏的准确性与公平性权衡

Conclusion: RISE工具支持更明智的模型选择，通过可视化残差分析提供比传统标量度量更深入的公平性洞察

Abstract: Evaluating fairness under domain shift is challenging because scalar metrics often obscure exactly where and how disparities arise. We introduce \textit{RISE} (Residual Inspection through Sorted Evaluation), an interactive visualization tool that converts sorted residuals into interpretable patterns. By connecting residual curve structures to formal fairness notions, RISE enables localized disparity diagnosis, subgroup comparison across environments, and the detection of hidden fairness issues. Through post-hoc analysis, RISE exposes accuracy-fairness trade-offs that aggregate statistics miss, supporting more informed model selection.

</details>


### [66] [UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching](https://arxiv.org/abs/2602.04344)
*Kou Misaki,Takuya Akiba*

Main category: cs.LG

TL;DR: 提出了UnMaskFork框架，利用蒙特卡洛树搜索优化掩码扩散语言模型的生成路径，在推理时计算中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型具有迭代和非自回归生成特性，天生适合高级搜索策略，而现有测试时缩放方法主要依赖随机采样，未能充分利用这种优势

Method: 提出UnMaskFork框架，将去掩码轨迹建模为搜索树，使用蒙特卡洛树搜索优化生成路径，通过多个MDLMs执行确定性部分去掩码动作来探索搜索空间

Result: 在复杂编码基准测试中一致优于现有测试时缩放基线，在数学推理任务上也展现出强大的可扩展性

Conclusion: 掩码扩散语言模型通过结构化搜索策略能有效利用推理时计算，UnMaskFork框架为这类模型提供了优于随机采样的优化方法

Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.

</details>


### [67] [MirrorLA: Reflecting Feature Map for Vision Linear Attention](https://arxiv.org/abs/2602.04346)
*Weikang Meng,Liangyu Huo,Yadan Luo,Yaowei Wang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: MirrorLA提出了一种几何框架，通过可学习的Householder反射将特征几何旋转到非负象限，解决了线性注意力因非负约束导致性能下降的问题，实现了线性效率而不损失表示保真度。


<details>
  <summary>Details</summary>
Motivation: 线性注意力将Transformer的计算复杂度从二次降低到线性，但性能始终落后于基于softmax的注意力。研究发现根本原因是核特征映射的非负约束：标准投影如ReLU作为"被动截断"操作，不加区分地丢弃负域中的语义信息。

Method: 提出MirrorLA几何框架，用主动重定向替代被动截断。通过可学习的Householder反射将特征几何旋转到非负象限以最大化信息保留。采用多尺度设计：1)通过块级等距优化局部可区分性；2)使用方差感知调制稳定长上下文动态以多样化激活；3)通过跨头反射集成分散子空间以诱导全局协方差混合。

Result: MirrorLA在标准基准测试中实现了最先进的性能，证明严格线性效率可以在不损害表示保真度的情况下实现。

Conclusion: 线性注意力性能下降的根本原因是非负约束导致的被动信息截断。通过几何重定向框架MirrorLA，可以主动保留语义信息，实现线性效率与表示保真度的平衡，为高效Transformer架构提供了新方向。

Abstract: Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as "passive truncation" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.

</details>


### [68] [Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation](https://arxiv.org/abs/2602.04352)
*Sayan Biswas,Davide Frey,Romaric Gaudel,Nirupam Gupta,Anne-Marie Kermarrec,Dimitri Lerévérend,Rafael Pires,Rishi Sharma,François Taïani,Martijn de Vos*

Main category: cs.LG

TL;DR: Mosaic Learning是一个去中心化学习框架，通过将模型分解为片段并在网络中独立传播，减少相关参数间的冗余通信，提高学习性能而不增加通信成本。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习（DL）允许在没有中央服务器的情况下进行协作机器学习，适用于无法集中托管训练数据的场景。现有方法存在参数间冗余通信和传播多样性不足的问题。

Method: Mosaic Learning将模型分解为片段，并在网络中独立传播这些片段。这种方法减少了相关参数间的冗余通信，并能在不增加通信成本的情况下实现更丰富的信息传播。

Result: 理论分析显示：(i) 具有最先进的最坏情况收敛率；(ii) 利用ML模型中的参数相关性，通过降低简化系统的最高特征值来改进收敛性。在四个学习任务上的实证评估显示，相比最先进的基线方法（流行病学习EL），节点级测试准确率最高提升12个百分点。

Conclusion: Mosaic Learning在不牺牲效用或效率的情况下提高了去中心化学习性能，有望成为新的DL标准。

Abstract: Decentralized learning (DL) enables collaborative machine learning (ML) without a central server, making it suitable for settings where training data cannot be centrally hosted. We introduce Mosaic Learning, a DL framework that decomposes models into fragments and disseminates them independently across the network. Fragmentation reduces redundant communication across correlated parameters and enables more diverse information propagation without increasing communication cost. We theoretically show that Mosaic Learning (i) shows state-of-the-art worst-case convergence rate, and (ii) leverages parameter correlation in an ML model, improving contraction by reducing the highest eigenvalue of a simplified system. We empirically evaluate Mosaic Learning on four learning tasks and observe up to 12 percentage points higher node-level test accuracy compared to epidemic learning (EL), a state-of-the-art baseline. In summary, Mosaic Learning improves DL performance without sacrificing its utility or efficiency, and positions itself as a new DL standard.

</details>


### [69] [Counterfactual Explanations for Hypergraph Neural Networks](https://arxiv.org/abs/2602.04360)
*Fabiano Veglianti,Lorenzo Antonelli,Gabriele Tolomei*

Main category: cs.LG

TL;DR: CF-HyperGNNExplainer：一种针对超图神经网络的反事实解释方法，通过最小结构修改生成可解释的预测解释


<details>
  <summary>Details</summary>
Motivation: 超图神经网络（HGNNs）能有效建模现实世界系统中的高阶交互，但缺乏可解释性限制了其在高风险场景中的应用。现有方法难以解释HGNN的决策过程，需要开发专门的反事实解释方法。

Method: 提出CF-HyperGNNExplainer方法，通过识别改变模型预测所需的最小结构变化来生成反事实解释。方法使用可操作编辑（移除节点-超边关联或删除超边）生成反事实超图，产生简洁且结构有意义的解释。

Result: 在三个基准数据集上的实验表明，CF-HyperGNNExplainer能生成有效且简洁的反事实解释，成功识别出对HGNN决策最关键的高阶关系。

Conclusion: 该方法为超图神经网络提供了实用的可解释性工具，通过反事实解释揭示了模型决策依赖的关键高阶结构特征，有助于在高风险场景中部署可信的HGNN模型。

Abstract: Hypergraph neural networks (HGNNs) effectively model higher-order interactions in many real-world systems but remain difficult to interpret, limiting their deployment in high-stakes settings.
  We introduce CF-HyperGNNExplainer, a counterfactual explanation method for HGNNs that identifies the minimal structural changes required to alter a model's prediction. The method generates counterfactual hypergraphs using actionable edits limited to removing node-hyperedge incidences or deleting hyperedges, producing concise and structurally meaningful explanations. Experiments on three benchmark datasets show that CF-HyperGNNExplainer generates valid and concise counterfactuals, highlighting the higher-order relations most critical to HGNN decisions.

</details>


### [70] [EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets](https://arxiv.org/abs/2602.04365)
*Jiapeng Liu,Liang Li,Bing Li,Peng Fu,Xiyan Gao,Chengyang Fang,Xiaoshuai Hao,Can Ma*

Main category: cs.LG

TL;DR: EXaMCaP：一种基于熵增益最大化的图表理解数据集子集选择方法，用于高效评估MLLMs在完整数据集微调后的能力提升，避免全量微调的时间成本。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解数据集合成方法需要全量微调MLLMs来评估能力提升，这导致高昂的时间成本，阻碍了数据集的迭代优化。研究发现子集可以探测MLLMs从全量微调中获得的能力增益。

Method: 提出EXaMCaP方法，基于熵增益最大化选择高多样性子集。通过迭代选择样本最大化集合熵相对于当前集合的增益，近似获得完整数据集的最大熵子集。

Result: 实验表明EXaMCaP在探测图表理解训练集能力增益方面优于基线方法，在不同子集大小下都表现出强有效性，且与多种MLLM架构兼容。

Conclusion: EXaMCaP提供了一种高效评估图表理解数据集质量的方法，通过选择高多样性子集来近似全量微调的效果，显著降低了评估成本，促进了数据集的迭代优化。

Abstract: Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.

</details>


### [71] [Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis](https://arxiv.org/abs/2602.04369)
*Zongjiang Shang,Dongliang Cui,Binqing Wu,Ling Chen*

Main category: cs.LG

TL;DR: MSH-LLM：一种多尺度超图方法，通过超边机制增强时间序列语义信息，跨模态对齐模块对齐不同尺度的自然语言和时间序列，以及混合提示机制提升LLM对时间序列多尺度模式的理解能力，在27个真实数据集上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前利用预训练大语言模型进行时间序列分析时，虽然取得了成功，但未能充分考虑自然语言和时间序列的多尺度结构，导致LLM能力利用不足。需要更好地对齐这两种模态的多尺度特征。

Method: 提出MSH-LLM方法：1）超边机制增强时间序列语义空间的多尺度语义信息；2）跨模态对齐模块在不同尺度上对齐自然语言和时间序列的模态；3）混合提示机制提供上下文信息，增强LLM理解时间序列多尺度时序模式的能力。

Result: 在5个不同应用的27个真实世界数据集上进行实验，MSH-LLM取得了最先进的结果。

Conclusion: MSH-LLM通过充分考虑多尺度结构，有效对齐自然语言和时间序列的模态，显著提升了LLM在时间序列分析中的性能，为跨模态时间序列分析提供了新思路。

Abstract: Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.

</details>


### [72] [Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time](https://arxiv.org/abs/2602.04373)
*Geethen Singh,Jasper A Slingsby,Tamara B Robinson,Glenn Moncrieff*

Main category: cs.LG

TL;DR: 该研究提出"Common Ground"方法，利用时间稳定区域作为隐式监督，实现无需更新标签的多时相遥感分类，在入侵树种制图中准确率提升21-40%。


<details>
  <summary>Details</summary>
Motivation: 遥感分类依赖最新参考标签，但动态或偏远生态系统中持续收集新标签数据成本高昂且困难。需要开发能够有效进行时间泛化的方法，减少对持续人工标注的依赖。

Method: 提出"Common Ground"方法，结合变化检测和半监督学习，利用时间稳定区域（光谱或语义特征变化小的区域）作为动态区域的隐式监督源，构建半监督框架。

Result: 在入侵树种制图中，相比简单时间迁移方法准确率提升21-40%，相比黄金标准方法提升10-16%；在欧洲土地覆盖分类中提升2%。方法在多种分类器、传感器和生态用例中验证有效。

Conclusion: 结合稳定参考筛选与半监督学习可实现可扩展、标签高效的多时相遥感分类，无需在初始时间步后手动更新参考标签，为动态生态监测提供实用解决方案。

Abstract: Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, "Common Ground", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.

</details>


### [73] [Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning](https://arxiv.org/abs/2602.04380)
*Rui Yuan,Mykola Khandoga,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: GBMPO扩展了基于群体的策略优化，使用灵活的Bregman散度（包括手工设计的L2概率空间散度和学习的神经镜像映射），在数学推理和代码生成任务上显著优于仅使用KL散度的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的策略优化方法（如GRPO）仅使用KL散度进行策略正则化，散度函数的选择这一关键设计维度尚未被探索。

Method: 提出Group-Based Mirror Policy Optimization (GBMPO)框架，将基于群体的策略优化扩展到灵活的Bregman散度，包括手工设计的替代方案（概率空间中的L2散度）和学习的神经镜像映射。

Result: 在GSM8K数学推理上，手工设计的ProbL2-GRPO达到86.7%准确率，比Dr. GRPO基线提升5.5个百分点；在MBPP代码生成上，神经镜像映射达到60.1-60.8% pass@1；随机初始化已能获得大部分收益。

Conclusion: 散度选择是基于群体的策略优化中一个关键且先前未被探索的设计维度，随机初始化神经镜像映射已足够大多数实际应用，进化策略元学习主要提供方差减少和效率提升。

Abstract: Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\pm$0.2 versus $\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.

</details>


### [74] [Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting](https://arxiv.org/abs/2602.04384)
*Fabio Turazza,Alessandro Neri,Marcello Pietri,Maria Angela Butturi,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 该研究探索了联邦学习在可持续供应链管理中的应用，特别是针对生鲜食品的杂货零售业需求预测，通过区块链联邦学习模型实现零售商间无需直接共享数据的协作预测，显著减少食物浪费。


<details>
  <summary>Details</summary>
Motivation: 准确的需求预测对减少食物浪费至关重要，但零售商间因数据隐私顾虑难以协作，限制了预测准确性的提升潜力。

Method: 首先开发了单个零售商场景下的基线预测模型，然后引入了基于区块链的联邦学习模型，该模型可在多个零售商间协作训练而无需直接数据共享。

Result: 初步结果显示，联邦学习模型的性能几乎等同于理想情况下各方共享数据的场景，并且明显优于单个零售商不共享数据构建的模型，能够减少浪费并提高效率。

Conclusion: 联邦学习为可持续供应链管理提供了一种有效的解决方案，能够在保护数据隐私的同时实现协作预测，显著改善需求预测准确性并减少食物浪费。

Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.

</details>


### [75] [On the use of LLMs to generate a dataset of Neural Networks](https://arxiv.org/abs/2602.04388)
*Nadia Daoudi,Jordi Cabot*

Main category: cs.LG

TL;DR: 利用大语言模型自动生成包含608个样本的神经网络数据集，用于验证神经网络可靠性工具的有效性评估


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开的多样化神经网络数据集来系统评估神经网络验证、重构和迁移等工具的有效性，这阻碍了对神经网络可靠性和适应性的研究进展

Method: 使用大语言模型自动生成神经网络数据集，覆盖多样化的架构组件、输入数据类型和任务，并通过静态分析和符号追踪验证生成网络的正确性

Result: 生成了包含608个样本的神经网络数据集，每个样本都符合精确的设计选择，数据集已公开可用

Conclusion: 通过LLM生成的多样化神经网络数据集填补了评估神经网络可靠性工具的数据空白，为社区推进神经网络可靠性和适应性研究提供了支持

Abstract: Neural networks are increasingly used to support decision-making. To verify their reliability and adaptability, researchers and practitioners have proposed a variety of tools and methods for tasks such as NN code verification, refactoring, and migration. These tools play a crucial role in guaranteeing both the correctness and maintainability of neural network architectures, helping to prevent implementation errors, simplify model updates, and ensure that complex networks can be reliably extended and reused. Yet, assessing their effectiveness remains challenging due to the lack of publicly diverse datasets of neural networks that would allow systematic evaluation. To address this gap, we leverage large language models (LLMs) to automatically generate a dataset of neural networks that can serve as a benchmark for validation. The dataset is designed to cover diverse architectural components and to handle multiple input data types and tasks. In total, 608 samples are generated, each conforming to a set of precise design choices. To further ensure their consistency, we validate the correctness of the generated networks using static analysis and symbolic tracing. We make the dataset publicly available to support the community in advancing research on neural network reliability and adaptability.

</details>


### [76] [LoRDO: Distributed Low-Rank Optimization with Infrequent Communication](https://arxiv.org/abs/2602.04396)
*Andrej Jovanović,Alex Iacob,Mher Safaryan,Ionut-Vlad Modoranu,Lorenzo Sani,William F. Shen,Xinchi Qiu,Dan Alistarh,Nicholas D. Lane*

Main category: cs.LG

TL;DR: LoRDO框架将低秩优化与低频通信相结合，在减少10倍通信的同时，在语言建模任务上达到接近低秩DDP的性能


<details>
  <summary>Details</summary>
Motivation: 分布式训练基础模型时，DDP受限于互连带宽。虽然低频通信策略减少了同步频率，但仍受限于优化器状态的内存和通信需求。低秩优化器可以缓解这些约束，但在本地更新机制下，工作节点缺乏计算低秩投影所需的完整批次梯度，导致性能下降。

Method: 提出LoRDO框架，统一低秩优化与低频同步。首先发现基于伪梯度的全局投影虽然理论上有优势，但会永久限制优化轨迹在低秩子空间。为恢复子空间探索，引入了全秩拟双曲更新。

Result: 在125M-720M模型规模的语言建模和下游任务中，LoRDO达到接近低秩DDP的性能，同时减少约10倍通信。在极低内存设置下（小秩/小批次），LoRDO性能提升更明显。

Conclusion: LoRDO提供了一种原则性框架，有效结合低秩优化与低频同步，在显著减少通信开销的同时保持模型性能，特别适用于内存受限的分布式训练场景。

Abstract: Distributed training of foundation models via $\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\texttt{LoRDO}$ achieves near-parity with low-rank $\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\approx 10 \times$. Finally, we show that $\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.

</details>


### [77] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli,Marco Benedetti,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: 该论文提出了扩散模型中物种形成转变的通用理论，适用于任意具有明确定义类别的目标分布，超越了仅依赖一阶矩的传统分析。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析仅限于类别可通过一阶矩（如均值分离的高斯混合）识别的情况，无法处理类别通过高阶或集体特征区分的更一般分布。需要建立适用于任意目标分布的物种形成理论。

Method: 通过贝叶斯分类形式化类别结构，用类别间的自由能差表征物种形成时间。该框架可处理多类别和连续细化分类，并在两个可解析处理的示例（不同温度的Ising模型混合和零均值不同协方差的高斯混合）中验证理论。

Result: 理论恢复了高斯混合模型的已知结果，同时扩展到类别无法通过一阶矩区分的情况。在Ising模型示例中，通过映射到随机场Ising模型并使用复本方法获得物种形成时间的显式表达式。

Conclusion: 该工作为扩散生成模型中的物种形成转变提供了统一且广泛适用的理论描述，能够处理任意具有明确定义类别的目标分布，包括通过高阶或集体特征区分的类别。

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


### [78] [Separation-Utility Pareto Frontier: An Information-Theoretic Characterization](https://arxiv.org/abs/2602.04408)
*Shizhou Xu*

Main category: cs.LG

TL;DR: 本文通过信息论视角研究效用与分离公平性之间的帕累托前沿，提出基于条件互信息的正则化方法，在多个数据集上有效减少分离违规同时保持或提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 研究如何在机器学习中平衡模型效用（预测准确性）与分离公平性（预测独立于敏感属性），为实际应用中的权衡选择提供理论指导。

Method: 1) 从信息论角度分析效用-分离帕累托前沿特性；2) 提出基于条件互信息（CMI）的正则化器，监控预测与敏感属性在真实结果条件下的依赖关系；3) 开发与基于梯度的深度学习模型兼容的训练方法。

Result: 1) 理论证明了帕累托前沿的凹性和分离的边际成本递增特性；2) 在COMPAS、UCI Adult、UCI Bank和CelebA数据集上，该方法显著减少分离违规，同时匹配或超越基线方法的效用表现。

Conclusion: 本研究提供了一个可证明、稳定且灵活的方法来在深度学习中实施分离公平性，为效用与公平性的权衡提供了理论指导和实用工具。

Abstract: We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.

</details>


### [79] [EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL](https://arxiv.org/abs/2602.04417)
*Lunjun Zhang,Jimmy Ba*

Main category: cs.LG

TL;DR: 提出EMA-PG方法，通过指数移动平均锚定策略和Top-k KL估计器改进LLM的强化学习，在数学推理和智能体任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大型语言模型中的策略梯度算法仍有改进空间，特别是锚定策略和KL散度估计方面需要更稳定和灵活的方法

Method: 1. 使用指数移动平均（EMA）替代固定锚定策略，类似于深度Q学习中的目标网络；2. 提出Top-k KL估计器，可在精确KL和采样KL之间灵活插值，提供无偏估计

Result: 在数学推理任务中，R1蒸馏的Qwen-1.5B在OlympiadBench上达到53.9%（GRPO为50.8%）；在智能体任务中，Qwen-3B在7个搜索问答数据集上平均提升33.3%，如HotpotQA从29.7%提升到44.1%

Conclusion: EMA-PG是一种简单、有理论依据且强大的方法，能够有效扩展LLM的强化学习能力，显著提升数学推理和智能体任务的性能

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&A with search engines, including 29.7% $\rightarrow$ 44.1% on HotpotQA, 27.4% $\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg

</details>


### [80] [MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems](https://arxiv.org/abs/2602.04431)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: MaMa算法通过元代理与元对手的Stackelberg博弈，自动设计能抵御部分代理被攻击的多代理系统，在保持任务性能的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: LLM多代理系统虽然能力强大，但当个别代理失效或被恶意攻击时，会带来严重的安全风险。需要设计能够抵御部分代理被攻击的安全系统。

Method: 将问题形式化为Stackelberg安全博弈，提出MaMa算法：元代理迭代提出系统设计，元对手发现最强攻击并提供反馈，通过LLM对抗搜索近似求解博弈。

Result: MaMa设计的系统能持续抵御最坏情况攻击，同时保持与仅优化任务性能的系统相当的表现。系统还能泛化到更强的对手、不同攻击目标和底层LLM。

Conclusion: MaMa算法能自动设计安全的多代理系统，在训练环境之外也展现出鲁棒的安全性，为解决LLM多代理系统的安全挑战提供了有效方法。

Abstract: LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.

</details>


### [81] [Hand Gesture Recognition from Doppler Radar Signals Using Echo State Networks](https://arxiv.org/abs/2602.04436)
*Towa Sano,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于回声状态网络（ESN）的雷达手势识别方法，使用FMCW雷达信号，通过多特征图并行处理和轻量级分类器，在保持高识别性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 基于多普勒雷达的手势识别在车载界面和机器人系统中应用广泛，但现有深度学习方法计算成本高，不适用于资源受限环境。需要开发轻量级、计算高效的识别技术。

Method: 将原始雷达数据转换为距离-时间和多普勒-时间特征图，输入到一个或多个基于循环神经网络的储备池中，然后使用岭回归、支持向量机或随机森林等轻量级分类器处理储备池状态。

Result: 在Soli数据集的11类手势识别任务中优于现有方法，在Dop-NET数据集的4类手势识别任务中超越现有深度学习模型，同时保持低计算成本。

Conclusion: 多储备池ESN方法能有效识别时空和时频域中的时序模式，在保持高识别性能的同时显著降低计算成本，在资源受限环境中具有巨大应用潜力。

Abstract: Hand gesture recognition (HGR) is a fundamental technology in human computer interaction (HCI).In particular, HGR based on Doppler radar signals is suited for in-vehicle interfaces and robotic systems, necessitating lightweight and computationally efficient recognition techniques. However, conventional deep learning-based methods still suffer from high computational costs. To address this issue, we propose an Echo State Network (ESN) approach for radar-based HGR, using frequency-modulated-continuous-wave (FMCW) radar signals. Raw radar data is first converted into feature maps, such as range-time and Doppler-time maps, which are then fed into one or more recurrent neural network-based reservoirs. The obtained reservoir states are processed by readout classifiers, including ridge regression, support vector machines, and random forests. Comparative experiments demonstrate that our method outperforms existing approaches on an 11-class HGR task using the Soli dataset and surpasses existing deep learning models on a 4-class HGR task using the Dop-NET dataset. The results indicate that parallel processing using multi-reservoir ESNs are effective for recognizing temporal patterns from the multiple different feature maps in the time-space and time-frequency domains. Our ESN approaches achieve high recognition performance with low computational cost in HGR, showing great potential for more advanced HCI technologies, especially in resource-constrained environments.

</details>


### [82] [Mixture of Masters: Sparse Chess Language Models with Player Routing](https://arxiv.org/abs/2602.04447)
*Giacomo Frisoni,Lorenzo Molfetta,Davide Freddi,Gianluca Moro*

Main category: cs.LG

TL;DR: 提出MoM（专家混合）架构，通过多个小型GPT专家模拟不同国际象棋大师风格，解决传统密集模型风格同质化问题，在性能和多样性上超越基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋语言模型是密集Transformer，在大量对局数据上训练，但容易陷入"模式平均"行为，模糊了风格边界，压制了罕见但有效的策略，导致同质化问题。

Method: 引入Mixture-of-Masters（MoM）架构：1）多个小型GPT专家分别模拟世界级大师风格；2）结合自监督学习和强化学习（使用棋类特定奖励）训练每个专家；3）可学习的门控网络根据棋局状态动态选择最合适的专家风格。

Result: 在未见过的标准对局中评估，MoM在对抗Stockfish时表现优于：1）密集的单个专家网络；2）在聚合数据上训练的流行GPT基线。同时确保了生成多样性、可控性和可解释性。

Conclusion: MoM架构通过专家混合方法有效解决了国际象棋AI的风格同质化问题，实现了动态风格切换（如塔尔攻击性或彼得罗相防守稳固性），在保持高性能的同时提升了多样性和可控性。

Abstract: Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.

</details>


### [83] [RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models](https://arxiv.org/abs/2602.04448)
*Jiacheng Liang,Yuhui Wang,Tanqiu Jiang,Ting Wang*

Main category: cs.LG

TL;DR: RASA：针对MoE模型的路由感知专家级安全对齐框架，通过修复安全关键专家而非全局参数更新来提升安全性


<details>
  <summary>Details</summary>
Motivation: MoE语言模型的安全对齐面临独特挑战，因为稀疏路由机制可能导致在标准全参数微调下出现退化优化行为。初步实验发现，对MoE模型进行全参数安全微调可能通过路由或专家主导效应降低攻击成功率，而非直接修复安全关键专家。

Method: 提出RASA框架：1）识别被成功越狱攻击过度激活的专家；2）在固定路由下仅对这些安全关键专家进行选择性微调；3）随后在安全对齐上下文中强制执行路由一致性。

Result: 在两种代表性MoE架构和多样化越狱攻击测试中，RASA实现了近乎完美的鲁棒性、强大的跨攻击泛化能力，显著减少了过度拒绝，同时在MMLU、GSM8K和TruthfulQA等基准测试中保持了通用能力。

Conclusion: 稳健的MoE安全对齐受益于有针对性的专家修复而非全局参数更新，RASA为先前方法提供了实用且保持架构的替代方案。

Abstract: Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.

</details>


### [84] [Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning](https://arxiv.org/abs/2602.04491)
*Yuxi Guo,Paul Sheridan*

Main category: cs.LG

TL;DR: 提出Greedy-Gnorm算法，通过动态计算注意力头重要性分数进行剪枝，优于静态评分方法


<details>
  <summary>Details</summary>
Motivation: 现有注意力头剪枝方法依赖静态重要性分数，无法捕捉剪枝过程中注意力头角色的动态变化，需要更有效的动态评估方法

Method: 提出Greedy-Gnorm算法：1) 使用验证集估计每个注意力头的Q/K/V梯度块的l2范数元素乘积作为重要性分数；2) 在每次贪婪剪枝迭代后动态重新计算分数；3) 逐步移除重要性最低的注意力头

Result: 在BERT、ALBERT、RoBERTa和XLM-RoBERTa上的实验表明，Greedy-Gnorm在大量移除注意力头的情况下仍能保持准确率，优于注意力熵方法

Conclusion: Greedy-Gnorm通过动态梯度重要性评估有效压缩Transformer模型大小同时保持性能，为实现更节能的Transformer模型部署提供了有前景的方法

Abstract: Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.

</details>


### [85] [Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning](https://arxiv.org/abs/2602.04536)
*Abdulrahman Alotaibi,Irene Tenison,Miriam Kim,Isaac Lee,Lalana Kagal*

Main category: cs.LG

TL;DR: 提出迭代联邦适应(IFA)方法，通过代际遗忘与进化策略提升异构联邦学习中的泛化能力，在非IID数据分布下平均提升21.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 现实Web系统天然具有异构性（用户设备、地理区域、浏览模式等），导致高度多样化的非IID数据分布。联邦学习(FL)虽能实现隐私保护的协作学习，但在非IID客户端分布下性能严重下降，需要提升异构联邦设置中的泛化能力。

Method: 提出迭代联邦适应(IFA)训练范式：将训练分为多个代际，每代结束时选择部分模型参数（随机或从后层）进行重新初始化。这种迭代遗忘与进化策略让模型逃离局部最优，保留全局相关表示。该方法可应用于任何联邦算法之上。

Result: 在CIFAR-10、MIT-Indoors和Stanford Dogs数据集上的实验表明，该方法显著提升全局准确率，特别是在跨客户端数据非IID时。平均获得21.5%的改进，有效增强异构联邦学习中的泛化性能。

Conclusion: IFA通过代际遗忘与进化策略有效解决联邦学习在非IID数据下的性能退化问题，为实现可扩展、隐私保护的异构分布式Web系统智能提供了重要进展。

Abstract: The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.

</details>


### [86] [Continual Learning through Control Minimization](https://arxiv.org/abs/2602.04542)
*Sander de Haan,Yassine Taoudi-Benchekroun,Pau Vilimelis Aceituno,Benjamin F. Grewe*

Main category: cs.LG

TL;DR: 将持续学习重新定义为控制问题，通过保护信号与学习信号的竞争来防止灾难性遗忘，无需显式存储曲率信息


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在顺序学习任务时面临的灾难性遗忘问题，传统方法需要显式存储曲率信息或使用回放机制

Method: 将持续学习重新表述为控制问题，将正则化惩罚转换为保护信号来保护先前任务的表示，通过最小化控制努力来整合新任务，同时与先前任务的保护竞争

Result: 该方法能够恢复真实的先前任务曲率，实现任务区分，在标准基准测试中优于现有方法且无需回放机制

Conclusion: 通过控制理论框架，神经活动在平衡时产生隐式编码完整先前任务曲率的权重更新，称为持续自然梯度，为持续学习提供了新的有效方法

Abstract: Catastrophic forgetting remains a fundamental challenge for neural networks when tasks are trained sequentially. In this work, we reformulate continual learning as a control problem where learning and preservation signals compete within neural activity dynamics. We convert regularization penalties into preservation signals that protect prior-task representations. Learning then proceeds by minimizing the control effort required to integrate new tasks while competing with the preservation of prior tasks. At equilibrium, the neural activities produce weight updates that implicitly encode the full prior-task curvature, a property we term the continual-natural gradient, requiring no explicit curvature storage. Experiments confirm that our learning framework recovers true prior-task curvature and enables task discrimination, outperforming existing methods on standard benchmarks without replay.

</details>


### [87] [Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions](https://arxiv.org/abs/2602.04548)
*Dmitry Yarotsky,Eugene Golikov,Yaroslav Gusev*

Main category: cs.LG

TL;DR: 该论文提出了一个分析大规模学习问题中梯度流缩放机制的数学框架，使用类似费曼图的图表示法展开损失演化，揭示了张量CP分解的不同学习阶段，并提出了将形式展开求和简化为可解PDE的方法。


<details>
  <summary>Details</summary>
Motivation: 需要理解大规模学习问题中梯度流的缩放机制，特别是在高维张量分解等复杂模型中，不同参数缩放如何影响学习动态和收敛行为。

Method: 开发了基于形式幂级数展开的数学框架，使用类似费曼图的图表示编码系数，分析大规模极限下的学习动态，特别关注张量CP分解模型，并将形式展开求和简化为可解的一阶PDE。

Result: 发现了张量CP分解模型存在多种极端懒惰和丰富梯度流机制，包括自由演化、NTK以及欠参数化和过参数化平均场等不同学习阶段，这些机制依赖于参数缩放、张量阶数和模型对称性。

Conclusion: 提出的数学框架能够有效分析大规模学习中的梯度流缩放机制，理论预测与实验结果高度一致，为理解复杂模型的学习动态提供了系统化工具。

Abstract: We develop a general mathematical framework to analyze scaling regimes and derive explicit analytic solutions for gradient flow (GF) in large learning problems. Our key innovation is a formal power series expansion of the loss evolution, with coefficients encoded by diagrams akin to Feynman diagrams. We show that this expansion has a well-defined large-size limit that can be used to reveal different learning phases and, in some cases, to obtain explicit solutions of the nonlinear GF. We focus on learning Canonical Polyadic (CP) decompositions of high-order tensors, and show that this model has several distinct extreme lazy and rich GF regimes such as free evolution, NTK and under- and over-parameterized mean-field. We show that these regimes depend on the parameter scaling, tensor order, and symmetry of the model in a specific and subtle way. Moreover, we propose a general approach to summing the formal loss expansion by reducing it to a PDE; in a wide range of scenarios, it turns out to be 1st order and solvable by the method of characteristics. We observe a very good agreement of our theoretical predictions with experiment.

</details>


### [88] [Finding Structure in Continual Learning](https://arxiv.org/abs/2602.04555)
*Pourya Shamsolmoali,Masoumeh Zareapoor*

Main category: cs.LG

TL;DR: 使用Douglas-Rachford Splitting方法将持续学习重构为可塑性和稳定性两个解耦目标的协商过程，避免梯度冲突和复杂策略


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法通常通过求和竞争损失项来处理可塑性与稳定性之间的权衡，这会导致梯度冲突，需要复杂低效的策略如外部记忆回放或参数正则化

Method: 提出使用Douglas-Rachford Splitting方法重构持续学习目标，将学习过程视为可塑性（新任务）和稳定性（旧知识）两个解耦目标之间的协商，通过近端算子迭代寻找共识

Result: 该方法在可塑性与稳定性之间实现了高效平衡，无需辅助模块或复杂附加组件，为持续学习系统提供了更简单但更强大的范式

Conclusion: DRS方法为持续学习提供了更原则性和稳定的学习动态，通过解耦目标和协商机制解决了传统方法的梯度冲突问题

Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.

</details>


### [89] [Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs](https://arxiv.org/abs/2602.04574)
*Jonathan Klees,Tobias Riedlinger,Peter Stehr,Bennet Böddecker,Daniel Kondermann,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出一种概率标签传播方法，通过图扩散传播单标注来估计标签的偶然和认知不确定性，显著减少标注预算并达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 感知任务的安全AI面临高质量标注数据不足的挑战，标注本身存在偶然和认知不确定性，而传统众包多标注方法在大规模应用中不切实际。

Method: 基于特征空间标签平滑性假设，使用图扩散方法传播单标注，构建概率标签传播算法，即使在每个数据点标注数趋近于零时也能提供一致的概率估计。

Result: 相比基线方法，该方法在常见图像数据集上显著减少达到目标标签质量所需的标注预算，并在Data-Centric图像分类基准上达到新的最先进水平。

Conclusion: 提出的概率标签传播方法能够可靠估计标签不确定性，为大规模感知任务提供高效的数据标注解决方案，推动安全AI发展。

Abstract: Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.

</details>


### [90] [Stochastic Decision Horizons for Constrained Reinforcement Learning](https://arxiv.org/abs/2602.04599)
*Nikola Milosevic,Leonard Franz,Daniel Haeufle,Georg Martius,Nico Scherf,Pavel Kolev*

Main category: cs.LG

TL;DR: 提出基于随机决策时域的Control as Inference框架，通过状态-动作相关延续机制将约束违反转化为奖励衰减和规划时域缩短，实现与经验回放兼容的生存加权目标，提升离策略学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统CMDP方法使用加性成本约束和对偶变量，阻碍了离策略学习的可扩展性。需要一种既能处理约束又能保持离策略学习效率的新框架。

Method: 基于随机决策时域的Control as Inference框架，约束违反会衰减奖励贡献并通过状态-动作相关延续缩短有效规划时域，产生生存加权目标。提出两种违反语义：吸收终止和虚拟终止，共享相同的生存加权回报但形成不同的优化结构，分别对应SAC和MPO风格的策略改进。

Result: 实验显示在标准基准上提高了样本效率，获得了有利的回报-违反权衡。虚拟终止MPO（VT-MPO）能有效扩展到高维肌肉骨骼Hyfydy设置。

Conclusion: 提出的生存加权目标框架成功解决了CMDP中离策略学习的可扩展性问题，通过将约束违反转化为奖励衰减和规划时域缩短，实现了与经验回放兼容的高效学习。

Abstract: Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.

</details>


### [91] [Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations](https://arxiv.org/abs/2602.04608)
*Maya Janvier,Julien Salomon,Etienne Meunier*

Main category: cs.LG

TL;DR: 提出两种基于雅可比方向导数正则化的方法，用于稳定神经微分方程的长期积分，降低训练成本


<details>
  <summary>Details</summary>
Motivation: 混合模型和神经微分方程在物理系统建模中越来越重要，但长期积分时存在稳定性和精度问题。传统基于展开轨迹的训练方法虽然能限制发散，但计算梯度成本过高

Method: 设计了两种正则化方法：1）已知动力学时直接推导动态的方向导数；2）未知动力学时使用有限差分近似方向导数。两种方法都通过正则化NDE模型的雅可比矩阵来稳定长期积分

Result: 两种方法在训练成本远低于长轨迹展开的情况下，成功提高了多个常微分方程和偏微分方程长期模拟的稳定性

Conclusion: 该方法为训练大规模系统长期积分的NDE方法打开了大门，在保持低成本的同时显著改善了长期模拟的稳定性

Abstract: Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.

</details>


### [92] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: AdaCNP是一种用于极端天气下电力负荷概率预测的模型，通过共享嵌入空间学习相似性，自适应重加权历史上下文信息，实现少样本适应和可靠的概率输出。


<details>
  <summary>Details</summary>
Motivation: 极端天气会显著改变电力消费行为，导致负荷曲线出现尖峰和剧烈波动。不准确的预测可能导致电力系统供应短缺或局部过载，需要紧急措施如减载，增加服务中断和公共安全风险。问题难点在于极端事件会触发负荷模式的突然变化，而相关极端样本稀少且不规则，使得可靠学习和校准具有挑战性。

Method: AdaCNP是一种用于数据稀缺条件下的概率预测模型。它在共享嵌入空间中学习相似性，对每个目标数据评估历史上下文段与当前条件的相关性，并相应地对上下文信息进行重加权。这种设计即使在极端样本稀少时也能突出最有信息量的历史证据，实现对新极端模式的少样本适应，并产生用于风险感知决策的预测分布，无需在目标域上进行昂贵的微调。

Result: 在真实电力系统负荷数据上的评估显示，AdaCNP在极端时期更加稳健，相对于最强基线减少了22%的均方误差，同时实现了最低的负对数似然，表明其概率输出更加可靠。

Conclusion: AdaCNP能有效缓解突然分布变化和极端样本稀缺的综合影响，为极端事件下的弹性电力系统运行提供更可信的预测。

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [93] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: QUATRO提出了一种基于精确信任域约束的强化学习微调方法，解决了现有GRPO风格方法中启发式信任域近似导致的优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO风格的强化学习微调算法依赖启发式的信任域近似，导致优化行为脆弱。全局重要性比率裁剪和分组归一化无法有效调节超出裁剪范围的样本，需要更原则性的优化方法。

Method: QUATRO通过精确的信任域约束直接强制执行策略更新限制，采用原则性优化方法。该方法产生清晰可解释的目标函数，能够显式控制策略更新，实现稳定、熵控制的优化，稳定项自然地从精确信任域公式中产生。

Result: 在多种数学推理基准测试中，QUATRO在增加策略陈旧性和激进学习率的情况下仍能保持稳定训练，在整个训练过程中维持良好控制的熵。

Conclusion: QUATRO提供了一种更稳定、可控的强化学习微调方法，通过精确信任域约束解决了现有启发式方法的局限性，为LLM微调提供了更可靠的优化框架。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [94] [RIGA-Fold: A General Framework for Protein Inverse Folding via Recurrent Interaction and Geometric Awareness](https://arxiv.org/abs/2602.04637)
*Sisi Yuan,Jiehuang Chen,Junchuang Cai,Dong Xu,Xueliang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: RIGA-Fold是一个用于蛋白质逆折叠的几何感知循环交互框架，通过几何注意力更新模块和全局上下文桥接解决现有GNN方法感受野受限和单次推理误差累积问题，其增强版本RIGA-Fold*结合ESM先验知识，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的蛋白质逆折叠方法存在两个主要瓶颈：1）受限的感受野无法捕捉长程依赖关系；2）"单次"推理范式导致误差累积。需要新的框架来解决这些问题。

Method: 提出RIGA-Fold框架：1）微观层面使用几何注意力更新（GAU）模块，以边特征作为注意力键确保SE(3)不变性；2）宏观层面设计基于注意力的全局上下文桥接作为软门控机制；3）增强版RIGA-Fold*集成可训练几何特征与ESM-2/ESM-IF的冻结进化先验；4）采用"预测-循环-精炼"的迭代去噪策略。

Result: 在CATH 4.2、TS50和TS500基准测试上的广泛实验表明，几何框架具有高度竞争力，RIGA-Fold*在序列恢复和结构一致性方面显著优于现有最先进基线方法。

Conclusion: RIGA-Fold通过结合几何感知、全局上下文桥接和进化先验，有效解决了蛋白质逆折叠中的长程依赖和误差累积问题，为蛋白质设计提供了强大的新框架。

Abstract: Protein inverse folding, the task of predicting amino acid sequences for desired structures, is pivotal for de novo protein design. However, existing GNN-based methods typically suffer from restricted receptive fields that miss long-range dependencies and a "single-pass" inference paradigm that leads to error accumulation. To address these bottlenecks, we propose RIGA-Fold, a framework that synergizes Recurrent Interaction with Geometric Awareness. At the micro-level, we introduce a Geometric Attention Update (GAU) module where edge features explicitly serve as attention keys, ensuring strictly SE(3)-invariant local encoding. At the macro-level, we design an attention-based Global Context Bridge that acts as a soft gating mechanism to dynamically inject global topological information. Furthermore, to bridge the gap between structural and sequence modalities, we introduce an enhanced variant, RIGA-Fold*, which integrates trainable geometric features with frozen evolutionary priors from ESM-2 and ESM-IF via a dual-stream architecture. Finally, a biologically inspired ``predict-recycle-refine'' strategy is implemented to iteratively denoise sequence distributions. Extensive experiments on CATH 4.2, TS50, and TS500 benchmarks demonstrate that our geometric framework is highly competitive, while RIGA-Fold* significantly outperforms state-of-the-art baselines in both sequence recovery and structural consistency.

</details>


### [95] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: 提出MTS-JEPA架构，通过多分辨率预测目标和软码本瓶颈解决多元时间序列异常预测中的表示坍塌和跨时间尺度信号捕捉问题


<details>
  <summary>Details</summary>
Motivation: 多元时间序列对关键基础设施至关重要，需要提前预测异常以主动降低风险。现有JEPA框架存在表示坍塌问题，且无法捕捉不同时间尺度的前兆信号

Method: 提出MTS-JEPA架构，整合多分辨率预测目标和软码本瓶颈，显式解耦瞬态冲击和长期趋势，利用码本捕捉离散状态转换

Result: 该方法有效防止退化解，在早期预警协议下达到最先进性能，码本约束还起到内在正则化作用确保优化稳定性

Conclusion: MTS-JEPA成功解决了JEPA在多元时间序列预测中的局限性，为关键基础设施的异常预测提供了有效框架

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [96] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: SAFE是一种新的RLHF算法，通过双软最小评论家、熵门控KL调节和PID控制自适应阈值，解决了PPO在语言模型RLHF中的不稳定问题，实现了更稳定高效的训练。


<details>
  <summary>Details</summary>
Motivation: PPO作为RLHF中的标准RL方法存在启发式动机、KL约束处理随意、奖励振荡、熵崩溃、价值函数漂移和策略突然发散等问题，需要频繁重启和大量超参数调优。

Method: 提出SAFE算法：1）双软最小评论家用于悲观价值估计；2）多层稳定框架结合熵门控KL调节；3）PID控制自适应阈值。与PPO的对称KL惩罚不同，SAFE区分高熵探索和低熵模式崩溃，并根据奖励速度动态调整惩罚。

Result: 在3B参数模型上，SAFE比PPO训练平均奖励提升5.15%（0.725 vs 0.689），奖励崩溃可忽略，KL控制更优，计算开销最小，提供可解释、抗崩溃的RLHF框架。

Conclusion: SAFE提供了一个稳定、可解释的RLHF框架，在保持快速学习的同时确保长期优化稳定性，适合生产部署。

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [97] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 该论文系统分析了扩散模型强化学习的设计空间，发现基于ELBO的似然估计是影响RL优化效果的关键因素，而非特定的策略梯度损失函数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉任务中应用广泛，但由于其似然函数难以处理，直接应用策略梯度方法存在障碍。现有方法主要基于LLM目标构建新目标，使用临时估计器，缺乏对估计如何影响算法性能的系统分析。

Method: 通过解耦三个因素进行系统分析：1) 策略梯度目标；2) 似然估计器；3) 轨迹采样方案。研究发现基于ELBO的模型似然估计器（仅从最终生成样本计算）是实现有效、高效和稳定RL优化的关键因素。

Result: 在多个奖励基准测试中使用SD 3.5 Medium验证，所有任务均显示一致趋势。方法在90 GPU小时内将GenEval分数从0.24提升到0.95，比FlowGRPO效率高4.6倍，比SOTA方法DiffusionNFT效率高2倍且无奖励黑客问题。

Conclusion: 基于ELBO的似然估计是扩散模型强化学习优化的主导因素，其重要性超过特定策略梯度损失函数的选择，为扩散模型的RL优化提供了系统性的设计指导。

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [98] [Delving into Muon and Beyond: Deep Analysis and Extensions](https://arxiv.org/abs/2602.04669)
*Xianbiao Qi,Marco Chen,Jiaquan Ye,Yelin He,Rong Xiao*

Main category: cs.LG

TL;DR: Muon优化器通过谱变换视角被重新审视，研究发现其本质是一种有效的谱归一化方法，但并非普遍优于Adam的优化器。


<details>
  <summary>Details</summary>
Motivation: Muon优化器因其强大的经验性能和正交化更新而受到关注，但其底层机制与Adam等自适应优化器的关系尚不明确。本研究旨在通过统一的谱视角来理解这些问题。

Method: 将Muon视为谱变换族UΣ^pV'的p=0端点，考虑p=1/2、p=1/4和p=1的变体。这些变换应用于一阶矩更新（如动量SGD）和RMS归一化梯度更新（如Adam）。开发了耦合牛顿迭代以避免显式奇异值分解。

Result: RMS归一化更新比一阶矩更新更稳定；谱压缩在一阶矩更新下提供强稳定化效果；Muon更新(p=0)并不始终优于Adam。

Conclusion: Muon最好被理解为一种有效的谱归一化形式，但不是普遍优越的优化方法。谱视角为理解Muon与自适应优化器的关系提供了统一框架。

Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \boldsymbolΣ^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.

</details>


### [99] [Generalized Schrödinger Bridge on Graphs](https://arxiv.org/abs/2602.04675)
*Panagiotis Theodoropoulos,Juno Nam,Evangelos Theodorou,Jaemoo Choi*

Main category: cs.LG

TL;DR: GSBoG是一个可扩展的数据驱动框架，用于在任意图上学习可执行的连续时间马尔可夫链策略，通过似然优化方法学习轨迹级策略，避免全局求解器，提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图传输方法缺乏可执行策略的表达能力，依赖限制性假设，无法在稀疏拓扑上泛化，且随着图大小和时间范围扩展性差。需要一种能够学习可执行策略、尊重拓扑约束并优化应用特定成本的方法。

Method: 提出广义薛定谔桥在图上的框架(GSBoG)，通过似然优化方法学习轨迹级策略，满足端点边际分布，同时优化状态相关运行成本下的中间行为，避免使用密集的全局求解器。

Result: 在具有挑战性的真实世界图拓扑上进行广泛实验，GSBoG能够可靠地学习准确、尊重拓扑的策略，同时优化应用特定的中间状态成本，展示了其广泛的适用性。

Conclusion: GSBoG为一般图上的成本感知动态传输开辟了新途径，提供了一个可扩展的数据驱动框架，能够学习可执行策略并优化应用特定成本。

Abstract: Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schrödinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.

</details>


### [100] [REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency](https://arxiv.org/abs/2602.04677)
*Ondrej Tybl,Lukas Neumann*

Main category: cs.LG

TL;DR: REDistill提出了一种基于鲁棒统计的知识蒸馏框架，用幂散度损失替代传统KL散度，自适应降低不可靠教师输出的权重，无需特定超参数调优即可提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法基于KL散度，假设教师模型提供可靠的软目标，但实际上教师预测往往存在噪声或过度自信。现有校正方法依赖启发式规则和大量超参数调优，泛化能力受限。

Method: 提出REDistill框架，基于鲁棒统计理论，用幂散度损失替代标准知识蒸馏目标。该损失函数是KL散度的泛化，能自适应降低不可靠教师输出的权重，同时保留信息丰富的logit关系。仅需logits，无缝集成到现有蒸馏流程，计算开销可忽略。

Result: 在CIFAR-100和ImageNet-1k上的大量实验表明，REDistill在不同师生架构中持续提升学生模型准确率。无需针对特定模型进行超参数调优，展示了其鲁棒性和对未见师生对的强泛化能力。

Conclusion: REDistill提供了一个简单而原则性的知识蒸馏框架，通过鲁棒统计方法有效处理教师噪声，无需复杂调优即可在各种师生架构中实现稳定性能提升，具有良好的泛化能力。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.

</details>


### [101] [Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting](https://arxiv.org/abs/2602.04678)
*Zhen Zhou,Zhirui Wang,Qi Hong,Yunyang Shi,Ziyuan Gu,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 提出多专家学习分布标签框架，通过混合专家架构和分布学习能力，在时间序列预测中平衡预测精度与可解释的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测需要高预测精度和可解释的不确定性量化。传统点预测方法无法捕捉数据内在不确定性，而现有概率方法难以平衡计算效率与可解释性。

Method: 提出两种互补方法：1) 多专家LDL，使用具有不同学习参数的多个专家捕捉多样时间模式；2) 模式感知LDL-MoE，通过专门子专家将时间序列分解为可解释组件（趋势、季节性、变化点、波动性）。两种框架都扩展传统点预测为分布学习，通过最大均值差异实现丰富的不确定性量化。

Result: 在基于M5数据集的聚合销售数据上评估，相比基线方法表现更优。连续多专家LDL获得最佳整体性能，模式感知LDL-MoE通过组件分析提供增强的可解释性。

Conclusion: 该框架成功平衡预测精度与可解释性，适用于需要性能和可操作洞察的现实世界预测应用。

Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.

</details>


### [102] [Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean](https://arxiv.org/abs/2602.04689)
*Mahima Lakra,Ronan Fablet,Lucas Drumetz,Etienne Pauthenet,Elodie Martinez*

Main category: cs.LG

TL;DR: 该研究探索使用深度学习模型预测全球海洋浮游植物生物量的时空分布，发现UNet架构在重现季节和年际模式方面表现最佳，自回归UNet可用于短期预测（最多5个月）。


<details>
  <summary>Details</summary>
Motivation: 浮游植物是海洋食物网的基础，对生态过程和全球生物地球化学循环至关重要。然而，由于参数化有限、观测数据稀疏以及海洋过程复杂，准确模拟浮游植物动态仍然是生物地球化学数值模型的主要挑战。

Method: 研究探索了多种深度学习架构，包括CNN、ConvLSTM、4CastNet和UNet，使用卫星观测和环境条件数据预测浮游植物生物量。特别测试了使用1-2个月环境数据作为输入的UNet，以及自回归版本的UNet（利用自身先前预测来预测未来条件）。

Result: UNet架构在重现浮游植物生物量的季节和年际模式方面表现最佳，优于其他模型。使用1-2个月输入数据时表现更好，但倾向于低估低频变化的幅度。自回归UNet在短期预测（最多5个月）中表现良好，但长期预测性能下降。

Conclusion: 结合海洋物理预测因子与深度学习可以重建和短期预测浮游植物动态。这些模型可能成为监测海洋健康和支持海洋生态系统管理的强大工具，特别是在气候变化背景下。

Abstract: Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.

</details>


### [103] [Towards Understanding and Avoiding Limitations of Convolutions on Graphs](https://arxiv.org/abs/2602.04709)
*Andreas Roth*

Main category: cs.LG

TL;DR: 该论文对消息传递神经网络(MPNNs)进行了深入理论分析，识别了限制其性能的关键属性，并提出了解决这些问题的多个框架。


<details>
  <summary>Details</summary>
Motivation: 尽管MPNNs显示出有希望的结果，但其实际应用影响仍然有限。虽然已经识别了各种限制，但其理论基础仍然理解不足，导致研究努力分散。需要深入的理论分析来理解MPNNs的根本限制。

Method: 1. 识别MPNNs的两个关键属性：共享组件放大(SCA)和组件主导(CD)；2. 提出多关系分割(MRS)框架来避免SCA；3. 引入多特征通道的谱图卷积(MIMO-GC)及其局部变体LMGC；4. 基于个性化PageRank提出MPNNs变体来解决CD问题。

Result: 1. 识别了导致节点表示秩崩溃的SCA和CD属性；2. 将过平滑现象推广和分解，实现更深入的理解；3. 提出了避免SCA和CD的具体解决方案框架；4. 建立了MPNNs与PageRank算法之间的紧密联系。

Conclusion: 这些结果深化了对MPNNs的理论理解，提供了更针对性的解决方案，并促进了该领域内更精确的交流。通过解决SCA和CD问题，论文为改进MPNNs性能提供了理论基础和实用框架。

Abstract: While message-passing neural networks (MPNNs) have shown promising results, their real-world impact remains limited. Although various limitations have been identified, their theoretical foundations remain poorly understood, leading to fragmented research efforts. In this thesis, we provide an in-depth theoretical analysis and identify several key properties limiting the performance of MPNNs. Building on these findings, we propose several frameworks that address these shortcomings. We identify two properties exhibited by many MPNNs: shared component amplification (SCA), where each message-passing iteration amplifies the same components across all feature channels, and component dominance (CD), where a single component gets increasingly amplified as more message-passing steps are applied. These properties lead to the observable phenomenon of rank collapse of node representations, which generalizes the established over-smoothing phenomenon. By generalizing and decomposing over-smoothing, we enable a deeper understanding of MPNNs, more targeted solutions, and more precise communication within the field. To avoid SCA, we show that utilizing multiple computational graphs or edge relations is necessary. Our multi-relational split (MRS) framework transforms any existing MPNN into one that leverages multiple edge relations. Additionally, we introduce the spectral graph convolution for multiple feature channels (MIMO-GC), which naturally uses multiple computational graphs. A localized variant, LMGC, approximates the MIMO-GC while inheriting its beneficial properties. To address CD, we demonstrate a close connection between MPNNs and the PageRank algorithm. Based on personalized PageRank, we propose a variant of MPNNs that allows for infinitely many message-passing iterations, while preserving initial node features. Collectively, these results deepen the theoretical understanding of MPNNs.

</details>


### [104] [Bounded-Abstention Multi-horizon Time-series Forecasting](https://arxiv.org/abs/2602.04714)
*Luca Stradiotti,Laurens Devos,Anna Monreale,Jesse Davis,Andrea Pugnana*

Main category: cs.LG

TL;DR: 该论文针对多时间步预测中的弃权学习问题，提出了三种弃权策略并设计了相应算法，在24个数据集上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 多时间步预测在医疗、金融等高风险领域应用广泛，但现有弃权学习策略仅适用于单预测场景，无法处理多时间步预测的结构化相关性，导致模型在不确定性高时无法有效弃权。

Method: 形式化了多时间步预测中的弃权学习问题，提出了三种自然弃权概念：逐点弃权、序列弃权和混合弃权。通过理论分析推导最优弃权策略，并设计实现算法。

Result: 在24个数据集上的广泛评估表明，提出的算法显著优于现有基线方法，能够有效降低错误预测风险。

Conclusion: 多时间步预测的弃权学习具有结构化特性，需要专门设计的弃权策略。提出的三种弃权概念和相应算法为高风险应用提供了更可靠的预测框架。

Abstract: Multi-horizon time-series forecasting involves simultaneously making predictions for a consecutive sequence of subsequent time steps. This task arises in many application domains, such as healthcare and finance, where mispredictions can have a high cost and reduce trust. The learning with abstention framework tackles these problems by allowing a model to abstain from offering a prediction when it is at an elevated risk of making a misprediction. Unfortunately, existing abstention strategies are ill-suited for the multi-horizon setting: they target problems where a model offers a single prediction for each instance. Hence, they ignore the structured and correlated nature of the predictions offered by a multi-horizon forecaster. We formalize the problem of learning with abstention for multi-horizon forecasting setting and show that its structured nature admits a richer set of abstention problems. Concretely, we propose three natural notions of how a model could abstain for multi-horizon forecasting. We theoretically analyze each problem to derive the optimal abstention strategy and propose an algorithm that implements it. Extensive evaluation on 24 datasets shows that our proposed algorithms significantly outperforms existing baselines.

</details>


### [105] [Identifying Intervenable and Interpretable Features via Orthogonality Regularization](https://arxiv.org/abs/2602.04718)
*Moritz Miller,Florent Draye,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 通过正交化惩罚改进稀疏自编码器，减少特征间的干扰和叠加，同时保持性能不变，提高特征的可识别性和可解释性


<details>
  <summary>Details</summary>
Motivation: 当前基于固定稀疏自编码器的语言模型微调中，特征之间存在干扰和叠加问题，这影响了特征的可识别性和可解释性。研究者希望通过正交化方法减少特征间的干扰，同时保持模型性能

Method: 在稀疏自编码器的解码器矩阵上施加正交化惩罚，使特征几乎正交。该方法确保分解的唯一性，同时保持目标数据集上的性能基本不变。通过增加正交化惩罚的强度，可以控制特征间的正交程度

Result: 正交化惩罚成功减少了特征间的干扰和叠加，同时保持了模型性能。随着正交化惩罚的增强，特征解释之间的距离增加，这有利于可解释性。实验表明，这些正交化特征支持独立的干预操作

Conclusion: 正交化方法能够产生模块化的表示，符合独立因果机制原则，有利于因果干预。该方法提高了特征的可识别性和可解释性，为语言模型的解释性研究提供了新思路

Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\texttt{https://github.com/mrtzmllr/sae-icm}$.

</details>


### [106] [Benchmarking and Enhancing PPG-Based Cuffless Blood Pressure Estimation Methods](https://arxiv.org/abs/2602.04725)
*Neville Mathew,Yidan Shen,Renjie Hu,Maham Rahimi,George Zouridakis*

Main category: cs.LG

TL;DR: 基于PPG的无袖带血压筛查研究，创建标准化基准数据集NBPDB，评估现有模型均未达到临床标准，通过添加人口统计学数据显著提升模型性能，MInception模型达到AAMI/ISO标准可比精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于PPG的血压估计模型在临床数值标准（AAMI/ISO 81060-2）上表现不一致，且公开数据集缺乏生理控制条件下的公平基准测试，需要建立标准化评估框架。

Method: 创建标准化基准数据集NBPDB（来自MIMIC-III和VitalDB的101,453个高质量PPG片段），系统评估多个SOTA模型，通过修改模型架构并添加年龄、性别、BMI等人口统计学数据来提升性能。

Result: 所有评估模型均未达到AAMI/ISO标准（平均误差<5 mmHg，标准差<8 mmHg）；添加人口统计学数据后所有模型性能一致提升，MInception模型误差降低23%，达到4.75 mmHg（SBP）和2.90 mmHg（DBP）的平均绝对误差，接近临床标准。

Conclusion: 现有PPG血压估计模型在标准化条件下缺乏临床实用性，但通过整合人口统计学信息可显著提升准确性和生理有效性，为可扩展心血管健康评估提供改进方向。

Abstract: Cuffless blood pressure screening based on easily acquired photoplethysmography (PPG) signals offers a practical pathway toward scalable cardiovascular health assessment. Despite rapid progress, existing PPG-based blood pressure estimation models have not consistently achieved the established clinical numerical limits such as AAMI/ISO 81060-2, and prior evaluations often lack the rigorous experimental controls necessary for valid clinical assessment. Moreover, the publicly available datasets commonly used are heterogeneous and lack physiologically controlled conditions for fair benchmarking. To enable fair benchmarking under physiologically controlled conditions, we created a standardized benchmarking subset NBPDB comprising 101,453 high-quality PPG segments from 1,103 healthy adults, derived from MIMIC-III and VitalDB. Using this dataset, we systematically benchmarked several state-of-the-art PPG-based models. The results showed that none of the evaluated models met the AAMI/ISO 81060-2 accuracy requirements (mean error $<$ 5 mmHg and standard deviation $<$ 8 mmHg). To improve model accuracy, we modified these models and added patient demographic data such as age, sex, and body mass index as additional inputs. Our modifications consistently improved performance across all models. In particular, the MInception model reduced error by 23\% after adding the demographic data and yielded mean absolute errors of 4.75 mmHg (SBP) and 2.90 mmHg (DBP), achieves accuracy comparable to the numerical limits defined by AAMI/ISO accuracy standards. Our results show that existing PPG-based BP estimation models lack clinical practicality under standardized conditions, while incorporating demographic information markedly improves their accuracy and physiological validity.

</details>


### [107] [DMFlow: Disordered Materials Generation by Flow Matching](https://arxiv.org/abs/2602.04734)
*Liming Wu,Rui Jiao,Qi Li,Mingze Li,Songyou Li,Shifeng Jin,Wenbing Huang*

Main category: cs.LG

TL;DR: DMFlow是一个专门为无序晶体设计的生成框架，通过流匹配模型联合生成所有结构组件，在晶体结构预测和新结构生成任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数深度生成模型只关注完美有序晶体，忽略了无序材料这一重要类别。为了填补这一空白，需要开发专门针对无序晶体的生成框架。

Method: 1) 引入有序、置换无序和位置无序晶体的统一表示；2) 使用黎曼流匹配框架和球面重参数化确保概率单纯形上的物理有效无序权重；3) 采用包含物理对称性和专门消息传递方案的新型图神经网络学习向量场；4) 通过两阶段离散化过程将连续权重转换为多热原子分配。

Result: 在晶体结构预测和新结构生成任务上，DMFlow显著优于从有序晶体生成改编的最先进基线方法。作者还发布了从晶体学开放数据库中整理的包含置换无序、位置无序和混合结构的基准数据集。

Conclusion: DMFlow为AI驱动的无序材料发现提供了基础，填补了当前生成模型在无序晶体领域的空白，有望推动材料科学中这一重要类别的研究。

Abstract: The design of materials with tailored properties is crucial for technological progress. However, most deep generative models focus exclusively on perfectly ordered crystals, neglecting the important class of disordered materials. To address this gap, we introduce DMFlow, a generative framework specifically designed for disordered crystals. Our approach introduces a unified representation for ordered, Substitutionally Disordered (SD), and Positionally Disordered (PD) crystals, and employs a flow matching model to jointly generate all structural components. A key innovation is a Riemannian flow matching framework with spherical reparameterization, which ensures physically valid disorder weights on the probability simplex. The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme. Finally, a two-stage discretization procedure converts the continuous weights into multi-hot atomic assignments. To support research in this area, we release a benchmark containing SD, PD, and mixed structures curated from the Crystallography Open Database. Experiments on Crystal Structure Prediction (CSP) and De Novo Generation (DNG) tasks demonstrate that DMFlow significantly outperforms state-of-the-art baselines adapted from ordered crystal generation. We hope our work provides a foundation for the AI-driven discovery of disordered materials.

</details>


### [108] [From Data to Behavior: Predicting Unintended Model Behaviors Before Training](https://arxiv.org/abs/2602.04735)
*Mengru Wang,Zhenqian Xu,Junfeng Fang,Yunzhi Yao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.LG

TL;DR: 提出Data2Behavior任务和MDF方法，用于在训练前预测大语言模型可能从数据中习得的非预期偏见，无需微调即可评估数据风险


<details>
  <summary>Details</summary>
Motivation: 大语言模型即使从看似良性的训练数据中也可能习得非预期的偏见，而现有方法难以在微调前检测这些风险，导致事后评估成本高且效率低

Method: 提出Manipulating Data Features (MDF)方法：通过候选数据的平均表示来总结数据特征，将其注入基础模型的前向传播中，让数据中的潜在统计信号影响模型激活，从而揭示潜在偏见和安全风险，无需更新任何参数

Result: MDF方法仅需约20%的GPU资源（相比微调），在Qwen3-14B、Qwen2.5-32B-Instruct和Gemma-3-12b-it等模型上实验证实，能够有效预测非预期行为并揭示预训练漏洞

Conclusion: Data2Behavior任务和MDF方法为训练前数据风险评估提供了高效解决方案，能够在资源消耗较少的情况下预测模型可能习得的非预期偏见，有助于提高模型安全性

Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.

</details>


### [109] [Rationality Measurement and Theory for Reinforcement Learning Agents](https://arxiv.org/abs/2602.04737)
*Kejiang Qian,Amos Storkey,Fengxiang He*

Main category: cs.LG

TL;DR: 本文提出了强化学习智能体的理性度量框架，定义了理性风险及其分解，为算法在动态环境中的泛化性提供了理论分析。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的理性属性日益重要但鲜有研究，需要建立理论框架来量化智能体在训练和部署环境中的理性表现差异。

Method: 定义完美理性动作、期望理性风险和理性风险缺口，将风险缺口分解为外在环境偏移和内在算法泛化性两部分，并用1-Wasserstein距离和Rademacher复杂度上界分析。

Result: 理论分析表明正则化方法（层归一化、L2正则化、权重归一化）和领域随机化有助于降低理性风险，而环境偏移会增大风险，实验结果完全支持这些假设。

Conclusion: 提出的理性度量框架为强化学习智能体的理性分析提供了理论基础，揭示了环境偏移和算法泛化性对理性风险的影响，为改进算法设计提供了指导。

Abstract: This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.

</details>


### [110] [Decomposing Query-Key Feature Interactions Using Contrastive Covariances](https://arxiv.org/abs/2602.04752)
*Andrew Lee,Yonatan Belinkov,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 提出一种分析Transformer注意力头的新方法，通过研究查询-键（QK）空间，使用对比协方差方法将其分解为低秩、可解释的组件，以理解模型为何关注特定token。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力头在Transformer中处于核心地位，但缺乏工具来理解模型为何关注特定token。需要开发方法来解释注意力机制的工作原理。

Method: 研究查询-键（QK）空间——查询和键之间的双线性联合嵌入空间。提出对比协方差方法，将QK空间分解为低秩、人类可解释的组件。当键和查询中的特征在这些低秩子空间中对齐时，会产生高注意力分数。

Result: 首先在简化设置中进行分析和实证研究，然后将该方法应用于大型语言模型，识别出分类语义特征和绑定特征的可解释QK子空间。最后展示如何将注意力分数归因于识别出的特征。

Conclusion: 提出的方法能够有效分解QK空间，识别人类可解释的特征子空间，为理解Transformer注意力机制提供了新的分析工具，有助于解释模型为何关注特定token。

Abstract: Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.

</details>


### [111] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 提出基于TransUNet的双阶段多源降水融合框架DDL-MSPMF，整合6种多源降水产品和4种ERA5物理预测因子，提升中国区域降水估计精度，特别改善极端降水检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有卫星反演和再分析的多源降水产品存在空间异质性偏差，对极端降水事件的识别能力有限，限制了其在水文气候监测中的应用价值。

Method: 开发双阶段TransUNet融合框架：第一阶段分类器估计日降水发生概率，第二阶段回归器融合分类器输出和所有预测因子，在0.25度分辨率上估计日降水量。整合6种多源降水产品和4种ERA5近地面物理预测因子。

Result: 相比多种深度学习基准，TransUNet-TransUNet配置表现最佳（R=0.75；RMSE=2.70 mm/day）。对强降水（>25 mm/day）在华东大部分地区提高公平威胁评分，更好再现2021年7月郑州暴雨空间分布。在青藏高原的独立验证也支持其在数据稀缺区域的适用性。

Conclusion: DDL-MSPMF框架为降水融合和极端事件评估提供了可扩展且可解释的方法，通过SHAP分析揭示了降水发生概率和地表压力的重要性，增强了极端降水事件的检测能力。

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


### [112] [Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations](https://arxiv.org/abs/2602.04761)
*Hang Yu,Yu-Hu Yan,Peng Zhao*

Main category: cs.LG

TL;DR: 本文改进了带梯度变化的Bandit凸优化算法，通过精化非连续梯度变化分析，在两点反馈设置下提升了凸和强凸函数的维度依赖，并扩展到单点线性优化、动态/通用遗憾最小化及博弈等任务。


<details>
  <summary>Details</summary>
Motivation: 梯度变化在线学习在博弈论和优化中有重要应用，但在bandit反馈下研究不足。现有两点反馈的Bandit凸优化中梯度变化分析不够精细，维度依赖有待改进。

Method: 提出对非连续梯度变化的精化分析，这是bandit设置下梯度变化的核心量。改进两点反馈Bandit凸优化的分析框架，并扩展到单点线性优化、动态/通用遗憾最小化及bandit博弈等场景。

Result: 1. 在两点反馈Bandit凸优化中，对凸和强凸函数改进了维度依赖，超越了Chiang等人(2013)的最佳结果；2. 首次为单点bandit线性优化在超矩形域上建立了梯度变化界；3. 在动态/通用遗憾最小化和bandit博弈中验证了有效性，建立了首个梯度变化动态和通用遗憾界。

Conclusion: 通过精化非连续梯度变化分析，本文显著推进了bandit反馈下的梯度变化在线学习理论，提供了更优的维度依赖和更广泛的应用场景，为相关领域提供了新的理论工具。

Abstract: Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.

</details>


### [113] [Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty](https://arxiv.org/abs/2602.04763)
*Rui Liu,Pratap Tokekar,Ming Lin*

Main category: cs.LG

TL;DR: A2MAML是一个不确定性感知的多智能体多模态学习框架，通过模态级贝叶斯融合和主动选择可靠模态，提升多智能体协作的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多智能体系统配备异构多模态传感器带来感知丰富性，但也引入了模态特定和智能体依赖的不确定性。现有协作框架通常在智能体级别推理，假设同质感知，并隐式处理不确定性，在传感器损坏时鲁棒性受限

Method: A2MAML将每个模态特定特征建模为带有不确定性预测的随机估计，主动选择可靠的智能体-模态对，并通过贝叶斯逆方差加权聚合信息。支持细粒度模态级融合和不对称模态可用性

Result: 在互联自动驾驶场景的协作事故检测实验中，A2MAML始终优于单智能体和协作基线，实现了高达18.7%的事故检测率提升

Conclusion: A2MAML提供了一个原则性的不确定性感知多智能体多模态协作框架，能够有效抑制损坏或噪声模态，提升系统鲁棒性

Abstract: Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate.

</details>


### [114] [Billion-Scale Graph Foundation Models](https://arxiv.org/abs/2602.04768)
*Maya Bechler-Speicher,Yoel Gottlieb,Andrey Isakov,David Abensur,Ami Tavory,Daniel Haimovich,Ido Guy,Udi Weinsberg*

Main category: cs.LG

TL;DR: GraphBFF是首个用于构建十亿参数图基础模型的端到端框架，包含可扩展的Transformer架构、神经缩放定律和规模化训练方法，在未见图上实现卓越的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言和视觉领域取得了成功，但将其扩展到真实世界的通用图数据面临挑战。需要解决如何为异构、十亿级图构建可扩展的基础模型的问题。

Method: 提出GraphBFF框架，包括：1）GraphBFF Transformer（灵活可扩展的架构）；2）图神经缩放定律；3）数据批处理、预训练和微调的具体方法；4）在10亿样本上预训练14亿参数模型。

Result: 在10个未见图的下游任务（节点/链接分类回归）中，GraphBFF实现了卓越的零样本和探测性能，在少样本设置下PRAUC提升高达31个百分点，展示了首个图神经缩放定律。

Conclusion: GraphBFF为构建工业级图基础模型提供了实用框架，展示了图基础模型的可行性，并讨论了将GFM发展为图学习实用基础的关键挑战和机会。

Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.

</details>


### [115] [NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image](https://arxiv.org/abs/2602.04769)
*Yan Chen,Jie Peng,Moajjem Hossain Chowdhury,Tianlong Chen,Yunmei Liu*

Main category: cs.LG

TL;DR: NeuroCanvas：基于LLM的癫痫检测新框架，通过熵引导通道选择器和神经信号画布模块，解决EEG多通道异质性和计算效率问题，在多个数据集上F1分数提升20%，推理延迟降低88%。


<details>
  <summary>Details</summary>
Motivation: 癫痫检测对临床干预至关重要，但人工审查长期EEG记录劳动密集。现有将EEG编码到LLM的方法面临两大挑战：1）多通道异质性（癫痫相关信息在不同EEG通道中差异显著）；2）计算效率低（EEG信号需要编码为大量token进行预测）。

Method: 提出NeuroCanvas框架，包含两个模块：1）熵引导通道选择器（ECS）：选择与癫痫相关的通道输入LLM；2）神经信号画布（CNS）：将选定的多通道异质EEG信号转换为结构化视觉表示。ECS缓解多通道异质性问题，CNS使用紧凑的视觉token表示EEG信号以提高计算效率。

Result: 在多个癫痫检测数据集上评估，F1分数显著提升20%，推理延迟降低88%。证明NeuroCanvas是临床实践中实时且资源高效的癫痫检测解决方案。

Conclusion: NeuroCanvas通过解决多通道异质性和计算效率问题，为基于LLM的癫痫检测提供了可扩展且有效的解决方案，具有临床实用价值。

Abstract: Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\%$ in F1 score and reductions of $88\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.

</details>


### [116] [Generative Modeling via Drifting](https://arxiv.org/abs/2602.04770)
*Mingyang Deng,He Li,Tianhong Li,Yilun Du,Kaiming He*

Main category: cs.LG

TL;DR: 提出Drifting Models新范式，通过训练时演化pushforward分布实现一步推理，在ImageNet 256×256上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现有生成模型如扩散模型和流模型需要多步推理，作者希望开发能够实现高质量一步生成的模型

Method: 提出Drifting Models范式，引入drifting field控制样本移动，在训练时演化pushforward分布，当分布匹配时达到平衡，神经网络优化器负责演化分布

Result: 在ImageNet 256×256分辨率上取得SOTA结果：潜在空间FID 1.54，像素空间FID 1.61

Conclusion: Drifting Models为高质量一步生成开辟了新机会，实现了训练时分布演化与一步推理的统一

Abstract: Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.

</details>


### [117] [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](https://arxiv.org/abs/2602.04775)
*Yuqi Li,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 提出用于区间值预测的不确定性感知ROC框架，引入AUC_L和AUC_U作为理论最优AUC的上下界，支持选择性预测和不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 在高风险预测中，区间值预测对不确定性量化至关重要，但标准ROC曲线和AUC仅适用于点预测，无法评估预测不确定性对排序性能的影响。

Method: 提出不确定性感知ROC框架，专门针对区间值预测，引入AUC_L和AUC_U两个新指标，将ROC平面分解为正确、错误和不确定三个区域，支持选择性预测。

Result: 证明在有效的类条件覆盖下，AUC_L和AUC_U为理论最优AUC(AUC*)提供形式化的上下界，刻画可实现判别能力的物理极限。在真实基准数据集上的实验验证了框架的正确性和实用性。

Conclusion: 该框架为区间值预测模型提供了不确定性感知的评估工具，支持选择性预测决策，优化弃权率与判别可靠性之间的权衡，适用于各种区间构建方法。

Abstract: In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.

</details>


### [118] [Dynamical Regimes of Multimodal Diffusion Models](https://arxiv.org/abs/2602.04780)
*Emil Albrychiewicz,Andrés Franco Valiente,Li-Ching Chen*

Main category: cs.LG

TL;DR: 本文提出了一个耦合扩散模型的理论框架，使用耦合Ornstein-Uhlenbeck过程作为可处理模型，揭示了多模态生成由相互作用时间尺度的谱层次结构而非同时分辨率控制，并预测了"同步间隙"现象。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在高维数据合成方面取得了前所未有的保真度，但多模态生成的理论机制仍然知之甚少。本文旨在建立理解多模态生成的理论框架。

Method: 使用耦合Ornstein-Uhlenbeck过程作为可处理模型，应用非平衡统计物理中的动力学相变理论，推导对称和各向异性耦合机制下的解析条件，并通过MNIST数据集上的扩散模型和精确分数采样器进行实验验证。

Result: 证明了多模态生成由相互作用时间尺度的谱层次结构控制，预测了"同步间隙"现象（反向生成过程中不同特征模以不同速率稳定的时间窗口），推导了物种形成和崩溃时间的解析条件，建立了避免不稳定对称破缺的耦合强度严格界限。

Conclusion: 耦合强度作为谱滤波器对生成施加可调的时间层次结构，这些结果启发了针对模态特定时间尺度的时间依赖耦合调度方法，为替代启发式指导调优提供了潜在方案。

Abstract: Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.

</details>


### [119] [Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data](https://arxiv.org/abs/2602.04782)
*Mumin Zhang,Haochen Zhang,Xin Zhi Khoo,Yilin Zhang,Nuo Chen,Ting Zhang,Junjie Tang*

Main category: cs.LG

TL;DR: 提出WMF-CPK-MSLMU集成模型，用于风电场集群的短期风速预测，结合加权均值滤波降噪、基于Kendall秩相关系数的补偿参数和多切片LMU，实现准确、快速、鲁棒的预测。


<details>
  <summary>Details</summary>
Motivation: 随着风电场集群化并网，短期风速预测对电力系统正常运行至关重要。需要充分利用集群数据的时空相关性，实现准确、快速、鲁棒的预测。

Method: 1) 使用加权均值滤波(WMF)对单场风速数据降噪；2) 创新应用Legendre记忆单元(LMU)进行风速预测；3) 结合基于Kendall秩相关系数的补偿参数(CPK)构建多切片LMU(MSLMU)；4) 提出WMF-CPK-MSLMU集成模型，包含数据预处理、预测和多切片补偿三个关键模块。

Result: 在不同风电场集群上的测试结果表明，提出的WMF-CPK-MSLMU集成模型在短期预测方面相比现有模型具有有效性和优越性。

Conclusion: WMF-CPK-MSLMU模型通过充分利用风电场集群的时空相关性，实现了准确、快速、鲁棒的短期风速预测，为电力系统正常运行提供了有效支持。

Abstract: With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.

</details>


### [120] [From independent patches to coordinated attention: Controlling information flow in vision transformers](https://arxiv.org/abs/2602.04784)
*Kieran A. Murphy*

Main category: cs.LG

TL;DR: 在视觉Transformer中引入变分信息瓶颈来显式控制注意力传输的信息量，实现从独立补丁处理到全局注意力的可控谱系，提高模型可分析性和可控性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer中的注意力机制信息传输不透明，难以分析和控制。研究者希望使注意力传输的信息成为可测量的显式量，从而获得更易进行机制分析和控制的模型。

Method: 在所有注意力对残差流的写入操作中插入变分信息瓶颈，不改变其他架构。通过信息成本训练模型，获得从独立补丁处理到全局注意力的可控谱系。

Result: 在ImageNet-100上表征了分类行为和信息路由如何沿该谱系演化，通过分析首批传输信息的注意力头，初步洞察全局视觉表征如何从局部补丁处理中涌现。

Conclusion: 通过约束内部通信的学习偏置，该方法产生了更易于机制分析和控制的模型，为理解视觉Transformer内部工作机制提供了新工具。

Abstract: We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.

</details>


### [121] [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785)
*Congjing Zhang,Ryan Feng Lin,Ruoxuan Bao,Shuai Huang*

Main category: cs.LG

TL;DR: T²框架利用LLM团队协作生成高质量表格数据，并通过三层质量控制管道优化数据质量，解决表格数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现实机器学习应用中高质量表格数据获取成本高、难度大，现有数据集常存在类别不平衡、选择偏差和低保真度等问题，需要高效的数据生成方法。

Method: 提出Team-then-Trim (T²)框架：1) 组织专业LLM团队，基于领域知识按顺序生成不同数据组件；2) 实施三层插件式质量控制管道，系统评估合成数据的多个质量维度。

Result: 在模拟和真实数据集上的实验表明，T²在生成高质量表格数据方面优于现有最先进方法，能够有效支持下游模型训练。

Conclusion: T²框架为解决表格数据稀缺问题提供了有效方案，当直接数据收集不可行时，能够通过LLM协作生成高质量合成数据支持机器学习应用。

Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.

</details>


### [122] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: 提出最大体积非负矩阵分解（MaxVol NMF），通过最大化H的体积来获得更稀疏、可解释的分解，相比最小体积NMF在噪声下表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统最小体积NMF（MinVol NMF）虽然能获得可解释和唯一的解，但在噪声环境下可能产生秩亏缺解。作者提出研究其对偶方法——最大体积NMF，以解决这些问题并获得更稀疏的分解。

Method: 提出最大体积NMF（MaxVol NMF），通过最大化因子H的体积而非最小化W的体积。开发了两种求解算法，并提出归一化变体，该变体可解释为标准NMF和正交NMF之间的连续体。

Result: MaxVol NMF在无噪声情况下与MinVol NMF具有相同的可识别性，但在噪声环境下表现更好：能提取更稀疏的分解，不产生秩亏缺解，且最大体积解对应于将X的列聚类到不相交的簇中。

Conclusion: MaxVol NMF是MinVol NMF的有效对偶方法，在噪声环境下表现更优，能获得更稀疏、更稳定的分解，归一化变体性能最佳，在高光谱解混等应用中具有实用价值。

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


### [123] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: 提出Afferent Learning框架，通过进化优化和强化学习的两层架构，生成自适应内部风险信号(CATs)用于损伤避免学习，在生物力学数字孪生中实现高效学习和年龄鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受生物系统启发，需要为损伤避免学习开发自适应内部风险信号，传统方法难以在长期时间尺度上有效学习损伤避免策略，特别是在生物力学数字孪生等复杂场景中。

Method: 采用两层架构：外层进化优化发现能够支持有效策略学习的传入感知架构，内层强化学习使用这些信号训练损伤避免策略。将传入感知形式化为提供高效学习的归纳偏置。

Result: 在生物力学数字孪生（数十年生命历程）中，CATs进化架构比手工设计基线显著提高效率（23%高风险动作减少）和年龄鲁棒性，实现年龄依赖性行为适应。消融研究验证了CAT信号、进化和预测差异的重要性。

Conclusion: Afferent Learning框架通过进化优化的传入感知架构为损伤避免学习提供有效内部风险信号，在长期时间尺度复杂系统中实现高效学习和适应性行为，具有理论保证和实际应用价值。

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>


### [124] [Beyond Rewards in Reinforcement Learning for Cyber Defence](https://arxiv.org/abs/2602.04809)
*Elizabeth Bates,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 稀疏奖励函数在网络安全强化学习训练中表现优于密集奖励函数，能产生更可靠、更有效、风险更低的防御策略，且无需显式惩罚成本。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全防御智能体通常使用密集、高度设计的奖励函数进行训练，这些函数结合了多种惩罚和激励。密集奖励有助于缓解复杂环境探索的挑战，但可能导致智能体偏向次优且风险更高的解决方案，这在复杂网络环境中是严重问题。

Method: 使用多种稀疏和密集奖励函数，在两个成熟的网络训练环境中，针对不同网络规模，结合策略梯度和基于价值的强化学习算法，全面评估奖励函数结构对学习和策略行为特征的影响。采用新颖的基准评估方法，允许直接比较不同奖励函数。

Result: 稀疏奖励（只要目标对齐且能频繁遇到）能提供更好的训练可靠性和更有效的网络安全防御智能体，产生风险更低的策略。令人惊讶的是，稀疏奖励还能产生更符合网络安全防御目标、更少使用成本高昂防御动作的策略，而无需显式的基于奖励的数值惩罚。

Conclusion: 稀疏奖励函数在网络安全强化学习训练中具有独特优势，能产生更可靠、更有效、风险更低的防御策略，且无需显式惩罚成本。这挑战了当前普遍使用密集奖励函数的做法，为网络安全防御智能体的训练提供了新方向。

Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.

</details>


### [125] [Robust Generalizable Heterogeneous Legal Link Prediction](https://arxiv.org/abs/2602.04812)
*Lorenz Wendlinger,Simon Alexander Nonn,Abdullah Al Zubaer,Michael Granitzer*

Main category: cs.LG

TL;DR: 该论文通过引入边丢弃和特征拼接技术改进法律引用网络的链接预测，错误率降低达45%，并提出基于多语言节点特征和非对称解码器的方法，增强了对新西兰等地理和语言隔离数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律引用网络链接预测方法在处理具有丰富元特征的异构网络时仍有改进空间，特别是在处理地理和语言隔离的不同法律系统数据时，需要更好的泛化能力和归纳迁移性。

Method: 1. 引入边丢弃和特征拼接技术学习更鲁棒的表征；2. 提出基于多语言节点特征的方法，配合改进的非对称解码器以确保兼容性；3. 扩展应用到地理和语言隔离的新西兰法律数据。

Result: 1. 链接预测错误率降低达45%；2. 能够有效泛化到地理和语言隔离的新西兰法律数据；3. 提高了不同法律系统间的归纳迁移能力。

Conclusion: 通过边丢弃、特征拼接和多语言节点特征结合非对称解码器的方法，显著提升了法律引用网络链接预测的性能和泛化能力，为处理地理和语言隔离的法律系统数据提供了有效解决方案。

Abstract: Recent work has applied link prediction to large heterogeneous legal citation networks \new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decoder for compatibility, which allows us to generalize and extend the prediction to more, geographically and linguistically disjoint, data from New Zealand. Our adaptations also improve inductive transferability between these disjoint legal systems.

</details>


### [126] [Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning](https://arxiv.org/abs/2602.04821)
*Joydeep Chandra,Satyam Kumar Navneet,Aleksandr Algazinov,Yong Zhang*

Main category: cs.LG

TL;DR: STREAM-RL是一个统一的交通管理框架，通过不确定性引导的预测、异常检测和安全强化学习，提供端到端理论保证，在真实交通数据上实现了高效覆盖、可控误报率和显著安全提升。


<details>
  <summary>Details</summary>
Motivation: 城市交通管理需要能够同时预测未来状况、检测异常并采取安全纠正措施的系统，同时需要提供可靠性保证。现有方法缺乏从预测到异常检测再到安全策略学习的端到端不确定性传播和理论保证。

Method: STREAM-RL包含三个核心算法：1) PU-GAT+：不确定性引导的自适应共形预测器，使用预测不确定性通过置信度单调注意力动态重新加权图注意力；2) CRFN-BY：共形残差流网络，通过具有Benjamini-Yekutieli FDR控制的归一化流建模不确定性归一化残差；3) LyCon-WRL+：具有Lyapunov稳定性证书、认证Lipschitz边界和不确定性传播想象展开的不确定性引导安全世界模型RL代理。

Result: 在多个真实世界交通轨迹数据上的实验表明：STREAM-RL实现了91.4%的覆盖效率，在验证的依赖性下将FDR控制在4.1%，安全率提升至95.2%（标准PPO为69%），同时获得更高奖励，端到端推理延迟为23ms。

Conclusion: STREAM-RL是首个从预测到异常检测再到安全策略学习传播校准不确定性的框架，具有端到端理论保证，在真实交通场景中实现了高效覆盖、可控误报率和显著安全改进。

Abstract: Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.

</details>


### [127] [It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task](https://arxiv.org/abs/2602.04832)
*Hannah Pinson*

Main category: cs.LG

TL;DR: 该论文研究了梯度下降如何降低神经网络的理论容量，通过分析单隐藏层ReLU网络中单个神经元的学习动态，发现了三个动力学原理来解释容量缩减机制。


<details>
  <summary>Details</summary>
Motivation: 神经网络的理论理解落后于其经验成功。一个重要未解现象是：为什么以及如何在梯度下降训练过程中，神经网络的理论容量会缩减为适合任务的有效容量。作者旨在揭示梯度下降实现这一过程的机制。

Method: 通过分析单隐藏层ReLU网络中单个神经元的学习动态，识别出三个动力学原理：互相对齐、解锁和竞争。这些原理解释了为什么训练后可以通过合并等效神经元或修剪低范数权重来成功缩减容量。

Result: 发现了三个关键动力学原理：1) 互相对齐 - 神经元在训练过程中趋向对齐；2) 解锁 - 某些神经元从初始状态解锁；3) 竞争 - 神经元之间竞争权重增长。这些原理共同解释了彩票票假设的机制，即为什么某些具有有益初始条件的神经元会获得更高的权重范数。

Conclusion: 梯度下降通过互相对齐、解锁和竞争这三个动力学原理，在训练过程中将神经网络的理论容量缩减为有效容量。这解释了为什么训练后可以通过合并等效神经元或修剪低范数权重来减少容量，并阐明了彩票票假设背后的机制。

Abstract: Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.

</details>


### [128] [The Key to State Reduction in Linear Attention: A Rank-based Perspective](https://arxiv.org/abs/2602.04852)
*Philipp Nazari,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 线性注意力模型在实践中常呈现低秩状态，这限制了其表达能力。本文通过理论分析揭示了低秩状态会放大查询噪声影响检索误差，并提出一种基于硬件感知的结构化剪枝框架，通过修剪查询和键矩阵来减小状态规模，同时保持与现有CUDA内核的兼容性。


<details>
  <summary>Details</summary>
Motivation: 线性注意力作为softmax注意力的高效替代方案，在实践中常呈现低秩状态，这表明模型未能充分利用其表达能力。这种现象限制了线性注意力的实际性能，需要理论分析和解决方案来优化其状态表示。

Method: 1. 对线性注意力中秩的作用进行理论分析，揭示低有效秩如何影响检索误差；2. 提出硬件感知的结构化剪枝框架，修剪查询和键矩阵以减小状态规模；3. 基于秩揭示QR分解提出新的结构化剪枝方法；4. 将现有剪枝策略适配到该框架中。

Result: 实验结果表明，该框架在不同规模模型和各种下游任务中均有效。能够移除50%的查询和键通道，仅导致困惑度边际增加，实现了更快、更内存高效的模型。

Conclusion: 线性注意力的低秩状态问题可以通过理论分析和结构化剪枝有效解决。提出的硬件感知剪枝框架能够在保持性能的同时显著减小状态规模，为线性注意力模型的实用化提供了有效方案。

Abstract: Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.

</details>


### [129] [From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures](https://arxiv.org/abs/2602.04861)
*Ryan Liu,Eric Qu,Tobias Kreiman,Samuel M. Blau,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 提出BSCT基准测试，通过可控键变形探测势能面平滑性，比传统分子动力学评估更高效，能指导MLIP模型设计改进。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习原子间势能（MLIPs）有时无法再现量子势能面的物理平滑性，导致下游模拟出现错误。传统评估方法如微正则分子动力学计算成本高且主要探测近平衡态，需要更高效的评估指标。

Method: 引入键平滑性表征测试（BSCT），通过可控键变形探测势能面，检测不连续性、人工极小值和虚假力等非平滑现象。使用无约束Transformer架构作为测试平台，结合新的可微分k近邻算法和温度控制注意力机制来减少BSCT识别的伪影。

Result: BSCT与分子动力学稳定性强相关，但计算成本远低于分子动力学。通过BSCT指导模型设计，得到的MLIP同时实现了低传统能量/力回归误差、稳定的分子动力学模拟和稳健的原子性质预测。

Conclusion: BSCT既可作为验证指标，也可作为"循环内"模型设计代理，能高效识别当前MLIP基准无法评估的物理挑战，为MLIP开发提供重要指导。

Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.

</details>


### [130] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: 论文提出Logit-Linear-Selection (LLS)方法，通过选择偏好数据集的子集来激发LLM中的隐藏效应，揭示了数据集如何传递不可直接观察的信号。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型训练使用多种算法和数据集来激发特定行为，但数据集可能传递无法从单个数据点直接观察的信号，这挑战了以数据集为中心的理解方式，需要揭示此类现象的基本机制。

Method: 受LLM线性结构研究的启发，提出Logit-Linear-Selection (LLS)方法，该方法规定了如何从通用偏好数据集中选择子集来激发广泛的隐藏效应。

Result: 应用LLS发现真实数据集子集，使训练模型表现出特定偏好、用数据集中不存在的语言回应提示、采用不同人格等行为。该效应在选择子集中持续存在，且在不同架构模型中通用。

Conclusion: LLS揭示了数据集传递隐藏信号的通用机制，为理解数据集对LLM属性的影响提供了新视角，证明了隐藏子文本在通用数据集中产生的普遍性。

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [131] [CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation](https://arxiv.org/abs/2602.04868)
*Yannick Denker,Alexander Gepperth*

Main category: cs.LG

TL;DR: 提出了一个基于Gazebo模拟器的持续强化学习机器人基准套件CRoSS，包含轮式机器人和机械臂两种平台，支持多种传感器和快速运动学仿真。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需要在不忘记先前策略的情况下学习一系列任务，但缺乏具有物理真实性的机器人基准测试。现有基准要么过于简单，要么缺乏真实的传感器模拟。

Method: 开发了CRoSS基准套件，包含：1）两轮差速机器人用于循线和推物任务；2）七关节机械臂用于笛卡尔位置控制和关节角度控制任务。提供运动学仿真变体，速度提升两个数量级。

Result: 创建了可扩展的基准套件，支持高物理真实性和任意传感器模拟。提供了容器化部署方案，测试了DQN和策略梯度等标准RL算法，验证了其作为可扩展、可复现基准的适用性。

Conclusion: CRoSS为持续强化学习研究提供了具有高物理真实性的机器人基准，支持快速仿真和可扩展设计，有助于推动CRL在机器人领域的实际应用研究。

Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.

</details>


### [132] [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870)
*Chenwei Cui,Rockwell Jackson,Benjamin Joseph Herrera,Ana María Tárano,Hannah Kerner*

Main category: cs.LG

TL;DR: 提出Multi-Head LatentMoE和Head Parallel方法，解决稀疏专家混合模型训练中的通信成本、负载均衡和通信确定性三大问题，实现O(1)通信成本，训练速度提升1.61倍


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合模型通过条件计算降低训练成本，但标准专家并行方法存在三个主要问题：通信成本随激活专家数线性增长、负载不均衡影响延迟和内存使用、数据依赖通信需要元数据交换

Method: 提出Multi-Head LatentMoE架构和Head Parallel并行方法，实现O(1)通信成本、完全均衡的流量和确定性通信，同时保持与专家并行的兼容性；还提出IO感知路由和专家计算来加速

Result: 相比专家并行方法，Multi-Head LatentMoE与Head Parallel结合训练速度提升1.61倍，性能相同；在加倍粒度下仍能提升1.11倍速度并实现更高整体性能

Conclusion: 该方法解决了稀疏专家混合模型训练的关键瓶颈，使数十亿参数的基础模型研究更加可及，为大规模语言模型训练提供了更高效的解决方案

Abstract: Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.

</details>


### [133] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: 论文提出DPPO算法，用基于策略散度的直接估计替代PPO中的启发式裁剪机制，解决了PPO在大词汇量LLM微调中的结构性问题，提高了训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: PPO作为RL微调LLM的事实标准算法，其核心的比例裁剪机制在大词汇量场景下存在结构性问题：对低概率token的更新过度惩罚，而对高概率token的潜在灾难性偏移约束不足，导致训练效率低下和不稳定。

Method: 提出Divergence Proximal Policy Optimization (DPPO)，用基于策略散度（如总变差或KL散度）的直接估计约束替代启发式裁剪。为避免巨大内存开销，引入高效的Binary和Top-K近似方法来以可忽略的开销捕获关键散度信息。

Result: 广泛的实证评估表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于RL的LLM微调提供了更稳健的基础。

Conclusion: DPPO通过更原则性的策略散度约束解决了PPO在大词汇量LLM微调中的结构性问题，为RL-based LLM fine-tuning提供了更稳定高效的算法基础。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [134] [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881)
*Ajesh Koyatan Chathoth*

Main category: cs.LG

TL;DR: 本文综述了对比持续学习在物联网中的应用，连接算法设计与系统现实，提出统一问题框架、参考架构和评估指南。


<details>
  <summary>Details</summary>
Motivation: 物联网部署在非平稳动态环境中运行，传感器漂移、用户行为演变和隐私需求变化会影响应用效用。持续学习通过适应模型而不发生灾难性遗忘来解决这一问题，而对比学习作为强大的表示学习范式能提高鲁棒性和样本效率。

Method: 提出对比持续学习的统一问题表述，推导结合对比损失和蒸馏损失的共同目标，设计面向物联网的参考架构（设备端、边缘、云端），提供评估协议和指标指南。

Result: 建立了连接算法设计（重放、正则化、蒸馏、提示）与物联网系统现实（TinyML约束、间歇连接、隐私）的框架，为物联网对比持续学习提供系统化指导。

Conclusion: 本文为物联网对比持续学习提供了全面综述和框架，并指出了该领域面临的独特挑战，包括处理表格和流式数据、概念漂移、联邦设置和能量感知训练等开放问题。

Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.

</details>


### [135] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu,Cheng-Yen Hsieh,Zaixiang Zheng,Ge Liu,Quanquan Gu*

Main category: cs.LG

TL;DR: PAR是首个多尺度自回归蛋白质骨架生成框架，通过粗到细的尺度预测实现分层结构生成，解决了自回归模型的曝光偏差问题，并展示了强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 利用蛋白质的层次化特性，开发能够模仿雕塑过程（从粗拓扑到精细结构细节）的蛋白质结构生成方法，同时解决自回归模型中训练与生成过程不匹配导致的曝光偏差问题。

Method: 包含三个核心组件：1) 多尺度下采样操作，在训练中表示多尺度蛋白质结构；2) 自回归Transformer编码多尺度信息并产生条件嵌入；3) 基于流的骨架解码器根据嵌入生成骨架原子。采用噪声上下文学习和计划采样缓解曝光偏差。

Result: PAR有效学习蛋白质分布，生成高质量骨架结构，表现出良好的缩放行为。在无条件生成基准测试中表现优异，支持灵活的人为提示条件生成和基序支架，无需微调。

Conclusion: PAR作为蛋白质结构生成的有前景框架，通过多尺度自回归方法实现了高质量骨架生成，解决了曝光偏差问题，并展示了强大的零样本泛化能力。

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [136] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: 研究共享对象的可交换感知线性化实现的进展条件，提出冲突阻塞自由概念，证明在异步读写共享内存模型中不可能实现冲突阻塞自由的通用构造


<details>
  <summary>Details</summary>
Motivation: 观察到可交换操作可以并行执行，希望通过允许仅在遇到非可交换操作冲突时才需要同步来改进进展条件

Method: 引入冲突阻塞自由概念：进程只要长时间运行而不遇到与非可交换操作的步骤冲突，就能完成其操作。这推广了阻塞自由和等待自由条件

Result: 证明在异步读写共享内存模型中不可能实现冲突阻塞自由的通用构造，揭示了冲突感知通用构造的基本限制

Conclusion: 冲突感知通用构造存在根本限制：非可交换操作的调用本身就带来同步成本，进展需要最终解决待处理的冲突

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [137] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: 本文实证量化了将5G LDPC解码从Grace CPU卸载到Blackwell GB10 GPU的性能优势，在DGX Spark平台上实现了约6倍的吞吐量加速，GPU解码延迟保持在0.5ms时隙内，而CPU解码会超时。


<details>
  <summary>Details</summary>
Motivation: 5G NR物理层中的LDPC解码是计算密集型核心任务，必须在0.5ms传输时间间隔内完成。许多现有系统仍在通用CPU上执行LDPC解码，随着带宽、调制阶数和用户复用的增加，存在时隙错过和可扩展性受限的问题。

Method: 使用NVIDIA Sionna PHY/SYS在TensorFlow上构建NR-like链路级链，包含LDPC5G编码器/解码器、16-QAM调制和AWGN信道。通过扫描并行解码的码字数量和置信传播迭代次数，测量解码阶段的性能，同时记录CPU和GPU的利用率和功耗。

Result: GPU相比CPU实现了约6倍的平均吞吐量加速。CPU解码延迟在20次迭代时达到约0.71ms（超过0.5ms时隙），而GB10 GPU在相同工作负载下保持在时隙的6-24%范围内。CPU解码通常消耗约10个Grace核心，而GPU解码仅比空闲状态增加10-15W功耗。

Conclusion: GPU卸载能显著提升LDPC解码性能并满足时隙要求，同时释放CPU资源用于更高层任务。由于使用高层Sionna而非手动优化的CUDA，这些结果是可实现的加速器性能的保守下限，为未来平台上的物理层内核评估提供了可重用、可脚本化的方法。

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [138] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: CONFINE：基于可信执行环境（TEE）的保密性保护跨组织流程挖掘方法，通过四阶段协议实现安全数据交换和处理，解决跨组织流程挖掘中的数据保密问题。


<details>
  <summary>Details</summary>
Motivation: 现实业务流程常涉及多个独立组织，但跨组织流程挖掘面临严重的数据保密挑战：数据分析可能泄露参与组织不愿向彼此或第三方披露的信息，这阻碍了跨组织流程挖掘的实际应用。

Method: 提出CONFINE方法，利用可信执行环境（TEE）部署可信应用程序，通过四阶段协议保护数据交换和处理，采用基于分段的策略将事件日志分批传输到TEE以避免内存限制问题。

Result: 对真实世界和合成数据的评估表明，该方法能处理实际工作负载，内存使用随事件日志大小呈对数增长，随供应组织数量呈线性增长，具有良好的可扩展性。

Conclusion: CONFINE通过TEE技术有效解决了跨组织流程挖掘中的数据保密问题，提供了可行的解决方案，并展示了进一步优化的潜力。

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [139] [SPPAM: Signature Pattern Prediction and Access-Map Prefetcher](https://arxiv.org/abs/2602.04100)
*Maccoy Merrell,Lei Wang,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: SPPAM是一种新的缓存预取方法，结合了SPP和AMPM的优点，通过在线学习构建访问模式图，使用置信度节流的推测性前瞻来提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 处理器速度与内存系统性能之间的差距持续限制许多工作负载的性能。现有预取技术如SPP易受缓存重排序影响，而AMPM无法进行区域外推测，需要一种能克服这些限制的新方法。

Method: SPPAM采用在线学习构建一组访问模式图，这些模式用于置信度节流的推测性前瞻。针对二级缓存设计，结合了SPP的推测能力和AMPM的重排序抵抗特性。

Result: SPPAM与最先进的预取器Berti和Bingo结合，相比无预取系统性能提升31.4%，相比Berti和Pythia基线提升6.2%。

Conclusion: SPPAM通过结合在线学习访问模式和置信度节流推测，有效解决了现有预取技术的局限性，显著提升了系统性能。

Abstract: The discrepancy between processor speed and memory system performance continues to limit the performance of many workloads. To address the issue, one effective and well studied technique is cache prefetching. Many prefetching designs have been proposed, with varying approaches and effectiveness. For example, SPP is a popular prefetcher that leverages confidence throttled recursion to speculate on the future path of program's references, however it is very susceptible to the reference reordering of higher-level caches and the OoO core. Orthogonally, AMPM is another popular approach to prefetching which uses reordering-resistant access maps to identify patterns within a region, but is unable to speculate beyond that region. In this paper, we propose SPPAM, a new approach to prefetching, inspired by prior works such as SPP and AMPM, while addressing their limitations. SPPAM utilizes online-learning to build a set of access-map patterns. These patterns are used in a speculative lookahead which is throttled by a confidence metric. Targeting the second-level cache, SPPAM alongside state-of-the-art prefetchers Berti and Bingo improve system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

</details>


### [140] [Crypto-RV: High-Efficiency FPGA-Based RISC-V Cryptographic Co-Processor for IoT Security](https://arxiv.org/abs/2602.04415)
*Anh Kiet Pham,Van Truong Vo,Vu Trung Duong Le,Tuan Hai Vu,Hoai Luan Pham,Van Tinh Nguyen,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: Crypto-RV是一个RISC-V协处理器架构，统一支持多种加密算法和抗量子密码，在单个64位数据路径中实现高性能、高能效的加密处理。


<details>
  <summary>Details</summary>
Motivation: 当前RISC-V平台缺乏对全面加密算法家族和抗量子密码的高效硬件支持，而加密操作对物联网、边缘计算和自主系统的安全至关重要。

Method: 提出Crypto-RV协处理器架构，包含三个关键创新：高带宽内部缓冲区(128x64位)、专门优化的加密执行单元(四阶段流水线数据路径)、以及针对大哈希优化的双缓冲机制和自适应调度。

Result: 在Xilinx ZCU102 FPGA上实现，运行频率160MHz，动态功耗0.851W，相比基线RISC-V核心获得165-1061倍加速，相比强大CPU实现5.8-17.4倍能效提升，仅占用34,704 LUTs、37,329 FFs和22 BRAMs。

Conclusion: Crypto-RV展示了在资源受限的物联网环境中实现高性能、高能效加密处理的可行性，为RISC-V平台提供了全面的加密硬件支持。

Abstract: Cryptographic operations are critical for securing IoT, edge computing, and autonomous systems. However, current RISC-V platforms lack efficient hardware support for comprehensive cryptographic algorithm families and post-quantum cryptography. This paper presents Crypto-RV, a RISC-V co-processor architecture that unifies support for SHA-256, SHA-512, SM3, SHA3-256, SHAKE-128, SHAKE-256 AES-128, HARAKA-256, and HARAKA-512 within a single 64-bit datapath. Crypto-RV introduces three key architectural innovations: a high-bandwidth internal buffer (128x64-bit), cryptography-specialized execution units with four-stage pipelined datapaths, and a double-buffering mechanism with adaptive scheduling optimized for large-hash. Implemented on Xilinx ZCU102 FPGA at 160 MHz with 0.851 W dynamic power, Crypto-RV achieves 165 times to 1,061 times speedup over baseline RISC-V cores, 5.8 times to 17.4 times better energy efficiency compared to powerful CPUs. The design occupies only 34,704 LUTs, 37,329 FFs, and 22 BRAMs demonstrating viability for high-performance, energy-efficient cryptographic processing in resource-constrained IoT environments.

</details>


### [141] [Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference](https://arxiv.org/abs/2602.04595)
*Xinyu Wang,Jieyu Li,Yanan Sun,Weifeng He*

Main category: cs.AR

TL;DR: Harmonia是一个算法-硬件协同设计框架，通过全层BFP激活、非对称位分配策略和专用硬件组件，在保持精度的同时显著提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大但内存和计算成本高。现有量化方法在线性层使用BFP激活，但无法扩展到注意力层，导致精度严重下降，限制了整体效率提升。

Method: 1) 系统探索BFP配置以实现全层精度与激活压缩的平衡；2) 引入非对称位分配策略和混合离线-在线异常值平滑技术，将KV缓存从FP16压缩到4位尾数BFP；3) 设计专用硬件组件，包括支持混合数据格式的可重构PE、实时FP16到BFP转换器和分块感知数据流。

Result: 在8个广泛使用的LLM上评估，相比先前工作，Harmonia平均实现3.84倍（最高5.05倍）的面积效率、2.03倍（最高3.90倍）的能效和3.08倍（最高4.62倍）的加速。

Conclusion: Harmonia成功解决了注意力层BFP激活的精度问题，通过算法-硬件协同设计实现了全层BFP激活，显著提升了LLM推理的效率和性能。

Abstract: Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [142] [A Comparative Study of Digital Memristor-Based Processing-In-Memory from a Device and Reliability Perspective](https://arxiv.org/abs/2602.04035)
*Thomas Neuner,Henriette Padberg,Lior Kornblum,Eilam Yalon,Pedram Khalili Amiri,Shahar Kvatinsky*

Main category: cs.ET

TL;DR: 这篇综述回顾了存内计算中基于新兴非易失性存储器（如RRAM、PCM、MRAM）的状态和非状态逻辑技术进展，分析了实验和仿真设计，重点关注可靠性挑战和设备优化对可扩展商业系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 数据密集型应用对传统计算系统造成压力，存内计算通过减少内存与处理单元间的数据传输来缓解"内存墙"问题，成为有前景的范式。需要全面评估新兴非易失性存储器技术在存内计算中的逻辑实现方法。

Method: 首先概述相关逻辑家族、忆阻器件类型和可靠性指标，然后分析每个逻辑家族如何利用不同器件特性实现逻辑技术，通过代表性器件堆栈和性能参数的对比表格说明权衡和质量指标。

Result: 全面分析了状态和非状态逻辑技术在存内计算中的应用，识别了可靠性方面的关键挑战，强调了器件级优化对实现可扩展和商业可行存内计算系统的重要性。

Conclusion: 通过综合分析支持开发优化的、稳健的忆阻器件，为下一代存内计算应用提供基础，器件级优化是实现可扩展商业存内计算系统的关键。

Abstract: As data-intensive applications increasingly strain conventional computing systems, processing-in-memory (PIM) has emerged as a promising paradigm to alleviate the memory wall by minimizing data transfer between memory and processing units. This review presents the recent advances in both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and magnetoresistive random-access memory (MRAM). Both experimentally demonstrated and simulated logic designs are critically examined, highlighting key challenges in reliability and the role of device-level optimization in enabling scalable and commercial viable PIM systems. The review begins with an overview of relevant logic families, memristive device types, and associated reliability metrics. Each logic family is then explored in terms of how it capitalizes on distinct device properties to implement logic techniques. A comparative table of representative device stacks and performance parameters illustrates trade-offs and quality indicators. Through this comprehensive analysis, the development of optimized, robust memristive devices for next-generation PIM applications is supported.

</details>


### [143] [The Dynamics of Attention across Automated and Manual Driving Modes: A Driving Simulation Study](https://arxiv.org/abs/2602.04164)
*Yuan Cai,Mustafa Demir,Farzan Sasangohar,Mohsen Zare*

Main category: cs.ET

TL;DR: 研究自动驾驶中驾驶员注意力在不同驾驶模式（自动、手动、过渡）下的动态分配，发现注意力模式随驾驶模式变化，为自适应人机界面设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆引入交通系统带来安全担忧，特别是模式转换时驾驶员重新参与的风险。过往事故显示过度依赖自动化的危险，需要理解动态注意力分配以支持自动驾驶安全。

Method: 使用高保真驾驶模拟和眼动追踪技术，测量不同驾驶模式（自动、手动、过渡）下的注视持续时间、注视次数和首次注视时间，评估驾驶员对不同兴趣区域的注意力分配。

Result: 驾驶员注意力在不同驾驶模式下显著变化：手动模式下持续关注道路；自动模式下长时间注视嵌入式HMI；交接和接管阶段注意力在环境和技术元素间动态切换。

Conclusion: 驾驶员注意力分配是模式依赖的，这些发现为设计符合驾驶员注意力模式的自适应HMI提供信息。通过根据驾驶情境呈现相关信息，可以增强驾驶员-车辆交互，支持有效过渡并提高整体安全。

Abstract: This study aims to explore the dynamics of driver attention to various zones, including the road, the central mirror, the embedded Human-Machine Interface (HMI), and the speedometer, across different driving modes in AVs. The integration of autonomous vehicles (AVs) into transportation systems has introduced critical safety concerns, particularly regarding driver re-engagement during mode transitions. Past accidents underscore the risks of overreliance on automation and highlight the need to understand dynamic attention allocation to support safety in autonomous driving. A high-fidelity driving simulation was conducted. Eye-tracking technology was used to measure fixation duration, fixation count, and time to first fixation across distinct driving modes (automated, manual, and transition), which were then used to assess how drivers allocated attention to various areas of interest (AOIs). Findings show that drivers' attention varies significantly across driving modes. In manual mode, attention consistently focuses on the road, while in automated mode, prolonged fixation on the embedded HMI was observed. During the handover and takeover phases, attention shifts dynamically between environmental and technological elements. The study reveals that driver attention allocation is mode-dependent. These findings inform the design of adaptive HMIs in AVs that align with drivers' attention patterns. By presenting relevant information according to the driving context, such systems can enhance driver-vehicle interaction, support effective transitions, and improve overall safety. Systematic analysis of visual attention dynamics across driving modes is gaining prominence, as it informs adaptive HMI designs and driver readiness interventions. The GLMM findings can be directly applied to the design of adaptive HMIs or driver training programs to enhance attention and improve safety.

</details>


### [144] [Self-evolving Embodied AI](https://arxiv.org/abs/2602.04411)
*Tongtong Feng,Xin Wang,Wenwu Zhu*

Main category: cs.ET

TL;DR: 论文提出"自进化具身AI"新范式，使智能体能在动态开放环境中自主进化，具备记忆自更新、任务自切换、环境自预测、具身自适应和模型自进化能力。


<details>
  <summary>Details</summary>
Motivation: 现有具身AI局限于人工设定的静态环境，无法适应现实世界中变化的具身形态和动态开放环境。需要一种能够自主适应和进化的新范式。

Method: 提出自进化具身AI框架，包括定义、框架结构、组件和机制。系统回顾了已实现组件的先进工作，并讨论了实际应用。

Result: 建立了一个新的研究范式，为具身AI在动态开放环境中的自主适应和进化提供了系统框架和理论指导。

Conclusion: 自进化具身AI使智能体能够以类人方式自主学习和与环境交互，为实现通用人工智能提供了新视角。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence.

</details>


### [145] [Quantum-Based Resilient Routing in Networks: Minimizing Latency Under Dual-Link Failures](https://arxiv.org/abs/2602.04495)
*Maher Harb,Nader Foroughi,Matt Stehman,Bob Lutz,Nati Erez,Erik Garcell*

Main category: cs.ET

TL;DR: 该论文将电信网络中的延迟弹性Layer 3路由优化问题建模为图优化问题，使用量子近似优化算法（QAOA）在量子模拟器和硬件上求解，验证了该公式在5节点7边测试拓扑上的有效性。


<details>
  <summary>Details</summary>
Motivation: 电信网络优化问题具有指数级增长的组合搜索空间，计算复杂度高。需要解决具有预定义Layer 1光链路的Layer 3路由优化问题，目标是：最小化延迟、创建从每个站点到互联网骨干网的顶点不相交路径、通过限制双链路故障影响来最大化整体弹性。

Method: 将问题建模为图优化问题，目标函数包含延迟最小化和弹性最大化。具体化为寻找两条不相交的最短路径，并在目标函数中加入弹性组件。将该数学公式适配为量子近似优化算法（QAOA），在量子模拟器和量子硬件上执行。在包含5个顶点和7条边的测试拓扑上进行实验，考虑两种极限场景：独立链路故障和高度相关故障。

Result: 在两个探索的场景中都产生了最优网络设计，对应出现频率最高的有效解和最小能量状态。验证了所提出的公式在量子系统上优化Layer 3路由的有效性。

Conclusion: 成功将延迟弹性Layer 3路由优化问题适配为量子优化问题，并通过QAOA在测试拓扑上验证了解决方案的有效性，为未来量子系统上的网络优化提供了可行路径。

Abstract: Network optimization problems represent large combinatorial search spaces that grow exponentially with network size, making them computationally intensive to solve. This paper addresses the latency-resilient Layer 3 routing optimization problem in telecommunications networks with predefined Layer 1 optical links. We formulate this problem as a graph-based optimization problem with the objective of minimizing latency, creating vertex-disjoint paths from each site to the internet backbone, and maximizing overall resiliency by limiting the impact of dual-link failures. By framing the problem as finding two disjoint shortest paths, coupled together with a resiliency component to the objective function, we establish a single formulation to produce optimal path design. The mathematical formulation was adapted to solve the problem using quantum approximate optimization algorithm (QAOA) executed over both quantum simulator and quantum hardware. QAOA was tested on a toy graph topology with 5 vertices and 7 edges and considering two limiting scenarios respectively representing independent (uncorrelated) link failures and highly correlated failure for one pair of edges. Both explored scenarios produced the optimal network design-corresponding to the valid solution with highest frequency of occurrence and minimum energy state, hence, validating the proposed formulation for optimizing Layer 3 routing on quantum systems of the future.

</details>
