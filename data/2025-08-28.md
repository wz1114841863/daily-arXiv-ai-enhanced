<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 72]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 首个将调度器与数字孪生技术集成的高性能计算框架，支持部署前的参数配置影响分析和调度决策评估


<details>
  <summary>Details</summary>
Motivation: 传统调度器评估方法局限于部署后分析或模拟器，无法模拟相关基础设施，需要能够在部署前进行what-if研究的解决方案

Method: 开发首个具有调度能力的数字孪生框架，集成多种顶级HPC系统数据集，扩展外部调度模拟器集成，实现激励结构和机器学习调度的评估

Result: 创建了支持HPC系统what-if场景评估的数字孪生元框架，能够评估可持续性和对模拟系统的影响

Conclusion: 该工作为HPC调度提供了创新的数字孪生集成方法，实现了部署前的参数配置和调度决策影响分析，为原型调度提供了新途径

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


### [2] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: HAP是一种动态混合并行策略选择方法，通过层次化分解MoE架构为注意力模块和专家模块，使用整数线性规划优化并行配置，显著提升MoE模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型推理系统主要采用静态并行策略，缺乏灵活性，无法在不同推理场景下始终获得最优性能。需要一种能够自适应不同计算需求的动态并行化方法。

Method: 将MoE架构层次化分解为注意力模块和专家模块，为每个模块构建专门的推理延迟模拟模型。使用整数线性规划(ILP)求解最优混合并行配置，最大化推理效率。

Result: 在A100、A6000和V100 GPU平台上分别实现了1.68倍、1.77倍和1.57倍的加速比。在Mixtral和Qwen系列模型上展现出优秀的泛化能力。

Conclusion: HAP方法通过动态选择混合并行策略，显著提升了MoE模型的推理效率，且具有良好的泛化性能，为MoE模型的高效推理提供了有效的解决方案。

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [3] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: 本文对Algorand共识协议进行了形式化建模和验证，通过概率进程演算分析其在无对抗和恶意节点攻击下的行为，使用CADP工具进行等价性检查，验证了协议的鲁棒性和局限性。


<details>
  <summary>Details</summary>
Motivation: Algorand是一个可扩展的安全无许可区块链，采用密码学自排序和二进制拜占庭共识。为了进行严格的形式化验证，需要建立其共识协议的代数模型。

Method: 使用概率进程演算对Algorand共识协议进行建模，捕获参与者行为和各共识步骤的结构化交替。在无对抗环境下验证正确性后，扩展模型以分析恶意节点强制提交空块的影响。

Result: 验证了协议在无对抗环境下的正确性，分析了恶意攻击场景下协议的鲁棒性和局限性，发现恶意节点可以强制提交空块而非提议块。

Conclusion: 这项工作展示了形式化方法在分析区块链共识算法中的附加价值，既突出了Algorand协议在对抗假设下的鲁棒性，也揭示了其局限性。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [4] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: 本文探讨了生成式AI作为6G网络实现环境智能的核心技术，通过生成合成数据、语义消息转换、网络预测和数字孪生更新等能力，将6G从快速网络转变为环境智能生态系统。


<details>
  <summary>Details</summary>
Motivation: 实现全球规模的环境智能需要6G网络具备实时感知、推理和行动能力，而传统AI无法完全满足这些需求，因此需要生成式AI来弥补关键差距。

Method: 回顾了生成式AI的基础模型（GANs、VAEs、扩散模型、生成式变换器），并将其与频谱共享、超可靠低延迟通信、智能安全和上下文感知数字孪生等实际应用案例相结合。

Result: 研究表明生成式AI能够有效解决环境智能中的关键问题，包括生成合成传感器数据、转换用户意图、预测网络条件和更新数字孪生，同时保持隐私保护。

Conclusion: 生成式AI不是外围补充，而是将6G从快速网络转变为环境智能生态系统的基础要素，但仍需解决能效、可信合成数据、联邦生成学习和标准化等开放挑战。

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [5] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale是一个针对Prefill-Decode解耦架构的协调自动扩缩框架，通过拓扑感知调度和基于生产环境大规模实证研究的指标驱动策略，显著提升GPU利用率并节省大量GPU小时。


<details>
  <summary>Details</summary>
Motivation: 传统自动扩缩器在处理大型语言模型服务时效果不佳，特别是面对现代Prefill-Decode解耦架构时，存在异构硬件利用效率低、网络瓶颈和预填充-解码阶段不平衡等操作挑战。

Method: 结合拓扑感知调度器（适应异构硬件和网络约束）和基于大规模生产环境实证研究的新型指标驱动策略，使用单一稳健指标联合扩缩预填充和解码资源池。

Result: 在数万GPU的大规模生产环境中部署，平均GPU利用率提升26.6个百分点，每天节省数十万GPU小时，同时满足严格的服务级别目标。

Conclusion: HeteroScale有效解决了Prefill-Decode解耦架构的核心挑战，实现了高效的资源管理和架构平衡，证明了其在生产环境中的实用性和有效性。

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [6] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: 这篇论文分析了混合关键性系统中IOMMU结构导致的继时干扰问题，通过实验证明小内存交易的翻译开销会导致DMA传输延迟达到1.79倍


<details>
  <summary>Details</summary>
Motivation: 异构计算平台中的加速器和DMA设备作为独立总线主控设备直接访问内存，导致安全性和时间可预测性挑战，需要研究IOMMU在性能干扰方面的影响

Method: 使用Xilinx UltraScale+ ZCU104平台分析IOMMU结构内的争用效应，重点研究共享TLB缓存机制导致的时间不可预测性

Result: 实验结果显示IOMMU干扰主要影响小内存交易，在Arm SMMUv2实现中导致DMA传输延迟达到1.79倍，证明了IOTLB争用效应在不同架构上的类似行为

Conclusion: IOMMU作为保障安全的关键组件，其共享结构特性同时引入了时间不可预测性风险，小内存交易的翻译开销需要特别关注

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [7] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 本文通过引入新问题（ETE、HET、TAR(d)*等）分析了移动机器人模型中能力、灯光可观测性和调度器同步性之间的复杂交互关系，扩展了14个经典模型的分离图谱。


<details>
  <summary>Details</summary>
Motivation: 理解移动机器人系统的计算能力是分布式计算中的基本挑战。先前工作主要关注模型间的两两分离，本文旨在探索机器人能力、灯光可观测性和调度器同步性之间更复杂的交互方式。

Method: 通过定义和分析多个新问题（ETE、HET、TAR(d)*、LP-MLCv、VEC、ZCC、VTR、LP-Cv），在不同同步性和能力设置下进行理论分析，展示内存、灯光和同步性之间的替代和互补关系。

Result: 发现ETE问题只能在最强模型（完全同步+全互见灯光）中解决；在弱同步下内部内存不足，完全同步可替代灯光和内存；在异步设置下揭示了FSTA和FCOM机器人之间的细粒度分离；展示了内部内存在对称设置中的局限性。

Conclusion: 研究结果扩展了已知的机器人模型分离图谱，揭示了只有通过高阶比较才能看到的结构现象，提供了新的不可能性标准，并深化了对可观测性、内存和同步性如何共同塑造移动机器人计算能力的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs](https://arxiv.org/abs/2508.19299)
*Rishov Sarkar,Cong Hao*

Main category: cs.AR

TL;DR: OmniSim是一个HLS仿真框架，通过软件多线程和FIFO表技术，实现了对复杂数据流设计的高速准确仿真，解决了现有工具无法支持循环依赖和非阻塞FIFO访问等高级特性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前HLS工具在仿真复杂数据流设计时存在两个主要限制：1）功能验证需要依赖缓慢的RTL综合和仿真；2）性能指标获取也依赖RTL仿真。现有工具无法完全克服第一个限制，而现有解决方案如LightningSim对高级数据流特性支持不足。

Method: OmniSim采用软件多线程技术，通过查询和更新记录精确硬件时序的FIFO表来协调线程执行。采用灵活耦合和重叠的功能与性能仿真方法，实现近C级仿真速度和近RTL级精度。

Result: OmniSim成功仿真了11个之前任何HLS工具都不支持的设计，相比传统C/RTL协同仿真获得最高35.9倍加速，相比最先进的LightningSim在其基准测试套件上获得最高6.61倍加速。

Conclusion: OmniSim显著扩展了学术和商业HLS工具的仿真能力，能够高效准确地仿真复杂数据流设计，特别是商业工具明确声明不支持的设计，在保持高精度的同时实现了接近C级的仿真速度。

Abstract: High-Level Synthesis (HLS) is increasingly popular for hardware design using
C/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware
behavior in a sequential language like C/C++, HLS tools introduce constructs
such as infinite loops and dataflow modules connected by FIFOs. However,
efficiently and accurately simulating these constructs at C level remains
challenging. First, without hardware timing information, functional
verification typically requires slow RTL synthesis and simulation, as the
current approaches in commercial HLS tools. Second, cycle-accurate performance
metrics, such as end-to-end latency, also rely on RTL simulation. No existing
HLS tool fully overcomes the first limitation. For the second, prior work such
as LightningSim partially improves simulation speed but lacks support for
advanced dataflow features like cyclic dependencies and non-blocking FIFO
accesses.
  To overcome both limitations, we propose OmniSim, a framework that
significantly extends the simulation capabilities of both academic and
commercial HLS tools. First, OmniSim enables fast and accurate simulation of
complex dataflow designs, especially those explicitly declared unsupported by
commercial tools. It does so through sophisticated software multi-threading,
where threads are orchestrated by querying and updating a set of FIFO tables
that explicitly record exact hardware timing of each FIFO access. Second,
OmniSim achieves near-C simulation speed with near-RTL accuracy for both
functionality and performance, via flexibly coupled and overlapped
functionality and performance simulations.
  We demonstrate that OmniSim successfully simulates eleven designs previously
unsupported by any HLS tool, achieving up to 35.9x speedup over traditional
C/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less
capable simulator, LightningSim, on its own benchmark suite.

</details>


### [9] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: GENIE-ASI是首个基于大型语言模型的免训练模拟子电路识别方法，通过上下文学习和代码生成实现自动化识别，在简单结构中达到完美性能，在复杂电路中展现潜力。


<details>
  <summary>Details</summary>
Motivation: 传统模拟子电路识别方法需要大量人工专业知识、基于规则的编码或标注数据集，限制了自动化程度和可扩展性。

Method: 采用两阶段方法：首先通过上下文学习从少量示例推导自然语言指令，然后将指令转换为可执行的Python代码来识别未见过的SPICE网表中的子电路。

Result: 在提出的基准测试中，GENIE-ASI在简单结构上达到F1分数1.0（与基于规则方法相当），在中等抽象级别上保持竞争力（F1=0.81），在复杂子电路上展现潜力（F1=0.31）。

Conclusion: 大型语言模型可以作为模拟设计自动化中适应性强的通用工具，为基础模型在模拟设计自动化中的应用开辟了新的研究方向。

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>


### [10] [RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs](https://arxiv.org/abs/2508.19530)
*Yanyun Wang,Dingcui Yu,Yina Lv,Yunpeng Song,Yumiao Zhao,Liang Shi*

Main category: cs.AR

TL;DR: RARO是一种可靠性感知的混合闪存管理方案，通过智能数据迁移优化QLC闪存的读取性能，减少不必要的模式转换和容量损失。


<details>
  <summary>Details</summary>
Motivation: QLC闪存虽然成本低容量大，但可靠性有限导致频繁读取重试，严重影响读取性能。现有混合存储方案主要关注写入性能，基于数据热度进行迁移决策，导致过多模式切换和容量开销。

Method: RARO只在热数据位于高读取重试次数的QLC块时才触发数据迁移，减少不必要的转换。支持细粒度多模式转换(SLC-TLC-QLC)，利用实时读取重试统计和闪存特性来缓解过度转换。

Result: 在FEMU平台上的实验表明，RARO显著提高了各种工作负载下的读取性能，对可用容量的影响可以忽略不计。

Conclusion: RARO通过可靠性感知的智能数据迁移策略，在最小化容量成本的同时有效优化了QLC闪存的读取性能，解决了现有混合存储方案的过度转换问题。

Abstract: Quad-level cell (QLC) flash offers significant benefits in cost and capacity,
but its limited reliability leads to frequent read retries, which severely
degrade read performance. A common strategy in high-density flash storage is to
program selected blocks in a low-density mode (SLC), sacrificing some capacity
to achieve higher I/O performance. This hybrid storage architecture has been
widely adopted in consumer-grade storage systems. However, existing hybrid
storage schemes typically focus on write performance and rely solely on data
temperature for migration decisions. This often results in excessive mode
switching, causing substantial capacity overhead.
  In this paper, we present RARO (Reliability-Aware Read performance
Optimization), a hybrid flash management scheme designed to improve read
performance with minimal capacity cost. The key insight behind RARO is that
much of the read slowdown in QLC flash is caused by read retries. RARO triggers
data migration only when hot data resides in QLC blocks experiencing a high
number of read retries, significantly reducing unnecessary conversions and
capacity loss. Moreover, RARO supports fine-grained multi-mode conversions
(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time
read retry statistics and flash characteristics, RARO mitigates over-conversion
and optimizes I/O performance. Experiments on the FEMU platform demonstrate
that RARO significantly improves read performance across diverse workloads,
with negligible impact on usable capacity.

</details>


### [11] [Support Vector Machines Classification on Bendable RISC-V](https://arxiv.org/abs/2508.19656)
*Polykarpos Vergos,Theofanis Vergos,Florentia Afentaki,Konstantinos Balaskas,Georgios Zervakis*

Main category: cs.AR

TL;DR: 这篇论文提出了一种用于可弯曲电子设备的开源机器学习协处理器框架，采用支持向量机加速器设计，实现了21倍的推理速度和能源效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决可弯曲电子设备在机器学习应用中遇到的大尺寸和高功耗挑战，为边缘设备提供低功耗智能化方案。

Method: 开发了一种开源框架，为Bendable RISC-V核设计ML协处理器，包括支持向量机加速器设计，支持OvO和OvR算法，以及4-、8-、16-bit精度可缩放的权重表示。

Result: 实验结果显示在推理执行时间和能源效率上均实现21倍的提升，证明了该方案在低功耗可弯曲边缘智能方面的潜力。

Conclusion: 该开源框架和加速器设计有效解决了可弯曲电子设备的ML应用挑战，为边缘智能应用提供了高效、低功耗的解决方案。

Abstract: Flexible Electronics (FE) technology offers uniquecharacteristics in
electronic manufacturing, providing ultra-low-cost, lightweight, and
environmentally-friendly alternatives totraditional rigid electronics. These
characteristics enable a rangeof applications that were previously constrained
by the costand rigidity of conventional silicon technology. Machine learning
(ML) is essential for enabling autonomous, real-time intelligenceon devices
with smart sensing capabilities in everyday objects. However, the large feature
sizes and high power consumption ofthe devices oppose a challenge in the
realization of flexible ML applications. To address the above, we propose an
open-source framework for developing ML co-processors for the Bendable RISC-V
core. In addition, we present a custom ML accelerator architecture for Support
Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)
algorithms. Our ML accelerator adopts a generic, precision-scalable design,
supporting 4-, 8-, and 16-bit weight representations. Experimental results
demonstrate a 21x improvement in both inference execution time and energy
efficiency, on average, highlighting its potential for low-power, flexible
intelligence on the edge.

</details>


### [12] [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](https://arxiv.org/abs/2508.19868)
*Geraldo F. Oliveira*

Main category: cs.AR

TL;DR: 这篇论文提出了四个主要贡献来解决DRAM-based PIM架构的工具、编程模型和系统支持问题，包括DAMOV方法论、MIMDRAM硬件/软件协同设计、Proteus硬件框架和DaPPA编程框架。


<details>
  <summary>Details</summary>
Motivation: 为缓解当前和未来系统中处理内存(PIM)架构的采用障碍，特别是DRAM-based解决方案，需要提供更好的工具、编程模型和系统支持。

Method: 1) DAMOV - 数据移动瓶颈表征方法和基准测试套件
2) MIMDRAM - 硬件/软件协同设计解决PUD架构的可编程性和灵活性限制
3) Proteus - 硬件框架通过三种方式降低PUD操作延迟
4) DaPPA - 编程框架简化通用PNM架构的可编程性

Result: 开发了一套完整的工具链和方法论，显著改善了PIM架构的可编程性、灵活性和执行效率，特别是降低了PUD操作的延迟和能耗。

Conclusion: 通过四个创新性的贡献，为DRAM-based PIM架构提供了全面的解决方案，有望促进PIM技术在现有和未来系统中的广泛应用。

Abstract: Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 提出了一种基于正则化最小二乘的物理信息回归(PIR)方法，用于参数线性非线性动态模型的参数估计，在计算效率和精度上优于物理信息神经网络(PINN)。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性动态模型参数估计的效率问题，特别是对于参数线性模型，需要一种既能结合物理理论又能快速处理时间序列数据的方法。

Method: 使用正则化普通最小二乘法对参数线性系统进行参数估计，通过物理信息回归(PIR)将理论模型与数据驱动方法结合。

Result: PIR方法在复杂度和参数数量不同的流行病模型上表现优于PINN，计算速度更快，特别是在高复杂度模型中优势明显，并能成功应用于COVID-19真实数据的时间变化参数估计。

Conclusion: PIR方法为参数线性非线性动态模型提供了可靠且快速的参数估计方案，支持实时应用，在计算效率和性能上都优于PINN方法。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [14] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 扩展ZipNN压缩方法到低精度浮点格式(FP8/FP4)，通过分离压缩指数和尾数部分，实现高达83%的压缩比，并在LLM的K/V缓存张量中也发现可压缩模式。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型越来越大，减少神经网络权重的存储和传输成本越来越重要。虽然ZipNN等无损压缩方法在FP32和BF16高精度格式中表现优异，但需要扩展到正在估计中普遍使用的低精度格式(FP8/FP4)。

Method: 设计了一种压缩方法，将浮点数的指数和尾数组件分离并独立地使用使用低权编码进行压缩。扩展了ZipNN方法以支持FP8和FP4低精度浮点格式。

Result: 压缩比达到BF16的62%和FP8的83%。还发现大语言模型中的关键值(K/V)缓存张量同样呈现出可压缩的模式，能够在部署过程中节省内存。

Conclusion: 该方法成功将高效的无损压缩扩展到低精度浮点格式，为深度学习模型的存储和传输提供了显著的成本节省，并在LLM部署中发现了新的内存优化机会。

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [15] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 首次全面探索低功耗柔性应力分类器的设计空间，涵盖多种机器学习分类器、特征选择和神经网络简化算法，设计了1200多个柔性分类器，实现了比现有方法更高精度的实时应力监测。


<details>
  <summary>Details</summary>
Motivation: 传统应力监测依赖间歇性、症状导向的干预，缺乏连续、可及且成本效益高的解决方案。现有硅基可穿戴设备虽然功能多样，但不够轻便灵活，而柔性电子虽具灵活性但集成复杂电路如机器学习分类器存在挑战。

Method: 进行全面的设计空间探索，包括多种机器学习分类器、特征选择算法和神经网络简化技术，设计完全定制化的低精度算术电路，优化硬件效率。

Result: 开发了超过1200个柔性分类器，实现了比现有方法更高的准确度，同时保持低成本、可贴合性、低功耗和小尺寸。

Conclusion: 这项工作为设计实时应力分类器提供了重要见解，实现了在柔性电子平台上高效运行机器学习算法，为连续应力监测提供了可行的技术方案。

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [16] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: POT是一种仅需提示的黑盒攻击框架，通过LLM迭代优化生成隐蔽且语义自然的对抗提示，使模型产生过度冗长的推理链而消耗计算资源，无需外部数据访问或模型检索。


<details>
  <summary>Details</summary>
Motivation: 现有过度思考攻击需要外部知识源进行数据投毒、依赖可检索的污染内容以及结构明显的模板，限制了实际应用。需要开发更实用的攻击方法。

Method: 提出POT框架，使用基于LLM的迭代优化来生成隐蔽且语义自然的对抗提示，无需外部数据访问和模型检索，实现黑盒攻击。

Result: 在多种模型架构和数据集上的广泛实验表明，POT相比其他方法具有优越性能，能够有效诱导模型产生计算效率低下的冗长推理过程。

Conclusion: POT成功解决了现有过度思考攻击的限制，提供了一种更实用和有效的黑盒攻击方法，揭示了CoT推理过程中的新型安全漏洞。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [17] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: 提出了一种在真实分布式物联网环境中训练深度强化学习模型的新框架，使用ACK反馈信息进行训练，实现了基于DRL的信道选择方法


<details>
  <summary>Details</summary>
Motivation: 现有研究很少探索在真实分布式物联网系统中使用真实数据训练DRL模型，需要弥合这一研究空白

Method: 物联网设备使用基于DRL的方法选择通信信道，通过实际数据传输获得的ACK信息作为反馈来训练DRL模型

Result: 通过帧成功率(FSR)的性能评估证明了所提框架的可行性和有效性

Conclusion: 该框架为在真实分布式物联网环境中训练DRL模型提供了一种有效的解决方案，展示了实际应用的潜力

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [18] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame是一个插件模块，通过关联记忆缓冲区整合少量专家数据来提升离线强化学习性能，仅需0.1%的专家轨迹就能在D4RL MuJoCo任务上获得显著改进


<details>
  <summary>Details</summary>
Motivation: 离线强化学习经常面临次优数据的问题，难以获得大规模专家数据集。核心挑战是如何有效利用稀缺的专家演示和丰富的低质量数据

Method: 提出Re:Frame模块，在标准离线RL策略基础上增加关联记忆缓冲区(AMB)，存储专家轨迹。策略通过基于内容的关联检索专家数据并整合到决策中，无需环境交互或修改主干架构

Result: 在D4RL MuJoCo任务上，仅使用60条专家轨迹(数据集的0.1%)就在4个设置中的3个上显著优于Decision Transformer基线，最高提升10.7个标准化点

Conclusion: Re:Frame提供了一种简单且数据高效的方法来注入稀缺专家知识，显著改善从低质量数据集进行的离线强化学习

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [19] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 提出了NCMemo框架来量化图神经网络在半监督节点分类中的标签记忆现象，发现图同质性越低记忆效应越强，并提出了图重布线方法来缓解记忆效应和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被证明会记忆训练数据，但图神经网络的记忆分析研究相对不足，需要系统研究GNN在节点分类任务中的记忆行为及其与图结构特性的关系。

Method: 开发了NCMemo框架来量化标签记忆，分析了图同质性与记忆的关系，研究了GNN训练动态和隐式偏差，并探索了图重布线作为缓解记忆的方法。

Result: 发现图同质性与记忆呈反比关系，低同质性图导致更高的记忆；节点特征空间邻域中标签不一致性高的节点更容易被记忆；图重布线能有效减少记忆而不影响模型性能。

Conclusion: 该研究深化了对GNN学习机制的理解，揭示了图结构与记忆的关系，提出的图重布线方法有助于实现更隐私保护的GNN部署，为GNN的理论分析和实际应用提供了重要见解。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [20] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解(SVD)的多源过渡学习方法，通过分解源模型为基础组件并精准聚合知识，解决了现有方法的效率和精度问题。


<details>
  <summary>Details</summary>
Motivation: 传统过渡学习方法没有充分利用网上众多源模型的知识，而现有的多源过渡学习方法缺乏细粒度知识提取能力和高效聚合能力。

Method: 使用奇异值分解(SVD)将每个源模型分解为基础的秩一组件，然后选择所有源中最显著的组件进行聚合，最后通过细调合并矩阵的主要奇异值来适应目标任务。

Result: 该方法实现了高效的过渡学习，对输入层面和参数空间的干扰具有稳健性，并且计算效率良好。

Conclusion: 基于SVD的细粒度知识提取和聚合方法有效解决了多源过渡学习中的效率和精度问题，为涉及大量源模型或高参数模型的知识聚合提供了可行方案。

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [21] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: 这篇论文介绍了图数据模型在化学科学中的应用，重点讲解了图神经网络如何用于分子、蛋白质和化学过程的分析与预测。


<details>
  <summary>Details</summary>
Motivation: 化学科学中的分子、蛋白质和反应过程天然具有图结构特征，需要专门的图数据建模方法来处理这些复杂的相互关系，以推动新一代化学发现。

Method: 介绍了图设计的基础原理、关键预测任务，以及图神经网络等机器学习算法在图结构数据上的操作方法。

Result: 提供了化学科学中图方法应用的典型案例，展示了图神经网络在分子性质预测、蛋白质功能分析等任务中的有效性。

Conclusion: 图数据建模为化学发现提供了强大的工具，图神经网络等学习方法能够有效处理化学科学中的复杂图结构数据，为下一代化学研究奠定基础。

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [22] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级深度学习模型，使用RR间期和给合TCN与Mamba模型，能够提前2小时预测房颤，准确率高且计算效率优异。


<details>
  <summary>Details</summary>
Motivation: 房颤(AF)是最常见的异常心律，特别是发作性房颤(PAF)因其突然发作和短暂持续时间而难以检测。早期预测AF可以通过预防性治疗减缓疾病进展。

Method: 使用仅RR间期(RRIs)数据，结合时序卷积网络(TCN)进行位置编码，与选择性状态空间模型Mamba结合，实现高效并行序列建模。

Result: 在主体测试中，模型达到敏感度0.908、特异性0.933、F1分0.930、AUROC 0.972和AUPRC 0.932。模型仅有73.5K参数和38.3 MFLOPs计算量，能用30分钟输入数据预测未来2小时的AF。

Conclusion: 该轻量级模型在准确性和计算效率方面都超过传统CNN-RNN方法，为早期AF预测提供了可行的预防干预时间窗口。

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [23] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 提出了首个基于信息几何和扩散动力学的多模态大语言模型幻觉量化框架，通过谱嵌入和图拉普拉斯算子将幻觉转化为可数学度量的语义失真能量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型幻觉评估方法主要基于启发式定性基准，缺乏理论保证和量化标准，特别是在医学、法律等高风险多模态领域存在严重盲区。

Method: 将多模态LLM输出表示为多模态图拉普拉斯算子的谱嵌入，通过RKHS嵌入中的特征模态分解，建立时间相关温度剖面下的瑞利-里茨紧界来量化幻觉能量。

Result: 开发了模态感知、理论可解释的度量指标，能够捕捉幻觉随时间和输入提示的演化过程，通过温度退火实现幻觉的量化分析。

Conclusion: 该框架为幻觉的量化界定建立了理论基础，将幻觉从定性风险转变为可处理、可分析的数学现象，为可信AI提供了严格的理论保证。

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [24] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 基于LLaMA 3.2的视觉语言模型在HEP中微子相互作用分类任务上表现优于传统CNN，支持多模态推理和上下文整合


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多模态推理方面的潜力，特别是在高能物理实验中的中微子相互作用分类任务

Method: 使用基于LLaMA 3.2的视觉语言模型进行微调，与NOvA和DUNE实验中使用的CNN基线模型进行性能对比

Result: VLM在分类准确率、精确度、召回率和AUC-ROC等指标上达到或超过CNN性能，并支持更丰富的推理和辅助文本语义上下文整合

Conclusion: 视觉语言模型为HEP中的事件分类提供了有前景的通用骨干网络，为实验性中微子物理中的多模态方法开辟了新途径

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [25] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 重点研究了量子机器学习(QML)在恶意软件分类中的应用，比较了QMLP和QCNN两种混合量子-经典模型的性能，在多个数据集上得到了过那经典机器学习的精度结果。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的出现，量子机器学习(QML)为改善恶意软件检测提供了范式转移的机会，但该领域仍未得到充分探索。本研究希望填补这一空白，探索QML在恶意软件分类中的应用潜力。

Method: 研究使用了两种混合量子-经典模型：量子多层感知机(QMLP)和量子卷积神经网络(QCNN)。两种模型都采用角度嵌入技术将恶意软件特征编码为量子状态。QMLP通过全量子比特测量和数据重新上传来捕捉复杂模式，而QCNN则通过量子卷积和池化层减少活跃量子比特来实现更快的训练。

Result: 在二进制分类任务中：API-Graph数据集达到95-96%精度，AZ-Domain数据集91-92%精度，EMBER-Domain数据集77%精度。在多分类任务中：API-Graph数据集91.6-95.7%精度，AZ-Class数据集41.7-93.6%精度，EMBER-Class数据集60.7-88.1%精度。总体而言，QMLP在复杂的多分类任务上表现更优，而QCNN虽然减少了精度但提高了训练效率。

Conclusion: 量子机器学习在恶意软件分类中展现出了强大的潜力，能够达到过那经典机器学习的精度水平。QMLP模型在复杂分类任务中表现更优，而QCNN模型在训练效率方面有明显优势。这为量子计算在网络安全领域的应用开启了新的可能性。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [26] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 提出DETNO架构，结合Transformer神经算子和扩散模型，解决交通流量预测中高频特征丢失和长期预测误差累积问题


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在交通流量预测中会产生平滑预测，无法重建高频特征（如密度梯度），导致多步预测时误差快速累积，影响实时交通管理

Method: 采用统一的扩散增强Transformer神经算子架构，包含具有交叉注意力机制的Transformer神经算子（提供模型表达能力和超分辨率）和基于扩散的细化组件（通过渐进去噪迭代重建高频交通细节）

Result: 在混沌交通数据集上的综合评估显示，该方法在扩展预测方面优于传统和基于Transformer的神经算子，能够保持高频成分并提高长期预测稳定性

Conclusion: DETNO架构有效克服了标准神经算子的固有平滑限制和预测不稳定性，为长期交通流量预测提供了更准确的解决方案

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [27] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: 一种量子-经典混合架构，通过量子编码与经典序列模型相结合，在SMILES字符串重构任务中实现了84%量子保真度和60%经典相似度，超越现有量子基准方法。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习在分子设计领域展示出潜力，但在SMILES字符串重构等序列任务中仍然面临保真度下降的挑战，需要探索更有效的量子-经典混合方案。

Method: 提出了一种混合量子-经典架构，将量子编码与经典序列模型相结合，以提高量子保真度和经典相似度。

Result: 在SMILES重构任务中实现了约84%的量子保真度和60%的经典重构相似度，表现超越现有量子基准方法。

Conclusion: 该方法为未来量子机器学习应用奠定了基础，在表达力强的量子表示和经典序列模型之间找到了平衡，有望推动分子和药物发现领域的量子感知序列模型研究。

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [28] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: 提出了基于Kolmogorov-Arnold表示理论的哈密顿神经网络(KAR-HNN)，用单变量变换替代MLP，解决了传统HNN对超参数敏感的问题，能更好地捕捉高频多尺度动力学，减少能量漂移，提高长期预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统哈密顿神经网络(HNNs)虽然通过直接从数据学习哈密顿函数来保证能量守恒，但基于MLP的实现方式在处理复杂能量景观时对超参数过于敏感，影响了模型的稳定性和准确性。

Method: 采用Kolmogorov-Arnold表示理论，用单变量变换替代多层感知机(MLPs)，利用局部函数逼近来更好地捕捉高频和多尺度动力学特性，同时保持哈密顿系统的辛形式结构。

Result: 在弹簧质量系统、单摆、二体和三体问题四个基准测试中，KAR-HNN表现出优异的性能，显著减少了能量漂移，提高了长期预测稳定性。

Conclusion: KAR-HNN方法为高维度和参数稀缺的现实物理过程提供了准确稳定的建模方案，具有很好的物理一致性和可解释性，有望在复杂物理系统建模中发挥重要作用。

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [29] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct在聊天格式中错误判断"9.11"大于"9.8"，研究发现偶数注意力头专门处理数值比较，需要至少8个偶数头才能修复此bug，揭示了Transformer的模块化结构和计算阈值。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在格式依赖推理失败中的机制，特别是数值比较错误在不同格式中的表现差异，以深入理解模型内部工作机制。

Method: 通过系统性干预实验，分析注意力头的奇偶索引功能分化，使用稀疏自编码器(SAE)分析特征表示，并测试不同数量注意力头组合的修复效果。

Result: 发现偶数头专门处理数值比较，需要至少8个偶数头才能完美修复bug；特征表示在层7分离(10%重叠)，在层10重新纠缠(80%重叠)；失败格式中特定特征放大1.5倍。

Conclusion: Transformer模型具有高度模块化的子结构，看似需要完整模块的功能实际上可以通过部分组件实现，这对模型可解释性和效率优化有重要启示。

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [30] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 提出了一种物理信息机器学习工作流，结合可微分多相流模拟器和CNN，通过迁移学习大幅减少多相流模拟需求，从千万级降至三千次


<details>
  <summary>Details</summary>
Motivation: 地下储层压力控制面临地质异质性和多相流动力学的挑战，传统高保真物理模拟计算成本极高，且需要大量模拟来处理不确定性

Method: 使用完全可微分多相流模拟器（DPFEHM框架）与卷积神经网络耦合，CNN学习从渗透率场预测流体提取率以控制压力。采用预训练（单相稳态模拟）加微调（多相场景）策略

Result: 仅需不到3000次全物理多相流模拟即可实现高精度训练，相比之前需要上千万次模拟大幅降低计算成本，准确度优于先前工作

Conclusion: 该方法通过物理信息机器学习和迁移学习策略，显著降低了多相流储层模拟的计算负担，为实际注采场景提供了更实用准确的预测能力

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [31] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 提出基于对比学习的无监督框架，使用双视角突变特征对43种癌症类型进行聚类分析


<details>
  <summary>Details</summary>
Motivation: 理解泛癌突变景观对肿瘤发生机制至关重要，目前队列水平的癌症聚类主要依赖传统统计方法，缺乏先进的机器学习技术

Method: 使用COSMIC数据库的编码突变数据，构建基因水平和染色体水平的互补突变特征，通过TabNet编码器和多尺度对比学习目标(NT-Xent损失)学习统一的癌症类型嵌入表示

Result: 学习到的潜在表征能够产生具有生物学意义的癌症类型聚类，与已知的突变过程和组织起源一致

Conclusion: 这是对比学习在队列水平癌症聚类中的首次应用，为突变驱动的癌症亚型分析提供了可扩展且可解释的框架

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [32] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 提出了一种基于空间填充采样的数据增强策略，用于从计算机模型中生成神经PDE训练数据，相比传统轨迹数据采样方法更高效。


<details>
  <summary>Details</summary>
Motivation: 传统神经PDE训练需要长时间积分获得的轨迹数据，存在大量时空冗余，且可能遗漏状态空间中很少访问但对泛化重要的状态。

Method: 通过空间填充采样局部"模板"状态，从计算机模型生成合成训练数据，减少时空冗余并过采样重要状态。

Result: 仅需相当于10个时间步长的数值模拟计算量即可学习到准确的神经PDE模板算子，如果有完整轨迹模拟数据可进一步改进精度。

Conclusion: 该方法在多个PDE系统中表现出色，相比朴素采样的模板数据能获得更好的训练神经模板算子性能。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [33] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: 提出在生成模型中引入张量分解来降低多维数据生成成本，通过生成较小的张量因子而非完整张量来减少输出和参数数量


<details>
  <summary>Details</summary>
Motivation: 大型复杂模拟数据集生成耗时耗资源，特别是昂贵实验需要为下游任务生成合成数据时，现有生成模型如GAN和扩散模型效率仍有提升空间

Method: 对多维数据（张量）采用内部张量分解，生成较小的张量因子而不是完整张量，显著减少模型输出和总体参数

Result: 实验表明该方法能有效降低复杂模拟数据的生成成本，同时生成的数据仍保持实用性

Conclusion: 张量分解有潜力提高生成模型的效率，特别是在生成多维数据或张量时

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [34] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: 这篇论文证明了现代神经网络架构（如GPT风格变换器和扩散模型）几乎总是满射的，这意味着任何输出都可能被生成，包括有害内容，从而引发模型安全和盗窃漏洞的担忧。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的满射性问题，因为满射性意味着生成模型可能产生任何输出，包括有害内容，这会对模型安全和防止盗窃攻击造成威胁。

Method: 通过数学证明方法，分析现代神经网络的核心组件（如前置层标准化和线性注意力模块）的满射性质。

Result: 证明了许多现代神经网络架构几乎总是满射的，包括GPT风格变换器和使用确定性ODE求解器的扩散模型都允许任意输出的逆映射。

Conclusion: 这项研究提供了一个形式化框架，说明了现代神经网络架构对广泛类别的对手攻击存在无法避免的漏洞，为模型安全研究提供了重要的理论基础。

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [35] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 本文研究了成员推理攻击的样本复杂度，发现在高斯均值估计场景中，攻击者需要Ω(n + n²ρ²)个参考样本才能与完全知情攻击者竞争，这比训练算法使用的样本数量更多。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推理攻击通常假设攻击者拥有来自相同分布的参考样本，但实践中攻击者获取样本的能力有限。本文旨在量化攻击者成功进行成员推理所需的最小参考样本数量。

Method: 研究在高斯均值估计的基本设置下，学习算法从d维高斯分布N(μ,Σ)中获取n个样本，估计μ的误差为E[‖μ̂-μ‖²_Σ]≤ρ²d。分析攻击者进行成员推理所需的最小参考样本数量。

Result: 结果显示，对于这种设置下的成员推理，任何能与完全知情攻击者竞争的attack都需要Ω(n + n²ρ²)个样本。这是首次证明攻击者有时需要比训练算法使用样本数量多得多的样本。

Conclusion: 这一发现对实践有重要意义：当前实践中使用的攻击都限制在O(n)样本量，无法从ω(n)样本中获益，因此可能低估了成员推理的可能性。当分布信息容易获取时，可能存在更好的攻击方法。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [36] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 该论文研究了连续度量空间中无限臂多臂老虎机的激励探索问题，提出了通过补偿激励近视代理进行探索的算法，在存在奖励漂移的情况下实现了次线性累积遗憾和次线性总补偿。


<details>
  <summary>Details</summary>
Motivation: 经典老虎机模型无法处理无限臂空间和激励导致的奖励漂移问题，需要开发新的算法来同时优化探索效率和激励成本。

Method: 提出激励探索算法，通过对无限臂空间进行均匀离散化处理，设计补偿机制来激励代理探索非贪婪选择。

Result: 算法实现了Õ(T^{(d+1)/(d+2)})的遗憾和补偿界限，其中d是度量空间的覆盖维度，并将结果推广到上下文老虎机场景。

Conclusion: 该研究为无限臂空间中的激励探索问题提供了有效的解决方案，理论分析和数值模拟验证了算法的优越性能。

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [37] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas算法通过生成数据的局部低维表示并训练深度神经网络来映射局部嵌入和原始数据，使用拓扑失真评估数据集是否符合流形假设及其维度。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习工具只能生成全局嵌入，无法提供数学定义流形所需的局部映射，也不能验证流形假设是否适用于特定数据集。

Method: 生成数据局部邻域的低维表示，训练深度神经网络在局部嵌入和原始数据之间建立映射，利用拓扑失真评估流形假设和确定维度。

Result: 在测试数据集上成功学习流形结构，发现许多真实数据集（如单细胞RNA测序）不符合流形假设，对符合流形假设的数据可构建生成模型。

Conclusion: DeepAtlas能够验证流形假设并确定数据维度，为符合流形假设的数据集提供了应用微分几何强大工具的可能性，并具有生成建模能力。

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [38] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了SAFT框架来解决表格学习中的分布偏移问题，通过连续表示生成范式实现可微分优化，在多种现实分布偏移场景下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 表格学习在训练和测试数据分布偏移时效果下降，需要解决分布偏移表格学习(DSTL)问题

Method: SAFT框架将表格学习重构为连续表示生成范式，包含三个机制：嵌入去相关和样本重加权的抗偏移表示、次优嵌入平均的平坦感知生成、训练测试分布归一化对齐

Result: 大量实验表明SAFT在鲁棒性、有效性和泛化能力方面持续优于先前的表格学习方法

Conclusion: SAFT通过可微分优化和三个鲁棒机制有效解决了分布偏移下的表格学习问题，具有优异的性能表现

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [39] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE是一个数据高效的精调框架，通过蒸馏将基础模型适配到低数据场景下的符号方程发现任务，结合符号-数值对齐和评估器引导的嵌入优化，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基础模型在大规模方程数据集上预训练后，在小型领域特定数据集上应用时经常出现负迁移和泛化能力差的问题，需要一种数据高效的适配方法。

Method: EQUATE框架结合符号-数值对齐与评估器引导的嵌入优化，将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，以数据-方程拟合度和简洁性为指导。

Result: 在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上，EQUATE在准确性和鲁棒性方面始终优于最先进的基线方法，同时保持低复杂度和快速推理。

Conclusion: EQUATE为基于基础模型蒸馏的数据高效符号回归提供了一个实用且可推广的解决方案。

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [40] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: 提出了PoolFlip环境和Flip-PSRO算法，通过多智能体强化学习解决FlipIt游戏中防御者对未知攻击策略的泛化问题，相比基线方法效果提升2倍


<details>
  <summary>Details</summary>
Motivation: 现有FlipIt框架依赖少量启发式或专门学习技术，导致脆弱性和无法适应新攻击，需要更强大的防御决策自动化方法

Method: 扩展FlipIt游戏为PoolFlip多智能体环境，提出基于种群训练的Flip-PSRO多智能体强化学习方法，设计基于所有权的效用函数

Result: Flip-PSRO防御者对训练中未暴露的启发式攻击泛化能力比基线方法强2倍，同时保持高水平控制权

Conclusion: PoolFlip环境和Flip-PSRO方法有效提升了网络防御中对未知自适应攻击策略的泛化能力和防御效果

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [41] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: 使用Python程序表示游戏策略，通过大型语言模型进行代码优化，让智能体通过执行轨迹和自然语言反馈自我进化，在Atari游戏中达到与深度强化学习相当的性能但训练效率更高


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法需要大量环境交互和训练时间，希望找到更高效的游戏智能体训练方法，利用程序化策略表示和LLM优化来实现快速自进化

Method: 将决策策略表示为Python程序，以当前观察为输入、游戏动作为输出，使用LLM根据执行轨迹和自然语言反馈对代码进行迭代优化，实现自我改进

Result: 在Atari游戏中达到与深度强化学习基线竞争的性能，同时显著减少训练时间和环境交互次数

Conclusion: 程序化策略表示结合LLM优化为构建高效、适应性强的智能体提供了有前景的途径，能够处理复杂的长期推理任务

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [42] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: MobText-SISA是一个针对异构时空数据的机器学习遗忘框架，通过分片、隔离、切片和聚合训练来实现高效的隐私合规数据删除。


<details>
  <summary>Details</summary>
Motivation: 现代移动平台存储了大量GPS轨迹和文本数据，GDPR等隐私法规要求能够按需删除个人数据，但为每个删除请求重新训练深度模型是不可行的。

Method: 将行程的数值和语言特征嵌入共享潜在空间，使用相似性感知聚类将样本分布到分片中，每个分片增量训练，删除时仅需重新训练受影响的分片。

Result: 在10个月的真实移动日志实验中，MobText-SISA保持了基准预测准确性，在错误率和收敛速度上均优于随机分片方法。

Conclusion: MobText-SISA为城市规模多模态移动数据的隐私合规分析提供了实用的基础框架。

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [43] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs在表格数据拟合中存在严重脆弱性，任务无关的数据表示变化（如变量名更改）会导致预测结果大幅波动，即使专门设计的表格基础模型也无法完全避免此问题


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于数据拟合任务，需要评估其在面对任务无关数据变化时的鲁棒性，以确定其是否适合作为可靠的数据分析工具

Method: 通过改变变量名等任务无关的数据表示方式，测试LLMs在上下文学习和监督微调下的预测敏感性，并分析注意力模式来解释敏感性原因

Result: 简单的变量名更改可使预测误差波动高达82%，注意力分析显示非均匀的注意力分布导致对提示中特定位置的过度关注

Conclusion: 尽管LLMs具有强大的预测能力，但目前缺乏基本的鲁棒性，不能作为原则性的数据拟合工具使用

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [44] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: Bi-LoRA方法通过引入辅助LoRA模块来建模SAM的对抗性权重扰动，在保持内存效率的同时实现更平坦的最小值，消除了SAM的双倍训练成本。


<details>
  <summary>Details</summary>
Motivation: SAM方法虽然能通过寻找平坦最小值来提升泛化能力，但其巨大的内存和计算开销使其不适用于大型模型。直接对LoRA参数应用SAM会限制锐度优化到受限子空间，影响效果。

Method: 提出双向低秩适应(Bi-LoRA)，引入辅助LoRA模块来建模SAM的权重扰动。主LoRA模块通过标准梯度下降适应特定任务，辅助模块通过梯度上升捕捉损失景观的锐度。

Result: 在多种任务和架构上的广泛实验证明了Bi-LoRA在提升泛化能力方面的效率和有效性。

Conclusion: Bi-LoRA通过双模块设计成功解决了SAM在大型模型上的应用限制，在保持内存效率的同时显著提升了模型的泛化性能。

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [45] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: 提出了一种基于因果推理的多模态反事实奖励模型，通过Counterfactual Trust Score来减少RLHF中的偏见，在假新闻检测中达到89.12%准确率并改善公平性。


<details>
  <summary>Details</summary>
Motivation: RLHF中奖励模型会学习和放大数据集中的潜在偏见，导致有缺陷的奖励信号和公平性下降，现有偏见缓解方法在因果混淆下可能失效。

Method: 结合因果推理和多模态表示学习，提出反事实奖励模型，包含四个组件的Counterfactual Trust Score：反事实偏移、重构不确定性、公平规则违反检测和时间奖励偏移。

Result: 在多模态真假新闻数据集上评估，准确率达到89.12%，优于基线奖励模型，减少了伪相关性和不公平的强化信号。

Conclusion: 该框架为公平感知的RLHF提供了鲁棒且可解释的方法，具有可调节的偏见减少阈值，提高了动态实时策略制定的可靠性。

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [46] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: 本教程介绍生成式合成数据的基础知识和最新进展，涵盖关键方法、实践框架、评估策略和应用，帮助数据挖掘研究者利用生成式模型解决数据稀缺、隐私和标注问题。


<details>
  <summary>Details</summary>
Motivation: 生成式模型（如大语言模型、扩散模型、生成对抗网络）为数据挖掘中的数据稀缺、隐私保护和标注困难提供了可扩展的解决方案，需要系统性地介绍其理论基础和实践应用。

Method: 教程涵盖生成式合成数据生成的关键方法论和实用框架，包括不同生成模型的技术原理和实现方式。

Result: 参与者将获得关于如何利用生成式合成数据增强数据挖掘研究和实践的可操作见解。

Conclusion: 生成式合成数据技术为数据挖掘领域提供了革命性的解决方案，能够有效应对数据稀缺、隐私和标注等核心挑战。

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [47] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: 提出SyReM方法解决运动预测中持续学习的稳定性-可塑性困境，通过协同记忆回放机制在保持旧知识的同时有效学习新数据


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在运动预测中面临灾难性遗忘问题，现有持续学习方法过度强调记忆稳定性会损害学习可塑性，需要平衡两者关系

Method: 维护紧凑记忆缓冲区，使用不等式约束确保记忆稳定性，基于损失梯度余弦相似度选择相似样本进行选择性记忆回放以增强可塑性

Result: 在11个自然驾驶数据集上验证，相比非持续学习和基线方法，SyReM显著减轻灾难性遗忘并提高新场景预测精度

Conclusion: SyReM通过协同记忆回放机制有效解决了持续学习中的稳定性-可塑性困境，在运动预测任务中表现出色

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [48] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: Delta-Attribution是一个模型无关的框架，通过差分特征归因来解释模型版本间的变化原因，提供轻量级的更新审计方法。


<details>
  <summary>Details</summary>
Motivation: 模型更新（如超参数、核函数、深度、求解器或数据变化）会影响性能，但变化的原因往往不透明，需要一种系统化的方法来解释模型版本间的差异。

Method: 提出Delta-Attribution框架，通过计算特征归因的差异Δφ(x)=φ_B(x)-φ_A(x)来分析模型变化。使用标准化空间中的快速遮挡/钳位方法，配合类别锚定边界和基线平均，并开发了包含多个质量指标的Δ-Attribution质量套件进行评估。

Result: 在45个设置中验证：归纳偏置变化产生大的行为对齐delta（如SVC poly→rbf：BAC≈0.998，DCE≈6.6），而表面调整（如SVC gamma参数变化）显示rank-overlap@10=1.0和DCE≈0。最显著的重分布在更深的GB模型上出现（JSD≈0.357）。

Conclusion: Delta-Attribution提供了一种轻量级的更新审计方法，通过区分良性变化与行为上有意义或风险性的依赖转移，补充了准确性的评估。

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [49] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: Dual-LS是一种受人类大脑互补学习系统启发的在线持续学习范式，通过双记忆重放机制解决DNN车辆运动预测中的灾难性遗忘问题，显著提升预测稳定性并大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 智能城市服务依赖AI，但深度神经网络在车辆运动预测中面临灾难性遗忘问题。传统方法数据收集成本高、样本效率低，且无法平衡长短期经验，无法实现人类般的持续学习。

Method: 提出Dual-LS范式，采用双协同记忆重放机制，加速经验检索并动态协调长短期知识表示，实现任务无关的在线持续学习。

Result: 在三国自然数据测试中（77.2万辆车，11,187公里累计测试里程），Dual-LS将灾难性遗忘减少74.31%，计算资源需求降低94.02%，显著提升预测稳定性且不增加数据需求。

Conclusion: Dual-LS为基于DNN的车辆运动预测提供了计算高效、人类般的持续学习适应性，适合智能城市应用。

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [50] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: RLTR框架通过工具使用完整性奖励信号，解耦训练过程，专注于优化LLM智能体的规划能力，相比端到端方法提升8-12%规划性能，并带来5-6%的最终响应质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多目标优化训练范式存在目标分配不平衡和可验证数据稀缺的问题，难以有效提升智能体的规划能力。

Method: 提出RLTR框架，通过基于工具使用完整性的奖励信号，解耦训练过程，实现规划模块的单目标优化，无需可验证数据。

Result: 实验显示RLTR相比端到端基线方法在规划性能上提升8-12%，规划能力的提升进一步带来整体系统最终响应质量5-6%的提升。

Conclusion: RLTR通过解耦训练和工具使用奖励机制，有效解决了LLM智能体规划能力训练的关键挑战，为智能体系统性能提升提供了新思路。

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [51] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast是首个专门为金融时间序列预测设计的基础模型，通过大规模金融数据集训练，在零样本情况下表现出色，无需领域特定微调即可捕捉多样化模式，超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测对经济稳定、政策制定和可持续投资至关重要，但由于时间非平稳性、多领域多样性和不同时间分辨率等模式变化而具有挑战性。现有深度学习方法容易过拟合且需要大量领域特定微调。

Method: 开发了FinCast基础模型，在大规模金融数据集上进行训练，专门针对金融时间序列预测设计，能够处理时间非平稳性、多领域多样性和不同时间分辨率的问题。

Result: FinCast展现出强大的零样本性能，无需领域特定微调即可有效捕捉多样化模式。全面的实证和定性评估表明，FinCast超越了现有最先进方法。

Conclusion: FinCast作为首个金融时间序列预测基础模型，具有强大的泛化能力，能够有效解决金融时间序列预测中的复杂挑战，为金融预测领域提供了新的解决方案。

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [52] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: ALSA是一种在logit空间中直接操作的新颖框架，通过锚点建模策略来估计模型在未见未标记数据集上的准确性，避免了softmax概率的信息损失和相似性方法的计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖softmax概率或数据相似性度量，但softmax压缩logits会导致信息损失，而相似性方法计算昂贵且领域特定。需要一种能保留更丰富信息、计算高效且广泛适用的准确性估计方法。

Method: ALSA在logit空间中操作，使用多个可学习锚点，每个锚点分配影响函数来捕捉logits的细微变化。基于理论洞察和实证观察，利用logits的聚合和分布与模型预测性能的强相关性。

Result: 在视觉、语言和图基准测试上的广泛实验表明，ALSA优于基于softmax和相似性的基线方法，在显著分布偏移下表现出强大的鲁棒性。

Conclusion: ALSA是一个实用的可靠模型评估工具，通过直接在logit空间中操作并提供准确的性能估计，在各种分布偏移场景下都表现出色。

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [53] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: pFedBayesPT是一个基于视觉提示调优的细粒度实例级个性化联邦学习框架，通过贝叶斯方法处理客户端内部数据异质性，在特征和标签异质性设置下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统个性化联邦学习方法假设每个客户端数据遵循单一分布，但实际中单个客户端可能包含来自多个源或领域的数据，导致显著的客户端内部异质性和次优性能。

Method: 基于视觉提示调优的细粒度实例级pFL框架，从贝叶斯角度制定实例级提示生成，将提示后验建模为隐式分布以捕捉多样化视觉语义，在半隐式变分推理框架下推导变分训练目标。

Result: 在基准数据集上的大量实验表明，pFedBayesPT在特征和标签异质性设置下始终优于现有的pFL方法。

Conclusion: pFedBayesPT有效解决了客户端内部数据异质性挑战，通过实例级个性化方法提升了联邦学习在现实场景中的性能表现。

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [54] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: SCAR是一个用于量化数据集内在结构特性的框架，包含规模、覆盖度、真实性和丰富度四个维度，能够识别保持泛化能力的最小基础数据子集，并指导多模态数据集的高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心化方法主要关注数据数量和训练效率，缺乏对数据质量结构特性的理论理解，特别是在样本缩放时数据特性如何影响泛化能力的研究不足。

Method: 提出SCAR框架量化数据集的四个结构特性，识别基础数据子集，建模单模态任务为阶跃函数，估计基础数据大小分布以捕捉多模态数据集中的泛化偏差，并开发SCAR指导的数据补全策略。

Result: 在多个多模态数据集和模型架构上的实验验证了SCAR在预测数据效用和指导数据采集方面的有效性。

Conclusion: SCAR提供了一个稳定且通用的数据理解基础，能够有效指导多模态数据集的结构化扩展，提升模型泛化能力。

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [55] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 论文证明了适当正则函数可以用有理函数和有理神经网络在C¹范数下近似，并给出了关于网络宽度、深度以及有理函数次数的逼近率。


<details>
  <summary>Details</summary>
Motivation: 研究有理函数和有理神经网络在C¹范数下的逼近能力，特别是在符号回归和物理定律学习中的重要应用。

Method: 使用有理函数和有理神经网络进行函数逼近，分析逼近率与网络宽度、深度以及有理函数次数的关系。

Result: 获得了在C¹范数下的逼近结果，包括EQL÷和ParFam架构的有理神经网络都能实现有效的C¹逼近。

Conclusion: 有理函数和有理神经网络是C¹逼近的有效工具，在符号回归和物理定律学习领域具有重要应用价值。

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [56] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: 该论文研究图上游走（Lipschitz序列）的度量结构，引入加权度量来处理序列，基于逐步顶点距离和加权范数定义游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造，支持度量建模经典工具的应用。


<details>
  <summary>Details</summary>
Motivation: 研究图上游走的度量结构，为分析相对距离的较弱形式（邻近度）提供基础，支持在网络上进行Lipschitz回归和强化学习策略开发。

Method: 引入加权度量处理序列，基于逐步顶点距离和加权范数定义游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造。

Result: 建立了游走的度量框架，支持从游走子空间扩展Lipschitz函数，保持基本性质，为邻近度估计和基于探索游走的强化学习策略提供基础。

Conclusion: 提出的度量框架为图上游走的度量分析提供了理论基础，支持邻近度估计和网络结构上的Lipschitz回归应用。

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [57] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: 提出了Adam-PFN，一种用于Adam超参数优化的预训练代理模型，结合新的学习曲线增强方法CDF-augment，显著提升了学习曲线外推能力和超参数优化效率。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在深度学习中被广泛使用，但其超参数调优过程繁琐且成本高昂。现有的Freeze-thaw贝叶斯优化方法受限于通用代理模型，缺乏对超参数如何影响学习的先验知识。

Method: 开发了Adam-PFN代理模型，在TaskSet的学习曲线上进行预训练，并提出了CDF-augment学习曲线增强方法，通过人工增加训练样本数量来提升性能。

Result: 在TaskSet评估任务上显著改善了学习曲线外推能力并加速了超参数优化，在分布外任务上也表现出强劲性能。

Conclusion: Adam-PFN结合CDF-augment方法为Adam超参数优化提供了有效的解决方案，在保持高性能的同时降低了调优成本。

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [58] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP是一种无需训练的图分区方法，通过负校正机制利用超出常规频率范围的低频信息，结合谱GNN和随机输入，在效率和分区质量上表现出色。


<details>
  <summary>Details</summary>
Motivation: 从图信号处理的角度发现，带负校正的图拉普拉斯矩阵可以产生超出常规范围[0,2]的图频率，探索这些低频信息是否能编码更多社区结构信息。

Method: 采用谱GNN作为主干，结合低通滤波器和负校正机制，仅输入随机信号，通过一次前向传播获得图嵌入，然后使用BIRCH进行聚类。

Result: 在静态和流式图分区任务中，InfraredGP比基线方法快16-23倍，同时保持有竞争力的分区质量。

Conclusion: 负校正机制能够有效放大超出常规频率范围的低频信息，无需训练即可获得高质量的图分区结果，具有显著的计算效率优势。

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [59] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: 提出基于3D扩散模型的生成管道，直接合成任意大小的物理真实颗粒介质装配体，显著加速离散元法模拟的初始化阶段


<details>
  <summary>Details</summary>
Motivation: 离散元法模拟颗粒介质时，初始化阶段计算成本高昂，涉及大位移和动能，占据总模拟时间的主要部分

Method: 两阶段管道：首先训练扩散模型生成独立的3D体素网格；其次采用基于掩码输入的3D修复模型无缝拼接网格，探索多种掩码策略并采用2D重绘技术

Result: 计算时间与样本大小呈线性缩放，1.2米长的道碴轨道合成仅需20秒，相当于3小时的DEM模拟

Conclusion: 该方法能够实现物理一致、实时、可扩展的颗粒介质合成，适用于工业应用，生成的体素网格可后处理提取颗粒几何形状以兼容DEM

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [60] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: EUREKA框架通过选择有趣而非最准确的特征来构建分类器，利用大语言模型评估特征有趣度，生成可解释且新颖的模型


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型追求预测准确性，但本文探索构建"有趣分类器"的目标，即使用不寻常或意外的特征，即使准确性低于最佳模型，以支持知识发现和沟通

Method: 提出EUREKA框架：1）使用大语言模型根据感知有趣度对特征进行排序 2）仅使用选定的有趣特征构建可解释分类器

Result: 在多个基准数据集上，EUREKA始终识别出非显而易见但仍具有预测性的特征。例如：在Occupancy Detection数据集中选择湿度而非CO2水平；在Twin Papers数据集中发现标题含冒号的论文更可能被引用

Conclusion: 这种模型支持新的知识发现和沟通方式，特别适用于中等准确性足够但新颖性和可解释性受到重视的场景

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [61] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: PSO-Merging是一种基于粒子群优化的数据驱动模型融合方法，通过初始化粒子群并进行多轮迭代，有效解决了现有方法在性能和效率方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法存在性能限制：数据无关方法缺乏数据指导，梯度方法计算成本高，无梯度方法优化步数有限效果不佳。需要一种更高效可扩展的融合方案。

Method: 基于粒子群优化(PSO)算法，使用预训练模型、专家模型和稀疏化专家模型初始化粒子群，通过多轮迭代优化，最终选择全局最优粒子作为融合模型。

Result: 在不同语言模型上的实验表明，PSO-Merging普遍优于基线融合方法，提供了更高效和可扩展的解决方案。

Conclusion: PSO-Merging通过粒子群优化算法成功解决了现有模型融合方法的局限性，在保持高性能的同时显著提升了计算效率和可扩展性。

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [62] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 提出了一种新的辛卷积神经网络架构，结合辛神经网络、适当辛分解和张量技术，确保卷积层保持辛结构，并在波动方程、非线性薛定谔方程和正弦-戈登方程上表现优于线性辛自编码器。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络缺乏物理约束，无法保持哈密顿系统的辛结构。为了在深度学习模型中保持物理系统的固有几何特性，需要开发能够保持辛结构的卷积神经网络架构。

Method: 首先引入卷积层的数学等价形式，然后使用辛神经网络参数化CNN层以确保卷积层保持辛性。构建完整的自编码器时引入了辛池化层。通过张量技术和适当辛分解来实现架构设计。

Result: 在波动方程、非线性薛定谔方程和正弦-戈登方程三个示例上的数值结果表明，所提出的辛CNN在性能上优于通过适当辛分解获得的线性辛自编码器。

Conclusion: 成功开发了一种保持辛结构的卷积神经网络架构，该架构能够有效处理物理系统的学习问题，并在多个基准问题上展现出优于传统方法的性能。

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [63] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: 提出混合有限元方法与物理信息DeepONet的框架，用于多孔介质中流体输运建模，实现高精度流场计算和快速输运动力学推理


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理尖锐高斯源引起的陡峭梯度时计算效率低下的问题，同时保持流场计算的高精度

Method: 使用FEM求解达西流方程获得速度场，然后通过物理信息DeepONet学习从源函数到溶质浓度分布的映射，并引入自适应采样策略处理陡峭梯度

Result: 数值实验表明该方法与参考解吻合良好，相比传统求解器实现数量级的速度提升

Conclusion: 该混合框架在保持精度的同时显著提升计算效率，适用于实际多孔介质流体输运建模应用

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [64] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: 这篇论文研究了量子潜在分布在生成模型中的优势，证明在某些条件下量子潜在分布能够生成经典方法无法高效生成的数据分布，并通过实验验证了量子潜在分布在GAN模型中的性能提升效果。


<details>
  <summary>Details</summary>
Motivation: 虽然简单的经典潜在分布被广泛使用，但更复杂的分布能够提高生成模型的性能。最近的研究发现量子处理器产生的分布能够带来实验性的改善，但对于何时以及如何重现这些改善仍是开放性问题。

Method: 论文证明了在某些条件下，量子潜在分布能够让生成模型产生经典潜在分布无法高效生成的数据分布。研究还提供了可操作的直观见解来识别量子优势可能出现的实际场景。在合成量子数据集和QM9分子数据集上进行了基准测试实验，使用了模拟和真实的光子量子处理器。

Result: 实验结果显示，与一系列经典基准比较，量子潜在分布能够在GAN模型中导致生成性能的提升。研究还探索了液体模型和流匹配模型，识别了与量子潜在分布相兼容的架构。

Conclusion: 这项工作确认了近期量子处理器能够扩展深度生成模型的能力，量子潜在分布在某些条件下具有经典方法无法模拟的优势，为量子生成模型的实际应用提供了理论基础和实验支撑。

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [65] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: 基于结构多样性的无参数图神经网络SDGNN框架，通过结构驱动和特征驱动的补充建模，在不依赖训练参数的情况下显著提升了图表征学习的适配性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN方法依赖大量可训练参数和固定聚合规则，难以适应具有强结构异质性和复杂特征分布的图数据，导致节点表征过度平滑和语义退化。

Method: 受结构多样性理论启发，设计了统一的结构多样性消息传递机制，同时捐捕邻域结构的异质性和特征语义的稳定性，无需引入额外可训练参数。

Result: 在8个公共标准数据集和一个跨学科PubMed引用网络上，SDGNN在低监督、类不平衡和跨域转移等挑战性条件下一贵超过主流GNN方法。

Conclusion: 该工作为无参数图神经网络设计提供了新的理论视角和通用方法，进一步验证了结构多样性作为图表征学习核心信号的重要性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [66] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: 提出NM-Hebb两阶段训练框架，结合神经启发的局部可塑性和距离感知监督，在多个数据集和骨干网络上显著提升CNN准确率和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统CNN依赖全局梯度优化，容易导致过拟合、冗余滤波器和可解释性降低的问题，需要更生物启发的训练方法

Method: 两阶段训练：阶段1在交叉熵损失基础上加入Hebbian正则器和可学习神经调节器；阶段2使用成对度量学习损失进行微调，压缩类内距离扩大类间间隔

Result: 在CIFAR-10、CIFAR-100和TinyImageNet上，Top-1准确率提升2.0-10.0个百分点，NMI提升最高0.15，产生更结构化和选择性的特征

Conclusion: 结合局部Hebbian可塑性和度量学习微调，不仅提高CNN准确性，还增强可解释性，对资源受限和安全关键AI部署具有实际价值

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [67] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC提出了一种自适应策略约束缩放框架，通过二阶可微分方法动态平衡强化学习和行为克隆，无需针对不同数据集进行超参数调优，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法需要针对不同任务和数据集质量精心调整策略约束的超参数，这既耗时又不实用，需要一种自适应的方法来解决这个问题。

Method: 提出了自适应策略约束缩放（ASPC）框架，这是一个二阶可微分的方法，能够在训练过程中动态平衡强化学习和行为克隆，避免繁琐的超参数调优。

Result: 在4个D4RL领域的39个数据集上，ASPC使用单一超参数配置就超越了其他自适应约束方法和需要逐数据集调优的最先进离线RL算法，且计算开销极小。

Conclusion: ASPC提供了一个有效且实用的解决方案，能够自动适应不同数据集，显著减少了离线强化学习中的超参数调优需求，同时保持了优异的性能表现。

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [68] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: GegenNet是一个用于符号二分图链接符号预测的新型谱卷积神经网络模型，通过Gegenbauer多项式基滤波器实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单分图符号预测，忽略了二分图的节点异质性和独特特征，且传统谱卷积算子最初为无符号图设计，不适用于从已知链接推断缺失正负链接

Method: 提出三个技术贡献：1) 快速理论基础的谱分解节点特征初始化；2) 基于Gegenbauer多项式基的新谱图滤波器；3) 交替处理正负边的多层符号感知谱卷积网络

Result: 在6个基准SBG数据集上，相比11个强竞争方法，GegenNet在AUC指标上提升达4.28%，在F1指标上提升达11.69%

Conclusion: GegenNet通过创新的谱卷积架构和Gegenbauer多项式滤波器，显著提升了符号二分图链接符号预测的性能，证明了其在处理节点异质性和符号推断方面的有效性

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [69] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: 提出基于UMLS本体驱动的放射学报告检索方法，通过标准化医学实体提取和语义相似度计算，在长尾医疗影像任务中优于现有嵌入方法


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP或CXR-BERT等高维文本嵌入的方法难以解释、计算昂贵，且与医学知识结构化特性不匹配，需要更透明、可解释的检索策略

Method: 使用RadGraph-XL和SapBERT增强管道从自由文本报告中提取标准化医学实体，链接到UMLS概念，基于改进的加权Tversky指数定义任务自适应相似度度量

Result: 在MIMIC-CXR放射影像分类任务中优于最先进的基于嵌入的检索方法，特别是在长尾设置下，并为MIMIC-CXR生成本体支持的疾病标签

Conclusion: 该方法为临床AI系统提供了更可解释、可靠和任务特定的检索策略，特别是在需要可解释性和领域知识整合的场景中

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [70] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer是一个基于BERT的预训练模型，专门用于网络流量分析，通过创新的流量表示方法和预训练任务，显著提升了流量分类准确性和少样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量分类方法难以有效捕捉数据包结构特征、流级行为、分层协议语义和包间上下文关系，需要设计专门的预训练模型来解决这些挑战。

Method: 提出了FlowletFormer模型，包含：1）连贯行为感知流量表示模型，将流量分割为语义单元；2）协议栈对齐嵌入层，捕获多层协议语义；3）字段特定和上下文感知预训练任务，增强包间和流间学习。

Result: 实验结果表明，FlowletFormer在流量表示效果、分类准确性和少样本学习能力方面显著优于现有方法，并能更好地理解网络传输原理（如TCP状态连接）。

Conclusion: FlowletFormer通过有效整合领域特定的网络知识，为流量分析提供了一个更鲁棒和可信赖的框架，在多个关键指标上表现出色。

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [71] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: 提出基于逆动态博弈的算法，从多智能体局部广义纳什均衡交互数据中学习参数化约束，通过MILP编码KKT条件恢复与纳什平稳性一致的约束，具有理论保证并能用于设计满足约束的鲁棒运动规划


<details>
  <summary>Details</summary>
Motivation: 从多智能体交互演示中学习约束条件对于理解智能体行为和安全交互至关重要，现有方法难以从纳什均衡交互中有效恢复约束

Method: 使用混合整数线性规划(MILP)编码交互智能体的KKT条件，通过逆动态博弈框架恢复与纳什平稳性一致的参数化约束

Result: 方法能够学习真实安全集和非安全集的内近似，在仿真和硬件实验中成功推断出凸和非凸约束，并设计出满足底层约束的交互运动规划

Conclusion: 该方法为从纳什均衡交互演示中学习约束提供了有效框架，具有理论保证和实际应用价值，适用于非线性动力学的多智能体系统

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [72] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: 提出了全局排列熵(GPE)，一种新的复杂度指标，不仅考虑连续段落的排列模式，还包含所有可能长度的非连续模式，通过高效算法提取完整排列分布，在合成数据集上展示了比标准排列熵更好的结构信息提取能力。


<details>
  <summary>Details</summary>
Motivation: 标准排列熵只基于连续段落的相对顺序模式，可能遗漏重要的非连续结构信息，需要一种能够捕捉更全面排列模式的复杂度度量方法。

Method: 开发全局排列熵(GPE)指标，利用新算法高效提取所有可能长度的排列分布（包括非连续模式），然后应用香农熵进行复杂度量化。

Result: 在合成数据集上的实验表明，GPE能够揭示标准排列熵无法获取的结构信息，提供了更全面的复杂度分析。

Conclusion: 全局排列熵(GPE)是对传统排列熵的重要扩展，能够更全面地捕捉时间序列的复杂度特征，为复杂系统分析提供了新的工具。

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [73] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 基于机器学习的工业离心泵短期故障预测框架，使用随机森林和XGBoost模型，通过60分钟和120分钟滑动窗口提取统计特征，在不同时间尺度上实现故障预警。


<details>
  <summary>Details</summary>
Motivation: 工业离心泵的实时故障预测对于预防性维护至关重要，需要开发能够提前预警的预测模型来减少设备停机时间和维护成本。

Method: 使用滑动窗口方法（60分钟和120分钟）提取统计特征（均值、标准差、最小值、最大值、线性趋势），采用SMOTE算法处理类别不平衡，训练随机森林和XGBoost分类器进行故障预测。

Result: 随机森林模型在60分钟窗口下表现最佳：5分钟前预警召回率69.2%，15分钟前64.9%，30分钟前48.6%。120分钟窗口下，15和30分钟前预警召回率达到65.6%。

Conclusion: 预测性能受历史数据长度和预测时间尺度影响，不同故障模式在不同时间尺度演化。该方法为工业监控系统提供了可解释且可扩展的预测性维护解决方案。

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [74] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: 通过数据驱动模拟分析了不同停车搜索策略的效果，提出了一种基于概率估计的协调停车策略(Cord-Approx)，在马德里实际模拟中将用户寻找停车时间从19.98分钟降低到6.69分钟，减少72-73%。


<details>
  <summary>Details</summary>
Motivation: 在密集城市区域，路边停车搜索加剧了交通拕塞。虽然基于手机的实时助手已经被提出，但其效果很少被定量研究。

Method: 通过数据驱动的马德里路边停车生态系统模拟，分析四种策略：无协调搜索(Unc-Agn)、协调停车但不知道非用户信息(Cord-Agn)、理想化神谕系统(Cord-Oracle)和新颖的Cord-Approx策略。Cord-Approx使用过去占用分布概率性估计非用户行为，通过拉长系统用户与备选停车位之间的距离，并解决匈牙利匹配问题来调度。

Result: 在高保真度的马德里停车网络模拟中，Cord-Approx用户平均只需6.69分钟找到停车位，而无应用程序的非用户需19.98分钟。区域级判断显示，Cord-Approx在中央植心区域将系统用户的搜索时间减少72%(67-76%)，在住宅区域可达73%。

Conclusion: 这种基于概率估计的协调停车策略(Cord-Approx)实际可行且高效，显著减少了城市停车搜索时间，有效缓解交通拕塞问题。

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [75] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: 语言模型在密码验证任务中表现不佳，推理能力反而会泄露机密信息，当前前沿模型不适合处理机密信息


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在关键场景中作为自主代理部署，确保其可靠遵循用户定义规则已成为关键安全问题，需要研究模型是否具备上下文鲁棒性

Method: 开发PasswordEval基准测试，测量语言模型在密码验证任务中的表现，通过对抗性用户压力和长对话增加测试难度

Result: 当前开源和闭源模型在这个看似简单的任务中表现不佳，推理能力通常不会改善性能，反而经常泄露机密信息

Conclusion: 当前前沿模型不适合处理机密信息，推理能力可能需要以不同方式训练才能在高风险环境中安全部署

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [76] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出一种新的自监督预训练方法，通过双层优化和平衡约束来处理异构数据，提高模型在下游任务中的适应性


<details>
  <summary>Details</summary>
Motivation: 传统自监督预训练方法将所有异构数据混合并最小化全局平均损失，无法确保模型对每个异构数据源都达到局部最优，限制了模型在下游任务中的适应性

Method: 采用双层优化框架，在K步梯度下降后对每个异构数据源施加平衡约束，确保模型达到局部最优。使用一阶近似方法求解该优化问题，并与模型无关元学习(MAML)建立联系

Result: 在多领域和多语言数据集上的实验表明，该方法能显著提高自监督预训练模型在下游监督微调任务中的适应性

Conclusion: 提出的平衡约束和双层优化方法有效解决了异构数据自监督预训练问题，为处理复杂数据场景提供了新思路

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [77] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出基于梯度估计的演示样本选择算法，用于上下文学习中的示例选择，相比传统方法效率提升37.7倍，性能提升11%


<details>
  <summary>Details</summary>
Motivation: 解决在上下文学习中如何从n个示例中快速选择k个最佳演示样本的问题，该问题在提示调优和思维链推理中有广泛应用

Method: 基于输出在输入嵌入空间中的梯度，通过一阶近似估计模型输出，对多个随机采样子集应用该估计，聚合结果形成影响分数，选择最相关的k个示例

Result: 梯度估计程序在六个数据集上的近似误差小于1%，选择速度比完整推理快37.7倍（340亿参数模型），性能比基于输入嵌入的方法平均提升11%

Conclusion: 提出的梯度估计方法能够高效准确地选择演示样本，为大规模模型的上下文学习提供了实用的解决方案

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [78] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: 这项研究发展了一种多模态层次分类框架，解决电子商务产品分类中的平台异质性和分类结构问题，通过结合文本、图像和视觉-语言表征，并在商业平台中部署。


<details>
  <summary>Details</summary>
Motivation: 解决电子商务产品分类中的两大挑战：不同平台的异质性和现有分类系统的结构限制，提高分类的准确性和可扩展性。

Method: 使用40个国际时尚电商平台的271,700个产品数据，整合RoBERTa文本特征、ViT视觉特征和CLIP视觉-语言表征。采用早期融合、晚期融合和注意力融合策略，并使用动态遮置保证分类一致性。还提出了自监督的产品重新分类流程。

Result: CLIP嵌入通过MLP晚期融合策略获得最高层次F1分数(98.59%)，超越单模态基线。自监督方法发现了精细化的新分类(如鞋子子类)，聚类纯度超过86%。跨平台实验显示了准确性与泛化能力的担杀。

Conclusion: 该多模态层次分类框架能够有效解决市场异质性问题，并通过两阶段推理流水线实现了商业部署的可扩展性，在保持高准确性的同时控制成本。

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [79] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: 该研究开发了一个检测和量化微调过程中快速行为转变的框架，通过统计差异测量和基于语言的有序参数来分析LLM在有害数据集上微调时出现的广泛不对齐现象。


<details>
  <summary>Details</summary>
Motivation: 理解在狭窄有害数据集上微调LLM时，何时以及如何出现与人类价值观广泛不对齐的突发行为，需要系统性的检测和量化方法。

Method: 结合分布变化检测方法和基于自然语言的有序参数（由LLM评估），使用客观统计差异度量来量化微调过程中的相变对模型多方面的影响。

Result: 发现行为转变实际发生在训练后期，比梯度范数峰值指示的时间更晚；能够分解总体转变中不同方面（如对齐度、冗长度）的贡献比例；实现了基于语言的有序参数的自动发现和量化。

Conclusion: 该框架能够自动检测和量化微调过程中的关键行为转变，为理解LLM在有害数据微调时的不对齐现象提供了系统性的分析工具，适用于从知识问答到政治伦理等多个领域。

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [80] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony是一个去中心化的多智能体系统，通过分布式账本、信标选择协议和加权投票机制，使轻量级LLM能够在消费级GPU上协同工作，解决了集中式编排的高成本、通信拓扑僵化和适应性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体框架主要采用集中式编排，存在部署成本高、通信拓扑结构僵化、适应性有限等问题，需要一种更高效、灵活的分布式解决方案。

Method: 提出Symphony系统，包含三个核心机制：1）记录能力的分布式账本；2）动态任务分配的信标选择协议；3）基于思维链的加权结果投票机制。

Result: 在推理基准测试中优于现有基线方法，实现了显著的准确率提升，并在不同容量模型间展现出良好的鲁棒性。

Conclusion: Symphony提供了一种隐私保护、可扩展、容错且低开销的去中心化编排方案，使轻量级LLM能够在消费级硬件上有效协作。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [81] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop是一个用于神经网络预测模型中人为引导偏差缓解的工具，通过从神经网络提取决策树让用户检查和修改不公平决策逻辑，然后微调原始模型实现更公平的预测。


<details>
  <summary>Details</summary>
Motivation: 敏感属性（如性别、年龄）在机器学习任务中可能导致不公平预测，特别是在不考虑上下文的情况下使用时。现有方法通常统一排除敏感属性，但需要选择性处理敏感属性的影响。

Method: FairLoop从神经网络中提取决策树，允许用户检查和修改不公平的决策逻辑，然后使用修改后的逻辑对原始模型进行微调，实现上下文感知的偏差消除。

Result: 相比其他公平性方法，FairLoop通过人为参与实现上下文感知的偏差消除，能够选择性地处理敏感属性的影响，而不是统一排除它们。

Conclusion: FairLoop提供了一种人类引导的偏差缓解方法，通过决策树可视化和修改机制，使神经网络预测模型能够实现更公平和上下文感知的预测结果。

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [82] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: 使用大型语言模型为个性化营销邮件生成主题标题，通过离线和在线实验验证能有效提升用户参与度


<details>
  <summary>Details</summary>
Motivation: 传统营销邮件的标题模板固定，无法充分激发用户对个性化内容的兴趣，需要更吸引人的标题来提升用户参与

Method: 利用大型语言模型(LLMs)生成反映邮件个性化内容的主题标题，进行离线模拟和百万级用户的在线实验

Result: 技术有效改善了客户与邮件之间的互动参与度，成功实现了为百万用户安全自动化生成邮件标题

Conclusion: LLMs在生成个性化营销邮件标题方面具有显著潜力，能够安全地规模化应用于实际业务场景

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [83] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文研究了通过注意力头剪枝来防御预训练语言模型中的后门攻击，提出了六种剪枝策略，实验表明梯度剪枝对语法触发攻击效果最佳，强化学习和贝叶斯剪枝对风格攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对预训练语言模型的性能和完整性构成严重威胁，即使经过微调仍然存在漏洞。由于用户通常不了解攻击触发器，传统检测方法难以防御，因此需要事后净化方法来消除这些威胁。

Method: 设计了六种剪枝策略：梯度剪枝、层间方差剪枝、结构化L1/L2稀疏化梯度剪枝、随机集成剪枝、强化学习引导剪枝和贝叶斯不确定性剪枝。每种方法都迭代移除信息量最小的注意力头，同时监控验证准确率以避免过度剪枝。

Result: 实验评估显示，梯度剪枝在防御语法触发攻击方面表现最佳，而强化学习和贝叶斯剪枝在抵御风格攻击方面效果更好。

Conclusion: 注意力头剪枝是一种有效的后门攻击防御方法，无需了解攻击触发器或访问干净参考模型。不同剪枝策略适用于不同类型的攻击，为后门防御提供了新的解决方案。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [84] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: 本文通过将多臂老虎机强化学习算法应用于Failure-Directed Search，在调度问题上实现了显著的性能提升，相比原始算法和业界最优算法都有显著加速。


<details>
  <summary>Details</summary>
Motivation: 分析Failure-Directed Search算法的特性，发现其搜索树最小化与多臂老虎机问题密切相关，希望通过强化学习技术来优化这一搜索过程。

Method: 将多臂老虎机强化学习算法应用于FDS，结合问题特定的改进和参数调优，在作业车间调度和资源约束项目调度问题上进行验证。

Result: 增强版FDS在JSSP上快1.7倍，在RCPSP上快2.1倍；相比IBM CP Optimizer 22.1的最优算法，在JSSP上快3.5倍，在RCPSP上快2.1倍。在900秒时间限制下，改进了大量基准实例的下界。

Conclusion: 多臂老虎机强化学习算法能有效提升Failure-Directed Search的性能，在调度问题上取得了显著的加速效果和更好的求解质量。

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>
