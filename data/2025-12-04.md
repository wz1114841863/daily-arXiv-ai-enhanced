<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Accelerating Detailed Routing Convergence through Offline Reinforcement Learning](https://arxiv.org/abs/2512.03594)
*Afsara Khan,Austin Rovinski*

Main category: cs.AR

TL;DR: 提出使用强化学习动态调整布线成本权重，相比传统静态调度方法，在ISPD19基准测试中实现平均1.56倍、最高3.01倍的加速，同时保持或减少设计规则违规数量。


<details>
  <summary>Details</summary>
Motivation: 详细布线是现代物理设计中最复杂耗时的步骤之一。传统详细布线器使用静态调度成本权重，无法根据具体设计或技术进行调整，导致收敛到零设计规则违规的运行时成本过高。

Method: 采用保守Q学习（CQL）模型，学习动态选择布线成本权重以最小化算法迭代次数。通过从先前设计中学习，使路由器能够根据具体设计情况自适应调整成本权重。

Result: 在ISPD19基准测试中，相比基线布线器实现平均1.56倍、最高3.01倍的运行时加速，所有情况下保持或改善了设计规则违规数量。学习还显示出跨技术泛化能力，在一个技术中学到的知识可提升其他技术的布线效果。

Conclusion: 强化学习能够有效加速详细布线收敛过程，通过动态成本权重调整显著减少运行时，同时保持布线质量。该方法具有跨技术泛化潜力，为物理设计优化提供了新思路。

Abstract: Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.
  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.

</details>


### [2] [KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing](https://arxiv.org/abs/2512.03608)
*Lishuo Deng,Shaojie Xu,Jinwu Chen,Changwei Yan,Jiajie Wang,Zhe Jiang,Weiwei Shan*

Main category: cs.AR

TL;DR: KVNAND：首个基于闪存内计算（IFC）的无DRAM架构，将模型权重和KV缓存完全存储在3D NAND闪存中，解决了长上下文推理中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型面临内存瓶颈：传统DRAM方案中，随着上下文长度增加，KV缓存会超过模型权重大小，导致DRAM成本过高且容量不足；而将KV缓存卸载到闪存又面临严重的性能惩罚。

Method: 1. 提出KVNAND架构，完全在支持计算的3D NAND闪存中存储模型权重和KV缓存；2. 利用IFC减少数据传输开销；3. 引入头组并行性提升吞吐量；4. 采用页面级KV缓存映射，使令牌访问模式与闪存组织对齐；5. 设计空间探索框架评估不同KVNAND变体，自动找到最优设计权衡。

Result: 在MHA 7B和GQA 70B LLM上的评估显示：在128/1K/10K令牌上下文长度下，相比配备DRAM的IFC设计，KVNAND分别获得1.98×/1.94×/2.05×的几何平均加速比，并在100K上下文长度下解决了内存不足故障。

Conclusion: KVNAND通过将闪存转变为长上下文KV存储的实用介质，有效缓解了延迟、能耗和可靠性问题，为边缘设备上的大语言模型部署提供了可行的内存解决方案。

Abstract: Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.
  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.

</details>


### [3] [Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State](https://arxiv.org/abs/2512.03616)
*Christian Ewert,Amrit Sharma Poudel,Mouadh Ayache,Andrija Neskovic,Rainer Buchty,Mladen Berekovic,Sebastian Berndt,Saleh Mulhem*

Main category: cs.AR

TL;DR: 提出统一哈希引擎支持Sha-3和Shake，采用字节级原地分区Keccak状态机制，并基于立方体结构部署二维奇偶校验实现故障检测，在保持竞争性故障检测能力的同时显著降低面积开销。


<details>
  <summary>Details</summary>
Motivation: 哈希函数已成为后量子密码学方案的关键部分，需要轻量级实现。故障弹性设计对于确保整个PQC系统的可靠性至关重要，特别是在资源受限的应用中。

Method: 1) 提出统一哈希引擎，支持Sha-3和Shake，采用字节级原地分区Keccak状态机制；2) 基于Keccak立方体结构部署二维奇偶校验进行故障检测；3) 在ASIC和FPGA上实现，并集成到RISC-V环境中。

Result: 实现100%的三故障检测和接近100%的高数量故障检测；相比现有方案，面积开销减少3.7倍，整体故障弹性引擎设计缩小4.5倍；集成到RISC-V环境中仅增加不到8%的面积开销。

Conclusion: 该方法为资源受限的PQC应用提供了鲁棒且轻量级的故障检测解决方案，覆盖所有标准哈希配置，在保持高故障检测率的同时显著降低了硬件开销。

Abstract: Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.

</details>


### [4] [The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates](https://arxiv.org/abs/2512.03781)
*Joscha Ilmberger,Johannes Schemmel*

Main category: cs.AR

TL;DR: BrainScaleS-2 SoC系统通过FPGA互连扩展计算规模，使用Aggregator单元连接多个Node-FPGA，在标准机架中实现低延迟芯片间通信


<details>
  <summary>Details</summary>
Motivation: 扩展BrainScaleS-2神经形态计算系统的规模，通过FPGA互连实现多芯片协同工作，构建更大规模的神经形态计算平台

Method: 使用基于FPGA的Aggregator单元，提供12个收发器链路连接Node-FPGA背板，4个收发器通道用于扩展，将两个互连背板集成到4U标准机架中

Result: 在所有脉冲速率下，每个背板内的芯片间延迟（经过三个FPGA的四跳）低于1.3微秒，成功构建了可扩展的神经形态计算系统

Conclusion: FPGA互连架构有效扩展了BrainScaleS-2系统的计算规模，实现了低延迟的芯片间通信，为大规模神经形态计算提供了可行的硬件平台

Abstract: The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$μ$s are achieved within each backplane.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: 本研究比较了智能加工中常用AI模型的能耗、精度和速度，引入超维计算作为替代方案，在保持精度的同时大幅降低能耗（训练200倍，推理175-1000倍）并提升速度（训练200倍，推理300-600倍）。


<details>
  <summary>Details</summary>
Motivation: 智能制造虽能提高效率和降低能耗，但AI模型的高能耗可能抵消这些优势。需要寻找既能保持精度又能显著降低能耗的AI解决方案。

Method: 采用基于原位传感的几何质量预测方法，比较常用AI模型的能耗、精度和速度。引入超维计算作为替代方案，并与传统模型进行对比分析。

Result: 超维计算在精度上与传统模型相当，但能耗显著降低：训练能耗降低200倍，推理能耗降低175-1000倍。同时训练时间减少200倍，推理时间减少300-600倍。

Conclusion: 超维计算是实现节能智能制造的有前景的替代方案，能在保持精度的同时大幅降低能耗和计算时间，有助于解决AI模型能耗抵消智能制造优势的问题。

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [6] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 开发了一个结合物理知识与机器学习的计算框架，用于预测钢的连续冷却转变（CCT）图，该模型在4100个图上训练，能在5秒内生成完整CCT图，相分类F1分数超过88%，相变温度回归MAE低于20°C。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习在材料科学中已用于新化合物发现和制造过程优化，但将其应用于钢铁等复杂工业材料仍具挑战性。主要障碍在于难以准确捕捉化学成分、工艺参数与微观结构/性能之间的复杂关系。

Method: 引入了一个结合物理洞察与机器学习的计算框架，开发了物理信息化的连续冷却转变（CCT）模型。该模型在4100个CCT图的数据集上训练，将物理原理与机器学习相结合来预测钢的相变行为。

Result: 模型计算效率高，生成包含100条冷却曲线的完整CCT图仅需不到5秒。在合金钢中表现出强泛化能力：所有相的相分类F1分数均超过88%；除贝氏体（MAE 27°C）外，所有相的相变温度回归平均绝对误差均低于20°C。

Conclusion: 该框架可扩展为通用数字孪生平台，通过集成补充模拟工具和针对性实验，进一步支持加速材料设计工作流程。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [7] [Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation](https://arxiv.org/abs/2512.03053)
*Andrew S. Cassidy,Guillaume Garreau,Jay Sivagnaname,Mike Grassi,Bernard Brezzo,John V. Arthur,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 论文提出一种利用LLM进行无损编码解码的方法，用于可逆问题（如LCT到HDL转换），通过双向验证减少LLM的幻觉和遗漏问题，显著提高硬件设计生产力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码生成任务中的幻觉和遗漏问题，特别是在硬件设计领域（如从逻辑条件表生成HDL代码），需要一种可靠的方法来验证LLM生成的正确性。

Method: 采用信息论中无损压缩的思路：1）使用LLM作为无损编码器从源域（LCT）到目标域（HDL）；2）再用LLM作为无损解码器从HDL回译到LCT；3）比较原始LCT和重建LCT来验证正确性。

Result: 使用7种不同LLM生成了二维网络芯片路由器的完整HDL代码（13个单元，1500-2000行代码），通过双向验证方法：1）确认正确生成的逻辑；2）检测错误生成的逻辑；3）帮助开发者发现设计规范错误。

Conclusion: 提出的无损编码解码方法能有效缓解LLM的幻觉和遗漏问题，显著提高硬件设计生产力，不仅验证LLM生成逻辑的正确性，还能辅助发现设计规范错误。

Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.

</details>


### [8] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: 提出一种面向绿色AI的自适应层冻结策略，用于联邦学习中的MRI-to-CT转换任务，在保持模型性能的同时减少23%的训练时间、能耗和碳排放。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能促进医疗平等，但其高资源需求会排除计算基础设施有限的机构，加剧医疗不平等。需要降低联邦学习的资源消耗，使其更具可持续性和包容性。

Method: 提出自适应层冻结策略：基于轮次间编码器权重的相对差异监控，选择性冻结编码器权重；采用基于耐心的机制，仅在更新持续最小时才执行冻结；使用CodeCarbon库跟踪能耗和碳排放。

Result: 相比未冻结的对照方法，训练时间、总能耗和CO2eq排放减少高达23%；MRI-to-CT转换性能基本保持，MAE仅有小幅变化；5种架构中3种无显著差异，2种有显著改进。

Conclusion: 该工作为满足临床需求同时确保气候、社会和经济可持续性的DL框架提供了新范式，为推进隐私、公平和AI驱动医疗中的正义奠定了基础。

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [9] [Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins](https://arxiv.org/abs/2512.03055)
*Xiaowu Sun,Thabo Mahendiran,Ortal Senouf,Denise Auberson,Bernard De Bruyne,Stephane Fournier,Olivier Muller,Pascal Frossard,Emmanuel Abbe,Dorina Thanou*

Main category: cs.LG

TL;DR: PINS-CAD：基于物理信息自监督学习的框架，通过预训练图神经网络预测冠状动脉血流动力学，无需计算流体力学或标记数据，在临床数据上微调后能预测心血管事件。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，冠状动脉疾病是最常见形式，需要早期风险预测。传统3D冠状动脉数字孪生分析依赖计算密集的计算流体力学，可扩展性有限；数据驱动方法受限于标记数据稀缺和缺乏生理学先验知识。

Method: 提出PINS-CAD物理信息自监督学习框架：预训练图神经网络于20万个合成冠状动脉数字孪生，基于1D Navier-Stokes方程和压降定律预测压力和血流，无需CFD或标记数据；在635名患者的多中心FAME2研究临床数据上进行微调。

Result: 预测未来心血管事件的AUC为0.73，优于临床风险评分和数据驱动基线；生成空间分辨的压力和血流储备分数曲线，提供可解释的生物标志物；物理信息预训练提高了样本效率并产生生理学有意义的表示。

Conclusion: 通过将物理先验嵌入几何深度学习，PINS-CAD将常规血管造影转化为无需模拟、具有生理感知的框架，实现可扩展的预防性心脏病学。

Abstract: Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.

</details>


### [10] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: Delta Sampling (DS) 是一种在推理时实现不同架构基础模型间知识迁移的方法，无需原始训练数据，通过利用模型预测差异来指导新基础模型的去噪过程。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生态系统中的适配组件（如LoRA、LyCORIS、ControlNet）与特定基础模型紧密耦合，当基础模型升级时难以重用，因为模型参数和架构发生了重大变化。

Method: 提出Delta Sampling方法，在推理时利用"delta"（基础模型适配前后的预测差异）来指导新基础模型的去噪过程，实现跨架构的知识迁移。

Result: 在不同Stable Diffusion版本上评估，DS在各种采样策略下都能一致地改善期望效果（视觉风格、语义概念和结构）的创建。

Conclusion: DS是一种有效、即插即用的知识迁移机制，可用于基于扩散的图像合成，解决了适配组件与基础模型紧密耦合的问题。

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [11] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR: 该论文研究了预训练Transformer模型中token的动态特性，分析了连续时间极限下的动力系统，提出了改善Transformer架构的简单改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究预训练Transformer模型中token的动态行为，理解不同参数设置下token如何相互靠近或远离，探索这些动态特性对模型性能的影响，为改进Transformer架构提供理论基础。

Method: 分析预训练模型的连续时间极限动力系统，表征解的渐近行为，基于模型参数提供token收敛到零或发散到无穷的充分条件，研究绝对位置编码和旋转位置编码对动态机制的影响。

Result: 提出了比先前工作更广泛、更适用于实际模型的充分条件，发现收敛场景对模型性能有负面影响，针对绝对和旋转位置编码提出了能缓解收敛行为的Transformer架构改进方法。

Conclusion: 该研究为改进Transformer模型提供了理论基础和设计原则，通过理解token动态特性并相应调整架构，可以提升模型性能。

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [12] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出了一种基于分层深度强化学习的安全框架，用于解决电动公交车充电调度问题，该框架在多源不确定性下优化运营成本并确保电池安全。


<details>
  <summary>Details</summary>
Motivation: 电动公交车与光伏等可再生能源结合是实现低碳公共交通的有前景方案，但在实际运营中面临光伏发电不确定性、动态电价、可变行驶时间、充电设施有限等多重挑战，需要优化充电调度以最小化运营成本并确保电池安全。

Method: 将问题建模为带约束的马尔可夫决策过程，提出新型分层深度强化学习算法DAC-MAPPO-Lagrangian，高层采用集中式PPO-Lagrangian学习安全充电桩分配策略，底层采用MAPPO-Lagrangian在CTDE范式下学习分散式充电功率决策。

Result: 基于真实数据的实验表明，该方法在成本最小化和安全合规性方面优于现有基线方法，同时保持了较快的收敛速度。

Conclusion: 提出的安全分层深度强化学习框架能有效解决电动公交车充电调度问题，在多源不确定性下实现成本优化和安全运营，为可持续公共交通提供了可行的技术方案。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [13] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 提出大规模工业框架，利用数亿Snapchat用户的实验数据估计异质性处理效应，通过整合多个实验发现潜在用户特征，并实现稳定的处理效应估计。


<details>
  <summary>Details</summary>
Motivation: 传统处理效应模型假设所有用户对处理的反应相同，但现实中用户对广告等处理的反应存在异质性。需要大规模框架来估计异质性处理效应，以更精准地理解用户对不同干预的反应差异。

Method: 构建大规模工业框架，包括：1) 实验选择策略；2) 基础学习器设计；3) 增量训练方法。通过整合数百个实验的数据，发现潜在用户特征并稳定估计处理效应。

Result: 框架成功应用于两个场景：用户对广告的影响力敏感度和用户对广告的敏感度。在线A/B测试显示，使用影响力分数进行定向投放的关键业务指标提升效果是通常认为显著水平的6倍以上。

Conclusion: 该大规模HTE估计框架能够有效发现潜在用户特征，提供稳定的处理效应估计，并在实际应用中显著提升业务效果，证明了异质性处理效应模型在工业场景中的实用价值。

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [14] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 提出两种基于物理启发的SVD LLM压缩改进方法：FermiGrad算法通过费米函数将离散奇异值截断松弛为连续优化来确定全局最优层秩；PivGa利用参数化中的规范自由度对低秩因子进行无损压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对计算资源需求很高，低秩分解（如SVD）是LLM压缩的有前景方法，但存在层秩选择和参数冗余等实际障碍。

Method: 1. FermiGrad：基于梯度下降算法，使用费米函数将离散奇异值截断松弛为连续优化，确定全局最优层秩；2. PivGa：利用低秩因子参数化中的规范自由度进行无损压缩。

Result: 论文提出了两种改进SVD LLM压缩的方法，解决了层秩选择和参数冗余问题，但摘要中未提供具体的实验结果数据。

Conclusion: 通过物理启发的FermiGrad和PivGa方法，改进了基于SVD的LLM压缩技术，解决了层秩优化和参数冗余问题。

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [15] [Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors](https://arxiv.org/abs/2512.03287)
*Dario Fenoglio,Mohan Li,Davide Casnici,Matias Laporte,Shkurta Gashi,Silvia Santini,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 提出多频联邦学习用于头戴设备的人体活动识别，在保护隐私的同时处理不同采样频率的设备数据


<details>
  <summary>Details</summary>
Motivation: 传统人体活动识别需要集中用户数据，存在隐私问题；头戴设备领域相对未被充分探索；设备间采样频率不同需要统一处理

Method: 采用多频联邦学习方法，支持隐私保护的机器学习，并能够联合学习不同采样频率设备的模型

Result: 在两个数据集上相比频率特定方法有所改进，显示了多频联邦学习在人体活动识别任务中的潜力

Conclusion: 多频联邦学习为头戴设备的人体活动识别提供了隐私保护和跨频率设备协同学习的有效解决方案，代码已开源供进一步研究

Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.

</details>


### [16] [Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning](https://arxiv.org/abs/2512.03065)
*Nihir Chadderwala*

Main category: cs.LG

TL;DR: 提出结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，让AI代理仅从用户反馈中学习最优决策策略，在生命科学领域实现15-30%的用户满意度提升。


<details>
  <summary>Details</summary>
Motivation: 生命科学中的生成式AI代理面临关键挑战：需要为从简单事实性问题到复杂机制推理的多样化查询确定最优方法。传统方法依赖固定规则或昂贵的标注训练数据，都无法适应变化条件或用户偏好。

Method: 结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，优化三个关键维度：生成策略选择（直接生成vs.思维链）、工具选择（文献搜索、药物数据库等）和领域路由（药理学、分子生物学、临床专家）。

Result: 在生命科学查询的实证评估中，相比随机基线实现了15-30%的用户满意度提升，在20-30个查询后出现清晰的学习模式。

Conclusion: 该方法无需真实标签，能持续适应用户偏好，为代理AI系统中的探索-利用困境提供了原则性解决方案。

Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.

</details>


### [17] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 使用预拓扑学建模建筑能耗曲线，开发基于预拓扑空间特性的多准则分层聚类算法，实现大规模建筑能耗的自动化分类与优化管理。


<details>
  <summary>Details</summary>
Motivation: 对数千栋建筑进行逐个深度能耗审计需要大量时间、资金和专业人员，因此需要开发自动化方法来建立有效的能耗管理推荐系统。

Method: 使用预拓扑学建模场地能耗曲线，开发基于预拓扑空间特性的多准则分层聚类算法，并实现为Python库。

Result: 在2D点数据集上能根据位置和大小识别聚类；在生成的时间序列数据集上使用皮尔逊相关系数获得ARI=1的完美聚类效果；在400个真实能耗站点的数据集上验证了方法有效性。

Conclusion: 提出的预拓扑学方法和多准则分层聚类算法能够有效建模和分类大规模分布式区域的能耗曲线，为建筑能耗优化管理提供自动化解决方案。

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [18] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 提出基于预拓扑空间的混合数据聚类方法，解决大数据环境下数值和分类变量混合的聚类挑战


<details>
  <summary>Details</summary>
Motivation: 大数据时代需要处理数值和分类变量混合的异构数据，传统聚类方法难以有效处理这种复杂性，需要专门针对混合数据的聚类方法

Method: 基于预拓扑空间的聚类方法，能够处理混合数据类型，提供层次化和可解释的聚类结果

Result: 与经典数值聚类算法和现有预拓扑方法进行基准测试，评估所提方法在大数据范式下的性能和有效性

Conclusion: 预拓扑空间方法为混合数据聚类提供了有效的解决方案，支持大数据环境下的结构化、可解释聚类分析

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [19] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 提出基于预拓扑的新算法，无需降维即可聚类混合数据，使用逻辑规则和可调超参数构建用户定义的分层聚类，在保持数据完整性的同时实现准确可解释的聚类结果。


<details>
  <summary>Details</summary>
Motivation: 解决混合数据聚类中的挑战，避免传统降维技术导致的信息损失，提高聚类结果的可解释性，为异构数据集提供定制化解决方案。

Method: 基于预拓扑的算法，利用析取范式制定可定制的逻辑规则和可调超参数，支持用户定义的分层聚类构建，直接从原始数据中识别聚类结构。

Result: 通过分层树状图分析和比较聚类指标，该方法表现出优越性能，能够准确、可解释地划分聚类，保持数据完整性，并在聚类可解释性方面具有显著优势。

Conclusion: 该工作通过创新性地使用逻辑规则而非传统降维技术，在混合数据聚类领域做出重要贡献，增强了聚类形成和清晰度，为聚类数据可解释性问题提供了有效解决方案。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [20] [Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information](https://arxiv.org/abs/2512.03074)
*Mahdi Tavassoli Kejani,Fadi Dornaika,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一种用于图神经网络的新型公平性正则化框架，适用于敏感属性仅部分可用的现实场景，在保持分类性能的同时显著减少偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平性GNN方法假设敏感属性完全可用，但这在实际中因隐私和数据收集限制难以实现。需要解决敏感属性仅部分可用的现实场景下的公平性问题。

Method: 提出模型无关的公平性正则化框架，将平等机会和统计奇偶性作为可微正则化项集成到目标函数中，适用于敏感属性部分可用的场景。

Result: 在五个真实世界基准数据集上的实验表明，该方法在关键公平性指标上显著减少偏见，同时保持竞争力的节点分类性能，在公平性-准确性权衡方面优于基线模型。

Conclusion: 该框架为敏感属性不完全可用的现实场景提供了有效的公平性解决方案，在减少偏见的同时最小化预测准确性损失，实现了良好的公平性-准确性平衡。

Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.

</details>


### [21] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 该论文将倾斜风险（entropic risk）应用于流匹配（Flow Matching），提出风险敏感的流匹配损失函数，能更好地捕捉数据流形的几何结构和少数分支。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配使用均方误差损失，将所有到达同一时空点的速度目标压缩为单一条件均值，忽略了高阶条件信息（方差、偏度、多模态），这些信息编码了数据流形的精细几何结构和少数分支。

Method: 将标准风险敏感（对数指数）变换应用于条件流匹配损失，得到的倾斜风险损失是每个时空点上有意义的条件熵流匹配目标的上界。通过该条件熵目标梯度的小阶展开，得到两个可解释的一阶修正：流匹配残差的协方差预处理，以及偏好非对称或稀有分支的偏尾项。

Result: 在专门设计用于探测模糊性和尾部的合成数据上，风险敏感损失比标准整流流匹配改善了统计指标，并更忠实地恢复了几何结构。

Conclusion: 倾斜风险损失为流匹配提供了一种自然框架，能够更好地捕捉数据流形的几何结构，特别是对于稀有事件和非对称分支，同时保持可优化性。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [22] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: ALARM是一个基于多模态大语言模型(MLLM)的视觉异常检测框架，集成了不确定性量化(UQ)和质量保证技术，在复杂环境中实现可靠决策


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，视觉异常检测面临异常具有高度上下文相关性和模糊性的挑战，需要不确定性量化能力来确保MLLM系统的可靠性

Method: ALARM框架将不确定性量化与推理链、自我反思和MLLM集成等质量保证技术相结合，基于严格的概率推理流程和计算过程设计

Result: 在真实世界的智能家居基准数据和伤口图像分类数据上进行广泛评估，显示ALARM具有优越性能，并在不同领域具有通用适用性

Conclusion: ALARM框架通过集成不确定性量化，为MLLM基视觉异常检测系统提供了可靠决策能力，适用于复杂环境中的跨领域应用

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [23] [Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration](https://arxiv.org/abs/2512.03102)
*Yiwei Shi,Hongnan Ma,Mengyue Yang,Cunjia Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 提出扩散驱动的贝叶斯探索框架，解决早期状态估计错误导致的永久性支持不变性问题，在危险气体定位任务中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 在应急响应等高风险应用中，基于有限或偏差信息的早期状态估计可能与现实严重不符，导致灾难性后果。传统粒子滤波器存在"平稳性诱导后验支持不变性"问题，初始先验排除的区域将永久无法探索，即使新证据与当前信念矛盾也无法修正

Method: 提出扩散驱动的贝叶斯探索框架，通过熵正则化采样和协方差缩放扩散扩展后验支持，使用Metropolis-Hastings检查验证提议并保持推理对意外证据的自适应性

Result: 在现实危险气体定位任务中，当先验正确时与强化学习和规划基线相当；在先验错配情况下显著优于传统SMC扰动和基于RL的方法，并提供理论保证证明DEPF能解决S-PSI问题同时保持统计严谨性

Conclusion: 扩散驱动的贝叶斯探索框架能够原则性地实时修正早期状态估计错误，解决了传统粒子滤波器的支持不变性问题，在高风险应用中具有重要价值

Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.

</details>


### [24] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: ECLIPSE是一个检测大语言模型幻觉的框架，通过结合语义熵估计和困惑度分解来测量模型对检索证据的利用程度，在金融问答数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会产生流畅但无依据的答案（幻觉），这限制了其在高风险领域的安全部署。需要一种机制来检测和减少幻觉。

Method: 提出ECLIPSE框架，将幻觉视为模型语义熵与可用证据容量之间的不匹配。结合多样本聚类的熵估计和新的困惑度分解方法来测量模型如何使用检索到的证据。

Result: 在金融问答数据集上，ECLIPSE达到ROC AUC 0.89和平均精度0.90，显著优于仅使用语义熵的基线（AUC 0.50）。实验表明ECLIPSE依赖于校准的token级不确定性。

Conclusion: ECLIPSE是一个有效的幻觉检测机制，其有效性依赖于token级不确定性校准。证据利用特征是幻觉检测的核心，但需要在更广泛领域和自然幻觉上进行验证。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [25] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: 提出e-valuator方法，将任意黑盒验证器分数转换为具有可证明错误警报率控制的决策规则，用于在线监控AI代理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理验证器（如LLM评委和过程奖励模型）的启发式评分缺乏正确性保证，无法可靠判断代理轨迹是否成功，需要统计保证的决策框架。

Method: 将成功与失败轨迹区分问题构建为序贯假设检验，基于e-processes开发序贯假设检验方法，在代理轨迹的每一步保持统计有效性，支持在线监控任意长动作序列。

Result: 在6个数据集和3种代理上，e-valuator相比其他策略提供更高统计功效和更好错误警报率控制，并能快速终止问题轨迹以节省token。

Conclusion: e-valuator提供轻量级、模型无关的框架，将验证器启发式转换为具有统计保证的决策规则，支持部署更可靠的代理系统。

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [26] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: SISR是一个统一的非线性解释框架，通过同时学习单调变换恢复可加性和施加L0稀疏约束，解决Shapley值在非可加性高维场景中的失真问题。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值框架假设可加性，但现实世界中的非高斯分布、重尾、特征依赖等常违反此假设，导致归因失真；同时在高维空间中计算密集Shapley值再阈值化成本过高且不一致。

Method: 提出Sparse Isotonic Shapley Regression (SISR)，同时学习单调变换恢复可加性（无需闭式规范）并施加L0稀疏约束；使用Pool-Adjacent-Violators进行高效保序回归和归一化硬阈值进行支持选择。

Result: SISR能在多种场景中恢复真实变换，在高噪声下实现强支持恢复；实验表明SISR能稳定不同收益方案下的归因，正确过滤无关特征，而标准Shapley值存在严重排序和符号失真。

Conclusion: SISR通过统一非线性变换估计和稀疏性追求，推进了非线性可解释性的前沿，提供了理论扎实且实用的归因框架。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [27] [Temporal Graph Neural Networks for Early Anomaly Detection and Performance Prediction via PV System Monitoring Data](https://arxiv.org/abs/2512.03114)
*Srijani Mukherjee,Laurent Vuillon,Liliane Bou Nassif,Stéphanie Giroux-Julien,Hervé Pabiou,Denys Dutykh,Ionnasis Tsanakas*

Main category: cs.LG

TL;DR: 提出基于时序图神经网络的方法，利用环境参数预测光伏系统输出功率并检测异常


<details>
  <summary>Details</summary>
Motivation: 光伏系统快速增长需要先进的性能监测和异常检测方法以确保最优运行

Method: 使用时序图神经网络，基于辐照度、模块温度和环境温度等关键参数构建图结构的时间关系来预测电功率输出

Result: 基于法国里昂屋顶光伏设施的数据进行验证，包括光伏模块功率测量和气象参数

Conclusion: 时序图神经网络方法能有效预测光伏输出功率并检测异常，为光伏系统性能监测提供新方案

Abstract: The rapid growth of solar photovoltaic (PV) systems necessitates advanced methods for performance monitoring and anomaly detection to ensure optimal operation. In this study, we propose a novel approach leveraging Temporal Graph Neural Network (Temporal GNN) to predict solar PV output power and detect anomalies using environmental and operational parameters. The proposed model utilizes graph-based temporal relationships among key PV system parameters, including irradiance, module and ambient temperature to predict electrical power output. This study is based on data collected from an outdoor facility located on a rooftop in Lyon (France) including power measurements from a PV module and meteorological parameters.

</details>


### [28] [Real-Time Structural Health Monitoring with Bayesian Neural Networks: Distinguishing Aleatoric and Epistemic Uncertainty for Digital Twin Frameworks](https://arxiv.org/abs/2512.03115)
*Hanbin Cho,Jecheon Yu,Hyeonbin Moon,Jiyoung Yoon,Junhyeong Lee,Giyoung Kim,Jinhyoung Park,Seunghwa Ryu*

Main category: cs.LG

TL;DR: 提出集成PCA、贝叶斯神经网络和HMC推理的SHM框架，从稀疏应变测量重建全场应变分布并量化不确定性，在CFRP试件上验证了准确性和实时不确定性分析能力。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测需要可靠的全场不确定性量化来支持可信决策，但现有方法难以同时获得空间分辨的随机不确定性和认知不确定性。

Method: 结合主成分分析(PCA)、贝叶斯神经网络(BNN)和哈密顿蒙特卡洛(HMC)推理，将稀疏应变计测量映射到主导PCA模式，重建全场应变分布并量化不确定性。

Result: 在碳纤维增强聚合物(CFRP)试件的循环四点弯曲试验中验证，实现了准确的全场应变重建(R²>0.9)，同时实时生成不确定性场，能够区分数据固有和模型相关的低置信区域。

Conclusion: 该框架通过同时提供互补的不确定性场，支持可靠的局部诊断和决策，推动结构健康监测向可信数字孪生部署和风险感知诊断发展。

Abstract: Reliable real-time analysis of sensor data is essential for structural health monitoring (SHM) of high-value assets, yet a major challenge is to obtain spatially resolved full-field aleatoric and epistemic uncertainties for trustworthy decision-making. We present an integrated SHM framework that combines principal component analysis (PCA), a Bayesian neural network (BNN), and Hamiltonian Monte Carlo (HMC) inference, mapping sparse strain gauge measurements onto leading PCA modes to reconstruct full-field strain distributions with uncertainty quantification. The framework was validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with varying crack lengths, achieving accurate strain field reconstruction (R squared value > 0.9) while simultaneously producing real-time uncertainty fields. A key contribution is that the BNN yields robust full-field strain reconstructions from noisy experimental data with crack-induced strain singularities, while also providing explicit representations of two complementary uncertainty fields. Considered jointly in full-field form, the aleatoric and epistemic uncertainty fields make it possible to diagnose at a local level, whether low-confidence regions are driven by data-inherent issues or by model-related limitations, thereby supporting reliable decision-making. Collectively, the results demonstrate that the proposed framework advances SHM toward trustworthy digital twin deployment and risk-aware structural diagnostics.

</details>


### [29] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: MoDE提出一种轻量级可扩展架构，通过解耦模态特定更新和知识蒸馏，解决统一多模态生成模型中的模态间和模态内灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态生成模型在持续学习新任务时面临严重的灾难性遗忘问题，包括模态内遗忘和模态间遗忘。虽然模态内遗忘已有研究，但模态间遗忘尚未被充分探索，其根源在于模态间的梯度冲突。

Method: 提出MoDE架构：1) 隔离模态特定更新以缓解梯度冲突；2) 利用知识蒸馏防止灾难性遗忘并保留预训练能力；3) 显式解耦模态以避免干扰，与之前保持模态耦合的方法不同。

Result: 在多样化基准测试中，MoDE显著缓解了模态间和模态内遗忘，在统一多模态生成设置中优于先前的持续学习基线方法。

Conclusion: MoDE通过解耦模态特定更新有效解决了UMGMs中的模态间遗忘问题，为多模态持续学习提供了轻量级且可扩展的解决方案。

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [30] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: ChefNMR：首个端到端框架，仅使用1D NMR谱和化学式直接预测未知分子结构，在天然产物结构解析中达到超过65%的准确率


<details>
  <summary>Details</summary>
Motivation: NMR谱解析是确定小分子结构的关键技术，但传统方法耗时且需要大量专业知识。天然产物和临床治疗药物的发现急需自动化结构解析方法，以加速分子发现过程。

Method: 将结构解析框架为条件生成问题，采用基于非等变transformer架构的原子扩散模型。为模拟天然产物中的复杂化学基团，构建了包含超过111,000个天然产物的模拟1D NMR谱数据集。

Result: ChefNMR在具有挑战性的天然产物化合物结构预测中取得了超过65%的准确率，这是目前最高的准确率水平。

Conclusion: 该研究在自动化小分子结构解析这一重大挑战上迈出了重要一步，展示了深度学习在加速分子发现方面的巨大潜力，代码已开源。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [31] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: 提出基于VQ-VAE的无监督病毒变异检测框架，用于废水基因组监测，无需参考基因组或变异标签，在SARS-CoV-2数据上实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 废水基因组监测面临高测序噪声、低病毒覆盖率、片段化读取和缺乏标注变异等计算挑战，传统基于参考的变异检测方法难以处理新突变且计算资源需求大。

Method: 使用向量量化变分自编码器(VQ-VAE)从k-mer标记化序列中学习基因组模式的离散码本，无需参考基因组或变异标签。扩展基础架构包括掩码重建预训练（处理缺失数据）和对比学习（获得高判别性嵌入）。

Result: 在约10万条SARS-CoV-2废水测序数据上，VQ-VAE达到99.52%的平均标记级准确率和56.33%的精确序列匹配率，码本利用率为19.73%（512个码中101个活跃）。对比微调显著改善聚类：64维嵌入的轮廓系数提高35%（0.31到0.42），128维嵌入提高42%（0.31到0.44）。

Conclusion: 该无参考框架为基因组监测提供了可扩展、可解释的方法，可直接应用于公共卫生监测，解决了废水病毒监测中的关键计算挑战。

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [32] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 提出交错推理(IR)和Plantain方法，让语言模型在推理过程中交替输出中间结果，减少用户等待时间并允许早期干预，相比传统"先思考后回答"方法提升6%准确率并减少60%首次响应时间。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型采用"先思考后回答"方式，用户在模型推理期间无法了解其思考过程，也无法在模型基于错误前提推理时及时干预纠正，造成时间浪费和糟糕体验。人类对话中会进行轻量级、渐进式的确认来确保双方理解一致，因此研究语言模型是否能学习类似行为。

Method: 提出交错推理(IR)方法，模型在思考和生成最终答案之间交替输出中间响应。进一步提出Plantain(计划-思考-答案交错)方法，其中第一个中间响应是执行任务的明确、逐步计划，允许用户干预并为后续推理步骤提供早期反馈。

Result: Plantain方法在多个具有挑战性的数学推理和编程基准测试中，pass@1指标提升约6%，同时相对于"先思考后回答"基线，首次响应时间减少超过60%。

Conclusion: 交错推理方法通过早期向用户提供有用信息，在不影响最终响应质量的情况下减少感知延迟。Plantain的计划优先策略特别有效，允许用户干预和早期反馈，显著提升模型性能并改善用户体验。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [33] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: FiRE/FiRE.1和Enhash：用于单细胞RNA测序异常检测和流数据概念漂移检测的高效算法


<details>
  <summary>Details</summary>
Motivation: 处理大规模单细胞RNA测序数据中罕见细胞亚群的快速识别问题，以及流数据中概念漂移的高效检测需求

Method: FiRE/FiRE.1采用基于草图的异常检测算法；Enhash使用投影哈希的集成学习器进行概念漂移检测

Result: FiRE/FiRE.1在罕见细胞亚群检测上优于现有技术；Enhash在各种漂移类型中在时间和准确性上都具有高度竞争力

Conclusion: 提出的两种算法分别在单细胞RNA测序异常检测和流数据概念漂移检测领域提供了高效且性能优越的解决方案

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [34] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出了几种改进的算法，用于在无限时域设置中学习带记忆的策略，包括已知环境模型和通过模拟的情况，并在大型POMDP问题上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在部分可观测环境中学习动作机制方面受到关注，但在需要记忆的问题上表现不佳，需要改进带记忆策略的学习算法。

Method: 开发了多种改进算法：当环境模型已知时直接学习带记忆策略；通过模拟学习带记忆策略；在无限时域设置中优化策略。

Result: 在大型POMDP问题上进行了比较测试，包括噪声机器人导航和多智能体问题，验证了算法的有效性。

Conclusion: 提出的改进算法能够有效学习带记忆的策略，解决了策略梯度方法在需要记忆环境中的局限性问题。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [35] [A Multi-Agent, Policy-Gradient approach to Network Routing](https://arxiv.org/abs/2512.03211)
*Nigel Tao,Jonathan Baxter,Lex Weaver*

Main category: cs.LG

TL;DR: OLPOMDP算法成功应用于模拟网络路由，多个分布式路由器代理无需显式通信即可学习协作行为，避免个体最优但损害整体性能的行为，通过奖励塑形显著提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 网络路由是一个分布式决策问题，具有自然的数值性能度量（如平均包传输时间）。研究如何应用强化学习算法解决网络路由中的协作决策问题，使分布式路由器能够学习协作行为而不需要显式通信。

Method: 使用OLPOMDP（一种策略梯度强化学习算法）应用于模拟网络路由。采用多个分布式代理（路由器）在多种网络模型下学习，通过奖励塑形技术对奖励信号进行优化，明确惩罚某些次优行为模式。

Result: 分布式路由器成功学习到协作行为，避免了仅对个体有利但损害整体性能的行为。奖励塑形技术显著提高了算法的收敛速度，使学习过程更加高效。

Conclusion: OLPOMDP算法能够有效解决网络路由中的分布式决策问题，通过奖励塑形可以大幅提升学习效率，实现无需显式通信的多代理协作路由决策。

Abstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.

</details>


### [36] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0生物声学基础模型在海洋哺乳动物音频任务上表现出色，通过少样本迁移学习超越其他预训练模型


<details>
  <summary>Details</summary>
Motivation: 尽管Perch 2.0在训练数据中几乎不包含海洋哺乳动物音频，但研究者希望评估其在海洋哺乳动物和水下音频任务上的迁移学习能力，以验证其作为通用生物声学基础模型的适用性。

Method: 使用Perch 2.0生成的嵌入进行线性探测（linear probing），与其他预训练生物声学模型（包括Perch 1.0、SurfPerch、AVES-bio、BirdAVES、Birdnet V2.3等）进行少样本迁移学习性能比较。

Result: Perch 2.0的嵌入在少样本迁移学习中表现出持续的高性能，在大多数任务上优于其他嵌入模型，特别是在海洋哺乳动物分类任务中。

Conclusion: Perch 2.0是开发海洋哺乳动物分类线性分类器的推荐模型，特别是在标记样本有限的情况下，其嵌入具有优异的迁移学习能力。

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [37] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK框架通过三阶段方法解决过程奖励模型训练数据稀缺问题：1) 使用生成器和验证器模型创建合成训练数据；2) 微调生成式过程奖励模型；3) 在强化学习中应用PRM-CoT作为奖励信号，在数学推理任务上超越基于真实标签的方法。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型需要密集的步骤级反馈，但现有方法受限于昂贵的步骤级标注或真实参考数据。作者希望开发一种无需真实标签或参考数据的方法，在缺乏可验证答案或难以获取真实标签的领域中实现有效的强化学习训练。

Method: 提出三阶段SPARK框架：第一阶段，生成器模型产生多样解，验证器模型通过并行扩展（自一致性）和序列扩展（元批判）进行评估；第二阶段，使用验证输出作为合成训练数据微调生成式过程奖励模型；第三阶段，将PRM-CoT作为强化学习的奖励模型，并引入格式约束防止奖励攻击。

Result: 在ProcessBench上达到67.5 F1，优于参考引导训练的66.4和GPT-4o的61.9；使用Qwen2.5-Math-7B在六个数学推理基准上达到47.4%平均准确率，超越基于真实标签的RLVR（43.9%）。

Conclusion: SPARK框架实现了无需参考的强化学习训练，性能超越基于真实标签的方法，为缺乏可验证答案或难以获取真实标签的领域开辟了新可能性。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [38] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型在事实回忆任务上表现下降，原因是实体表示形成过晚，无法有效利用LLM已有的知识回忆机制。


<details>
  <summary>Details</summary>
Motivation: 许多视觉语言模型相比其LLM骨干模型在事实回忆性能上有所下降，这引发了对多模态微调有效性的质疑。研究旨在探究VLMs如何将视觉输入与LLM已有的知识回忆机制对齐。

Method: 对14个不同架构、规模和训练设置的VLMs进行基准测试，使用归因修补、激活修补和探测技术分析性能差异。选择高/低性能退化模型进行深入机制分析。

Result: 11/14的模型表现出事实回忆性能下降。性能下降的VLMs因实体表示形成过晚而无法有效利用LLM已有的知识回忆电路。性能高的VLMs能早期解析实体表示。

Conclusion: 早期实体解析的速度是VLMs能否有效利用预训练LLM机制的关键因素。研究展示了机制分析如何解释多模态对齐中的系统性失败，并提出了两种性能恢复方法。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [39] [BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark](https://arxiv.org/abs/2512.03280)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Matthew C. Jones,Faez Ahmed*

Main category: cs.LG

TL;DR: BlendedNet++：一个包含12,000+混合翼体飞机几何形状的大规模空气动力学数据集和基准测试，提供集成力和表面场数据，用于前向代理预测和基于扩散模型的逆向设计任务。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习空气动力学代理模型面临大规模、高分辨率数据集的缺乏，限制了精确点预测和可重复逆向设计的进展。需要标准化数据集和基准来公平比较不同架构和优化范式。

Method: 1) 构建包含12,000+独特混合翼体飞机几何形状的数据集，每个在单一飞行条件下进行稳态RANS CFD模拟；2) 提供集成力系数和密集表面场数据；3) 建立前向代理基准，评估六种模型架构；4) 提出基于条件扩散模型的逆向设计任务，并与梯度优化和混合方法对比。

Result: 创建了包含12,490个空气动力学结果的大规模数据集，标准化了前向预测和逆向设计的基准测试协议，提供了多模型基线，为公平、可重复的架构比较和优化范式评估奠定了基础。

Conclusion: BlendedNet++为场级空气动力学和逆向设计提供了统一的基准测试框架，有望促进该领域的可重复研究。数据集、分割、基线和脚本将在论文接受后公开发布。

Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.

</details>


### [40] [ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics](https://arxiv.org/abs/2512.03290)
*Julian Evan Chrisnanto,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: ASPEN是一种新型PINN架构，通过自适应谱层和可学习傅里叶特征解决传统PINN在求解刚性、多尺度非线性PDE时的频谱偏差问题，成功应用于复杂的Ginzburg-Landau方程。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在处理刚性、多尺度非线性PDE系统时存在严重局限性，主要原因是标准MLP架构的频谱偏差无法充分表示高频分量，导致求解失败。

Method: 提出ASPEN架构，在网络输入阶段集成自适应谱层和可学习傅里叶特征，使模型能够在训练过程中动态调整自身的谱基，有效学习和表示解所需的精确频率内容。

Result: 在复杂的Ginzburg-Landau方程上，传统PINN架构完全失败并产生非物理振荡，而ASPEN成功求解，预测解与高分辨率真值在视觉上无法区分，中位物理残差仅为5.10×10^-3，且能正确捕捉涌现的物理特性。

Conclusion: 通过引入自适应谱基，ASPEN为传统PINN无法处理的复杂动力系统提供了鲁棒且物理一致的求解器，为机器学习在挑战性物理领域的应用开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.

</details>


### [41] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 本文提出一种结合深度切换状态空间模型与自适应共形推理的方法，为存在体制转换的非平稳时间序列提供分布自由的预测不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的体制转换会破坏平稳性，使得校准的不确定性与点预测精度同等重要。需要为存在体制切换的预测问题提供分布自由的预测区间。

Method: 将深度切换状态空间模型与自适应共形推理（ACI）及其聚合变体（AgACI）结合。还提出一个统一的共形包装器，可应用于S4、MC-Dropout GRU、稀疏高斯过程和变点局部模型等序列基线模型，在非平稳性和模型误设下提供在线预测区间。

Result: 在合成和真实数据集上，共形化预测器实现了接近名义水平的覆盖率，具有竞争力的准确性，并普遍提高了区间效率。

Conclusion: 所提出的方法能够为存在体制转换的非平稳时间序列提供具有有限样本边际保证的预测区间，在覆盖率和区间效率方面表现良好。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [42] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: HydroDCM：一个用于跨水库入库流量预测的可扩展域泛化框架，通过空间元数据构建伪域标签指导对抗学习，并在推理时通过轻量级条件层进行目标水库特定适应


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在水库入库流量预测中表现良好，但在应用于不同水库时性能下降，这被称为域偏移问题。传统域泛化方法在水文环境中面临挑战，因为每个水库都有独特的入库模式，而空间信息等元数据对预测有间接但显著的影响

Method: 提出HydroDCM框架：1）利用水库空间元数据构建伪域标签指导对抗学习，提取域不变的时间特征；2）在推理时，通过轻量级条件层结合目标水库的元数据，实现域泛化的不变性与位置特定适应的平衡

Result: 在科罗拉多河上游流域30个真实水库上的实验表明，该方法在多域条件下显著优于最先进的域泛化基线方法，同时保持计算效率

Conclusion: HydroDCM通过结合空间元数据指导的对抗学习和轻量级条件适应，有效解决了多域水文系统中的域泛化问题，为跨水库入库流量预测提供了可扩展的解决方案

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [43] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: RTFM提出了一种对抗性训练框架，通过参数化生成器分布来强调对模型特别具有挑战性的合成数据集，从而提升表格基础模型的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型(TFMs)在结构化数据上展现出超越传统ML方法的潜力。现有研究主要关注设计高质量的数据生成器先验来提升预训练性能，但本文作者发现参数化生成器分布可以从对抗鲁棒性角度出发，在训练中调整生成器以强调对模型特别具有挑战性的数据集。

Method: 提出了RTFM（Robust Tabular Foundation Models）框架，引入最优性间隙度量（TFM性能与XGBoost、CatBoost、Random Forests等强基线最佳可达性能的差异），并基于此进行模型无关的对抗性训练。通过参数化生成器分布，在训练过程中适应性地生成对模型特别具有挑战性的数据集。

Result: 在TabPFN V2分类器上应用RTFM，相比原始TabPFN和其他基线算法，平均归一化AUC提升高达6%，且仅需不到10万个额外的合成数据集。

Conclusion: RTFM展示了仅使用合成数据进行针对性对抗训练和微调表格基础模型的新方向，通过强调具有挑战性的数据集来提升模型鲁棒性和性能。

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [44] [Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates](https://arxiv.org/abs/2512.03309)
*Aniruddha Bora,Shixuan Zhang,Khemraj Shukla,Bryce Harrop,George Em. Karniadakis,L. Ruby Leung*

Main category: cs.LG

TL;DR: 提出一个基于算子学习的框架，通过在线应用偏差修正趋势来改进地球系统模型预测，使用两种U-Net变体架构，在E3SM模型中实现稳定且可扩展的混合建模。


<details>
  <summary>Details</summary>
Motivation: 传统的数据同化偏差修正方法在模型自由运行时效果有限，需要一种能够在线修正偏差、提高地球系统模型预测准确性的新方法。

Method: 开发了两种基于U-Net的算子学习架构：Inception U-Net (IUNet) 和多尺度网络(M&M)，结合多种上采样和感受野，在E3SM运行时约束下捕捉多尺度非线性特征。使用两年E3SM模拟数据训练，将模型状态映射到偏差修正趋势并在集成过程中在线应用。

Result: 两种架构在离线测试中均优于标准U-Net基线，表明功能丰富性而非参数数量驱动性能。在线混合E3SM运行中，M&M在变量和垂直层次上提供最一致的偏差减少。ML增强配置在多年模拟中保持稳定且计算可行。

Conclusion: 该框架强调长期稳定性、可移植性和更新频率限制，展示了表达性ML算子在学习和结构化跨尺度关系以及改造传统ESM方面的实用性，为可扩展混合建模提供了实用途径。

Abstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.

</details>


### [45] [Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs](https://arxiv.org/abs/2512.03324)
*Ngoc Bui,Shubham Sharma,Simran Lamba,Saumitra Mishra,Rex Ying*

Main category: cs.LG

TL;DR: TRIM-KV：一种通过学习token内在重要性来管理KV缓存的新方法，使用轻量级保留门预测token保留分数，在内存受限时淘汰低分token，在多种长上下文任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长序列LLM推理中，自注意力的二次计算成本和不断增长的KV缓存成为核心瓶颈。现有方法如量化、卸载或启发式KV淘汰要么协调成本高，要么依赖不可靠的重要性代理。

Method: 提出TRIM-KV方法，通过轻量级保留门在token创建时学习其内在重要性。每个门预测一个随时间衰减的标量保留分数，反映token对特定层和头的长期效用。当内存预算超限时淘汰低分token。

Result: 在数学推理（GSM8K、MATH-500、AIME24）、过程生成（LongProc）、对话长记忆（LongMemEval）和长上下文理解（LongBench、SCBench）等任务中，TRIM-KV始终优于强淘汰和可学习检索基线，尤其在低内存情况下表现突出。在某些设置中甚至超过全缓存模型。

Conclusion: TRIM-KV不仅提高了内存效率，其学习的保留分数与人类直觉一致，自然恢复了sink token、滑动窗口和要点压缩等启发式方法。保留分数还提供了层和头特定角色的见解，为LLM可解释性开辟了新途径。

Abstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.

</details>


### [46] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: SAFLe框架通过引入结构化头部和稀疏分组嵌入，实现可扩展的非线性表达能力，同时保持单轮聚合优势，在联邦学习中取得新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信开销大和非IID数据性能下降两大挑战。现有方法存在局限性：AFL只能用于线性模型，而DeepAFL等非线性方法牺牲了单轮聚合优势。

Method: 提出SAFLe框架，引入结构化头部（分桶特征）和稀疏分组嵌入，证明该非线性架构在数学上等价于高维线性回归，从而可利用AFL的单轮不变聚合定律。

Result: SAFLe在联邦视觉任务中建立了新的SOTA，在所有基准测试中显著优于线性AFL和多轮DeepAFL，实现了高效可扩展的解决方案。

Conclusion: SAFLe成功打破了联邦学习中非线性表达能力和单轮聚合之间的权衡，为异构数据联邦学习提供了高效可扩展的解决方案。

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [47] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出首个确定性广告拍卖中的离策略评估框架，利用出价景观模型近似倾向得分，实现稳定评估，在工业平台验证中达到92%平均方向准确率。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试消耗大量工程资源且存在收入损失风险，而传统离策略评估方法在确定性拍卖环境中因非获胜广告曝光概率为零而失效，需要新的评估框架。

Method: 重新利用出价景观模型近似倾向得分，推导稳健的近似倾向得分，使自归一化逆倾向得分等稳定估计器能够在确定性拍卖环境中进行反事实评估。

Result: 在AuctionNet仿真基准和大型工业平台2周在线A/B测试中验证，点击率预测达到92%平均方向准确率，显著优于参数基线，与在线结果高度一致。

Conclusion: 这是首个实用且经验证的确定性拍卖环境可靠离策略评估框架，为昂贵且有风险的在线实验提供了高效替代方案。

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [48] [A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning](https://arxiv.org/abs/2512.03363)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出A2G（自适应双增益聚合）框架，用于量子经典混合联邦学习系统，通过几何增益和QoS增益联合调节模型聚合，提升异构噪声环境下的性能


<details>
  <summary>Details</summary>
Motivation: 量子赋能和异构经典网络中的联邦学习面临客户端质量不均、量子隐形传态保真度随机、设备不稳定以及局部与全局模型几何不匹配等问题，传统基于欧几里得拓扑和均匀通信可靠性的聚合规则不适用于新兴量子联邦系统

Method: 提出A2G双增益框架：1）几何增益调节模型几何混合；2）QoS增益基于隐形传态保真度、延迟和不稳定性调节客户端重要性。开发A2G更新规则，在平滑性和有界方差假设下建立收敛保证

Result: A2G在量子经典混合测试平台上表现出改进的稳定性和更高的准确性，特别是在异构和噪声条件下。该框架可退化为FedAvg、QoS感知平均和基于流形的聚合等特殊情况

Conclusion: A2G为量子联邦学习系统提供了一种有效的自适应聚合框架，能够处理量子经典混合网络中的异构性和噪声问题，为未来量子联邦学习系统设计提供了理论基础和实践指导

Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

</details>


### [49] [MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems](https://arxiv.org/abs/2512.03375)
*Mahdi Arab Loodaricheh,Mohammad Hossein Manshaei,Anita Raja*

Main category: cs.LG

TL;DR: MAGE-ID是一个基于扩散模型的多模态攻击生成框架，用于入侵检测系统的数据增强，通过联合训练Transformer和CNN编码器，将表格流量特征与图像转换相结合，解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代入侵检测系统面临异构网络流量、不断演变的网络威胁以及良性流量与攻击流量之间数据严重不平衡的挑战。现有生成模型仅限于单一模态，无法捕捉跨域依赖关系。

Method: 提出MAGE-ID（多模态攻击生成器），这是一个基于扩散的生成框架，通过统一的潜在先验将表格流量特征与其转换后的图像耦合。联合训练基于Transformer和CNN的变分编码器与EDM风格的去噪器，实现平衡且连贯的多模态合成。

Result: 在CIC-IDS-2017和NSL-KDD数据集上的评估显示，MAGE-ID在保真度、多样性和下游检测性能方面显著优于TabSyn和TabDDPM，证明了其用于多模态IDS增强的有效性。

Conclusion: MAGE-ID通过多模态生成方法有效解决了入侵检测中的数据不平衡问题，为IDS数据增强提供了更有效的解决方案，展示了扩散模型在多模态网络安全应用中的潜力。

Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.

</details>


### [50] [UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs](https://arxiv.org/abs/2512.03383)
*Hung-Yueh Chiang,Chi-Chih Chang,Yu-Chen Lu,Chien-Yu Lin,Kai-Chiang Wu,Mohamed S. Abdelfattah,Diana Marculescu*

Main category: cs.LG

TL;DR: UniQL是一个统一的边缘LLM后训练量化和低秩压缩框架，支持在设备上配置剪枝率，实现4-5.7倍内存减少和2.7-3.4倍吞吐提升，精度损失在5%以内。


<details>
  <summary>Details</summary>
Motivation: 在移动平台上部署大型语言模型面临内存有限和计算资源共享的挑战，设备工作负载直接影响资源可用性，增加了模型部署的不确定性。

Method: 提出UniQL统一框架，集成量化和低秩压缩，支持Transformers、SSMs和混合模型。包括高效结构化权重排序（加速20倍）、量化感知SVD、SSMs状态感知权重排序、剪枝模型的融合RoPE内核。在云端单次流程中完成权重排序、微调、量化，支持设备端配置最高35%剪枝率。

Result: 量化剪枝模型实现4-5.7倍内存减少和2.7-3.4倍token吞吐提升，在15%剪枝率下，Llama3、Qwen2.5、Mamba2、Nemotron-H、Bamba-v2等模型精度损失控制在5%以内。

Conclusion: UniQL为边缘LLM部署提供了有效的统一压缩框架，平衡了模型压缩与精度保持，支持多种模型架构，代码和量化模型已开源。

Abstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.

</details>


### [51] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出一种基于隐式正则化的免调参框架，用于解决多测量向量（MMV）中的联合稀疏信号恢复问题，无需先验知识或参数调整。


<details>
  <summary>Details</summary>
Motivation: 传统MMV方法如M-OMP和M-FOCUSS需要仔细的参数调整或对信号稀疏度和噪声方差的先验知识，这限制了实际应用。

Method: 通过过参数化引入隐式正则化，将估计矩阵重新参数化为因子，分离共享行支持与个体向量条目。对标准最小二乘目标应用梯度下降，优化动态自然促进期望的行稀疏结构。

Result: 理论证明：在足够小且平衡的初始化下，优化动态呈现"动量效应"，使真实支持中的行范数增长远快于其他行，保证解轨迹收敛到理想行稀疏解。实证结果显示性能与现有方法相当。

Conclusion: 提出的免调参框架通过隐式正则化有效解决MMV问题，无需先验信息或参数调整，为联合稀疏信号恢复提供了实用解决方案。

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [52] [VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing](https://arxiv.org/abs/2512.03394)
*Hamed Poursiami,Shay Snyder,Guojing Cong,Thomas Potok,Maryam Parsa*

Main category: cs.LG

TL;DR: VS-Graph：一种基于向量符号架构的图学习框架，通过尖峰扩散机制和关联消息传递，在保持HDC高效性的同时达到接近GNN的表达能力，训练速度提升高达450倍。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）在图分类任务中表现优异但计算成本高，限制了其在资源受限设备上的部署。超维计算（HDC）虽然轻量高效，但现有HDC方法在预测性能上难以匹敌GNN。需要一种既能保持HDC效率又能接近GNN表达能力的方法。

Method: 提出VS-Graph框架，包含两个核心组件：1）尖峰扩散机制（Spike Diffusion）用于拓扑驱动的节点识别；2）关联消息传递方案（Associative Message Passing）用于在高维向量空间内进行多跳邻域聚合。无需基于梯度的优化或反向传播。

Result: 在MUTAG和DD等标准基准测试中，比之前的HDC基线提升4-5%的准确率，与多个现代GNN基线相当或更优。训练速度提升高达450倍。即使在超向量维度降至D=128时仍保持高准确率，展示了在维度压缩下的鲁棒性。

Conclusion: VS-Graph成功缩小了HDC效率与消息传递表达能力之间的差距，为在边缘和神经形态硬件上实现超高效执行铺平了道路，提供了图分类任务中轻量级高性能的替代方案。

Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.

</details>


### [53] [Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value](https://arxiv.org/abs/2512.03399)
*Joe Edelman,Tan Zhi-Xuan,Ryan Lowe,Oliver Klingefjord,Vincent Wang-Mascianica,Matija Franklin,Ryan Othniel Kearns,Ellie Hain,Atrisha Sarkar,Michiel Bakker,Fazl Barez,David Duvenaud,Jakob Foerster,Iason Gabriel,Joseph Gubbels,Bryce Goodman,Andreas Haupt,Jobst Heitzig,Julian Jara-Ettinger,Atoosa Kasirzadeh,James Ravi Kirkpatrick,Andrew Koh,W. Bradley Knox,Philipp Koralus,Joel Lehman,Sydney Levine,Samuele Marro,Manon Revel,Toby Shorin,Morgan Sutherland,Michael Henry Tessler,Ivan Vendrov,James Wilken-Smith*

Main category: cs.LG

TL;DR: 论文提出"全栈对齐"概念，认为仅对齐AI系统与操作者意图不够，需要同时对齐AI系统和塑造它们的机构与人类价值观，并建议使用"厚价值模型"来有效表示价值观。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在局限：即使AI系统完美对齐其操作组织的意图，如果该组织的目标与其他机构和个人目标不一致，仍可能导致不良社会结果。现有价值观表示方法（如效用函数、偏好排序、非结构化文本）难以有效区分价值观与其他信号、支持规范性推理、建模集体利益。

Method: 提出"厚价值模型"方法，结构化表示价值观和规范，使系统能够：1）区分持久价值观与短暂偏好；2）建模个体选择的社会嵌入性；3）进行规范性推理，将价值观应用于新领域。在五个领域展示该方法：AI价值管理、规范性能力智能体、双赢谈判系统、意义保持经济机制、民主监管机构。

Result: 论文论证了厚价值模型能够解决现有价值观表示方法的不足，支持更有效的全栈对齐。通过五个具体应用领域展示了该方法的可行性和实用性。

Conclusion: 需要从单纯的技术对齐转向全栈对齐，同时对齐AI系统和塑造它们的机构。厚价值模型为实现这一目标提供了有效的理论框架和方法论，能够在不过度规定个人或集体繁荣愿景的情况下，更好地将AI系统与人类价值观对齐。

Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

</details>


### [54] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: 研究显示，在Transformer中显式加入世界建模目标能提升状态表征的线性可解码性和因果可操控性，进而增强强化学习后训练的效果，尤其在复杂任务状态下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索显式世界建模目标如何影响Transformer在不同训练阶段的内在表征和下游能力，特别是在序列规划任务中，了解世界模型质量对强化学习后训练性能的影响。

Method: 使用2x2x2魔方作为测试环境，比较标准的下一个token预测与两种显式世界建模策略：(i)状态预测预训练和(ii)状态预测+下一个token联合目标。使用GRPO进行后训练，通过线性探针和因果干预评估表征质量。

Result: 显式世界建模能产生更线性可解码和因果可操控的状态表征。更重要的是，改进的状态表征能显著提升GRPO的性能增益，特别是在更难的魔方状态下。状态表征的锐化能提高序列规划任务后训练的有效性。

Conclusion: 显式世界建模通过改善状态表征质量，能够有效提升Transformer在序列规划任务中的强化学习后训练效果，这为改进复杂任务的学习提供了重要启示。

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [55] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出GaussDetect-LiNGAM，一种新的双变量因果发现方法，通过利用前向模型噪声的高斯性与反向模型残差独立性之间的等价关系，消除了显式高斯性检验的需求。


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法依赖脆弱且对样本敏感的高斯性检验，这限制了其在现实场景中的可靠性和实用性。需要一种更稳健的方法来改进因果推断的效率和适用性。

Method: 基于理论证明：在LiNGAM的线性、无环和外生性假设下，前向模型噪声的高斯性等价于反向模型中回归变量与残差的独立性。利用这一等价关系，用稳健的核基独立性检验替代脆弱的高斯性检验。

Result: 实验验证了理论等价性，表明GaussDetect-LiGAM在各种噪声类型和样本量下保持高一致性，同时减少了每个决策所需的检验次数（TPD）。

Conclusion: 该方法提高了因果推断的效率和实际应用性，使LiNGAM在现实场景中更加可访问和可靠，无需显式高斯性检验。

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [56] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: 研究发现，在模型完成"顿悟"（grokking）阶段后进行机器遗忘，相比早期停止的模型，能实现更高效的遗忘、更少的副作用和更稳定的更新。


<details>
  <summary>Details</summary>
Motivation: 探索"顿悟"（延迟泛化现象）是否有助于机器遗忘，即在不完全重新训练的情况下移除特定数据的影响。

Method: 在视觉（CNN/ResNet在CIFAR、SVHN、ImageNet）和语言（transformer在TOFU风格设置）任务上，比较在顿悟前后应用标准遗忘方法的效果。

Result: 从顿悟后检查点开始遗忘，能实现：(1) 更高效的遗忘（达到目标遗忘水平需要更少更新），(2) 更少的附带损害（保留数据和测试性能下降更小），(3) 更稳定的跨种子更新。

Conclusion: 顿悟后模型学习到更模块化的表示，减少了遗忘和保留子集之间的梯度对齐，这有助于选择性遗忘。模型何时训练（顿悟前后）是改进现有遗忘方法的正交杠杆。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [57] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 提出用于金融情感分析的端到端深度学习框架，整合时效性意见和流行性意见两种模态，通过跨模态注意力机制显著提升分类准确率至83.5%


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以有效整合多样化的意见模态，无法捕捉模态间的细粒度交互，而时效性意见（市场更新）和流行性意见（集体情绪）代表了不同的信息渠道，需要专门的方法来融合

Method: 使用BERT（Chinese-wwm-ext）进行特征嵌入，提出金融多头交叉注意力（FMHCA）结构促进两种意见模态间的信息交换，通过Transformer层优化特征，采用多模态因子双线性池化进行情感分类（负面、中性、正面）

Result: 在涵盖837家公司的综合数据集上，该方法达到83.5%的准确率，显著优于包括BERT+Transformer在内的基线方法（提升21%）

Conclusion: 该框架通过有效整合时效性和流行性两种金融意见模态，能够支持更准确的金融决策和风险管理，展示了跨模态注意力机制在金融情感分析中的潜力

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [58] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯亚型事件模型（BEBMS），在合成数据和真实阿尔茨海默病数据上均优于现有的SuStaIn方法。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病在不同患者中的进展方式存在差异，通常存在少数几种亚型。现有的SuStaIn方法虽然广泛应用，但其鲁棒性需要验证。本文旨在开发更稳健的贝叶斯方法，并评估其相对于SuStaIn的性能。

Method: 开发了基于贝叶斯框架的亚型事件模型（BEBMS），通过合成数据实验（包含不同程度的模型误设）比较BEBMS与SuStaIn在排序、分期和亚型分配任务上的性能，并在真实阿尔茨海默病数据集上进行应用验证。

Result: BEBMS在排序、分期和亚型分配任务上显著优于SuStaIn。在真实阿尔茨海默病数据中，BEBMS的结果与科学共识更加一致。

Conclusion: BEBMS作为SuStaIn的贝叶斯改进版本，在疾病亚型分析中表现出更强的鲁棒性和准确性，为疾病进展建模提供了更可靠的工具。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [59] [SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening](https://arxiv.org/abs/2512.03471)
*Ian Henriques,Lynda Elhassar,Sarvesh Relekar,Denis Walrave,Shayan Hassantabar,Vishu Ghanakota,Adel Laoui,Mahmoud Aich,Rafia Tir,Mohamed Zerguine,Samir Louafi,Moncef Kimouche,Emmanuel Cosson,Niraj K Jha*

Main category: cs.LG

TL;DR: SweetDeep是一个轻量级神经网络，利用三星智能手表在自由生活条件下收集的生理和人口统计数据，实现了82.5%的2型糖尿病检测准确率。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病全球发病率上升，需要可扩展且经济有效的筛查方法。现有诊断方法需要侵入性且昂贵的生化检测，而可穿戴设备为机器学习疾病检测提供了新机会，但先前研究仅限于受控环境。

Method: 开发了SweetDeep紧凑神经网络，使用285名参与者的生理和人口统计数据训练，数据通过三星Galaxy Watch 7在自由生活条件下收集6天，每人每天提供多个2分钟传感器记录，总计约20个记录。模型参数少于3000个。

Result: 在三折交叉验证下，SweetDeep达到82.5%的患者级准确率（82.1%宏F1，79.7%灵敏度，84.6%特异性），预期校准误差5.5%。对低置信度预测（少于10%的患者）弃权后，剩余患者准确率提升至84.5%。

Conclusion: 工程化特征与轻量级架构结合，能够在真实世界可穿戴设备环境中实现准确、快速且可泛化的2型糖尿病检测，为大规模筛查提供了有前景的解决方案。

Abstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.

</details>


### [60] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: JPM是一个概率框架，用于从横断面数据推断混合神经退行性疾病的联合进展，将单疾病轨迹视为部分排序并构建联合进展先验，相比标准单疾病EBM提高了排序准确性。


<details>
  <summary>Details</summary>
Motivation: 标准事件模型假设每个个体只有单一疾病，但神经退行性疾病中混合病理很常见，需要能够处理多种疾病同时进展的模型。

Method: 提出联合进展模型(JPM)，将单疾病轨迹视为部分排序，构建联合进展先验，研究四种变体：Pairwise、Bradley-Terry、Plackett-Luce和Mallows，分析校准性、分离度和锐度三个属性。

Result: 所有JPM变体都具有校准性，分离度接近完美；锐度因变体而异，可通过输入部分排序的简单特征预测。在合成实验中，JPM比强基线SA-EBM提高约21%的排序准确性。在NACC数据中，Mallows变体和基线模型的结果与AD和VaD混合病理进展的现有文献更一致。

Conclusion: JPM为从横断面数据推断混合神经退行性疾病的联合进展提供了有效的概率框架，相比单疾病模型显著提高了准确性，其中Mallows变体在真实数据中表现与现有知识最一致。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [61] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: ATHENA是一个自主代理框架，通过知识驱动的HENA循环（建模为上下文老虎机问题）管理端到端计算研究生命周期，在科学计算和科学机器学习中实现超人类性能，达到10^{-14}的验证误差。


<details>
  <summary>Details</summary>
Motivation: 解决科学计算和科学机器学习中理论概念化与计算实现之间的鸿沟，这是一个主要的瓶颈。当前标准自动化系统无法处理复杂的科学问题，需要超越传统自动化的智能框架。

Method: 引入ATHENA框架，核心是HENA循环（知识驱动的诊断过程，建模为上下文老虎机问题）。系统作为在线学习器分析先前试验，从组合空间中选择结构动作，通过专家蓝图指导，将动作转换为可执行代码生成科学奖励。结合混合符号-数值工作流和人类在环协作。

Result: 实现超人类性能，达到10^{-14}的验证误差。在科学计算中自主识别数学对称性获得精确解析解，在基础模型失败时推导稳定数值求解器。在科学机器学习中解决不适定问题，通过耦合PINNs与FEM等方法解决多物理问题。

Conclusion: ATHENA代表了从实现机制到方法创新的范式转变，通过自主实验室框架加速科学发现，结合人类在环干预可提高一个数量级的结果质量。

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [62] [Modal Logical Neural Networks](https://arxiv.org/abs/2512.03491)
*Antonin Sulc*

Main category: cs.LG

TL;DR: 提出Modal Logical Neural Networks (MLNNs)，这是一个结合深度学习和模态逻辑形式语义的神经符号框架，能够进行必然性和可能性推理。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够整合深度学习能力和形式逻辑推理的框架，特别是能够处理模态逻辑中的必然性和可能性概念，同时保持端到端的可微性。

Method: 基于Kripke语义学，引入专门的模态算子神经元（□和◇），在可能世界集合上操作。框架允许用户固定可及关系来强制执行已知规则，或者通过神经网络参数化可及关系作为归纳特征。整个框架端到端可微，通过最小化逻辑矛盾损失进行学习。

Result: 在四个案例研究中展示了MLNNs的应用：语法护栏、未知的公理检测、多智能体认知信任、以及自然语言谈判中的建设性欺骗检测。实验表明，强制执行或学习可及关系可以提高逻辑一致性和可解释性，而不改变底层任务架构。

Conclusion: MLNNs提供了一个灵活的神经符号框架，能够整合深度学习和模态逻辑推理，既可以强制执行已知逻辑规则，也可以从数据中学习逻辑系统的关系结构，同时保持端到端的可微性和对不一致知识的鲁棒性。

Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.

</details>


### [63] [Physics-Driven Learning Framework for Tomographic Tactile Sensing](https://arxiv.org/abs/2512.03512)
*Xuanxuan Yang,Xiuyang Zhang,Haofeng Chen,Gang Ma,Xiaojie Wang*

Main category: cs.LG

TL;DR: PhyDNN：一种物理驱动的深度学习框架，通过将EIT前向模型嵌入学习目标，改善了电阻抗断层扫描触觉传感的重建质量，减少了伪影并提高了物理合理性。


<details>
  <summary>Details</summary>
Motivation: 电阻抗断层扫描（EIT）因其布线简单和形状灵活而成为大面积触觉传感的有吸引力的解决方案，但其非线性逆问题常导致严重伪影和不准确的接触重建。

Method: 提出PhyDNN框架，将EIT前向模型直接嵌入学习目标，联合最小化预测与真实电导率图的差异，并强制与正向偏微分方程的一致性。设计了可微分前向算子网络来近似非线性EIT响应，实现快速物理引导训练。

Result: 在16电极软传感器上的仿真和真实触觉实验表明，PhyDNN在重建接触形状、位置和压力分布方面持续优于NOSER、TV和标准DNN方法，产生更少伪影、更清晰边界和更高度量分数。

Conclusion: PhyDNN通过结合物理模型和深度学习，提高了EIT触觉传感的重建质量，展示了其在高质量断层触觉传感中的有效性。

Abstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.

</details>


### [64] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 提出自适应稀疏感知框架，结合变分自编码器先验与强化学习进行顺序测量选择，优于传统压缩感知、最优传感器布置和生成模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：压缩感知依赖通用基和随机测量；最优传感器布置使用固定线性基无法适应非线性变化；生成模型压缩感知仍采用次优随机采样。需要能自适应选择测量的方法。

Method: 结合变分自编码器先验与强化学习，通过强化学习顺序选择测量位置，利用变分自编码器提供生成先验，形成自适应稀疏感知框架。

Result: 实验表明该方法在稀疏测量下的重建质量优于传统压缩感知、最优传感器布置和生成模型压缩感知方法。

Conclusion: 自适应稀疏感知框架通过顺序测量选择和生成先验的结合，有效提升了稀疏采样下的重建性能。

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [65] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: 提出DLC方法，通过LoRA插件扩展范式解决类增量学习中的遗忘问题，仅需少量参数即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法存在遗忘问题或稳定性-可塑性困境，扩展方法虽然准确率高但参数增加显著。需要一种高效、参数友好的增量学习方案。

Method: 将基于重放或蒸馏训练的特征提取器作为基础模型，为每个任务使用LoRA注入任务特定残差。推理时聚合带任务特定残差的表示，并引入轻量级加权单元来抑制非目标LoRA插件的干扰。

Result: 在ImageNet-100上，仅需标准ResNet-18 4%的参数，即可实现8%的准确率提升。在固定内存预算下超越现有最优方法。

Conclusion: DLC方法提供了一种即插即用的扩展范式，能高效增强基础方法，在类增量学习中实现了参数效率和性能的良好平衡。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [66] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 本文提出一种针对扩散模型遗忘学习的新攻击方法DiMRA，能够逆转基于微调的遗忘学习效果，同时提出更鲁棒的遗忘学习方法DiMUM来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成合成图像方面表现出色，但存在安全、隐私和版权问题，需要遗忘学习技术来让模型忘记特定训练数据。现有基于微调的遗忘学习方法虽然高效，但存在安全漏洞，可能被攻击者逆转。

Method: 1. 提出DiMRA攻击方法：在无先验知识的情况下，通过在辅助数据集上优化已遗忘的扩散模型，逆转遗忘学习效果，使模型重新生成之前遗忘的内容。
2. 提出DiMUM防御方法：不同于传统遗忘方法，该方法通过记忆替代数据或特征来替换目标遗忘内容，防止生成这些元素。

Result: 1. DiMRA成功逆转了最先进的基于微调的扩散模型遗忘学习方法，揭示了此类技术的脆弱性。
2. DiMUM在保持扩散模型生成性能的同时，显著增强了对DiMRA攻击的鲁棒性。

Conclusion: 基于微调的扩散模型遗忘学习方法存在被攻击者逆转的严重安全漏洞，需要更鲁棒的解决方案。DiMUM通过记忆替代内容的方法，在保持模型性能的同时提供了更强的防御能力。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [67] [When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate](https://arxiv.org/abs/2512.03578)
*Florent Forest,Amaury Wei,Olga Fink*

Main category: cs.LG

TL;DR: MAGNETS是一个用于时间序列外生回归的固有可解释神经网络架构，它通过学习一组可理解的概念来提供透明预测，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列外生回归模型虽然预测性能强，但通常是黑箱模型，难以理解驱动决策的时间模式。现有的后验可解释性技术产生粗糙、嘈杂或不稳定的解释，而固有可解释方法需要概念标注、无法捕捉特征交互、表达能力有限且难以扩展到高维数据。

Method: 提出MAGNETS（Mask-and-AGgregate NEtwork for Time Series），通过掩码聚合学习一组紧凑的人类可理解概念，每个概念对应选定输入特征的掩码聚合，明确揭示哪些特征驱动预测以及何时重要。预测通过透明的加法结构组合这些学习到的概念。

Result: 该方法能够学习无需标注的可理解概念，明确展示特征重要性和时间相关性，提供透明的决策过程洞察。

Conclusion: MAGNETS解决了现有可解释时间序列回归方法的局限性，提供了一种固有可解释的神经网络架构，能够在保持预测性能的同时提供透明推理。

Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.

</details>


### [68] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 该论文为高斯分布下的最优传输和Gromov-Wasserstein对齐提供了全面的闭式解，解决了未中心化高斯分布的对齐问题，并扩展到多边际OT和IGW重心计算，应用于知识蒸馏和异构聚类。


<details>
  <summary>Details</summary>
Motivation: 最优传输和Gromov-Wasserstein对齐是数据科学中比较、转换和聚合异构数据集的重要几何框架，但由于计算成本高，大规模应用通常依赖于高斯分布下的二次成本闭式解。现有文献存在一些空白，限制了这些框架的广泛应用。

Method: 1. 针对可分离希尔伯特空间上未中心化高斯分布的IGW对齐，给出了闭式表达式（需通过酉算子进行二次优化），并推导了紧致的解析上下界；2. 当至少一个高斯分布中心化时，提供完全闭式解；3. 扩展到中心化高斯分布的IGW重心解析解；4. 将具有成对二次成本的高斯多边际OT简化为可处理的优化问题，并提出使用秩不足约束的高效算法。

Result: 填补了高斯分布下OT和IGW对齐理论的多个空白，提供了更全面的闭式解框架，使这些方法能够应用于更广泛的场景，包括未中心化分布和多个分布的情况。

Conclusion: 该工作扩展了高斯分布下最优传输和Gromov-Wasserstein对齐的理论基础，提供了更完整的闭式解，增强了这些几何框架在实际应用中的实用性，特别是在知识蒸馏和异构聚类等任务中。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [69] [Federated Learning and Trajectory Compression for Enhanced AIS Coverage](https://arxiv.org/abs/2512.03584)
*Thomas Gräupl,Andreas Reisenbauer,Marcel Hecko,Anil Rasouli,Anita Graser,Melitta Dragaschnig,Axel Weissenfeld,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.LG

TL;DR: VesselEdge系统利用联邦学习和带宽受限轨迹压缩技术，通过扩展AIS覆盖范围来增强海上态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决海上AIS覆盖范围有限的问题，通过将船舶转变为移动传感器，实现实时异常检测和低带宽环境下的高效数据传输。

Method: 集成M3fed联邦学习模型和BWC-DR-A轨迹压缩算法，优先传输异常数据，在带宽受限条件下优化数据传输。

Result: 初步结果显示VesselEdge能有效提高AIS覆盖范围和态势感知能力，使用历史数据验证了系统有效性。

Conclusion: VesselEdge系统通过联邦学习和轨迹压缩技术，成功扩展了海上AIS覆盖范围，提升了海上态势感知能力。

Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.

</details>


### [70] [Observation-driven correction of numerical weather prediction for marine winds](https://arxiv.org/abs/2512.03606)
*Matteo Peduto,Qidong Yang,Jonathan Giezendanner,Devis Tuia,Sherrie Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Transformer的深度学习架构，通过同化最新现场观测数据来校正全球天气预报系统(GFS)输出，从而改进海洋风预报。


<details>
  <summary>Details</summary>
Motivation: 准确的海洋风预报对于安全航行、船舶路径规划和能源运营至关重要，但由于海洋观测数据稀疏、异构且时间变化大，预报仍然具有挑战性。现有数值天气预报(NWP)模型存在系统误差，需要有效的后处理方法进行校正。

Method: 将风预报重新定义为观测信息化的NWP模型校正问题。提出基于Transformer的深度学习架构，通过掩码和基于集合的注意力机制处理不规则和时间变化的观测集，使用交叉注意力将预测条件化于最近的观测-预报对，采用循环时间嵌入和坐标感知位置表示，实现任意空间坐标的单次推理。

Result: 在大西洋区域使用ICOADS观测数据评估，模型在所有48小时预报时效内都降低了GFS 10米风速RMSE，在1小时预报时效改善45%，48小时预报时效改善13%。空间分析显示在海岸线和航运路线等观测密集区域改进最显著。

Conclusion: 该研究展示了一种实用、低延迟的后处理方法，通过学习校正系统预报误差来补充NWP模型。该架构能自然处理异构观测平台，在单次前向传播中同时生成站点特定预测和流域尺度网格产品。

Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.

</details>


### [71] [CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion](https://arxiv.org/abs/2512.03610)
*Julius Lenz*

Main category: cs.LG

TL;DR: CoGraM是一种多阶段、上下文敏感的神经网络融合方法，通过分层、神经元和权重级别的迭代优化来提升联邦学习中的模型融合效果，相比传统方法（如权重平均和Fisher融合）能显著提高准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习和分布式学习中需要在不重新训练的情况下融合神经网络模型。现有方法如权重平均或Fisher融合通常会导致精度损失，并且在不同的随机种子下表现不稳定，因此需要更有效的融合方法。

Method: CoGraM是一种多阶段、上下文敏感、基于损失的迭代优化方法，在层、神经元和权重级别进行操作。它通过将决策与损失差异和阈值对齐来防止有害更新，并通过回滚机制确保融合质量。

Result: CoGraM能够显著改善融合后的网络性能，解决了Fisher等传统方法的弱点，提高了融合模型的准确性和稳定性。

Conclusion: CoGraM为联邦学习和分布式学习中的神经网络融合提供了一个有效的优化方法，通过多阶段、上下文敏感的迭代优化，显著提升了融合模型的性能，克服了传统方法的局限性。

Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.

</details>


### [72] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: 使用视觉语言模型从视频编码的网格天气数据直接生成航运预报文本，探索多模态基础模型在气象产品服务中的应用


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型具有潜力，但在气象产品和服务生成方面的应用仍处于起步阶段。本文旨在加速这类模型在气象领域的应用和采纳。

Method: 探索性地使用视觉语言模型，将视频编码的网格天气数据直接转换为航运预报文本。

Result: 初步结果显示，这种方法为提升气象企业及更广泛领域的产品效率和服务创新提供了有前景的可扩展技术机会。

Conclusion: 通过将视觉语言模型应用于气象数据到文本的转换，展示了多模态基础模型在气象服务中的创新应用潜力，为提升生产效率和推动服务创新提供了新途径。

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [73] [Conditional updates of neural network weights for increased out of training performance](https://arxiv.org/abs/2512.03653)
*Jan Saynisch-Wagner,Saran Rajendran Sari*

Main category: cs.LG

TL;DR: 提出一种增强神经网络在训练数据与应用数据不相似（如分布外问题、模式/机制转移）时性能的方法，通过权重异常检测、回归建模和权重外推实现时空和跨域外推


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在训练数据与应用数据不相似时的性能下降问题，特别是在分布外问题、模式转移和机制转移等场景中，这在气候科学等实际应用中很常见

Method: 三步骤方法：1) 在训练数据集的合理子集上重新训练神经网络并记录权重异常；2) 选择合理的预测因子，建立预测因子与权重异常之间的回归关系；3) 将权重（从而神经网络）外推到应用数据

Result: 在气候科学的三个用例中展示了该方法，成功实现了神经网络的时间、空间和跨域外推

Conclusion: 该方法能有效增强神经网络在训练数据与应用数据不相似时的性能，为处理分布外问题和模式转移提供了实用解决方案

Abstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.

</details>


### [74] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 提出统一深度学习框架，结合循环时间编码与LSTM-CNN混合架构，用于多步能源预测，在七个预测时间范围均取得改进。


<details>
  <summary>Details</summary>
Motivation: 准确的电力消费预测对需求管理和智能电网运营至关重要。现有方法未能充分结合时间编码、日历特征和混合架构的优势。

Method: 使用正弦余弦编码系统转换基于日历的属性以保持周期结构；采用集成模型包含LSTM、CNN和针对每个预测范围的MLP回归器元学习器；通过相关性分析评估预测相关性。

Result: 在一年全国消费数据集上的实验显示，混合模型在所有七个预测范围均取得更低的RMSE和MAE，优于单个架构和先前方法。

Conclusion: 循环时间表示与互补深度学习结构的结合具有显著优势，这是首个在统一短期能源预测框架中联合评估时间编码、日历特征和混合集成架构的工作。

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [75] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: DSAS是一种动态缩放激活引导框架，可自适应调整现有引导方法的强度，只在检测到不良行为时进行干预，在毒性缓解和效用保持之间实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常对所有输入统一应用干预，导致在不需要引导时模型性能下降。需要一种能自适应判断何时需要引导以及如何引导的方法。

Method: DSAS将"何时引导"与"如何引导"解耦，在生成时计算上下文相关的缩放因子，选择性地调整任何引导方法的强度。该方法可与引导函数联合端到端优化，并适用于文本到图像扩散模型。

Result: DSAS与现有引导方法结合时，在毒性缓解和效用保持的权衡方面持续改进帕累托前沿。在文本到图像扩散模型中，自适应引导可调节特定概念，且计算开销最小，同时提高可解释性。

Conclusion: DSAS提供了一种方法无关的引导框架，通过动态缩放激活引导，只在需要时进行干预，在保持模型效用的同时有效缓解毒性内容，具有广泛适用性和实际价值。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [76] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 提出了一种特征感知的时间调制机制，通过调整特征的统计特性来应对表格数据中的时间分布偏移，平衡模型的泛化性和适应性。


<details>
  <summary>Details</summary>
Motivation: 表格机器学习在现实部署中面临时间分布偏移的挑战，静态模型假设固定映射以确保泛化，而自适应模型可能过度拟合瞬态模式，形成了鲁棒性与适应性之间的困境。

Method: 提出特征感知的时间调制机制，将特征表示条件化于时间上下文，调制统计特性如尺度和偏度，通过跨时间对齐特征语义实现轻量级但强大的适应。

Result: 基准评估验证了该方法在处理表格数据时间偏移方面的有效性，实现了泛化性和适应性之间的平衡。

Conclusion: 通过分析构建有效动态映射的关键因素，发现特征转换策略能够减轻跨时间阶段特征表示的差异，提出的特征感知时间调制机制为处理表格数据时间分布偏移提供了有效解决方案。

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [77] [Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns](https://arxiv.org/abs/2512.03696)
*Mohammad Doost,Mohammad Manthouri*

Main category: cs.LG

TL;DR: 提出QTGNN框架，结合量子嵌入、变分图卷积和拓扑数据分析，用于大规模金融网络中的欺诈交易检测，在NISQ设备上实现稳定训练并达到良好性能。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法难以捕捉复杂交易动态和结构异常，需要结合量子计算、图神经网络和拓扑分析来提升检测能力。

Method: 量子数据嵌入与纠缠增强、变分量子图卷积、高阶拓扑不变量提取、混合量子-经典异常学习、可解释的拓扑归因决策。

Result: 在PaySim和Elliptic金融数据集上，QTGNN在ROC-AUC、精确率和误报率等指标上优于经典和量子基线方法。

Conclusion: QTGNN为金融欺诈检测提供了理论可靠、可解释且实用的解决方案，桥接了量子机器学习、图论和拓扑分析。

Abstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.

</details>


### [78] [Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm](https://arxiv.org/abs/2512.03744)
*Xuhui Lin,Qiuchen Lu*

Main category: cs.LG

TL;DR: 提出一种物理约束的哈密顿学习算法，通过结构不可逆性检测和能量景观重建，识别城市交通系统的隐藏结构损伤，解决了传统表面指标无法检测"虚假恢复"的问题。


<details>
  <summary>Details</summary>
Motivation: 当前城市交通系统韧性评估方法依赖表面恢复指标，无法检测隐藏的结构损伤，导致无法区分真正的恢复和"虚假恢复"（交通指标正常化但系统动力学永久退化）。

Method: 开发了物理约束的哈密顿学习算法，结合结构不可逆性检测和能量景观重建，提取低维状态表示，通过物理约束优化识别准哈密顿结构，并通过能量景观比较量化结构变化。

Result: 对伦敦2021年极端降雨事件的分析显示，虽然表面指标完全恢复，但该算法检测到64.8%的传统监测方法遗漏的结构损伤。

Conclusion: 该框架为主动结构风险评估提供了工具，使基础设施投资能够基于真实的系统健康状况而非误导性的表面指标。

Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.

</details>


### [79] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 研究发现近60个科学模型（涵盖字符串、图、3D原子和蛋白质等模态）在化学系统上具有高度对齐的表示，表明基础模型学习到了物理现实的共同底层表示，但当前模型仍受限于训练数据和归纳偏置，未能编码真正通用的结构。


<details>
  <summary>Details</summary>
Motivation: 理解不同模态和架构的科学模型是否学习到相似的内部表示对于构建可靠泛化的科学基础模型至关重要。虽然在语言和视觉领域已观察到表示收敛现象，但在科学领域尚未得到系统探索。

Method: 分析了近60个科学模型（包括字符串、图、3D原子和蛋白质等不同模态）在化学系统上的表示对齐情况，研究了在不同数据集上训练的模型以及机器学习原子间势能模型在表示空间中的收敛行为。

Result: 1) 不同数据集训练的模型对小分子具有高度相似的表示；2) 机器学习原子间势能模型在性能提升时在表示空间中收敛；3) 发现两种不同机制：在类似训练输入时，高性能模型紧密对齐，弱模型在表示空间中发散到局部最优；在完全不同结构时，几乎所有模型都坍缩到低信息表示。

Conclusion: 表示对齐可作为科学模型基础级泛化能力的定量基准。当前模型仍受训练数据和归纳偏置限制，未能编码真正通用的结构。该工作可追踪物质通用表示随模型规模扩展的出现，并为选择跨模态、物质领域和科学任务的最佳表示迁移模型提供指导。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [80] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 提出条件轨迹编码器，联合学习空间与运动表示，通过几何特征保持起点依赖的不对称性，量化城市形态造成的认知不平等。


<details>
  <summary>Details</summary>
Motivation: 当前城市轨迹分析方法存在方法碎片化：轨迹学习捕捉运动模式但忽略空间上下文，空间嵌入方法编码街道网络但缺失时间动态。存在三个主要差距：(1)缺乏整合时空表示的联合训练，(2)起点无关的处理忽略导航中的方向不对称性，(3)过度依赖辅助数据而非城市空间的基本几何属性。

Method: 引入条件轨迹编码器，使用双向LSTM处理可见性比率和曲率等几何特征，以可学习的起点嵌入为条件，通过对比学习将表示分解为共享的城市模式和起点特定的特征。

Result: 在六个合成城市和北京西城区的真实世界验证表明，城市形态会造成系统性的认知不平等。该方法为城市规划者提供了评估体验公平性的定量工具，为建筑师提供了布局决策认知影响的见解，并为导航系统实现了起点感知分析。

Conclusion: 提出的框架将城市导航分解为共享认知模式和起点特定的空间叙事，能够跨起始位置量化认知不对称性，为解决城市分析中的方法碎片化问题提供了有效的解决方案。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [81] [Deep Unfolding: Recent Developments, Theory, and Design Guidelines](https://arxiv.org/abs/2512.03768)
*Nir Shlezinger,Santiago Segarra,Yi Zhang,Dvir Avrahami,Zohar Davidov,Tirza Routtenberg,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 深度展开（deep unfolding）是一种将迭代优化算法转换为结构化、可训练的机器学习架构的框架，旨在结合经典优化方法的理论保证与机器学习的数据驱动能力。


<details>
  <summary>Details</summary>
Motivation: 经典迭代优化算法虽然具有可解释性和理论保证，但通常依赖代理目标函数、需要仔细的超参数调优，且计算延迟较大。而机器学习虽然具有强大的数据驱动建模能力，但缺乏优化驱动推理所需的结构、透明度和效率。深度展开旨在弥合这两个范式之间的差距。

Method: 深度展开通过系统地将迭代优化算法转换为结构化、可训练的机器学习架构来实现。文章介绍了四种代表性的设计范式，并讨论了由其迭代性质产生的独特训练方案。该方法将优化求解器转换为机器学习模型。

Result: 深度展开框架提供了将优化与机器学习相结合的统一视角。最近的理论进展为展开的优化器建立了收敛性和泛化保证。比较性研究表明了在复杂性、可解释性和鲁棒性方面的相对权衡。

Conclusion: 深度展开是一个有前景的框架，它通过将迭代优化算法转换为可训练的机器学习架构，成功地将经典优化的理论优势与机器学习的数据驱动能力相结合，为信号处理中的优化驱动推理提供了新的可能性。

Abstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.

</details>


### [82] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 该研究提出了一种基于机器学习的方法，将智能手机和智能手表的运动传感器数据转换为不同身体活动的似然比，用于法医调查中的活动识别。


<details>
  <summary>Details</summary>
Motivation: 智能手机和智能手表在日常生活中的普及提供了丰富的用户行为信息，其内置的运动传感器产生的数字痕迹为法医调查人员了解个人身体活动提供了机会。

Method: 开发基于机器学习的方法，将数字痕迹转换为不同身体活动的似然比；使用新的NFI_FARED数据集进行评估，该数据集包含四种不同iPhone型号的19种活动标记数据；扩展方法以同时分析多个活动（或活动组）并创建活动时间线。

Result: 在171个可能的活动配对中，该方法能够为167个配对产生有用的似然比系统；成功创建了活动时间线以辅助法医调查的早期和后期阶段；数据集和代码已公开以促进进一步研究。

Conclusion: 基于机器学习的数字痕迹分析方法能够有效识别身体活动，为法医调查提供有价值的工具，公开数据集和代码将推动该领域的进一步发展。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [83] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 提出基于流程挖掘的两阶段临床路径建模方法，利用一致性检查扩展临床路径知识库，针对疾病变体或组合创建更具体的模型。


<details>
  <summary>Details</summary>
Motivation: 临床路径的手动建模困难且难以反映不同疾病变体或组合的最佳实践，需要自动化方法来扩展和改进临床路径知识库。

Method: 两阶段建模方法：第一阶段从历史数据中提取治疗流程模型；第二阶段将新数据与参考模型进行一致性检查，根据检查结果扩展知识库，为新的疾病变体或组合创建更具体的模型。

Result: 使用Synthea基准数据集（模拟SARS-CoV-2感染和COVID-19并发症）验证，方法能以95.62%的AUC精度扩展临床路径知识库，同时保持67.11%的弧度简单性。

Conclusion: 提出的流程挖掘方法能够有效扩展临床路径知识库，为不同疾病变体和组合创建更具体的模型，提高临床路径的适应性和实用性。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [84] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 本文提出EfficientECG模型，基于EfficientNet构建轻量级心电图分类模型，并引入跨注意力机制融合多导联和多特征信息，实现高精度、轻量化的心电图分析。


<details>
  <summary>Details</summary>
Motivation: 心电图作为快速、无创且信息丰富的诊断信号，在心脏异常检测中有广泛应用。现有ECG模型误诊率高，需要开发能够自动提取特征、减少医疗工作者负担的深度学习技术。

Method: 首先基于EfficientNet设计EfficientECG分类模型，处理高频长序列多导联ECG数据；然后提出跨注意力特征融合模型，整合多导联ECG数据与性别、年龄等多特征信息。

Result: 在代表性ECG数据集上的评估验证了模型在精度、多特征融合和轻量化方面的优越性，优于现有最先进方法。

Conclusion: 提出的深度学习方法能够有效管理和分析ECG数据，构建准确快速的诊断模型，显著减轻医疗工作者负担，为ECG分析提供了新的技术方案。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [85] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文系统研究了深度强化学习在动态算法配置中的应用，针对(1+(λ,λ))-GA算法的种群规模参数控制问题，揭示了DDQN和PPO的两大挑战：可扩展性退化和学习不稳定性，并提出了自适应奖励偏移机制等解决方案。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习已被用于算法配置，但将其应用于动态算法配置(DAC)仍面临挑战且需要大量领域专业知识。本文旨在通过系统研究深度强化学习在DAC中的表现，识别关键问题并提供解决方案。

Method: 以控制(1+(λ,λ))-GA在OneMax问题上的种群规模参数为案例，系统分析DDQN和PPO算法。提出自适应奖励偏移机制解决探索不足问题，使用无折扣学习解决规划视野覆盖问题，并进行超参数依赖性分析。

Result: 发现DDQN和PPO存在可扩展性退化和学习不稳定性两大挑战。自适应奖励偏移机制显著改善了DDQN的探索能力，无折扣学习有效解决了规划视野覆盖问题。配备自适应奖励偏移的DDQN性能接近理论最优策略，样本效率比现有DAC方法高出数个数量级。

Conclusion: 深度强化学习在DAC中面临系统性挑战，但通过针对性解决方案可以克服。DDQN配合自适应奖励偏移机制在DAC中表现出色，而PPO存在根本性方差问题需要重新设计。该方法为DAC提供了高效且可扩展的解决方案。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [86] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出一种基于logprobs的廉价LLM API监控方法，仅需单个token输出即可检测微小模型变化，比现有方法敏感1000倍且成本低


<details>
  <summary>Details</summary>
Motivation: LLM API提供商需要保持模型一致性以确保下游应用可靠性和研究可复现性，但现有审计方法成本过高无法定期监控广泛可用的LLM API，导致模型更新在实践中基本未被监控

Method: 利用LLM log probabilities（logprobs）虽然通常是非确定性的，但可作为成本效益高的连续监控基础。采用基于每个token logprob平均值的简单统计测试，仅需请求单个token输出

Result: 该方法能检测小至一次微调步骤的模型变化，比现有方法更敏感，同时成本降低1000倍。引入TinyChange基准来衡量审计方法对小型现实模型变化的敏感性

Conclusion: 基于logprobs的简单统计测试提供了一种经济高效的LLM API连续监控解决方案，解决了现有审计方法成本过高的问题，确保模型一致性监控的可行性

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [87] [Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission](https://arxiv.org/abs/2512.03819)
*Junlin Chang,Yubo Han,Hnag Yue,John S Thompson,Rongke Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度联合信源信道编码的3D点云语义无线传输框架，通过预测接收端语义正交特征池的组合权重实现紧凑表示和鲁棒重建，在带宽受限场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度传感器的普及，点云获取门槛降低，但点云数据传输面临带宽限制和噪声干扰的挑战。现有方法在高带宽下表现良好，但在带宽受限场景下性能下降，需要更高效的语义传输方案。

Method: 1) 基于DeepJSCC的语义无线传输框架；2) 发送端预测接收端语义正交特征池的组合权重，而非传输原始特征；3) 折叠式解码器将2D网格变形为3D点云，保持流形连续性和几何保真度；4) 使用Chamfer Distance和正交正则化进行训练。

Result: 在ModelNet40数据集上测试不同SNR和带宽：1) 高带宽下与SEPT性能相当；2) 带宽受限场景下明显优于SEPT；3) PSNR和CD指标均有持续改进；4) 消融实验证实正交化和折叠先验的有效性。

Conclusion: 提出的语义传输框架通过正交特征池和折叠解码器，实现了高效的点云压缩和鲁棒重建，在带宽受限场景下显著优于现有方法，为点云无线传输提供了有效的解决方案。

Abstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.

</details>


### [88] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO：结合条件风险理论与分布价值建模的新RL框架，用于LLM后训练，在噪声监督下平衡鲁棒性与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界LLM部署常面临噪声或不完整监督，现有方法（如RFQI、CQL、PPO、GRPO）要么忽视泛化能力，要么产生过于保守的策略，在不同场景下表现不稳定

Method: DVPO结合条件风险理论与分布价值建模：学习token级价值分布提供细粒度监督，应用非对称风险正则化调整分布尾部——压缩下尾抑制噪声负偏差，扩展上尾保持探索多样性

Result: 在多轮对话、数学推理和科学QA等广泛实验中，DVPO在噪声监督下持续优于PPO、GRPO和基于鲁棒Bellman的PPO

Conclusion: DVPO展示了在现实世界LLM后训练中的潜力，能更好地平衡鲁棒性与泛化能力

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [89] [Scalable Decision Focused Learning via Online Trainable Surrogates](https://arxiv.org/abs/2512.03861)
*Gaetano Signorelli,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出一种基于无偏估计器的替代方法，加速决策导向学习，减少训练时昂贵的内层求解器调用，同时保持解的质量。


<details>
  <summary>Details</summary>
Motivation: 传统训练的估计器用于决策支持系统可能导致次优解，而决策导向学习虽然能解决此问题，但训练时计算成本高、可扩展性差。

Method: 使用无偏估计器构建高效替代函数，替代昂贵的损失函数评估。该方法适用于黑盒设置，能补偿优化模型简化并考虑补救行动，同时提供局部置信度信息以便必要时切换回备选方法。

Result: 该方法显著减少了昂贵的内层求解器调用次数，解的质量与现有最先进技术相当。

Conclusion: 提出的基于无偏估计器的替代方法有效解决了决策导向学习的可扩展性问题，在保持解质量的同时大幅提升了训练效率。

Abstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.

</details>


### [90] [Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models](https://arxiv.org/abs/2512.03882)
*Haidong Kang,Wei Wu,Hanling Wang*

Main category: cs.LG

TL;DR: 本文提出ACraft方法，利用大语言模型自动生成针对少样本类增量学习(FSCIL)的攻击方法，无需人工专家参与，显著降低攻击成本并超越传统攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有FSCIL研究主要关注提升学习效果，而忽视了其安全性问题。传统人工设计的攻击方法（如PGD、FGSM）要么无法有效攻击基类，要么依赖大量专家知识导致成本高昂，因此需要专门针对FSCIL的攻击方法。

Method: 提出ACraft方法：1）利用大语言模型自动发现针对FSCIL的最优攻击方法；2）引入基于近端策略优化(PPO)的强化学习来优化大语言模型的推理，通过建立正反馈机制让LLM生成更好的攻击方法。

Result: 在主流的基准测试中，ACraft方法显著降低了最先进FSCIL方法的性能，大幅超越了人工专家设计的攻击方法，同时保持了最低的攻击成本。

Conclusion: 本文首次系统研究了FSCIL的安全性问题，提出的ACraft方法能够自动生成高效攻击，揭示了FSCIL系统在实际部署中面临的安全风险，为后续防御研究奠定了基础。

Abstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.

</details>


### [91] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 该论文为模糊单纯集提供了概率论解释框架，将UMAP中的模糊权重解释为随机尺度下Vietoris-Rips滤过的边际分布，并展示了如何基于此框架推导新的降维方法。


<details>
  <summary>Details</summary>
Motivation: 模糊单纯集在降维和流形学习中（特别是UMAP）很重要，但现有的代数拓扑定义缺乏概率解释，使其与这些领域常用的理论框架脱节。

Method: 提出一个框架，将模糊单纯集解释为单纯集上概率测度的边际分布。具体展示了UMAP的模糊权重源于随机尺度下采样Vietoris-Rips滤过的生成模型，并连接了模糊单纯集与面偏序集上的概率模型。

Result: 该框架为模糊单纯集提供了统一的概率理论基础，阐明了UMAP在该框架中的角色，并能够系统性地推导新的降维方法（如使用Čech滤过和三元组采样的UMAP推广）。

Conclusion: 概率视角为模糊单纯集提供了坚实的理论基础，澄清了UMAP的数学基础，并开启了基于此框架设计新降维方法的可能性。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [92] [Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations](https://arxiv.org/abs/2512.03923)
*Xiang Rao,Yina Liu,Yuxuan Shen*

Main category: cs.LG

TL;DR: 提出离散变量量子-经典物理信息神经网络（DV-QCPINN）解决油藏渗流PDE问题，相比传统数值方法和经典PINNs，具有参数效率高、高维表达能力强、非线性拟合优的特点。


<details>
  <summary>Details</summary>
Motivation: 油藏渗流PDE求解对油气田开发优化和生产预测至关重要。传统数值方法存在网格依赖误差和计算成本高的问题，而经典PINNs在参数效率、高维表达和强非线性拟合方面面临瓶颈。

Method: 提出DV-QCPINN，将经典预处理/后处理网络与离散变量量子核心集成，利用量子叠加和纠缠增强高维特征映射，同时嵌入物理约束确保解的一致性。测试了三种量子电路拓扑（级联、交叉网格、交替）。

Result: 数值实验表明，QCPINNs比经典PINNs使用更少参数实现高预测精度。交替拓扑在非均质单相流和两相BL方程模拟中表现最佳，级联拓扑在对流-扩散-吸附耦合的组分流中表现最优。

Conclusion: 验证了QCPINN在油藏工程应用中的可行性，弥合了量子计算研究与油气工程工业实践之间的差距。

Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.

</details>


### [93] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: DiVAE是一种轻量级VAE正则化器，通过将VAE先验概率与数据估计密度对齐，改善潜在空间与数据空间密度结构的匹配。


<details>
  <summary>Details</summary>
Motivation: 传统VAE将潜在变量匹配到简单先验分布，忽略了数据空间中的密度结构。这导致潜在空间不能很好地反映数据分布特性，影响分布对齐、先验覆盖和OOD不确定性校准。

Method: DiVAE在ELBO中添加了一个鲁棒的、精度加权的惩罚项，使编码器按数据空间密度比例分配后验质量，并推动可学习先验向高密度区域靠近。该方法计算开销极小。

Result: 在合成数据集上，DiVAE：(i)改善了潜在对数密度与其真实对应物的分布对齐；(ii)提高了先验覆盖；(iii)获得了更好的OOD不确定性校准。在MNIST上，DiVAE改善了先验与外部密度估计的对齐，提供了更好的可解释性，并提高了可学习先验的OOD检测能力。

Conclusion: DiVAE是一种有效的轻量级正则化方法，通过将VAE先验与数据密度对齐，显著改善了潜在空间表示的质量、分布对齐性能和OOD检测能力。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [94] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文综述了文本数据集蒸馏领域的发展历程，从最初借鉴视觉领域方法到形成独立研究方向，涵盖了基于Transformer的方法、离散文本生成、大规模解码器模型等里程碑，并指出了该领域在基准标准化、处理文本离散性、复杂任务和实际应用方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 文本数据集蒸馏的研究动机源于需要将大型文本数据集压缩为小型合成数据集，同时保持相似的训练效果。与视觉领域相比，文本数据集蒸馏研究相对较少，且文本模态的特殊性（如离散性）带来了独特挑战，需要专门的研究方法。

Method: 本文采用文献综述方法，系统回顾了文本数据集蒸馏的发展历程。重点分析了不同的蒸馏策略，包括：1）从视觉领域迁移的方法；2）基于Transformer模型的方法；3）离散合成文本生成技术；4）扩展到超过10亿参数的大规模解码器模型。

Result: 综述发现文本数据集蒸馏经历了从借鉴视觉方法到形成独立研究分支的发展过程。主要里程碑包括：Transformer模型的应用、离散合成文本的生成、大规模解码器模型的扩展。然而，该领域仍处于成熟阶段，面临基准标准化不足、文本离散性处理困难、复杂任务应对能力有限等挑战。

Conclusion: 文本数据集蒸馏是一个正在发展的研究领域，虽然取得了显著进展，但仍需在基准标准化、处理文本离散性、应对复杂任务和展示实际应用价值等方面进一步改进。该综述为未来研究提供了方向指引。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [95] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: GFP通过结合多步流匹配策略和蒸馏单步演员，实现选择性模仿数据集中的高价值动作而非盲目模仿所有状态-动作对，在离线强化学习中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习中的行为正则化方法对所有动作一视同仁，无法区分高价值和低价值动作，限制了策略性能的提升。

Method: 提出引导流策略(GFP)，将多步流匹配策略与蒸馏单步演员耦合。演员通过加权行为克隆指导流策略专注于模仿数据集中的高价值动作，而流策略则约束演员保持与数据集最佳转移对齐的同时最大化价值函数。

Result: 在OGBench、Minari和D4RL基准测试的144个状态和像素任务中达到最先进性能，特别是在次优数据集和挑战性任务上取得显著提升。

Conclusion: GFP通过演员和流策略的相互引导机制，有效区分并专注于数据集中的高价值动作，克服了传统行为正则化的局限性，在离线强化学习中实现了卓越的性能。

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [96] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: 提出一种无需训练的高效方法，将政策违规检测视为分布外检测问题，通过白化技术处理隐藏激活，使用欧几里得范数作为合规分数，在政策基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感领域（法律、金融、医疗）的部署，组织需要可靠机制检测内部政策违规。现有内容审核框架局限于安全领域，缺乏对组织细微政策的鲁棒性，而LLM-as-a-judge和微调方法延迟高且缺乏可解释性。

Method: 将政策违规检测视为分布外检测问题，受白化技术启发，对模型隐藏激活应用线性变换以去相关并标准化为零均值和单位方差，在变换空间中使用欧几里得范数作为合规分数检测违规。仅需政策文本和少量示例样本。

Result: 在具有挑战性的政策基准上取得最先进结果，超越了现有护栏和微调推理模型。方法轻量、易于部署，为组织提供实用且统计基础的政策感知监督框架。

Conclusion: 为组织提供了一种实用且统计基础的政策感知监督框架，推进了可部署AI治理的广泛目标。该方法无需训练、高效，仅需政策文本和少量示例，在政策违规检测方面表现出色。

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [97] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 提出PEGP框架，将交通流物理模型嵌入高斯过程中，解决稀疏观测下的交通状态估计问题，相比纯数据驱动方法具有更好的泛化能力和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 在低渗透率、空间稀疏的车辆探测数据下，纯数据驱动方法缺乏物理解释且泛化能力差，而物理模型难以整合不确定性和捕捉真实交通复杂性。现有结合方法依赖惩罚调优且缺乏不确定性校准，对模型误设敏感。

Method: 提出物理嵌入高斯过程(PEGP)，设计基于经典交通流模型(LWR和ARZ)的多输出核函数，通过线性化微分算子的显式应用构建物理结构，将领域知识整合到数据驱动方法中。

Result: 在HighD和NGSIM数据集上的实验显示，PEGP相比非物理基线方法有持续改进。PEGP-ARZ在稀疏观测下更可靠，PEGP-LWR在密集观测下误差更低。消融研究表明PEGP-ARZ残差与物理模型更一致且产生校准的不确定性，而PEGP-LWR残差更正交且产生几乎恒定的方差场。

Conclusion: PEGP框架成功结合了物理先验和不确定性量化，为交通状态估计提供了可靠支持，解决了现有方法在稀疏观测下的局限性。

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [98] [Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics](https://arxiv.org/abs/2512.04006)
*Connall Garrod,Jonathan P. Keating,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 该论文分析了交叉熵损失在多类分类中的优化动力学，首次证明了梯度流在两层线性神经网络上会收敛到神经崩溃几何，并构建了全局收敛的Lyapunov函数。


<details>
  <summary>Details</summary>
Motivation: 现有理论通常用平方损失替代交叉熵损失或局限于凸模型，无法捕捉交叉熵损失的真实动态。交叉熵损失和平方损失产生根本不同的优化动态，而凸线性模型无法捕捉非凸优化的复杂性。

Method: 分析一个规范的两层线性神经网络，使用标准基向量作为输入。通过Hadamard初始化对角化softmax算子，冻结权重矩阵的奇异向量，将动态完全简化为奇异值的演化。构建显式的Lyapunov函数来证明全局收敛。

Result: 首次证明了梯度流在交叉熵损失下会收敛到神经崩溃几何。尽管存在虚假临界点，但通过Lyapunov函数建立了全局收敛性。Hadamard初始化技术为分析交叉熵训练动态开辟了新途径。

Conclusion: 该工作为理解交叉熵损失在非凸优化中的动态提供了理论基础，证明了神经崩溃几何的收敛性，并提出了Hadamard初始化这一关键技术，为更广泛地分析交叉熵训练动态开辟了道路。

Abstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.

</details>


### [99] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 提出首个差分隐私算法，其隐私验证成本显著低于训练成本，在随机凸优化任务中实现近乎最优的隐私-效用权衡


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私算法的验证计算量与训练计算量相当，数据提供者和公众缺乏高效验证DP保证的方法，需要降低验证成本

Method: 通过私有化最小化一系列正则化目标函数，仅使用标准DP组合边界，在DP随机凸优化中实现紧致的隐私-效用权衡

Result: 获得首个DP-SCO算法，具有近乎最优的隐私-效用权衡，且验证成本远低于训练成本，显著减少大型数据集上的验证开销

Conclusion: 该方法首次实现了差分隐私算法验证成本的大幅降低，为数据提供者提供了更实用的DP保证验证手段

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [100] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 本文从信息论角度解释了为什么在单域数据集上训练的模型在OOD检测中会灾难性失败，揭示了域特征坍缩现象，并提出通过域过滤保留域信息来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的OOD检测方法在单域数据集训练时会出现灾难性失败（如MNIST上FPR@95仅53%），本文旨在从信息论角度首次为这一现象提供理论解释。

Method: 1) 从信息瓶颈优化理论证明单域监督学习必然导致域特征坍缩（I(x_d; z)=0）；2) 使用Fano不等式量化实际场景中的部分坍缩；3) 引入Domain Bench基准验证理论；4) 提出通过域过滤保留域信息（I(x_d; z)>0）来解决失败模式。

Result: 理论证明单域训练会导致模型完全丢弃域特定信息，仅依赖类别特征，从而在OOD检测中失败。实验验证通过域过滤保留域信息可以解决这一问题，为信息论框架提供了强有力证据。

Conclusion: 本文解释了单域监督学习在OOD检测中的根本局限性，揭示了信息瓶颈优化导致的域特征坍缩现象，对迁移学习以及何时微调与冻结预训练模型具有更广泛的意义。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [101] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: MarkTune是一种用于开放权重语言模型的理论化、基于策略的微调框架，通过将水印信号作为奖励同时正则化文本质量下降，显著改善了GaussMark的质量-可检测性权衡，使开放权重模型的水印性能接近推理时水印水平。


<details>
  <summary>Details</summary>
Motivation: 开放权重语言模型对水印技术提出了严峻挑战，因为一旦模型权重公开，就无法强制执行推理时干预。现有技术如GaussMark虽然能通过小幅度修改权重嵌入可检测信号，但通常需要在检测能力和生成质量之间做出权衡，检测能力往往不如推理时水印。

Method: 提出MarkTune框架，将GaussMark水印信号作为奖励函数，同时通过正则化防止文本质量下降。该方法在模型表示空间中进行更细粒度、水印感知的权重更新，通过基于策略的微调优化质量-可检测性权衡。

Result: MarkTune显著改善了GaussMark的质量-可检测性权衡，将开放权重模型的水印性能推近推理时水印水平。该方法对改写和微调攻击具有鲁棒性，并展现出强泛化能力：在一个数据集上微调的模型在未见数据集上仍保持显著的水印检测能力。

Conclusion: MarkTune为开放权重语言模型嵌入鲁棒、高质量水印提供了一种通用策略，通过理论化的基于策略微调框架，在保持生成质量的同时显著提升水印检测能力，解决了开放权重模型水印的关键挑战。

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [102] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 提出一种离散更新规则的量化训练方法，避免对连续更新进行量化，为具有内在离散结构的模型开辟高效训练新途径


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要巨大的计算资源，量化训练通过低比特整数表示训练组件来降低计算需求，但现有方法通常依赖于对实值更新进行离散化

Method: 引入离散更新规则方法，避免对连续更新进行量化设计，建立此类离散方案的收敛保证，并提出多项式更新规则作为具体示例

Result: 建立了离散更新方案的收敛性理论保证，并通过实证评估支持了多项式更新规则的有效性

Conclusion: 这种离散更新视角为高效训练开辟了新途径，特别适用于具有内在离散结构的模型，提供了量化训练的替代方案

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [103] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: 提出Eval Factsheets框架，为AI系统评估提供结构化文档标准，解决当前评估方法缺乏系统化文档的问题


<details>
  <summary>Details</summary>
Motivation: 当前AI领域基准测试激增，但评估方法缺乏像Datasheets和Model Cards那样的结构化文档框架，导致可复现性、透明度和决策制定方面存在挑战

Method: 提出Eval Factsheets框架，包含五个核心维度：Context（评估者与时间）、Scope（评估内容）、Structure（评估构建方式）、Method（工作原理）、Alignment（可靠性/有效性/鲁棒性），并实现为包含强制和推荐元素的问卷

Result: 通过多个基准测试的案例研究，证明Eval Factsheets能够有效捕捉从传统基准到LLM-as-judge方法等多样化评估范式，同时保持一致性和可比性

Conclusion: 希望Eval Factsheets能被纳入现有和新发布的评估框架中，从而提高透明度和可复现性

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


### [104] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 开发一个网约车比价网站，通过API获取Ola、Uber、Rapido的价格数据，为用户提供最优出行选择


<details>
  <summary>Details</summary>
Motivation: 用户在选择网约车服务时面临困难，难以找到既经济又高效的出行方案，需要提高服务透明度和用户体验

Method: 构建Web应用程序，使用Python后端通过API获取多家网约车公司（Ola、Uber、Rapido）的实时价格数据，进行比价分析

Result: 开发了能够为用户提供网约车价格比较和最优选择的系统，解决了API数据获取、Android模拟器、Appium和位置比较等技术挑战

Conclusion: 该项目提高了网约车服务的透明度，增强了出行效率，为用户提供了更好的体验，实现了成本效益和时间优化的平衡

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


### [105] [Learning Steerable Clarification Policies with Collaborative Self-play](https://arxiv.org/abs/2512.04068)
*Jonathan Berant,Maximillian Chen,Adam Fisch,Reza Aghajani,Fantine Huot,Mirella Lapata,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 训练可调控的AI助手策略，通过自博弈学习在不确定情况下决定何时猜测用户意图、枚举可能意图或提问澄清，策略可根据不同成本参数调整行为


<details>
  <summary>Details</summary>
Motivation: AI助手需要智能策略来处理模糊查询，但现有策略缺乏上下文适应性，无法根据用户偏好、设备限制（如小屏幕或语音场景）等因素动态调整响应方式

Method: 使用自博弈训练可调控策略：两个智能体分别模拟用户和AI助手，生成包含模糊查询的对话；模型输入澄清问题和每个生成词的成本数值，通过强化自训练（ReST）最大化成本惩罚后的准确率奖励

Result: 训练出的策略可根据提供的成本参数可预测地调整行为，获得更高的奖励和准确率；方法还能泛化到训练时未观察到的成本数值

Conclusion: 通过自博弈和强化自训练可以开发出可调控的AI助手不确定性管理策略，这些策略能根据上下文成本因素自适应调整，并在未见过的成本参数上保持泛化能力

Abstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [106] [AI/ML in 3GPP 5G Advanced - Services and Architecture](https://arxiv.org/abs/2512.03728)
*Pradnya Taksande,Shwetha Kiran,Pranav Jha,Prasanna Chaporkar*

Main category: cs.ET

TL;DR: 3GPP Release 19在SA技术规范组中引入了AI/ML相关技术进展，包括AI赋能网络优化和网络支持AI应用两大范式


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML技术在各行业的广泛应用，3GPP从Release 18开始将AI/ML集成到5G高级系统中，Release 19需要进一步推进AI/ML在移动网络中的标准化应用

Method: 通过3GPP SA技术规范组的标准制定工作，在Release 19中引入两大AI/ML范式：AI赋能网络（如资源优化）和网络支持AI（如图像识别应用）

Result: Release 19标准化了AI/ML在5G高级系统中的关键技术特征，为移动网络智能化提供了标准框架，支持网络优化和AI应用部署

Conclusion: 3GPP Release 19在AI/ML标准化方面取得重要进展，为5G高级系统的智能化发展奠定了基础，同时为Release 20及未来的AI/ML集成铺平道路

Abstract: The 3rd Generation Partnership Project (3GPP), the standards body for mobile networks, is in the final phase of Release 19 standardization and is beginning Release 20. Artificial Intelligence/ Machine Learning (AI/ML) has brought about a paradigm shift in technology and it is being adopted across industries and verticals. 3GPP has been integrating AI/ML into the 5G advanced system since Release 18. This paper focuses on the AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP. The advancements relate to two paradigms: (i) enhancements that AI/ML brought to the 5G advanced system (AI for network), e.g. resource optimization, and (ii) enhancements that were made to the 5G system to support AI/ML applications (Network for AI), e.g. image recognition.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [107] [Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas](https://arxiv.org/abs/2512.03565)
*Luis Gall,Samuel James Newcome,Fabio Alexander Gratl,Markus Mühlhäußer,Manish Kumar Mishra,Hans-Joachim Bungartz*

Main category: cs.DC

TL;DR: 该论文研究通过SIMD向量化优化分子动力学模拟中的粒子对力计算，重点关注粒子值加载到向量寄存器的顺序，并扩展AutoPas的动态调优机制以在运行时选择最优向量化顺序。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟在原子尺度上提供有价值见解，但计算成本高。先前研究表明最优MD算法可能在运行时变化，因此需要研究如何通过SIMD向量化优化粒子对力计算，特别是考虑模拟特定参数（如粒子密度）和邻居识别算法的影响。

Method: 探索多种SIMD向量化技术，重点关注粒子值加载到向量寄存器的顺序优化。研究模拟特定参数（粒子密度、邻居识别算法）对性能的影响。扩展AutoPas的动态调优机制，使其能够在运行时选择最优向量化顺序。

Result: 基准测试表明，在运行时考虑不同粒子相互作用顺序相比AutoPas先前方法，能够显著提高力计算的性能。动态调优机制能够根据模拟条件选择最优向量化策略。

Conclusion: SIMD向量化顺序对分子动力学模拟性能有重要影响，通过动态调优机制在运行时选择最优向量化顺序可以显著提升性能，这为粒子模拟库AutoPas提供了有效的优化方法。

Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.

</details>


### [108] [TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity](https://arxiv.org/abs/2512.03416)
*Ruiqi Lai,Hongrui Liu,Chengzhi Lu,Zonghao Liu,Siyu Cao,Siyang Shao,Yixin Zhang,Luo Mai,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: TokenScale：针对LLM服务中预填充/解码分离架构的自动扩缩框架，通过Token Velocity指标和Convertible Decoders创新，显著提升SLO达成率并降低成本。


<details>
  <summary>Details</summary>
Motivation: LLM服务中的预填充/解码分离架构虽然提高了资源利用率，但难以应对现代工作负载的突发性。现有自动扩缩策略（如AIBrix和DistServe）依赖滞后指标（GPU利用率、请求数），导致对负载峰值反应缓慢，造成TTFT和TPOT SLO违规以及过度配置成本。

Method: 1. 提出Token Velocity指标：统一预填充、网络和解码阶段的工作速率，作为系统背压的领先指标，实现主动扩缩。2. 设计Convertible Decoders：允许解码GPU在流量高峰时动态执行预填充任务，创建快速响应缓冲区，吸收突发流量并消除新预填充器的初始化延迟。

Result: 在GPU集群上使用生产轨迹评估，TokenScale将SLO达成率从50-88%提升至80-96%，相比DistServe、BlitzScale和AIBrix等先进系统，成本降低4-14%。

Conclusion: 通过结合预测性指标和灵活的系统设计，TokenScale显著提升了分离式LLM服务基础设施的性能和效率，解决了现有自动扩缩策略在应对突发工作负载时的性能不匹配问题。

Abstract: The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.

</details>


### [109] [Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks](https://arxiv.org/abs/2512.03487)
*Zhen Wang,Bin Lin,Qiang,Ye*

Main category: cs.DC

TL;DR: 提出面向空天海一体化网络的双边缘辅助计算卸载与资源分配方案，利用无人机和低轨卫星为海上自主船舶提供并行计算服务，通过优化算法最小化能耗


<details>
  <summary>Details</summary>
Motivation: 空天海一体化网络中海事自主船舶的计算需求日益增长，需要高效的计算卸载方案来满足低延迟和高能效要求

Method: 采用双边缘架构（无人机+低轨卫星），提出基于交替优化和分层方法的联合优化算法，优化卸载模式、卸载量和计算资源分配

Result: 通过仿真验证了所提方案相比基准算法在能效方面的有效性和优越性

Conclusion: 提出的双边缘辅助计算卸载与资源分配方案能够显著降低空天海一体化网络的能耗，同时满足延迟约束

Abstract: In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.

</details>


### [110] [FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management](https://arxiv.org/abs/2512.03644)
*Bohan Zhao,Yuanhong Wang,Chenglin Liu,Jiagi Pan,Guang Yang,Ruitao Liu,Tingrui Zhang,Kai Luo,Wei Xu*

Main category: cs.DC

TL;DR: FFTrainer利用剩余网络容量快速保存和加载状态，减少LLM训练中的回滚和加速恢复，相比现有检查点方法减少98%恢复时间


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型集群规模扩大，节点故障、长时间恢复和大检查点文件降低了训练效率。异步检查点要么触发昂贵的回滚，要么增加过高开销

Method: 利用剩余网络容量快速保存和加载训练状态，防止回滚并加速恢复过程

Result: 相比现有检查点方法，恢复时间减少高达98%，GPU利用率损失减少高达68%，且不影响正常训练

Conclusion: FFTrainer通过有效利用网络资源，为大语言模型训练提供了高效且鲁棒的系统解决方案

Abstract: Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.

</details>


### [111] [On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs](https://arxiv.org/abs/2512.03697)
*Rafael Ravedutti Lucio Machado,Jan Eitzinger,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 该论文分析了在Fritz和Alex HPC集群上使用合成基准测试和Gromacs软件包进行能效评估时遇到的挑战，提出了最佳实践建议。


<details>
  <summary>Details</summary>
Motivation: 研究高性能计算集群中能效分析的挑战，特别是在使用不同硬件架构（Intel Ice Lake/Sapphire Rapids CPU和Nvidia A40/A100 GPU）和并行计算框架时，为未来的能效研究提供指导。

Method: 在Fritz和Alex HPC集群上使用MPI并行化，在完整的CPU插槽和GPU上运行合成基准测试和Gromacs软件包，使用Likwid和Nvidia性能分析工具收集指标和测量数据。

Result: 展示了使用不同硬件配置的性能指标和测量结果，揭示了实验和分析过程中遇到的各种挑战和陷阱，包括工具使用、数据收集和结果解释方面的问题。

Conclusion: 论文总结了高性能计算能效分析中的关键挑战，并为未来的能效研究提出了最佳实践建议，帮助研究人员避免常见陷阱并提高分析质量。

Abstract: This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.

</details>


### [112] [Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods](https://arxiv.org/abs/2512.03825)
*Aingeru Ramos,Jose A Pascual,Javier Navaridas,Ivan Coluzza*

Main category: cs.DC

TL;DR: 该论文提出了一种使用OpenMP和CUDA并行化的Metropolis-Hastings并行回火算法实现，在CPU和GPU上分别实现了52倍和986倍的加速，为未来量子算法实现提供基准。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在处理复杂构型空间时采样精度不足，并行回火技术虽然提高了精度但计算成本显著增加，需要通过并行化来抵消这种计算开销。

Method: 使用OpenMP和CUDA分别在现代CPU和GPU上实现Metropolis-Hastings并行回火算法的并行化版本。

Result: OpenMP版本在48核CPU上实现了最大52倍加速，CUDA版本在GPU上实现了986倍加速。

Conclusion: 提出的并行实现显著提高了MCMC/PT算法的计算效率，为研究更大模型提供了可能，同时为未来量子算法实现建立了性能基准。

Abstract: Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.

</details>


### [113] [OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference](https://arxiv.org/abs/2512.03927)
*Liujianfu Wang,Yuyang Du,Yuchen Pan,Soung Chang Liew,Jiacheng Liu,Kexin Chen*

Main category: cs.DC

TL;DR: OD-MoE：一种无需专家缓存的分布式MoE推理框架，通过按需加载专家参数，在边缘设备上实现高效推理


<details>
  <summary>Details</summary>
Motivation: 混合专家模型在内存受限的边缘设备上部署面临挑战，现有的专家卸载方法虽然将专家参数存储在CPU内存中，但GPU内存中为专家缓存保留的空间利用率不足，相比密集LLM仍有优化空间

Method: 提出OD-MoE框架，基于两个关键技术：1）在分布式边缘节点间并行化专家加载和专家计算；2）超准确的模拟预测器，在专家计算进行时提前多层预测专家激活

Result: 实验显示：1）OD-MoE达到99.94%的专家激活预测准确率，远超现有方法；2）仅使用1/3的GPU内存就能实现约75%的完全GPU缓存MoE部署的解码速度

Conclusion: OD-MoE通过消除专家缓存需求，使MoE推理能够在GPU内存小于1GB的边缘节点上运行，为低成本物联网设备在边缘部署MoE模型铺平了道路

Abstract: Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.

</details>


### [114] [A Chronological Analysis of the Evolution of SmartNICs](https://arxiv.org/abs/2512.04054)
*Olasupo Ajayi,Ryan Grant*

Main category: cs.DC

TL;DR: 该论文对智能网卡(SNICs)进行了历时15年(2010-2024)的系统分析，基于370篇文献研究了SNICs的演进、制造商、用例和应用领域，旨在澄清其确切用途和适用性。


<details>
  <summary>Details</summary>
Motivation: 随着互联网设备的普及和对更快网络访问需求的增长，传统网卡已升级为智能网卡(SNICs)。然而，尽管SNICs日益流行，其确切用途和适用性仍存在争议，特别是随着加速器的加入使SNICs能够分担主机CPU任务，这些争议更加复杂。因此需要系统分析来澄清SNICs的发展和应用现状。

Method: 采用历时分析方法，收集并分析了2010年至2024年间发表的370篇相关文章，对SNICs进行系统性研究。分析内容包括SNICs的演进历程、主要制造商、实际用例以及应用领域等方面。

Result: 通过对15年间370篇文献的分析，论文揭示了SNICs的演进轨迹、主要制造商格局、实际应用场景以及在不同领域的应用情况，为理解SNICs的发展现状提供了实证依据。

Conclusion: 该研究通过系统性的历时分析，阐明了智能网卡(SNICs)的发展历程和应用现状，为学术界和工业界理解SNICs的演进、制造商格局、用例和应用领域提供了重要参考，有助于澄清关于SNICs适用性的争议。

Abstract: Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.

</details>
