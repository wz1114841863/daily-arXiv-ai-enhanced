<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 184]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的自主VR测试代理，能够实时导航VR环境并与人机交互，解决了VR内容质量保证的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的人工VR内容质量保证方法劳动密集且无法适应行业快速增长，而现有的自动化测试方法难以应对VR的高维感官输入和实时性能要求。

Method: 使用增强型动作分块变换器从人类演示中学习，预测多步动作序列，并引入动态可调滑动视界来平衡响应性和精确度。

Result: 在商业VR游戏上评估显示，VRScout仅用有限训练数据就达到专家级性能，并在消费级硬件上保持60 FPS的实时推理。

Conclusion: VRScout为自动化VR游戏测试提供了一个实用且可扩展的框架，可直接应用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [2] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)进行特征选择和定向引导，在Llama-3 8B模型上实现了安全性能提升18.9%同时实用性提升11.1%，突破了传统安全-实用性权衡的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要调整模型权重且过程昂贵，同时缺乏系统性的特征选择方法和原则性的安全-实用性权衡评估。

Method: 使用对比提示方法从AI生成提示数据集中高效选择最佳引导特征，采用不同引导特征和引导强度通过稀疏自编码器进行定向引导。

Result: 在Llama-3 8B模型上，安全性能提升18.9%，实用性同时提升11.1%。

Conclusion: 通过原则性特征选择方法识别最优特征，定向SAE引导能够克服传统安全-实用性权衡。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [3] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 机器遗忘技术虽然能有效移除不良知识，但会意外造成"知识漏洞"——标准基准测试无法检测到的良性知识损失。


<details>
  <summary>Details</summary>
Motivation: 发现现有机器遗忘技术在移除不良内容时，可能无意中造成标准基准测试无法捕捉的良性知识损失，需要重新评估遗忘技术的知识保留效果。

Method: 提出测试用例生成框架，探索遗忘内容邻近区域和更广泛潜在失败区域的模型表现。

Result: 评估显示遗忘模型在高达98.7%的测试用例中产生无关或荒谬回答，而这些内容在预训练模型中是可回答的。

Conclusion: 需要重新思考评估机器遗忘知识保留的传统方法，超越标准静态基准测试。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [4] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出了Skip-Block Routing (SBR)框架，用于Transformer神经算子，通过路由机制学习token复杂度排名，在推理时根据复杂度动态调整计算资源分配，减少约50%计算量同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子在求解偏微分方程时存在计算开销大的问题，主要原因是模型对所有区域采用统一计算成本，而物理场不同区域的复杂度差异很大，造成计算资源浪费。

Method: SBR框架包含路由机制学习token复杂度和排名，在推理时根据排名决定后续层中传递的token数量，让模型将更多计算能力集中在复杂区域。

Result: SBR能无缝集成到各种神经算子中，减少约50%的浮点运算量，推理速度提升达2倍，且不牺牲精度。

Conclusion: SBR框架通过动态分配计算资源有效解决了神经算子在工程应用中的计算效率问题，为大规模PDE求解提供了高效解决方案。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [5] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出基于神经架构搜索（NAS）的自动化框架，用于发现平衡计算效率、预测性能和泛化能力的时间序列模型，用于能源生产的全局多步短期预测。


<details>
  <summary>Details</summary>
Motivation: 能源领域需要预测准确性和运行时效率，但手动配置复杂方法耗时且易出错，同时需要考虑时间动态性和泛化能力。

Method: 设计包含高效组件的搜索空间，提出考虑时间上下文性能泛化和搜索空间探索的新目标函数，通过NAS自动发现轻量级架构。

Result: 在能源生产时间序列上，NAS发现的轻量级架构集成模型在效率和准确性方面均优于Transformer等最先进技术及预训练预测模型。

Conclusion: NAS框架能够自动发现高效且准确的能源预测模型，解决了手动配置的复杂性和泛化问题。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [6] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化(SSPO)方法，利用少量配对偏好标签和大量未配对样本同时学习，显著降低对齐成本。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法严重依赖大量配对反馈数据，导致资源消耗巨大，需要更高效的数据利用方法。

Method: 基于理论证明的最优奖励阈值，对未配对数据进行伪标注，然后利用这些伪标签从大规模未配对数据中提取潜在偏好。

Result: 实验表明SSPO具有显著的数据效率，仅用1%的UltraFeedback数据训练的Llama3-8B-Instruct模型，性能超过使用10%数据训练的强基线。

Conclusion: SSPO能够有效从大规模未配对数据中提取偏好信息，在保持人类对齐的同时大幅降低数据获取成本。

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [7] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 本文验证了物理信息神经网络(PINNs)在求解各类工程和生物动力学系统ODE问题中的预测能力，通过系统评估其在基准问题上的准确性、训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高刚度、冲击、不规则域、奇异摄动、高维或边界不连续等挑战性问题时往往难以收敛，而PINNs通过将物理定律嵌入学习过程，为处理这些复杂数值场景提供了强大方法。

Method: 使用经典ODE问题作为受控测试平台，系统评估PINNs框架；分析基准问题的存在性和唯一性；通过精心平衡数据损失、初始条件损失和残差损失分量，并系统调整网络深度、层宽、激活函数、学习率等超参数；嵌入先验知识并在网络架构上施加硬约束。

Result: PINNs能够收敛到正确解，但需要适当平衡损失函数分量；系统超参数调优对获得准确解至关重要；嵌入先验知识和施加硬约束显著增强了PINNs的预测能力。

Conclusion: PINNs虽然不能解决所有问题，但通过将物理定律直接嵌入学习过程，能够在复杂问题中实现优越结果，为处理传统数值方法难以解决的挑战性问题提供了有效途径。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [8] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出ReLaX-Net架构，通过时间复用层来扩展物理神经网络深度，提高参数利用效率


<details>
  <summary>Details</summary>
Motivation: 物理神经网络在规模上落后数字神经网络数个数量级，需要参数高效利用的方法来提升性能

Method: 采用层间时间复用方案，仅需在现有PNN基础上添加快速开关，实现参数共享和网络深度扩展

Result: 在图像分类和自然语言处理任务中验证，ReLaX-Net相比传统RNN/DNN在相同参数数量下性能更优

Conclusion: ReLaX-Net通过简单修改显著提升PNN计算性能，展现出良好的扩展性

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [9] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了DynBERG模型，将Graph-BERT与GRU结合，用于动态金融交易网络的欺诈检测，在比特币交易数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-BERT模型主要针对静态图和无向边设计，而金融交易网络具有动态演化和有向边的特点，需要专门的方法来处理这些挑战。

Method: 结合Graph-BERT与GRU层来捕捉多时间步的时序演化，并修改底层算法以支持有向边，适用于动态金融交易分析。

Result: 在Elliptic比特币交易数据集上评估，在加密货币市场重大事件前后均表现出色，优于EvolveGCN和GCN等现有方法。

Conclusion: DynBERG模型能有效适应市场重大变化，GRU组件在建模金融交易时序动态方面发挥关键作用。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [10] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出了一种基于自监督学习的时空图神经网络框架，用于改进多变量天气预报，在ERA5和MERRA-2数据集上优于传统数值天气预报模型和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统固有的时空复杂性，准确稳健的天气预报仍然是一个基本挑战。

Method: 整合图神经网络进行空间推理，采用自监督预训练方案进行表示学习，并引入时空适应机制以增强不同预报时长的泛化能力。

Result: 在ERA5和MERRA-2再分析数据集上的广泛实验表明，该方法相比传统数值天气预报模型和近期深度学习方法具有更优性能。北京和上海的定量评估和可视化分析证实了模型捕捉细粒度气象模式的能力。

Conclusion: 该框架为未来数据驱动的天气预报系统提供了一个可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [11] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出了FLoRA方法，一种融合前向-后向适配器的参数高效微调技术，结合了LoRA和并行适配器的优点，在保持相似参数预算的同时显著提升了准确性和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增长，参数高效微调(PEFT)变得日益重要。虽然已有多种PEFT方法被研究，但该领域仍有很大自由度未被探索。

Method: FLoRA融合了流行的LoRA和并行适配器的思想，通过将前向和后向适配器融合到基础模型的现有投影层中来最小化延迟。

Result: 实验结果表明，在相似参数预算下，提出的FFB适配器在准确性和延迟方面都显著优于常用的LoRA方法。

Conclusion: FLoRA为LLM的下游任务微调提供了一种高效的参数高效微调解决方案，在性能和效率方面都有显著提升。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [12] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文分析了DoRA方法的成功机制，发现其通过增加权重更新矩阵的奇异值熵来提升性能，但存在计算开销大的问题。作者提出了一个统一的PEFT框架，并基于此开发了Pre-Diag和SORA两种新方法，在性能和效率上均优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然提升了性能，但其机制不明确且计算开销大，需要更高效且性能更好的参数高效微调方法。

Method: 首先分析DoRA的成功机制，然后将其重新表述为更高效的矩阵形式，提出统一的PEFT框架，并在此框架下开发了Pre-Diag和SORA两种新方法。

Result: 在自然语言理解和生成任务上的广泛实验表明，提出的方法在性能和效率上都优于LoRA和DoRA。

Conclusion: 通过分析DoRA机制并建立统一框架，成功开发出更高效且性能更好的PEFT方法，为参数高效微调提供了新的设计思路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [13] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 本文评估了特征引导分析（FGA）在MNIST和LSC数据集上的适用性，发现FGA在基准测试中比文献结果具有更高的精确度，同时神经网络架构、训练和特征选择对FGA的召回率有显著影响但对精确度影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络决策原因对安全关键应用至关重要，但现有特征引导方法在工业环境中的适用性需要更多实证证据。

Method: 在MNIST和LSC数据集基准上评估FGA的有效性，分析神经网络架构、训练过程和特征选择对FGA性能的影响。

Result: FGA在基准测试中比文献结果具有更高的精确度；神经网络架构、训练和特征选择对FGA的召回率有显著影响，但对精确度影响可忽略。

Conclusion: FGA在解释神经网络行为方面具有潜力，但需要仔细考虑神经网络架构和特征选择以获得更好的召回率。

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [14] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种基于二次形式加权的训练目标QDF，通过考虑标签自相关效应和异质任务权重，改进了时间序列预测模型的训练效果


<details>
  <summary>Details</summary>
Motivation: 现有训练目标如均方误差将每个未来步骤视为独立、等权重的任务，这导致两个问题：(1)忽略未来步骤间的标签自相关效应，造成训练目标偏差；(2)无法为不同预测任务设置异质权重，限制了预测性能

Method: 提出二次形式加权训练目标，其中权重矩阵的非对角元素考虑标签自相关效应，非均匀对角元素匹配不同未来步骤预测任务的最优权重。开发了QDF学习算法，使用自适应更新的二次形式权重矩阵训练预测模型

Result: 实验表明QDF有效提升了各种预测模型的性能，达到了最先进的结果

Conclusion: QDF通过同时解决标签自相关效应和异质任务权重问题，显著改进了时间序列预测模型的训练效果

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [15] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在Intel Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在边缘设备上实现高效持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI系统在开放世界中面临的数据分布变化和新类别出现的挑战，克服传统离线训练在功率受限环境中的不足。

Method: 采用脉冲神经网络架构，包含事件驱动稀疏局部学习、自归一化三因子学习规则、神经发生和元可塑性机制。

Result: 在OpenLORIS少样本学习实验中，达到与回放方法相当的准确率，同时实现70倍速度提升和5600倍能效提升。

Conclusion: 脑启发算法与神经形态硬件的协同设计能够突破传统精度-效率权衡，为未来边缘AI系统提供新路径。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [16] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出了SpatialTraceGen框架，通过蒸馏大型教师模型的推理过程来生成高质量的多步骤、多工具推理轨迹数据集，解决了视觉语言模型在复杂空间推理任务中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理方面表现不佳，需要问题分解和策略性工具使用。微调小型可部署模型是高效路径，但缺乏高质量的分步推理数据成为主要瓶颈。

Method: 开发SpatialTraceGen框架，从大型教师模型蒸馏推理过程生成多跳多工具推理轨迹。关键创新是自动化验证器，可扩展地确保每个推理步骤的保真度，替代昂贵的人工标注。

Result: 在CLEVR-Humans基准测试中，验证器引导的过程使轨迹平均质量得分提高17%，质量方差降低超过40%。

Conclusion: SpatialTraceGen提供了专家轨迹数据集，为有效微调和样本高效的离线强化学习提供了结构化的分步工具使用示例。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [17] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究了在高度波动的边缘环境中联邦学习的模型准确性与公平性之间的权衡，通过实证评估比较了不同客户端选择算法在公平性、模型性能和时间方面的表现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘智能中具有重要作用，但边缘环境的波动性和客户端能力的异质性给实现高准确性和公平参与带来了挑战，需要研究准确性与公平性之间的权衡关系。

Method: 使用三个基准数据集（CIFAR10、FashionMNIST和EMNIST），对基于公平性的客户端选择算法（如RBFF和RBCSF）与随机和贪婪选择算法进行了广泛的实证评估。

Result: 研究发现，更公平的客户端选择算法虽然能为客户端提供更好的参与机会，但在波动环境中可能导致全局训练速度变慢。

Conclusion: 在波动边缘环境中存在公平性与性能、公平性与速度之间的权衡，需要进一步研究来解决公平客户端选择策略中的现有缺陷。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [18] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在无人机热成像分割任务中的实际应用效果，比较了不同FL方法与集中式学习的性能差异，包括准确性、训练时间、通信开销和能耗等指标。


<details>
  <summary>Details</summary>
Motivation: 联邦学习能够绕过传统集中式机器学习的数据共享限制，特别适合无人机热成像这种数据自然分布的场景，因为隐私和技术限制使得数据无法集中存储。

Method: 在实际部署场景中评估FL算法，比较多种FL方法与集中式学习基线，分析客户端控制和工作流与服务器控制工作流的差异。

Result: 研究发现FL方法在保持模型性能的同时能够有效保护数据隐私，但在通信开销和训练时间方面存在权衡。

Conclusion: 这项工作为理解FL方法在无人机成像分割任务中的实际应用和局限性提供了有价值的参考。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [19] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出了模块重要性采样方法MISA，通过将Transformer层划分为更小的模块并基于重要性采样来激活模块，相比层级优化能更有效地减少内存需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有层级优化方法虽然能节省内存，但忽略了层内模块的重要性差异，导致性能次优，且内存节省有限。

Method: 将每个Transformer层划分为更小的模块，为每个模块分配重要性分数，使用加权随机采样机制激活模块，可证明降低梯度方差。

Result: 建立了在非凸和随机条件下的O(1/√K)收敛率，内存分析显示MISA优于现有基线方法，多个学习任务的实验验证了有效性。

Conclusion: MISA通过模块级优化克服了层级优化的局限性，在保证收敛的同时实现了更好的内存效率和性能。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [20] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 开发了一种基于决策树的自动化方法，用于识别和解释OthelloGPT模型中编码游戏规则的MLP神经元，发现约一半神经元可通过紧凑的规则决策树准确描述，并通过干预实验验证了这些模式的因果相关性。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT为可解释性研究提供了理想测试平台，模型足够复杂以展示丰富的计算模式，同时基于规则的游戏逻辑使得逆向工程具有意义。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活决策路径并转换为人类可读的逻辑形式，通过针对性干预验证因果相关性。

Result: 发现第5层约一半神经元（913/2048）可由紧凑规则决策树准确描述（R²>0.7），其余可能参与分布式或非规则计算。干预实验显示特定模式的合法移动预测能力下降5-10倍。

Conclusion: 该方法成功识别了模型中有意义的计算结构，提供了将游戏行为映射到实现神经元的Python工具，为未来可解释性研究提供资源。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [21] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: EVINGCA是一种基于密度-方差的聚类算法，将聚类形成视为最近邻图上的自适应演化过程，通过局部统计反馈替代固定密度阈值。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在限制性假设：K-Means和高斯混合假设凸形、类高斯簇，而DBSCAN和HDBSCAN能捕获非凸性但对参数高度敏感。

Method: 在最近邻图上通过广度优先搜索扩展根图，使用持续更新的局部距离和形状统计来指导聚类过程，具有对数线性复杂度。

Result: 在各种合成、真实世界、低维和高维数据集上，EVINGCA与基线方法相比表现出竞争力。

Conclusion: EVINGCA通过自适应演化过程和局部统计反馈，提供了一种更灵活、鲁棒的聚类方法。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [22] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 该研究探讨了预训练模型的不同层如何反映大脑在语言理解过程中的分层处理机制，通过比较wav2vec2和CLIP模型的嵌入与脑电图信号的对应关系。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声学到丰富多模态关联的分层处理机制启发，研究旨在确定预训练模型中哪些层最能反映大脑的这种分层处理过程。

Method: 使用在自然语音感知期间记录的EEG数据，通过岭回归和对比解码评估wav2vec2和CLIP模型的嵌入与大脑活动的对齐程度，测试了三种策略：单独层、渐进连接和渐进求和。

Result: 研究发现结合多模态、层感知的表征可能更接近解码大脑如何理解语言，而不仅仅是声音处理。

Conclusion: 多模态层感知表征的结合可能使我们更接近解码大脑如何将语言理解为体验而不仅仅是声音。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [23] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: 提出了TR-GRPO方法，通过基于token概率的权重调节来解决GRPO中低概率token梯度过度放大的问题，从而稳定训练并提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在关键问题：低概率token因其梯度幅度大而主导梯度更新，导致训练不稳定并抑制高概率token的学习贡献

Method: 在GRPO基础上引入token级权重调节，权重与模型预测概率正相关，降低低概率token权重，强调高概率token

Result: 在逻辑、数学和智能体推理等RLVR任务上，TR-GRPO始终优于GRPO

Conclusion: 调节token贡献在RL训练中很重要，TR-GRPO是增强LLM推理能力的稳健框架

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [24] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种无需显式域标签的领域泛化方法，通过自动发现训练数据中的潜在域，将未见目标域表示为这些潜在域的组合，从而提高视觉语言模型在域偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法大多依赖可能不可用或模糊的域标签，因此需要研究无需显式域标签的DG设置，使模型能在真实应用中更好地应对域偏移。

Method: 在图像特征上进行潜在域聚类，并根据输入图像与每个潜在域的相似度融合域特定的文本特征，实现跨域知识自适应迁移。

Result: 在四个基准测试上的实验表明，该方法相比基于VLM的基线模型取得了持续的性能提升。

Conclusion: 该策略为在域偏移下提高模型鲁棒性提供了新的见解，证明了无需显式域标签的领域泛化方法的有效性。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [25] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 该研究比较了大型语言模型与贝叶斯优化在约束多目标回归任务中的表现，发现在逆设计问题上，微调的LLMs可以成为计算快速的替代方案，但专门的BO框架在收敛保证方面仍占优势。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在约束连续高维数值空间中的优化能力，这些任务并非LLMs的原始设计目标，但在材料信息学等领域的逆设计中具有重要应用价值。

Method: 采用贝叶斯优化框架（BoTorch Ax和qEHVI）与微调的LLMs（包括WizardMath-7B）进行对比研究，LLMs通过参数高效微调方法进行训练，将问题构建为带自定义输出头的回归任务。

Result: BoTorch qEHVI实现了完美收敛（GD=0.0），而最佳LLM（WizardMath-7B）的世代距离为1.21，显著优于传统BoTorch Ax基线（GD=15.03）。

Conclusion: 专门的BO框架在保证收敛方面仍是性能领导者，但微调的LLMs被验证为有前景的计算快速替代方案，为AI驱动优化领域提供了重要比较指标。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [26] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 使用小波特征提取和无监督聚类的方法来检测数字的奇偶性，实现了69.67%的分类准确率，展示了信号处理技术在离散符号领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索将经典信号处理技术应用于纯粹离散符号领域的问题，特别是奇偶性检测这一经典问题，以验证这些技术能否揭示离散数据中的潜在结构。

Method: 将整数转换为小波域表示，从中提取多尺度统计特征，然后使用k-means算法进行无监督聚类，而不依赖模运算。

Result: 在没有任何标签监督的情况下，实现了69.67%的分类准确率，特征空间揭示了奇数和偶数之间的有意义结构差异。

Conclusion: 经典信号处理技术能够揭示离散符号领域中的潜在结构，为特征工程和聚类在非传统机器学习问题中的应用提供了新的视角，可能连接符号推理和基于特征的学习。

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [27] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 该论文研究视觉语言模型理解视觉信息几何结构的能力，通过将象形文字识别构建为程序合成任务，训练VLM将光栅图像反编译为由贝塞尔曲线组成的程序。模型在仅训练现代汉字的情况下，能够零样本重建古代甲骨文，表明模型获得了可迁移的几何语法。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型理解视觉信息底层几何结构的能力，而不仅仅是语义能力。象形文字结合了视觉形式和符号结构，是测试这种能力的理想案例。

Method: 将视觉识别挑战构建为数学领域的程序合成任务，训练VLM将光栅图像反编译为由贝塞尔曲线组成的可执行程序，模型作为"视觉反编译器"。

Result: 模型性能优于包括GPT-4o在内的强零样本基线。关键发现是仅在现代汉字上训练的模型能够零样本重建古代甲骨文，表明获得了抽象可迁移的几何语法。

Conclusion: 模型超越了像素级模式识别，获得了更结构化的视觉理解形式，即抽象可迁移的几何语法，这为VLM的几何结构理解能力提供了有力证据。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [28] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个R包，提供模块化、可扩展的框架，用于构建可重现的机器学习流程工作流，特别关注算法公平性领域。


<details>
  <summary>Details</summary>
Motivation: 算法公平性领域快速发展，新指标、缓解策略和方法不断涌现，现有工具要么过于狭窄只关注单一干预，要么将可重现性和可扩展性视为次要考虑而非核心设计原则。

Method: 引入统一架构，包含标准化引擎用于数据分割、执行、预处理、训练、处理中、后处理、评估和报告。每个引擎封装一个方法任务，通过轻量级接口通信，确保工作流透明、可审计且易于扩展。

Result: 通过将公平性方法构建为可互换的引擎，研究人员可以在整个建模流程中集成、比较和评估干预措施。该架构还可推广到可解释性、鲁棒性和合规性指标，无需核心修改。

Conclusion: 虽然受公平性需求驱动，但flowengineR最终为任何需要可重现性、透明度和可扩展性的工作流环境提供了通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [29] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出了Fix-GCN模型，通过固定点迭代图卷积网络来防御对抗攻击，利用频谱调制滤波器选择性衰减高频分量，保护图结构信息。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击对图神经网络构成严重威胁，特别是在图结构和节点特征易受操纵的任务中，需要开发鲁棒的防御机制。

Method: 引入固定点迭代图卷积网络，使用频谱调制滤波器，通过固定点迭代推导特征传播规则，选择性衰减高频分量，保留低频结构信息。

Result: 在多个基准图数据集上的实验表明，该模型能有效抵御对抗攻击，保持网络性能。

Conclusion: Fix-GCN提供了一个灵活高效的框架，能在保留必要图信息的同时减轻对抗操纵的影响，具有实际应用价值。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [30] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 该论文探讨了使用序数回归技术自动预测纸笔RPG中怪物等级的方法，以替代目前耗时的手动评估方式。


<details>
  <summary>Details</summary>
Motivation: 纸笔RPG市场快速增长，出版商需要设计新对手并评估其挑战等级，但目前只能通过耗时的手动测试和专家评估来完成，缺乏自动化方法。

Method: 构建专用数据集，开发基于人类思维的基准模型，使用序数回归技术评估最先进的方法，并设计基于领域知识的专门评估程序。

Result: 论文提供了各种机器学习算法与人类基准方法的比较，并建立了专门的评估框架来评估模型性能。

Conclusion: 该研究为纸笔RPG出版商提供了自动化的怪物等级预测方法，能够显著提高效率并减少资源消耗。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [31] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: MaGNet是一个基于Mamba双超图的股票预测网络，通过三个关键创新：MAGE块进行上下文时序建模、2D时空注意力模块融合多变量特征和跨股票依赖关系、双超图框架分别捕捉细粒度因果依赖和市场范围模式，在六个主要股票指数上表现出优越的预测性能和投资回报。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉时序依赖和动态股票间交互，往往忽略横截面市场影响、依赖静态相关性、对节点和边采用统一处理，并混淆多样关系。

Method: 提出MaGNet网络，包含：(1) MAGE块：利用双向Mamba和自适应门控机制进行上下文时序建模，集成稀疏专家混合层适应不同市场条件；(2) 2D时空注意力模块：精确融合多变量特征和跨股票依赖关系；(3) 双超图框架：包含捕捉细粒度因果依赖的TCH和建模市场范围模式的GPH。

Result: 在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进方法，并具有稳健的风险管理能力。

Conclusion: MaGNet通过集成Mamba时序建模、2D时空注意力和双超图框架，有效解决了股票趋势预测中的时序依赖和动态交互挑战，实现了优越的性能。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [32] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 提出了Agent-REINFORCE框架，通过LLM代理增强的强化学习方法，在固定计算预算下搜索最优的多LLM协作图架构。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽略了不同任务需要不同的最优架构和模型组合。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，将REINFORCE流程映射为采样-反馈-更新，使用文本反馈作为梯度来更新概率图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: 该方法成功解决了测试时扩展中多LLM协作图搜索的挑战，为不同任务找到计算最优的模型组合和架构。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [33] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 本文提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解耦和保留机制来处理嵌入漂移和决策边界偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要关注单域内的任务增量和类别增量场景，而图域增量学习（Domain-IL）在多图域间的模型更新问题尚未被探索，这对图基础模型的发展至关重要。

Method: GraphKeeper方法包含三个核心组件：1）域特定的参数高效微调及域内和域间解耦目标；2）无偏差知识保留以维持稳定决策边界；3）针对不可观测图域的域感知分布判别以获得精确嵌入。

Result: 实验表明GraphKeeper在6.5%~16.6%的性能提升上优于第二名方法，且遗忘效应可忽略不计。该方法可无缝集成到各种代表性图基础模型中。

Conclusion: GraphKeeper是首个解决图域增量学习问题的方法，通过知识解耦和保留机制有效缓解了灾难性遗忘，展现了广泛的适用潜力。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [34] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出了一种基于条件标签生成对抗网络的无监督损伤检测和数字孪生方法，该方法无需系统健康状态的先验信息，在Z24桥梁基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于人工智能的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时存在预测不确定性，需要开发无需先验信息的鲁棒损伤检测框架。

Method: 使用条件标签生成对抗网络，通过将不同损伤级别的测量数据作为输入，强制模型条件收敛到两个不同的损伤状态，比较收敛得分来识别不同损伤状态。

Result: 该方法能够准确捕获健康测量数据上的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 提出的无监督框架在Z24桥梁基准测试中验证有效，能够同时生成不同损伤状态的数字孪生数据，并支持模式识别和机器学习数据生成。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [35] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短期记忆网络和卷积神经网络在动态结构载荷识别中的性能，并与基于物理的残差卡尔曼滤波器进行对比，重点考察小数据集条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中由于测试数据稀少或结构模型不可识别导致的动态载荷识别不确定性问题和预测精度差的问题。

Method: 使用三种深度学习网络（GRU、LSTM、CNN）和残差卡尔曼滤波器（RKF），分别对模拟结构在顶部激振、加州建筑在地震激励、以及IASC-ASCE基准问题中的冲击和瞬时载荷进行识别分析。

Result: 不同方法在不同载荷场景下表现各异，RKF在物理参数可识别的情况下优于神经网络方法。

Conclusion: 各种方法在不同载荷识别场景中各有优势，没有单一最优方法，RKF在物理参数明确可识别的情况下表现最佳。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [36] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个虚拟化多LoRA框架，统一了LoRA微调和推理，通过虚拟化模块和优化计算流实现了高效的多适配器管理和计算。


<details>
  <summary>Details</summary>
Motivation: 现有方法在统一LoRA微调和推理方面存在差距，需要一种能够同时支持高效微调和推理的框架。

Method: 提出虚拟化模块隔离PEFT修改并支持多适配器共享基础模型，以及优化的计算流设计，在正向传播中合并微调和推理路径。

Result: 在三个任务设置上的实验表明，Loquetier在性能和灵活性上均优于现有基线，推理任务吞吐量达到最先进共服务系统的3.0倍，统一微调和推理任务的SLO达成率比PEFT高46.4倍。

Conclusion: Loquetier成功统一了LoRA微调和推理，提供了高效的多适配器管理框架，显著提升了性能和灵活性。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [37] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出了一种混合框架，从噪声轨迹数据中自动发现守恒量，包含神经ODE、Transformer和符号-数值验证器三个组件，显著优于直接在轨迹数据上操作的基线方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中识别守恒定律是科学进步的关键，但目前从观测数据中识别这些不变量仍然是一个重大挑战。

Method: 集成三个组件：(1) 神经ODE学习系统动力学的连续模型，(2) Transformer基于学习到的向量场生成符号候选不变量，(3) 符号-数值验证器为候选不变量提供强数值验证。

Result: 在典型物理系统上测试表明，该框架显著优于直接在轨迹数据上操作的基线方法。

Conclusion: 这项工作证明了分离的'先学习后搜索'方法在从不完美数据中发现数学原理方面的鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [38] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0是参数规模从70亿到720亿的开源具身大脑模型家族，通过DPPO框架和metaloop机制实现智能自适应学习，性能提升20.3%，超越100B级开源模型10.6%。


<details>
  <summary>Details</summary>
Motivation: 将强大智能嵌入各种具身体现中，构建大规模开源具身多模态大脑模型。

Method: 采用DPPO（刻意练习策略优化）框架，通过RL-Refine-Diagnose-SFT循环的metaloop机制，从40亿+token原始数据集中蒸馏高质量数据集，在1000+ A800 GPU集群上训练。

Result: 相比基础模型性能提升20.3%，超越100B级开源模型10.6%，在知名具身基准测试中与领先专有系统相当。

Conclusion: Pelican-VL 1.0是目前最大规模的开源具身多模态大脑模型，通过数据力量与智能自适应学习机制的深度融合实现了卓越性能。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [39] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: MeixnerNet是一种新颖的谱图神经网络，使用离散正交Meixner多项式作为滤波器，解决了传统连续域多项式与离散图结构不匹配的问题，具有更好的超参数鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统谱GNN使用连续正交多项式（如Chebyshev）作为滤波器，这与离散的图结构存在理论不匹配，可能导致性能不佳和对超参数设置的脆弱性。

Method: 提出MeixnerNet架构，使用离散正交Meixner多项式M_k(x; β, c)作为滤波器，并将两个形状参数β和c设为可学习参数，通过Laplacian缩放和逐基LayerNorm解决数值不稳定性问题。

Result: 在K=2的最优设置下，MeixnerNet在3个基准测试中的2个上表现优于ChebyNet基线；更重要的是，MeixnerNet对多项式阶数K的变化具有异常鲁棒性，而ChebyNet对此高度脆弱。

Conclusion: MeixnerNet通过使用离散正交多项式解决了谱GNN中连续域滤波器与离散图结构的不匹配问题，在保持竞争力的同时显著提高了对超参数变化的鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [40] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个可持续液体冷却基准环境，用于强化学习控制策略，优化高性能计算系统的能效液体冷却。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载增加，高密度数据中心需要液体冷却进行热管理，而基于机器学习的控制器对于提高能效和可靠性、促进可持续性至关重要。

Method: 基于橡树岭国家实验室Frontier超级计算机冷却系统的高保真数字孪生，提供详细的Modelica端到端模型，涵盖从站点级冷却塔到数据中心机柜和服务器刀片组。通过Gymnasium接口，RL代理优化关键热控制参数。

Result: 创建了多目标实时优化挑战，平衡局部热调节和全局能效，支持集中式和分散式多代理RL方法，演示了策略蒸馏为可解释控制，并探索了基于LLM的方法。

Conclusion: LC-Opt为ML社区、运营商和供应商提供了详细、可定制的液体冷却模型，促进可持续数据中心液体冷却控制解决方案的开发。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [41] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源的高保真模拟基准，用于可持续的、地理时间任务调度，结合真实世界数据集和物理信息模型，支持可重复的可持续计算研究。


<details>
  <summary>Details</summary>
Motivation: 大规模AI的能源需求和碳足迹不断增加，需要智能的全球分布式数据中心工作负载管理，但缺乏能真实捕捉环境因素、数据中心物理和网络动态相互作用的基准。

Method: 结合精选的真实世界数据集（AI工作负载轨迹、电网碳强度、电力市场、天气等）和物理信息化的数据中心运营模型，提供模块化奖励系统和Gymnasium API。

Result: 提出了一个具有挑战性的调度问题，顶层协调代理必须动态重新分配或延迟任务，以优化多个目标（碳排放、能源成本、服务水平协议和水资源使用）。

Conclusion: DCcluster-Opt通过提供真实、可配置且可访问的测试平台，加速了用于地理分布式数据中心的下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [42] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 开发机器学习模型预测足球中的防线突破，使用XGBoost分类器分析球员位置、速度和空间配置特征，模型准确率高且发现防线突破与射门机会密切相关。


<details>
  <summary>Details</summary>
Motivation: 足球中突破对手防线是创造得分机会的关键战术行为，但以往研究主要关注射门或进球机会，缺乏对如何突破防线的系统性分析。

Method: 使用2023年J1联赛赛季的事件和追踪数据，构建包含189个特征的机器学习模型，包括球员位置、速度和空间配置，采用XGBoost分类器预测防线突破概率。

Result: 模型预测准确率高，AUC为0.982，Brier分数为0.015。SHAP分析显示进攻球员速度、防线缺口和进攻球员空间分布是防线突破的关键因素。预测的防线突破概率与球队失球射门次数呈中度正相关。

Conclusion: 防线突破与得分机会创造密切相关，为理解足球战术动态提供了量化框架。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [43] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 该论文使用统计物理学中的交叉涨落分析分数扩散模型的采样动态，发现样本从初始分布到目标分布经历离散的相变过程，这些相变可以通过交叉涨落检测，并能提升采样效率和多种任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究分数扩散模型中采样动态的演化过程，特别是样本从初始分布到目标分布的形成机制，旨在理解这一过程中的相变现象并提升采样效率。

Method: 使用统计物理学中的交叉涨落作为中心矩统计量，分析分数扩散模型的采样动态。对于方差保持SDE，推导了交叉涨落的闭式解，可在反向轨迹中高效计算。

Result: 发现样本在扩散过程中经历尖锐的离散相变，这些相变可通过交叉涨落检测。直接检测这些相变能提升采样效率，加速类条件生成和稀有类生成，并改进图像分类和风格迁移等零样本任务。

Conclusion: 该框架将离散马尔可夫链理论、相变分析与现代生成建模统一起来，为理解扩散模型的采样动态提供了新的视角和实用工具。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [44] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出动态多专家门控框架，自适应选择最优轨迹预测器，在nuPlan-mini数据集上FDE降低9.5%，达到oracle性能的57.8%


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾驾驶场景中不可靠，传统"一模型适用所有"范式存在局限性，物理模型有时优于先进网络

Method: 基于内部模型信号（稳定性和不确定性）的动态多专家门控框架，将轨迹专家选择建模为成对排序问题，包含物理信息LSTM、Transformer和微调GameFormer三个专家

Result: 在nuPlan-mini数据集上FDE为2.567m，比GameFormer降低9.5%；左转场景FDE降低约10%，在离线和开环评估中均表现一致改进

Conclusion: 自适应混合系统能增强安全关键自动驾驶中的轨迹可靠性，为超越静态单模型范式提供实用路径

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [45] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种集成到井下工具中的CCL信号采集系统，用于构建数据集，并评估了多种数据增强预处理方法在基于AlexNet的套管接箍识别模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的井下深度测量对油气井作业至关重要，但CCL信号识别的预处理方法发展不足，且真实井数据有限，给需要大量数据集的神经网络模型训练带来挑战。

Method: 开发集成到井下工具的CCL信号采集系统，提出包括标准化、标签分布平滑、随机裁剪、标签平滑正则化、时间缩放和多重采样等综合预处理方法，使用基于AlexNet的神经网络模型进行系统实验。

Result: 实验表明标准化、标签分布平滑和随机裁剪是模型训练的基本要求，而标签平滑正则化、时间缩放和多重采样显著增强模型泛化能力。两个基准模型的F1分数从0.937和0.952最大提升至1.0和1.0。

Conclusion: 该工作解决了在CCL数据有限环境中训练套管接箍识别模型的数据增强方法空白，在真实CCL波形上的性能验证证实了方法的有效性和实际适用性。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [46] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 比较监督微调(SFT)、LoRA和上下文学习(ICL)在数据稀缺场景下的表现，发现LoRA在技能获取与通用知识保留之间提供最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在特定应用中的适应性问题，避免完全微调带来的计算成本高和灾难性遗忘问题，探索不同适应策略的权衡。

Method: 对SFT、LoRA和ICL三种适应方法在数据稀缺场景下进行对比分析，评估它们在技能获取和知识保留方面的表现。

Result: LoRA在技能获取与通用知识保留之间提供最佳平衡；SFT擅长技能获取但易发生灾难性遗忘；ICL适合事实知识整合但难以处理复杂技能。

Conclusion: 提出了选择LLM适应策略的实用框架，强调了技能获取与知识整合的关键区别，阐明了任务特定性能与通用能力保留之间的权衡关系。

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [47] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种增强随机森林分类器的新框架，通过概率特征采样和模拟退火超参数调优，在预测准确性和泛化能力方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统随机森林在捕捉数据相关信号方面的局限性，通过强调特征重要性和自适应超参数配置来提升分类性能。

Method: 集成概率特征采样和模拟退火超参数调优，引导模型关注对分类更有意义的特征，并进行动态参数优化。

Result: 在多个领域（信用风险评估、物联网异常检测、早期医疗诊断、高维生物数据分析）中实现了准确率的持续提升，并提供了特征相关性的有意义的见解。

Conclusion: 结合重要性感知采样和元启发式优化能有效提升随机森林分类器的性能，展示了该框架在多样化分类任务中的有效性。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [48] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 该研究量化了植被结构和功能对印度138个城市热指数的影响，发现植被在特定条件下会逆转冷却效应并加剧热应激，为气候适应性绿化策略提供了定量阈值。


<details>
  <summary>Details</summary>
Motivation: 城市绿化降温效果不均，因为植被在冷却表面的同时可能加剧空气湿度，导致体感温度升高。目前对植物生理活动如何调节这种冷却与湿度积累之间的权衡关系了解不足，缺乏有效的缓解政策和设计指导。

Method: 使用极端感知的1公里分辨率热指数重建数据和可解释机器学习框架（结合SHAP和ALE分析），在印度138个城市中分离植被-气候相互作用，涵盖热带稀树草原、半干旱草原和湿润亚热带气候。

Result: 当EVI≥0.4、LAI≥0.05时冷却效果增强，但当EVI≥0.5、LAI≥0.2、fPAR≥0.5时开始逆转为增温，在潮湿密集核心区fPAR≥0.25时更早出现逆转。高生理活性植被在潮湿环境中提升近地表湿度的速度超过散热速度。

Conclusion: 研究确定了植被驱动冷却的气候限制，为促进公平和热弹性城市的气候特异性绿化策略提供了定量阈值。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [49] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight是一个基于双LLM架构的交通信号控制系统，通过Herald引导提示增强，解决了现有LLM方法固定时长信号和幻觉错误的问题，以及RL方法缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法存在固定信号时长和幻觉错误的问题，而RL方法在信号时序决策上缺乏鲁棒性和泛化能力，需要一种更有效的交通信号控制方法。

Method: 提出HeraldLight双LLM架构：Herald模块提取上下文信息并预测各交通相位排队长度；LLM-Agent进行细粒度交通信号控制；LLM-Critic修正LLM-Agent的输出错误和幻觉；通过基于分数的微调提高准确性和鲁棒性。

Result: 在CityFlow模拟实验中，使用覆盖济南(12)、杭州(16)和纽约(196)共224个交叉口的真实数据集，HeraldLight优于现有最佳基线方法，在所有场景中平均旅行时间减少20.03%，在济南和杭州场景中平均排队长度减少10.74%。

Conclusion: HeraldLight通过双LLM架构和Herald引导提示，有效解决了交通信号控制中的关键挑战，在多个真实场景中表现出优越性能。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [50] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与智能粒子群优化的决策模型，用于提高后端集中式冗余供应链的决策和规划效率。


<details>
  <summary>Details</summary>
Motivation: 提高后端集中式冗余供应链的决策和规划效率，解决动态环境下的实时决策调整和路径优化问题。

Method: 构建分布式节点部署模型和最优规划路径，使用卷积神经网络从历史数据中提取特征，线性规划捕获高阶统计特征，采用模糊关联规则调度和深度强化学习优化模型，神经网络拟合动态变化，通过"深度学习特征提取-智能粒子群优化"混合机制进行全局优化。

Result: 仿真结果显示资源消耗减少，空间规划增强，在动态环境中改善了实时决策调整、配送路径优化和鲁棒智能控制。

Conclusion: 所提出的混合模型能有效提高供应链决策效率，在动态环境下具有更好的适应性和鲁棒性。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [51] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 本研究评估稀疏自编码器(SAEs)在识别和控制LLMs中种族与污名化概念关联的能力，发现SAEs可以识别与黑人相关的潜在特征，但通过SAE引导缓解偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，需要开发方法来检测模型是否错误地依赖患者种族信息进行预测。

Method: 使用稀疏自编码器识别Gemma-2模型中与黑人个体相关的潜在特征，并通过激活这些潜在特征来引导模型输出，评估其对偏见关联的影响。

Result: 发现与黑人相关的潜在特征在合理输入序列和问题词汇上都会被激活，激活该特征会增加模型将患者预测为"好斗"的风险，SAE引导在简单场景中能改善偏见但在复杂临床任务中效果有限。

Conclusion: SAEs可作为识别LLMs对人口统计学特征问题依赖的有用工具，但通过SAE引导缓解偏见在现实临床任务中的实用性有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [52] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP是一个通过用更便宜的LLM推理替代昂贵的科学计算来降低计算成本的框架，能在减少60-75%计算评估的情况下实现更优的求解器精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的测试时计算方法需要执行大量求解器样本来识别高精度求解器，对于需要大量计算资源进行数值评估的复杂PDE尤其昂贵。

Method: 采用三阶段框架：(1)分析：包括PDE分类、解类型检测和稳定性分析的数学思维链分析；(2)生成：基于前一阶段数学见解的求解器生成；(3)合成：通过LLM评委迭代精化实现的协作选择-杂交锦标赛。

Result: 平均只需少于13次求解器评估（基线方法需要30+次），在测试的PDE上平均精度提高4倍，在不同LLM架构上均表现出稳健性能。

Conclusion: PDE-SHARP能显著降低计算成本，同时提高求解器精度，适用于从通用到专用推理模型的各种LLM架构。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [53] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 提出了针对LLM隐私的新任务：实体级成员推断风险发现，专注于敏感信息。现有MIA方法只能检测整个提示或文档，无法捕捉更细粒度的风险。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击方法在检测LLM训练数据中的敏感信息时粒度不够细，无法有效识别实体级别的隐私风险。

Method: 提出EL-MIA框架用于审计LLM中的实体级成员风险，构建了基准数据集，系统比较了现有MIA技术和两种新方法。

Result: 发现现有MIA方法在敏感属性实体级成员推断方面能力有限，但这种易受攻击性可以通过相对简单的方法来识别。

Conclusion: 需要更强的对抗方法来充分测试威胁模型，突显了改进实体级隐私保护的必要性。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [54] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出一个新颖框架，将资源密集型的对抗性提示优化问题转化为高效的摊销推理任务，利用预训练的非自回归生成LLM直接条件生成提示。


<details>
  <summary>Details</summary>
Motivation: 传统的对抗性提示优化需要大量计算资源，本文旨在通过摊销推理方法降低优化成本，提高效率。

Method: 使用预训练的非自回归生成LLM（如Diffusion LLMs）作为提示搜索的代理模型，直接条件生成提示，替代昂贵的逐实例离散优化。

Result: 生成的提示具有低困惑度、多样性，且对包括鲁棒训练和专有LLM在内的各种黑盒目标模型表现出强可迁移性。

Conclusion: 该框架不仅适用于对抗性提示，还为红队测试、自动提示优化以及新兴的Flow和Diffusion-based LLMs应用开辟了新方向。

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [55] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 本文系统比较了扩散模型在药物发现中两种主要治疗模式（小分子和肽类药物）的应用，分析了统一去噪框架如何适应不同的分子表示、化学空间和设计目标，并指出了各自面临的挑战和共享问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为生成建模的主要框架，在加速和改变传统缓慢昂贵的药物发现过程中显示出巨大潜力。本文旨在系统比较该模型在小分子和肽类药物设计中的应用差异。

Method: 采用系统性比较分析方法，研究扩散模型的迭代去噪框架如何适应小分子和肽类药物的不同分子表示、化学空间和设计目标。

Result: 对于小分子，扩散模型擅长基于结构的设计，能生成新颖的配体，但面临化学可合成性挑战；对于肽类药物，重点在于生成功能性序列和设计新结构，主要挑战是生物稳定性、正确折叠和免疫原性。

Conclusion: 扩散模型的全部潜力需要通过弥合模式特定差距并将其整合到自动化DBTL平台中来释放，从而将范式从化学探索转向靶向创造新型治疗药物。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [56] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的多奖励信号微调基础模型的新方法，通过迭代优化多个奖励函数来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，如文本生成和药物发现，仅使用单一奖励信号进行优化可能不够理想，因为通常需要多个评估标准。

Method: 采用迭代微调策略，在多个奖励函数上进行强化学习优化，扩展了现有最先进的基于RL的方法。

Result: 在文本、生物序列和小分子生成等多个领域的实验结果表明，该方法相比现有基线方法具有更好的效果。

Conclusion: 该方法为多奖励强化学习微调提供了有效的解决方案，并通过理论分析深入理解了其性能表现。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [57] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 提出一种结合集成学习和可解释人工智能（XAI）的机器学习模型，用于提高黑色素瘤检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗诊断中缺乏可解释性的问题，提高对AI系统预测结果的信任度。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用XAI技术来解释预测依据。

Result: 能够以高准确度检测黑色素瘤，同时提供可解释的预测结果。

Conclusion: 集成学习结合XAI技术可以有效提高黑色素瘤检测的可靠性和可解释性，为医疗诊断提供更可信的AI支持。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [58] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: 本文确定了非随机多臂老虎机专家建议问题中的极小极大最优期望遗憾，通过证明与Kale(2014)上界匹配的下界，得出最优期望遗憾为Θ(√(TK log(N/K)))，其中K是臂数，N是专家数，T是时间范围。


<details>
  <summary>Details</summary>
Motivation: 解决非随机多臂老虎机专家建议问题中的极小极大最优期望遗憾边界问题，填补理论空白。

Method: 通过证明与现有上界匹配的下界来确定极小极大最优期望遗憾。

Result: 确定了极小极大最优期望遗憾为Θ(√(TK log(N/K)))，其中K是臂数，N是专家数，T是时间范围。

Conclusion: 该研究完全解决了非随机多臂老虎机专家建议问题的极小极大最优期望遗憾边界问题，为理论分析提供了完整答案。

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [59] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 提出基于xLSTM的车辆轨迹预测框架X-TRAJ及其物理感知变体X-TRACK，通过集成车辆运动学约束生成更真实可行的轨迹，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管xLSTM在时间序列预测中表现出色，但在车辆轨迹预测领域尚未得到充分探索。传统LSTM存在长期依赖建模限制，而xLSTM通过指数门控和增强内存结构解决了这些问题。

Method: 开发了X-TRAJ框架及其物理感知变体X-TRACK，后者将车辆运动学约束显式集成到模型学习过程中，确保生成的轨迹在物理上可行。

Result: 在highD和NGSIM数据集上的综合评估表明，X-TRACK在车辆轨迹预测任务中优于最先进的基线方法。

Conclusion: xLSTM架构结合物理约束能够有效提升车辆轨迹预测性能，生成的轨迹更加真实可行，为智能交通系统提供了可靠的技术支持。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [60] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 该论文提出了一种基于领域知识的强化学习方法，用于控制混沌对流流动，特别是在Rayleigh-Bénard对流系统中，通过引入领域知识到奖励函数中，显著提高了控制的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流动在微流体设备和化学反应器等实际系统中普遍存在，但稳定这些流动具有挑战性。传统控制方法在混沌状态下往往失效，而强化学习在层流控制中表现出潜力，但在混沌和湍流动力学下的泛化能力和鲁棒性尚未充分探索。

Method: 采用领域知识增强的强化学习代理，使用近端策略优化算法在多样初始条件和流动状态下进行训练。在奖励函数中引入鼓励Bénard单元合并的领域知识项，作为期望的宏观特性。

Result: 在层流状态下，领域知识增强的强化学习代理将对流热传输减少了33%；在混沌流动状态下，仍实现了10%的减少，显著优于传统控制器。与无领域知识的代理相比，该方法产生了稳定流动、训练收敛更快，并且无需重新训练即可在不同流动状态下泛化。

Conclusion: 优雅的领域知识先验可以极大地增强基于强化学习的混沌流动控制的鲁棒性，使实际部署更加可行。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [61] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLMs的校准能力在网络深度中演化，上层存在置信度修正阶段，并识别出残差流中的低维校准方向，扰动该方向可改善校准指标而不影响准确率。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs内在校准能力如何随网络深度演化，理解置信度调节机制在模型中的分布方式。

Method: 分析多个开源权重模型在MMLU基准上的表现，研究校准在残差流中的演化过程，识别低维校准方向并进行扰动实验。

Result: 发现上层存在置信度修正阶段，识别出残差流中的低维校准方向，扰动该方向可显著改善ECE和MCE指标而不损害准确率。

Conclusion: 校准是分布式现象，在整个网络前向传播过程中形成，而不仅仅在最终投影层，为理解LLMs中的置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [62] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 该研究比较了8种不确定性量化技术在医疗时间序列数据上的表现，重点关注心房颤动检测和血压回归任务，发现不同方法在不同评估指标下的可靠性表现各异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗时间序列数据上的部署存在性能不佳风险，可靠的不确定性估计能为临床医生提供模型输出的可信度指导。

Method: 实现了8种不确定性量化技术，在两个临床相关预测任务（心房颤动检测分类和血压回归）上进行测试，并制定了全面的评估程序。

Result: 不同技术在不同任务中的不确定性可靠性表现复杂，最优方法取决于不确定性表达方式、评估指标和可靠性评估尺度。局部校准和适应性评估提供了传统全局指标无法获得的实用见解。

Conclusion: 评估不确定性量化技术的标准应适应模型的实际使用场景，在保持预测性能的同时，重点关注小尺度可靠性。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [63] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 本文提出了一种结合模型协变量合成和因果结构保持的混合框架，用于生成保持因果参数（如平均处理效应）的合成表格数据。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和GAN方法生成的合成数据虽然预测保真度高，但往往无法保持关键的因果参数，导致因果效应估计偏差。

Method: 采用混合生成框架：模型基协变量合成（通过距离过滤监控）+ 分别学习的倾向得分和结果模型，确保(W,A,Y)三元组保持底层因果结构，并引入合成配对策略缓解正性违反问题。

Result: 提出的方法能够生成保持因果结构的合成数据，并通过利用无限合成样本的评估协议，在复杂协变量分布下对传统估计器（IPTW、AIPW、替代）进行基准测试。

Conclusion: 这项工作为支持稳健因果分析的LLM驱动数据管道奠定了基础，代码已开源。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [64] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 提出了一种新的解码策略——Pivot-Aware Speculative Decoding，通过只拒绝会导致最终输出效用下降的关键token（pivot tokens），在保持任务性能的同时显著提高接受率，实现最高2.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统的Speculative Decoding要求严格匹配目标模型的分布，导致接受率过低，限制了加速潜力。作者认为实际应用中任务特定性能（如代码正确性、事实准确性）比采样分布更重要。

Method: 提出Pivot-Aware Speculative Decoding策略，训练轻量级分类器识别关键token（pivot tokens），只拒绝那些会导致最终输出效用下降的token，而不是所有不匹配的token。

Result: 在多个数据集上的评估显示，该方法在保持可比效用的同时，能够实现最高2.5倍的加速效果。

Conclusion: 通过放宽严格的分布匹配要求，专注于任务特定性能的保持，可以显著提高解码效率，为实际LLM应用提供更实用的加速方案。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [65] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 提出基于稀疏性的统一框架来评估算法公平性，该框架与现有公平性标准一致，适用于多种机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 算法公平性在机器学习中仍面临挑战，现有公平性标准缺乏跨问题的通用性，需要统一的评估框架。

Method: 研究各种稀疏性度量在促进公平性方面的联系与差异，提出基于稀疏性的统一公平性评估框架。

Result: 通过多个数据集和偏差缓解方法的广泛实验，验证了所提框架作为评估指标的有效性。

Conclusion: 通过稀疏性和社会公平性的视角为算法公平性研究提供了新思路，对公平性研究和应用具有广泛影响潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [66] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 比较模糊推理方法(ANFIS-FBCSP-PSO)与深度学习基准(EEGNet)在运动想象EEG分类中的表现，模糊方法在个体内测试表现更好，而深度学习方法在跨个体测试中泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象EEG分类中准确性与可解释性难以兼顾的挑战，为BCI系统设计提供选择依据。

Method: 使用ANFIS-FBCSP-PSO（结合滤波器组共空间模式特征提取和粒子群优化的模糊IF-THEN规则）与EEGNet（直接从原始EEG数据学习层次时空表示）在BCI Competition IV-2a数据集上进行对比。

Result: 个体内测试：模糊神经网络表现更好（准确率68.58%±13.76%，kappa=58.04%±18.43）；跨个体测试：深度模型泛化能力更强（准确率68.20%±12.13%，kappa=57.33%±16.22）。

Conclusion: 根据设计目标（可解释性或跨用户鲁棒性）选择合适的MI-BCI系统，未来基于Transformer和混合神经符号框架有望推进透明EEG解码。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [67] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态聚合物发现框架，结合了PolyBERT的化学语言表示和图编码器的分子图表示，通过多模态嵌入进行候选聚合物的检索和排序。


<details>
  <summary>Details</summary>
Motivation: 利用多模态方法整合化学语言和分子图表示，实现高效的聚合物发现和跨相关属性的稳健排序。

Method: 首先使用基于语言相似性检索候选聚合物，然后使用融合的多模态嵌入根据多个目标属性进行排序。

Result: 建立了可推广的多模态范式，推进了AI引导的下一代聚合物发现设计。

Conclusion: PolyRecommender通过整合多模态知识，为聚合物发现提供了高效且稳健的AI指导设计方法。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [68] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 提出UME-R1生成式多模态嵌入框架，通过两阶段训练（监督微调+强化学习）统一嵌入任务到生成范式，在MMEB-V2基准测试中显著优于传统判别式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的嵌入方法本质上是判别式的，限制了其从推理驱动的生成范式中获益的能力。

Method: 两阶段训练策略：冷启动监督微调赋予模型推理能力，使其能生成判别式和生成式嵌入；后续强化学习增强推理并优化生成式嵌入质量。

Result: 在MMEB-V2基准的78个任务上显著优于传统判别式嵌入模型，生成式嵌入通过利用MLLMs的强大生成推理能力实现性能提升。

Conclusion: 生成式嵌入为更可解释、推理驱动的生成式多模态嵌入奠定了基础，揭示了判别式和生成式嵌入的互补性以及推理时采样的可扩展潜力。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [69] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出梯度引导采样(GGS)方法，解决对抗攻击在迁移场景中攻击强度与泛化能力之间的权衡困境，通过在梯度上升方向进行采样来平衡这两个目标。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型迁移时面临攻击强度(Exploitation)与泛化能力(Exploration)的根本矛盾：传统方法过度追求攻击强度而削弱泛化，新方法过度追求泛化而削弱攻击强度。

Method: 基于MI-FGSM，引入内迭代随机采样，使用前一次内迭代的梯度来引导采样方向，采样幅度由随机分布决定，使对抗样本位于平衡区域。

Result: 在多种DNN架构和多模态大语言模型上的综合实验表明，该方法在迁移攻击方面优于现有最先进方法。

Conclusion: GGS方法通过梯度引导采样有效平衡了攻击强度与泛化能力，解决了对抗攻击迁移性的根本困境。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [70] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出Tree Training方法，通过树形打包和梯度恢复技术，在智能体LLM训练中重用共享前缀计算，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前训练流程将树形轨迹分解为独立线性段，导致共享前缀被重复计算，造成计算效率低下。

Method: 采用树形打包技术重用跨轨迹的共享计算，并通过梯度恢复确保重用前缀的正确梯度传播。

Result: 在多个开源模型上实验，总训练时间最多减少3.9倍。

Conclusion: Tree Training实现了更高效的智能体LLM监督微调和强化学习训练。

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [71] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 本文提出了一种结构保持的物理信息神经网络框架，用于求解非线性KdV方程，通过将质量守恒和哈密顿能量守恒直接嵌入损失函数，确保物理一致的演化过程。


<details>
  <summary>Details</summary>
Motivation: 传统的PINNs在长期积分中往往无法保持关键的物理不变量，特别是在处理非线性色散波传播问题时。

Method: 使用正弦激活函数增强频谱表达能力，并将质量守恒和哈密顿能量守恒直接嵌入损失函数进行约束优化。

Result: 模型成功重现了KdV动力学的典型行为，包括单孤子传播、双孤子相互作用和余弦脉冲初始化，同时保持守恒不变量。

Conclusion: 结合不变量约束优化和正弦特征映射的计算高效方法，为哈密顿偏微分方程提供了鲁棒、能量一致的PINNs解决方案。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [72] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM是一个将规划与离线学习紧密结合的强化学习框架，通过引导循环和联合学习的世界模型，解决了规划过程中数据与策略行为不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽然能提高强化学习的样本效率和最终性能，但规划用于环境交互会导致收集的数据与策略实际行为不一致，从而降低模型学习和策略改进的效果。

Method: 提出BOOM框架，通过引导循环将规划与离线学习紧密结合：策略初始化规划器，规划器通过行为对齐来引导策略。使用联合学习的世界模型支持规划器模拟未来轨迹并提供价值目标。核心是使用无似然对齐损失和软价值加权机制。

Result: 在DeepMind Control Suite和Humanoid-Bench等高维环境上的实验表明，BOOM在训练稳定性和最终性能方面都达到了最先进的结果。

Conclusion: BOOM通过紧密集成规划和离线学习，有效解决了规划过程中数据与策略行为不一致的问题，在复杂环境中表现出优异的性能。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [73] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 该研究提出了一种基于ROI引导掩码策略的fMRI基础模型，使用AAL3图谱对大脑区域进行选择性掩码，相比随机掩码方法在ADHD分类任务上提升了4.23%的准确率。


<details>
  <summary>Details</summary>
Motivation: 推动神经影像学基础模型的发展，利用大规模脑成像数据集，探索超越随机区域掩码的区域感知重建策略，以提升模型在下游fMRI任务中的泛化能力。

Method: 使用AAL3图谱的ROI引导掩码策略，直接对完整4D fMRI体积进行选择性掩码，在自监督预训练中掩码语义连贯的大脑区域。基于ADHD-200数据集的973名受试者静息态fMRI数据。

Result: 相比传统随机掩码，ADHD分类准确率提升4.23%。区域级归因分析显示边缘系统和大脑小脑区域对重建保真度和模型表示贡献最大。

Conclusion: 在模型预训练中掩码解剖区域不仅能增强可解释性，还能产生更鲁棒和区分性的表示。未来将扩展到更多神经影像数据集，并开发基于区域感知重建目标的新损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [74] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出基于深度自编码器的异常检测方法，用于检测企业级ETL数据流中的多种异常类型，通过编码器-解码器结构和正则化约束实现高效异常识别。


<details>
  <summary>Details</summary>
Motivation: 解决企业级ETL数据流中经常出现的异常问题，包括延迟、缺失值、重复加载和突发异常变化等，确保数据处理的质量和稳定性。

Method: 采用深度自编码器结构，将高维输入压缩为潜在表示并重建，利用重建误差衡量异常程度；在潜在空间引入正则化约束增强特征稀疏性和分布学习。

Result: 在不同超参数设置、环境变化和数据特征下，该方法在AUC、ACC、Precision和Recall指标上表现优异，能有效识别多种异常。

Conclusion: 基于深度自编码器的检测机制能有效捕捉企业级ETL数据流的潜在分布模式，为数据处理和智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [75] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 该论文从理论角度解释了联邦学习中数据异构性导致性能下降的原因，指出客户端数据的异构性会产生不同的局部最优解，这既提高了全局目标的下界，又导致全局模型在训练后期振荡而非收敛。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法虽然在理论和实践中都能保证收敛，但在数据异构情况下性能下降的原因尚不清楚。本文旨在填补这一理论空白，解释非独立同分布数据下性能退化的根本原因。

Method: 引入客户端数据异构导致不同局部最优解的假设，分析该假设的两个关键后果：1）客户端局部最优解之间的距离提高了全局目标的下界；2）训练后期全局模型在区域内振荡而非收敛到单一最优解。

Result: 理论分析表明数据异构性使得完美拟合所有客户端数据变得不可能，并且限制了模型充分拟合数据的能力。通过多个任务和神经网络架构的实验验证了这一理论解释。

Conclusion: 本文提供了一个理论框架来解释联邦学习中数据异构性导致的性能下降问题，为理解非独立同分布设置下的性能限制提供了原则性解释。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [76] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 提出了一种基于变分自编码器(VAE)的传感器校准新方法，通过在潜在空间训练校准输出，能够同时实现传感器校准和自编码功能。


<details>
  <summary>Details</summary>
Motivation: 探索使用VAE进行传感器数据校准的新途径，利用潜在空间作为校准输出，为传感器校准提供新的解决方案。

Method: 使用变分自编码器框架，将潜在空间训练为校准输出，并在多传感器气体数据集上进行概念验证。

Result: 提出的校准VAE能够同时作为校准模型和自编码器工作，生成与真实数据统计相似的校准输出和重建输出。

Conclusion: 该方法展示了VAE在传感器校准中的潜力，为未来的测试和扩展工作奠定了基础。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [77] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习构建共享表示空间，学习模型推理能力和查询-方法兼容性，结合概率边界作为正则化器，在平衡准确性和计算成本的同时选择最优推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择输出答案，假设更多候选答案能带来更高准确性。本文重新审视这一假设，通过理论分析发现固定生成分布和候选数量下的标准聚合方法存在准确率边界。

Method: 提出EPIC框架：1）通过对比学习构建共享表示空间，捕捉模型推理能力和查询-方法兼容性；2）将理论推导的概率边界作为正则化器；3）进行效用驱动的优化，平衡准确性和计算成本。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，在提高准确性的同时减少计算开销。

Conclusion: EPIC框架通过理论指导的表示学习和优化策略，有效解决了语言模型生成中推理方法选择的关键挑战，实现了准确性和效率的平衡。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [78] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 该论文旨在设计和评估多种机器学习模型来预测PM2.5浓度，包括线性回归、集成方法、深度学习模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重威胁，预测未来PM2.5水平可以提供早期预警，帮助预防相关疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、深度学习模型（如循环神经网络和变换器）以及大型语言模型。

Result: 论文比较了不同模型在PM2.5预测任务上的性能表现。

Conclusion: 通过系统评估多种机器学习方法，为PM2.5浓度预测提供了有效的模型选择和性能比较。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [79] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出Lo-Hp框架，通过解耦的两阶段权重生成方法解决生成权重时的过耦合和长视野问题，采用混合策略子轨迹平衡目标学习局部优化策略，在多种任务中展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成权重的方法存在过耦合和长视野问题，前者限制了优化器的灵活性，后者导致推理效率低下和准确性不足。

Method: 提出Lo-Hp框架，采用解耦的两阶段权重生成方法，结合混合策略子轨迹平衡目标，整合在线和离线策略学习来捕捉局部优化策略。

Result: 理论证明仅学习局部优化策略即可解决长视野问题并增强全局最优权重的生成，在迁移学习、少样本学习、领域泛化和大语言模型适应等任务中验证了优越的准确性和推理效率。

Conclusion: Lo-Hp框架通过解耦设计和局部策略学习有效解决了权重生成中的关键问题，在多种需要频繁权重更新的任务中表现出色。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [80] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制框架，通过集中决策避免多智能体协调复杂性，利用DreamerV3世界模型高效学习控制策略，显著减少队列长度并展示抗波动能力。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵主要由交叉口排队引起，传统交通信号控制模型难以捕捉真实交通复杂性。需要开发能够适应动态交通环境的智能控制方法。

Method: 使用单智能体强化学习框架，通过邻接矩阵统一编码路网拓扑、实时队列状态和信号配时参数，利用DreamerV3世界模型学习控制策略，动作序列选择交叉口并调整信号相位配时。

Result: SUMO仿真实验显示，在10%、20%、30%起讫点需求波动场景下，该框架具有鲁棒的抗波动能力，显著减少了队列长度。

Conclusion: 建立了一种与探测车辆技术兼容的智能交通控制新范式，未来研究将关注训练中引入随机起讫点需求波动和应急事件区域优化机制。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [81] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用时序融合变换器(TFT)对沃尔玛周度销售进行多周期预测，融合静态商店标识和动态外部因素，在5周预测期内优于基准模型，为库存规划和节假日优化提供实用价值。


<details>
  <summary>Details</summary>
Motivation: 准确的零售多周期预测对库存管理和促销活动至关重要，需要融合多种影响因素并保持模型可解释性。

Method: 采用时序融合变换器(TFT)模型，结合静态商店标识符和时序外部信号(节假日、CPI、燃料价格、温度)，通过分位数损失生成概率预测，并提供变量选择网络、静态丰富化和时序注意力机制的可解释性分析。

Result: 在2012年固定测试集上，TFT实现每店周RMSE为57.9k美元，R²为0.9875；在5折时序交叉验证中，平均RMSE为64.6k美元，R²为0.9844，优于XGB、CNN、LSTM和CNN-LSTM基准模型。

Conclusion: TFT模型在零售销售预测中表现出色，不仅预测精度高，还保持模型透明度，对库存规划和节假日优化具有实际应用价值。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [82] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 提出了一种轻量级黑盒红队测试方法，通过迭代反馈和上下文学习来发现激活探针的脆弱性模式，无需微调或梯度访问。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控器成本低、延迟小，但其在真实黑盒对抗压力下的鲁棒性尚未充分探索，需要发现故障模式并最小化测试成本。

Method: 使用现成的LLM包装器，结合迭代反馈和上下文学习进行黑盒红队测试，无需微调、梯度或架构访问。

Result: 在高风险交互探针案例研究中，发现了可解释的脆弱性模式（如法律术语导致的假阳性、平淡程序性语调导致的假阴性），以及在场景约束攻击下持续存在的漏洞。

Conclusion: 简单的提示式红队测试框架可以在部署前预测故障模式，并为未来探针的加固提供有前景的可操作见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [83] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出FTT-GRU混合模型，结合快速时序Transformer和GRU，在NASA CMAPSS数据集上实现剩余寿命预测，在准确性和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LSTM和CNN难以同时建模多元传感器数据的全局时序依赖和细粒度退化趋势，需要更有效的模型架构。

Method: 使用快速时序Transformer（基于FFT的线性化注意力）与GRU层结合的混合模型，首次在CMAPSS数据集上应用FTT-GRU进行剩余寿命预测。

Result: 在CMAPSS FD001数据集上，FTT-GRU达到RMSE 30.76、MAE 18.97、R²=0.45，CPU延迟1.12ms，相比最佳基线TCN-Attention，RMSE提升1.16%，MAE提升4.00%。

Conclusion: 紧凑的Transformer-RNN混合架构能够在CMAPSS上实现准确高效的剩余寿命预测，适用于实时工业预测性维护。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [84] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 提出一个以LLM为核心的贝叶斯网络结构发现统一框架，支持无数据和有数据两种场景，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法需要大量观测数据且计算成本高，现有LLM方法仅将其作为辅助工具，未充分发挥LLM在核心学习过程中的作用。

Method: 提出PromptBN（无数据时查询LLM）和ReActBN（有数据时结合ReAct推理和结构评分进行迭代优化），保持LLM在整个发现过程中的主动参与。

Result: 实验表明该方法在低数据或无数据场景下显著优于现有LLM方法和传统数据驱动算法。

Conclusion: 将LLM置于贝叶斯网络结构发现的核心位置是有效的，特别是在数据稀缺情况下表现优异。

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [85] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出了一种结合稀疏参数估计和非参数技术的新框架，用于从数据中发现动态系统模型方程，特别适用于复杂生物系统建模。


<details>
  <summary>Details</summary>
Motivation: 传统参数模型在准确表示复杂系统中的某些非线性特性方面存在不足，特别是在系统生物学中，自下而上的建模方法往往不可行。

Method: 将稀疏参数估计（如Sindy算法）与非参数技术相结合，无需事先了解非线性函数形式或扩展函数库即可捕获Sindy无法描述的非线性特性。

Result: 该方法在多个复杂生物现象估计的示例中得到了验证。

Conclusion: 该框架能够有效捕获复杂系统中的非线性特性，为数据驱动的模型发现提供了更强大的工具。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [86] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 该研究提出了一个临床医生中心的框架来量化大型语言模型在脊柱手术决策支持中的幻觉风险，评估了六个领先LLM在30个专家验证的脊柱病例上的表现。DeepSeek-R1表现最佳，研究发现推理增强模型变体并不总是优于标准版本，多维压力测试暴露了模型特定的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在脊柱手术临床决策支持中具有变革潜力，但存在幻觉风险，可能危及患者安全，需要量化这些风险以确保临床应用的安全性。

Method: 引入临床医生中心框架，评估诊断精度、推荐质量、推理鲁棒性、输出一致性和知识对齐，在30个专家验证的脊柱病例上测试六个领先LLM，并进行多维压力测试。

Result: DeepSeek-R1总体表现最佳（总分：86.03±2.08），在创伤和感染等高风险领域表现突出。推理增强模型变体（如Claude-3.7-Sonnet的扩展思维模式）并未优于标准版本。压力测试下推荐质量下降7.4%，而合理性、可读性和诊断略有改善。

Conclusion: 研究主张在临床工作流程中整合可解释性机制（如推理链可视化），并为手术LLM部署建立安全感知的验证框架，强调扩展推理链本身不足以确保临床可靠性。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [87] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出了一个统一的数据驱动框架，通过五个阶段量化并提升冰球比赛中的进攻势头和得分可能性，结果显示采用优化的事件序列和阵型能显著提高15%的得分潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在为冰球分析提供基于因果推断的战术优化方法，为教练和分析师提供实时、可操作的洞察。

Method: 五阶段端到端流水线：1)逻辑回归进行可解释的势头加权；2)梯度提升决策树进行非线性xG估计；3)LSTM网络进行时序建模；4)PCA和K-Means聚类发现空间阵型；5)X-Learner因果推断估计器量化最优序列和阵型的平均处理效应。

Result: 观察到平均处理效应为0.12（95%置信区间：0.05-0.17，p < 1e-50），对应得分潜力相对提升15%。

Conclusion: 战略结构化的序列和紧凑阵型能够因果性地提升进攻表现，该框架推动了冰球分析向原则性、因果基础的战术优化发展。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [88] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的贝叶斯框架来解释大语言模型的控制方法，将基于提示和基于激活的干预视为改变模型对潜在概念信念的不同方式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索看似不同的LLM控制方法（上下文学习和激活引导）是否可以被统一到一个更广泛的框架中，从贝叶斯角度理解这些方法的共同机制。

Method: 开发了一个基于贝叶斯视角的统一预测模型，认为上下文和激活干预通过改变模型对潜在概念的信念来影响行为：激活引导改变概念先验，而上下文学习导致证据积累。

Result: 该贝叶斯模型能够准确预测LLM在各种上下文和激活干预下的行为，解释了先前的经验现象（如S型学习曲线），并预测了新的现象（如两种干预在log信念空间的可加性）。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一的理论解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [89] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 本文研究了具有稀疏成本的对抗性随机最短路径问题，提出了ℓ_r-范数正则化器来适应稀疏性，在已知转移设置下将遗憾从√log(SA)改进为√log(M)，其中M是实际产生成本的状态-动作对数量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于负熵正则化的在线镜像下降方法在已知转移设置下的遗憾界为√log(SA)，这在最坏情况下是最优的，但无法充分利用问题的稀疏性优势，当只有少量状态-动作对产生成本时表现不佳。

Method: 提出了一族ℓ_r-范数正则化器（r∈(1,2)），这些正则化器能够适应问题的稀疏性，在已知转移设置下实现与√log(M)相关的遗憾界。

Result: 在已知转移设置下，新方法将遗憾从√log(SA)改进为√log(M)，其中M≪SA，并通过匹配下界证明这是最优的。在未知转移设置下，稀疏性的好处有限，任何学习者的极小极大遗憾都与SA呈多项式关系。

Conclusion: ℓ_r-范数正则化器能够有效适应稀疏成本问题，在已知转移设置下实现最优遗憾界，M而非SA捕获了问题的有效维度。但在未知转移设置中，稀疏性的优势受到限制。

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [90] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 研究表明RBMs在训练前剪枝80%连接仍能保持高质量生成性能，证实存在可行子网络。但训练后进一步剪枝无法通过重训练完全恢复性能，且重训练网络表现不如从头训练的同等稀疏网络。


<details>
  <summary>Details</summary>
Motivation: 研究在神经网络规模不断增大带来计算和环境成本的背景下，探索更简单、稀疏的网络是否能保持强大性能，基于彩票假设验证RBMs在极端剪枝条件下的表现。

Method: 研究受限玻尔兹曼机在极端剪枝条件下的表现，包括训练前剪枝和训练后剪枝的对比实验，分析关键连接被破坏时的性能突变。

Result: RBMs在训练前剪枝80%连接仍能保持高质量生成性能，但训练后进一步剪枝会导致性能急剧下降，且重训练网络表现不如从头训练的同等稀疏网络。

Conclusion: 稀疏网络要有效工作，剪枝应在训练早期实施而非训练后尝试，初始条件对网络能力有持久影响，为高效神经网络架构开发提供实践指导。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [91] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive是一个异步联邦学习框架，通过无数据知识蒸馏来缓解陈旧更新问题，提高训练效率和最终精度。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)虽然提高了大规模异构环境中的训练效率，但会引入陈旧更新（基于过时全局模型的客户端更新），这会破坏优化稳定性并阻碍收敛。

Method: FedRevive结合参数空间聚合与轻量级服务器端无数据知识蒸馏(DFKD)，使用元学习生成器合成伪样本进行多教师蒸馏，并通过混合聚合方案结合原始更新和DFKD更新。

Result: 在各种视觉和文本基准测试中，FedRevive相比异步基线实现了高达32.1%的更快训练速度和高达21.5%的更高最终精度。

Conclusion: FedRevive通过无数据知识蒸馏有效缓解了异步联邦学习中的陈旧更新问题，在保持AFL可扩展性的同时显著提升了训练效率和模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [92] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 提出了一种基于伴随状态方法的生成流模型梯度计算技术，用于气候科学中的敏感性分析，大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型进行敏感性分析计算成本过高，而AI生成模型虽然评估速度快，但计算敏感性仍是瓶颈。

Method: 应用伴随状态方法计算生成流模型（特别是扩散模型）的梯度，并提出了梯度自一致性检查来验证敏感性结果。

Result: 该方法将敏感性分析的计算成本从超级计算机上的数周降低到GPU上的数小时，初步证明能够产生可靠的梯度。

Conclusion: 该方法简化了气候科学中的关键工作流程，为高效可靠的敏感性分析提供了可行方案。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [93] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: KAPPA是一种推理时方法，通过结合KL散度、置信度和熵的评分函数来指导渐进式剪枝，在保持准确性的同时显著减少内存和令牌使用。


<details>
  <summary>Details</summary>
Motivation: 传统Best-of-N方法计算成本高，而Self-Truncation Best-of-N依赖一致性启发式方法，无法直接评估分支质量。

Method: 使用KL散度、置信度和熵构建评分函数，在探索阶段促进多样性，并选择性地消除低分分支。

Result: 在GSM8K和MATH500上的实验显示，KAPPA在较小模型中稳定性能，相比BoN减少约60%峰值内存和90%令牌生成，对准确性影响最小。

Conclusion: KAPPA提供了一种原则性的推理时剪枝方法，能有效平衡计算效率和推理准确性。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [94] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: 提出Pub2Priv框架，利用公开知识生成隐私时间序列数据，通过自注意力机制编码公开数据，使用扩散模型生成合成私有序列，在隐私-效用权衡方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 敏感时间序列数据（如金融、医疗、能源消费）因隐私问题难以共享。现有隐私感知数据生成方法往往忽略与公开上下文元数据的相关性，导致隐私-效用权衡不理想。

Method: 使用自注意力机制将公开数据编码为时间和特征嵌入，作为扩散模型的条件输入来生成合成私有序列。引入实用性指标评估合成数据的可识别性。

Result: 在金融、能源和大宗商品交易领域的实验表明，Pub2Priv在改善隐私-效用权衡方面持续优于最先进的基准方法。

Conclusion: Pub2Priv框架通过利用异构公共知识，能够有效生成隐私时间序列数据，在保持数据效用的同时提供更好的隐私保护。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [95] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 该论文研究了知识追踪模型在在线学习平台中面临的概念漂移问题，发现所有模型都会出现性能下降，其中贝叶斯知识追踪模型最为稳定，而复杂的注意力模型性能下降最快。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习过程是静态的，但实际在线学习平台中学生行为和概念会随时间变化，需要研究概念漂移对模型性能的影响。

Method: 使用四种知识追踪模型在五年学术数据上进行测试，评估模型在单学年内和跨学年的性能变化。

Result: 所有四类知识追踪模型都表现出性能下降，贝叶斯知识追踪模型在新数据上最稳定，而复杂的注意力模型预测能力下降最快。

Conclusion: 知识追踪模型容易受到概念漂移影响，需要更长期的评估，贝叶斯知识追踪模型在变化环境中相对更稳定。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [96] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1是一个统一的描述性-预测性-生成性架构，在单一编码器-解码器框架内整合了统计、机制和生成推理。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够连接可解释性、准确性和创造性的通用智能架构蓝图。

Method: 使用变分目标联合优化描述性表示学习、预测推理和生成合成，在单一编码器-解码器框架内实现。

Result: 在MNIST数据集上的实验验证了描述性重建、预测分类和生成采样可以在一个模型中稳定共存。

Conclusion: 该框架为实现连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [97] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 该论文提出了一种融合卫星和雷达数据的多模态临近预报模型，用于预测5、15和30分钟内的降水，显著优于仅使用雷达的方法，特别是在强降水预测方面。


<details>
  <summary>Details</summary>
Motivation: 传统地面传感器在监测城市强降雨事件方面存在局限（德国2001-2018年间仅17.3%的强降雨事件被记录），雷达数据单独用于强降雨预报仍具挑战性，因此需要融合卫星数据来提高预报准确性。

Method: 开发了一个多模态临近预报模型，结合雷达和卫星图像数据来预测降水，在5、15和30分钟三个时间尺度上进行预报。

Result: 多模态策略显著优于仅使用雷达的方法，特别是对强降水的预测。在5分钟预报中，强降雨的临界成功指数提高4%，暴雨提高3%，且在更长时间尺度上保持更好的预测能力。

Conclusion: 融合卫星和雷达数据的多模态模型能够提供更详细准确的强降雨预报，为及时可靠的灾害预警提供支持，具有重要的生命安全保障意义。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [98] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: STDiffusion是一个创新的多变量时间序列生成框架，结合扩散概率模型和可学习序列分解技术，通过分别学习趋势和季节性成分来提升生成过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法缺乏可解释的分解技术，无法有效合成有意义的趋势和季节性模式，限制了生成数据的质量。

Method: 使用MLP结构捕捉趋势，自适应小波蒸馏进行多分辨率季节性成分学习，并设计了全面的校正机制确保生成成分的内部一致性和相互关系。

Result: 在8个真实世界数据集上的实验表明，STDiffusion在时间序列生成任务中达到最先进性能，并在多窗口长序列生成中表现出鲁棒性和多功能性。

Conclusion: STDiffusion通过可解释的分解方法有效提升了时间序列生成的质量和可解释性，为多变量时间序列生成提供了新的解决方案。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [99] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 提出了一种基于双凸优化的PINN方法，将特征对搜索重新表述为双凸优化问题，通过交替凸搜索实现快速收敛，比传统梯度训练快500倍。


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法求解特征值问题时速度比经典数值方法慢几个数量级，需要更高效的训练策略。

Method: 将特征对搜索重新表述为双凸优化问题，采用交替凸搜索(ACS)方法，对特征值和特征函数进行交替优化，使用解析最优更新。

Result: PINN-ACS方法实现了高精度，收敛速度比基于梯度的PINN训练快500倍。

Conclusion: 提出的双凸优化框架为PINN求解特征值问题提供了快速且可证明收敛的替代方案。

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [100] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 提出了PREPO方法，通过利用内在数据特性提高RLVR的数据效率，包含两个互补组件：基于提示困惑度的适应性学习和基于相对熵差异的探索优先机制，在数学推理基准上使用比基线少3倍的rollouts实现有效结果。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了大型语言模型的推理能力，但训练成本高昂，因为许多rollouts对优化的贡献很小。本研究探索如何利用几乎免费的内在数据特性来提高RLVR的数据效率。

Method: PREPO方法包含两个组件：1）使用提示困惑度作为模型适应性学习的指标，让模型从易到难学习；2）通过区分rollouts的相对熵来放大差异，优先选择探索程度更高的序列。

Result: 在Qwen和Llama模型上，PREPO在数学推理基准测试中取得了有效结果，使用的rollouts比基线少达3倍。

Conclusion: PREPO通过两个互补机制减少了rollouts需求，同时保持了竞争力性能，并通过理论和深入分析解释了提高RLVR数据效率的基本原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [101] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 该论文分析了预训练Transformer在微调时出现的输出饱和和梯度抑制问题，提出了诊断指标来识别关键层，并采用选择性LoRA适配器注入策略来恢复梯度信号。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时容易过度依赖源域模式，难以形成新的目标域模式，这源于输出饱和导致的梯度抑制问题。

Method: 通过层间诊断指标识别关键层，然后在关键层选择性注入LoRA适配器来恢复被抑制的梯度信号。

Result: 实验表明，在过训练初始化时，关键层LoRA注入能提升性能；在欠训练初始化时，则需要全路径解阻塞才能实现低层重建。

Conclusion: 基于诊断的轻量微调策略能有效解决梯度抑制问题，但需要根据基础特征强度选择不同的解阻塞策略。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [102] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个基于GFlowNets的概念擦除框架，通过探索去噪轨迹空间来从文本到图像生成器中安全移除有害或专有概念，无需精心设计的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 当前的概念擦除技术要么导致图像质量下降，要么依赖脆弱的对抗损失，或者需要大量重新训练。这些方法对扩散模型去噪轨迹的理解有限。

Method: 将概念遗忘视为去噪路径空间的探索，使用配备轨迹平衡目标的GFlowNets进行优化，通过采样整个轨迹而非单个最终状态来学习随机策略。

Result: EraseFlow在性能上优于现有基线，实现了性能和先验保持之间的最佳权衡，能有效泛化到未见概念并避免可被攻击的奖励。

Conclusion: EraseFlow通过重新定义概念擦除为去噪轨迹探索问题，提供了一种无需精心设计奖励模型的有效解决方案，在安全性和性能之间取得了良好平衡。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [103] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑信息强化学习(LIRL)，通过逻辑投影确保混合动作空间中探索步骤的可行性，无需惩罚调优，在多个CPS场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CPS的混合动作空间优化中存在全局最优性不足、约束保证困难等问题，需要一种能保证约束满足且不依赖惩罚调优的方法。

Method: 为标准策略梯度算法配备投影机制，将低维潜在动作映射到由一阶逻辑定义的可行混合流形上，确保每个探索步骤的可行性。

Result: 在工业制造、电动汽车充电站和交通信号控制等场景中均优于现有方法，以机器人减速器装配系统为例，相比传统分层调度方法，在制造周期-能耗联合目标上最多减少36.47%-44.33%，且始终保持零约束违反。

Conclusion: LIRL通过声明式逻辑约束公式化，可无缝迁移到其他领域，为大规模CPS的安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [104] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一个均衡策略泛化（EPG）框架，用于在对抗性游戏中学习具有跨图零样本性能的泛化策略，特别针对追逃游戏（PEG）场景。


<details>
  <summary>Details</summary>
Motivation: 解决追逃游戏中图结构变化时现有方法需要重新计算或微调的问题，提高实时应用能力。

Method: 使用动态规划算法生成纯策略纳什均衡作为单图策略的均衡预言机，通过跨图训练RL策略对抗均衡策略，并设计了分组机制和序列模型来处理多追捕者情况。

Result: 实验表明EPG框架在各种未见过的真实世界图中实现了良好的零样本性能，在带出口的图中，泛化追捕者策略甚至能与最先进方法的微调策略相媲美。

Conclusion: EPG框架成功实现了追逃游戏中的跨图策略泛化，为对抗性游戏中的均衡学习提供了有效的解决方案。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [105] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT是一种针对边缘设备优化的视觉Transformer设计，通过在Transformer架构中集成LUT神经元层，显著减少了模型大小、计算量和能耗，同时保持与基线Transformer相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer在边缘设备上的计算、内存和能耗需求过高，而现有的基于逻辑和LUT的网络在视觉任务上表现不佳，需要一种既高效又准确的边缘优化方案。

Method: 设计了一种基于LUT的通道混合器替代传统MLP层，采用神经网络学习方法原生学习LUT函数，并开发了相应的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的准确率，减少了60%的模型权重和50%的乘法运算，相比量化ViT加速器实现了1.9倍能效提升和1.3倍延迟降低。

Conclusion: LL-ViT提供了一种计算和能效高效的视觉Transformer推理解决方案，特别适合边缘设备部署。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [106] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 开发了一个交互式应用程序，用于油气管道的段塞流实时检测，集成了数据探索、模型训练、结果可视化和实时推理功能。


<details>
  <summary>Details</summary>
Motivation: 现有段塞流检测方法通常是离线的，需要领域专业知识，且缺乏实时可解释性，难以满足工业应用需求。

Method: 构建端到端数据驱动的检测系统，包含数据探索标注、可配置模型训练评估、时间序列可视化分类结果，以及基于持久性的实时推理警报模块。

Result: 开发出轻量级、便携且易于部署的工具，支持从标注CSV上传到实时推理的无缝工作流程，具有快照持久化、视觉标注和实时警报等创新UI/UX功能。

Conclusion: 该交互式人机协同ML系统能够弥合数据科学方法与关键过程工业中实际决策之间的差距，在油气管道的段塞流检测及其他时间序列故障诊断任务中具有广泛应用前景。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [107] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache是一种分层KV缓存管理系统，利用KV头的时序稳定性来减少GPU内存使用和计算开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务受限于不断增长的KV缓存大小，现有系统难以在不降低精度的情况下有效利用注意力关键令牌的稀疏性，特别是在长生成场景中。

Method: FlexiCache将KV头分类为稳定和不稳定：不稳定头的所有KV缓存页面保留在GPU内存中，而稳定头仅保留前K个页面在GPU上，其余卸载到主机内存。利用时序稳定性，对稳定头执行周期性重排以获取新提升的顶部页面。

Result: 在vLLM上实现，FlexiCache将长上下文请求的GPU内存占用减少高达70%，离线服务吞吐量提高1.38-1.55倍，在线令牌延迟降低1.6-2.1倍。

Conclusion: FlexiCache通过利用KV头的时序稳定性，在保持长上下文、长生成场景精度的同时，显著提升了内存效率和性能。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [108] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 研究表明，在量化训练中增加批次大小可以补偿反向传播精度降低的影响，权重和激活值的量化对梯度方差有不同影响。


<details>
  <summary>Details</summary>
Motivation: LLM训练资源密集，量化训练能提高计算和内存效率，但会引入量化噪声，影响收敛和模型精度。随机舍入虽然提供无偏梯度估计，但其与批次大小等训练因素的相互作用尚未充分研究。

Method: 对带有随机舍入的mini-batch随机梯度下降进行理论和实证研究，分析量化权重和激活值对梯度方差的不同影响。

Result: 实验验证了理论见解：增加批次大小可以补偿反向传播精度降低，权重和激活值的量化对梯度方差产生不同影响。

Conclusion: 通过适当调整批次大小，可以在量化训练中保持模型性能，权重量化和激活值量化需要不同的处理策略。

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [109] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种结合Kronecker分解近似曲率(K-FAC)二阶策略优化与安全感知梯度操作的安全强化学习算法，通过边界感知梯度混合和KL回滚策略，在安全约束下实现更高的平均回报。


<details>
  <summary>Details</summary>
Motivation: 解决安全强化学习中奖励最大化与约束满足之间的权衡问题，避免传统方法中固定硬阈值导致的性能下降和不稳定问题。

Method: 使用K-FAC高效近似Fisher信息矩阵进行自然梯度更新，引入边界感知梯度操作机制自适应调整奖励和成本梯度影响，采用小批量KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium基准测试中，KFCPO比最佳基线方法平均回报提高10.3%到50.2%，在保持安全约束的同时实现更好的性能平衡。

Conclusion: KFCPO通过二阶优化和自适应梯度操作，在安全强化学习中实现了安全约束与性能的更好平衡，证明了所提方法的有效性。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [110] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图划分的可解释聚类新方法，能够为任何给定的非可解释聚类或数据集构建解释树，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释聚类方法主要针对特定聚类目标最小化可解释性代价，缺乏通用的方法来为任意给定聚类构建解释树，而不受限制。

Method: 基于谱图划分设计可解释聚类算法，使用Trevisan(2013)的广义框架将现有算法统一解释为同时在两个图上优化割的图划分方法。

Result: 实验表明，该方法在多个数据集上相比基线方法表现出更好的性能。

Conclusion: 提出的谱图划分方法为可解释聚类提供了通用解决方案，能够灵活适应各种聚类场景，并统一了现有算法的理论框架。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [111] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了一种基于范畴对称性的学习框架，通过将时间、尺度和传感器层次结构等对称性因素融入特征表示，使模型在现实扭曲下保持稳定，显著提升了人体活动识别的分布外准确率。


<details>
  <summary>Details</summary>
Motivation: 人体活动识别面临传感器信号随环境、运动等因素变化的挑战，需要模型在环境变化时保持稳定性。

Method: 构建范畴对称感知学习框架，将时间、尺度和传感器层次结构的对称性因素融入特征表示结构，使模型自动保持传感器间关系并在时间偏移、幅度漂移和设备方向变化等现实扭曲下保持稳定。

Result: 在UCI人体活动识别基准测试中，该方法将分布外准确率提高了约46个百分点（约3.6倍于基线）。

Conclusion: 抽象对称性原理通过范畴等变表示理论可以在日常感知任务中转化为具体的性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [112] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 该论文通过布尔函数分析研究LIF脉冲神经网络的噪声敏感性和稳定性，发现宽LIF-SNN分类器在平均意义上是稳定的，这源于其傅里叶频谱在低频成分上的集中。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)在能效计算方面具有潜力，但其理论基础特别是关于稳定性和鲁棒性的研究相比人工神经网络仍有限。

Method: 使用布尔函数分析框架研究离散时间LIF-SNNs，引入频谱简单性概念来形式化傅里叶频谱集中的简单性。

Result: 主要结果表明宽LIF-SNN分类器在平均意义上是稳定的，随机LIF-SNNs偏向于简单函数，实验证实这些稳定性特性在实践中持续存在。

Conclusion: 这些结果为SNNs的稳定性和鲁棒性特性提供了新的理论见解，连接了深度网络中观察到的简单性偏差。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [113] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出基于能量的统一框架来理解Transformer注意力机制，将标准softmax注意力视为最小化亥姆霍兹自由能量的特例，并基于该框架提出了动量、NAG和牛顿法等新注意力结构。


<details>
  <summary>Details</summary>
Motivation: Transformer已成为现代大语言模型的核心架构，但其工作机制仍有待探索。能量视角长期以来为理解神经计算提供了有价值的原则，本文旨在通过能量视角重新审视基于注意力的Transformer模型。

Method: 提出了统一能量框架，包含三个关键组件：全局能量F*、能量函数Ei和梯度下降形式。将标准softmax注意力视为最小化亥姆霍兹自由能量的特例，并将线性注意力自然纳入该框架。基于经典GD算法扩展了原始注意力公式，提出了动量GD、NAG和牛顿法变体。

Result: 实验初步支持了基于能量框架设计注意力机制的潜力。

Conclusion: 能量视角为理解Transformer注意力机制提供了统一框架，能够自然地解释标准注意力并启发新注意力结构的设计，为Transformer工作机制的深入理解提供了新视角。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [114] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: RhythmiNet是一种结合PPG和加速度计信号的多模态神经网络，通过时间与通道注意力机制改进心房颤动检测，在嘈杂的临床数据中实现三类心律分类（AF、窦性心律、其他）


<details>
  <summary>Details</summary>
Motivation: 腕戴式PPG设备在连续心律监测中易受运动伪影和生理噪声影响，现有方法多依赖单通道PPG且仅限于二元AF检测，无法捕捉临床中更广泛的心律失常类型

Method: 提出RhythmiNet残差神经网络，集成时间和通道注意力模块，联合利用PPG和加速度计信号，按运动强度百分位数分层测试数据而不排除任何片段

Result: RhythmiNet相比仅使用PPG的基线模型在macro-AUC上提升4.3%，比基于手工HRV特征逻辑回归模型性能提升12%

Conclusion: 多模态融合和注意力学习在嘈杂真实临床数据中具有显著优势，能有效提高心律分类的鲁棒性和准确性

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [115] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，从容量控制的角度解释归一化层在深度神经网络中的作用，证明了归一化能指数级降低Lipschitz常数，从而平滑损失景观和约束网络容量。


<details>
  <summary>Details</summary>
Motivation: 归一化方法在现代深度神经网络中是基础组件，经验上已知能稳定优化动态和改善泛化能力，但其理论机制，特别是在使用多个归一化层时，仍缺乏解释。

Method: 开发了一个理论框架，通过容量控制的角度分析归一化作用，证明了未归一化DNN可能具有指数级大的Lipschitz常数，而归一化层能指数级降低该常数。

Result: 归一化层能以指数速率减少Lipschitz常数，这平滑了损失景观（促进更快更稳定的优化）并约束了网络有效容量（增强泛化保证）。

Conclusion: 该研究为深度学习中的归一化方法的经验成功提供了原则性解释，揭示了其在优化和泛化方面的理论机制。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [116] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 提出一种使用优化合成数据来评估机器学习模型性能的方法，在有限标注数据条件下实现准确可靠的测试误差估计。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估方法需要大量标注测试集，但在许多实际应用中获取大规模标注数据成本高昂且耗时，因此需要在有限标注样本下进行可靠评估。

Method: 开发了考虑合成数据的泛化边界理论，提出基于理论指导的优化合成数据生成方法，用于模型评估。

Result: 在仿真和表格数据集上的实验表明，相比现有基线方法，该方法能够获得更准确可靠的测试误差估计。

Conclusion: 合成数据可以有效地用于模型性能评估，生成器质量对评估准确性具有重要影响，理论指导的优化合成数据生成方法优于现有方法。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [117] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: NicheFlow是一个基于流的生成模型，用于推断连续空间切片中细胞微环境的时序轨迹，通过最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。


<details>
  <summary>Details</summary>
Motivation: 理解细胞微环境的时空演化对于解析组织发育和疾病进展至关重要。现有方法在单细胞水平建模细胞演化，忽略了组织中细胞状态的协调发育。

Method: 将局部细胞邻域表示为点云，使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。

Result: 该方法在从胚胎到大脑发育的多种时空数据集中成功恢复了全局空间结构和局部微环境组成。

Conclusion: NicheFlow能够有效推断细胞微环境的时空演化轨迹，为理解组织发育提供了新工具。

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [118] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出了一种解决多模态学习中模态不平衡问题的新框架，通过互信息量化模态间交互，采用跨模态知识蒸馏和多任务式训练来平衡不同模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习面临模态不平衡问题，特别是在生物数据分析中，数据集有限、获取成本高且质量不均，传统方法难以同时利用模态间协同效应和解决模态冲突。

Method: 采用两阶段策略：跨模态知识蒸馏预训练阶段利用强模态增强弱模态预测能力；主训练阶段使用多任务式学习机制，基于模态特定性能指标和模态间互信息动态校准梯度贡献。

Result: 该方法有效缓解了模态不平衡问题，显著提升了多模态模型的整体性能。

Conclusion: 提出的统一框架通过互信息量化和平衡学习策略成功解决了多模态学习中的模态不平衡挑战，为生物数据分析等领域的多模态应用提供了有效解决方案。

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [119] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: Hydra是一个双头元上下文记忆模块，通过二维递归在时间和变量维度上学习记忆模式，解决了现有时间序列模型在时间归纳偏置、变量间依赖关系和长序列建模效率方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型（如Transformer、MLP和线性模型）存在三个主要问题：(1)缺乏时间归纳偏置，(2)无法捕捉时间和变量维度间的相互依赖，(3)长序列建模效率低。线性RNN虽然解决了部分问题，但仅限于单序列且会传播误差。

Method: 提出Hydra模型，采用双头元上下文记忆模块，通过二维递归在时间和变量维度上同时处理信息。虽然二维递归训练不可并行化，但开发了2D分块训练算法，在保持有效性的同时将效率提升10倍。

Result: 在时间序列预测、分类和异常检测等多个任务和数据集上的实验结果表明，Hydra相比最先进的基线方法表现出优越性能。

Conclusion: Hydra通过二维递归机制有效解决了现有时间序列模型的局限性，在多个任务上实现了更好的性能，同时通过创新的训练算法保证了效率。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [120] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 提出ProfBO算法，通过MDP先验建模相关源任务的优化轨迹，显著减少黑盒优化所需的函数评估次数


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在现实关键应用中（如药物发现、材料设计）因评估成本高而变得不实用，需要减少评估次数

Method: 使用MDP先验建模优化轨迹，嵌入到先验拟合神经网络中，采用模型无关元学习快速适应新任务

Result: 在真实Covid和Cancer基准测试及超参数调优任务中，ProfBO始终优于最先进方法，用更少评估获得高质量解

Conclusion: ProfBO算法通过过程知识建模显著减少了函数评估需求，已准备好实际部署

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [121] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 提出SymRegg算法，利用等式图(e-graph)结构避免符号回归中冗余表达式计算，提高搜索效率


<details>
  <summary>Details</summary>
Motivation: 遗传编程在符号回归中因中性概念产生大量冗余表达式计算(可达总评估数的60%)，需要更高效的搜索方法

Method: 基于e-graph结构的新搜索算法：从e-graph中采样表达式进行扰动，若生成未访问表达式则插入e-graph并生成等价形式

Result: SymRegg提高了搜索效率，在不同数据集上保持准确结果，且只需极简超参数选择

Conclusion: e-graph结构能有效避免符号回归中的冗余计算，SymRegg算法在效率和准确性方面表现优异

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [122] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文主张在AI智能体设计中应将数据能力作为首要考虑因素，提出了实现数据智能智能体的四个关键能力：主动数据获取、复杂数据处理、交互式测试数据合成和持续适应。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体研究主要关注推理能力，但数据管理能力被忽视。为了确保智能体在现实世界中的可靠部署，需要设计能够持续获取、处理和演化数据的智能体。

Method: 提出了四个关键能力框架：1) 主动数据获取 - 智能体自主收集关键知识或寻求人类输入填补数据空白；2) 复杂数据处理 - 上下文感知和灵活处理多样化数据挑战；3) 交互式测试数据合成 - 从静态基准转向动态生成的交互式测试数据；4) 持续适应 - 迭代优化数据和背景知识以适应环境变化。

Result: 通过强调数据智能能力，为构建更可靠、适应性更强的AI智能体提供了理论框架和设计指导。

Conclusion: 数据智能智能体应成为以数据为中心AI的下一个前沿领域，当前研究需要从单纯关注推理转向同等重视数据管理能力。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [123] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的短期停电预测系统，用于极端天气事件期间预测，通过两阶段特征工程和鲁棒优化策略，在24-48小时预测范围内比基线方法提升8.4%性能。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件导致电力中断频发，需要准确的短期停电预测来支持应急响应和资源调配。

Method: 采用两阶段特征工程（数据清洗+相关性过滤），结合时间嵌入、多尺度滞后特征和天气变量作为SARIMAX模型的外生输入，实施分层拟合策略和鲁棒优化方法。

Result: 模型在短期预测中达到RMSE 177.2，相比基线方法（RMSE 193.4）提升了8.4%的性能。

Conclusion: 提出的特征工程和鲁棒优化策略能有效提升极端天气相关停电预测的准确性，为电力系统应急管理提供可靠工具。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [124] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 评估GAN生成医疗数据的公平性，提出MedEqualizer框架改善受保护人口属性的代表性不平衡问题


<details>
  <summary>Details</summary>
Motivation: 合成医疗数据可提高数据可访问性，但需确保跨受保护属性的公平性，避免临床研究和决策中的偏见

Method: 使用MIMIC-III数据集评估多种GAN模型的公平性，引入MedEqualizer模型无关增强框架来丰富代表性不足的子群体

Result: 观察到合成数据中存在显著子群体代表性不平衡，MedEqualizer显著改善了合成数据的人口统计学平衡

Conclusion: MedEqualizer为更公平和代表性的医疗数据合成提供了可行路径

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [125] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 使用COLET数据集通过窗口化特征生成和机器学习/深度学习技术对认知负荷进行分类，深度学习模型特别是表格架构在各项指标上优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 认知负荷在健康、心理学和国防应用等领域日益重要，需要有效的方法来实时评估复杂动态任务中的认知负荷水平。

Method: 采用窗口化时间分割方法增强特征，然后使用机器学习和深度学习模型对认知负荷的不同水平进行分类。

Result: 深度学习模型特别是表格架构在精确度、F1分数、准确率和分类精度方面都优于传统机器学习方法。

Conclusion: 窗口化时间特征提取方法有效，深度学习技术在实时认知负荷评估中具有巨大潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [126] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 该论文挑战了反向传播(BP)对高性能至关重要的长期假设，提出Mono-Forward(MF)算法在MLP架构上超越BP基准，实现更高分类精度、更低能耗和更快训练。


<details>
  <summary>Details</summary>
Motivation: 挑战反向传播在深度学习中的核心地位，探索更高效、可持续的替代算法，解决BP在能耗和训练效率方面的局限性。

Method: 提出从Forward-Forward(FF)到Cascaded Forward(CaFo)再到Mono-Forward(MF)的进化路径，在相同架构和超参数优化框架下进行公平比较，并重新评估无BP方法的内存效率。

Result: MF算法在MLP架构上持续超越优化调优的BP基准，分类精度更高，能耗降低达41%，训练速度提升达34%。

Conclusion: MF算法被确立为MLP的实用、高性能且可持续的BP替代方案，为深度学习训练范式提供了新的可能性。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [127] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于幸福度概念的新公平性框架，通过线性规划计算最优公平后处理策略，统一并扩展了多种已知公平性定义


<details>
  <summary>Details</summary>
Motivation: 现有公平性定义缺乏直观性，需要更人性化且数学严谨的方法来度量群体从决策结果中获得的效用

Method: 基于幸福度概念构建公平性框架，使用线性规划求解最优公平后处理策略

Result: 方法高效且可扩展，能够统一和扩展多种已知公平性定义，在多样化场景中表现出实际优势

Conclusion: 幸福度框架为公平性提供了直观且数学严谨的度量方法，通过线性规划实现高效公平决策

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [128] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 本文挑战AI研究中的"规模原教旨主义"，提出以能力-资源比而非单纯能力为导向的LLM开发框架，通过梯度影响模式指导资源分配，显著提高AI生命周期效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中无限制的模型规模和计算增长导致了不可持续的环境影响和资源不平等扩大，需要从根本上重新定位LLM开发方向。

Method: 提出基于梯度影响模式的资源分配理论框架，利用transformer模型中少数参数具有超大影响力（遵循重尾分布）的特性，仅更新高影响力参数，使用简单梯度范数作为识别高影响力组件的计算高效代理，协调参数和数据选择实现效率倍增。

Result: 仅更新高影响力参数在性能-资源基础上严格优于全参数调优，梯度范数能有效识别高影响力组件，协调参数和数据选择可减少数个数量级的资源需求。

Conclusion: 通过将资源意识嵌入模型开发、适应和评估过程，可以重塑AI进步方向，实现更可持续和公平的未来，将硬件变通方案转变为理论最优策略，民主化尖端AI能力访问同时显著减少环境影响。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [129] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS是一个双代理架构，通过将推理（教师）与执行（学生）解耦，并利用持久学习记忆实现梯度自由的持续学习，在推理时动态调整操作策略。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖基于梯度的重训练，不适合需要实时适应的部署代理。

Method: 采用双代理架构，教师负责推理，学生负责执行，包含持久学习记忆存储经验指导，通过编排层在推理时动态调整操作策略。

Result: 在微软ExCyTIn-Bench基准测试中，ATLAS使用GPT-5-mini获得54.1%成功率，比GPT-5（High）高13%，成本降低86%。跨事件验证显示泛化能力：从事件#5的冻结手册将准确率从28%提升到41%。

Conclusion: 梯度自由持续学习是构建自适应、可部署AI系统的可行路径，为训练显式世界模型提供了因果标注轨迹。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [130] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 本文提出了一种基于Kolmogorov-Arnold网络的温和生成型神经算子模型，用于求解具有随机终止时间的二阶倒向随机微分方程(2BSDEs)家族。


<details>
  <summary>Details</summary>
Motivation: 解决无限族二阶倒向随机微分方程在正则有界欧几里得域上的数值近似问题，特别是针对具有随机终止时间的情况。

Method: 使用Kolmogorov-Arnold网络构建生成型神经算子模型，通过多项式参数数量来近似2BSDEs的解算子。

Result: 证明了对于广泛的2BSDEs族，其解算子可以通过适当的神经算子模型进行近似，并且识别出一个结构化子类，其神经算子近似仅需要在倒数近似率的多项式数量参数。

Conclusion: 该方法相比一般最坏情况下的神经算子保证，将参数需求从指数级降低到多项式级，提高了计算效率。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [131] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 提出了一种新的搜索方向，使得一阶和零阶随机在线双层优化算法无需窗口平滑即可实现次线性随机双层遗憾。


<details>
  <summary>Details</summary>
Motivation: 当前在线双层优化方法依赖确定性窗口平滑遗憾最小化，在函数快速变化时无法准确反映系统性能。

Method: 引入新的搜索方向，减少超梯度估计的oracle依赖，同时更新内外层变量和线性系统解，使用零阶方法估计Hessian、Jacobian和梯度。

Result: 实验验证了在在线参数损失调优和黑盒对抗攻击任务上的有效性。

Conclusion: 该框架提供了无需窗口平滑的次线性随机双层遗憾保证，并提高了效率。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [132] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 使用几何不变量理论(GIT)研究深度线性网络(DLN)，通过Kempf-Ness定理建立L2正则化器在平衡流形上最小化，将训练动态分解为纤维上的正则化流和平衡流形上的学习流。


<details>
  <summary>Details</summary>
Motivation: 为深度学习和线性系统理论中的平衡性提供统一的数学框架，从模型简化和贝叶斯原理角度解释平衡性。

Method: 应用几何不变量理论和Kempf-Ness定理，将训练动态分解为两个梯度流：纤维上的正则化流（使用矩映射精确求解）和平衡流形上的学习流。

Result: 建立了L2正则化器在平衡流形上最小化的理论框架，实现了训练动态的分解和正则化流的精确求解。

Conclusion: 该框架为深度学习和线性系统理论中的平衡性概念提供了统一的数学解释，连接了模型简化和贝叶斯原理。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [133] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出了一种结合元学习和领域适应的统一框架，使自动调制分类系统能够抵抗对抗攻击和环境变化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现出色，但易受对抗攻击和数据分布变化影响，阻碍了在实际动态环境中的部署。

Method: 采用两阶段策略：离线阶段使用元学习在单一源域上训练模型，使其对未见过的攻击具有泛化防御能力；在线阶段应用领域适应将模型特征与新目标域对齐。

Result: 该框架显著提高了调制分类在面对组合威胁时的准确性。

Conclusion: 该框架为解决现代AMC系统的部署和操作挑战提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [134] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种基于函数逼近定理的正交函数自适应(OFA)方法，用于提升多治疗场景下的提升建模效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多治疗提升建模技术通常从二元治疗工作改编而来，分为结构自适应和特征自适应两种类型，但这些方法在不同数据特征下无法保持有效性。

Method: 基于函数逼近定理提出正交函数自适应(OFA)方法，通过综合实验评估各种模型自适应技术的有效性和鲁棒性。

Result: 实验结果表明，提出的OFA方法相比其他基础自适应方法能显著提高提升模型性能，并展现出最高的鲁棒性。

Conclusion: OFA方法在多治疗提升建模中具有显著优势，能够有效应对各种数据特征挑战，提高估计能力和鲁棒性。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [135] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: 该论文研究了思维链(CoT)是否能增强transformer模型在所有推理任务中的能力，发现推理本质上是记忆问题，证明了CoT在某些推理任务中并不能提升transformer的能力。


<details>
  <summary>Details</summary>
Motivation: 探索思维链(CoT)是否能在所有推理任务中增强transformer模型的能力，分析CoT transformer的记忆能力。

Method: 通过分析固定精度transformer在有/无CoT情况下的记忆能力，给出记忆有限推理数据集的充要条件，并推导参数数量的上下界。

Result: 证明了有/无CoT的transformer记忆有限数据集所需参数数量均为Θ(N)，表明存在CoT无法增强transformer推理能力的任务。

Conclusion: CoT并不能在所有推理任务中增强transformer的能力，某些简单无限数据集甚至无法被有/无CoT的transformer记忆。

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [136] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出一个基于多任务RF信号分类的鲁棒框架，用于共享频谱环境中的发射机识别和协议分类，使用CNN处理信号重叠和环境变化等挑战，在POWDER平台数据上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着频谱共享日益重要，频谱监测和发射机识别对于执行频谱使用政策、提高频谱利用效率和保障网络安全至关重要。

Method: 设计卷积神经网络(CNN)，采用多通道输入策略提取有意义的信号特征，解决信号特征重叠和环境变化等关键挑战。

Result: 在POWDER平台RF数据上取得了显著准确率：协议分类90%，发射基站分类100%，联合分类任务92%。

Conclusion: 该方法在增强现代无线网络中的频谱监测、管理和安全方面具有显著潜力。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [137] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC协议，用于在LLM性能和API成本波动的情况下，确保测试时计算方法的公平评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效，需要一种公平的评估协议。

Method: 设计了FEval-TTC协议，标准化少样本提示和答案提取过程，支持在多个LLM和多样化数据集上评估基于思维链的测试时计算方法，并提供成本建模程序。

Result: 开发了开源评估框架，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC协议为测试时计算方法的评估提供了标准化和公平的基准，有助于在LLM性能波动的情况下保持研究结论的有效性。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [138] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站选址，通过混合奖励函数和双Q网络减少平均等待时间53.28%。


<details>
  <summary>Details</summary>
Motivation: 电动汽车快速增长需要优化充电站布局，但现有强化学习方法因确定性奖励系统无法处理现实世界动态不确定性，导致评估成本高且不准确。

Method: 集成深度强化学习与基于代理模拟，使用混合RL代理和双Q网络选择最优位置和配置充电端口，采用结合确定性因素和模拟反馈的混合奖励函数。

Result: 在越南河内的案例研究中，相比初始状态平均等待时间减少53.28%，优于静态基线方法。

Conclusion: 该可扩展自适应解决方案增强了电动汽车基础设施规划，有效应对现实世界复杂性并改善用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [139] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: WindMiL是一个结合系统化数据集生成和对称感知图神经网络的机器学习框架，用于高效预测建筑风荷载。


<details>
  <summary>Details</summary>
Motivation: 传统风洞测试和大涡模拟(LES)成本过高，每个LES案例需要至少24小时计算，无法进行大规模参数研究。

Method: 1) 通过符号距离函数插值生成屋顶几何形状，模拟462个不同形状和风向的LES案例；2) 开发反射等变图神经网络，确保在镜像几何下物理一致的预测。

Result: 在插值和外推评估中，WindMiL对表面压力系数的均值和标准差都达到高精度(RMSE ≤ 0.02)，在反射测试中保持96%以上的命中率，而非等变基线模型下降超过10%。

Conclusion: 通过将系统化数据集与等变代理模型结合，WindMiL实现了对建筑风荷载的高效、可扩展和准确预测。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [140] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 变量消除算法（如VarPro）通过重塑临界点结构，将原始优化问题中的鞍点转化为简化问题中的局部极大值，从而有效导航复杂非凸优化景观。


<details>
  <summary>Details</summary>
Motivation: 理解为什么变量消除算法在实践中表现出优越的收敛性和鲁棒性，特别是在处理大规模非凸优化中的鞍点问题时。

Method: 基于Hessian惯性和Schur补的严格几何分析，比较原始和简化公式的优化景观，证明变量消除如何重塑临界点结构。

Result: 在非凸矩阵分解、双参数神经网络和深度残差网络训练中验证了方法的有效性，显著提高了稳定性和收敛到更优极小值的能力。

Conclusion: 通过鞍点变换实现景观简化是一个强大原则，可以指导设计更鲁棒高效的优化算法。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [141] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: KAT-GNN是一个结合临床知识和时序动态的图神经网络框架，用于电子健康记录的风险预测，在冠状动脉疾病和院内死亡率预测任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据具有异构性和不规则时序特性，这给临床风险预测带来了挑战。需要开发能够整合临床知识和时序动态的模型来提高预测准确性。

Method: 构建模态特定的患者图，使用SNOMED CT本体知识和EHR共现先验进行图增强，然后使用时序感知transformer捕捉纵向动态。

Result: 在冠状动脉疾病预测中AUROC达到0.9269±0.0029，在MIMIC-III和MIMIC-IV的院内死亡率预测中分别达到0.9230±0.0070和0.8849±0.0089，均优于基线方法。

Conclusion: 将临床知识整合到图表示中，结合时序注意力机制，为不同临床任务和数据集的风险预测提供了有效且可泛化的方法。

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [142] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的在线鲁棒张量恢复算法，用于智能交通系统中的交通数据恢复，能够同时处理缺失值和异常值，在保持高恢复精度的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统批处理方法计算和存储需求大，难以应对持续增长的交通数据量；现有的在线张量恢复方法在复杂现实场景中性能下降严重，未能充分利用交通数据的内在结构特性。

Method: 将交通数据恢复问题重新表述为流式框架，提出同时利用交通数据的全局时空相关性和局部一致性的在线鲁棒张量恢复算法。

Result: 在三个真实交通数据集上的实验结果表明，该方法实现了高恢复精度，同时计算效率相比最先进的批处理方法提高了三个数量级。

Conclusion: 该方法作为智能交通系统中交通数据质量增强的可扩展有效解决方案具有巨大潜力。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [143] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一个对抗性时空注意力网络，用于从多变量EEG信号预测癫痫发作，通过级联注意力块联合建模空间大脑连接性和时间神经动态，实现高敏感性和低误报率。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测在医疗时间序列预测中面临关键挑战，需要高敏感性、低误报率和受试者特异性适应性。现有方法假设固定的发作前持续时间或分别处理空间和时间特征，无法有效捕捉时空模式的双向依赖关系。

Method: 提出STAN模型，通过级联注意力块交替使用空间和时间模块，联合建模空间大脑连接性和时间神经动态。采用带梯度惩罚的对抗训练来鲁棒区分发作间期和发作前状态。

Result: 在两个基准EEG数据集上取得最先进性能：CHB-MIT头皮数据集96.6%敏感性、0.011次/小时误报；MSSM颅内数据集94.2%敏感性、0.063次/小时误报。模型计算高效（230万参数，45ms延迟，180MB内存），适合实时边缘部署。

Conclusion: STAN框架不仅为癫痫发作预测提供了有效解决方案，还为医疗和其他时间序列领域的时空预测提供了一个通用范式，特别适用于个体异质性和可解释性至关重要的场景。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [144] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 开发了一个轻量级一维卷积神经网络CaptureNet-Deep，用于自动检测纳米孔蛋白质测序中的捕获阶段，将分析时间从几天缩短到30分钟以内。


<details>
  <summary>Details</summary>
Motivation: 纳米孔蛋白质测序产生的电流信号中，捕获阶段的识别需要专家手动分析，耗时数天且依赖领域专业知识。

Method: 使用轻量级一维CNN在降采样信号窗口中检测捕获阶段，并与CNN-LSTM混合模型、基于直方图的分类器和其他CNN变体进行比较。

Result: 最佳模型CaptureNet-Deep在测试数据上达到F1分数0.94和精确度93.39%，支持低延迟推理并集成到实验仪表板中。

Conclusion: 使用简单可解释的轻量级机器学习模型可以实现高效的实时捕获检测，在测序工作流程中具有广泛应用前景。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [145] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 提出了一种基于归纳偏置的神经控制李雅普诺夫函数学习方法，通过将李雅普诺夫条件作为指导原则设计神经网络结构，实现了CLF和控制器的端到端学习，相比现有方法具有更高的收敛率和更大的吸引域。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习-验证框架的深度学习方法将李雅普诺夫条件作为复杂约束进行优化，难以实现全局收敛，且验证过程复杂。需要改进这一框架以提升学习效率和性能。

Method: 将李雅普诺夫条件作为归纳偏置，设计神经CLF和基于CLF的控制器，实现稳定优化过程和端到端学习，仅需有限约束。

Result: 在大量实验案例中，相比现有方法，该方法实现了更高的收敛率和更大的吸引域。同时深入揭示了先前方法在学习过程中成功率下降的原因。

Conclusion: 通过将李雅普诺夫条件作为归纳偏置指导神经网络设计，可以显著提升CLF学习的效果，实现更稳定的优化过程和更好的性能表现。

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [146] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 该论文探索使用数据驱动的Koopman方法来建模无人机轨迹动态，以解决飞行自组织网络(FANETs)中动态环境带来的挑战，通过集中式和分布式方法预测信号质量，提高网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习技术在静态无线环境中表现良好，但在高度动态的FANETs环境中存在局限性，需要开发能够适应网络拓扑不断变化的新方法。

Method: 利用Koopman算子理论，提出集中式和分布式两种方法，通过预测无人机轨迹动态和信号干扰加噪声比(SINR)来建模FANETs通信性能。

Result: 实验结果显示这些方法能够准确预测导致通信中断的连接和隔离事件，为无人机基于预测结果调度传输提供可能。

Conclusion: 数据驱动的Koopman方法能够有效应对FANETs动态环境挑战，提高无人机网络的通信可靠性和性能。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [147] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed是一个鲁棒且通信高效的联邦学习框架，通过局部敏感哈希将高维梯度压缩为二进制表示，在保护隐私的同时有效检测恶意梯度，显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在信任缺失环境中易受推理攻击和投毒攻击，现有防御方法存在通信计算成本高或检测精度有限的问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，使用多超平面局部敏感哈希将高维梯度投影为紧凑的二进制表示，仅通过不可逆哈希形式检测恶意梯度。

Result: 实验表明，即使50%参与者为合谋攻击者，LSHFed仍能保持高模型性能，梯度验证通信量比全梯度方法降低1000倍。

Conclusion: LSHFed在增强聚合鲁棒性和隐私保护的同时，显著提升了联邦学习的通信效率。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [148] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 提出基于去噪扩散概率模型(DDPM)的云原生网络功能(CNF)放置新框架，将放置问题重构为生成式图到分配任务，通过图神经网络去噪器迭代优化分配矩阵，实现快速可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统方法(混合整数非线性规划、启发式算法、强化学习)在可扩展性、约束处理和泛化能力方面存在局限，无法满足5G/6G网络中CNF在云连续体上的高效放置需求。

Method: 将CNF放置问题编码为异构图，训练图神经网络去噪器迭代优化噪声CNF到云的分配矩阵，在损失函数中直接集成约束特定损失以学习可行解空间。

Result: 在多种拓扑结构上的广泛评估表明，该模型能持续产生可行解，推理速度比MINLP求解器快数个数量级。

Conclusion: 基于扩散的生成建模在约束网络嵌入问题中具有巨大潜力，为实现分布式云原生网络功能的实用、可扩展编排做出了贡献。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [149] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出了MiniFool算法，这是一种基于物理启发的对抗攻击方法，用于测试粒子物理和天体物理中的神经网络分类任务。该算法通过最小化结合χ²检验统计量和目标分数偏差的成本函数来工作。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够测试神经网络在粒子物理和天体物理分类任务中鲁棒性的对抗攻击算法，特别针对IceCube中微子天文台的天体τ中微子搜索，同时验证其跨领域的通用性。

Method: 基于最小化成本函数的方法，该函数结合了χ²检验统计量（基于实验不确定性量化扰动概率）与期望目标分数的偏差。通过攻击参数缩放实验不确定性来测试分类变化的稳健性。

Result: 研究发现，对于正确分类和错误分类的事件，分类翻转的可能性存在差异。通过攻击参数可以量化网络决策的鲁棒性，并且能够测试未标记实验数据的分类稳健性。

Conclusion: MiniFool算法提供了一种有效的方法来评估神经网络分类器在粒子物理和天体物理应用中的鲁棒性，能够量化决策稳健性并适用于未标记数据，具有跨领域的通用适用性。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [150] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 提出可验证的分割学习框架，通过集成zk-SNARK证明来确保分割学习中客户端和服务器端计算的正确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能实现数据分离下的协作训练，但缺乏验证各方计算正确性和诚实性的能力。

Method: 在服务器端的前向传播和反向传播中为双方生成zk-SNARK证明和验证，确保双向可验证性。

Result: 与仅记录更新但不生成零知识证明的区块链系统相比，zk-SNARK测试实现了可验证性和正确性，而区块链虽然轻量但不可验证。

Conclusion: 应用zk-SNARK证明能够在分割学习中实现计算的可验证性和正确性保障。

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [151] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出了一种基于重参数化的多模态强化学习方法，通过距离多样性正则化在多样性关键场景中实现更好的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法使用确定性或单峰高斯策略，无法表达复杂的多模态决策分布，这限制了在多样性关键场景中的性能。

Method: 首先将现有难处理的多模态策略统一框架化，证明可以通过重参数化直接优化；然后提出基于距离的多样性正则化，无需显式计算决策概率。

Result: 在多目标达成和生成式RL等多样性关键领域表现出优势，特别是在少样本鲁棒性方面；在传统MuJoCo基准测试中也显示出竞争力。

Conclusion: 摊销策略是一种有前景的策略模型类别，具有强大的多模态表达能力和高性能。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [152] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: 本文研究了在LeNet神经网络上应用遗忘技术来防御FGSM对抗攻击，发现该方法能显著提升模型对这类攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易受到对抗攻击，特别是FGSM攻击，需要开发有效的防御方法来提高模型安全性。

Method: 在LeNet神经网络上应用机器遗忘技术，通过重新训练模型来"忘记"对抗样本中的扰动。

Result: 实验表明，遗忘FGSM攻击的方法能显著提升LeNet网络对这类对抗攻击的鲁棒性。

Conclusion: 机器遗忘技术是防御FGSM对抗攻击的有效方法，能显著提高神经网络模型的鲁棒性。

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [153] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出了首个实域完全原位FFT框架(rdFFT)，通过利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，消除中间缓存使用，实现输入输出内存空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有FFT实现（包括标准FFT和实FFT）无法实现真正的原位计算，特别是实FFT将大小为n的输入映射到大小为n/2+1的复数输出，导致维度不匹配并需要额外内存分配。

Method: 利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，完全消除中间缓存使用，保持输入输出内存空间一致性。

Result: 在多个自然语言理解任务上的实验表明，该方法能有效降低训练内存成本。

Conclusion: rdFFT为频域轻量级适配提供了一个有前景的方向，实现了真正的原位FFT计算。

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [154] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出基于图的卫星嵌入方法，通过建模空间关系和模糊标签损失来预测撒哈拉以南非洲的贫困指数，提高财富预测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全球南方地区缺乏精细化的贫困地图，DHS调查数据空间覆盖有限且坐标因隐私保护而随机位移，降低了数据质量。

Method: 使用低维AlphaEarth卫星嵌入，通过图结构建模调查点和未标记位置的空间关系，引入概率模糊标签损失来处理坐标位移问题。

Result: 在37个DHS数据集上的实验表明，相比仅使用图像的基线方法，加入图结构略微提高了预测准确性。

Conclusion: 紧凑的EO嵌入在大规模社会经济制图中具有潜力，图结构方法能够改善财富预测的泛化性能。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [155] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了CG-FKAN方法，通过稀疏化和仅传输关键系数来压缩KAN网络中的扩展网格，在通信受限的联邦学习环境中降低RMSE达13.6%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私关键应用中广泛使用，但存在可解释性有限的问题。KAN网络通过可学习的样条函数解决了这一限制，但现有研究忽略了网格扩展引入的通信开销。

Method: 提出CG-FKAN方法，在通信预算约束下，通过稀疏化并仅传输必要的系数来压缩扩展网格。

Result: 实验表明，在通信受限设置下，CG-FKAN比固定网格KAN实现了高达13.6%的RMSE降低。

Conclusion: 该方法不仅在实际性能上优于现有方法，还推导出了其近似误差的理论上界。

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [156] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出了一种在输入空间中定义的曲率度量——曲率率λ，通过高阶输入导数的指数增长率来衡量神经网络的功能平滑度，并开发了相应的正则化方法CRR。


<details>
  <summary>Details</summary>
Motivation: 现有的锐度度量通常在参数空间中定义（如Hessian特征值），存在计算昂贵、对重参数化敏感、难以在功能层面解释等问题。

Method: 引入曲率率λ作为输入空间中的标量曲率度量，通过log ||D^n f||与n的斜率估计，并开发了基于导数的正则化方法CRR来直接塑造输入空间几何。

Result: 在解析函数和神经网络上的实验表明，λ在训练过程中可预测地演化，CRR在达到相似准确率的同时，产生了更平坦的输入空间几何和改善的置信度校准。

Conclusion: 曲率率λ通过微分动力学将曲率概念基础化，为学习模型的功能平滑度提供了一个紧凑、可解释且参数化不变的特征描述符。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [157] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出了一种新的图曲率度量——有效电阻曲率，用于替代计算复杂度高的Ollivier-Ricci曲率，在保持几何表达能力的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 图曲率为图神经网络提供几何先验，但现有的Ollivier-Ricci曲率计算复杂度过高，限制了其在大规模图数据集上的应用。

Method: 使用节点对之间的有效电阻来量化沿图边传递消息的容易程度，替代最优传输距离，提出有效电阻曲率度量方法。

Result: 理论证明了有效电阻曲率的低计算复杂度，并在多种GNN任务上的广泛实验表明，该方法在保持与Ollivier-Ricci曲率相当性能的同时大幅减少计算开销。

Conclusion: 有效电阻曲率是Ollivier-Ricci曲率的有效替代方案，能够在保持几何表达能力的同时显著提高计算效率，适用于大规模图数据集。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [158] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 提出了DAMBench，首个大规模多模态基准测试，用于在真实大气条件下评估数据驱动的数据同化模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的数据同化研究存在两个关键局限：(1)依赖过度简化的合成观测场景；(2)缺乏标准化基准进行公平模型比较。

Method: 整合来自先进预报系统的高质量背景状态和真实多模态观测数据（气象站和卫星图像），提供统一评估协议并基准测试代表性方法，包括潜在生成模型和神经过程框架。

Result: 建立了严谨的研究基础，促进可重复性、公平比较和扩展到真实多模态场景的能力。

Conclusion: DAMBench为未来数据同化研究提供了标准化基准，数据集和代码已公开。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [159] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: MS-HGFN模型通过分层图神经网络和多尺度时空特征融合，解决了股票预测中忽略的股票内部属性模式和特征采样偏差问题，在美中股市数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度图神经网络在股票预测中经常忽略两个关键点：影响股票间相关性的股票内部属性模式，以及在多尺度采样中对粗细粒度特征的偏置关注。

Method: 提出MS-HGFN模型，包含分层GNN模块学习不同时间尺度上的内部属性模式和外部属性特征，形成动态图；采用自上而下的门控方法融合多尺度时空特征。

Result: 在美中股市真实数据集上的实验表明，MS-HGFN优于传统和先进模型，预测准确率提升高达1.4%，并在收益模拟中表现出更强的稳定性。

Conclusion: MS-HGFN通过全面捕捉时空依赖关系并有效融合多尺度特征，显著提升了股票市场预测的准确性和稳定性。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [160] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的特征提取方法，使用Hadamard矩阵的列向量或行向量作为不同长度的卷积核，在保持与现有方法兼容的同时提升计算效率、鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决当前时间序列分类方法计算复杂度高、参数调优和训练周期长的问题，同时改进轻量级方法在核选择和计算开销方面的不足。

Method: 使用Hadamard矩阵的列向量或行向量作为不同长度的卷积核进行特征提取，利用核的正交性提升性能。

Result: 在UCR时间序列数据集上实现SOTA性能：F1分数比ROCKET提升至少5%，训练时间比最快的miniROCKET缩短50%，可在超低功耗嵌入式设备上部署。

Conclusion: 提出的Hadamard卷积变换方法在保持兼容性的同时，显著提升了时间序列分类的计算效率和性能。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [161] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 提出了并行解耦框架(PDF)，通过利用MLLMs的可控性生成多个并行嵌入，解决了传统SSC范式将丰富多模态输入压缩为单一嵌入的局限性，显著提升了多模态嵌入模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态嵌入模型采用SSC范式（单一输入、单一嵌入、对比监督），无法充分利用MLLMs的能力，将丰富的多模态输入压缩为单一嵌入，限制了模型的表达能力。

Method: 使用共享MLLM骨干网络，通过不同的可学习前缀生成多个并行路径，获得并行嵌入。采用互信息最小化(MIM)约束确保并行多样性，结合每路径对比监督保持语义对齐。

Result: 在MMEB基准测试中显著提升性能：VLM2Vec-LLaVA-1.6-LR模型提升8.9%(7B)，VLM2Vec-Qwen2VL模型提升4.2%(2B)和3.1%(7B)。2B模型仅用一半计算预算就超越基线2.6%。

Conclusion: PDF框架有效解决了SSC范式的局限性，通过并行嵌入生成实现了更好的语义覆盖和泛化能力，且推理时仅需单次前向传播，计算开销可忽略。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [162] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 该研究提出了一个物理信息框架，将领域知识嵌入机器学习中，通过专用输入空间显著提高了航空航天复合材料冲击能量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前冲击能量预测方法受限于数据稀疏性、信号噪声、复杂特征依赖关系、非线性动力学、大规模设计空间以及逆问题的不适定性，需要更有效的解决方案。

Method: 结合物理动机特征设计和目标特征选择，从时域、频域和时频域提取特征，通过统计显著性、相关性过滤、降维和噪声鲁棒性确保物理相关性和可解释性。

Result: 使用优化的输入空间训练全连接神经网络，在多种冲击场景下验证，冲击能量预测误差比传统时间序列技术和纯数据驱动模型降低了三倍。

Conclusion: 该方法产生了具有统计鲁棒性和物理意义的紧凑特征集，实现了可解释且可追溯到可测量结构响应的冲击能量预测。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [163] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 本文重新审视了托普利兹协方差估计问题，通过过参数化梯度下降方法，证明了当参数数量是协方差矩阵维度的2-4倍时，能够实现全局收敛，并提出了加速变体方法。


<details>
  <summary>Details</summary>
Motivation: 受深度学习过参数化模型成功的启发，重新审视传统托普利兹协方差估计问题，探索简单梯度下降在过参数化设置下的表现。

Method: 将P×P协方差建模为K个复正弦波的叠加，通过梯度下降优化参数。当K=2P或4P时实现过参数化，并提出了分离振幅和频率学习率的加速变体。

Result: 过参数化梯度下降在挑战性设置下能够匹配或超越最先进方法的精度，同时保持简单性和可扩展性。当固定频率仅优化振幅时，优化景观是渐近良性的。

Conclusion: 过参数化梯度下降为托普利兹协方差估计提供了一种简单有效的替代方案，在适当过参数化下能够实现全局收敛并达到高精度。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [164] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多代理图推理系统，通过分解推理任务、优化LLM服务架构，显著提升了图推理的准确性、效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-CoT系统存在准确性低、token使用过多、延迟高和吞吐量低的问题，主要由于单代理整体提示、重复上下文重新编码和低效服务执行。

Method: 将推理分解为分类、推理、动作生成和图检索等专门代理，引入图感知的LLM推理机制，包括图特定KV缓存管理、基于优先级的驱逐和流水线执行。

Result: GLM将答案准确性提升高达38%，token成本降低95.7%，推理延迟降低90.3%，吞吐量提升15.1倍。

Conclusion: GLM通过多代理架构和优化服务机制，实现了高效的大规模复杂现实世界图推理。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [165] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: XTNet是一种用于多类别多值处理效应估计的新型网络架构，通过交叉效应估计模块和动态掩码机制捕捉处理交互，无需限制性结构假设，在合成和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 反事实因果推断在多类别、多值处理场景下面临挑战，现有方法局限于二元或单类型处理，存在假设限制、可扩展性不足和复杂干预场景评估框架不完善的问题。

Method: 提出XTNet架构，包含交叉效应估计模块和动态掩码机制，采用分解策略分离基本效应与交叉处理交互，实现组合处理空间的高效建模，并提出MCMV-AUCC评估指标。

Result: 在合成和真实数据集上的广泛实验表明，XTNet在排序准确性和效应估计质量上始终优于最先进的基线方法，真实世界A/B测试结果进一步证实其有效性。

Conclusion: XTNet为多类别多值处理效应估计提供了有效的解决方案，能够准确建模复杂处理交互，在真实场景中表现出优越性能。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [166] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出基于卡尔曼滤波的贝叶斯近似自然梯度下降方法，用于CLIP模型的少样本微调，提升分布外泛化能力和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有基于一阶梯度的微调方法收敛慢、对步长超参数敏感、在分布外数据上泛化能力差，而二阶方法能利用损失函数曲率信息提供更高效的更新。

Method: 使用卡尔曼滤波器对自然梯度下降进行贝叶斯近似，结合二阶优化和贝叶斯推断的优势，避免计算昂贵的Fisher信息矩阵逆。

Result: 在多个图像分类数据集上的实验表明，该方法在分布内性能上达到最优或可比水平，在分布外鲁棒性上有显著提升。

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调，实现了视觉语言任务中更鲁棒和高效的学习。

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [167] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出统一框架联合优化用户关联和资源分配，支持高效并行推测解码，在移动边缘计算中实现低延迟LLM推理


<details>
  <summary>Details</summary>
Motivation: 移动设备上LLM推理需求增长，需要高效的移动边缘计算解决方案。推测解码虽能分区生成token，但存在通信开销和异步延迟问题

Method: 使用多智能体深度强化学习算法解决用户关联和资源分配问题，通过Sionna模拟器在真实条件下评估

Result: 方法实现端到端延迟最高减少28.0%，平均减少23.7%，且不影响推理准确性

Conclusion: 该方法能够在MEC系统中实现可扩展和低延迟的LLM服务

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [168] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 该论文提出了一种通过交换合成数据而非共享模型参数来处理分布式学习中异构模型和数据挑战的新方法，将本地模型视为黑盒，并基于博弈论原理构建学习框架。


<details>
  <summary>Details</summary>
Motivation: 分布式学习面临的主要挑战是处理异构的本地模型和数据，传统参数共享方法难以应对这种异质性。

Method: 使用生成模型交换合成数据而非模型参数，将本地模型视为黑盒，基于博弈论原理构建合作游戏学习框架，支持半监督学习和不同概率空间的本地模型。

Result: 证明了指数族本地模型存在唯一纳什均衡，提出的学习方法能够收敛到该均衡，在图像分类和条件生成的标准基准数据集上展示了优势。

Conclusion: 通过交换合成数据的方法能够有效处理分布式学习中的异构性问题，为处理多模态异构数据提供了可行的解决方案。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [169] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: 提出了HyperNQ，第一个基于超图神经网络的QLDPC解码器，通过利用超边来捕获高阶稳定子约束，在伪阈值区域下比BP和GNN方法显著改善逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 传统QLDPC解码方法如BP在短循环存在时收敛性差，而GNN方法受限于Tanner图的成对交互，无法捕获高阶相关性。

Method: 使用超图神经网络，采用两阶段消息传递方案，通过超边捕获高阶稳定子约束，实现高度表达性和紧凑解码。

Result: 在伪阈值标记以下，HyperNQ比BP改善逻辑错误率高达84%，比GNN策略改善50%，性能优于现有最先进解码器。

Conclusion: HyperNQ通过超图神经网络有效解决了QLDPC解码中的高阶约束捕获问题，显著提升了解码性能。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [170] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出Networked Mixture-of-Experts (NMoE)系统，通过联邦学习框架解决大型AI模型在移动边缘计算中的资源限制问题，实现客户端间的协同推理。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型对计算资源和训练数据的高需求与边缘设备有限的存储和计算能力之间存在冲突，阻碍了在边缘端训练和部署大型模型。

Method: 引入NMoE系统，客户端根据专家能力将任务分发给合适的邻居并聚合结果；提出结合监督学习和自监督学习的联邦学习框架，平衡个性化与泛化能力。

Result: 通过大量实验验证了NMoE系统的有效性，为NMoE训练算法提供了见解和基准。

Conclusion: NMoE系统能够有效解决边缘设备资源限制问题，在保持通信效率和数据隐私的同时实现大型AI模型的边缘部署。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [171] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个用于电池应用异常检测的开源基准测试框架，通过比较15种不同算法，提供系统化的异常检测方法评估，并展示了物理统计特征转换和贝叶斯优化调参在提升检测性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 电池安全在消费电子、电动汽车和飞机等应用中至关重要，未检测到的异常可能引发安全隐患或高昂的停机成本。需要建立统一的基准测试框架来系统比较不同异常检测方法。

Method: 开发了OSBAD开源基准测试框架，包含15种统计、距离和无监督机器学习算法；提出物理统计特征转换工作流将集体异常分解为点异常；使用贝叶斯优化管道进行超参数调优；在液态和固态电池数据集上进行验证。

Result: OSBAD能够系统比较异质数据集上的异常检测方法；特征转换增强了异常可分性；贝叶斯优化解决了无监督学习中标签不完整的问题；验证了跨化学体系的泛化能力。

Conclusion: OSBAD为开发安全、可扩展和可转移的电池分析异常检测工具建立了统一基础，强调了物理统计特征工程和概率超参数调优在推进安全关键能源系统可信数据驱动诊断中的重要性。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [172] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC是一种后训练方法，通过动态标准验证来解决开放生成任务中评估标准多样且验证成本高的问题。该方法使用LLM作为批评家动态识别最可能的失败模式，然后由外部验证器验证，共同优化生成器和批评家。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样且隐含的任务特定评估标准，但标准数量庞大导致验证成本过高且评估不完整，使得基于标准的奖励强化学习难以扩展。此外，如何将这些标准组合成单一奖励也高度依赖于具体提示。

Method: 提出RLAC方法，使用LLM作为批评家动态识别最可能的失败模式（如事实错误或未处理的边缘情况），然后由外部验证器验证，共同优化生成器和批评家。

Result: 实验表明RLAC在文本生成中提高了事实准确性，在代码生成中提高了正确性，同时优于穷举验证和奖励模型方法。动态批评家比固定批评家更有效。

Conclusion: RLAC展示了将RL后训练扩展到自由形式生成任务的潜力，通过动态批评家减少所需验证次数，同时提高生成器输出质量和批评家错误检测能力。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [173] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种新的参数高效微调方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来实现稀疏适配，相比QLoRA在减少灾难性遗忘方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，现有的PEFT方法如LoRA存在秩约束限制，稀疏适配提供了一种替代方案。

Method: 提出RIGSA方法：随机初始化全秩适配器，使用ReZero类似的门控机制，通过迭代幅度剪枝实现稀疏化。在SmolLM2-1.7B-Instruct模型上评估，使用Textual MNIST任务测试学习能力，在PIQA、HellaSwag和GSM8k上测量遗忘程度。

Result: RIGSA能够成功学习Textual MNIST任务，虽然比QLoRA有更多可训练参数，但在GSM8k等任务上表现出更少的灾难性遗忘，性能与随机掩码相当。

Conclusion: RIGSA作为一种稀疏适配方法，在减少灾难性遗忘方面优于QLoRA，为参数高效微调提供了新的有效方案。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [174] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了分数扩散桥模型(FDBM)，这是一种基于分数布朗运动近似的新型生成扩散桥框架，能够捕捉真实随机过程中的记忆效应、长程依赖性和异常扩散现象。


<details>
  <summary>Details</summary>
Motivation: 真实随机过程具有记忆效应、时间相关性、长程依赖性和异常扩散等特征，这些在标准扩散或桥模型中由于使用布朗运动而无法捕捉。

Method: 利用分数布朗运动的马尔可夫近似(MA-fBM)构建FDBM，保持了分数布朗运动的非马尔可夫特性，同时实现了可处理的推理。扩展到Schrödinger桥问题并推导出学习无配对数据转换的原则性损失函数。

Result: 在蛋白质构象预测和无配对图像翻译任务中，FDBM相比布朗运动基线表现更优：蛋白质结构预测中Cα原子位置的均方根偏差更低，无配对图像翻译中Fréchet Inception距离更低。

Conclusion: FDBM框架能够有效建模真实随机过程的复杂特性，在多个任务中优于传统基于布朗运动的模型。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [175] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出了一种基于个性化核心集的加权联邦学习方法，使用代表性数据点而非完整客户端数据进行训练更新，在理论分析和实验中均显示出优于随机采样和其他子模优化方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端在整个数据集上训练过于繁琐的问题，通过核心集选择代表性数据点来降低计算负担。

Method: 个性化核心集加权联邦学习，每个客户端仅基于核心集代表性数据点向中央服务器发送训练更新。

Result: 理论分析显示平均泛化误差达到极小极大最优，实验在多个基准数据集和医疗数据集上相比随机采样和其他子模优化方法有显著提升。

Conclusion: 智能选择训练样本能有效提升联邦学习性能，核心集方法在降低计算复杂度的同时保持良好泛化能力。

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [176] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经场模型，使用多尺度傅里叶特征编码从稀疏噪声超声数据中估计血流，无需地面真值监督，在合成和真实数据集上实现了低均方误差的去噪和修复效果。


<details>
  <summary>Details</summary>
Motivation: 血流分析对疾病诊断很重要，但超声成像受深度衰减影响，质量有限。传统EchoPIV技术在测量血流速度方面存在局限性，物理信息机器学习可以增强准确性和鲁棒性。

Method: 使用物理信息神经场模型结合多尺度傅里叶特征编码，从稀疏噪声超声数据中重建血流场，无需地面真值监督。

Result: 模型在合成和真实数据集上实现了持续低均方误差的去噪和修复效果，与参考流场和真实流量测量结果验证一致。

Conclusion: 将其他成像模态中证明有效的方法适应于超声血流重建，物理信息神经场模型能够有效处理超声数据的稀疏性和噪声问题。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [177] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 提出了一种基于度量学习的无秩张量分解框架，用判别性的相似性优化替代传统重构目标，在语义相似性度量上优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法基于重构和固定秩约束，难以捕捉高维数据中的语义结构，需要一种能直接反映语义相似性的新方法。

Method: 通过优化带有多样性和均匀性正则化的三元组损失，学习数据驱动的嵌入表示，构建距离直接反映语义相似性的特征空间。

Result: 在多个领域（人脸识别、脑连接分析、模拟数据）的评估中，该方法在聚类指标上显著优于PCA、t-SNE、UMAP和传统张量分解方法，且在数据稀缺场景下表现优异。

Conclusion: 度量学习为张量分析提供了新范式，优先考虑语义相关性而非像素级保真度，在数据稀缺场景下具有计算优势。

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [178] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 该论文提出了一种基于域对抗ConvNeXt自编码器(A-CNT)的方法，用于在室内环境变化下实现鲁棒的恶意干扰源定位，解决了UWB定位系统在房间布局改变时的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: UWB定位系统虽然能提供厘米级精度，但易受干扰攻击，且在室内环境布局变化时定位性能会严重下降。现有方法在跨房间布局的干扰源定位方面研究不足。

Method: 引入两个UWB数据集，提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐不同域间的CIR特征，实现特征层面的域适应。

Result: 在原始数据集上，Random Forest达到最高F1-macro分数0.95，XGBoost达到最低平均欧几里得误差20.16cm。但在修改布局后，XGBoost误差增加十倍至207.99cm。A-CNT方法将误差降至34.67cm，比非对抗迁移学习提升77%，比最佳基线提升83%。

Conclusion: 域对抗特征对齐方法能够在环境变化情况下实现鲁棒且可迁移的室内干扰源定位，有效解决了跨域性能退化问题。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [179] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 本文研究了科学机器学习中数据保真度与计算成本之间的权衡关系，通过重新制定经典缩放定律，将数据集轴分解为计算预算和数据集组成，揭示了计算-性能缩放行为。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习通常受限于通过数值模拟生成训练数据的高昂成本，但通过调整建模假设和近似，可以以计算成本换取模拟保真度，这是其他领域所不具备的特性。

Method: 使用低保真度和高保真度的雷诺平均纳维-斯托克斯（RANS）模拟来研究神经代理模型中数据保真度与成本之间的权衡，重新制定经典缩放定律，将数据集轴分解为计算预算和数据集组成。

Result: 实验揭示了计算-性能缩放行为，并展示了在给定数据集配置下，预算依赖的最优保真度混合。

Conclusion: 这些发现为多保真度神经代理数据集提供了首个经验缩放定律研究，并为科学机器学习中计算高效的数据集生成提供了实际考虑。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [180] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法解决视觉语言模型在连续微调中的灾难性遗忘问题，能够在保持基础能力的同时整合新任务，无需同时访问所有任务数据。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在连续微调新任务时会出现灾难性遗忘，而多任务学习需要同时访问所有数据集且计算开销随任务数量线性增长。

Method: 采用基于路由的方法，在InternVL-2模型（2B和8B参数）上实现新任务集成，同时保持预训练获得的基础知识。

Result: 路由方法在ChartQA、MMBench和DocVQA等通用基准上保持性能，同时在专业任务上提高准确性，且无需所有任务数据并发访问。

Conclusion: 基于路由的学习具有可扩展性和鲁棒性，特别适用于语义相关的新任务，并能实现跨模态知识迁移，优于现有持续学习方法。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [181] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 该论文提出时间特征分析(Temporal Feature Analysis)作为新的可解释性方法，通过考虑语言的时间动态特性来改进稀疏自编码器(SAEs)的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念是独立方向，但这无法捕捉语言的丰富时间结构。稀疏自编码器强加了跨时间概念独立性的先验，与语言模型表示中存在的丰富时间动态相冲突。

Method: 引入时间特征分析，将给定时间的表示分解为两部分：可预测部分（可从上下文推断）和残差部分（捕获上下文无法解释的新信息）。该方法具有时间归纳偏置。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界，并更广泛地区分抽象的慢速信息和新颖的快速信息，而现有SAEs在上述任务中都显示出显著缺陷。

Conclusion: 研究结果强调了在设计稳健的可解释性工具时需要与数据匹配的归纳偏置，时间特征分析为理解语言模型的时间动态提供了更有效的方法。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [182] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 该研究结合可解释机器学习和符号建模，揭示美国红河流域10个水库的水温动态驱动因素，使用Kolmogorov Arnold Networks (KANs)将数据驱动洞察转化为紧凑的解析表达式，实现高预测精度(R²=0.97)与可解释性的平衡。


<details>
  <summary>Details</summary>
Motivation: 准确预测水库水温对可持续水资源管理、生态系统健康和气候韧性至关重要，但仅靠预测无法深入了解控制物理过程，需要将黑盒模型转化为透明替代模型。

Method: 使用集成和神经网络模型(RF、XGBoost、MLP)预测水温，结合SHAP量化物理驱动因素贡献，开发KANs符号近似方法，从简单到复杂逐步推导10个方程。

Result: 获得高预测精度(最佳RMSE=1.20°C，R²=0.97)，通过SHAP揭示空气温度、深度、风速和湖容量的贡献模式，KAN方程从单预测因子(R²=0.84)逐步提升到10个预测因子(R²=0.92)，但超过5个预测因子后增益递减。

Conclusion: 该框架通过耦合预测精度与解释能力，展示了KANs和可解释机器学习如何将黑盒模型转化为透明替代模型，促进水库热动力学的预测和理解，深度是关键次要预测因子，降水影响有限。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [183] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 提出了一种终身表示学习框架，使用多任务经验风险最小化作为子程序，并基于新引入的任务逃避者维度建立了样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 终身学习中，学习者面临一系列具有共享结构的任务，需要在线地利用现有知识并持续收集部分信息来加速学习，这与多任务学习或元学习中任务预先可用的设置不同。

Method: 提出了一个简单的算法，使用多任务经验风险最小化作为子程序，并引入了任务逃避者维度来建立样本复杂度分析。

Result: 建立了一个适用于广泛学习问题的样本复杂度界限，并在分类和回归任务等具体实例中验证了结果。

Conclusion: 该框架为终身表示学习提供了一个通用的理论分析工具，能够处理包含通用函数类的各种学习问题。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [184] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 提出一种坐标上升算法，通过最大似然估计以监督方式学习动态状态估计中的动态和测量模型。模型假设为高斯分布，算法学习神经网络参数来建模动态和测量函数以及噪声协方差矩阵。训练后的模型与非线性卡尔曼滤波器结合用于测试阶段的状态估计。


<details>
  <summary>Details</summary>
Motivation: 在动态状态估计中，准确建模动态和测量模型对性能至关重要。传统方法通常假设这些模型已知或使用简单参数化，限制了在复杂系统中的适用性。本文旨在通过数据驱动方法学习这些模型，提高状态估计的准确性。

Method: 采用坐标上升算法进行最大似然估计，假设动态和测量模型为高斯分布。使用神经网络参数化动态和测量函数，同时学习噪声协方差矩阵。训练完成后，将学习到的模型与非线性卡尔曼滤波器集成进行状态估计。

Result: 算法能够有效学习动态和测量模型的神经网络参数以及噪声协方差。训练后的模型与卡尔曼滤波器结合，在测试阶段能够准确估计系统状态。

Conclusion: 提出的坐标上升算法为动态状态估计提供了一种有效的数据驱动建模方法，通过联合学习动态模型、测量模型和噪声特性，提升了状态估计的性能和适应性。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [185] [Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver](https://arxiv.org/abs/2511.01001)
*Johansell Villalobos,Daniel Caviedes-Voullième,Silvio Rizzi,Esteban Meneses*

Main category: cs.DC

TL;DR: 本文对SERGHEI-SWE浅水方程求解器在四种异构HPC系统上的性能进行了全面研究，展示了良好的可扩展性和性能可移植性，同时识别了内存带宽是主要性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 气候变化对数值建模提出了重大挑战，水文建模对高分辨率实时模拟的需求日益增长，需要采用GPU加速平台和性能可移植编程框架。

Method: 在四种异构HPC系统（Frontier、JUWELS Booster、JEDI、Aurora）上评估SERGHEI-SWE求解器，进行强扩展（1024 GPU）和弱扩展（2048 GPU）测试，使用屋顶线分析和性能可移植性指标。

Result: 展示了良好的可扩展性，速度提升32倍，效率超过90%；屋顶线分析显示内存带宽是主要瓶颈；性能可移植性在调整问题大小时可达70%。

Conclusion: SERGHEI-SWE是一个稳健、可扩展且可移植的模拟工具，但仍有通过Kokkos团队和架构特定参数进行内核优化的空间。

Abstract: Current climate change has posed a grand challenge in the field of numerical
modeling due to its complex, multiscale dynamics. In hydrological modeling, the
increasing demand for high-resolution, real-time simulations has led to the
adoption of GPU-accelerated platforms and performance portable programming
frameworks such as Kokkos. In this work, we present a comprehensive performance
study of the SERGHEI-SWE solver, a shallow water equations code, across four
state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS
Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We
assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,
demonstrating consistent scalability with a speedup of 32 and an efficiency
upwards of 90\% for most almost all the test range. Roofline analysis reveals
that memory bandwidth is the dominant performance bottleneck, with key solver
kernels residing in the memory-bound region. To evaluate performance
portability, we apply both harmonic and arithmetic mean-based metrics while
varying problem size. Results indicate that while SERGHEI-SWE achieves
portability across devices with tuned problem sizes (<70\%), there is room for
kernel optimization within the solver with more granular control of the
architecture specifically by using Kokkos teams and architecture specific
tunable parameters. These findings position SERGHEI-SWE as a robust, scalable,
and portable simulation tool for large-scale geophysical applications under
evolving HPC architectures with potential to enhance its performance.

</details>


### [186] [AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios](https://arxiv.org/abs/2511.00038)
*Suman Raj,Radhika Mittal,Rajiv Mayani,Pawel Zuk,Anirban Mandal,Michael Zink,Yogesh Simmhan,Ewa Deelman*

Main category: cs.DC

TL;DR: AeroResQ是一个边缘加速的无人机框架，用于野火场景中可扩展、弹性和协作的逃生路线规划，采用多层编队架构，实现低延迟和高任务完成率。


<details>
  <summary>Details</summary>
Motivation: 在野火响应中，配备机载摄像头和DNN模型的无人机机队对于实时时空决策至关重要，需要监控火灾动态、支持消防员协调和促进安全疏散。

Method: 采用服务无人机(SDs)和协调无人机(CDs)的多层编队架构，SDs使用边缘加速器运行火灾检测和人体姿态识别DNN模型，CDs使用Apache IoTDB动态生成最优地面逃生路线，并基于加权A*搜索算法进行协作路径规划。

Result: 在模拟南加州野火的实验中，AeroResQ实现了≤500ms的端到端延迟，远低于2秒的请求间隔，同时保持超过98%的任务重新分配和完成成功率。

Conclusion: AeroResQ展示了在紧急响应和消防员安全操作中实时现场部署的可行性，具有低延迟和高可靠性的特点。

Abstract: Drone fleets equipped with onboard cameras, computer vision, and Deep Neural
Network (DNN) models present a powerful paradigm for real-time spatio-temporal
decision-making. In wildfire response, such drones play a pivotal role in
monitoring fire dynamics, supporting firefighter coordination, and facilitating
safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV
framework designed for scalable, resilient, and collaborative escape route
planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration
architecture comprising service drones (SDs) and coordinator drones (CDs), each
performing specialized roles. SDs survey fire-affected areas, detect stranded
individuals using onboard edge accelerators running fire detection and human
pose identification DNN models, and issue requests for assistance. CDs,
equipped with lightweight data stores such as Apache IoTDB, dynamically
generate optimal ground escape routes and monitor firefighter movements along
these routes. The framework proposes a collaborative path-planning approach
based on a weighted A* search algorithm, where CDs compute context-aware escape
paths. AeroResQ further incorporates intelligent load-balancing and resilience
mechanisms: CD failures trigger automated data redistribution across IoTDB
replicas, while SD failures initiate geo-fenced re-partitioning and
reassignment of spatial workloads to operational SDs. We evaluate AeroResQ
using realistic wildfire emulated setup modeled on recent Southern California
wildfires. Experimental results demonstrate that AeroResQ achieves a nominal
end-to-end latency of <=500ms, much below the 2s request interval, while
maintaining over 98% successful task reassignment and completion, underscoring
its feasibility for real-time, on-field deployment in emergency response and
firefighter safety operations.

</details>


### [187] [COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement](https://arxiv.org/abs/2511.00263)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: OciorACOOL是COOL协议的异步自适应变体，在异步设置下实现无错误、信息论安全的拜占庭共识，具有O(max{nℓ, nt log q})通信复杂度、O(1)轮次和最优弹性n≥3t+1。


<details>
  <summary>Details</summary>
Motivation: 将COOL协议的同步拜占庭共识扩展到异步环境，同时保持其低通信复杂度和信息论安全性。

Method: 采用自适应变体设计，保留COOL的(n,k)纠错编码和解码(k=t/3)，使用单个异步二进制BA调用。

Result: 在异步设置下实现了与COOL相当的通信复杂度O(max{nℓ, nt log q})，O(1)轮次，仍保持最优弹性n≥3t+1。

Conclusion: OciorACOOL成功将COOL协议扩展到异步环境，保持了原有的低复杂度和安全性特性。

Abstract: COOL (Chen'21) is an error-free, information-theoretically secure Byzantine
agreement (BA) protocol proven to achieve BA consensus in the synchronous
setting for an $\ell$-bit message, with a total communication complexity of
$O(\max\{n\ell, nt \log q\})$ bits, four communication rounds in the worst
case, and a single invocation of a binary BA, under the optimal resilience
assumption $n \geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may
behave dishonestly. Here, $q$ denotes the alphabet size of the error correction
code used in the protocol.
  In this work, we present an adaptive variant of COOL, called OciorACOOL,
which achieves error-free, information-theoretically secure BA consensus in the
asynchronous setting with total $O(\max\{n\ell, n t \log q\})$ communication
bits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA
protocol, still under the optimal resilience assumption $n \geq 3t + 1$.
Moreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$
error-correction encoding and decoding as COOL, with $k=t/3$.

</details>


### [188] [Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.00294)
*Lucas Almeida,Maycon Peixoto*

Main category: cs.DC

TL;DR: 提出了Tetris应用放置策略，在边缘-云连续体中通过启发式算法分配计算服务，减少SLA违规约76%


<details>
  <summary>Details</summary>
Motivation: 边缘-云连续体需要高效的模块应用放置策略来平衡用户需求和基础设施约束，实现资源高效利用

Method: 使用启发式算法，基于SLA紧急程度和资源效率优先级来分配计算服务，避免系统过载

Result: 相比基准方法，Tetris减少了约76%的SLA违规

Conclusion: Tetris为边缘-云连续体环境中的延迟敏感应用提供了有效的放置方法，提升了服务质量

Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a
flexible and scalable infrastructure. This paradigm can minimize latency by
processing data closer to the source at the edge while leveraging the vast
computational power of the cloud for more intensive tasks. In this context,
module application placement requires strategic allocation plans that align
user demands with infrastructure constraints, aiming for efficient resource
use. Therefore, we propose Tetris, an application placement strategy that
utilizes a heuristic algorithm to distribute computational services across edge
and cloud resources efficiently. Tetris prioritizes services based on SLA
urgencies and resource efficiency to avoid system overloading. Our results
demonstrate that Tetris reduces SLA violations by approximately 76% compared to
the baseline method, which serves as a reference point for benchmarking
performance in this scenario. Therefore, Tetris offers an effective placement
approach for managing latency-sensitive applications in Edge-Cloud Continuum
environments, enhancing Quality of Service (QoS) for users.

</details>


### [189] [EPARA: Parallelizing Categorized AI Inference in Edge Clouds](https://arxiv.org/abs/2511.00603)
*Yubo Wang,Yubo Cui,Tuo Shi,Danyang Li,Wenxin Li,Lide Suo,Tao Wang,Xin Xie*

Main category: cs.DC

TL;DR: EPARA是一个端到端的边缘AI并行推理框架，通过基于任务对延迟/频率敏感度和GPU资源需求的分类，实现请求级和服务级的任务资源分配，提升边缘AI服务能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和计算机视觉AI等应用的广泛采用，AI推理系统的计算需求持续增长，需要在现有硬件基础上提升边缘云的任务处理能力。

Method: EPARA包含三个核心组件：1)任务分类并行分配器决定每个任务的并行模式；2)分布式请求处理器执行具体请求计算；3)状态感知调度器定期更新边缘云中的服务放置。

Result: 在包含边缘服务器、嵌入式设备和微计算机的测试平台实验中，EPARA在生产工作负载中实现了比现有框架高达2.1倍的良好吞吐量，并能适应各种边缘AI推理任务。

Conclusion: EPARA通过任务分类和资源分配策略，有效提升了边缘AI推理系统的处理能力和适应性。

Abstract: With the increasing adoption of AI applications such as large language models
and computer vision AI, the computational demands on AI inference systems are
continuously rising, making the enhancement of task processing capacity using
existing hardware a primary objective in edge clouds. We propose EPARA, an
end-to-end AI parallel inference framework in edge, aimed at enhancing the edge
AI serving capability. Our key idea is to categorize tasks based on their
sensitivity to latency/frequency and requirement for GPU resources, thereby
achieving both request-level and service-level task-resource allocation. EPARA
consists of three core components: 1) a task-categorized parallelism allocator
that decides the parallel mode of each task, 2) a distributed request handler
that performs the calculation for the specific request, and 3) a state-aware
scheduler that periodically updates service placement in edge clouds. We
implement a EPARA prototype and conduct a case study on the EPARA operation for
LLMs and segmentation tasks. Evaluation through testbed experiments involving
edge servers, embedded devices, and microcomputers shows that EPARA achieves up
to 2.1$\times$ higher goodput in production workloads compared to prior
frameworks, while adapting to various edge AI inference tasks.

</details>


### [190] [AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs](https://arxiv.org/abs/2511.00796)
*Ran Yan,Youhe Jiang,Tianyuan Wu,Jiaxuan Gao,Zhiyu Mei,Wei Fu,Haohui Mai,Wei Wang,Yi Wu,Binhang Yuan*

Main category: cs.DC

TL;DR: AReaL-Hex是一个面向异构GPU的异步强化学习训练系统，通过两阶段调度器在保持数据新鲜度的同时最大化训练吞吐量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 为了降低LLM强化学习的训练成本并使其技术更普及，需要充分利用异构GPU资源。RL训练的三个阶段（生成、奖励计算、策略更新）具有不同的计算特性，适合在异构硬件上部署。

Method: 使用两阶段调度器：1）通过混合整数线性规划选择各阶段的并行化策略和工作负载分配；2）通过图分区分配异构GPU和互连以最大化端到端吞吐量。系统将内存受限的生成阶段和计算受限的优化阶段映射到更经济高效的资源上。

Result: 在数学推理任务上，相比同构部署的最先进异步RL系统：相同预算下训练吞吐量提升1.50倍；相同吞吐量下训练成本降低1.46倍。

Conclusion: AReaL-Hex证明了在异构GPU上部署RL训练的有效性，能够显著提升训练效率和成本效益。

Abstract: Maximizing training throughput and cost-efficiency of RL for LLMs is
essential to democratize this advanced technique. One promising but challenging
approach is to deploy such a computational workflow over heterogeneous GPUs.
Unlike conventional large-scale LLM pretraining, RL training generally
decomposes into three coupled stages, i.e., rollout generation, reward
computation, and policy/value updates, which exhibit markedly different compute
intensities, memory footprints, and communication patterns. Recent research
shows that fully asynchronous RL training can disaggregate these stages across
disjoint hardware pools without sacrificing training stability, creating a
great opportunity for real-world heterogeneous deployment. To this end, we
present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that
effectively schedules how to execute rollout generation and policy model
training over heterogeneous GPUs while enforcing data staleness bounds.
Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to
select per-stage parallelization strategies and workload assignments given a
resource budget, and (ii) a graph-partitioning step that allocates
heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built
atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound
generation and compute-bound optimization to more cost-efficient resources and
balances their producer-consumer interactions to avoid both idleness and stale
rollout trajectories. On the mathematical reasoning task with various model
scales (1.5B, 7B, and 14B), compared to homogeneous deployments of
state-of-the-art asynchronous RL systems: (i) When maintaining the same total
budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When
achieving the same training throughput, AReaL-Hex results in up to 1.46x
reduction in training cost.

</details>


### [191] [FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs](https://arxiv.org/abs/2511.00807)
*Xuan He,Zequan Fang,Jinzhao Lian,Danny H. K. Tsang,Baosen Zhang,Yize Chen*

Main category: cs.DC

TL;DR: FREESH是一个用于优化LLM服务系统的框架，通过联合路由和调度策略，在异构GPU集群中最小化碳排放或能耗目标。


<details>
  <summary>Details</summary>
Motivation: LLM和AI代理不断增长的计算和能源需求，以及异构GPU集群在地理分布部署时，LLM负载在查询流量和服务模式上的多样性，导致不同时间和地点运行相同查询会产生显著不同的碳足迹。

Method: 通过观察LLM服务需求并利用时空计算灵活性，提出联合路由和调度问题解决方案。FREESH识别平衡负载服务的最优配置，匹配不同GPU实例的功率-吞吐量特性与可预测的LLM查询长度和工作负载。结合动态GPU频率调节和LLF服务策略确保延迟和公平性要求。

Result: 在生产工作负载上进行1小时服务测试，FREESH减少了28.6%的能耗和45.45%的碳排放，同时提高了SLO达成率和公平性。

Conclusion: FREESH通过协同运行多个数据中心，在满足服务质量要求的同时，显著降低了LLM服务的能源消耗和碳足迹。

Abstract: The ever-increasing computation and energy demand for LLM and AI agents call
for holistic and efficient optimization of LLM serving systems. In practice,
heterogeneous GPU clusters can be deployed in a geographically distributed
manner, while LLM load also observes diversity in terms of both query traffic
and serving patterns. LLM queries running on advanced GPUs during a
high-emission hour at one location can lead to significantly higher carbon
footprints versus same queries running on mid-level GPUs at a low-emission time
and location. By observing LLM serving requirements and leveraging
spatiotemporal computation flexibility, we consider the joint routing and
scheduling problem, and propose FREESH to cooperatively run a group of data
centers while minimizing user-specified carbon or energy objectives. FREESH
identifies the optimal configurations of balanced load serving by matching
distinct GPU instance's power-throughput characteristics with predictable LLM
query length and workloads. To ensure both latency and fairness requirements,
FREESH identifies optimized parallelism and query routing schedules together
with dynamic GPU frequency scaling for power saving, and Least-Laxity-First
(LLF) serving strategy for query scheduling. During the 1-hour serving on
production workloads, FREESH reduces energy by 28.6% and emissions by 45.45%
together with improvements in SLO attainment and fairness.

</details>


### [192] [Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks](https://arxiv.org/abs/2511.01127)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 提出基于脉冲神经网络的新型任务卸载框架，在边缘计算中实现高效、节能的实时任务编排


<details>
  <summary>Details</summary>
Motivation: 传统任务卸载策略依赖静态启发式或数据密集型机器学习模型，不适合高度动态和资源受限的边缘计算环境

Method: 将SNN决策模块集成到边缘节点中，使用YAFS和Brian2混合仿真环境评估各种IoT工作负载场景

Result: 相比传统启发式和基于ML的策略，在高负载条件下实现了26%延迟降低、32%能耗减少和25%成功率提升

Conclusion: SNN框架显著降低了任务处理延迟和能耗，同时提高了任务成功率，证明了生物启发方法在边缘计算中的有效性

Abstract: Traditional task offloading strategies in edge computing often rely on static
heuristics or data-intensive machine learning models, which are not always
suitable for highly dynamic and resource-constrained environments. In this
paper, we propose a novel task-offloading framework based on Spiking Neural
Networks inspired by the efficiency and adaptability of biological neural
systems. Our approach integrates an SNN-based decision module into edge nodes
to perform real-time, energy-efficient task orchestration. We evaluate the
model under various IoT workload scenarios using a hybrid simulation
environment composed of YAFS and Brian2. The results demonstrate that our
SNN-based framework significantly reduces task processing latency and energy
consumption while improving task success rates. Compared to traditional
heuristic and ML-based strategies, our model achieves up to 26% lower latency,
32% less energy consumption, and 25\% higher success rate under high-load
conditions.

</details>


### [193] [Scalable Maxflow Processing for Dynamic Graphs](https://arxiv.org/abs/2511.01235)
*Shruthi Kannappan,Ashwina Kumar,Rupesh Nasre*

Main category: cs.DC

TL;DR: 提出了一种新颖的GPU并行最大流算法，能够动态更新批量边修改后的最大流，并设计了高性能的静态GPU算法用于初始最大流计算。


<details>
  <summary>Details</summary>
Motivation: 最大流问题是图论和组合优化的基础问题，具有广泛应用。Push-Relabel算法因其效率和并行性适合GPU加速，但现有研究缺乏对动态图的有效支持。

Method: 开发了GPU并行最大流算法，支持动态图的增量重计算；设计了高性能静态GPU算法；实现了一系列CUDA特定的性能优化。

Result: 算法在GPU平台上实现了高性能、可扩展性和内存效率，能够高效处理动态图的最大流更新。

Conclusion: 提出的GPU并行最大流算法有效解决了动态图最大流计算问题，为相关应用提供了高效的解决方案。

Abstract: The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and
combinatorial optimization, aiming to determine the largest possible flow from
a designated source node to a sink node within a capacitated flow network. It
has extensive applications across diverse domains such as computer networking,
transportation systems, and image segmentation. The objective is to maximize
the total throughput while respecting edge capacity constraints and maintaining
flow conservation at all intermediate vertices.
  Among the various algorithms proposed for solving the Max-Flow problem, the
Push--Relabel algorithm is particularly notable for its efficiency and
suitability for parallelization, owing to its localized vertex-based
operations. This property has motivated extensive research into GPU-accelerated
Max-Flow computation, leveraging the high degree of parallelism inherent to
modern GPU architectures.
  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of
incrementally recomputing the maximum flow of a dynamic graph following a batch
of edge updates. In addition, we introduce a high-performance static GPU
algorithm designed for efficiently computing the initial Max-Flow on static
graphs. We further describe a series of CUDA-specific implementation
optimizations that enhance performance, scalability, and memory efficiency on
GPU platforms.

</details>


### [194] [Design of quasi phase matching crystal based on differential gray wolf algorithm](https://arxiv.org/abs/2511.01255)
*He Chen,ZiHua Zheng,JingHua Sun*

Main category: cs.DC

TL;DR: 本文提出了一种融合混合优化算法和GPU并行加速技术的方案，用于解决非周期极化晶体性能优化这一高维离散组合NP难题。


<details>
  <summary>Details</summary>
Motivation: 传统算法在非周期极化晶体优化中存在收敛慢、易陷入局部最优的问题，而启发式方法受限于CPU串行计算效率低下。

Method: 采用差分进化算法(DE)进行全局搜索，灰狼优化算法(GWO)加强局部搜索和收敛速度，两者协同平衡全局和局部优化需求，并利用GPU多核架构实现线程级并行计算。

Result: 该方案有效突破了高维离散空间优化问题，提高了晶域控制精度，相比传统CPU串行计算，准相位匹配设计效率提升了数百至数千倍。

Conclusion: 为复杂非线性光学器件设计提供了新范式，有助于推动量子光学和激光加工等领域相关器件的性能突破和工业应用。

Abstract: This paper focuses on the key problem in the development of nonlinear optical
technology, the performance optimization of aperiodically polarized crystals.
The performance of the crystal depends on the precise control of the micro
distribution of crystal domains, but its optimization belongs to the
high-dimensional discrete combination "NP hard" problem. The traditional
algorithm has the bottleneck of slow convergence and easy to fall into local
optimization, while the heuristic methods such as genetic algorithm are limited
by the CPU serial calculation and inefficient. In order to solve the above
challenges, this paper proposes the fusion scheme of hwsda hybrid optimization
algorithm and GPU parallel acceleration technology: the differential evolution
algorithm (DE) is used to realize the global search, and the gray wolf
optimization algorithm (GWO) is used to strengthen the local search and
convergence speed, and the two coordinate to balance the global and local
optimization requirements; At the same time, it relies on GPU multi-core
architecture to realize thread level parallel computing and improve
optimization efficiency. This scheme effectively breaks through the
optimization problem of high-dimensional discrete space, improves the accuracy
of crystal domain control, improves the efficiency of quasi phase matching
design by hundreds to thousands of times compared with traditional CPU serial
computing, provides a new paradigm for the design of complex nonlinear optical
devices, and helps promote the performance breakthrough and industrial
application of related devices in the fields of quantum optics and laser
processing.

</details>


### [195] [Transformer-Based Sparse CSI Estimation for Non-Stationary Channels](https://arxiv.org/abs/2511.01333)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Hassan Rizwan,Sagnik Bhattacharya,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出基于Flash-Attention Transformer的CSI估计框架，在非平稳信道条件下实现高效信道状态信息估计，相比传统方法显著降低导频开销并提升性能


<details>
  <summary>Details</summary>
Motivation: 解决下一代无线系统中非平稳条件下的CSI估计问题，传统导频辅助估计方法开销大，深度学习方法在动态导频模式和时变衰落条件下性能下降

Method: 结合模型驱动导频获取与数据驱动CSI重建的框架，使用补丁级自注意力机制和物理感知复合损失函数，强制相位对齐、相关一致性和时频平滑性

Result: 在3GPP NR配置下，相比LMMSE和LSTM基线，相位不变归一化均方误差提升约13dB，误码率显著降低，同时导频开销减少16倍

Conclusion: 基于注意力的架构能够实现可靠的CSI恢复和增强的频谱效率，不损害链路质量，解决了自适应、低开销信道估计的基本瓶颈

Abstract: Accurate and efficient estimation of Channel State Information (CSI) is
critical for next-generation wireless systems operating under non-stationary
conditions, where user mobility, Doppler spread, and multipath dynamics rapidly
alter channel statistics. Conventional pilot aided estimators incur substantial
overhead, while deep learning approaches degrade under dynamic pilot patterns
and time varying fading. This paper presents a pilot-aided Flash-Attention
Transformer framework that unifies model-driven pilot acquisition with data
driven CSI reconstruction through patch-wise self-attention and a physics aware
composite loss function enforcing phase alignment, correlation consistency, and
time frequency smoothness. Under a standardized 3GPP NR configuration, the
proposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB
in phase invariant normalized mean-square error (NMSE) with markedly lower
bit-error rate (BER), while reducing pilot overhead by 16 times. These results
demonstrate that attention based architectures enable reliable CSI recovery and
enhanced spectral efficiency without compromising link quality, addressing a
fundamental bottleneck in adaptive, low-overhead channel estimation for
non-stationary 5G and beyond-5G networks.

</details>


### [196] [Gradient Clock Synchronization with Practically Constant Local Skew](https://arxiv.org/abs/2511.01420)
*Christoph Lenzen*

Main category: cs.DC

TL;DR: 本文改进了梯度时钟同步(GCS)模型，通过考虑测量误差和频率误差的稳定性而非最坏情况，突破了现有的Ω(ΔlogD)下界，实现了O(Δ+δlogD)的局部偏差界限，其中δ≪Δ。


<details>
  <summary>Details</summary>
Motivation: 现有GCS方法存在两个主要问题：1) 局部偏差界限依赖于整个系统生命周期内必须保证的偏移估计上界；2) 假设本地振荡器存在最坏情况频率偏差，但实际上频率在短期内更加稳定。实际部署的同步方法虽然能适应真实误差，但无法提供非平凡的局部偏差保证。

Method: 提出了精炼的模型和对现有GCS技术的新分析，仅要求测量和频率误差的稳定性，从而规避现有下界。通过考虑相关时间尺度上的估计误差变化δ而非最坏情况Δ，以及单个振荡器频率变化而非所有振荡器的最坏情况边界。

Result: 在链路具有统一最坏情况估计误差Δ和相关时间尺度上估计误差变化δ≪Δ的情况下，实现了网络直径D的局部偏差界限O(Δ+δlogD)，显著优于现有的Ω(ΔlogD)下界。同时确保了自稳定化，并将结果扩展到外部同步场景。

Conclusion: 通过考虑误差稳定性而非最坏情况假设，本文在非常通用的条件下实现了GCS性能的显著提升，突破了现有理论下界，为实际时钟同步系统提供了更实用的保证。

Abstract: Gradient Clock Synchronization (GCS) is the task of minimizing the local
skew, i.e., the clock offset between neighboring clocks, in a larger network.
While asymptotically optimal bounds are known, from a practical perspective
they have crucial shortcomings:
  - Local skew bounds are determined by upper bounds on offset estimation that
need to be guaranteed throughout the entire lifetime of the system.
  - Worst-case frequency deviations of local oscillators from their nominal
rate are assumed, yet frequencies tend to be much more stable in the (relevant)
short term.
  State-of-the-art deployed synchronization methods adapt to the true offset
measurement and frequency errors, but achieve no non-trivial guarantees on the
local skew.
  In this work, we provide a refined model and novel analysis of existing
techniques for solving GCS in this model. By requiring only stability of
measurement and frequency errors, we can circumvent existing lower bounds,
leading to dramatic improvements under very general conditions. For example, if
links exhibit a uniform worst-case estimation error of $\Delta$ and a change in
estimation errors of $\delta\ll \Delta$ on relevant time scales, we bound the
local skew by $O(\Delta+\delta \log D)$ for networks of diameter $D$,
effectively ``breaking'' the established $\Omega(\Delta\log D)$ lower bound,
which holds when $\delta=\Delta$. Similarly, we show how to limit the influence
of local oscillators on $\delta$ to scale with the change of frequency of an
individual oscillator on relevant time scales, rather than a worst-case bound
over all oscillators and the lifetime of the system.
  Moreover, we show how to ensure self-stabilization in this challenging
setting. Last, but not least, we extend all of our results to the scenario of
external synchronization, at the cost of a limited increase in stabilization
time.

</details>


### [197] [Adaptive Multidimensional Quadrature on Multi-GPU Systems](https://arxiv.org/abs/2511.01573)
*Melanie Tonarelli,Simone Riva,Pietro Benedusi,Fabrizio Ferrandi,Rolf Krause*

Main category: cs.DC

TL;DR: 提出了一种分布式自适应求积方法，将多维积分作为多GPU架构上的分层域分解问题来解决，通过循环轮询策略实现负载均衡。


<details>
  <summary>Details</summary>
Motivation: 解决多维积分计算中GPU负载不平衡的问题，提高高维情况下的计算效率和鲁棒性。

Method: 采用分层域分解方法，将积分域递归划分为子域，使用局部误差估计器指导细化，通过非阻塞的CUDA感知MPI通信实现动态负载重分配。

Result: 相比最先进的GPU定制包，在高维情况下具有更高效率，对积分函数正则性和目标精度具有更好的鲁棒性。

Conclusion: 该方法在多GPU架构上有效解决了自适应积分中的负载均衡问题，显著提升了高维积分计算的性能。

Abstract: We introduce a distributed adaptive quadrature method that formulates
multidimensional integration as a hierarchical domain decomposition problem on
multi-GPU architectures. The integration domain is recursively partitioned into
subdomains whose refinement is guided by local error estimators. Each subdomain
evolves independently on a GPU, which exposes a significant load imbalance as
the adaptive process progresses. To address this challenge, we introduce a
decentralised load redistribution schemes based on a cyclic round-robin policy.
This strategy dynamically rebalance subdomains across devices through
non-blocking, CUDA-aware MPI communication that overlaps with computation. The
proposed strategy has two main advantages compared to a state-of-the-art
GPU-tailored package: higher efficiency in high dimensions; and improved
robustness w.r.t the integrand regularity and the target accuracy.

</details>


### [198] [LARK - Linearizability Algorithms for Replicated Keys in Aerospike](https://arxiv.org/abs/2511.01843)
*Andrew Goodng,Kevin Porter,Thomas Lopatic,Ashish Shinde,Sunil Sayyaparaju,Srinivasan Seshadri,V. Srinivasan*

Main category: cs.DC

TL;DR: LARK是一个同步复制协议，通过消除有序日志和引入分区可用性条件，在保证线性一致性的同时显著提高系统可用性，比传统基于日志的共识协议有3-10倍的可用性提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于日志的共识协议如Raft、Paxos等在故障恢复时需要重建副本和重放日志，导致系统暂停提交操作，可用性较低。LARK旨在设计一个既能保证线性一致性，又能最大化系统可用性的复制协议。

Method: LARK引入分区可用性条件(PAC)，基于整个数据库集群而非固定副本集进行决策；消除有序日志，使分区在领导者变更后立即可用；采用f+1数据副本策略，在数据节点故障时仍能继续提交操作。

Result: LARK在容忍一个故障时可用性提升约3倍，容忍两个故障时提升约10倍；支持零停机滚动重启；在同等存储预算下比基于日志的协议具有更高的可用性。

Conclusion: LARK协议通过创新的设计在保证安全性的同时显著提高了分布式系统的可用性，为高可用数据库系统提供了新的解决方案。

Abstract: We present LARK (Linearizability Algorithms for Replicated Keys), a
synchronous replication protocol that achieves linearizability while minimizing
latency and infrastructure cost, at significantly higher availability than
traditional quorum-log consensus. LARK introduces Partition Availability
Conditions (PAC) that reason over the entire database cluster rather than fixed
replica sets, improving partition availability under independent failures by
roughly 3x when tolerating one failure and 10x when tolerating two. Unlike
Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,
enabling immediate partition readiness after leader changes -- with at most a
per-key duplicate-resolution round trip when the new leader lacks the latest
copy. Under equal storage budgets -- where both systems maintain only f+1 data
copies to tolerate f failures -- LARK continues committing through data-node
failures while log-based protocols must pause commits for replica rebuilding.
These properties also enable zero-downtime rolling restarts even when
maintaining only two copies. We provide formal safety arguments and a TLA+
specification, and we demonstrate through analysis and experiments that LARK
achieves significant availability gains.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [199] [PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories](https://arxiv.org/abs/2511.00075)
*Qianhui Li,Weiya Wang,Qianqi Zhao,Tong Qu,Jing He,Xuhong Qiang,Jingwen Hou,Ke Chen,Bao Zhang,Qi Wang*

Main category: cs.AR

TL;DR: 提出PDA-LSTM模型，通过LSTM神经网络优化3D NAND闪存中的数据排列，抑制横向电荷迁移，降低比特错误率。


<details>
  <summary>Details</summary>
Motivation: QLC 3D NAND闪存因存储密度增加导致读取裕度变窄，易受横向电荷迁移影响。现有算法主要关注页内数据映射，但页间数据排列也能有效抑制电荷迁移。

Method: 使用LSTM神经网络计算数据排列概率矩阵，将输出矩阵转换为非重复序列生成概率矩阵，优化页间数据排列以最小化横向电荷迁移的全局影响。

Result: PDA-LSTM相比无数据排列策略平均BER降低80.4%，相比WBVM和DVDS（码长64）分别降低18.4%和15.2%，且无需额外标志位。

Conclusion: PDA-LSTM通过智能数据排列有效抑制横向电荷迁移，显著改善QLC 3D NAND闪存的可靠性，优于现有方法。

Abstract: Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant
storage solution in the era of artificial intelligence. QLC 3D NAND flash
stores 4 bit per cell to expand the storage density, resulting in narrower read
margins. Constrained to read margins, QLC always suffers from lateral charge
migration (LCM), which caused by non-uniform charge density across adjacent
memory cells. To suppress charge density gap between cells, there are some
algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we
observe inter-page data arrangements also approach the suppression. Thus, we
proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM
suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM
applies a long-short term memory (LSTM) neural network to compute a data
arrangement probability matrix from input page data pattern. The arrangement is
to minimize the global impacts derived from the LCM among wordlines. Since each
page data can be arranged only once, we design a transformation from output
matrix of LSTM network to non-repetitive sequence generation probability matrix
to assist training process. The arranged data pattern can decrease the bit
error rate (BER) during data retention. In addition, PDA-LSTM do not need extra
flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.
The experiment results show that the PDA-LSTM reduces the average BER by 80.4%
compared with strategy without data arrangement, and by 18.4%, 15.2% compared
respectively with WBVM and DVDS with code-length 64.

</details>


### [200] [Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim](https://arxiv.org/abs/2511.01244)
*Wajid Ali,Ayaz Akram,Deepak Shankar*

Main category: cs.AR

TL;DR: 使用VisualSim模拟多芯片系统级芯片架构，重点研究基于小芯片的系统建模和性能分析，评估通信延迟、内存访问效率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 传统单片芯片面临制造成本、能效和性能扩展的挑战，小芯片技术通过集成多个模块化硅单元提供了更灵活、可扩展且成本更低的替代方案。

Method: 开发了基于小芯片系统的详细仿真模型，包含多核ARM处理器集群，通过ARM CMN600片上网络实现高效通信，使用VisualSim仿真框架评估系统性能。

Result: 仿真分析揭示了影响小芯片系统性能的关键因素，包括芯片间通信延迟、内存访问效率和功耗性能权衡。

Conclusion: 该研究为优化未来基于小芯片的半导体设计提供了基础，通过仿真驱动的洞察力指导系统性能优化。

Abstract: This paper focuses on the simulation of multi-die System-on-Chip (SoC)
architectures using VisualSim, emphasiz- ing chiplet-based system modeling and
performance analysis. Chiplet technology presents a promising alternative to
traditional monolithic chips, which face increasing challenges in manufactur-
ing costs, power efficiency, and performance scaling. By integrat- ing multiple
small modular silicon units into a single package, chiplet-based architectures
offer greater flexibility and scalability at a lower overall cost. In this
study, we developed a detailed sim- ulation model of a chiplet-based system,
incorporating multicore ARM processor clusters interconnected through a ARM
CMN600 network-on-chip (NoC) for efficient communication [4], [7]. The
simulation framework in VisualSim enables the evaluation of critical system
metrics, including inter-chiplet communication latency, memory access
efficiency, workload distribution, and the power-performance tradeoff under
various workloads. Through simulation-driven insights, this research highlights
key factors influencing chiplet system performance and provides a foundation
for optimizing future chiplet-based semiconductor designs.

</details>


### [201] [H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention](https://arxiv.org/abs/2511.00295)
*Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: H-FA通过混合浮点和对数域定点计算优化FlashAttention硬件实现，在保持性能的同时显著降低面积和功耗


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在长序列上计算成本高，FlashAttention通过分块计算解决了这个问题，但其GPU设计仍有硬件优化空间

Method: 使用浮点数计算注意力分数，在对数域用定点算术进行softmax归一化和值矩阵乘法的融合计算，将乘除运算转换为加减运算

Result: 在28nm工艺下，相比纯浮点实现的FlashAttention硬件架构，H-FA平均减少26.5%面积和23.4%功耗，且不影响性能

Conclusion: 混合浮点-对数域定点计算方法能有效优化FlashAttention的硬件实现，为Transformer的长序列处理提供更高效的硬件加速方案

Abstract: Transformers have significantly advanced AI and machine learning through
their powerful attention mechanism. However, computing attention on long
sequences can become a computational bottleneck. FlashAttention mitigates this
by fusing the softmax and matrix operations into a tiled computation pattern
that decouples performance from sequence length. Though designed for GPUs, its
simplicity also makes it well suited for direct hardware acceleration. To
improve hardware implementation, we compute FlashAttention using a mixture of
floating-point and fixed-point logarithm domain representations. Floating-point
is used to compute attention scores from query and key matrices, while
logarithmic computation simplifies the fused computation of softmax
normalization and the multiplication with the value matrix. This
transformation, called H-FA, replaces vector-wide floating-point multiplication
and division operations by additions and subtractions implemented efficiently
with fixed-point arithmetic in the logarithm domain. Exponential function
evaluations are effectively omitted and fused with the rest operations, and the
final result is directly returned to floating-point arithmetic without any
additional hardware overhead. Hardware implementation results at 28nm
demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction
in power, on average, compared to FlashAttention parallel hardware
architectures built solely with floating-point datapaths, without hindering
performance.

</details>


### [202] [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)
*Dowon Kim,MinJae Lee,Janghyeon Kim,HyuckSung Kwon,Hyeonggyu Jeong,Sang-Soo Park,Minyong Yoon,Si-Dong Roh,Yongsuk Kwon,Jinin So,Jungwook Choi*

Main category: cs.AR

TL;DR: 提出了一种基于CXL的PNM KV缓存管理系统，通过内存近处理技术解决大语言模型长上下文推理中的内存和计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型上下文窗口扩展到百万token级别，KV缓存管理面临严重的内存和计算瓶颈，现有CXL非驱逐框架在长上下文下仍存在昂贵的数据传输问题。

Method: 设计CXL启用的KV缓存管理系统，将token页面选择卸载到CXL内存中的PNM加速器，消除昂贵的召回操作；引入混合并行化策略和稳定token选择机制。

Result: 在405B参数和1M token上下文的LLM上实现一致性能提升，PNM-KV和PnG-KV方案相比基线实现21.9倍吞吐量提升、60倍每token能耗降低和7.3倍总成本效率提升。

Conclusion: CXL启用的多PNM架构可以作为未来长上下文LLM推理的可扩展骨干系统。

Abstract: The expansion of context windows in large language models (LLMs) to
multi-million tokens introduces severe memory and compute bottlenecks,
particularly in managing the growing Key-Value (KV) cache. While Compute
Express Link (CXL) enables non-eviction frameworks that offload the full
KV-cache to scalable external memory, these frameworks still suffer from costly
data transfers when recalling non-resident KV tokens to limited GPU memory as
context lengths increase. This work proposes scalable Processing-Near-Memory
(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that
coordinates memory and computation beyond GPU limits. Our design offloads token
page selection to a PNM accelerator within CXL memory, eliminating costly
recalls and enabling larger GPU batch sizes. We further introduce a hybrid
parallelization strategy and a steady-token selection mechanism to enhance
compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM
system, our solution delivers consistent performance gains for LLMs with up to
405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)
and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x
throughput improvement, up to 60x lower energy per token, and up to 7.3x better
total cost efficiency than the baseline, demonstrating that CXL-enabled
multi-PNM architectures can serve as a scalable backbone for future
long-context LLM inference.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [203] [PEARL: Power- and Energy-Aware Multicore Intermittent Computing](https://arxiv.org/abs/2511.00316)
*Khakim Akhunov,Eren Yildiz,Kasim Sinan Yildirim*

Main category: cs.ET

TL;DR: PEARL是一种新型系统支持，使现有多核微控制器平台适合高效间歇计算，仅需三阈值电压跟踪电路和外部快速非易失性存储器，性能提升30倍，能耗降低32倍。


<details>
  <summary>Details</summary>
Motivation: 低功耗多核平台适合并行运行数据密集型任务，但在间歇供电下计算效率极低。

Method: 利用三阈值电压跟踪电路和外部快速非易失性存储器，通过软件运行时管理这些组件，执行能量和功率感知的多核配置适配，最小化备份开销并提升性能。

Result: 评估显示PEARL比最先进解决方案性能提升高达30倍，能耗降低高达32倍。

Conclusion: PEARL能够使现有多核MCU平台高效支持间歇计算，仅需少量硬件支持即可显著提升性能和能效。

Abstract: Low-power multicore platforms are suitable for running data-intensive tasks
in parallel, but they are highly inefficient for computing on intermittent
power. In this work, we present PEARL (PowEr And eneRgy-aware MuLticore
Intermittent Computing), a novel systems support that can make existing
multicore microcontroller (MCU) platforms suitable for efficient intermittent
computing. PEARL achieves this by leveraging only a three-threshold voltage
tracking circuit and an external fast non-volatile memory, which multicore MCUs
can smoothly interface. PEARL software runtime manages these components and
performs energy- and power-aware adaptation of the multicore configuration to
introduce minimal backup overheads and boost performance. Our evaluation shows
that PEARL outperforms the state-of-the-art solutions by up to 30x and consumes
up to 32x less energy.

</details>


### [204] [Edge-Enabled UAV Swarm Deployment for Rapid Post-Disaster Search and Rescue](https://arxiv.org/abs/2511.01459)
*Alaa Awad Abdellatif,Helder Fontes,Andre Coelho,Luis M. Pessoa,Rui Campos*

Main category: cs.ET

TL;DR: 提出了一种基于多无人机的联合雷达通信系统优化方案，通过分布式算法解决无人机定位和功率分配问题，在保证通信性能的同时最大化雷达感知能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中同时实现感知和通信目标，利用多无人机协同工作来提升雷达感知性能，解决传统方法难以兼顾雷达和通信性能的问题。

Method: 采用分布式JRC解决方案，通过设计高效奖励机制选择最优动作，解决NP-hard的无人机定位和功率分配组合优化问题。

Result: 仿真结果显示，与现有雷达或通信中心的轨迹规划方法相比，所提方案在性能上有显著提升，计算复杂度与无人机数量呈多项式关系，与迭代次数呈线性关系。

Conclusion: 该分布式JRC系统能够有效优化多无人机在目标区域的感知和通信性能，为复杂环境下的联合雷达通信应用提供了可行解决方案。

Abstract: This paper presents an optimized Joint Radar-Communication (JRC) system
utilizing multiple Unmanned Aerial Vehicles (UAVs) to simultaneously achieve
sensing and communication objectives. By leveraging UAVs equipped with dual
radar and communication capabilities, the proposed framework aims to maximize
radar sensing performance across all UAVs in challenging environments. The
proposed approach focuses on formulating and solving a UAV positioning and
power allocation problem to optimize multi-UAV sensing and communications
performance over multiple targets within designated zones. Due to the NP-hard
and combinatorial nature of the problem, we propose a Distributed JRC-based
(DJRC) solution. This solution employs an efficient reward for potential
actions and consistently selects the best action that maximizes the reward
while ensuring both communications and sensing performance. Simulation results
demonstrate significant performance improvements of the proposed solution over
state-of-the-art radar- or communication-centric trajectory planning methods,
with polynomial complexity dependent on the number of UAVs and linear
dependence on the iteration count.

</details>
