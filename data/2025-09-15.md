<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 51]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑图表示，在精神疾病诊断中优于现有方法，特别是在小样本标注数据场景下。


<details>
  <summary>Details</summary>
Motivation: 解决脑网络数据标注稀缺的问题，同时避免现有自监督学习方法中可能破坏脑图结构语义的数据增强策略。

Method: 提出两阶段框架：预训练阶段使用边缘掩码器在小标注子集上捕捉关键结构语义；自监督学习阶段利用结构先验指导结构感知的数据增强过程。

Result: 在两个真实精神疾病数据集上的实验表明，SAM-BG在性能上超越最先进方法，特别是在小标注数据设置下，并能发现临床相关的连接模式。

Conclusion: SAM-BG框架能够学习更具语义意义和鲁棒性的脑图表示，提高精神疾病诊断的准确性和可解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [2] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT是一种解耦跨注意力迁移框架，允许在推理时仅使用单一传感器进行跨模态知识迁移，解决了传统方法需要配对传感器数据的限制


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法在训练和推理时都需要配对传感器数据，这在资源受限环境中部署困难，因为完整传感器套件在经济和技术上不可行

Method: 结合自注意力模块进行特征提取，使用新颖的跨注意力对齐损失来对齐模态特定表示空间，无需耦合两种模态的分类流程

Result: 在分布内场景中，从高性能模态（如视频到IMU）迁移可获得高达10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能

Conclusion: D-CAT通过实现单传感器推理的跨模态知识迁移，减少了感知系统的硬件冗余，同时保持准确性，对成本敏感或自适应部署至关重要

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [3] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了一个完全自我改进的加密货币交易代理，无需人工监督，在多模态市场输入下表现出色。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测极其困难，价格变动受多种因素驱动（链上活动、新闻流、社交情绪），且标记训练数据稀缺昂贵，需要开发无需人工监督的自我改进交易系统。

Method: 从指令调优的LLM开始，代理在闭环架构中迭代扮演三个角色（执行者、评判者、元评判者），利用多模态市场输入和内部偏好反馈，持续精炼交易策略和评估标准。

Result: 在不同市场机制下的实验表明，Meta-RL-Crypto在真实市场的技术指标上表现良好，并优于其他基于LLM的基线方法。

Conclusion: 该研究提出了一个统一的元学习-强化学习框架，成功创建了自我改进的加密货币交易代理，证明了在无需人工监督的情况下实现有效交易策略的可行性。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [4] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF是一种新颖的联邦学习框架，通过在本地训练期间直接学习量化模型参数，逐比特更新参数来大幅减少通信开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在显著的通信开销问题，现有量化方法通常在本地训练后应用量化，导致量化误差影响模型精度。需要一种在训练过程中直接学习量化参数的方法。

Method: 服务器先将模型参数量化后传输给客户端，每个客户端每次只更新多比特参数表示中的单个比特，冻结其余比特，实现逐比特更新策略。

Result: 在5个常用数据集上的IID和非IID设置下，FedBiF实现了优异的通信压缩，促进模型稀疏性，仅使用1bpp上行和3bpp下行通信即可达到与FedAvg相当的精度。

Conclusion: FedBiF通过直接学习量化参数和逐比特更新策略，有效解决了联邦学习的通信开销问题，在保持模型精度的同时大幅减少了通信成本。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [5] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流信息损失来实现动态预算分配，在多种长文本基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法根据任务需求智能分配缓存资源。

Method: 通过分析层注意力输出损失，提出新的度量标准来比较不同头的缓存条目，实现层级压缩和动态头预算分配；通过跨层信息对比实现动态层预算分配。

Result: 在LongBench、Needle-In-A-Haystack、Ruler和InfiniteBench等基准测试中表现出优越性能，发现动态层预算对生成任务关键，动态头预算对抽取任务重要。

Conclusion: LAVa是首个统一的缓存淘汰和动态预算分配策略，无需训练或多策略组合，在各种任务类型中保持顶级性能。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [6] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO框架通过分离风险校准和偏好优化，为医疗补助人群的健康管理提供保守、可审计的决策支持，在控制不良事件风险的同时保持高安全覆盖率。


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群的健康管理项目需要协调纵向服务，必须确保安全、公平和可审计性，同时控制不良医疗事件的风险。

Method: 提出混合自适应符合离线强化学习(HACO)框架：1)训练轻量级风险模型预测不良事件；2)推导符合阈值以屏蔽不安全行动；3)在安全子集上学习偏好策略。使用280万次决策数据进行评估。

Result: HACO实现了强大的风险区分能力(AUC ~0.81)，校准阈值在α=0.10时为τ~0.038，同时保持高安全覆盖率。亚组分析显示不同人口统计特征的价值估计存在系统性差异。

Conclusion: 符合风险门控与离线强化学习无缝集成，可为人口健康管理团队提供保守、可审计的决策支持，强调了公平性审计的重要性。

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [7] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出了一个统一的LLM路由框架，使用单头交叉注意力机制动态选择最优LLM，在RouterBench基准测试中实现了6.6%的质量提升和2.9%的最大性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型部署中计算成本和性能差异的挑战，需要一种能够动态选择最优模型的成本感知路由方案。

Method: 采用单头交叉注意力机制联合建模查询和模型嵌入，预测响应质量和生成成本，并提出指数奖励函数来平衡性能与成本。

Result: 在RouterBench基准测试中，AIQ指标提升6.6%，最大性能提升2.9%，架构轻量且能有效跨域泛化。

Conclusion: 该路由框架为成本感知的LLM路由建立了新标准，在性能和效率方面均优于现有方法。

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [8] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步长去噪器及其在即插即用算法中的应用，该去噪器被训练为显式函数梯度的精确算子，同时保持最先进的去噪能力


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法使用现成去噪器替代图像先验的邻近算子或梯度下降算子，但通常这些先验是隐式的且无法表达

Method: 训练梯度步长去噪器，使其成为显式函数的梯度下降算子或邻近算子

Result: 梯度步长去噪器能够保持最先进的去噪性能，同时提供明确的数学表达

Conclusion: 梯度步长去噪器成功地将隐式图像先验转化为显式函数表示，为即插即用算法提供了理论基础和数学可解释性

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [9] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 本研究使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶反应，最高准确率达85.7%，并成功区分惊吓、惊讶和基线状态，准确率达74.9%。


<details>
  <summary>Details</summary>
Motivation: 意外事件会损害注意力和延迟决策，在高风险环境如航空中构成严重安全风险。惊吓和惊讶反应以不同方式影响飞行员表现，但实践中难以区分，现有研究多单独研究这些反应，缺乏对其组合效应或使用生理数据区分方法的关注。

Method: 使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶事件。采用SVM和XGBoost等算法，以及Late Fusion等多模态融合方法。

Result: 惊吓和惊讶事件可被可靠预测，SVM和Late Fusion组合达到最高平均准确率85.7%。扩展评估包括基线条件后，XGBoost和Late Fusion组合能区分惊吓、惊讶和基线状态，最高平均准确率达74.9%。

Conclusion: 该研究证明了使用生理信号和机器学习方法有效区分惊吓和惊讶反应的可行性，为高风险环境中意外事件的监测和干预提供了新途径。

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [10] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 本文重新审视了离散动作环境中的actor-critic方法，发现DSAC性能不佳的主要原因是actor和critic熵的耦合。通过解耦这两个组件并提出灵活的off-policy框架，在Atari游戏中达到了与DQN相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决离散动作环境中off-policy强化学习的问题。值基方法如DQN是默认选择，而常见的策略基方法要么无法有效利用off-policy数据（如PPO），要么在离散动作设置中表现不佳（如SAC）。

Method: 提出灵活的off-policy actor-critic框架：1）解耦actor和critic的熵组件；2）使用m步Bellman算子进行critic更新；3）将标准策略优化方法与熵正则化结合来实例化actor目标。

Result: 理论证明在表格设置中可以保证收敛到最优正则化值函数。实证表明在标准Atari游戏中能够接近DQN的性能，即使没有熵正则化或显式探索。

Conclusion: 通过解耦actor-critic的熵组件和提出灵活的框架，成功解决了离散动作off-policy强化学习的问题，为离散环境中的actor-critic方法提供了新的设计思路。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [11] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN是首个针对异质图的集成学习框架，通过元路径和变换优化管道集成多个学习器，提升分类准确率


<details>
  <summary>Details</summary>
Motivation: 异质图中节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来重大挑战，特别是在适应多样化图学习器方面

Method: 使用元路径结合随机丢弃创建等位基因图神经网络，通过残差注意力机制校准不同元路径的等位基因GNN，并使用相关性正则化项扩大不同元路径生成的嵌入矩阵差异

Result: 在五个异质网络上的实验验证HGEN始终以显著优势优于最先进的竞争对手

Conclusion: HGEN通过有效的集成学习框架成功解决了异质图学习中的挑战，提供了更高的正则化幅度和更好的性能

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [12] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如best-of-N或beam search），同时考虑token成本和延迟时间，以优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时扩展方法主要关注并行生成策略（如best-of-N），忽略了增量解码方法（如beam search），且大多只关注token使用量而忽视了延迟时间对用户体验的重要性。

Method: 将推理时扩展建模为动态计算分配和方法选择问题，系统根据每个查询的需求决定应用哪种策略以及分配多少计算资源，同时显式考虑token成本和wall-clock延迟。

Result: 在推理基准测试中，该方法始终优于静态策略，实现了良好的准确率-成本权衡，同时保持部署实用性。

Conclusion: 动态计算分配框架能够有效优化LLM推理性能，在保证准确率的同时控制成本和延迟，特别适合需要高效多查询的智能体工作流。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [13] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 本文提出了一种基于可观测变量的数据驱动计算框架，用于描述耗散动力系统的相空间演化，通过热力学拉格朗日方法和神经网络来保证热力学约束和熵增特性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法通常需要相空间变量的完整信息，但在耗散动力系统中，动量和熵等关键变量往往无法直接观测，这限制了传统方法的应用。

Method: 开发了基于热力学拉格朗日的新型神经网络框架，仅使用可观测变量（坐标），同时保证热力学约束和熵的非递减演化特性。

Result: 所提出的网络能够基于有限数据点和相对较少的系统参数，有效描述相空间演化过程。

Conclusion: 该方法为仅使用可观测变量的耗散动力系统建模提供了有效解决方案，具有参数效率高和数据需求少的优势。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [14] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 该论文提出了LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习问题，并在开放世界场景下扩展为LoFT-OW来处理分布外样本。


<details>
  <summary>Details</summary>
Motivation: 现有的长尾半监督学习方法通常需要从头训练模型，容易导致过自信和低质量伪标签问题。为了解决这些问题，作者将长尾半监督学习扩展到基础模型微调范式。

Method: 提出LoFT框架，通过参数高效微调预训练基础模型来生成更可靠的伪标签，并针对开放世界场景提出LoFT-OW来提升判别能力。

Result: 在多个基准测试中，该方法相比先前方法取得了优越性能，即使只使用1%的无标签数据也能超越之前的工作。

Conclusion: 微调的基础模型能够生成更可靠的伪标签，从而有益于不平衡学习，特别是在开放世界场景下处理分布外样本时表现优异。

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [15] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 提出了多播放组合半强盗(MP-CSB)模型，扩展了传统组合半强盗问题，支持非负整数动作和单臂多次反馈，适用于最优传输和背包等问题。提出了两种算法：基于Thompson采样的高效算法和适应随机与对抗环境的通用算法。


<details>
  <summary>Details</summary>
Motivation: 传统组合半强盗(CSB)问题限制在二元决策空间，无法处理涉及非负整数流或分配的重要应用场景，如最优传输和背包问题。需要扩展模型以支持更广泛的实际应用。

Method: 提出MP-CSB模型，允许选择非负整数动作并从单个臂获得多次反馈。开发了两种算法：1)基于Thompson采样的算法，计算高效且获得O(log T)遗憾；2)最佳两用算法，在随机环境下获得方差依赖遗憾，在对抗环境下获得数据依赖的最坏情况遗憾。

Result: Thompson采样算法在动作空间指数大时仍计算可行，在随机环境下获得O(log T)分布依赖遗憾。最佳两用算法在随机环境下获得O(log T)方差依赖遗憾，在对抗环境下获得数据依赖的O(√T)遗憾，适应最优动作累积损失、总二次变差和损失序列路径长度。数值实验显示优于现有方法。

Conclusion: MP-CSB成功扩展了CSB框架，提出的算法在理论和实验上都表现出色，为处理非负整数动作的组合优化问题提供了有效解决方案，在随机和对抗环境下都具有良好的性能保证。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [16] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 本文探讨使用LLMs作为科学机器学习代理，通过生成代码解决ODE问题，而非直接学习解函数。提出了新的诊断数据集和大规模基准测试，评估LLMs在科学计算任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法直接预测目标值存在精度和鲁棒性挑战，本文探索使用LLMs编写数值算法代码的替代方案，将学习负担转移到领域感知的数值选择上。

Method: 引入两个新数据集：诊断性误导问题和1000个多样化ODE任务的大规模基准。评估开源和闭源LLM模型在无引导vs领域知识引导提示、现成vs微调变体下的表现，测量可执行性和数值有效性。

Result: 研究发现，在充分上下文和引导提示下，较新的指令跟随模型在两个评估标准上都达到高精度。开源系统无需微调表现强劲，而较老或较小模型仍能从微调中受益。

Conclusion: 精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专用LLM代理，为科学机器学习提供了新的研究方向。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [17] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena模型通过将音频视觉线索转换为动态的每词元卷积核来直接调制文本特征提取，解决了多模态意图识别中模态间信息冲突和噪声问题，在MIntRec基准上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 多模态意图识别中，不同模态间可能存在意图无关和冲突信息，传统融合方法容易破坏主要语言特征，需要更细粒度的调制方法来让非语言信号调节而非简单增强文本含义。

Method: 提出DyKen-Hyena模型，将问题从特征融合重新定义为处理调制，将音频视觉线索转换为动态的每词元卷积核来直接调制文本特征提取过程。

Result: 在MIntRec和MIntRec2.0基准上达到最先进水平，特别是在超出范围检测中获得+10.46%的F1分数提升，验证了方法能创建更鲁棒的意图表示。

Conclusion: 通过细粒度的动态调制方法，DyKen-Hyena有效解决了多模态意图识别中的信息冲突问题，显著提升了模型性能和鲁棒性。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [18] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出一种无需训练的自适应token合并框架，通过动态合并语义冗余token来压缩transformer表示，在保持精度的同时显著降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 大规模transformer在语义通信中计算和通信成本过高，难以部署在资源受限的边缘设备上，需要一种无需重新训练的高效压缩方法

Method: 基于每层相似度阈值选择性合并语义冗余token，将合并策略发现建模为多目标优化问题，使用贝叶斯优化获得精度、推理成本和通信成本之间的帕累托最优权衡

Result: 在ImageNet分类上以30%更少的FLOPs和低于20%的原始通信成本匹配未修改transformer的精度；在VQA任务上以不到三分之一计算量和十分之一带宽实现与完整LLaVA模型相当的性能

Conclusion: 该框架为资源受限的边缘智能场景提供了实用且通用的transformer部署解决方案，具有跨信道条件的鲁棒性和固有的隐私保护优势

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [19] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine是一个合成表格数据生成框架，通过从可解释模型提取符号规则来指导生成，并采用双粒度过滤策略减少分布不平衡，在数据稀缺场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成方法（GANs、扩散模型、微调LLMs）需要充足参考数据，在领域特定数据库记录稀缺时效果有限。基于提示的LLMs虽然灵活但难以捕捉数据集特定的特征-标签依赖关系，且生成冗余数据导致下游任务性能下降。

Method: ReFine框架：(i)从可解释模型推导符号"if-then"规则并嵌入提示中，显式指导生成朝向领域特定特征分布；(ii)应用双粒度过滤策略，抑制过采样模式并选择性精炼稀有但信息丰富的样本以减少分布不平衡。

Result: 在多个回归和分类基准测试上的广泛实验表明，ReFine始终优于最先进方法，回归任务R平方绝对提升高达0.44，分类任务F1分数相对提升10.0%。

Conclusion: ReFine通过结合符号规则指导和双粒度过滤，有效解决了数据稀缺场景下的表格数据生成问题，显著提升了生成数据的质量和下游任务性能。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [20] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的虚拟服务器能耗估计方法，仅使用虚拟机资源利用率指标即可准确预测能耗，无需物理功率测量接口或主机特权访问


<details>
  <summary>Details</summary>
Motivation: 解决虚拟化环境（如云平台）中无法直接测量能耗的关键问题，为能源感知调度和成本优化提供可行方案

Method: 使用梯度提升回归器（Gradient Boosting Regressor），基于虚拟机收集的资源利用率指标训练模型，预测通过RAPL测量的主机能耗

Result: 在多样化工作负载实验中实现了高预测精度（0.90 ≤ R² ≤ 0.97），首次证明了仅凭虚拟机资源即可进行能耗估计的可行性

Conclusion: 该方法能够实现虚拟机侧的能耗估计，支持能源感知调度、成本优化和物理主机独立的能耗估算，填补了虚拟化环境中能耗测量的重要空白

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [21] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 该论文实证研究了深度回归模型中的神经缩放定律，发现在扭曲范德瓦尔斯磁体参数估计模型中，损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型成功证明了神经缩放定律的重要性，但这些定律在深度回归模型中的应用仍未被充分探索。研究者希望了解深度回归模型中是否存在类似的缩放规律。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，在不同架构（包括全连接网络、残差网络和视觉变换器）上实证研究损失与训练数据集大小和模型容量之间的关系。

Result: 观察到损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围从1到2，具体值取决于回归参数和模型细节。

Conclusion: 一致的缩放行为和大缩放指数表明，深度回归模型的性能可以随着数据量的增加而显著提高，这为资源管理提供了重要指导。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [22] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: 本文提出了IDEA（内在维度估计自编码器），能够准确估计线性和非线性流形数据集的内在维度，并通过重构损失项实现高质量的数据重构。


<details>
  <summary>Details</summary>
Motivation: 现有内在维度估计方法在处理复杂数据集时存在局限性，需要一种既能准确估计内在维度又能保持良好重构性能的统一框架。

Method: 使用带重加权双CancelOut层的自编码器结构，引入投影重构损失项，通过连续评估去除潜在维度后的重构质量来指导模型训练。

Result: 在理论基准测试中表现出良好的准确性和通用性，成功应用于垂直解析一维自由表面流数值解数据的内在维度估计和重构。

Conclusion: IDEA是一个有效且通用的内在维度估计工具，能够同时实现准确的维度估计和高质量的数据重构，适用于各种线性和非线性流形数据集。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [23] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 提出稀疏专家混合变分自编码器(SMoE-VAE)架构，在QuickDraw数据集上发现无监督专家路由比有监督基准表现更好，能识别超越人工类别边界的有意义子类别结构。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中神经网络内部组织的根本挑战，探索如何更好地理解模型如何发现与目标更一致的基础数据结构。

Method: 使用稀疏专家混合变分自编码器(SMoE-VAE)架构，在QuickDraw数据集上比较无监督专家路由和有监督基准，通过t-SNE可视化和重建分析进行研究。

Result: 无监督路由始终获得更好的重建性能，专家学会识别有意义且常超越人工定义类别边界的子类别结构。数据集大小研究揭示了数据量与专家专业化之间的权衡。

Conclusion: MoE模型能够发现比预定义标签更符合模型目标的基础数据结构，为设计高效MoE架构提供了指导。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [24] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出了一种用于稀疏字典编码的低秩编码模型AODL，通过凸松弛和交替优化方法学习字典，在保证重建质量的同时获得更稀疏的解，并展示了对未见数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决多字典场景下同时学习字典和编码系数的挑战，传统数据无关的分析字典虽然高效但稀疏性不足，而数据驱动的学习字典虽然更准确但学习复杂度高，特别是在多字典组合时编码系数数量爆炸增长的问题。

Method: 提出低秩编码模型用于双字典场景，采用凸松弛方法AODL，通过交替优化稀疏编码矩阵和学习字典，证明了该方法的收敛性。

Result: 在合成和真实数据集上验证了AODL在数据重建和缺失值填补方面的质量，相比非低秩和分析字典基线，在相同重建质量下学习到的解稀疏度提高了90%，且学习到的字典能揭示训练样本中的可解释模式。

Conclusion: AODL方法有效解决了多字典学习的数据复杂度问题，提供了更稀疏且可解释的解决方案，具有良好的泛化性能和应用价值。

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [25] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文提出了一种用符号前馈神经网络精确模拟概率有限自动机(PFA)的理论框架，通过矩阵向量乘积实现概率状态传播，建立了PFA与神经网络之间的形式等价性。


<details>
  <summary>Details</summary>
Motivation: 弥合符号计算与深度学习之间的差距，将概率自动机理论与神经架构在严格的代数框架下统一起来，实现可解释、可微分且可学习的概率自动机模拟。

Method: 使用符号前馈神经网络架构，将状态分布表示为向量，转移表示为随机矩阵，通过矩阵向量乘积实现概率状态传播，采用分层符号计算进行概率子集构造和ε闭包。

Result: 证明了PFA与特定类别神经网络的等价性，展示了这些符号模拟器不仅具有表达力而且可学习：通过标准梯度下降优化在标注序列数据上训练，能够准确恢复真实PFA的行为。

Conclusion: 该工作为概率自动机理论与神经架构的统一提供了严格的代数框架，实现了符号计算与深度学习的桥梁，具有重要的理论和实践意义。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [26] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP是一种新颖的联邦学习算法，结合随机投影和ADMM优化框架，在保护隐私的同时降低通信成本，并提供(ε,δ)-差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在IoT和医疗数据分析等敏感领域面临用户隐私保护和通信成本管理的挑战，需要开发既能保护隐私又能降低通信开销的新方法。

Method: 提出FedRP算法，将随机投影技术与ADMM优化框架结合，通过随机投影降低模型参数维度后再传输到中央服务器，减少通信成本。

Result: 实验结果表明FedRP不仅保持高模型精度，而且在隐私保护和通信效率方面优于传统差分隐私方法和FedADMM。

Conclusion: FedRP算法通过随机投影和ADMM的结合，有效解决了联邦学习中的隐私保护和通信效率问题，为实际应用提供了可行的解决方案。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [27] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 评估VBLL与TabPFN集成在不确定性校准中的性能，意外发现原始TabPFN在所有数据集上始终优于VBLL集成版本


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等安全关键应用中，可靠的不确定性估计至关重要。TabPFN是新兴的表格数据基础模型，VBLL是最先进的轻量级变分方法，本研究旨在评估两者集成在不确定性校准中的表现

Method: 在三个基准医疗表格数据集上进行实验，比较原始TabPFN和VBLL集成版本的性能

Result: 与预期相反，原始TabPFN在所有数据集的不确定性校准方面始终优于VBLL集成TabPFN

Conclusion: VBLL集成并未改善TabPFN的不确定性校准性能，原始TabPFN表现出更好的校准能力

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [28] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR是一个基于Kolmogorov Arnold Networks的新型符号回归框架，采用分治方法，结合深度学习技术和简化策略，能够准确恢复Feynman SRSD数据集中的真实方程，并能精确建模生物过程系统的动力学。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归通常使用遗传编程方法，本文旨在利用深度学习技术、专门的KAN网络和简化策略来提高符号回归的准确性和效率，特别是在科学发现和工程系统建模方面。

Method: 使用Kolmogorov Arnold Networks (KANs)构建符号回归框架，采用分治方法，结合深度学习技术、平移对称性和可分离性等简化策略，并与神经控制微分方程结合来建模动力学系统。

Result: 成功恢复了Feynman SRSD数据集中的真实方程，并精确建模了硅内生物过程系统的动力学，为其他工程系统的动态建模开辟了道路。

Conclusion: KAN-SR框架通过结合深度学习和符号回归技术，在科学发现和工程系统建模方面表现出色，为复杂系统的数学建模提供了有效的新方法。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [29] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的贝叶斯联邦学习个性化框架，通过将全局模型投影到用户本地模型的邻域来实现全局泛化与本地特化的可调权衡，在异构数据分布下有效平衡全局和本地性能。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习方法通常依赖MCMC采样或变分推断，需要个性化机制来适应本地数据分布。本文旨在开发一种计算效率高的个性化方法，在数据异构和隐私约束下实现可靠模型。

Method: 采用信息几何投影框架，将全局模型投影到用户本地模型的统计流形邻域，证明该投影等价于计算统计流形上的重心，从而获得闭式解和零成本个性化。结合改进变分在线牛顿优化器(IVON)应用于变分学习设置。

Result: 经验评估表明，该方法在异构数据分布下能有效平衡全局和本地性能，且计算开销极小。投影步骤的闭式解使得个性化过程计算成本极低。

Conclusion: 提出的信息几何投影框架为贝叶斯联邦学习提供了一种有效的个性化方法，实现了全局泛化与本地特化的最优权衡，具有计算效率高和性能优越的特点。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [30] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 提出了BenchECG标准化基准和xECG模型，通过统一的评估框架解决ECG基础模型缺乏公平比较的问题，xECG在多个数据集和任务上表现最优


<details>
  <summary>Details</summary>
Motivation: 现有的ECG基础模型研究缺乏一致的评估标准，使用不同的任务选择和数据集，阻碍了公平比较和进展

Method: 开发BenchECG标准化基准，包含全面的公开ECG数据集和多样化任务；提出基于xLSTM和SimDINOv2自监督学习的xECG循环模型

Result: xECG在BenchECG基准上取得了最佳分数，是唯一在所有数据集和任务上都表现优异的公开可用模型

Conclusion: BenchECG标准化评估能够促进ECG表示学习的严格比较和加速进展，xECG为未来ECG基础模型设立了新的基准

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [31] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 本文提出了一种联邦多智能体强化学习框架Fed-MARL，用于6G超密集边缘网络中的隐私保护、实时资源管理，通过跨层协调MAC层和应用层，在保证隐私的同时优化延迟、能效和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向超密集智能边缘环境发展，在严格的隐私、移动性和能耗约束下实现高效的资源管理变得至关重要。传统集中式方法面临隐私泄露和可扩展性问题。

Method: 采用联邦多智能体强化学习框架，每个智能体使用深度循环Q网络学习分散化策略，包括任务卸载、频谱接入和CPU能耗调节。引入基于椭圆曲线Diffie-Hellman密钥交换的安全聚合协议保护隐私。将问题建模为部分可观测多智能体马尔可夫决策过程。

Result: 仿真结果表明，Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保了强大的隐私保护和在动态资源受限6G边缘网络中的可扩展性。

Conclusion: Fed-MARL框架为6G边缘网络提供了一种有效的隐私保护资源管理解决方案，能够同时优化多个性能指标，满足URLLC、eMBB和mMTC等6G特定服务要求。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [32] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出了一种通过神经网络近似综合测量来重新优化表面码解码器的方法，解决了传统解码器只能获取错误概率分布的问题，在不同网络架构和码距下都提高了解码精度。


<details>
  <summary>Details</summary>
Motivation: 量子纠错码中表面码虽然具有高错误阈值，但传统解码器由于输入的非唯一性只能获取错误概率分布，无法准确预测错误，需要改进解码方法。

Method: 使用神经网络将综合测量近似为连续函数进行数学插值，将表面码解码问题重新构建为回归问题，通过深度学习技术重新优化解码器模型。

Result: 在码距5和7的多层感知机解码器，以及码距5的卷积神经网络、循环神经网络和Transformer解码器上，重新优化的解码器都比原始模型精度更高。

Conclusion: 将表面码解码问题重新构建为回归问题并用深度学习处理是一个有效的通用策略，该方法独立于码距和网络架构，具有普遍适用性。

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [33] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 该论文研究了深度残差网络在标准随机初始化下的梯度训练动态，证明了当深度趋于无穷时，训练动态收敛到神经平均ODE，并分析了不同残差缩放因子对特征学习的影响。


<details>
  <summary>Details</summary>
Motivation: 研究深度残差网络在标准随机初始化下的训练动态，特别是当网络深度趋于无穷时的极限行为，以及不同参数缩放对特征学习能力的影响。

Method: 使用数学分析工具，包括随机近似和传播混沌理论，研究ResNet的前向和后向传播行为，推导出训练动态收敛到平均ODE的条件和误差界限。

Result: 证明了深度残差网络在深度趋于无穷时收敛到神经平均ODE训练动态，获得了模型输出与极限之间的误差界限，并验证了这些界限的紧致性。发现特定残差缩放因子Θ(√D/LM)能实现完全特征学习。

Conclusion: 深度残差网络的训练动态可以通过平均ODE来刻画，不同的参数缩放会导致不同的特征学习行为，其中特定缩放因子能够实现完全的非线性特征学习，这为理解深度网络训练提供了新的数学视角。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [34] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了一个可扩展的框架，用于学习高分辨率3D物理模拟的确定性和概率性神经代理模型，采用混合CNN-Transformer架构，在速度和准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决高分辨率3D物理模拟中计算成本高、内存需求大的问题，需要开发高效的神经代理模型来替代传统的数值模拟方法。

Method: 引入混合CNN-Transformer骨干架构，支持在小块模拟域上进行预训练，然后融合获得全局解，可选地通过序列到序列模型包含长程依赖关系。

Result: 在14种不同类型3D PDE动力学学习任务中显著优于基线方法，能够扩展到512^3空间分辨率的高分辨率各向同性湍流，并能作为扩散模型生成不同雷诺数下高度湍流3D通道流的概率样本。

Conclusion: 该框架为高分辨率3D物理模拟提供了高效、可扩展的神经代理解决方案，在保持准确性的同时大幅降低了计算和内存需求。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [35] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出了一种新的集成学习框架，通过同时优化期望边际和边际方差来提升泛化性能，并将权重重新参数化到单位球面上以简化优化过程。


<details>
  <summary>Details</summary>
Motivation: 传统基于边际的集成方法主要关注最大化期望边际而忽略边际方差的重要性，这限制了模型的泛化能力并在噪声或不平衡数据集中容易过拟合。同时，传统方法在概率单纯形中优化集成权重存在计算效率低和可扩展性差的问题。

Method: 引入了一个新的集成学习框架，将边际方差明确纳入损失函数，联合优化负期望边际及其方差。通过将集成权重重新参数化到单位球面上来简化优化过程并提高计算效率。

Result: 在多个基准数据集上的广泛实验表明，所提出的方法 consistently 优于传统的基于边际的集成技术。

Conclusion: 该方法通过同时考虑期望边际和边际方差，显著提高了集成学习的鲁棒性和泛化性能，同时通过权重重新参数化解决了计算效率问题，具有很好的实用价值。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [36] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的飞机机翼疲劳寿命预测管道，通过飞行参数快速估算不同机翼位置的疲劳寿命，作为传统工程方法的补充。


<details>
  <summary>Details</summary>
Motivation: 传统疲劳寿命预测方法虽然可靠但耗时且流程复杂，需要多团队协作和大量计算资源。机器学习方法可以提供快速估算，指导决策并减少昂贵模拟的需求。

Method: 开发基于机器学习的管道，利用飞机不同任务的飞行参数来预测机翼各位置的疲劳寿命，并进行统计验证和不确定性量化。

Result: 在现实的疲劳寿命估算用例中验证了管道的有效性，获得了准确的预测结果。

Conclusion: 该机器学习管道能够显著减少计算和人力资源需求，是对传统疲劳寿命预测方法的有价值补充。

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [37] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统评估发现，简单的提示词注入攻击在LLM科学评审中极其有效（可达100%接受率），且LLM评审普遍存在接受偏向（>95%），这对LLM在同行评审中的使用讨论具有重大影响。


<details>
  <summary>Details</summary>
Motivation: 针对作者使用隐藏提示词注入操纵评审分数的现象，研究这种攻击的可行性和技术成功率，以影响关于LLM在科学同行评审中使用的持续讨论。

Method: 使用多种LLM对2024年ICLR论文的1000篇评审进行系统评估，分析简单提示词注入的效果和LLM评审的偏向性。

Result: 1) 非常简单提示词注入高度有效，达到100%接受率；2) LLM评审普遍偏向接受（许多模型>95%接受率）。

Conclusion: 研究结果对LLM在同行评审中的使用讨论具有重大影响，揭示了当前LLM评审系统的脆弱性和系统性偏向问题。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [38] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 提出基于神经推荐系统的迁移学习框架，利用COSMO-RS模拟数据和稀疏实验数据，准确预测离子液体的五种关键热物理性质


<details>
  <summary>Details</summary>
Motivation: 离子液体具有可定制的物理化学性质，但由于化学设计空间巨大和实验数据有限，准确预测其热物理性质仍具挑战性

Method: 两阶段方法：先在固定温压下使用COSMO-RS模拟数据预训练神经推荐系统学习结构嵌入，再用实验数据微调前馈神经网络进行性质预测

Result: 该框架支持性质和跨性质知识迁移，对五种性质中的四种实现了显著性能提升，能够预测超过70万种离子液体组合的性质

Conclusion: 结合模拟数据和迁移学习能有效克服实验数据稀疏性问题，为离子液体筛选提供可扩展的解决方案

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [39] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 提出一种基于SDN和AutoML的区块链安全能源交易架构，使用机器学习回归模型作为随机数生成器来确保交易完整性，特别适用于灾难场景下的太阳能家庭与移动充电单元之间的能源交易。


<details>
  <summary>Details</summary>
Motivation: 在传统能源基础设施受损的灾难场景中，需要确保太阳能家庭与移动充电单元之间能源交易的安全性和可追溯性。区块链网络需要强大且不可预测的随机数生成来保证交易完整性。

Method: 采用SDN架构实现灵活的数据流和能源路由控制，利用五种AutoML选择的回归模型（梯度提升、LightGBM、随机森林、额外树和K近邻）作为随机数生成器，通过9000个样本数据集评估这些模型产生随机输出的能力。

Result: 随机性分析显示随机森林和额外树回归器表现出完全的随机依赖性，梯度提升、K近邻和LightGBM也显示出很强的随机性得分（分别为97.6%、98.8%和99.9%）。树基集成模型特别适合作为轻量级随机数生成器。

Conclusion: 某些机器学习模型，特别是树基集成方法，可以作为区块链安全、SDN基础的能源交易基础设施中有效且轻量级的随机数生成器，增强灾难条件下的系统韧性。

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [40] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要大量模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习调度策略的离线方法

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择，支持从多样化数据源学习

Result: CDQAC显著优于原始数据生成启发式算法，超越最先进的离线和在线RL基线，仅需10-20个训练实例即可学习高质量策略

Conclusion: CDQAC是高效的离线RL调度方法，意外发现在随机启发式生成的数据上训练效果优于遗传算法和优先级调度规则生成的高质量数据

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [41] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 提出GraphCSVAE框架，通过深度学习、图表示和分类概率推理整合卫星时间序列数据和专家知识，用于建模灾害物理脆弱性，并在孟加拉国和塞拉利昂的灾害案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有灾害风险评估方法在物理脆弱性建模方面进展有限，限制了决策者对联合国仙台框架执行进展的评估能力。需要开发新的数据驱动方法来填补这一空白。

Method: GraphCSVAE（图分类结构化变分自编码器）框架，结合深度学习、图表示学习和分类概率推理，使用时间序列卫星数据和专家先验知识，引入弱监督一阶转移矩阵来反映物理脆弱性的时空分布变化。

Result: 在两个灾害频发且社会经济弱势地区（孟加拉国沿海社区和塞拉利昂弗里敦市）成功揭示了灾后物理脆弱性的区域动态变化，为本地化时空审计和可持续减灾策略提供了宝贵见解。

Conclusion: GraphCSVAE框架能够有效建模物理脆弱性，为灾害风险管理提供了新的数据驱动方法，有助于实现更精准的灾后风险评估和减灾策略制定。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [42] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 提出一种基于ARIMA模型启发的简单卷积模块ARMA，用于长期时间序列预测，包含趋势捕捉和局部变化细化的双卷积组件，可直接进行多步预测，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型需要迭代多步预测且难以扩展到多变量设置，需要一种简单有效的直接多步预测方法。

Method: 设计双卷积组件模块：一个卷积组件捕捉趋势（自回归），另一个卷积组件细化局部变化（移动平均），可直接进行多步预测。

Result: 在9个基准数据集上实验表明，ARMA方法达到竞争性精度，特别是在具有强趋势变化的数据集上表现优异，同时保持架构简单性。

Conclusion: 该模块不仅实现了有效的长期时间序列预测，还天然编码了绝对位置信息，有潜力作为序列模型中位置嵌入的轻量级替代方案。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [43] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一个基于结构保持数字孪生的自适应源定位机器学习框架，结合条件神经Whitney形式和变压器算子学习，实现实时轨迹规划和数据同化。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂流体输运系统中源定位的挑战，需要开发能够保持物理约束、适应实时传感器数据并保证数值稳定性的方法。

Method: 使用条件神经Whitney形式(CNWF)构建数字孪生，结合有限元外微积分(FEEC)的数值保证和基于变压器的算子学习。采用交错方案交替评估数字孪生和应用Lloyd算法指导传感器布置。

Result: 该方法在复杂几何形状中相比物理无关的变压器架构显示出更高的准确性，证明了结构保持为源识别提供了有效的归纳偏置。

Conclusion: 结构保持的数字孪生框架能够有效实现源定位，物理约束的强制执行提高了在复杂环境中的定位精度，正则性为定位提供了充分条件。

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [44] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 提出了一个统一框架来形式化数据集压缩问题，使用差异度量来量化概率分布之间的距离，将目标从泛化扩展到鲁棒性、隐私等更多属性


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法缺乏统一的理论框架，且主要关注泛化性能，需要扩展到更广泛的目标如鲁棒性和隐私保护

Method: 建立基于差异度量的统一框架，使用概率分布距离概念来形式化数据集压缩问题，涵盖现有方法并扩展目标范围

Result: 提出了一个能够统一现有数据集压缩方法的理论框架，将任务扩展到包括鲁棒性、隐私保护等更广泛的目标

Conclusion: 该框架为数据集压缩提供了更严格的形式化定义，扩展了应用范围，为未来研究提供了理论基础

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [45] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: CAPE模型通过对比学习在520万份心电图数据上预训练，发现预训练队列的人口统计学和健康状况分布影响下游任务性能，提出IDB策略提升跨群体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习在自监督预训练中对队列组成的依赖性，特别是不同人群特征如何影响心电图基础模型的下游性能表现。

Method: 使用Contrasting by Patient Augmented Electrocardiograms (CAPE)基础模型，在四大洲五个队列（n=5,203,352）上进行预训练，系统评估队列人口统计学、健康状况和多样性对下游预测任务的影响，并提出In-Distribution Batch (IDB)策略。

Result: 发现下游性能取决于预训练队列的分布特性，包括人口统计学和健康状况；多中心多样化队列预训练虽然提高分布内准确性，但会编码队列特异性伪影，降低跨群体泛化能力；IDB策略能保持队列内一致性并增强跨群体鲁棒性。

Conclusion: 这项工作为开发临床公平和可泛化的基础模型提供了重要见解，强调了预训练队列组成对模型性能的关键影响，并提出有效解决方案来提升跨群体泛化能力。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [46] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文提出了无限维希尔伯特空间中整流流的严格函数化表述，建立了基于连续性方程叠加原理的理论框架，并扩展到函数流匹配和概率流ODE，实验证明优于现有函数生成模型。


<details>
  <summary>Details</summary>
Motivation: 虽然许多生成模型已在有限维欧几里得空间中发展出无限维推广，但整流流在无限维空间的扩展仍未被探索。现有函数流匹配理论存在限制性测度论假设，需要更一般的理论框架。

Method: 基于无限维空间中连续性方程的叠加原理，建立整流流的严格函数化表述，并将其自然扩展到函数流匹配和函数概率流ODE，作为整流流的非线性推广。

Result: 实验证明该方法相比现有函数生成模型具有更优越的性能，同时移除了现有理论中的限制性测度论假设。

Conclusion: 成功建立了无限维希尔伯特空间中整流流的理论框架，提供了函数流匹配的更一般化表述，并在实验中验证了其有效性，为函数生成模型的发展提供了新的理论基础。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [47] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出Vendi信息增益(VIG)主动学习策略，通过考虑数据集整体预测不确定性来选择最具信息量和多样性的图像进行标注，在Snapshot Serengeti数据集上仅用不到10%的标签就达到了接近全监督的预测精度。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱生物多样性监测中，物种识别因标注资源有限成为主要瓶颈。传统主动学习方法只关注个体预测不确定性，忽略了整个数据集的不确定性。

Method: 引入Vendi信息增益(VIG)主动学习策略，选择对数据集整体预测不确定性影响最大的图像，同时捕获信息量和多样性。

Result: 在Snapshot Serengeti数据集上，VIG使用不到10%的标签就达到了接近全监督的预测精度，在各项指标和批次大小上均优于标准基线方法，并在特征空间中收集了更多样化的数据。

Conclusion: VIG方法在数据有限的环境中具有广泛适用性，对生物多样性监测具有重要价值，其设计理念可扩展到生态学之外的领域。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [48] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO是一种基于掩码扩散大语言模型（dLLM）修复能力的强化学习框架，通过插入部分真实推理轨迹来引导探索，提高样本效率，在数学推理任务上取得了SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决传统RL方法在LLM对齐中的探索挑战：稀疏奖励信号和样本浪费问题，利用dLLM独特的修复能力来指导探索

Method: 提出IGPO框架，在在线采样时策略性地插入部分真实推理轨迹，结合监督微调使用合成重写的简洁轨迹，并采用基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得了显著提升，为全注意力掩码dLLM实现了新的最先进结果

Conclusion: IGPO成功地将dLLM的修复能力与RL算法设计相结合，有效解决了探索效率问题，为dLLM的强化学习对齐提供了新思路

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [49] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe是一种高效的注意力机制近似方法，结合语义聚类和多极展开，通过分层两阶段注意力机制降低Transformer的二次计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列处理中的二次计算复杂度问题，通过聚类和多极展开来近似标准注意力机制，同时保持性能。

Method: 在查询和键的表示空间中分别进行聚类，使用分层两阶段注意力机制，包括基于质心的单极近似和捕捉方向方差的偶极修正。

Result: 在8k上下文长度下比CUDNN Flash Attention快3倍，相对平方误差低于20%；在16k上下文的30M参数模型预训练中实现12.2%的运行时间减少，仅损失0.36%的性能。

Conclusion: 多极近似方法为高效Transformer预训练提供了可行方案，能够在保持性能的同时显著降低计算复杂度。

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [50] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 使用过程挖掘和机器学习进行铁路系统运行时控制流异常检测与定位


<details>
  <summary>Details</summary>
Motivation: 随着铁路系统复杂性和关键性增加，需要增强系统韧性来应对设计时未知的残余故障、系统环境变化和新兴网络威胁

Method: 采用过程挖掘从执行轨迹中学习系统实际控制流，进行在线一致性检查的运行时监控，并通过无监督机器学习进行异常定位

Result: 在ERTMS/ETCS L2的RBC/RBC切换参考场景中测试，显示出高精度、高效性和可解释性的异常检测与定位能力

Conclusion: 过程挖掘结合机器学习的方法能有效提升铁路控制系统的运行时异常检测和定位能力，增强系统韧性

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [51] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究了Local SGD中外层优化器的作用，证明了新的收敛保证，发现调整外层学习率可以平衡优化误差和梯度噪声方差，并弥补内层学习率的不良调整。理论表明外层学习率有时应大于1，并扩展到动量、加速优化器的情况。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式机器学习中，通信成为主要瓶颈。Local SGD虽然能减少通信开销，但关于外层优化器及其超参数选择的研究相对较少，需要深入理解外层优化器的作用机制。

Method: 通过理论分析证明Local SGD的收敛性保证，研究外层学习率对优化误差和梯度噪声方差的影响，扩展到动量优化器和加速优化器，并进行数据依赖性分析。通过标准语言模型实验验证理论。

Result: 理论表明调整外层学习率可以在优化误差和梯度噪声方差之间进行权衡，并能补偿内层学习率的不良调整。外层学习率有时应设为大于1的值。加速优化器能改善通信轮数相关的收敛速率。

Conclusion: 外层优化器在Local SGD中扮演关键角色，适当调整外层学习率（包括可能大于1的设置）和使用加速技术能显著提升分布式训练的效率和性能，为实际应用提供了重要指导。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [52] [Towards An Approach to Identify Divergences in Hardware Designs for HPC Workloads](https://arxiv.org/abs/2509.09774)
*Doru Thom Popovici,Mario Vega,Angelos Ioannou,Fabien Chaix,Dania Mosuli,Blair Reasoner,Tan Nguyen,Xiaokun Yang,John Shalf*

Main category: cs.AR

TL;DR: 提出了一种分层分解数学核到基本原语的方法，在不同编程环境中实现这些原语并组装算法，通过自动分析频率和资源使用来比较设计效率


<details>
  <summary>Details</summary>
Motivation: 传统硬件加速器开发需要低层Verilog编程和大量手动优化，虽然出现了高级设计工具，但生成的硬件可能不如专家设计优化，需要理解效率差距的来源

Method: 将数学核（傅里叶变换、矩阵乘法、QR分解等）分层分解为通用构建块/原语，在不同编程环境中实现原语并组装算法，采用自动方法分析可达到的频率和所需资源

Result: 通过在各个层级进行实验，提供了设计间更公平的比较，并为工具开发者和硬件设计师提供了改进实践的指导

Conclusion: 该方法有助于识别不同编程环境中的效率差距，为硬件设计工具和方法的改进提供了有价值的见解

Abstract: Developing efficient hardware accelerators for mathematical kernels used in
scientific applications and machine learning has traditionally been a
labor-intensive task. These accelerators typically require low-level
programming in Verilog or other hardware description languages, along with
significant manual optimization effort. Recently, to alleviate this challenge,
high-level hardware design tools like Chisel and High-Level Synthesis have
emerged. However, as with any compiler, some of the generated hardware may be
suboptimal compared to expert-crafted designs. Understanding where these
inefficiencies arise is crucial, as it provides valuable insights for both
users and tool developers. In this paper, we propose a methodology to
hierarchically decompose mathematical kernels - such as Fourier transforms,
matrix multiplication, and QR factorization - into a set of common building
blocks or primitives. Then the primitives are implemented in the different
programming environments, and the larger algorithms get assembled. Furthermore,
we employ an automatic approach to investigate the achievable frequency and
required resources. Performing this experimentation at each level will provide
fairer comparisons between designs and offer guidance for both tool developers
and hardware designers to adopt better practices.

</details>


### [53] [Finesse: An Agile Design Framework for Pairing-based Cryptography via Software/Hardware Co-Design](https://arxiv.org/abs/2509.10051)
*Tianwei Pan,Tianao Dai,Jianlei Yang,Hongbin Jing,Yang Su,Zeyu Hao,Xiaotao Jia,Chunming Hu,Weisheng Zhao*

Main category: cs.AR

TL;DR: Finesse是一个基于协同设计方法的敏捷设计框架，通过专用编译器和多粒度硬件模拟器实现性能优化和设计空间探索，显著缩短设计周期并提高灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统配对密码学加速器设计方法面临设计周期长、性能与灵活性难以平衡、架构探索支持不足等挑战，需要新的设计框架来解决这些问题。

Method: 采用协同设计方法，利用专用编译器和多粒度硬件模拟器驱动的协同优化循环，采用模块化设计流程和通用抽象来确保跨曲线族和硬件架构的灵活性。

Result: 编译时间缩短至分钟级，在流行曲线上实现了34倍吞吐量提升和6.2倍面积效率提升，相比非灵活ASIC设计也有3倍吞吐量和3.2倍面积效率优势。

Conclusion: Finesse框架成功解决了配对密码学加速器设计中的关键挑战，提供了灵活性、高效性和快速原型设计能力，显著优于现有设计方法。

Abstract: Pairing-based cryptography (PBC) is crucial in modern cryptographic
applications. With the rapid advancement of adversarial research and the
growing diversity of application requirements, PBC accelerators need regular
updates in algorithms, parameter configurations, and hardware design. However,
traditional design methodologies face significant challenges, including
prolonged design cycles, difficulties in balancing performance and flexibility,
and insufficient support for potential architectural exploration.
  To address these challenges, we introduce Finesse, an agile design framework
based on co-design methodology. Finesse leverages a co-optimization cycle
driven by a specialized compiler and a multi-granularity hardware simulator,
enabling both optimized performance metrics and effective design space
exploration. Furthermore, Finesse adopts a modular design flow to significantly
shorten design cycles, while its versatile abstraction ensures flexibility
across various curve families and hardware architectures.
  Finesse offers flexibility, efficiency, and rapid prototyping, comparing with
previous frameworks. With compilation times reduced to minutes, Finesse enables
faster iteration cycles and streamlined hardware-software co-design.
Experiments on popular curves demonstrate its effectiveness, achieving
$34\times$ improvement in throughput and $6.2\times$ increase in area
efficiency compared to previous flexible frameworks, while outperforming
state-of-the-art non-flexible ASIC designs with a $3\times$ gain in throughput
and $3.2\times$ improvement in area efficiency.

</details>


### [54] [MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging Bit-Slice-enabled Sparsity and Repetitiveness](https://arxiv.org/abs/2509.10372)
*Huizheng Wang,Zichuan Wang,Zhiheng Yue,Yousheng Long,Taiquan Wei,Jianxun Yang,Yang Wang,Chao Li,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: MCBP是一种基于比特粒度的算法-硬件协同设计，通过利用比特切片的重现性和稀疏性来加速大语言模型推理，在计算和内存效率方面实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时面临GEMM操作、权重访问和KV缓存访问的低效问题，现有Transformer加速器难以同时优化计算和内存效率，需要一种新的协同设计方法。

Method: 提出三种关键技术：1) BS重现性计算减少(BRCR) - 通过比特切片向量间的冗余消除GEMM计算；2) BS稀疏性双态编码(BSTC) - 利用高阶比特切片权重中的稀疏性减少权重访问；3) 比特粒度渐进预测(BGPP) - 基于提前终止的比特粒度预测减少KV缓存访问。

Result: 在26个基准测试中，MCBP相比Nvidia A100 GPU实现了9.43倍加速和31.1倍能效提升，相比SOTA Transformer加速器Spatten、FACT和SOFA分别实现了35倍、5.2倍和3.2倍的能耗节省。

Conclusion: MCBP通过比特粒度的计算-内存协同优化设计，有效解决了LLM推理中的计算和内存瓶颈问题，为实时场景下的高效推理提供了可行方案。

Abstract: Large language models (LLMs) face significant inference latency due to
inefficiencies in GEMM operations, weight access, and KV cache access,
especially in real-time scenarios. This highlights the need for a versatile
compute-memory efficient accelerator. Unfortunately, existing Transformer
accelerators struggle to address both aspects simultaneously, as they focus on
value-level processing, missing fine-grained opportunities to optimize
computation and memory collaboratively. This paper introduces MCBP, a
bit-grained compute-memory efficient algorithm-hardware co-design that
leverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM
inference. MCBP features three key innovations: 1) BS-repetitiveness-enabled
computation reduction (BRCR), which eliminates redundant GEMM computations via
leveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state
coding (BSTC), which reduces weight access via exploiting significant sparsity
in high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),
which reduces KV cache access by leveraging early-termination-based bit-grained
prediction. These techniques, supported by custom accelerator designs,
effectively alleviate the burden in GEMM, weight access, and KV cache access.
Extensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up
and 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA
Transformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than
Spatten, FACT and SOFA, respectively.

</details>


### [55] [TurboFuzz: FPGA Accelerated Hardware Fuzzing for Processor Agile Verification](https://arxiv.org/abs/2509.10400)
*Yang Zhong,Haoran Wu,Xueqi Li,Sa Wang,David Boland,Yungang Bao,Kan Shi*

Main category: cs.AR

TL;DR: TurboFuzz是一个端到端的硬件加速验证框架，将整个测试生成-仿真-覆盖率反馈循环实现在单个FPGA上，相比软件模糊测试器在相同时间内获得2.23倍覆盖率，检测真实问题时性能提升571倍。


<details>
  <summary>Details</summary>
Motivation: 处理器设计复杂性增加和RISC-V等新ISA的出现需要更敏捷高效的验证方法，现有仿真方法性能差、测试质量不足，硬件加速方案存在通信开销大、测试模式生成效率低等问题。

Method: 在单个FPGA上实现端到端验证框架，采用优化的测试用例控制流、种子间高效调度和混合模糊器集成，使用反馈驱动生成机制加速覆盖率收敛。

Result: 在相同时间预算下比软件模糊测试器多获得2.23倍覆盖率，检测真实问题时性能提升571倍，同时保持完全可见性和调试能力，面积开销适中。

Conclusion: TurboFuzz提供了一个高效的硬件加速验证解决方案，显著提升了处理器验证的覆盖率和性能，同时保持了良好的调试能力。

Abstract: Verification is a critical process for ensuring the correctness of modern
processors. The increasing complexity of processor designs and the emergence of
new instruction set architectures (ISAs) like RISC-V have created demands for
more agile and efficient verification methodologies, particularly regarding
verification efficiency and faster coverage convergence. While simulation-based
approaches now attempt to incorporate advanced software testing techniques such
as fuzzing to improve coverage, they face significant limitations when applied
to processor verification, notably poor performance and inadequate test case
quality. Hardware-accelerated solutions using FPGA or ASIC platforms have tried
to address these issues, yet they struggle with challenges including host-FPGA
communication overhead, inefficient test pattern generation, and suboptimal
implementation of the entire multi-step verification process.
  In this paper, we present TurboFuzz, an end-to-end hardware-accelerated
verification framework that implements the entire Test
Generation-Simulation-Coverage Feedback loop on a single FPGA for modern
processor verification. TurboFuzz enhances test quality through optimized test
case (seed) control flow, efficient inter-seed scheduling, and hybrid fuzzer
integration, thereby improving coverage and execution efficiency. Additionally,
it employs a feedback-driven generation mechanism to accelerate coverage
convergence. Experimental results show that TurboFuzz achieves up to 2.23x more
coverage collection than software-based fuzzers within the same time budget,
and up to 571x performance speedup when detecting real-world issues, while
maintaining full visibility and debugging capabilities with moderate area
overhead.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [56] [Setchain Algorithms for Blockchain Scalability](https://arxiv.org/abs/2509.09795)
*Arivarasan Karmegam,Gabina Luz Bianchi,Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,César Sánchez*

Main category: cs.DC

TL;DR: Setchain通过放宽交易的严格全序要求来提高区块链可扩展性，提出了三种基于底层区块链的分类算法：Vanilla、Compresschain和Hashchain，实现了比底层区块链高几个数量级的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统区块链的严格全序交易要求限制了其可扩展性，Setchain旨在通过将交易组织成无序集合（epochs）来提升性能。

Method: 提出了三种Setchain算法：1）Vanilla作为基础参考实现；2）Compresschain将元素批量压缩后作为epochs追加；3）Hashchain将批次转换为固定长度哈希，并通过分布式服务获取批次内容。所有算法都维护epoch-proofs证明机制。

Result: 在4、7、10个服务器的集群配置下进行性能评估，Setchain算法达到比底层区块链高几个数量级的吞吐量，最终性延迟低于4秒。

Conclusion: Setchain通过放松交易排序要求有效提升了区块链的可扩展性，三种算法均表现出显著的性能改进，为高吞吐量区块链应用提供了可行方案。

Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the
strict total order requirement among transactions. Setchain organizes elements
into a sequence of sets, referred to as epochs, so that elements within each
epoch are unordered. In this paper, we propose and evaluate three distinct
Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is
a basic implementation that serves as a reference point. Compresschain
aggregates elements into batches, and compresses these batches before appending
them as epochs in the ledger. Hashchain converts batches into fixed-length
hashes which are appended as epochs in the ledger. This requires Hashchain to
use a distributed service to obtain the batch contents from its hash. To allow
light clients to safely interact with only one server, the proposed algorithms
maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the
hash of the epoch, cryptographically signed by a server. A client can verify
the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum
number of Byzantine servers assumed). All three Setchain algorithms are
implemented on top of the CometBFT blockchain application platform. We
conducted performance evaluations across various configurations, using clusters
of four, seven, and ten servers. Our results show that the Setchain algorithms
reach orders of magnitude higher throughput than the underlying blockchain, and
achieve finality with latency below 4 seconds.

</details>


### [57] [Ordered Consensus with Equal Opportunity](https://arxiv.org/abs/2509.09868)
*Yunhao Zhang,Haobin Ni,Soumya Basu,Shir Cohen,Maofan Yin,Lorenzo Alvisi,Robbert van Renesse,Qi Chen,Lidong Zhou*

Main category: cs.DC

TL;DR: 本文扩展了状态机复制(SMR)的有序共识，引入公平机会概念来防止排序操纵攻击，提出了基于可信硬件和阈值可验证随机函数的秘密随机预言机(SRO)设计，并在Bercow协议中实现可配置的公平性保障。


<details>
  <summary>Details</summary>
Motivation: 在基于SMR的区块链中，不同的命令排序会给客户端带来不同的财务奖励，现有有序共识主要关注拜占庭节点的影响，但现实中的排序操纵可能由网络优势等非拜占庭因素导致，需要引入公平性保障。

Method: 提出平等机会的公平性概念，利用随机性控制偏见，设计了两种秘密随机预言机(SRO)：基于可信硬件和基于阈值可验证随机函数，并在Bercow有序共识协议中实现可配置的公平性近似。

Result: 开发了Bercow协议，能够通过近似平等机会来有效缓解SMR区块链中已知的排序攻击，提供了可配置的公平性保障因子。

Conclusion: 通过将社会科学的平等机会概念引入有序共识，并利用随机性技术，可以有效解决区块链中的排序公平性问题，Bercow协议为SMR系统提供了实用的公平性解决方案。

Abstract: The specification of state machine replication (SMR) has no requirement on
the final total order of commands. In blockchains based on SMR, however, order
matters, since different orders could provide their clients with different
financial rewards. Ordered consensus augments the specification of SMR to
include specific guarantees on such order, with a focus on limiting the
influence of Byzantine nodes. Real-world ordering manipulations, however, can
and do happen even without Byzantine replicas, typically because of factors,
such as faster networks or closer proximity to the blockchain infrastructure,
that give some clients an unfair advantage. To address this challenge, this
paper proceeds to extend ordered consensus by requiring it to also support
equal opportunity, a concrete notion of fairness, widely adopted in social
sciences. Informally, equal opportunity requires that two candidates who,
according to a set of criteria deemed to be relevant, are equally qualified for
a position (in our case, a specific slot in the SMR total order), should have
an equal chance of landing it. We show how randomness can be leveraged to keep
bias in check, and, to this end, introduce the secret random oracle (SRO), a
system component that generates randomness in a fault-tolerant manner. We
describe two SRO designs based, respectively, on trusted hardware and threshold
verifiable random functions, and instantiate them in Bercow, a new ordered
consensus protocol that, by approximating equal opportunity up to within a
configurable factor, can effectively mitigate well-known ordering attacks in
SMR-based blockchains.

</details>


### [58] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: 本文对大规模多GPU系统中LLM训练进行了全面性能分析，发现硬件扩展并非性能唯一决定因素，不同并行策略和配置对硬件利用率、功耗和热行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模快速增长，训练工作负载已远超单节点分析能力，需要深入理解这些模型在大规模多GPU系统中的行为特征。

Method: 使用NVIDIA H100/H200和AMD MI250 GPU平台，分析密集和稀疏模型在不同并行策略（张量、流水线、数据和专家并行）下的性能，评估硬件利用率、功耗和热行为，并测试激活重计算和计算-通信重叠等优化效果。

Result: 研究发现：1）扩展硬件容量不是性能唯一决定因素；2）在通信受限场景中，较少但内存更大的scale-up系统可能优于scale-out系统；3）某些并行组合（如张量与流水线并行）会导致带宽利用不足；4）过大的微批次会导致突发执行和峰值功耗问题，加剧热节流。

Conclusion: 训练性能由硬件、系统拓扑和模型执行之间的复杂交互决定，需要精心调优配置。研究为未来LLM系统的可扩展性和可靠性提供了系统与硬件设计建议。

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [59] [eHashPipe: Lightweight Top-K and Per-PID Resource Monitoring with eBPF](https://arxiv.org/abs/2509.09879)
*Yuanjun Dai,Qingzhe Guo,Xiangren Wang*

Main category: cs.PF

TL;DR: eHashPipe是一个基于eBPF和HashPipe算法的轻量级实时资源监控系统，支持Top-k监控和特定PID跟踪，在CPU和内存监控方面具有高精度和低开销。


<details>
  <summary>Details</summary>
Motivation: 系统级资源监控需要同时具备精确性和效率，传统用户空间轮询工具存在延迟和上下文切换开销问题，需要一种在操作系统内核中运行的低开销实时监控方案。

Method: 结合eBPF技术和HashPipe草图算法，在内核中实现两个eBPF管道来监控CPU时间和内存使用情况，支持Top-k监控模式和特定PID跟踪模式。

Result: 在实验中，eHashPipe在k=1、5、10时达到100%的Top-k精度，k=20时为95.0/90.0%，k=30时为93.3/83.3%，时间分辨率比top工具精细约14倍，且开销极低。

Conclusion: eHashPipe能够以最小的影响提供准确、响应迅速的监控洞察，非常适合延迟敏感的云和边缘计算环境。

Abstract: System-level resource monitoring with both precision and efficiency is a
continuous challenge. We introduce eHashPipe, a lightweight, real-time resource
observability system utilizing eBPF and the HashPipe sketching algorithm.
eHashPipe supports two tracking modes: Top-k monitoring to identify the most
resource-demanding processes and specific PID tracking to detail the behavior
of selected processes. We implement two in-kernel eBPF pipelines for on-CPU
time and memory usage. Unlike traditional userspace polling tools, eHashPipe
operates in the kernel to reduce latency and context-switch overhead while
keeping the runtime footprint small. During our experiments, eHashPipe attains
100 percent Top-k precision for CPU and memory at k = 1, 5, and 10, 95.0/90.0
percent at k = 20, and 93.3/83.3 percent at k = 30 compared to the ground
truth. It exposes short-lived bursts with about 14 times finer temporal
resolution than top while imposing very low overhead. These results show that
eHashPipe delivers accurate, responsive insight with minimal impact, making it
well suited for latency-sensitive cloud and edge environments.

</details>
