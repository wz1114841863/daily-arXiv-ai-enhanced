<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 88]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 提出结合CXL.mem技术的MPI通信性能评估工具链和扩展性能模型，用于预测CXL.mem在数据交换中的潜在性能优势


<details>
  <summary>Details</summary>
Motivation: CXL.mem技术可实现多节点共享内存池，为高效节点间通信提供新可能，但需要评估其在实际MPI应用中的性能收益

Method: 扩展内存追踪采样工具Mitos，分析MPI应用的数据访问模式（节点内MPI缓冲区访问和跨节点MPI流量），构建细粒度性能模型预测CXL.mem的优化潜力

Result: 在2D热传导小型应用和HPCG基准测试上验证模型有效性，证明工具链能识别可从CXL.mem实现中受益的MPI调用

Conclusion: 提出的工具链和性能模型能够有效评估CXL.mem在MPI通信中的优化潜力，支持针对性的性能优化

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [2] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS) 是首个在POSIX兼容框架内集成多凭证功能的文件系统，基于边缘计算中的全局数据平面构建，使用DataCapsule作为存储提供者。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算环境中，需要一种能够支持多凭证访问控制的文件系统，以提供安全、灵活的共享数据访问机制，同时保持POSIX兼容性。

Method: CFS采用三层架构：1) DataCapsule服务器负责边缘存储和复制；2) 可信执行环境中的中间件管理写权限；3) POSIX兼容的客户端文件系统，支持多种架构。

Result: 实验评估显示CFS读写性能相对适中，但功能正确性高，适合实际软件开发应用场景。

Conclusion: CFS作为首个多凭证POSIX文件系统，在边缘计算中具有实用价值，未来可通过增强性能进一步提升其实用性。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [3] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: Chopper是一个用于分析和可视化多GPU LLM训练性能的框架，首次在AMD MI300X GPU上对Llama 3 8B训练进行全面端到端分析，发现频率管理是性能差距的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注内核级性能或单GPU微基准测试，缺乏对多GPU LLM训练中通信、计算、内存行为和电源管理之间复杂交互的系统性理解。

Method: 开发Chopper框架，收集、对齐和可视化多粒度GPU内核跟踪和硬件性能计数器，对八GPU AMD MI300X节点上的Llama 3 8B FSDP训练进行全面端到端分析。

Result: 发现内存确定性能够实现更高、更稳定的GPU和内存频率；频率开销（DVFS效应）是理论性能与实际性能差距的最大贡献者，超过MFMA利用率损失、通信/计算重叠和内核启动开销的影响。

Conclusion: Chopper首次在AMD MI300X GPU上提供了LLM训练的全方位多粒度分析，为优化训练框架、改进电源管理策略以及指导未来GPU架构和系统设计提供了可操作的见解。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [4] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: HERMES是一个新型框架，旨在解决云计算到边缘计算连续体中的可扩展性、互操作性和信任问题，通过资源货币化、编排和语义互操作性来转变连接性和数据利用。


<details>
  <summary>Details</summary>
Motivation: 工业需求推动超分布式应用从云端扩展到边缘，但现有解决方案在可扩展性、互操作性和信任方面存在固有局限性，无法满足智能制造、交通和农业等领域的需求。

Method: 提出HERMES框架，建立开放、无缝、安全的环境，实现从云服务器到小型边缘设备的智能编排，通过分布式市场实现数据和服务的货币化，并通过语义互操作性共享知识。

Result: HERMES通过整合资源货币化、编排和语义互操作性这三个关键方面，为新一代分布式应用奠定了基础，使应用更加高效、可信和自主。

Conclusion: HERMES框架为解决计算连续体中的关键挑战提供了创新解决方案，为更高效、可信和自主的分布式应用开辟了新途径。

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [5] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: 提出基于Ozaki-II方案的高性能复数矩阵乘法仿真方法，在INT8矩阵引擎上实现单/双精度复数矩阵乘法，相比cuBLAS原生实现获得4-6.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中低精度矩阵乘法单元比高精度单元具有更高吞吐量，因此利用低精度硬件仿真高精度矩阵乘法成为高性能计算领域的重要研究方向。

Method: 基于Ozaki-II方案，在INT8矩阵引擎上开发单精度和双精度复数矩阵乘法的高性能仿真方法。

Result: 在NVIDIA B200 GPU上，相比cuBLAS原生单/双精度复数矩阵乘法，分别获得4.0-5.6倍和4.4-6.5倍加速；可在精度与速度之间灵活权衡。

Conclusion: 该方法具有成为广泛应用默认算法的潜力，能够在精度要求范围内显著提升复数矩阵乘法性能。

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [6] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: 提出了一种名为Magneton的差分能量调试方法，用于检测和诊断机器学习系统中由软件设计不良导致的能量浪费问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的训练和部署能耗巨大，现有优化主要关注硬件能效，但软件设计不良导致的能量浪费被忽视。这些浪费通常源于冗余或设计不佳的操作，消耗更多能量却不提升性能，而开发者缺乏检测和诊断这些问题的工具。

Method: 提出差分能量调试方法，基于竞争性ML系统实现相似功能但能耗差异巨大的观察。设计实现Magneton能量分析器，在算子级别比较相似ML系统的能耗，自动定位导致过度能耗的代码区域和配置选择。

Result: 应用于9个流行的ML系统（涵盖LLM推理、通用ML框架和图像生成），Magneton检测并诊断了16个已知的软件能量低效案例，并进一步发现了8个先前未知的案例，其中7个已得到开发者确认。

Conclusion: 软件能量浪费是ML系统能效的重要问题，差分能量调试方法能有效检测和诊断这些问题，Magneton工具在实际应用中证明了其有效性。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [7] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 论文研究了为轻量级线程（协程）设计互斥锁的问题，发现传统OS线程锁在轻量级线程环境下会导致死锁，提出了针对TTAS和MCS锁的修改方案，并建议使用cohort锁作为通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统多线程数据结构是为操作系统线程设计的，但轻量级线程（协程/异步调用）的锁实现研究不足。轻量级线程虽然启动和上下文切换开销低，但需要手动切换上下文才能实现并行，这给锁设计带来了新的挑战。

Method: 提出了针对轻量级线程的TTAS和MCS锁修改方案，重点关注yielding和sleeping两种上下文切换机制。同时建议使用cohort锁，它结合了MCS队列和TTAS锁的优点，能在不同轻量级线程库中表现良好。

Result: 研究表明传统OS线程锁在轻量级线程环境下无法有效工作，会导致死锁。TTAS和MCS锁的性能在不同设置下差异显著，而cohort锁能在各种轻量级线程库中提供平衡的性能表现。

Conclusion: 轻量级线程需要专门的锁实现，不能直接使用传统OS线程锁。yielding和sleeping机制对锁设计至关重要，cohort锁是适用于各种轻量级线程库的通用解决方案。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [8] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 提出一种基于有限状态自动机的模型测试方法，为分布式系统生成全覆盖测试套件，无需修改代码或干扰执行环境


<details>
  <summary>Details</summary>
Motivation: 分布式系统设计与实现存在挑战，虽然通常有形式化规范并通过模型检查验证，但实现与规范之间仍存在差距，无法保证实现无bug

Method: 使用基于模型的测试方法，将系统模型解释为有限状态自动机，为actor模型编写的分布式系统生成覆盖所有可能状态和转换的测试套件，无需修改代码或干扰分布式系统执行环境

Result: 以基于Viewstamped Replication的复制算法为例，验证了真实系统中的实现

Conclusion: 该方法能有效弥合分布式系统实现与形式化规范之间的差距，提供全面的测试覆盖

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [9] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 通过时空转移云工作负载可显著降低碳、水和土地使用足迹，空间转移效果更显著（20-85%），时间转移提供额外增量效益，组合策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 研究时空云工作负载转移在减少碳足迹、水足迹和土地使用足迹方面的潜力，以应对云计算的环境影响问题。

Method: 使用真实世界数据（AWS和Azure云提供商数据）和工作负载轨迹（大数据分析和FaaS应用）进行仿真研究，分析空间转移和时间转移策略。

Result: 空间转移可大幅降低碳、水和土地使用足迹（20-85%），时间转移也有减少效果但较弱。组合策略效果最佳，主要依赖空间转移，时间调整提供额外增量效益。敏感性分析显示策略对电网混合数据预测误差和季节变化具有鲁棒性。

Conclusion: 时空云工作负载转移是减少云计算环境足迹的有效策略，空间转移是主要驱动力，时间转移提供补充效益，该策略在实际应用中具有鲁棒性。

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [Resonant and Stochastic Vibration in Neurorehabilitation](https://arxiv.org/abs/2512.08009)
*Ava Hays,Nolan Kosnic,Ryan Miller,Kunal Siddhawar*

Main category: cs.ET

TL;DR: 这篇综述论文探讨了振动干预在神经康复中的应用，重点分析了全身振动和局部肌肉振动两种模式，评估了它们在改善平衡、运动功能和神经可塑性方面的效果，并指出了当前研究的局限性和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 神经系统损伤和年龄相关的衰退会损害感觉处理和运动协调功能。随着神经可塑性机制的深入理解，基于振动的干预措施作为刺激感觉通路和运动回路以支持功能恢复的潜在工具受到关注。

Method: 本文采用文献综述方法，系统评估了随机振动和共振振动两种模式，描述了它们的机制、治疗原理和临床应用。特别关注了全身振动在改善老年人、中风幸存者和帕金森病患者平衡、活动能力和精细运动功能方面的证据，以及局部肌肉振动和可穿戴随机共振设备在上肢康复中的应用。

Result: 综述发现振动干预在改善神经功能方面显示出潜力，但存在参数优化、普适性和安全性方面的挑战。局部肌肉振动和可穿戴设备在临床应用中具有前景，但在可扩展性、生态效度和标准化方面存在局限。研究识别了影响治疗效果的关键变量，并强调了在改进协议、提高可用性和将振动技术整合到更广泛神经康复框架中的持续努力。

Conclusion: 振动干预在神经康复中具有重要潜力，但需要进一步研究来优化参数、提高标准化水平，并解决可扩展性和安全性问题。未来研究应致力于将振动干预转化为可靠且可部署的临床工具，重点关注协议细化、可用性改进和技术整合。

Abstract: Neurological injuries and age-related decline can impair sensory processing and disrupt motor coordination, gait, and balance. As mechanisms of neuroplasticity have become better understood, vibration-based interventions have gained attention as potential tools to stimulate sensory pathways and motor circuits to support functional recovery. This survey reviews stochastic and resonant vibration modalities, describing their mechanisms, therapeutic rationales, and clinical applications. We synthesize evidence on whole-body vibration for improving balance, mobility, and fine motor function in aging adults, stroke survivors, and individuals with Parkinson's disease, with attention to challenges in parameter optimization, generalizability, and safety. We also assess recent developments in focused muscle vibration and wearable stochastic resonance devices for upper-limb rehabilitation, evaluating their clinical promise along with limitations in scalability, ecological validity, and standardization. Across these modalities, we identify key variables that shape therapeutic outcomes and highlight ongoing efforts to refine protocols, improve usability, and integrate vibration techniques into broader neurorehabilitation frameworks. We conclude by outlining the most important research needs for translating vibration-based interventions into reliable and deployable clinical tools.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [11] [Multi-domain performance analysis with scores tailored to user preferences](https://arxiv.org/abs/2512.08715)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 论文提出了一种概率框架来分析跨领域性能评估，将性能视为概率测度，发现只有特定评分函数（如排序分数）的加权平均值才等于领域特定性能的加权算术平均值，并基于用户偏好定义了四种领域类型。


<details>
  <summary>Details</summary>
Motivation: 算法性能通常依赖于特定应用领域的案例分布。在多个领域进行评估后，计算加权平均性能并深入分析平均过程的内在机制具有重要价值，这有助于理解不同领域对整体性能的贡献。

Method: 采用概率框架，将性能视为概率测度（如分类任务中的归一化混淆矩阵）。证明加权平均对应于汇总化，只有特定评分函数（包括排序分数族）的汇总性能值才等于领域特定性能值的加权算术平均值。基于用户偏好定义了四种领域类型：最简单、最困难、主导和瓶颈领域。

Result: 建立了适用于各种任务的通用理论框架，并针对二分类任务开发了新的可视化工具。确定了用户偏好如何影响权重分配，以及不同领域类型如何影响整体性能评估。

Conclusion: 该研究为跨领域性能评估提供了严谨的理论基础，揭示了加权平均过程中的内在机制，并通过定义四种领域类型和开发可视化工具，帮助用户更好地理解和解释多领域评估结果。

Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: 该论文介绍了细菌生物威胁基准(B3)数据集的构建，这是生物威胁基准生成(BBG)框架的第二部分，通过三种方法生成了7,000多个基准候选，最终筛选出1,010个高质量基准用于评估AI模型的生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型(特别是大语言模型)可能被用于生物恐怖主义或获取生物武器，这引发了政策、学术界和公众的广泛担忧。为了量化和减轻这种风险，需要开发能够评估模型生物安全风险的基准测试工具。

Method: 采用三种互补方法生成基准：1)基于网络的提示生成，2)红队测试，3)挖掘现有基准语料库。生成了7,000多个潜在基准，通过去重、提升诊断性评估和质量控制，最终筛选出1,010个高质量基准，并与项目第一阶段开发的任务-查询架构相关联。

Result: 成功构建了包含1,010个最终基准的细菌生物威胁基准(B3)数据集。这些基准具有以下特点：a)在提供提升方面具有诊断性；b)直接与生物安全威胁相关；c)与更大的生物安全架构对齐，允许在不同分析层次进行细致分析。

Conclusion: 该研究开发了一个系统化的生物威胁基准生成框架，能够创建诊断性强、与生物安全威胁直接相关的高质量基准数据集，为评估AI模型的生物安全风险提供了重要工具，有助于模型开发者和政策制定者量化和减轻相关风险。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [13] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: B3数据集为评估大语言模型的生物安全风险提供了一个可行的基准测试方法，通过实际测试和人工评估识别风险来源并指导缓解措施。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器，这引发了政策、学术和公众的广泛担忧。模型开发者和政策制定者需要量化并缓解这种风险，因此需要开发能够评估特定模型生物安全风险的基准测试方法。

Method: 本文是生物威胁基准生成（BBG）框架系列论文的第三篇，介绍了B3（细菌生物威胁基准）数据集的试点实施。试点包括：1）在样本前沿AI模型上运行基准测试；2）人工评估模型响应；3）从多个维度对结果进行应用风险分析。

Result: 试点研究表明，B3数据集提供了一种可行且细致的方法，能够快速评估LLM的生物安全风险，识别风险的关键来源，并为缓解措施的优先领域提供指导。

Conclusion: B3基准测试框架为评估大语言模型的生物安全风险提供了有效的工具，能够帮助识别风险来源并指导风险缓解措施的优先级设置，对AI安全监管具有重要意义。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [14] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个自适应并行推理框架，在保持与流行序列推理模型相当精度的同时，显著降低推理延迟，在数学推理基准上达到1.53倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的序列解码导致高延迟，现有并行推理方法要么限于监督行为克隆，要么精度显著下降，且需要定制推理引擎，部署复杂。

Method: 1) 两阶段并行轨迹生成器产生大规模高质量带并行标注的CoT数据；2) 基于trie的训练-推理协同设计，可在现成自回归推理引擎上实现并行推理；3) 并行感知的强化学习框架，平衡精度与并行化效果。

Result: 在六个数学推理基准上，基于Qwen3-8B的ThreadWeaver达到与先进序列推理模型相当的精度（平均71.9%，AIME24上79.9%），同时实现最高1.53倍的平均token延迟加速。

Conclusion: ThreadWeaver在精度和效率之间建立了新的帕累托前沿，实现了与序列推理模型相当的精度同时显著降低推理延迟，且无需修改现有推理引擎。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [15] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: LayerPipe2 为神经网络的流水线训练提供了理论框架，通过延迟梯度适应和重定时技术，推导出各层所需的延迟量，并解决了历史权重存储问题。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的LayerPipe在卷积、全连接和脉冲神经网络的流水线训练中取得了经验性成功，但缺乏理论指导来确定各层需要引入多少梯度延迟才能实现期望的流水线效果。

Method: 使用变量延迟梯度适应和重定时技术，正式推导LayerPipe框架。识别合法插入延迟的位置，分析网络结构如何决定延迟需求。开发管道感知移动平均方法，重构历史状态而非显式存储，解决存储瓶颈。

Result: 建立了理论框架，能够预测延迟需求：内层需要较少延迟，外层需要较长延迟；每层流水线时延迟仅取决于下游阶段数量；分组流水线时组内各层共享相同延迟分配。同时解决了历史权重存储问题。

Conclusion: LayerPipe2提供了一个原则性框架，指导如何构建LayerPipe架构、预测延迟需求并减轻存储负担，从而实现可扩展的流水线训练，并控制通信计算权衡。

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [16] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: 该论文提出在长尾分布中解决特征空间与分类器权重空间错配问题的方法，通过三种可插拔的对齐策略提升现有长尾方法的性能，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 在长尾分布中，样本不平衡阻碍了神经坍缩现象的出现，导致泛化性能差。现有方法主要关注恢复ETF几何结构，但忽视了特征空间与分类器权重空间之间的严重错配问题。

Method: 首先通过最优错误指数分析从理论上量化错配的危害，然后提出三种显式的对齐策略：1) 特征-分类器对齐，2) 类内特征对齐，3) 跨类特征对齐。这些策略可以即插即用地集成到现有长尾方法中，无需改变网络架构。

Result: 在CIFAR-10-LT、CIFAR-100-LT和ImageNet-LT数据集上的大量实验表明，提出的对齐策略能持续提升现有基线方法的性能，并达到最先进的性能水平。

Conclusion: 特征与分类器权重空间的对齐是长尾学习中被忽视但至关重要的问题，提出的三种对齐策略能有效解决这一问题，显著提升模型在长尾分布下的泛化能力。

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [17] [CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics](https://arxiv.org/abs/2512.07847)
*Mohamed Elrefaie,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: CarBench是首个针对大规模3D汽车空气动力学的综合基准测试，在最大的公共汽车空气动力学数据集DrivAerNet++上评估了11种最先进模型，涵盖神经算子、几何深度学习、Transformer求解器和隐式场网络。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模CFD数据集为机器学习在空气动力学和工程设计中的应用提供了新机会，但工程设计中缺乏大规模数值模拟的标准化基准。现有计算机视觉和NLP领域的基准推动了算法创新，但工程领域缺乏类似的标准评估框架。

Method: 在DrivAerNet++数据集（包含8000多个高保真汽车模拟）上评估11种架构：神经算子方法（如傅里叶神经算子）、几何深度学习（PointNet、RegDGCNN、PointMAE、PointTransformer）、Transformer求解器（Transolver、Transolver++、AB-UPT）和隐式场网络（TripNet）。除了标准插值任务，还进行跨类别实验，将在单一汽车原型上训练的Transformer求解器评估于未见类别。

Result: 基准测试涵盖了预测准确性、物理一致性、计算效率和统计不确定性等多个维度。开源了基准框架，包括训练管道、基于自助重采样的不确定性估计例程和预训练模型权重。

Conclusion: CarBench为高保真CFD模拟的大规模学习建立了首个可复现的基础，旨在加速数据驱动工程领域的进展，填补了工程设计中大规模数值模拟标准化基准的空白。

Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.

</details>


### [18] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash：一个资源高效、可解释的小模型管道，用于纽约市机动车碰撞伤害严重程度预测，集成多表数据，使用树集成模型（XGBoost和随机森林）优于小型语言模型，SHAP分析揭示关键风险因素。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年发生超过10万起机动车碰撞事故，造成重大伤害和公共卫生负担。需要资源高效且可解释的预测模型来分析伤害严重程度，以支持城市规模的安全分析。

Method: 整合三个关联表（数千万条记录），构建统一特征模式的分区存储，训练紧凑的树集成模型（随机森林和XGBoost），与使用文本摘要提示的本地部署小型语言模型（SLMs）进行比较。采用特征工程、类别加权和SHAP可解释性分析。

Result: 在时间保留测试集上，XGBoost和随机森林分别达到0.7828和0.7794的准确率，明显优于SLMs（0.594和0.496）。类别不平衡分析显示简单类别加权可提高致命事故召回率，SHAP归因突出人类脆弱性因素、时间和位置是预测严重程度的主要驱动因素。

Conclusion: 可解释的小模型集成仍然是城市规模伤害分析的强大基线，而将表格预测器与SLM生成的叙述相结合的混合管道可以在不牺牲可扩展性的情况下改善沟通效果。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [19] [SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 论文研究发现LLM代理在长时程工具使用任务中，突变动作的偏差对失败影响最大，并提出CM框架通过突变门控验证、目标反思和上下文清理来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理发展迅速，但在长时程工具使用任务上的表现仍然脆弱。为了理解这种脆弱性，研究者探究是否所有动作对失败的影响相同，并分析突变动作与非突变动作对任务成功的影响差异。

Method: 1) 在τ-Bench和SWE-Bench数据集上分析执行轨迹，将动作分解为突变（改变环境）和非突变步骤；2) 形式化"决定性偏差"概念；3) 使用逻辑回归分析偏差影响；4) 提出CM框架：突变门控验证、目标反思、基于块的上下文清理；5) 创建τ-Bench Verified改进数据集。

Result: 研究发现：1) 每个突变动作的偏差使成功率降低92%（Airline）和96%（Retail）；2) 非突变动作偏差影响很小；3) 上下文长度增加导致错误增多；4) CM框架显著提升性能：Qwen3-Thinking在Airline上相对提升28%，Retail提升11%，SWE-Bench Verified提升7%；Claude也有类似提升。

Conclusion: 研究强调需要动作级分析、针对性安全措施和可靠评估作为构建鲁棒多轮代理的前提条件。CM框架通过聚焦突变动作的验证和反思有效提升代理性能，同时τ-Bench Verified数据集解决了原基准的局限性。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [20] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU内存使用量的框架，通过分解模型架构和分析训练行为来准确预测峰值内存使用，避免内存溢出错误


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型的规模和复杂度增加，GPU内存需求经常超过可用容量，导致内存溢出错误。现有研究仅关注单模态架构，无法推广到多模态模型，而多模态模型在智能体AI系统中很常见。

Method: 提出一个框架，通过分解多模态模型为组成层，应用因子化方法估计每层的内存使用量，分析模型架构和训练行为来预测峰值GPU内存使用

Result: 评估显示该框架实现了高预测准确率，平均MAPE约为8.7%

Conclusion: 该框架能有效预测多模态模型的GPU内存使用，帮助防止内存溢出错误，节省计算资源

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [21] [HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2512.07854)
*Yongyao Wang,Jingyuan Wang,Xie Yu,Jiahao Ji,Chao Li*

Main category: cs.LG

TL;DR: 提出HSTMixer框架，采用全MLP架构进行高效的大规模交通预测，通过分层时空混合块和多分辨率特征提取，结合自适应区域混合器动态捕捉不同区域的时空模式。


<details>
  <summary>Details</summary>
Motivation: 交通预测对现代城市管理很重要，现有模型在大规模场景下存在二次计算复杂度问题，不适用于真实世界的大规模交通网络。

Method: 提出HSTMixer框架：1) 使用全MLP架构；2) 分层时空混合块通过自底向上聚合和自顶向下传播提取多分辨率特征；3) 自适应区域混合器基于区域语义生成变换矩阵，动态捕捉不同区域的时空模式。

Result: 在四个大规模真实世界数据集上的实验表明，该方法不仅达到了最先进的性能，而且具有竞争力的计算效率。

Conclusion: HSTMixer是一个高效有效的大规模交通预测框架，解决了现有模型计算复杂度高的问题，在实际应用中具有实用价值。

Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.

</details>


### [22] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA提出了一种基于对数域注意力预测的算法-架构协同设计，通过消除昂贵乘法、减少累加开销等优化，实现了跨阶段稀疏Transformer的高效能加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在不同阶段的计算瓶颈随输入序列变化而动态变化，需要跨阶段稀疏加速策略。现有稀疏Transformer方法多为单阶段设计，其稀疏预测机制在多阶段应用中会产生显著功耗开销。

Method: 1) 设计非对称前导一计算(ALOC)方案消除昂贵乘法；2) 提出混合精度多轮移位累加(MRSA)机制减少累加开销；3) 设计数据特征依赖滤波器(DDF)与MRSA协同工作；4) 设计专用加速器将理论增强转化为实际硬件改进。

Result: LAPA相比当前最先进工作Spatten、Sanger和FACT，分别实现了3.52倍、3.24倍和2.79倍的能量效率提升。

Conclusion: LAPA通过算法-架构协同设计，有效解决了跨阶段稀疏Transformer的加速问题，显著提升了能量效率，为动态计算瓶颈的Transformer模型提供了实用的硬件加速解决方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [23] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: CLDD是一种基于图神经网络的疾病检测模型，通过利用疾病关联和患者相似性进行协同学习，无需依赖大量医学检测即可预测数千种疾病。


<details>
  <summary>Details</summary>
Motivation: 疾病检测通常需要大量医学测试且成本高昂，难以对每位患者进行所有可能的测试来诊断或预测数百上千种疾病。需要一种能够减少检测依赖、降低成本的疾病预测方法。

Method: 提出CLDD（疾病检测协同学习）模型，将疾病检测构建为协同学习任务，利用疾病间的关联和患者间的相似性。模型整合患者-疾病交互数据和人口统计特征，采用图神经网络架构自适应学习这些关系。

Result: 在MIMIC-IV数据集（61,191名患者，2,000种疾病）上的实验表明，CLDD在多个指标上优于基线方法，召回率提升6.33%，精确率提升7.63%。案例研究显示模型能够成功恢复被掩盖的疾病。

Conclusion: CLDD通过减少诊断成本和提升可及性，有望用于大规模疾病筛查和社会健康保障。模型具有可解释性和可靠性，能够有效预测多种疾病而不过度依赖医学检测。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [24] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，通过结构感知语义增强提升领域自适应表示能力，在节点和图分类任务中优于9个SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型(GFMs)在领域噪声、结构扰动和对抗攻击下的鲁棒性研究不足，关键限制在于对层次结构语义建模不充分，这影响了模型的泛化能力。

Method: 1) 通过将基于熵的编码树转换为结构感知文本提示来编码层次结构先验；2) 使用自监督信息瓶颈机制通过结构引导压缩蒸馏鲁棒可迁移表示；3) 引入专家自适应路由机制（混合专家架构+空专家设计）解决跨域适应中的负迁移；4) 提出微调模块通过联合社区内和社区间结构学习优化层次结构。

Result: 在广泛的实验中，SA^2GFM在节点和图分类任务中，针对随机噪声和对抗扰动的有效性和鲁棒性方面，优于9个最先进的基线方法。

Conclusion: SA^2GFM通过结构感知语义增强和自适应机制，显著提升了图基础模型在噪声和扰动环境下的鲁棒性和领域自适应能力。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [25] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM是一个轻量级的频率感知交互式Mamba模型，用于时间序列分类任务。它通过自适应滤波块提取频域特征，使用交互式Mamba块进行多粒度信息交互，并采用自监督预训练机制，在多个基准测试中超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在环境监测、医疗诊断等应用中很重要，但现有深度学习模型存在计算成本高、对噪声敏感、在小数据集上容易过拟合等问题。需要一种更轻量、鲁棒且高效的时间序列分类方法。

Method: 1. 自适应滤波块(AFB)：利用傅里叶变换提取频域特征，包含可学习的自适应阈值动态抑制噪声，通过全局和局部语义自适应滤波的元素级耦合深入建模不同频率分量间的协同作用。
2. 交互式Mamba块(IMB)：促进高效的多粒度信息交互，平衡细粒度判别特征和全面全局上下文信息的提取。
3. 自监督预训练机制：增强对复杂时间模式的理解，提高跨领域和高噪声场景的鲁棒性。

Result: 在多个基准测试上的广泛实验表明，FAIM始终优于现有的最先进方法，在准确性和效率之间实现了优越的权衡，并展现出卓越的性能。

Conclusion: FAIM是一个有效的轻量级时间序列分类模型，通过频域特征提取、多粒度信息交互和自监督预训练，解决了现有深度学习模型在计算成本、噪声敏感性和过拟合方面的问题，在准确性和效率之间取得了良好平衡。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [26] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD：将半监督异常检测重构为集合级任务，通过注意力集合编码器和分级学习目标，直接建模定义异常的复杂群体级交互


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法主要关注单个点或简单对，忽略了异常的上下文本质（异常由偏离群体定义），且未能利用集合组合生成的丰富监督信号，难以利用数据中的高阶交互

Method: 提出SetAD框架，将半监督异常检测重构为集合级任务；使用注意力集合编码器通过分级学习目标训练，学习量化整个集合的异常程度；提出上下文校准的异常评分机制，通过聚合点在多个不同上下文集合中相对于同伴行为的归一化偏差来评估异常分数

Result: 在10个真实世界数据集上的广泛实验表明，SetAD显著优于最先进模型；模型性能随集合大小增加而持续提升，为基于集合的异常检测公式提供了强有力的实证支持

Conclusion: SetAD通过集合级视角有效解决了传统点或对中心方法的局限性，直接建模了定义异常的复杂群体级交互，在性能和鲁棒性方面表现出色

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [27] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 使用无监督机器学习框架分析海关数据，自动检测可疑贸易模式，为环境条约监管提供优先级审查清单


<details>
  <summary>Details</summary>
Motivation: 需要新方法来监控《蒙特利尔议定书》等环境条约，通过分析大规模复杂海关数据集来发现可疑贸易活动

Method: 结合多种无监督机器学习技术：K-Means聚类发现贸易原型，隔离森林和IQR检测异常值，启发式标记识别模糊描述，最后整合为优先级评分系统

Result: 成功识别出1,351个价格异常值和1,288个高优先级货物，发现高优先级商品具有不同的价值重量比，模型检测到2021年初"大宗贸易"激增与美国AIM法案实施相关

Conclusion: 提出了可重复的无监督学习流程，将原始贸易数据转化为监管机构可用的优先级情报，验证了SHAP可解释AI确认模糊描述和高价值是最重要的风险预测因素

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [28] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典大规模登记数据转化为文本化生命轨迹，解决分类变量高基数性和编码不一致问题，使用NLP模型预测居住流动性，证明文本化登记数据能有效支持复杂纵向分析。


<details>
  <summary>Details</summary>
Motivation: 解决纵向数据分析中的两个长期挑战：分类变量的高基数性（大量类别）和时间上编码方案的不一致性。利用瑞典全面的人口登记数据，探索如何将结构化登记数据转化为文本化生命轨迹来支持复杂的社会科学分析。

Method: 将690万个体（2001-2013年）的登记数据转化为语义丰富的文本生命轨迹，包含人口统计信息和年度居住、工作、教育、收入、家庭状况变化。使用多种NLP架构（LSTM、DistilBERT、BERT、Qwen）预测个体后续年份（2013-2017年）的居住流动性，比较不同模型在捕捉时间序列和语义结构方面的效果。

Result: 序列模型和基于Transformer的模型比基线模型更有效地捕捉时间序列和语义结构。文本化登记数据保留了关于个体路径的有意义信息，支持复杂、可扩展的建模。瑞典登记数据的独特全面性为开发和评估新的序列建模方法提供了严格的测试平台。

Conclusion: 将语义丰富的登记数据与现代语言模型相结合，可以显著推进社会科学中的纵向分析。这种方法为解决传统纵向数据分析中的挑战提供了新途径，并为其他国家的类似研究提供了方法论参考。

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [29] [Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing](https://arxiv.org/abs/2512.07866)
*Maria Milena Araujo Felix*

Main category: cs.LG

TL;DR: 提出基于深度学习和NLP的DGA域名检测方法，使用LSTM网络达到97.2%准确率，优于传统熵分析方法


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件使用域名生成算法（DGA）生成大量动态地址，使基于静态黑名单的传统防火墙防御失效，需要更智能的检测方法

Method: 收集包含5万个合法域名和5万个恶意域名的混合数据库，提取词汇特征，训练LSTM循环神经网络进行检测

Result: LSTM方法在检测复杂DGA模式上表现出色，达到97.2%的准确率，在模糊合法流量场景中降低了误报率

Conclusion: 深度学习结合NLP技术能有效检测DGA域名，特别是对复杂模式识别优于传统统计熵分析方法，为网络安全防御提供了新方向

Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.

</details>


### [30] [Bayesian Optimization for Function-Valued Responses under Min-Max Criteria](https://arxiv.org/abs/2512.07868)
*Pouya Ahadi,Reza Marzban,Ali Adibi,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出MM-FBO框架，用于优化函数型响应的黑盒函数，直接最小化函数域上的最大误差，而非传统方法的平均误差。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化主要针对标量响应，而许多科学工程问题中响应是函数型的（随时间或波长变化）。现有方法最小化积分误差（平均性能），忽略了最坏情况偏差。

Method: 使用函数主成分分析表示函数响应，为PC得分构建高斯过程代理模型。提出集成不确定性获取函数，平衡最坏情况期望误差的利用和函数域上的探索。

Result: 提供两个理论保证：最坏情况目标的离散化边界，以及代理模型准确时获取函数收敛到真实min-max目标的证明。在合成基准和物理案例（电磁散射、气相渗透）上验证，MM-FBO始终优于现有基线。

Conclusion: MM-FBO框架有效解决了函数型响应的贝叶斯优化问题，强调了显式建模函数不确定性的重要性，在科学工程应用中具有实际价值。

Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.

</details>


### [31] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于混合专家(MoE)的噪声估计器，用于医学时间序列信号的扩散模型重建，通过RFAMoE模块自适应选择感受野，Fusion MoE模块并行生成多个噪声信号并融合，单次推理完成重建，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 医学时间序列信号具有多变量、高时序变异性、高噪声和易受伪影影响等独特特性，使得基于深度学习的插补等任务仍然具有挑战性。现有扩散模型方法在医学时间序列领域尚未充分探索，且多推理平均方法虽然能减少重建误差但计算成本高

Method: 在基于分数的扩散框架中提出混合专家(MoE)噪声估计器：1) RFAMoE模块使每个通道在扩散过程中自适应选择所需感受野；2) Fusion MoE模块利用MoE特性并行生成K个噪声信号，通过路由机制融合，单次推理完成信号重建

Result: 在多个任务和数据集上，提出的框架始终优于基于扩散模型的最先进方法，不仅性能提升，还消除了多推理过程带来的显著计算成本和延迟

Conclusion: 提出的MoE-based扩散模型框架为医学时间序列信号重建提供了一种高效解决方案，通过自适应感受野选择和并行噪声生成融合机制，在保持高性能的同时大幅降低计算开销

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [32] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: CRAG框架统一建模自动驾驶车辆测试中的正常驾驶行为和罕见安全关键行为，通过解耦潜在空间实现可控风险场景生成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要同时在日常驾驶和罕见安全关键条件下进行严格测试，但现有方法难以在保持真实性的同时模拟风险行为，特别是如何有效利用有限的真实事故数据。

Method: 构建结构化潜在空间解耦正常行为和风险相关行为，结合风险感知潜在表示和基于优化的模式转换机制，使智能体能够在长时间范围内平滑、可信地从安全状态过渡到风险状态。

Result: 实验表明CRAG相比现有基线提高了行为多样性，同时能够可控地生成风险场景，实现针对性和高效的自动驾驶鲁棒性评估。

Conclusion: CRAG框架成功统一了正常和风险行为的建模，通过解耦潜在空间有效利用有限事故数据，为自动驾驶系统提供了更全面、可控的测试场景生成能力。

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [33] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN通过将符号基元直接集成到训练中，解决了传统KAN激活函数缺乏符号保真度的问题，实现了可解释性与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统Kolmogorov-Arnold网络虽然提供了可解释机器学习的路径，但其训练后的激活函数往往缺乏符号保真度，学习到的分解没有有意义的对应关系，难以实现真正的可解释性。

Method: 提出Softly Symbolified KANs (S2KAN)，将符号基元直接集成到训练中。每个激活函数从符号项和密集项字典中提取，使用可学习的门控机制稀疏化表示。稀疏化过程是可微分的，支持端到端优化，并受最小描述长度目标指导。

Result: 在符号基准测试、动态系统预测和实际预测任务中，S2KAN实现了竞争性或更优的准确性，同时模型规模显著更小。观察到即使没有正则化压力，也会出现自稀疏化现象。

Conclusion: S2KAN通过将符号表示直接集成到训练过程中，在符号项足够时发现可解释形式，不足时优雅地退化为密集样条，实现了可解释性与模型性能的良好平衡。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [34] [Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling](https://arxiv.org/abs/2512.07876)
*Qi Chen,Mihai Anitescu*

Main category: cs.LG

TL;DR: 提出一种结合傅里叶增强的循环神经网络，用于电力负荷降尺度预测，在四个PJM区域表现优于传统基准方法


<details>
  <summary>Details</summary>
Motivation: 电力负荷预测需要从低分辨率输入中准确预测高分辨率负荷，传统方法在捕获季节模式和跨周期依赖关系方面存在局限

Method: 结合三个核心组件：(1) 由低分辨率输入驱动的循环神经网络主干，(2) 在潜在空间融合的显式傅里叶季节嵌入，(3) 捕获每个周期内高分辨率组件间依赖关系的自注意力层

Result: 在四个PJM区域，该方法相比传统Prophet基准（无论是否包含季节性和LAA）以及没有注意力或傅里叶特征的RNN消融实验，均获得更低的RMSE和更平坦的预测误差曲线

Conclusion: 傅里叶增强的循环神经网络结合自注意力机制能有效提升电力负荷降尺度预测的准确性，特别是在捕获季节模式和跨时间依赖关系方面

Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.

</details>


### [35] [Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design](https://arxiv.org/abs/2512.07877)
*Amogh Anshu N,Harish BP*

Main category: cs.LG

TL;DR: 提出基于机器学习的NoC设计空间探索框架，使用BookSim仿真和反向神经网络模型，比较MLP、条件扩散模型和CVAE三种架构，条件扩散模型在未见数据上获得最低MSE（0.463），大幅缩短设计探索时间。


<details>
  <summary>Details</summary>
Motivation: 传统NoC设计空间探索方法在高维配置空间中效率低下，难以处理复杂的非线性参数交互，需要自动化、高效的解决方案来满足严格的吞吐量和延迟约束。

Method: 开发机器学习驱动的框架，使用BookSim生成超过150,000个仿真数据点，比较三种神经网络架构（MLP、条件扩散模型、CVAE）来预测给定目标性能指标下的最优NoC参数。

Result: 条件扩散模型表现最佳，在未见数据上获得0.463的均方误差；该框架将设计探索时间减少了数个数量级，为快速、可扩展的NoC协同设计提供了实用解决方案。

Conclusion: 机器学习驱动的NoC设计空间探索框架显著提升了设计效率，条件扩散模型在参数预测方面表现最优，为复杂NoC系统的快速设计提供了有效工具。

Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.

</details>


### [36] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出SpecMatch-CL损失函数，通过最小化图嵌入的归一化拉普拉斯矩阵差异来对齐图对比学习中的视图特定图结构，在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）在优化图嵌入的跨视图对齐时，缺乏对视图特定图-图结构全局结构的控制机制，这限制了学习效果。

Method: 引入SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图结构。理论上证明该差异为理想完美对齐对比损失与当前损失差异的上界。

Result: 在8个TU基准测试的无监督学习和低标签率半监督学习中达到新的SOTA，在PPI-306K和ZINC 2M数据集的迁移学习中也获得一致提升。

Conclusion: SpecMatch-CL通过控制图嵌入的全局结构对齐，显著提升了图对比学习的性能，在多种学习场景下都表现出优越性。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [37] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 提出Cone Collapse算法从几何角度进行非负矩阵分解，通过收缩非负象限到数据生成的最小锥体来恢复极端射线，并在此基础上构建锥体感知的正交NMF模型CC-NMF，在聚类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有NMF算法主要从优化视角出发，未能充分利用NMF诱导的锥体几何结构。数据点位于凸锥中，其极端射线编码了基本方向或"主题"。从几何角度重新审视NMF，可以更好地理解数据结构并改进聚类性能。

Method: 提出Cone Collapse算法：从完整的非负象限开始，迭代收缩到数据生成的最小锥体。在温和假设下，算法有限步终止并恢复X^⊤的最小生成锥。在此基础上，通过将单正交NMF应用于恢复的极端射线，推导出锥体感知的正交NMF模型CC-NMF。

Result: 在16个基准基因表达、文本和图像数据集上，CC-NMF在聚类纯度方面始终匹配或优于强NMF基线方法，包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF。实验证明显式恢复数据锥体可以产生理论上有依据且经验上强大的基于NMF的聚类方法。

Conclusion: 从几何角度重新审视NMF，通过Cone Collapse算法显式恢复数据锥体，可以开发出理论上有依据且经验性能优越的NMF聚类方法。锥体感知的CC-NMF模型在多个领域数据集上表现出色，验证了几何视角对改进NMF聚类的重要性。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [38] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出CLOP损失函数，通过促进类别嵌入的正交线性子空间来防止对比学习中的维度坍塌问题，在图像分类和目标检测任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 对比学习在深度学习中表现出色，但维度坍塌问题（嵌入收敛到低维空间）在半监督和自监督设置中构成重大挑战，尤其是在学习率超过特定阈值时会导致坍塌解。

Method: 首先识别了标准对比损失收敛到坍塌解的关键学习率阈值，基于此提出CLOP损失函数，通过促进类别嵌入形成正交线性子空间来防止维度坍塌。

Result: 在真实和合成数据集上的大量实验表明，CLOP在图像分类和目标检测任务中提升了性能，同时在不同学习率和批量大小下表现出更好的稳定性。

Conclusion: CLOP通过防止维度坍塌有效提升了对比学习的性能稳定性，为半监督对比学习提供了更鲁棒的解决方案。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [39] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2是GSPN的改进版本，通过算法-系统联合重新设计解决了原实现中的GPU内核启动开销、内存传输和冗余计算问题，在保持精度的同时显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管GSPN通过线扫描传播方案将计算复杂度从二次降低到接近线性，但其现有实现仍存在GPU内核重复启动开销大、全局内存数据传输过多、以及每个通道维护单独传播权重导致的冗余计算等问题，需要进一步优化。

Method: 1. 系统优化：将数千个微内核启动合并为单个2D内核，为每个通道切片固定一个warp，将前一列的激活暂存到共享内存；2. 算法优化：引入紧凑通道传播策略，替代每通道矩阵，减少参数，并与transformer注意力中的亲和力图自然对齐。

Result: 在图像分类和文本到图像合成任务上，GSPN-2能够匹配transformer级别的精度，同时显著降低计算成本，为视觉应用中的全局空间上下文建模建立了新的效率前沿。

Conclusion: GSPN-2通过独特的结构化矩阵变换和GPU优化实现相结合，为高分辨率图像和长视频相关应用提供了高效的全局空间上下文建模解决方案。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [40] [ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking](https://arxiv.org/abs/2512.07885)
*Davide Donno,Donatello Elia,Gabriele Accarino,Marco De Carlo,Enrico Scoccimarro,Silvio Gualdi*

Main category: cs.LG

TL;DR: ByteStorm：基于深度学习和计算机视觉的热带气旋追踪框架，无需阈值调优，在东西北太平洋盆地表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统热带气旋追踪方案主要依赖主观阈值，这可能在特定地理区域应用中引入偏差。需要一种无需阈值调优的数据驱动方法来实现准确的热带气旋追踪。

Method: 提出ByteStorm框架：1）使用深度学习网络（分类和定位）检测热带气旋中心，仅使用850mb相对涡度和平均海平面气压；2）通过BYTE算法将检测到的中心连接成热带气旋轨迹。

Result: 在东、西北太平洋盆地评估显示：检测概率（ENP 85.05%，WNP 79.48%）、误报率（ENP 23.26%，WNP 16.14%）、年际变率相关性（ENP 0.75，WNP 0.69）均优于现有确定性追踪器。

Conclusion: ByteStorm展示了深度学习和计算机视觉在快速准确热带气旋追踪中的潜力，为传统方法提供了稳健的替代方案。

Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.

</details>


### [41] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种符号回归算法，结合了决策树分割和非线性常数优化，能够将基于规则的逻辑无缝集成到符号回归和分类模型中，在临床风险评分开发中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗决策经常使用结合风险方程和规则的算法，但传统符号回归难以建模这种决策过程。然而，符号回归具有数据驱动和可解释的优势，有望用于开发数据驱动的临床风险评分。

Method: Brush算法结合了类似决策树的分割算法和非线性常数优化，允许将基于规则的逻辑无缝集成到符号回归和分类模型中。

Result: Brush在SRBench上实现了帕累托最优性能，成功重建了两个广泛使用的临床评分系统，实现了高准确率和可解释模型。相比决策树、随机森林和其他符号回归方法，Brush获得了相当或更优的预测性能，同时产生更简单的模型。

Conclusion: Brush算法能够有效开发数据驱动的临床风险评分，结合了规则逻辑和符号回归的优势，在保持可解释性的同时提供优秀的预测性能。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [42] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net：一种无需示例的自解释原型模型，用于持续学习，避免存储过去示例，保持简单架构，同时提供有用解释和强大性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘的挑战，现有可解释AI方法大多使用事后解释或需要为每个新任务增加内存，导致可扩展性有限。需要一种无需存储过去示例、架构简单但仍能提供有用解释的持续学习方法。

Method: 提出CIP-Net（Continual Interpretable Prototype Network），一种无需示例的自解释原型模型。它避免存储过去示例，保持简单架构，通过原型机制实现自解释功能，在持续学习过程中维护知识。

Result: CIP-Net在任务增量学习和类别增量学习设置中，相比之前的无示例和自解释方法取得了最先进的性能，同时显著降低了内存相关开销。

Conclusion: CIP-Net为持续学习提供了一个实用且可解释的解决方案，通过自解释原型模型有效减少灾难性遗忘，无需存储过去示例，具有更好的可扩展性。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [43] [HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability](https://arxiv.org/abs/2512.07988)
*Sudhanva Manjunath Athreya,Paul Rosen*

Main category: cs.LG

TL;DR: HOLE方法通过持续同调分析深度神经网络，使用拓扑特征和可视化技术来增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然取得了显著成功，但其学习到的表示和决策过程仍然不透明且难以解释，需要新的分析方法来理解模型内部工作机制。

Method: HOLE方法从神经激活中提取拓扑特征，并使用桑基图、热图、树状图和blob图等多种可视化技术来呈现这些特征，便于分析各层的表示结构和质量。

Result: 在标准数据集上的评估表明，拓扑分析能够揭示与类别分离、特征解耦和模型鲁棒性相关的模式，为理解深度学习系统提供了补充视角。

Conclusion: HOLE方法通过拓扑分析为深度神经网络提供了有价值的可解释性工具，有助于理解和改进深度学习系统。

Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.

</details>


### [44] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 开发了一个用于时间序列预测的Web平台，旨在降低技术门槛，让研究人员和临床医生能够轻松分析数据、训练模型并解释结果。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在医疗健康等领域有广泛应用，但技术门槛较高，需要专业知识来分析数据、构建模型和解释结果，这限制了相关技术的使用。

Method: 开发了一个Web平台，支持数据上传和可视化分析，提供多种可高度定制的预测模型和训练技术，并集成大语言模型为用户提供参数选择建议和结果解释。

Result: 创建了一个使时间序列预测过程对研究人员和临床医生更易访问的平台，支持数据可视化、多模型训练和智能参数推荐，旨在集成到学习型健康系统中。

Conclusion: 该平台通过降低技术门槛，使时间序列预测技术更易被医疗领域的研究人员和临床医生使用，有望集成到学习型健康系统中实现持续数据收集和临床推理。

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [45] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 在ICU等重症监护环境中，本文比较了三种离线多目标强化学习算法与单目标基线方法，发现PEDA DT算法在MIMIC-IV数据集上提供了更好的灵活性，支持个性化可调整的临床决策。


<details>
  <summary>Details</summary>
Motivation: 重症监护中临床医生面临平衡患者生存率和资源利用率的复杂挑战。传统单目标强化学习方法使用固定的标量化奖励函数，导致策略僵化，无法适应变化的临床优先级。需要一种能够在离线环境下学习、支持动态偏好选择的方法。

Method: 在MIMIC-IV数据集上，对三种离线多目标强化学习算法（CPQL、Adaptive CPQL、PEDA DT）与三种标量化单目标基线方法（BC、CQL、DDQN）进行基准测试。使用离线策略评估指标进行比较分析。

Result: PEDA DT算法相比静态标量化基线方法展现出更优的灵活性。研究结果扩展了单目标决策变换器在医疗领域的先前发现，证实序列建模架构在多目标条件生成中仍然稳健有效。

Conclusion: 离线多目标强化学习是一个有前景的框架，能够在无需重新训练的情况下，实现重症监护中个性化、可调整的决策制定，为临床实践提供了更灵活的工具。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [46] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一个医学世界模型，通过在结构化潜在空间中预测疾病演化，整合时间间隔和患者特定数据，生成个体化治疗计划，并在胶质瘤数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前静态AI预测器无法预测动态疾病演化，现有医学世界模型方法通常忽略患者特定的时间和临床背景，缺乏将预测与治疗决策连接的反馈机制。

Method: CLARITY在结构化潜在空间中直接预测疾病演化，明确整合时间间隔（时间背景）和患者特定数据（临床背景），将治疗条件进展建模为平滑可解释的轨迹，并引入新的预测到决策框架。

Result: 在MU-Glioma-Post数据集上，CLARITY比最近的MeWM方法性能提升12%，显著超越所有其他医学专用大型语言模型，在治疗规划方面达到最先进性能。

Conclusion: CLARITY通过整合时间和临床背景，在结构化潜在空间中生成生理上可信的个体化治疗计划，为临床决策提供了透明可操作的建议，解决了现有医学世界模型的局限性。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [47] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA提出了一种可学习的核化线性注意力机制，在保持线性计算成本的同时，匹配甚至超越了二次注意力的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力存在O(n²)的二次计算成本，限制了其在长序列领域的应用。现有的线性注意力机制虽然将成本降至O(n)，但依赖于固定的随机特征映射，导致模型精度和计算效率之间存在根本性权衡。

Method: LUNA的核心思想是学习核特征映射而非固定使用先验特征。通过参数化核函数，LUNA学习针对特定数据和任务的特征基，克服了固定特征方法的表达能力限制。该方法实现了一个可学习的特征映射，该映射诱导出正定核并支持流式形式，从而在序列长度上实现线性的时间和内存扩展。

Result: 在Long Range Arena (LRA)基准测试中，LUNA在计算对等条件下（相同参数量、训练步数和近似FLOPs）取得了高效Transformer中的最佳平均准确率。此外，在BERT和ViT-B/16模型的后处理转换中，LUNA替换softmax并经过短暂微调后，能够恢复大部分原始性能，显著优于固定线性化方法。

Conclusion: LUNA成功消除了线性注意力机制中精度与效率之间的权衡，通过可学习的核特征映射实现了线性计算成本下的高性能，为长序列处理提供了有效的解决方案。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [48] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出了一种可解释的深度竞争风险模型DKAJ，通过自动学习的核函数将数据点表示为聚类加权组合，在保持Aalen-Johansen估计器优点的同时提供可视化解释能力。


<details>
  <summary>Details</summary>
Motivation: 传统Aalen-Johansen估计器在竞争风险分析中是非参数的，但缺乏处理复杂数据和提供解释性的能力。需要一种既能保持经典方法优势，又能提供可解释性和可视化能力的深度学习方法。

Method: 提出Deep Kernel Aalen-Johansen (DKAJ)估计器，将每个数据点表示为聚类的加权组合。通过自动学习的核函数衡量数据点之间的相似性，当数据点仅对一个聚类有非零权重时，其预测CIF对应于该聚类的经典Aalen-Johansen估计。

Result: 在四个标准竞争风险数据集上，DKAJ与最先进的基线方法表现相当，同时能够提供可视化辅助模型解释，展示了其竞争力和可解释性优势。

Conclusion: DKAJ成功地将经典Aalen-Johansen估计器推广到深度学习框架中，在保持预测性能的同时提供了可解释性和可视化能力，为竞争风险分析提供了一种有前景的新方法。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [49] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种因果引导的多模态领域泛化框架，通过对抗解耦和统一表示学习来解决社交媒体危机分类中的领域泛化问题，在未见灾难场景中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体危机分类需要从多模态帖子中提取可操作的灾害相关信息，但现有方法在未见危机类型上泛化能力差，主要因为：1. 未能解耦虚假特征和因果特征，导致领域偏移时性能下降；2. 未能对齐异构模态表示，阻碍了单模态领域泛化技术向多模态设置的直接迁移。

Method: 提出因果引导的多模态领域泛化框架，结合对抗解耦与统一表示学习。对抗目标鼓励模型解耦并关注领域不变的因果特征，统一表示将不同模态特征对齐到共享潜在空间，使单模态领域泛化策略能无缝扩展到多模态学习。

Result: 在不同数据集上的实验表明，该方法在未见灾难场景中实现了最佳性能。

Conclusion: 通过因果引导的对抗解耦和统一表示学习，提出的多模态领域泛化框架有效提升了社交媒体危机分类在未见灾难类型上的泛化能力。

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [50] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen,Alexander G. Hasson,Sara Tanovic*

Main category: cs.LG

TL;DR: 该研究将稀疏自编码器技术应用于化学语言模型，揭示了模型内部可解释的化学特征表示，建立了潜在特征与化学知识领域之间的关联。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在药物和材料发现等高风险应用中的使用，机器学习可解释性问题日益紧迫。虽然化学语言模型在分子性质预测和生成方面表现出色，但其内部如何表示化学知识仍不清楚。

Method: 将稀疏自编码器技术扩展到化学语言模型中，应用于FM4M SMI-TED化学基础模型，提取语义上有意义的潜在特征，并分析其在多样化分子数据集上的激活模式。

Result: 发现这些模型编码了丰富的化学概念景观，识别出特定潜在特征与不同化学知识领域（包括结构基序、物理化学性质和药理学药物类别）之间的相关性。

Conclusion: 该方法为揭示化学AI系统中潜在知识提供了通用框架，对基础理解和实际部署都有重要意义，有望加速计算化学研究。

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


### [51] [Complexity of One-Dimensional ReLU DNNs](https://arxiv.org/abs/2512.08091)
*Jonathan Kogan,Hayden Jananthan,Jeremy Kepner*

Main category: cs.LG

TL;DR: 研究一维ReLU深度神经网络在无限宽度极限下的表达能力，证明线性区域数量随神经元总数线性增长，并提出函数自适应稀疏性概念。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络的表达能力是深度学习理论的核心问题。线性区域数量是衡量ReLU网络表达能力的重要指标，但现有研究对随机初始化网络在无限宽度极限下的线性区域增长规律缺乏精确分析。

Method: 研究一维ReLU深度神经网络（He缩放、非零偏置）在无限宽度极限下的行为。通过理论分析推导线性区域数量的期望值，并提出函数自适应稀疏性概念，比较网络使用的区域数量与逼近目标函数所需的最小区域数量。

Result: 证明线性区域数量的期望增长为∑_{i=1}^L n_i + o(∑_{i=1}^L n_i) + 1，其中n_ℓ是第ℓ隐藏层的神经元数。这意味着线性区域数量随神经元总数线性增长。

Conclusion: 一维ReLU深度神经网络在无限宽度极限下具有线性区域数量的线性增长特性，提出的函数自适应稀疏性概念为评估网络效率提供了新视角，有助于理解深度神经网络的实际表达能力。

Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.

</details>


### [52] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 提出一种通过"忏悔"机制让大语言模型诚实报告自身缺陷的方法，在训练中单独奖励忏悔的诚实性，不干预主回答的奖励，从而激励模型在忏悔中诚实揭露不当行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在报告自身行为和信念时可能存在不诚实问题，例如夸大事实信心或掩盖不当行为。这种不诚实可能源于强化学习训练中的奖励塑造问题，无意中激励模型说谎或掩饰行为。

Method: 提出忏悔机制：在主回答后，模型被要求提供忏悔输出，完整说明其遵守政策和指令的情况。忏悔的奖励仅基于诚实性，不影响主回答的奖励。通过训练GPT-5-Thinking模型产生忏悔，并在分布外场景评估诚实性。

Result: 当模型在主回答中说谎或隐瞒缺陷时，它经常在忏悔中诚实地承认这些行为，且忏悔的诚实性随训练适度提高。忏悔机制支持多种推理时干预，包括监控、拒绝采样和向用户报告问题。

Conclusion: 忏悔机制为激励大语言模型诚实报告自身缺陷提供了可行方法，尤其在严重不当行为情况下效果显著。该方法为模型监控和诚实性改进提供了实用工具。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [53] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC提出基于动作块的模型强化学习，通过预测动作序列而非单步动作来减少模型误差累积，结合拒绝采样防止分布外动作，在复杂长时域离线RL任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中模型基方法面临的挑战：长时域任务中，值扩展需要多步想象轨迹，但更大的步数n会放大模型误差累积，导致未来预测质量下降。

Method: 1. 动作块模型：预测动作序列（动作块）而非单步动作的未来状态，减少复合误差；2. 拒绝采样：从表达性行为动作块策略中采样，防止模型被分布外动作利用。

Result: 在包含高达1亿转移的大规模数据集上的挑战性任务实验中，MAC在离线模型基RL算法中表现最佳，尤其在长时域任务上优势明显。

Conclusion: MAC通过动作块模型减少误差累积和拒绝采样防止模型利用，为复杂长时域离线RL任务提供了可扩展的解决方案，在挑战性任务上超越了现有模型基方法。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [54] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 本文提出在评估大语言模型时，应使用Youden's J统计量或平衡准确率来选择最佳分类器（评判者），而不是传统指标如准确率、精确率和F1分数，因为这些传统指标对类别不平衡敏感且可能扭曲流行率估计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的严格评估依赖于比较模型行为的流行率（如任务通过率或政策违规率）。这些流行率估计由分类器（LLM作为评判者或人工标注者）产生，因此分类器的选择对可信评估至关重要。传统指标（准确率、精确率、F1分数）对类别不平衡敏感，且受正类任意选择的影响，可能偏向扭曲流行率估计的评判者。

Method: 本文从理论上证明Youden's J统计量与选择最佳模型比较评判者相一致，并指出平衡准确率是J的等效线性变换。通过分析论证和实证示例及模拟，展示了使用平衡准确率选择评判者的优势。

Result: 研究表明，使用平衡准确率选择分类器能产生更好、更稳健的评判者选择，避免传统指标因类别不平衡和正类选择带来的偏差，从而获得更可靠的流行率估计。

Conclusion: 在大语言模型评估中，应优先使用Youden's J统计量或平衡准确率来选择分类器，而不是传统指标，以确保模型比较的可靠性和稳健性。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [55] [Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach](https://arxiv.org/abs/2512.08124)
*Zijiang Yang*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的新型加密货币投资组合管理方法，通过分析加密货币间的相对关系而非独立预测，利用神经网络预测未来收益排名进行权重分配，在3.5年市场周期中实现优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个加密货币（如比特币）的价格走势预测并据此交易，忽略了加密货币之间的相互关系。本文旨在通过分析加密货币间的相对关系来管理投资组合，利用横截面信息提升投资表现。

Method: 提出基于神经网络的排名预测方法：在每个时间步，利用神经网络预测所管理加密货币的未来收益排名，并根据排名结果分配投资权重。这种方法整合了横截面信息，考虑了加密货币间的相对表现关系。

Result: 在2020年5月至2023年11月的真实加密货币日度数据上进行回测，期间市场经历了完整的牛市、熊市和盘整周期。提出的方法在复杂市场条件下表现优异，夏普比率达到1.01，年化收益率64.26%，且对交易费用增加具有鲁棒性。

Conclusion: 通过分析加密货币间的相对关系而非独立预测，基于神经网络排名预测的投资组合管理方法在加密货币市场中表现优异，能够适应不同的市场条件，为加密货币投资组合管理提供了新的有效途径。

Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.

</details>


### [56] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出CSO方法，通过抑制内在特征来增强后门检测的敏感性，解决现有方法在易区分类别和隐蔽后门攻击上的不足


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法依赖目标类别的极端异常统计值，但在两类情况下会失效：1）某些非目标类别天然具有极端统计值；2）后门特征相对较弱时。需要更敏感的检测方法

Method: 提出类子空间正交化（CSO）方法：利用少量干净样本，通过约束优化问题，在优化检测统计量的同时与类别的内在特征正交化，从而抑制内在特征贡献

Result: CSO方法能够更敏感地检测后门攻击，特别是在具有挑战性的混合标签攻击和自适应攻击场景下表现良好

Conclusion: 通过抑制内在特征贡献，CSO方法显著提升了后门检测的敏感性，为应对隐蔽后门攻击提供了有效解决方案

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [57] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 论文提出了一个用于评估AI模型生物安全风险的基准生成框架（BBG），特别针对细菌生物威胁，通过分层结构和任务对齐查询来全面评估技术性和操作性风险因素。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型（特别是大语言模型）的快速发展，需要量化其可能被用于生物恐怖主义或获取生物武器的风险。现有基准往往忽视威胁的关键方面，如不同行为者能力水平和操作性风险因素。

Method: 开发了生物威胁基准生成（BBG）框架，采用分层结构（生物威胁类别、要素和任务），构建了细菌生物威胁模式作为任务-查询架构的基础，未来将转化为模型提示并实施评估。

Result: 提出了BBG框架的第一个组件——细菌生物威胁模式，为评估AI模型在细菌生物威胁方面的风险提供了一个稳健、可重复使用的结构，能够捕捉生物对手的完整技术性和操作性需求。

Conclusion: BBG框架（包括细菌生物威胁模式）为评估LLM在细菌生物风险方面提供了全面框架，考虑了不同能力水平的生物对手，未来将扩展为完整的基准评估系统。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [58] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文利用开放性和多智能体学习方法训练和评估能够泛化到新环境、分布外输入以及与其他智能体交互的鲁棒AI智能体，涵盖强化学习和大型语言模型领域。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的普及，需要能够成功导航和适应不断变化的开放世界的智能体。关键挑战是确保这些AI智能体不仅能在训练期间观察到的熟悉环境中表现出色，还能有效泛化到以前未见过的多样化场景。

Method: 1) 引入MiniHack：基于NetHack游戏的沙盒框架，通过程序内容生成创建多样化环境；2) 提出Maestro：生成对抗性课程的方法，逐步增强RL智能体在两人零和游戏中的鲁棒性和泛化能力；3) 利用质量多样性方法在复杂足球游戏领域系统识别预训练RL策略的漏洞；4) 使用进化搜索生成多样化有效输入，诊断和增强LLM对抗对抗性提示的鲁棒性。

Result: 开发了多个框架和方法来训练和评估鲁棒AI智能体：MiniHack为RL智能体创建多样化任务环境；Maestro通过对抗性课程增强智能体鲁棒性；质量多样性方法成功识别了预训练RL策略的漏洞；进化搜索方法有效生成对抗性提示来测试LLM鲁棒性。

Conclusion: 这项工作为AI鲁棒性的未来发展铺平了道路，使得智能体不仅能够适应不断发展的世界，还能在面对不可预见的挑战和交互时蓬勃发展。通过结合开放性和多智能体学习方法，论文展示了在强化学习和大型语言模型领域提高AI系统鲁棒性的有效途径。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [59] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于领域内语言检测和细粒度语言分类，通过两级对比学习框架，在计算和延迟受限的环境中实现高精度语言识别。


<details>
  <summary>Details</summary>
Motivation: 语言识别是多语言系统（如聊天机器人和虚拟助手）的关键第一步，但现有工具在关键场景（如音乐请求中歌曲标题与用户语言不同）表现不佳。开源工具速度快但精度低，而大型语言模型虽然有效但成本过高，不适合低延迟或低资源环境。

Method: 采用轻量级Transformer架构，结合两级对比学习框架：实例级分离和类级对齐，使用自适应边距，为即使是密切相关语言也能生成紧凑且良好分离的嵌入表示。

Result: 在两个具有挑战性的数据集上评估：Amazon Massive（多语言数字助手话语）和Song数据集（频繁代码切换的音乐请求），分别达到99.25%和98.15%的F1分数，超越Sonnet 3.5，同时使用参数减少10倍。

Conclusion: PolyLingua在计算和延迟受限的环境中实现了高精度语言识别，特别适合处理音乐请求等具有挑战性的场景，为多语言系统提供了高效实用的解决方案。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases -- such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets -- Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching) -- PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [60] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高生成模型与人类偏好对齐的训练效率，实现2.4倍加速训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于对齐生成模型与人类偏好至关重要，但其高昂的计算成本阻碍了广泛应用。现有方法在样本效率和信用分配方面存在局限。

Method: 将去噪过程重构为搜索树，从共享初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效重用其共同前缀。通过树结构实现高样本效率、细粒度信用分配（通过奖励反向传播计算步骤特定优势）和摊销计算（多子分支实现每次前向传递多个策略更新）。

Result: 在扩散和基于流的模型上的广泛实验表明，TreeGRPO实现了2.4倍更快的训练速度，并在效率-奖励权衡空间中建立了优越的帕累托前沿。该方法在多个基准测试和奖励模型上始终优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了一条可扩展且有效的途径，通过树结构搜索显著提高了训练效率，解决了传统轨迹方法在信用分配和样本效率方面的限制。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [61] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner是一个开源框架，支持在普通手机上直接进行端到端的LLM微调，解决了移动设备内存和能耗限制问题。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公开LLM数据接近枯竭，利用私有用户数据进行设备端微调成为重要方向，但现有方法主要基于模拟或依赖IoT设备/PC，普通手机领域尚未充分探索，缺乏开源框架。

Method: 提出MobileFineTuner统一开源框架，支持全参数微调和参数高效微调。针对手机内存和能耗限制，引入参数分片、梯度累积和能量感知计算调度等系统级优化。

Result: 在真实手机上成功微调了GPT-2、Gemma 3和Qwen 2.5模型，实验验证了优化策略的有效性，证明MobileFineTuner是设备端LLM训练的可行基础。

Conclusion: MobileFineTuner填补了移动设备LLM微调框架的空白，为未来设备端LLM训练研究提供了实用基础，支持隐私保护和个性化模型开发。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [62] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 本文挑战了AdamW中权重衰减与学习率γ成比例的传统设定，提出权重衰减应与γ²成比例，基于稳态时权重范数稳定的假设，并验证了这种设定能改善训练动态和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统AdamW优化器中，解耦权重衰减被设定为与学习率γ成比例，但这一假设缺乏理论依据。本文旨在重新审视这一设定，探索更合理的权重衰减配置方式。

Method: 通过理论分析，基于稳态时更新与权重无关的简单假设，推导出解耦权重衰减应与γ²成比例。同时推导了Scion优化器中每个小批量的总更新贡献(TUC)应使用动量相关的有效学习率来表征。

Result: 实验验证表明，权重衰减∝γ²的设定能实现稳定的权重和梯度范数，更好地控制训练动态，并提升模型性能。同时验证了TUC使用有效学习率的合理性。

Conclusion: 解耦权重衰减应与学习率的平方(γ²)成比例，而非传统认为的与γ成比例。这种设定基于稳态时权重范数稳定的理论推导，能改善优化器的训练动态和最终性能。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [63] [PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning](https://arxiv.org/abs/2512.08218)
*Ye Qin,Jingchao Wang,Yang Shi,Haiying Huang,Junxu Li,Weijian Liu,Tinghui Chen,Jinghui Qin*

Main category: cs.LG

TL;DR: 提出PR-CapsNet，将胶囊网络扩展到伪黎曼流形，通过自适应曲率路由解决图数据复杂几何结构建模问题，在节点和图分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统胶囊网络在固定曲率空间中建模图数据存在局限性，无法有效处理现实世界图的复杂几何结构。伪黎曼流形为图嵌入提供了特定归纳偏置，但如何利用其改进胶囊网络尚未充分探索。

Method: 1) 将欧几里得胶囊路由扩展到测地线不连通的伪黎曼流形；2) 使用伪黎曼切空间路由将胶囊状态分解为球面-时间和欧几里得-空间子空间；3) 开发自适应曲率路由，通过可学习曲率张量自适应融合不同曲率空间特征；4) 设计几何特性保持的伪黎曼胶囊分类器。

Result: 在节点和图分类基准测试中，PR-CapsNet优于最先进的模型，验证了其对复杂图结构的强大表示能力。

Conclusion: PR-CapsNet通过将胶囊网络扩展到自适应曲率的伪黎曼流形，有效建模了图的层次、聚类和循环结构，为复杂图表示学习提供了新方法。

Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.

</details>


### [64] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: 提出基于持久拓扑结构与上同调流的数学严谨框架，将神经计算重构为动态单纯复形上的上链映射演化，实现跨时空和功能脑状态的表示学习。


<details>
  <summary>Details</summary>
Motivation: 为大脑启发的表示学习建立数学严谨的基础，通过拓扑结构和上同调流来捕捉神经计算中的不变特征，解决现有方法在捕捉大脑状态跨时空和功能不变性方面的不足。

Method: 将神经计算重构为动态单纯复形上的上链映射演化，结合代数拓扑和微分几何构建上同调算子，在拓扑景观中推广基于梯度的学习。使用持久同调、层上同调和谱拉普拉斯分析合成数据和真实神经数据集。

Result: 模型在流形一致性和噪声鲁棒性方面优于图神经网络和基于流形的深度架构，为拓扑驱动的表示学习建立了连贯的数学基础。

Conclusion: 该框架为大脑启发的表示学习提供了数学严谨的基础，通过拓扑结构和上同调流实现了对神经计算中不变特征的捕捉，在表示学习中展现出优越的性能。

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [65] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET提出了一种基于原型的时间序列特征工程方法，在UCR和UEA数据集上表现与现有卷积算法相当，其MR-HY-SP集成在平均准确率排名上超越了之前最好的卷积集成HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法主要依赖特征工程，其中ROCKET通过随机核特征取得了优异性能。研究者希望探索基于原型的特征工程策略是否能进一步提升时间序列分类的性能和鲁棒性。

Method: SPROCKET（Selected Prototype Random Convolutional Kernel Transform）采用基于原型的特征工程策略，通过选择原型来生成特征。同时提出了MR-HY-SP集成方法，将MultiROCKET、HYDRA和SPROCKET组合起来。

Result: 在大多数UCR和UEA时间序列分类数据集上，SPROCKET达到了与现有卷积算法相当的性能。MR-HY-SP集成的平均准确率排名超过了之前最好的卷积集成HYDRA-MR。

Conclusion: 基于原型的特征变换能够提升时间序列分类的准确性和鲁棒性，SPROCKET为时间序列分类提供了新的有效特征工程方法。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [66] [Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations](https://arxiv.org/abs/2512.08256)
*Deepak Gupta,Himanshu Pandey,Ratikanta Behera*

Main category: cs.LG

TL;DR: 提出基于小波的物理信息量子神经网络框架，用于高效求解具有尖锐梯度、刚度、快速局部变化和高振荡行为的多尺度偏微分方程，相比传统方法显著减少参数和计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）及其量子版本（量子PINNs）在求解多尺度特征时面临挑战，且依赖自动微分构建损失函数导致计算开销大、训练时间长。

Method: 开发小波加速的物理信息量子神经网络，结合小波的多分辨率特性与量子神经网络架构，无需自动微分，增强网络捕捉多尺度问题局部和全局特征的能力。

Result: 相比经典小波PINNs，所需可训练参数少于5%，收敛更快；相比现有量子PINNs，速度提升3-5倍，在求解多尺度和振荡问题时表现出优越精度。

Conclusion: 该框架通过结合小波多分辨率特性和量子神经网络，有效解决了多尺度偏微分方程求解中的计算复杂度和精度问题，为挑战性多尺度振荡问题提供了高效解决方案。

Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.

</details>


### [67] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出统一几何-随机多模态深度学习框架，整合EEG、ECG、呼吸、SpO2、EMG和fMRI信号，建模癫痫猝死和卒中易感性。


<details>
  <summary>Details</summary>
Motivation: 癫痫猝死和急性缺血性卒中是涉及皮层、脑干和自主神经系统复杂相互作用的危及生命疾病，需要整合多模态信号进行早期检测和风险分层。

Method: 结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制，使用分数流行病扩散在结构脑图上建模卒中传播。

Result: 在MULTI-CLARID数据集上展示改进的预测准确性，从流形曲率、分数记忆指数、注意力熵和扩散中心性获得可解释的生物标志物。

Conclusion: 该框架为神经自主神经疾病的早期检测、风险分层和可解释多模态建模提供了数学原理基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [68] [Mathematical Foundations of Neural Tangents and Infinite-Width Networks](https://arxiv.org/abs/2512.08264)
*Rachana Mysore,Preksha Girish,Kavitha Jayaram,Shrey Kumar,Preksha Girish,Shravan Sanjeev Bagal,Kavitha Jayaram,Shreya Aravind Shastry*

Main category: cs.LG

TL;DR: 提出NTK-ECRN架构，结合傅里叶特征嵌入、残差连接和随机深度，在无限宽度机制下分析NTK动态，建立谱性质与泛化性能的理论联系。


<details>
  <summary>Details</summary>
Motivation: 研究无限宽度机制下神经网络的数学基础，通过神经正切核(NTK)理论分析神经网络训练动态，旨在建立理论分析与实际深度学习架构之间的桥梁。

Method: 提出NTK-ECRN架构，整合傅里叶特征嵌入、层间缩放残差连接和随机深度技术，理论推导NTK动态边界，分析特征值演化，建立谱性质与泛化优化的理论联系。

Result: 在合成和基准数据集上验证了预测的核行为，展示了改进的训练稳定性和泛化性能，为无限宽度理论与实际深度学习架构提供了综合框架。

Conclusion: 该工作建立了连接无限宽度理论和实用深度学习架构的全面框架，通过NTK-ECRN架构实现了对训练过程中核演化的严格分析，为理解神经网络优化和泛化提供了理论基础。

Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.

</details>


### [69] [SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing](https://arxiv.org/abs/2512.08267)
*Yi Ni,Xinkun Wang,Han Zhang*

Main category: cs.LG

TL;DR: SOFA-FL是一个自组织分层联邦学习框架，通过动态聚类、拓扑重构和自适应数据共享来解决数据异构性和固定网络拓扑问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在动态环境中面临数据异构性和固定网络拓扑结构的挑战，需要能够自适应演化的系统来应对数据分布的变化。

Method: 提出SOFA-FL框架，包含三个核心机制：1) DMAC动态多分支凝聚聚类构建初始分层结构；2) SHAPE自组织分层自适应传播与演化，通过嫁接、剪枝、合并和净化操作动态重构拓扑；3) 自适应聚类数据共享，允许客户端与集群节点之间进行受控的部分数据交换。

Result: SOFA-FL能够有效捕捉客户端之间的动态关系，增强个性化能力，而不依赖于预定的集群结构。

Conclusion: SOFA-FL为动态环境中的联邦学习提供了一个自适应的分层框架，能够应对数据异构性和网络拓扑变化，提高系统的适应性和个性化性能。

Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.

</details>


### [70] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: gHAWK：通过预计算局部和全局结构特征来加速大规模知识图谱上的图神经网络训练


<details>
  <summary>Details</summary>
Motivation: 现有消息传递图神经网络在大规模知识图谱上扩展性差，因为迭代消息传递过程效率低下，特别是在小批量训练中，节点只能看到部分邻居视图

Method: 提出gHAWK框架，在GNN训练开始前预计算结构特征：1) Bloom过滤器编码局部邻域结构；2) TransE嵌入表示节点的全局位置。这些特征与领域特定特征融合后输入到任何GNN技术中

Result: 在Open Graph Benchmark的大规模数据集上，gHAWK在节点属性预测和链接预测任务上实现了最先进的准确率，降低了训练时间，在三个图谱上位居OGB排行榜榜首

Conclusion: 通过将结构先验与消息传递训练相结合，gHAWK显著减少了内存使用，加速了收敛，并提高了模型准确性，为大规模知识图谱上的GNN训练提供了可扩展的解决方案

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [71] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF使用梯度信息计算全局线性预条件器，通过特征空间旋转使轴对齐决策树能处理旋转和交互依赖的决策边界，保持简单性的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树快速稳定但难以处理旋转或特征交互的决策边界，而倾斜森林虽然能解决但计算成本高且实现复杂。需要一种既能处理复杂边界又保持简单性的方法。

Method: 首先拟合轴对齐森林估计类别概率或回归输出，计算预测相对于每个特征的有限差分梯度，聚合为期望雅可比外积作为全局线性预条件器，对特征空间进行旋转后使用标准轴对齐森林处理。

Result: 在表格分类和回归基准测试中，这种预条件方法持续改进轴对齐森林性能，通常匹配或超越倾斜基线方法，同时提升训练时间。

Conclusion: 监督预条件方法能够恢复倾斜森林的大部分准确性，同时保持轴对齐树的简单性和鲁棒性，为处理复杂决策边界提供了一种高效简洁的解决方案。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [72] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出一种名为MAN的联邦学习正则化方法，通过最小化激活范数来约束优化问题的平坦性，提高联邦学习模型的泛化性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习训练过程中，聚合的全局模型容易收敛到"尖锐最小值"，这会影响模型的泛化能力。为了解决这个问题，需要改进联邦学习框架以提高模型的泛化性能。

Method: 提出平坦性约束的联邦学习优化问题，通过约束训练损失Hessian矩阵的最大特征值来实现。进一步提出名为MAN的计算高效正则化技术，通过最小化客户端模型中每层激活的范数来降低Hessian矩阵的最大特征值。

Result: 将提出的平坦性约束优化应用于现有联邦学习技术，获得了显著改进，建立了新的最先进性能。

Conclusion: 通过引入平坦性约束和MAN正则化技术，能够有效提高联邦学习模型的泛化性能，确保收敛到平坦最小值，从而提升模型在实际应用中的表现。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [73] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出一种考虑标签依赖关系的多标签数据集采样算法，使用多元伯努利分布建模，通过权重计算实现目标分布特征，应用于64个生物医学主题的科研文章数据集


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中标签通常非互斥且频率差异大，难以获得包含稀缺标签的样本，同时保持已知的分布偏差，需要解决标签依赖关系下的采样挑战

Method: 使用多元伯努利分布作为多标签问题的基础分布，利用观测标签频率估计分布参数，为每个标签组合计算权重，考虑标签依赖关系进行加权采样

Result: 应用于Web of Science的64个生物医学主题类别研究文章，成功保持类别频率顺序，减少最常⻅和最不常⻅类别间的频率差异，考虑类别依赖关系，产生更平衡的子样本

Conclusion: 该方法能有效增强少数类别的代表性，产生更平衡的子样本，解决了多标签数据集中标签频率差异大和依赖关系的采样问题

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [74] [Fully Decentralized Certified Unlearning](https://arxiv.org/abs/2512.08443)
*Hithem Lamri,Michail Maniatakos*

Main category: cs.LG

TL;DR: 提出RR-DU方法，在去中心化网络中实现可验证的机器遗忘，通过随机游走机制结合梯度操作、噪声注入和信任区域投影，提供收敛性保证和隐私证明。


<details>
  <summary>Details</summary>
Motivation: 现有可验证遗忘研究主要集中在中心化和联邦学习场景，而去中心化网络（无协调器的点对点通信）中的遗忘问题尚未充分探索。需要解决在去中心化环境下响应隐私请求或数据中毒时，如何有效移除特定数据影响的问题。

Method: 提出RR-DU方法：在遗忘客户端对遗忘集执行一步投影梯度上升，在其他节点对保留数据执行几何分布的投影梯度下降，结合子采样高斯噪声和将模型投影到原始模型周围的信任区域。使用随机游走机制在固定拓扑网络中传播更新。

Result: 在凸情况下提供收敛保证，在非凸情况下提供平稳性保证；通过子采样高斯Rényi差分隐私提供(ε,δ)网络遗忘证书；给出与遗忘-本地数据比率相关的删除容量界限。在MNIST和CIFAR-10上，RR-DU在满足相同(ε,δ)的同时，比去中心化差分隐私基线获得更高的测试准确率，并将遗忘准确率降至随机猜测水平（约10%）。

Conclusion: RR-DU是首个在去中心化网络中实现可验证遗忘的方法，通过随机游走机制有效平衡隐私与效用，为去中心化环境下的数据删除提供了理论保证和实用解决方案。

Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,δ)$ network-unlearning certificates on client views via subsampled Gaussian Rényi DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy-utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,δ)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

</details>


### [75] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 提出结合fMRI数据和DICOM元数据的Transformer多模态框架，提升脑状态解码的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法未能充分利用DICOM元数据提供的丰富上下文信息，限制了fMRI数据分析的潜力

Method: 采用基于Transformer的架构，整合fMRI数据和DICOM元数据作为多模态输入，利用注意力机制捕捉复杂的时空模式和上下文关系

Result: 该方法提高了模型的准确性、可解释性和鲁棒性，在临床诊断、认知神经科学和个性化医疗等领域具有应用潜力

Conclusion: 多模态Transformer框架能有效利用DICOM元数据增强fMRI分析，但需解决元数据变异性和计算需求等限制，未来可优化可扩展性和泛化能力

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [76] [Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability](https://arxiv.org/abs/2512.08475)
*Weiqi Guan,Junlin He*

Main category: cs.LG

TL;DR: 论文提出了一种解决GNN中Layer Normalization放置困境的新方法：Post-LN架构通过诱导代数平滑来避免过平滑和深度诅咒，支持更深的网络（达256层）且无需额外参数。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization在GNN中的放置位置与过平滑现象的关系尚未充分探索。Pre-LN架构能避免过平滑但受深度诅咒影响，Post-LN架构能避免深度诅咒但会出现过平滑，需要解决这一困境。

Method: 基于Post-LN架构提出新方法，通过诱导代数平滑来防止过平滑，同时避免深度诅咒。该方法参数高效，无需额外参数。

Result: 在五个基准测试上的实证结果表明，该方法支持更深的网络（最多256层）并提升性能，无需额外参数。

Conclusion: 提出的方法成功解决了Layer Normalization放置的困境，通过Post-LN架构诱导代数平滑，同时避免了过平滑和深度诅咒，使GNN能够构建更深层的网络。

Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.

</details>


### [77] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出一种针对离线强化学习的全局预算分配攻击策略，通过TD误差敏感度分配扰动预算，相比均匀扰动更高效且隐蔽


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用均匀扰动策略，对所有样本无差别处理，效率低下且缺乏隐蔽性，需要更智能的攻击方法

Method: 基于TD误差理论洞察，将攻击建模为全局资源分配问题，推导出在L2约束下按TD误差敏感度比例分配扰动幅度的闭式解

Result: 在D4RL基准测试中显著优于基线方法，仅用最小扰动就实现高达80%的性能下降，并能逃避最先进的统计和频谱防御检测

Conclusion: 全局预算分配攻击策略比均匀扰动更高效隐蔽，揭示了离线RL系统在智能攻击下的脆弱性，对安全防御设计有重要启示

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [78] [Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction](https://arxiv.org/abs/2512.08499)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出了两种基于距离感知不确定性的物理引导神经网络方法（PG-SNGP和PG-SNER），用于旋转机械轴承退化预测，在OOD条件下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有不确定性方法存在置信度校准不足、计算成本高、缺乏距离感知能力、在分布外数据下泛化能力差等问题，而旋转机械轴承等安全关键系统需要准确且具有不确定性感知的退化估计

Method: 1) PG-SNGP：基于谱归一化高斯过程，在隐藏层应用谱归一化保持输入到潜在空间的距离，用高斯过程层替换最终密集层；2) PG-SNER：基于深度证据回归，输出正态逆伽马参数建模不确定性；3) 设计了基于皮尔逊相关系数的新距离感知度量；4) 在损失函数中设计了动态加权方案平衡数据保真度和物理一致性

Result: 在PRONOSTIA轴承退化数据集上测试，与蒙特卡洛和深度集成PGNNs相比，PG-SNGP和PG-SNER提高了预测精度，在OOD条件下可靠泛化，对对抗攻击和噪声保持鲁棒性

Conclusion: 提出的距离感知不确定性方法能够有效解决现有方法的局限性，为安全关键系统的预测性维护提供了更可靠的不确定性估计框架

Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

</details>


### [79] [A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks](https://arxiv.org/abs/2512.08567)
*Nader Sadek,Mirette Moawad,Christina Naguib,Mariam Elzahaby*

Main category: cs.LG

TL;DR: 本文提出一种多模态方法，结合公司新闻和历史股价数据预测股市走势，使用图神经网络（GNN）模型优于传统LSTM基线，新闻标题比全文更具预测性。


<details>
  <summary>Details</summary>
Motivation: 股市预测是金融领域的长期挑战，传统模型主要依赖历史价格数据，但金融新闻能提供有用的外部信号。本文旨在探索结合公司新闻和历史股价数据的多模态方法，以提高预测性能。

Method: 采用多模态方法整合公司新闻文章和历史股票数据。历史数据使用LSTM编码，新闻标题使用语言模型嵌入。这些嵌入构成异构图中的节点，使用GraphSAGE捕捉文章、公司和行业之间的交互关系。比较了GNN模型与基线LSTM模型，评估两个目标：二元方向变化标签和基于显著性的标签。

Result: 在美国股票和Bloomberg数据集上的实验表明，GNN优于LSTM基线，在第一个目标上达到53%准确率，在第二个目标上获得4%的精确度提升。结果还显示，有更多相关新闻的公司预测准确率更高。此外，新闻标题比全文包含更强的预测信号。

Conclusion: 多模态方法结合新闻和历史数据能有效提升股市预测性能，图神经网络能更好地捕捉新闻、公司和行业间的复杂关系。新闻标题比全文在短期市场反应中更具预测价值，简洁的新闻摘要在市场预测中起重要作用。

Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.

</details>


### [80] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 本文构建了覆盖2004-05至2024-25赛季的纵向NBA数据集，并提出了基于LSTM的深度学习框架，通过长达9840场比赛（相当于8个完整赛季）的序列长度来捕捉球队长期动态变化，在NBA比赛结果预测中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 预测NBA比赛结果对于教练策略、球迷参与和体育博彩具有重要意义，但现有模型存在概念漂移、时间上下文有限和跨赛季不稳定的问题，需要更好的长期趋势建模方法。

Method: 构建了覆盖20个赛季的纵向NBA数据集，提出了基于LSTM的深度学习框架，使用长达9840场比赛（8个完整赛季）的序列长度来建模长期性能趋势和赛季间依赖关系。

Result: LSTM模型在所有指标上表现最佳：准确率72.35%，精确率73.15%，AUC-ROC 76.13%，显著优于逻辑回归、随机森林、MLP和CNN等传统机器学习基线模型。

Conclusion: 长序列时间建模对于篮球结果预测至关重要，新构建的多赛季数据集对于开发稳健、可泛化的NBA预测系统具有重要价值，LSTM架构能够有效捕捉球队动态演变和赛季间依赖关系。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [81] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了FedProxGrad算法的收敛性分析，提出DS FedProxGrad框架，证明在Robbins-Monro步长调度下可实现渐近平稳收敛，消除了方差诱导的噪声下限依赖。


<details>
  <summary>Details</summary>
Motivation: 现有FedProxGrad算法在非凸复合优化问题中只能收敛到噪声主导的平稳邻域，其收敛性显式依赖于方差诱导的噪声下限。本文旨在改进这一局限性，提供更优的渐近收敛分析。

Method: 提出DS FedProxGrad（衰减步长FedProxGrad）分析框架，采用Robbins-Monro步长调度策略，允许本地近端解存在一定不精确性，并包含显式公平性正则化。

Result: 在本地不精确性满足温和衰减条件下，证明了liminf期望梯度范数平方为零，即算法实现渐近平稳收敛，且收敛速率不依赖于方差诱导的噪声下限。

Conclusion: DS FedProxGrad框架显著改进了FedProxGrad的收敛性分析，消除了对噪声下限的依赖，为群体公平联邦学习中的非凸复合优化问题提供了更优的理论保证。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [82] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 提出基于动态时间规整和迁移学习的框架，用于增材制造零件认证，通过将低成本聚合物的应力-应变行为知识迁移到金属材料


<details>
  <summary>Details</summary>
Motivation: 增材制造零件认证需要准确预测复杂应力-应变行为，但金属材料数据获取成本高，而聚合物数据相对容易获取，因此希望通过迁移学习将聚合物知识迁移到金属材料

Method: 开发DTW-TL框架：使用动态时间规整选择与目标金属数据集最相关的聚合物源域，然后采用长短期记忆模型进行知识迁移。使用四种聚合物（尼龙、PLA、CF-ABS、树脂）和三种金属（AlSi10Mg、Ti6Al4V、碳钢）验证

Result: DTW-TL框架成功识别聚合物与金属的最佳匹配，选择单一聚合物作为源域。当三种金属作为目标域时，DTW-TL模型获得最低12.41%的平均绝对百分比误差和最高0.96的决定系数，优于无迁移学习的LSTM模型和基于四种聚合物预训练的迁移学习模型

Conclusion: DTW-TL框架能有效实现增材制造零件认证中的知识迁移，通过低成本聚合物数据预测金属材料的应力-应变行为，为零件认证提供高效解决方案

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [83] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS框架自动生成能最大化TTI模型偏见的提示词，通过LLM生成中性提示词，并用属性分类器引导LLM解码过程，发现稳定扩散模型中的微妙偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在社会偏见，但现有去偏见方法依赖人工或LLM生成的提示词数据集，成本高且可能遗漏未预料到的偏见触发提示。

Method: 提出Bias-Guided Prompt Search (BGPS)框架：1) LLM生成属性中性提示词；2) 属性分类器作用于TTI内部表示，引导LLM解码过程，放大感兴趣图像属性的偏见。

Result: 在Stable Diffusion 1.5和先进去偏见模型上发现大量微妙且先前未记录的偏见，严重恶化公平性指标。发现的提示词可解释且可被普通用户使用。

Conclusion: BGPS揭示了TTI模型的脆弱性，扩展了偏见搜索空间，可作为新的偏见缓解评估工具。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [84] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 该论文提出使用神经常微分方程（NODEs）作为动态框架，从多组学时间序列数据中学习蛋白质组和代谢组之间的复杂相互作用，用于预测生物系统行为。


<details>
  <summary>Details</summary>
Motivation: 尽管高通量多组学数据日益丰富，但将这些数据转化为可操作的预测模型仍是瓶颈。需要能够从观测数据中直接推断潜在相互作用的高容量、数据驱动的模拟系统，以支持个性化医疗和合成生物学中的下游干预效果预测。

Method: 引入神经常微分方程（NODEs）作为动态框架，应用于工程化大肠杆菌菌株的时间序列数据，建模代谢途径的连续动力学。

Result: NODE架构在捕捉系统动力学方面优于传统机器学习流程，在柠檬烯和异戊烯醇途径数据集上，均方根误差比基线改善超过90%（分别达94.38%和97.65%）。此外，NODE模型的推理时间加速了1000倍。

Conclusion: NODE模型被确立为可扩展、高保真度的工具，适用于下一代代谢工程和生物学发现，能够高效预测复杂生物系统的行为。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [85] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: LEAP提出了一种新的通用图提示调优方法，通过强化学习选择节点并编辑提示，在保持理论通用性的同时追求更理想的提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有选择性节点图提示调优方法会破坏通用图提示调优的理论基础，需要在保持理论通用性的同时实现更理想的提示效果。

Method: 提出LEAP模型：1) 构建基本通用图提示以保持理论基础；2) 使用actor-critic强化学习选择节点并编辑提示。

Result: 在多种预训练策略下的图级和节点级任务中，无论是全样本还是少样本场景，LEAP都持续优于微调和其他基于提示的方法。

Conclusion: LEAP通过强化学习在保持通用图提示调优理论基础的同时实现了更理想的提示效果，为图提示调优提供了新的有效范式。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [86] [De novo generation of functional terpene synthases using TpsGPT](https://arxiv.org/abs/2512.08772)
*Hamsini Ramanathan,Roman Bushuiev,Matouš Soldát,Jirí Kohout,Téo Hebra,Joshua David Smith,Josef Sivic,Tomáš Pluskal*

Main category: cs.LG

TL;DR: TpsGPT是基于ProtGPT2微调生成的萜烯合酶设计模型，通过多指标验证筛选出功能性酶，实验证明至少两个序列具有TPS活性。


<details>
  <summary>Details</summary>
Motivation: 萜烯合酶是生产抗癌药物等天然产物的关键酶，但传统定向进化方法成本高、速度慢，需要开发更高效的酶设计方法。

Method: 从UniProt挖掘79k个TPS序列微调ProtGPT2构建TpsGPT模型，生成候选序列后通过酶分类、结构置信度、序列多样性、结构比对等多重验证指标筛选。

Result: 从28k生成序列中筛选出7个满足所有验证标准的候选酶，实验验证确认至少两个序列具有TPS酶活性。

Conclusion: 在精心策划的酶类特定数据集上微调蛋白质语言模型，结合严格筛选，能够从头生成功能性的、进化距离较远的酶。

Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.

</details>


### [87] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN将图节点分类问题转化为表格学习问题，通过特征工程提取节点属性、结构特征、位置编码等，在异配图上优于GNNs


<details>
  <summary>Details</summary>
Motivation: 探索图节点分类能否有效转化为表格学习问题，利用TabPFN在表格数据上的成功经验，避免特定图神经网络训练和语言模型依赖

Method: 将图数据转换为表格特征：提取节点属性、结构属性、位置编码，以及可选的平滑邻域特征，然后使用TabPFN进行直接节点分类

Result: 在12个基准数据集上，TabPFN-GN在同配图上与GNNs竞争，在异配图上始终优于GNNs

Conclusion: 原则性特征工程可以弥合表格和图领域之间的差距，为特定任务GNN训练和LLM依赖的图基础模型提供实用替代方案

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [88] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 提出一种基于提升模型的联合反事实分布估计方法，使用双变量beta分布拟合提升分数，无需额外因果假设


<details>
  <summary>Details</summary>
Motivation: 提升建模仅估计干预的因果效应（处理与控制结果的差异），而反事实识别旨在恢复潜在结果的联合分布（如"如果给予营销优惠，该客户是否仍会流失"）。联合反事实分布比提升提供更丰富信息但更难估计，两者具有协同作用

Method: 提出反事实估计器，将双变量beta分布拟合到预测的提升分数上，产生反事实结果的后验分布。该方法仅需提升建模所需的因果假设，无需额外假设

Result: 模拟实验显示方法有效，可应用于电信客户流失问题，揭示标准机器学习或单独提升模型无法获得的洞察

Conclusion: 提升模型可用于反事实估计，提出的方法通过拟合双变量beta分布到提升分数，有效估计联合反事实分布，为因果推断提供更丰富信息

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [89] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: 提出了WAAPO框架，针对AI天气预测模型生成对抗性扰动，既能有效操纵预测结果，又能保持隐蔽性不被检测。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中的依赖度增加，需要评估其对对抗性扰动的脆弱性，以揭示潜在的安全风险。

Method: WAAPO框架通过引入通道稀疏性、空间定位和平滑性约束，生成物理上真实且不易察觉的对抗性扰动。

Result: 在ERA5数据集和FourCastNet上验证了WAAPO能生成与预定目标紧密对齐的对抗性轨迹，即使是在约束条件下。

Conclusion: AI驱动的天气预报模型存在严重脆弱性，微小扰动可导致预测模式显著偏差，需要建立强大的防护机制来防止对抗性攻击。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [90] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: TD(λ)函数逼近在强化学习中存在缺陷，会收敛到次优策略，作者提出了改进的STD(λ)方法，专注于状态值的相对排序而非绝对值误差。


<details>
  <summary>Details</summary>
Motivation: TD(λ)使用函数逼近时，虽然能最小化状态值的平方误差，但对于策略学习来说，状态值的相对排序比绝对值更重要。作者发现TD(λ)即使从最优策略开始，也会收敛到次优策略，这在实际问题（如西洋双陆棋）中是个严重缺陷。

Method: 提出了STD(λ)方法，在二元决策问题中，函数逼近器针对状态值的相对值进行训练。该方法包含理论分析，包括在两状态系统中单调策略改进的证明，并与Bertsekas的差分训练方法进行了比较。

Result: 在两状态系统和倒立摆问题的变体上成功演示了STD(λ)的有效性。STD(λ)能够避免TD(λ)收敛到次优策略的问题，在相对排序方面表现更好。

Conclusion: 对于策略学习，状态值的相对排序比绝对值误差更重要。STD(λ)通过专注于状态值的相对比较，能够获得更好的策略性能，是TD(λ)的有效改进方法。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [91] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出基于加速度二阶损失的文本到IMU运动合成框架，通过微调扩散模型提升IMU数据真实性和HAR性能


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动模型生成的IMU数据与真实IMU记录存在差异，需要专门针对IMU传感器特性的运动合成方法

Method: 在预训练扩散模型中集成加速度二阶损失(L_acc)，通过微调获得IMU特定的运动先验，结合表面建模和虚拟传感器仿真

Result: L_acc损失降低12.7%，高动态活动改进更显著；合成IMU数据分布更接近真实记录；HAR分类性能提升8.7%

Conclusion: 加速度感知的扩散模型细化能有效对齐运动生成和IMU合成，展示了深度学习管道在专门化通用文本到运动先验方面的灵活性

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [92] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: ContextGAN：一种上下文感知的差分隐私生成对抗网络，通过约束矩阵整合领域特定规则，生成既保护隐私又符合领域约束的高质量合成数据。


<details>
  <summary>Details</summary>
Motivation: 大数据应用引发隐私担忧，GDPR和HIPAA等法规对数据处理有严格要求。传统合成数据方法难以捕捉领域中的复杂隐式规则（如医疗领域的处方指南、药物相互作用限制），导致生成的合成数据可能不现实或不适用。

Method: 提出ContextGAN（上下文感知差分隐私生成对抗网络），通过约束矩阵编码显式和隐式领域知识，使用约束感知判别器评估合成数据是否符合领域规则，同时采用差分隐私保护原始数据的敏感信息。

Result: 在医疗、安全和金融领域验证表明，ContextGAN能生成高质量合成数据，既尊重领域规则又保护隐私，相比传统方法显著提高了数据的真实性和实用性。

Conclusion: ContextGAN通过整合领域约束和隐私保护机制，解决了传统合成数据方法忽略隐式规则的问题，适用于需要在严格隐私保证下同时遵循显式模式和隐式规则的应用场景。

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [93] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE：联邦自进化框架，解决LLM智能体在隐私约束下的异构任务优化问题，通过局部进化-全局聚合范式提升跨环境知识迁移


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛应用，但隐私约束限制了集中式优化和跨动态环境的协同进化。现有联邦学习主要针对静态数据集，而智能体的开放式自进化尚未充分探索。直接应用标准联邦学习面临挑战：异构任务和稀疏的轨迹级奖励导致严重的梯度冲突，破坏全局优化稳定性。

Method: 提出Fed-SE联邦自进化框架，采用局部进化-全局聚合范式。局部层面：智能体在过滤后的高回报轨迹上进行参数高效微调，实现稳定的梯度更新。全局层面：在低秩子空间内聚合更新，解耦环境特定动态，有效减少客户端间的负迁移。

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦基线平均任务成功率提升约18%，验证了其在隐私约束部署中实现鲁棒跨环境知识迁移的有效性。

Conclusion: Fed-SE成功解决了LLM智能体在联邦学习环境中的自进化挑战，通过局部稳定优化和全局解耦聚合，实现了在隐私约束下跨异构环境的有效知识迁移和性能提升。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [94] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM生成表格数据时存在隐私风险，会泄露训练数据中的数字模式，本文提出LevAtt攻击方法检测这种泄露，并开发防御策略减少隐私风险同时保持数据质量。


<details>
  <summary>Details</summary>
Motivation: LLM在生成高质量表格合成数据方面表现出色，但现有方法（微调小模型或提示大模型）存在隐私风险，可能泄露训练数据中的数字序列模式，需要系统分析这种风险并开发防御措施。

Method: 提出LevAtt攻击方法：仅基于生成的合成数据，针对数字字符串序列进行成员推理攻击。同时开发两种防御方法，包括一种新颖的采样策略，在生成过程中策略性地扰动数字。

Result: LevAtt攻击在多种模型和数据集上暴露了显著的隐私泄露，在某些情况下对最先进模型实现了完美的成员分类。提出的防御方法能够有效抵御这些攻击，同时最小化合成数据的保真度和效用损失。

Conclusion: LLM基础的合成数据生成存在独特的隐私漏洞，需要有效的防御措施。本文提出的防御策略能够在不显著影响数据质量的情况下保护隐私，为LLM生成表格数据的隐私保护提供了解决方案。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [95] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: DAO-GP是一种新型的漂移感知在线高斯过程模型，能够动态适应数据分布变化，无需手动调整超参数，解决了传统在线GP模型在概念漂移环境下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间演化的数据分布（概念漂移），忽视这一现象会显著降低模型预测精度。传统在线高斯过程模型存在多个关键限制：缺乏漂移感知能力、依赖固定超参数、易受数据窥探影响、缺乏原则性衰减机制以及内存效率低下。

Method: 提出DAO-GP（漂移感知在线高斯过程），这是一种完全自适应、无超参数、带衰减和稀疏的非线性回归模型。该模型内置漂移检测和适应机制，能根据漂移严重程度动态调整模型行为。

Result: 广泛的实证评估证实了DAO-GP在平稳条件、多种漂移类型（突变、增量、渐进）和不同数据特征下的鲁棒性。分析显示其具有动态适应能力、高效的内存和基于衰减的管理机制，以及演化的诱导点。与最先进的参数和非参数模型相比，DAO-GP始终表现出优越或竞争性的性能。

Conclusion: DAO-GP被确立为一种漂移弹性的解决方案，适用于在线非线性回归任务，解决了传统在线GP模型在概念漂移环境中的关键局限性。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [96] [Explainable Anomaly Detection for Industrial IoT Data Streams](https://arxiv.org/abs/2512.08885)
*Ana Rita Paupério,Diogo Risca,Afonso Lourenço,Goreti Marreiros,Ricardo Martins*

Main category: cs.LG

TL;DR: 提出结合无监督异常检测与交互式人机协同学习的数据流挖掘框架，用于工业维护决策支持，并在织布机故障检测中验证


<details>
  <summary>Details</summary>
Motivation: 工业维护在物联网和边缘计算背景下产生连续数据流，需要实时自适应决策，但实际中真实标签往往延迟或缺失，现有数据流挖掘方法大多假设完全监督设置，无法应对这一挑战

Method: 提出协作式数据流挖掘框架，集成无监督异常检测与交互式人机协同学习；使用在线隔离森林算法，通过增量部分依赖图和基于个体条件期望曲线偏离衰减平均的特征重要性评分增强可解释性，支持用户动态调整特征相关性和异常阈值

Result: 在提花织布机单元故障检测中实现实时部署并提供初步结果；框架能够支持用户动态重新评估特征相关性并调整异常阈值

Conclusion: 该框架为工业维护中的实时自适应决策提供了有效解决方案，通过结合无监督异常检测与人机协同学习解决了标签延迟问题；正在进行的工作旨在实现连续监测以预测和解释即将发生的轴承故障

Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

</details>


### [97] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: 该论文提出了一种直接建模基准性能随训练预算扩展的框架，挑战了传统认为下游任务性能难以预测的观点。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型扩展定律主要关注预训练损失等代理指标，而预测下游任务性能被认为不可靠。本文旨在挑战这一观点，建立直接预测下游任务性能的扩展模型。

Method: 提出直接框架，在固定token-参数比的情况下，使用简单的幂律来描述多个下游任务的log准确率扩展行为。引入跨token-参数比的函数形式，并考虑重复采样下的推理计算。

Result: 直接方法比之前提出的两阶段程序具有更好的外推能力，后者容易产生复合误差。在高达17B参数、350B token的两个数据集混合上验证了发现。

Conclusion: 下游任务性能可以通过直接建模训练预算来可靠预测，这挑战了传统观点。为支持可重复性和未来研究，发布了完整的预训练损失和下游评估结果。

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [98] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出一种基于拓扑数据分析的损失函数，用于无监督自动选择核密度估计的最优带宽


<details>
  <summary>Details</summary>
Motivation: 核密度估计在机器学习、贝叶斯推断等领域广泛应用，但带宽选择是关键超参数，需要人工调优。传统方法难以在高维空间中直观评估密度估计的质量，而拓扑数据分析可以量化高维数据的拓扑特征。

Method: 提出一种无监督学习方法，使用基于拓扑的损失函数来自动选择最优带宽。该方法利用拓扑数据分析来量化密度估计的拓扑特征（如连通分量、环、空洞等），通过优化拓扑损失函数来确定最佳带宽参数。

Result: 该方法在不同维度下进行了基准测试，与经典技术相比表现出潜力，能够自动选择出合适的带宽参数。

Conclusion: 基于拓扑的损失函数为核密度估计的带宽选择提供了一种有效的无监督自动化方法，特别适用于高维数据场景。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [99] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge发布了首个聚合物信息学社区基准数据集，包含1万个聚合物和5种性质，通过多任务预测竞赛推动可持续聚合物材料的机器学习发现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在发现可持续聚合物材料方面具有巨大潜力，但进展受到缺乏大规模、高质量、开放访问的聚合物数据集的限制。Open Polymer Challenge旨在填补这一空白。

Method: 发布包含10K聚合物和5种性质（热导率、回转半径、密度、自由体积分数、玻璃化转变温度）的基准数据集。举办多任务聚合物性质预测竞赛，参与者在数据量小、标签不平衡、模拟源异质等现实约束下开发模型，使用特征增强、迁移学习、自监督预训练、针对性集成策略等技术。

Result: 竞赛揭示了数据准备、分布偏移和跨组模拟一致性等方面的重要经验教训，为未来大规模聚合物数据集的最佳实践提供了指导。发布的模型、分析和数据为聚合物科学的分子AI建立了新基础。

Conclusion: Open Polymer Challenge创建的模型、分析和发布的数据为聚合物科学的分子AI建立了新基础，有望加速可持续和节能材料的开发。同时发布了测试数据集和数据生成管道。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [100] [NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge](https://arxiv.org/abs/2512.08089)
*Jebacyril Arockiaraj,Dhruv Parikh,Viktor Prasanna*

Main category: cs.AR

TL;DR: NysX：首个面向边缘设备的端到端FPGA加速器，用于基于Nyström的HDC图分类，通过混合地标选择、流式架构、完美哈希查找和稀疏感知SpMV引擎实现实时高效推理。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的实时、节能图分类应用需求日益增长。基于Nyström核近似的HDC方法在边缘加速面临四大挑战：均匀采样的地标冗余、片上内存有限的投影矩阵存储、昂贵且易冲突的码本查找、以及不规则稀疏性导致的负载不均衡。

Method: 提出NysX加速器，包含四大优化：1) 结合均匀采样与行列式点过程的混合地标选择策略减少冗余；2) 最大化外部内存带宽利用的流式Nyström投影矩阵架构；3) 低片上内存开销的O(1)最小完美哈希查找引擎；4) 静态负载均衡的稀疏感知SpMV引擎。

Result: 在AMD Zynq UltraScale+ FPGA上实现，相比优化CPU（GPU）基线获得6.85倍（4.32倍）加速和169倍（314倍）能效提升，在TUDataset基准测试中平均分类准确率提高3.4%。

Conclusion: NysX通过系统级优化解决了Nyström基HDC图分类在边缘加速的关键挑战，实现了资源受限平台上的实时、高能效推理，同时提升了分类准确率。

Abstract: Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nyström kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nyström projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nyström-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nyström projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\times$ ($4.32\times$) speedup and $169\times$ ($314\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.

</details>
