<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 51]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [LACIN: Linearly Arranged Complete Interconnection Networks](https://arxiv.org/abs/2601.05668)
*Ramón Beivide,Cristóbal Camarero,Carmen Martínez,Enrique Vallejo,Mateo Valero*

Main category: cs.AR

TL;DR: LACIN是一种使用相同索引端口连接交换机的完整图网络实现方法，可简化大规模并行计算网络的布线和路由复杂性


<details>
  <summary>Details</summary>
Motivation: 基于完整图拓扑的网络（如Dragonfly和HyperX）在大型网络中链接数量巨大且快速增长，导致布线复杂和路由困难，需要简化网络部署

Method: 提出LACIN方法，使用相同索引的端口连接交换机来实现完整图网络，通过这种统一的端口索引方式降低网络复杂性

Result: LACIN方法能够简化网络的布线和路由，使得从VLSI系统到最大规模超级计算机的不同规模并行计算网络部署更加容易

Conclusion: LACIN为基于完整图拓扑的大规模网络提供了一种简化的实现方案，能够有效降低网络复杂性和部署难度

Abstract: Several interconnection networks are based on the complete graph topology. Networks with a moderate size can be based on a single complete graph. However, large-scale networks such as Dragonfly and HyperX use, respectively, a hierarchical or a multi-dimensional composition of complete graphs.
  The number of links in these networks is huge and grows rapidly with their size. This paper introduces LACIN, a set of complete graph implementations that use identically indexed ports to link switches. This way of implementing the network reduces the complexity of its cabling and its routing. LACIN eases the deployment of networks for parallel computers of different scales, from VLSI systems to the largest supercomputers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Self-Evolving Distributed Memory Architecture for Scalable AI Systems](https://arxiv.org/abs/2601.05569)
*Zixuan Li,Chuanzhen Wang,Haotian Sun*

Main category: cs.DC

TL;DR: 提出了一种三层自演化分布式内存架构，统一管理计算、通信和部署层的内存，在多个基准测试中显著提升了内存利用率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 分布式AI系统面临跨计算、通信和部署层的关键内存管理挑战：RRAM内存计算存在可扩展性限制，去中心化AI框架在NAT受限网络中内存效率低下，多智能体部署系统应用逻辑与执行环境紧耦合阻碍自适应内存优化。这些问题的根源在于缺乏跨架构层的协调内存管理。

Method: 提出自演化分布式内存架构(SEDMA)，包含三层统一内存管理：1)基于设备特性的内存引导矩阵处理与动态分区；2)考虑网络拓扑和计算容量的内存感知对等节点选择；3)通过持续重配置实现运行时自适应部署优化。框架维护双内存系统，同时跟踪长期性能模式和短期工作负载统计。

Result: 在COCO 2017、ImageNet和SQuAD上的实验表明，该方法达到87.3%的内存利用效率和142.5 ops/s，优于Ray Distributed的72.1%和98.7 ops/s，同时通信延迟降低30.2%至171.2ms，资源利用率提升至82.7%。

Conclusion: 贡献包括：跨三个架构层的协调内存管理、工作负载自适应资源分配、以及支持动态系统优化的双内存架构。该框架解决了分布式AI系统中的关键内存管理挑战。

Abstract: Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficiency across NAT constrained networks due to static routing that ignores computational load. Multi agent deployment systems tightly couple application logic with execution environments, preventing adaptive memory optimization. These challenges stem from a fundamental lack of coordinated memory management across architectural layers. We introduce Self Evolving Distributed Memory Architecture for Scalable AI Systems, a three layer framework that unifies memory management across computation, communication, and deployment. Our approach features (1) memory guided matrix processing with dynamic partitioning based on device characteristics, (2) memory aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. The framework maintains dual memory systems tracking both long term performance patterns and short term workload statistics. Experiments on COCO 2017, ImageNet, and SQuAD show that our method achieves 87.3 percent memory utilization efficiency and 142.5 operations per second compared to Ray Distributed at 72.1 percent and 98.7 operations per second, while reducing communication latency by 30.2 percent to 171.2 milliseconds and improving resource utilization to 82.7 percent. Our contributions include coordinated memory management across three architectural layers, workload adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization.

</details>


### [3] [Performance-Portable Optimization and Analysis of Multiple Right-Hand Sides in a Lattice QCD Solver](https://arxiv.org/abs/2601.05816)
*Shiting Long,Gustavo Ramirez-Hidalgo,Stepan Nassyr,Jose Jimenez-Merchan,Andreas Frommer,Dirk Pleiter*

Main category: cs.DC

TL;DR: 论文扩展了DD-αAMG求解器以支持多右端项，通过优化数据布局和SIMD向量化提升性能，在x86和Arm架构上验证了性能可移植性，并评估了Arm SME指令集的潜力。


<details>
  <summary>Details</summary>
Motivation: 科学计算中稀疏线性系统迭代求解器的高计算成本和内存带宽限制是主要挑战，需要优化数据局部性和数据传输效率。

Method: 扩展DD-αAMG求解器支持多右端项，引入灵活接口支持多种数据布局，实现新的数据布局以优化SIMD向量化，在x86和Arm集群上评估，并探索Arm SME指令集实现。

Result: 在x86和Arm架构上都获得了相似的性能提升，证明了优化方案具有良好的性能可移植性，性能分析揭示了架构约束和编译器行为的复杂性。

Conclusion: 通过多右端项支持、数据布局优化和SIMD向量化，有效提升了DD-αAMG求解器的性能，并在不同架构上实现了性能可移植性，Arm SME指令集显示出潜在优势。

Abstract: Managing the high computational cost of iterative solvers for sparse linear systems is a known challenge in scientific computing. Moreover, scientific applications often face memory bandwidth constraints, making it critical to optimize data locality and enhance the efficiency of data transport. We extend the lattice QCD solver DD-$α$AMG to incorporate multiple right-hand sides (rhs) for both the Wilson-Dirac operator evaluation and the GMRES solver, with and without odd-even preconditioning. To optimize auto-vectorization, we introduce a flexible interface that supports various data layouts and implement a new data layout for better SIMD utilization. We evaluate our optimizations on both x86 and Arm clusters, demonstrating performance portability with similar speedups. A key contribution of this work is the performance analysis of our optimizations, which reveals the complexity introduced by architectural constraints and compiler behavior. Additionally, we explore different implementations leveraging a new matrix instruction set for Arm called SME and provide an early assessment of its potential benefits.

</details>


### [4] [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)
*Yuliang Chen,Xi Lin,Jun Wu,Xiangrui Cai,Qiaolun Zhang,Xichun Fan,Jiapeng Xu,Xiu Su*

Main category: cs.DC

TL;DR: FaST-PT是一个联邦领域泛化框架，通过多模态风格转移进行本地特征增强，使用双提示模块（全局和领域提示）和领域感知提示生成，在分布式环境中实现高效未见领域适应。


<details>
  <summary>Details</summary>
Motivation: 现有联邦领域泛化方法面临跨客户端数据异构性挑战，且通信和计算开销大。需要一种既能处理数据异质性又能降低开销的分布式框架。

Method: 1. 轻量级多模态风格转移：在文本监督下转换图像嵌入，扩展训练数据分布；2. 双提示模块：将提示分解为全局提示（从跨客户端增强嵌入中捕获通用知识）和领域提示（从本地数据中捕获领域特定知识）；3. 领域感知提示生成：自适应为每个样本生成合适提示，通过知识融合促进未见领域适应。

Result: 在PACS和DomainNet等四个跨领域基准数据集上的实验表明，FaST-PT优于FedDG-GA和DiPrompt等最先进的联邦领域泛化方法。消融研究进一步验证了其有效性和效率。

Conclusion: FaST-PT通过本地特征增强和高效未见领域适应，有效解决了联邦领域泛化中的数据异构性和计算开销问题，在多个基准数据集上表现出优越性能。

Abstract: Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze是一个内存高效的MoE训练框架，通过协同设计的系统方法解决MoE架构中的内存瓶颈问题，实现4倍加速和50%内存节省。


<details>
  <summary>Details</summary>
Motivation: 现代大规模MoE架构中的"内存墙"瓶颈被显著放大，MoE固有的架构稀疏性导致稀疏算术计算，同时引入大量激活内存开销（大型令牌路由缓冲区和中间张量缓冲需求）。这种内存压力限制了GPU上可容纳的最大批大小和序列长度，并导致过多的数据移动，阻碍性能和高效模型扩展。

Method: MoEBlaze采用协同设计的系统方法：1）端到端令牌调度和MoE训练方法，通过优化数据结构消除中间缓冲区和激活物化；2）协同设计的内核配合智能激活检查点，在减少内存占用的同时实现更好的性能。

Result: MoEBlaze相比现有MoE框架可以实现超过4倍的加速和超过50%的内存节省。

Conclusion: MoEBlaze通过协同设计的系统方法有效解决了MoE训练中的内存瓶颈问题，显著提升了训练效率和可扩展性。

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [6] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME框架通过引入时间标签和短思考块，让对话模型能根据上下文和时间线索进行简洁的即时推理，大幅减少推理token并提升时间感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的推理设计存在成本高、可审计性差、无法重新触发推理等问题，且对话模型缺乏时间结构感知能力，将不同时间间隔的回复视为等同。

Method: 引入TIME框架：1) 添加ISO 8601时间标签；2) 表示静默间隔的tick turns；3) 可在回复任意位置出现的短<think>块。采用四阶段课程训练，包括小规模、最大多样性的全批次对齐步骤。

Result: 在4B到32B规模的Qwen3模型上，TIME在TIMEBench基准测试中，无论是否开启思考模式，都优于基础Qwen3，同时将推理token减少约一个数量级。

Conclusion: TIME框架成功将显式推理转化为上下文敏感的资源，通过时间感知的对话增强，实现了更高效、更紧凑的推理过程，同时提升了时间相关的对话能力。

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [7] [Ontology Neural Networks for Topologically Conditioned Constraint Satisfaction](https://arxiv.org/abs/2601.05304)
*Jaehong Oh*

Main category: cs.LG

TL;DR: 提出增强型神经符号推理框架，结合拓扑条件与梯度稳定机制，通过Forman-Ricci曲率捕捉图拓扑、Deep Delta Learning实现稳定约束投影、CMA-ES优化参数，在约束满足任务中取得95%成功率，能量均值降至1.15（基线为11.68）。


<details>
  <summary>Details</summary>
Motivation: 神经符号推理系统在保持语义连贯性的同时满足物理和逻辑约束面临根本挑战。需要在基于梯度的优化中融入拓扑结构，而不牺牲可解释性或计算效率。

Method: 1. 使用Forman-Ricci曲率捕捉图拓扑结构；2. 采用Deep Delta Learning实现约束投影中的稳定秩一扰动；3. 应用协方差矩阵自适应进化策略（CMA-ES）进行参数优化；4. 将拓扑条件与梯度稳定机制集成到Ontology Neural Networks框架中。

Result: 1. 平均能量降低至1.15（基线为11.68）；2. 约束满足任务成功率95%；3. 表现出种子独立的收敛性；4. 在最多20个节点的问题上展现出优雅的扩展行为。

Conclusion: 拓扑结构可以有效地指导基于梯度的优化，同时保持可解释性和计算效率。该方法为神经符号推理系统提供了增强的语义连贯性和约束满足能力。

Abstract: Neuro-symbolic reasoning systems face fundamental challenges in maintaining semantic coherence while satisfying physical and logical constraints. Building upon our previous work on Ontology Neural Networks, we present an enhanced framework that integrates topological conditioning with gradient stabilization mechanisms. The approach employs Forman-Ricci curvature to capture graph topology, Deep Delta Learning for stable rank-one perturbations during constraint projection, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization. Experimental evaluation across multiple problem sizes demonstrates that the method achieves mean energy reduction to 1.15 compared to baseline values of 11.68, with 95 percent success rate in constraint satisfaction tasks. The framework exhibits seed-independent convergence and graceful scaling behavior up to twenty-node problems, suggesting that topological structure can inform gradient-based optimization without sacrificing interpretability or computational efficiency.

</details>


### [8] [When the Server Steps In: Calibrated Updates for Fair Federated Learning](https://arxiv.org/abs/2601.05352)
*Tianrun Yu,Kaixiang Zhao,Cheng Zhang,Anjun Gao,Yueyang Quan,Zhuqing Liu,Minghong Fang*

Main category: cs.LG

TL;DR: EquFL是一种新的服务器端去偏方法，通过生成校准更新来减少联邦学习中的偏见，同时保持FedAvg的收敛性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但在不同人口群体间存在公平性问题。现有去偏方法要么需要修改客户端训练协议，要么缺乏灵活的聚合策略。

Method: EquFL是一种服务器端去偏方法，服务器接收客户端模型更新后生成单个校准更新，然后将校准更新与聚合的客户端更新结合，产生调整后的全局模型以减少偏见。

Result: 理论上证明EquFL能收敛到FedAvg达到的最优全局模型，并有效减少训练轮次中的公平性损失。实证表明EquFL显著减轻了系统中的偏见。

Conclusion: EquFL提供了一种有效的服务器端去偏解决方案，既能保持联邦学习的隐私优势，又能改善不同群体间的公平性。

Abstract: Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.

</details>


### [9] [GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting](https://arxiv.org/abs/2601.05353)
*Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GlyRAG：基于LLM上下文提取和检索增强的CGM血糖预测框架，无需额外传感器，显著提升长期血糖预测精度和临床可靠性


<details>
  <summary>Details</summary>
Motivation: 现有血糖预测模型将CGM数据视为数值序列，要么忽略上下文信息，要么依赖难以大规模收集的额外传感器。LLM在时间序列预测中展现潜力，但其在糖尿病护理中作为上下文提取器的角色尚未充分探索。

Method: 提出GlyRAG框架：1) 使用LLM作为上下文提取代理，从CGM轨迹生成临床摘要；2) 通过语言模型嵌入摘要，并与基于patch的血糖表示在多模态Transformer中融合；3) 采用交叉翻译损失对齐文本和生理嵌入；4) 检索模块在嵌入空间识别相似历史事件，通过交叉注意力整合这些案例类比后进行预测。

Result: 在两个T1D队列上的评估显示：1) 相比SOTA方法，RMSE降低高达39%；2) 在基线基础上进一步降低1.7% RMSE；3) 85%的预测位于安全区域；4) 预测血糖异常事件的性能提升51%。

Conclusion: LLM驱动的上下文提取和CGM轨迹检索能够在不依赖额外传感器的情况下，显著提升长期血糖预测的准确性和临床可靠性，为未来糖尿病管理的智能决策支持工具奠定基础。

Abstract: Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.

</details>


### [10] [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)
*Md Shafiqul Islam,Shakti Prasad Padhy,Douglas Allaire,Raymundo Arróyave*

Main category: cs.LG

TL;DR: 提出基于核几何的贝叶斯优化框架，通过多维缩放将离散核库嵌入连续欧几里得流形，实现高效核搜索，在合成基准、时间序列和增材制造案例中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归的性能严重依赖于协方差核的选择，但核选择是概率建模中最具挑战性和计算成本最高的步骤之一。现有方法难以高效探索核空间。

Method: 基于核几何的贝叶斯优化框架：1) 使用基于期望散度的距离度量高斯过程先验之间的差异；2) 通过多维缩放将离散核库嵌入连续欧几里得流形；3) 将核组合作为输入空间，对数边际似然作为目标函数，MDS坐标作为特征化表示。

Result: 在合成基准测试、真实世界时间序列数据集和增材制造案例（预测熔池几何）中，该方法实现了优于基线（包括LLM引导搜索）的预测精度和不确定性校准。

Conclusion: 该框架为核搜索建立了可重用的概率几何结构，对高斯过程建模和深度核学习具有直接相关性，提供了一种高效探索核空间的新方法。

Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.

</details>


### [11] [Inverting Non-Injective Functions with Twin Neural Network Regression](https://arxiv.org/abs/2601.05378)
*Sebastian J. Wetzel*

Main category: cs.LG

TL;DR: 提出使用孪生神经网络回归结合k近邻搜索，为不可逆的非单射函数寻找输入参数的方法


<details>
  <summary>Details</summary>
Motivation: 非单射函数不可逆，但可以通过限制到局部单射的子域来实现可逆性，即使维度不匹配时也能选择优选解

Method: 使用孪生神经网络回归预测已知输入变量的调整量，结合k近邻搜索构建确定性框架

Result: 方法在玩具问题和机器人手臂控制等非单射函数上进行了验证，包括数据定义和数学公式两种情况

Conclusion: 孪生神经网络回归天然具备处理非单射函数可逆性的能力，提出的确定性框架能有效找到给定目标变量的输入参数

Abstract: Non-injective functions are not invertible. However, non-injective functions can be restricted to sub-domains on which they are locally injective and surjective and thus invertible if the dimensionality between input and output spaces are the same. Further, even if the dimensionalities do not match it is often possible to choose a preferred solution from many possible solutions. Twin neural network regression is naturally capable of incorporating these properties to invert non-injective functions. Twin neural network regression is trained to predict adjustments to well known input variables $\mathbf{x}^{\text{anchor}}$ to obtain an estimate for an unknown $\mathbf{x}^{\text{new}}$ under a change of the target variable from $\mathbf{y}^{\text{anchor}}$ to $\mathbf{y}^{\text{new}}$. In combination with k-nearest neighbor search, I propose a deterministic framework that finds input parameters to a given target variable of non-injective functions. The method is demonstrated by inverting non-injective functions describing toy problems and robot arm control that are a) defined by data or b) known as mathematical formula.

</details>


### [12] [Imitation Learning for Combinatorial Optimisation under Uncertainty](https://arxiv.org/abs/2601.05383)
*Prakash Gawas,Antoine Legrain,Louis-Martin Rousseau*

Main category: cs.LG

TL;DR: 本文提出了模仿学习中专家分类的系统框架，包含不确定性处理、最优性水平和交互模式三个维度，并开发了支持多专家查询的通用DAgger算法，在动态医疗分配问题上验证了随机专家和交互学习的优势。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为大规模组合优化问题提供了数据驱动的解决方案，但现有研究中专家构造方法多样且缺乏统一框架，无法系统分析不同专家类型对学习性能的影响。

Method: 提出专家分类的三维框架：1)不确定性处理（近视、确定性、全信息、两阶段随机、多阶段随机）；2)最优性水平（任务最优vs近似）；3)交互模式（一次性监督到迭代交互）。基于此开发了支持多专家查询、专家聚合和灵活交互策略的通用DAgger算法。

Result: 在动态医师-患者分配问题上验证：1)从随机专家学习的策略始终优于确定性或全信息专家；2)交互学习能用更少专家演示获得更高质量解；3)当随机优化计算困难时，聚合确定性专家是有效替代方案。

Conclusion: 系统专家分类框架为模仿学习在组合优化中的应用提供了理论基础，随机专家和交互学习策略能显著提升学习性能，聚合确定性专家在计算受限时是实用替代方案。

Abstract: Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.
  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.
  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.

</details>


### [13] [DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs](https://arxiv.org/abs/2601.05391)
*Namrata Banerji,Tanya Berger-Wolf*

Main category: cs.LG

TL;DR: 提出一种动态图上的端到端多步节点属性预测模型，通过可适应注意力偏置处理动态邻接矩阵，在多个领域优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有时空图神经网络通常假设静态邻接矩阵，无法处理动态变化的图结构，而动态图节点属性预测在金融、生物、社交网络等应用中至关重要。

Method: 基于Transformer的端到端动态边偏置时空模型，将动态邻接矩阵作为可适应注意力偏置注入，采用掩码节点-时间预训练目标、计划采样和水平加权损失来缓解长期预测的误差累积。

Result: 在均方根误差(RMSE)和平均绝对误差(MAE)指标上持续优于强基线方法，能够处理跨样本变化的动态图，适用于多系统设置。

Conclusion: 提出的动态边偏置时空模型能够有效处理动态图结构变化，在多个应用领域的节点属性多步预测任务中表现出色，为动态图预测提供了新方法。

Abstract: Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynamic edge-biased spatiotemporal model that ingests a multi-dimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node-time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon-weighted loss to mitigate compounding error over long horizons. Unlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems. Empirical results demonstrate that our method consistently outperforms strong baselines on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).

</details>


### [14] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT是一个用于多智能体强化学习的知识蒸馏框架，通过分层交互教师机制解决传统KD在MARL中的瓶颈，在复杂合作任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在MARL中存在三个关键瓶颈：1）复杂领域中合成高性能教学策略的挑战；2）教师在分布外状态下的推理困难；3）分散学生与集中教师观测空间不匹配。

Method: 提出HINT框架：1）利用分层RL提供可扩展的高性能教师；2）创新伪离策略RL，使教师策略能同时使用教师和学生经验进行更新；3）应用基于性能的过滤保留结果相关的指导。

Result: 在FireCommander（资源分配）和MARINE（战术战斗）等挑战性合作领域评估，HINT优于基线方法，成功率提升60%到165%。

Conclusion: HINT通过分层交互教师机制有效解决了MARL中知识蒸馏的关键瓶颈，在复杂合作任务中实现了显著的性能提升。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [15] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 本文系统研究了LLM作为评估者时的偏差校正方法，比较了测量误差校正和预测驱动推断两种方法，推导了高效估计器并分析了它们的理论性能。


<details>
  <summary>Details</summary>
Motivation: LLM作为生成AI输出的自动评估者存在系统性误差，需要有效的偏差校正方法来提高评估的准确性。目前主要有两种方法：基于测量误差校正的方法和基于预测驱动推断的方法，但缺乏对这两种方法的系统比较和理论分析。

Method: 利用半参数效率理论工具，推导了基于高效影响函数的有效估计器，统一了两种校正方法。通过理论分析比较了两种方法的渐近方差，并在模拟和真实数据中验证了理论结果。

Result: 理论分析表明，在某些条件下，预测驱动推断方法比测量误差校正方法具有更小的渐近方差。模拟和真实数据实验验证了理论结果的有效性。

Conclusion: 本文为LLM作为评估者的偏差校正提供了理论框架和实用工具，揭示了不同校正方法的性能差异，有助于在实际应用中更准确地使用LLM进行评估。

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [16] [Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion](https://arxiv.org/abs/2601.05431)
*Xiaowen He,Su Jiang,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 基于变分自编码器的数据空间反演框架用于预测CO₂封存项目中的压力、应力、应变场和断层滑移趋势，无需生成后验地质模型


<details>
  <summary>Details</summary>
Motivation: 在CO₂封存等地下作业中，准确评估断层滑移潜力至关重要。传统的基于模型的历史匹配方法在处理含断层的流固耦合问题时面临挑战，需要生成校准到观测数据的后验地质模型，这通常很困难。

Method: 开发了基于变分自编码器的数据空间反演框架：1) 使用地质统计软件生成1000个先验地质模型；2) 使用GEOS进行流固耦合模拟；3) 训练具有堆叠卷积LSTM层的VAE，将压力、应变、有效正应力和剪应力场表示为潜变量；4) 结合监测井的压力和应变观测数据，使用VAE参数化进行DSI后验预测。

Result: DSI-VAE框架能够准确预测压力、应变、应力场和断层滑移趋势，并显著降低关键地质力学和断层参数的不确定性。对合成真实模型的后验结果表明该方法具有良好性能。

Conclusion: 该研究提出的基于VAE的数据空间反演框架为CO₂封存项目中的断层滑移风险评估提供了一种有效方法，避免了传统方法中生成后验地质模型的困难，能够直接从先验模拟结果和观测数据推断后验分布。

Abstract: Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.

</details>


### [17] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://arxiv.org/abs/2601.05451)
*Marko Sterbentz,Kevin Cushing,Cameron Barrie,Kristian J. Hammond*

Main category: cs.LG

TL;DR: RingSQL是一个混合数据生成框架，结合了模式无关查询模板和LLM自然语言问题改写，在保持SQL正确性的同时提供丰富的语言多样性，相比其他合成数据方法在六个文本到SQL基准测试中平均提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL系统的进展受限于高质量训练数据的稀缺性。人工创建数据成本高，现有合成方法在可靠性和可扩展性之间存在权衡：模板方法能确保SQL正确但需要模式特定模板，LLM生成方法可扩展但缺乏质量和正确性保证。

Method: RingSQL采用混合数据生成框架，结合模式无关查询模板（确保SQL正确性）和基于LLM的自然语言问题改写（提供语言多样性）。这种方法能在不同数据库模式中保持SQL正确性，同时提供广泛的语言变化。

Result: 使用RingSQL生成的数据训练的模型，在六个文本到SQL基准测试中相比使用其他合成数据训练的模型，平均准确率提升了2.3%。

Conclusion: RingSQL通过结合模式无关模板和LLM改写的混合方法，解决了文本到SQL数据生成的可靠性和可扩展性权衡问题，显著提升了模型性能，为高质量训练数据生成提供了有效解决方案。

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.

</details>


### [18] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: ALVGL是一种新颖的可微分因果发现增强方法，通过稀疏低秩分解学习数据的精度矩阵，构建包含真实因果图的超结构，从而缩小搜索空间并提高优化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有可微分因果发现方法在处理高维数据或存在潜在混杂因素的数据时面临挑战：搜索空间庞大、目标函数复杂、图论约束非平凡。虽然利用超结构指导优化过程受到关注，但学习适当粒度的超结构并在不同设置下高效实现仍存在显著困难。

Method: ALVGL采用稀疏低秩分解学习数据的精度矩阵，设计ADMM优化过程识别与底层因果结构最相关的精度矩阵成分，将这些成分组合构建超结构（被证明是真实因果图的超集），然后用该超结构初始化标准可微分因果发现方法，聚焦搜索空间。

Result: 在合成和真实数据集上的广泛实验表明，ALVGL不仅达到了最先进的准确性，还显著提高了优化效率。该方法适用于多种结构因果模型（高斯和非高斯设置，有无未测量混杂因素），展现了良好的通用性。

Conclusion: ALVGL为可微分因果发现提供了一种可靠有效的解决方案，通过构建适当的超结构来指导优化过程，在保持高准确性的同时显著提升计算效率，适用于各种实际场景。

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [19] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: MaxCode提出了一种基于推理时搜索的LLM代码优化框架，通过执行反馈和自然语言诊断来指导模型迭代改进代码性能，在CUDA和C++优化任务上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: LLM在通用编码任务上表现出色，但在代码优化方面面临两大挑战：1) 编写优化代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法和特定语言的专业知识；2) 需要解释性能指标（如计时和设备利用率）而不仅仅是二进制正确性。

Method: MaxCode采用推理时搜索算法，在最大奖励强化学习框架下统一现有搜索方法。通过自然语言批判模型将原始执行反馈转换为错误和性能瓶颈的诊断见解，并使用生成式奖励模型对潜在解决方案进行重排序，以改善搜索探索。

Result: 在KernelBench（CUDA）和PIE（C++）优化基准测试中，MaxCode相比基线方法在绝对加速值和相对加速排名上分别实现了20.3%和10.1%的相对改进。

Conclusion: MaxCode通过统一的搜索框架、自然语言反馈转换和改进的探索策略，有效提升了LLM在代码优化任务中的性能，为解决复杂代码优化问题提供了新思路。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [20] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: Hi-ZFO提出分层混合优化框架，结合一阶优化的精确性和零阶优化的探索能力，通过自适应分层策略提升LLM微调效果


<details>
  <summary>Details</summary>
Motivation: 标准一阶优化方法容易使LLM微调陷入尖锐、泛化能力差的局部最小值，而零阶方法虽然探索性强但收敛慢且方差大，特别是在生成任务中输出和搜索空间巨大时问题更严重

Method: Hi-ZFO采用分层混合优化框架：1）通过层重要性分析自适应划分模型；2）对关键层使用精确的一阶梯度更新；3）对不敏感层使用零阶优化作为"有益随机性"来源帮助逃离局部最小值

Result: 在多种生成、数学和代码推理任务上验证，Hi-ZFO始终获得更优性能，同时显著减少训练时间

Conclusion: 分层混合优化框架有效结合了一阶优化的精确性和零阶优化的探索能力，为LLM微调提供了高效解决方案

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [21] [Over-Searching in Search-Augmented Large Language Models](https://arxiv.org/abs/2601.05503)
*Roy Xie,Deepak Gopinath,David Qiu,Dong Lin,Haitian Sun,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 论文系统评估了搜索增强型大语言模型中的过度搜索问题，提出了量化指标TPC，并发布了数据集促进研究


<details>
  <summary>Details</summary>
Motivation: 搜索增强型大语言模型在知识密集型任务中表现出色，但存在过度搜索问题——不必要地调用搜索工具，导致计算效率低下和幻觉问题

Method: 从多个维度系统评估过度搜索：查询类型、模型类别、检索条件、多轮对话；引入Tokens Per Correctness (TPC)量化指标；研究查询和检索层面的缓解方法

Result: 搜索通常提高可回答查询的准确性，但损害不可回答查询的弃权能力；过度搜索在复杂推理模型和深度研究系统中更明显，受噪声检索加剧，在多轮对话中累积；负面证据的存在改善弃权能力

Conclusion: 过度搜索是搜索增强型LLMs的重要问题，需要平衡性能与成本；提出的TPC指标和OverSearchQA数据集将促进高效搜索增强型LLMs的研究

Abstract: Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.

</details>


### [22] [Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management](https://arxiv.org/abs/2601.05521)
*Jiayu Fang,Zhiqi Shao,Haoning Xi,Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: MLA-STNet是一个跨城市事故预防系统，通过多任务学习整合多个城市数据，使用Mamba注意力机制处理时空依赖和城市异质性，在纽约和芝加哥数据上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 城市事故数据具有异质性、报告不一致、稀疏、周期性、噪声等固有特性，加上碎片化治理和不兼容的报告标准，阻碍了跨城市事故预防框架的建立。

Method: 提出MLA-STNet统一系统，将事故风险预测建模为跨城市多任务学习问题。包含两个互补模块：STG-MA（时空地理Mamba注意力）抑制不稳定时空波动并增强长程时间依赖；STS-MA（时空语义Mamba注意力）通过共享参数设计缓解城市异质性，同时保留个体语义表示空间。

Result: 在纽约和芝加哥真实数据集上进行75个实验，在全天和高频事故时段两种预测场景下，相比最先进基线方法，MLA-STNet实现RMSE降低6%、Recall提高8%、MAP提高5%，在50%输入噪声下性能变化小于1%。

Conclusion: MLA-STNet有效统一了异质城市数据集，构建了可扩展、鲁棒且可解释的跨城市事故预防系统，为协调和数据驱动的城市安全管理铺平了道路。

Abstract: The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.

</details>


### [23] [DeMa: Dual-Path Delay-Aware Mamba for Efficient Multivariate Time Series Analysis](https://arxiv.org/abs/2601.05527)
*Rui An,Haohao Qu,Wenqi Fan,Xuequn Shang,Qing Li*

Main category: cs.LG

TL;DR: DeMa：一种双路径延迟感知Mamba骨干网络，用于高效的多变量时间序列分析，通过分离时间动态和变量交互，在保持线性复杂度的同时提升性能


<details>
  <summary>Details</summary>
Motivation: Transformer在多变量时间序列分析中存在二次计算复杂度和高内存开销问题，而直接应用Mamba存在三个关键限制：缺乏显式跨变量建模、难以分离时间动态与变量交互、对潜在时间滞后效应建模不足

Method: 提出DeMa双路径延迟感知Mamba骨干网络：1）将MTS分解为时间动态和变量交互；2）时间路径使用Mamba-SSD模块捕获单变量长程动态；3）变量路径使用Mamba-DALA模块集成延迟感知线性注意力建模跨变量依赖

Result: 在五个代表性任务（长短期预测、数据插补、异常检测、序列分类）上实现最先进性能，同时保持卓越的计算效率

Conclusion: DeMa在保持Mamba线性复杂度优势的同时，显著提升了多变量时间序列建模的适用性，为高效MTS分析提供了有前景的解决方案

Abstract: Accurate and efficient multivariate time series (MTS) analysis is increasingly critical for a wide range of intelligent applications. Within this realm, Transformers have emerged as the predominant architecture due to their strong ability to capture pairwise dependencies. However, Transformer-based models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment in long-term and large-scale MTS modeling. Recently, Mamba has emerged as a promising linear-time alternative with high expressiveness. Nevertheless, directly applying vanilla Mamba to MTS remains suboptimal due to three key limitations: (i) the lack of explicit cross-variate modeling, (ii) difficulty in disentangling the entangled intra-series temporal dynamics and inter-series interactions, and (iii) insufficient modeling of latent time-lag interaction effects. These issues constrain its effectiveness across diverse MTS tasks. To address these challenges, we propose DeMa, a dual-path delay-aware Mamba backbone. DeMa preserves Mamba's linear-complexity advantage while substantially improving its suitability for MTS settings. Specifically, DeMa introduces three key innovations: (i) it decomposes the MTS into intra-series temporal dynamics and inter-series interactions; (ii) it develops a temporal path with a Mamba-SSD module to capture long-range dynamics within each individual series, enabling series-independent, parallel computation; and (iii) it designs a variate path with a Mamba-DALA module that integrates delay-aware linear attention to model cross-variate dependencies. Extensive experiments on five representative tasks, long- and short-term forecasting, data imputation, anomaly detection, and series classification, demonstrate that DeMa achieves state-of-the-art performance while delivering remarkable computational efficiency.

</details>


### [24] [Scalable Heterogeneous Graph Learning via Heterogeneous-aware Orthogonal Prototype Experts](https://arxiv.org/abs/2601.05537)
*Wei Zhou,Hong Huang,Ruize Shi,Bang Liu*

Main category: cs.LG

TL;DR: 提出HOPE框架解决异质图神经网络中的线性投影瓶颈问题，通过原型路由和专家正交化提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有HGNNs的解码/投影阶段仍使用单一共享线性头，无法处理异质图中的上下文多样性和长尾分布，导致全局头部错过细粒度语义、过拟合中心节点、忽视尾部节点

Method: 提出HOPE框架：1) 使用可学习的基于原型的路由机制，通过相似度将实例分配给专家；2) 添加专家正交化以鼓励多样性并防止专家崩溃；3) 作为即插即用模块替代标准预测头

Result: 在四个真实数据集上的实验表明，HOPE在多种SOTA HGNN骨干网络上均取得一致性能提升，且开销极小

Conclusion: HOPE框架有效解决了异质图神经网络中的线性投影瓶颈问题，通过原型路由和专家正交化机制显著提升了模型对异质图数据的处理能力

Abstract: Heterogeneous Graph Neural Networks(HGNNs) have advanced mainly through better encoders, yet their decoding/projection stage still relies on a single shared linear head, assuming it can map rich node embeddings to labels. We call this the Linear Projection Bottleneck: in heterogeneous graphs, contextual diversity and long-tail shifts make a global head miss fine semantics, overfit hub nodes, and underserve tail nodes. While Mixture-of-Experts(MoE) could help, naively applying it clashes with structural imbalance and risks expert collapse. We propose a Heterogeneous-aware Orthogonal Prototype Experts framework named HOPE, a plug-and-play replacement for the standard prediction head. HOPE uses learnable prototype-based routing to assign instances to experts by similarity, letting expert usage follow the natural long-tail distribution, and adds expert orthogonalization to encourage diversity and prevent collapse. Experiments on four real datasets show consistent gains across SOTA HGNN backbones with minimal overhead.

</details>


### [25] [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)
*Moe Shiina,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出一种基于混合整数线性优化的评分系统构建方法，直接最大化缓冲AUC（bAUC）作为AUC的最紧凹下界，相比传统正则化和逐步回归方法能获得更好的AUC性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于混合整数优化的评分系统构建方法没有直接最大化AUC这一重要评估指标，而评分系统作为高度可解释的分类器，需要同时保证预测性能和可解释性。

Method: 建立混合整数线性优化（MILO）框架，最大化缓冲AUC（bAUC）作为AUC的最紧凹下界，并加入组稀疏约束来限制评分系统中的问题数量。

Result: 在公开真实数据集上的计算实验表明，该MILO方法构建的评分系统相比基于正则化和逐步回归的基线方法具有更优的AUC值。

Conclusion: 该研究推动了混合整数优化技术在开发高度可解释分类模型方面的进展，为构建同时具备良好预测性能和可解释性的评分系统提供了有效框架。

Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.

</details>


### [26] [Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow](https://arxiv.org/abs/2601.05583)
*Xue Feng,Li Wang,Deanna Needell,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出一种自监督学习方法，无需数值求解JKO子问题即可学习JKO解算子，通过交替轨迹生成和算子更新的Learn-to-Evolve算法，高效计算Wasserstein梯度流。


<details>
  <summary>Details</summary>
Motivation: JKO方案为计算Wasserstein梯度流提供了稳定的变分框架，但实际应用中由于需要反复求解JKO子问题而计算成本高昂，限制了其实际使用。

Method: 提出自监督方法学习JKO解算子，无需任何JKO轨迹的数值解。引入Learn-to-Evolve算法，通过交替进行轨迹生成和算子更新，联合学习JKO算子及其诱导的轨迹。

Result: 数值实验表明该方法在各种能量函数和初始条件下都具有准确性、稳定性和鲁棒性。学习到的算子能够将输入密度直接映射到对应JKO子问题的最小化解。

Conclusion: 提出的Learn-to-Evolve方法通过自监督学习有效解决了JKO方案的高计算成本问题，同时通过数据增强提高了学习算子的泛化能力，为Wasserstein梯度流计算提供了高效实用的解决方案。

Abstract: The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.

</details>


### [27] [Poisson Hyperplane Processes with Rectified Linear Units](https://arxiv.org/abs/2601.05586)
*Shufei Ge,Shijia Wang,Lloyd Elliott*

Main category: cs.LG

TL;DR: 该论文建立了泊松超平面过程与两层ReLU神经网络之间的理论联系，提出了一种基于PHP的替代概率表示，并通过退火序列蒙特卡洛算法进行贝叶斯推断，实验证明优于经典两层ReLU网络。


<details>
  <summary>Details</summary>
Motivation: ReLU激活函数在神经网络中广泛使用，但缺乏理论上的概率解释。本文旨在建立泊松超平面过程与两层ReLU神经网络之间的理论联系，提供一种替代的概率表示方法。

Method: 1. 建立泊松超平面过程与两层ReLU神经网络的数学等价关系；2. 通过分解命题实现PHP构建的神经网络在大规模问题上的可扩展性；3. 提出退火序列蒙特卡洛算法进行贝叶斯推断。

Result: 数值实验表明，提出的基于PHP的神经网络模型在性能上优于经典的两层ReLU神经网络。代码已在GitHub开源。

Conclusion: 泊松超平面过程为两层ReLU神经网络提供了有效的概率表示框架，通过分解命题实现可扩展性，结合退火序列蒙特卡洛算法在贝叶斯推断中表现出优越性能。

Abstract: Neural networks have shown state-of-the-art performances in various classification and regression tasks. Rectified linear units (ReLU) are often used as activation functions for the hidden layers in a neural network model. In this article, we establish the connection between the Poisson hyperplane processes (PHP) and two-layer ReLU neural networks. We show that the PHP with a Gaussian prior is an alternative probabilistic representation to a two-layer ReLU neural network. In addition, we show that a two-layer neural network constructed by PHP is scalable to large-scale problems via the decomposition propositions. Finally, we propose an annealed sequential Monte Carlo algorithm for Bayesian inference. Our numerical experiments demonstrate that our proposed method outperforms the classic two-layer ReLU neural network. The implementation of our proposed model is available at https://github.com/ShufeiGe/Pois_Relu.git.

</details>


### [28] [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)
*Jingcheng Hu,Yinmin Zhang,Shijie Shang,Xiaobo Yang,Yue Peng,Zhewei Huang,Hebin Zhou,Xin Wu,Jie Cheng,Fanqi Wan,Xiangwen Kong,Chengyuan Yao,Kaiwen Yan,Ailin Huang,Hongyu Zhou,Qi Han,Zheng Ge,Daxin Jiang,Xiangyu Zhang,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: PaCoRe是一个并行协调推理框架，通过多轮消息传递架构实现大规模并行探索，突破语言模型测试时计算受限于固定上下文窗口的问题，显著提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在核心限制：无法在固定上下文窗口下将测试时计算（TTC）扩展到远超顺序推理的程度，这限制了模型的推理能力和性能提升。

Method: 提出并行协调推理（PaCoRe）框架，采用多轮消息传递架构：每轮启动多个并行推理轨迹，将发现压缩为上下文有界的消息，合成这些消息来指导下一轮并最终生成答案。通过大规模、基于结果的强化学习进行端到端训练。

Result: PaCoRe在多个领域带来显著改进，特别是在数学推理方面：一个8B参数模型在HMMT 2025上达到94.5%，超过GPT-5的93.2%，有效TTC扩展到约200万tokens而不超出上下文限制。

Conclusion: PaCoRe成功突破了语言模型测试时计算受限于固定上下文窗口的瓶颈，通过并行协调推理实现了多百万token级别的有效TTC，显著提升了推理能力，特别是在数学领域超越了前沿系统。

Abstract: We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.

</details>


### [29] [Good Allocations from Bad Estimates](https://arxiv.org/abs/2601.05597)
*Sílvia Casacuberta,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出了一种更高效的CATE估计方法，仅需O(M/ε)样本即可达到与标准方法相同的治疗效果，通过粗粒度估计实现近乎最优的治疗分配。


<details>
  <summary>Details</summary>
Motivation: 传统CATE估计方法需要O(M/ε²)样本才能准确估计治疗效果，但实际治疗分配并不需要如此精确的估计。本文旨在探索治疗分配所需的更少样本量，揭示治疗效果估计与治疗分配之间的根本区别。

Method: 提出一种新算法，利用治疗效应的自然分布特性，通过粗粒度估计实现近乎最优的治疗分配。关键洞察是：对于治疗分配而言，粗略估计就足以实现接近最优的效果。此外，还考虑了预算灵活性对样本复杂度的进一步降低。

Result: 算法在多个真实世界RCT数据集上评估，均能以极少的样本找到近乎最优的治疗分配。证明了治疗分配所需的样本量远少于治疗效果估计，仅需O(M/ε)样本即可达到与标准CATE方法相同的总治疗效果。

Conclusion: 本文揭示了治疗效果估计与治疗分配之间的根本区别：后者所需的样本量远少于前者。通过粗粒度估计和预算灵活性，可以显著降低样本复杂度，为实际医疗决策提供更高效的资源分配方法。

Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.

</details>


### [30] [Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR](https://arxiv.org/abs/2601.05607)
*Zijun Min,Bingshuai Liu,Ante Wang,Long Zhang,Anxiang Zeng,Haibo Zhang,Jinsong Su*

Main category: cs.LG

TL;DR: 提出DHPO方法，结合GRPO的细粒度信用分配和GSPO的序列级稳定性，通过动态混合token级和序列级重要性比率来优化语言模型推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法各有优缺点：GRPO使用token级重要性比率保持细粒度信用分配但方差高不稳定；GSPO使用序列级重要性比率匹配序列级奖励但牺牲token级信用分配。需要结合两者优势。

Method: 提出动态混合策略优化(DHPO)，在单个裁剪代理目标中结合token级和序列级重要性比率。探索两种混合机制：平均混合和熵引导混合。采用分支特定裁剪策略，在混合前分别约束两个分支的重要性比率。

Result: 在七个数学推理基准测试中，对Qwen3系列的密集模型和MoE模型进行实验，DHPO始终优于GRPO和GSPO。

Conclusion: DHPO成功结合了GRPO和GSPO的优势，通过动态混合token级和序列级重要性比率，在保持细粒度信用分配的同时提高训练稳定性，在数学推理任务中表现优异。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.

</details>


### [31] [PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes](https://arxiv.org/abs/2601.05613)
*Yiming Zhou,Mingyue Cheng,Hao Wang,Enhong Chen*

Main category: cs.LG

TL;DR: PiXTime是一个用于联邦学习的时间序列预测模型，能够处理多粒度、异构变量集的时间序列数据，通过个性化补丁嵌入和全局变量嵌入表实现跨节点有效预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据价值高但难以跨节点共享，联邦学习是很有前景的范式。然而，不同节点的采样标准导致时间粒度和变量集存在差异，阻碍了经典联邦学习的应用。

Method: 1) 使用个性化补丁嵌入将节点特定粒度的时间序列映射到统一维度的token序列；2) 使用全局变量嵌入表对齐跨节点的变量类别语义；3) 基于Transformer的共享模型捕获任意数量变量的辅助序列表示，并通过交叉注意力增强目标序列预测。

Result: 实验表明PiXTime在联邦学习设置中达到最先进的性能，并在八个广泛使用的真实世界传统基准测试中表现出优越性能。

Conclusion: PiXTime成功解决了联邦学习中时间序列的多粒度和异构变量集问题，实现了有效的跨节点预测，为分布式时间序列分析提供了新的解决方案。

Abstract: Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.

</details>


### [32] [Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks](https://arxiv.org/abs/2601.05616)
*ShaoZhen Liu,Xinting Huang,Houwen Peng,Xin Chen,Xinyang Song,Qi Li,Zhenan Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种新的两阶段训练框架，通过自生成长链思维数据来增强大语言模型的自我纠正能力，在数学推理任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖强化学习框架，忽视了监督微调方法在激活大语言模型内在推理能力方面的潜力。需要探索更资源高效的方法来优化复杂推理任务。

Method: 采用两阶段训练框架：第一阶段通过多轮对话策略生成包含验证、回溯、子目标分解和反向推理的思维链数据，并用预定义规则筛选高质量样本进行监督微调；第二阶段使用难度感知拒绝采样机制动态优化数据分布，增强模型处理复杂问题的能力。

Result: 实验结果显示在GSM8K和MATH500等数学基准测试上性能提升，微调后的模型在AIME24等竞赛级问题上取得显著改进，生成的推理链长度扩展超过4倍同时保持强可扩展性。

Conclusion: 监督微调能有效激活大语言模型的内在推理能力，为复杂任务优化提供了资源高效的途径，证明了自生成思维链数据在增强模型自我纠正能力方面的有效性。

Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.

</details>


### [33] [Continual Learning of Achieving Forgetting-free and Positive Knowledge Transfer](https://arxiv.org/abs/2601.05623)
*Zhi Wang,Zhongbin Wu,Yanni Li,Bing Liu,Guangxi Li,Yuping Wang*

Main category: cs.LG

TL;DR: 提出ETCL方法，在持续学习中同时解决灾难性遗忘问题并促进正向的前向和后向知识迁移，通过任务特定掩码、梯度对齐和双目标优化实现无遗忘的正向知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习研究主要关注解决灾难性遗忘问题，但理想的持续学习智能体不仅应克服遗忘，还应促进正向的前向知识迁移（用先前任务知识学习新任务）和后向知识迁移（用新任务知识改进先前任务性能）。

Method: 提出ETCL方法：1) 学习任务特定二进制掩码为每个任务隔离稀疏子网络；2) 新任务学习开始时，对齐新任务梯度与先前最相似任务的子网络梯度以确保正向FKT；3) 使用双目标优化策略和正交梯度投影方法，仅更新先前相似任务在分类层的权重以实现正向BKT；4) 理论估计导致负向FKT和BKT的边界，并基于此提出在线任务相似性检测策略。

Result: 在不相似、相似和混合任务序列上的广泛评估表明，ETCL显著优于强基线方法，实现了无遗忘和正向知识迁移。

Conclusion: ETCL方法成功解决了持续学习中的灾难性遗忘问题，同时促进了正向的前向和后向知识迁移，通过任务特定掩码、梯度对齐和双目标优化策略实现了优异的性能。

Abstract: Existing research on continual learning (CL) of a sequence of tasks focuses mainly on dealing with catastrophic forgetting (CF) to balance the learning plasticity of new tasks and the memory stability of old tasks. However, an ideal CL agent should not only be able to overcome CF, but also encourage positive forward and backward knowledge transfer (KT), i.e., using the learned knowledge from previous tasks for the new task learning (namely FKT), and improving the previous tasks' performance with the knowledge of the new task (namely BKT). To this end, this paper first models CL as an optimization problem in which each sequential learning task aims to achieve its optimal performance under the constraint that both FKT and BKT should be positive. It then proposes a novel Enhanced Task Continual Learning (ETCL) method, which achieves forgetting-free and positive KT. Furthermore, the bounds that can lead to negative FKT and BKT are estimated theoretically. Based on the bounds, a new strategy for online task similarity detection is also proposed to facilitate positive KT. To overcome CF, ETCL learns a set of task-specific binary masks to isolate a sparse sub-network for each task while preserving the performance of a dense network for the task. At the beginning of a new task learning, ETCL tries to align the new task's gradient with that of the sub-network of the previous most similar task to ensure positive FKT. By using a new bi-objective optimization strategy and an orthogonal gradient projection method, ETCL updates only the weights of previous similar tasks at the classification layer to achieve positive BKT. Extensive evaluations demonstrate that the proposed ETCL markedly outperforms strong baselines on dissimilar, similar, and mixed task sequences.

</details>


### [34] [Transformer Is Inherently a Causal Learner](https://arxiv.org/abs/2601.05647)
*Xinyue Wang,Stephen Wang,Biwei Huang*

Main category: cs.LG

TL;DR: Transformer在自回归训练中自然学习到时延因果结构，其梯度敏感性可直接恢复因果图，无需显式因果目标


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在非线性、长时依赖和非平稳系统上面临挑战，而Transformer作为基础模型可能自然编码因果结构

Method: 利用自回归训练的Transformer，通过聚合梯度归因提取因果结构，无需显式因果目标或结构约束

Result: 该方法在非线性动态、长时依赖和非平稳系统上大幅超越现有因果发现算法，且随数据量和异质性增加性能提升

Conclusion: 为未来范式奠定基础：基础模型通过因果视角获得可解释性和增强，因果发现通过基础模型视角进行

Abstract: We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.

</details>


### [35] [From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation](https://arxiv.org/abs/2601.05650)
*Miguel Matey-Sanz,Joaquín Torres-Sospedra,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 提出基于聚类的Wi-Fi指纹定位方法，通过聚类预处理指纹数据，在定位阶段先进行聚类估计，再在选定簇内进行定位，提高定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi指纹定位面临数据集规模与异质性、RSSI信号波动、多楼层环境模糊性等挑战，导致定位精度下降，特别是全局模型未考虑结构约束时。

Method: 采用聚类方法在定位前结构化指纹数据集，基于空间或无线特征进行分组，可在建筑或楼层级别应用聚类。定位阶段基于最强接入点的聚类估计将新指纹分配到最相关簇，仅在选定簇内进行定位。

Result: 在三个公共数据集和多个机器学习模型上评估，结果显示定位误差持续降低，特别是在建筑级策略下，但以降低楼层检测精度为代价。

Conclusion: 通过聚类显式结构化数据集是扩展室内定位的有效灵活方法，能显著提升定位精度。

Abstract: Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.

</details>


### [36] [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)
*George Ma,Zhongyuan Liang,Irene Y. Chen,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAEs）通过对比激活方法识别的"推理特征"主要捕捉的是推理的语言相关物，而非真正的推理计算过程。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器（SAEs）是否真的能识别大型语言模型（LLMs）中的真实推理特征，而不是仅仅捕捉与推理相关的表面语言模式。

Method: 采用证伪导向框架：1）因果令牌注入实验 - 将特征相关的令牌注入非推理文本；2）LLM引导的证伪 - 生成能激活特征的非推理输入和不激活特征的推理输入。在20个配置（多个模型家族、层和推理数据集）上进行测试。

Result: 1）59%-94%的特征对令牌级干预高度敏感，少量特征相关令牌注入非推理文本就能引发强激活；2）剩余特征中，LLM引导证伪总能找到激活特征的非推理输入和不激活特征的推理输入；3）没有特征满足真实推理行为标准；4）操控这些特征对基准性能影响很小或略有下降。

Conclusion: SAE通过对比方法识别的特征主要捕捉的是推理的语言相关物，而非底层的推理计算本身，这对当前特征解释方法提出了重要挑战。

Abstract: We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.

</details>


### [37] [AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces](https://arxiv.org/abs/2601.05680)
*Yeonsang Shin,Insoo Kim,Bongkeun Kim,Keonwoo Bae,Bohyung Han*

Main category: cs.LG

TL;DR: 提出AGDC框架，通过结合分类预测和扩散模型联合建模离散与连续值，解决Transformer自回归模型在生成高精度混合序列时的精度限制问题，并在半导体布局等场景验证有效性。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的自回归模型在数据生成方面表现出色，但依赖离散化token限制了其表示连续值的高精度能力。现有离散化方法在生成混合离散-连续序列（如半导体电路设计）时存在可扩展性限制，精度损失可能导致功能失效。

Method: 提出AGDC统一框架，采用混合方法：对离散值使用分类预测，对连续值使用基于扩散的建模。包含两个关键技术组件：1) 基于MLP的动态EOS logit调整机制，根据序列上下文调整EOS token logits；2) 集成到损失函数中的长度正则化项。还提出了ContLayNet大规模基准数据集。

Result: 在半导体布局（ContLayNet）、图形布局和SVG上的实验表明，AGDC在生成高保真混合向量表示方面优于离散化方法和固定模式基线，实现了跨领域可扩展的高精度生成。

Conclusion: AGDC框架通过联合建模离散和连续值，有效解决了自回归模型在高精度混合序列生成中的精度限制问题，为半导体设计等高精度领域提供了可扩展的解决方案。

Abstract: Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.

</details>


### [38] [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)
*Hongyaoxing Gul,Lijuan Hu,Shuzi Niu,Fangfang Liu*

Main category: cs.LG

TL;DR: FLRQ提出了一种灵活的低秩量化方法，通过快速识别各层最优秩并聚合来实现最小存储组合，在量化质量和算法效率上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统低秩PTQ方法需要昂贵的微调来确定不同数据和层的最佳秩，且SVD计算开销大，未能充分利用大模型的潜力。

Method: 包含两个核心组件：R1-FLR（基于R1-Sketch的灵活秩选择，使用高斯投影快速低秩近似）和BLC（在缩放和裁剪策略下通过迭代方法最小化低秩量化误差）。

Result: 在综合实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率上都达到了最先进的性能。

Conclusion: FLRQ通过快速识别各层最优秩并聚合，实现了高效的低秩量化，解决了传统方法需要昂贵微调和计算开销大的问题。

Abstract: Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.

</details>


### [39] [mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations](https://arxiv.org/abs/2601.05732)
*Yongyi Yang,Jianyang Gao*

Main category: cs.LG

TL;DR: mHC-lite：通过将双随机矩阵显式构造为置换矩阵的凸组合，解决了mHC中Sinkhorn-Knopp归一化的近似误差和工程复杂性，实现了精确的双随机性保证和更好的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有mHC方法存在两个问题：(1)有限次Sinkhorn-Knopp迭代无法保证精确的双随机性，近似误差会在网络深度中累积并影响稳定性；(2)高效的SK实现需要专门的CUDA内核，工程门槛高且可移植性差。

Method: 基于Birkhoff-von Neumann定理，提出mHC-lite方法，通过将双随机矩阵显式构造为置换矩阵的凸组合，确保精确的双随机性。该方法仅使用原生矩阵操作即可实现，无需专门的CUDA内核。

Result: 大量实验表明，mHC-lite在性能上匹配或超越mHC，同时通过简单实现获得更高的训练吞吐量，并消除了HC和mHC中观察到的残余不稳定性。

Conclusion: mHC-lite提供了一种简单有效的解决方案，既保证了精确的双随机性，又降低了工程复杂性，在保持或提升性能的同时显著提高了训练稳定性和效率。

Abstract: Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.

</details>


### [40] [Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms](https://arxiv.org/abs/2601.05759)
*Turkan Simge Ispak,Salih Tileylioglu,Erdem Akagunduz*

Main category: cs.LG

TL;DR: 本研究将P波检测重构为自监督异常检测任务，通过492种VAE配置的网格搜索发现：注意力机制优于跳跃连接，在0-40公里近源范围AUC达0.91，更适合地震预警应用。


<details>
  <summary>Details</summary>
Motivation: 强震记录中的P波检测面临高噪声、标记数据有限和复杂波形特征的挑战，这对地震早期预警至关重要。研究旨在探索如何通过架构设计平衡重建保真度与异常检测能力。

Method: 将P波到达检测重构为自监督异常检测任务，通过492种变分自编码器配置的网格搜索，比较跳跃连接和注意力机制等不同架构对重建误差和检测性能的影响。

Result: 跳跃连接最小化重建误差（MAE约0.0012）但导致"过度泛化"，会重建噪声而掩盖检测信号；注意力机制优先全局上下文而非局部细节，获得最高检测性能（AUC 0.875），在0-40公里近源范围AUC达0.91。

Conclusion: 优先全局上下文而非像素完美重建的架构约束对于鲁棒的自监督P波检测至关重要，注意力机制的VAE在近源范围表现优异，非常适合即时早期预警应用。

Abstract: Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.

</details>


### [41] [Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer](https://arxiv.org/abs/2601.05770)
*Yifan Zhang,Wei Bi,Kechi Zhang,Dongming Jin,Jie Fu,Zhi Jin*

Main category: cs.LG

TL;DR: 提出Discrete Transformer架构，通过功能解耦和温度退火采样，从连续表示中提取离散符号逻辑，实现无需演示的算法发现。


<details>
  <summary>Details</summary>
Motivation: Transformer在算法提取中存在叠加问题，纠缠的特征编码阻碍了符号表达式的提取。需要弥合连续表示与离散符号逻辑之间的鸿沟。

Method: 设计Discrete Transformer架构，强制功能解耦：数值注意力仅负责信息路由，数值MLP仅处理元素级算术运算。采用温度退火采样促进可读程序提取。

Result: 性能与RNN基线相当，但将可解释性扩展到连续变量域。退火过程显示从探索到利用的清晰相变，可通过归纳偏置精细控制合成程序。

Conclusion: Discrete Transformer为无需演示的算法发现提供了稳健框架，为Transformer可解释性提供了严格途径。

Abstract: Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.

</details>


### [42] [Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning](https://arxiv.org/abs/2601.05792)
*Manel Gil-Sorribes,Júlia Vilalta-Mor,Isaac Filella-Mercè,Robert Soliva,Álvaro Ciudad,Víctor Guallar,Alexis Molina*

Main category: cs.LG

TL;DR: Tensor-DTI：基于对比学习的多模态药物-靶点相互作用预测框架，整合分子图、蛋白质语言模型和结合位点信息，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有药物-靶点相互作用预测模型通常依赖单模态预定义分子描述符或序列嵌入，表征能力有限。需要整合多模态信息来改进相互作用建模。

Method: 提出Tensor-DTI对比学习框架，整合分子图、蛋白质语言模型和结合位点预测的多模态嵌入。采用孪生双编码器架构，捕捉化学和结构相互作用特征，区分相互作用与非相互作用对。

Result: 在多个DTI基准测试中优于现有序列和基于图的方法。在CDK2的十亿级化学库大规模推理实验中，即使训练时未包含CDK2，也能产生化学合理的命中分布。在富集研究中与Glide对接和Boltz-2共折叠器相比保持竞争力，并在严格家族保留分割下改善外家族靶点的筛选预算。

Conclusion: 整合多模态信息与对比学习目标能提高相互作用预测准确性，为虚拟筛选提供更可解释和可靠性感知的模型。该框架还可应用于蛋白质-RNA和肽-蛋白质相互作用。

Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.

</details>


### [43] [Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers](https://arxiv.org/abs/2601.05807)
*Mohamed Amine Hallam,Kuo-Kun Tseng*

Main category: cs.LG

TL;DR: 研究位置编码与词嵌入的融合机制对Transformer性能的影响，发现在长序列任务中融合策略选择对性能有显著影响


<details>
  <summary>Details</summary>
Motivation: 大多数现有研究关注设计新的位置编码，而忽略了位置信息与词嵌入的融合机制本身可能影响性能，特别是在长序列场景下

Method: 在相同Transformer架构、数据划分和随机种子下，对比三种经典融合策略：逐元素加法、拼接加投影、标量门控融合；在短、中、长三种序列长度的文本分类数据集上进行实验；使用配对种子分析和跨数据集比较验证结果稳定性

Result: 融合选择对短文本影响可忽略，但在长文档上产生一致性能提升；可学习的融合机制在不同位置编码家族中都能带来收益；轻量级卷积门控机制在长文档上引入局部归纳偏置

Conclusion: 位置编码融合是长序列Transformer中重要的设计选择，应作为明确的建模决策而非固定默认设置

Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.

</details>


### [44] [Learning Reconstructive Embeddings in Reproducing Kernel Hilbert Spaces via the Representer Theorem](https://arxiv.org/abs/2601.05811)
*Enrique Feito-Casares,Francisco M. Melgarejo-Meseguer,José-Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: 该论文提出了一种基于RKHS的自表示流形学习新算法，通过核对齐将高维数据的自重构几何结构转移到低维嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 受表示学习方法在揭示高维数据潜在结构方面日益增长的兴趣驱动，旨在开发能够更好地捕捉数据内在几何结构的流形学习算法。

Method: 1. 在RKHS中，将每个观测值重构为其他样本的线性组合，优化向量形式的表示定理以实现自表示特性。2. 使用可分离的算子值核将方法扩展到向量值数据。3. 通过核对齐任务将数据投影到低维潜在空间，使其Gram矩阵匹配高维重构核，从而将RKHS的自重构几何结构转移到嵌入中。

Result: 在模拟数据集（同心圆和瑞士卷）和真实数据集（癌症分子活性和物联网网络入侵）上的数值实验提供了经验证据，表明所提方法具有实际有效性。

Conclusion: 该研究通过利用和调整核学习理论的已知结果，扩展了自然数据普遍存在的自表示特性方法，为流形学习提供了新的RKHS框架。

Abstract: Motivated by the growing interest in representation learning approaches that uncover the latent structure of high-dimensional data, this work proposes new algorithms for reconstruction-based manifold learning within Reproducing-Kernel Hilbert Spaces (RKHS). Each observation is first reconstructed as a linear combination of the other samples in the RKHS, by optimizing a vector form of the Representer Theorem for their autorepresentation property. A separable operator-valued kernel extends the formulation to vector-valued data while retaining the simplicity of a single scalar similarity function. A subsequent kernel-alignment task projects the data into a lower-dimensional latent space whose Gram matrix aims to match the high-dimensional reconstruction kernel, thus transferring the auto-reconstruction geometry of the RKHS to the embedding. Therefore, the proposed algorithms represent an extended approach to the autorepresentation property, exhibited by many natural data, by using and adapting well-known results of Kernel Learning Theory. Numerical experiments on both simulated (concentric circles and swiss-roll) and real (cancer molecular activity and IoT network intrusions) datasets provide empirical evidence of the practical effectiveness of the proposed approach.

</details>


### [45] [Detecting Autism Spectrum Disorder with Deep Eye Movement Features](https://arxiv.org/abs/2601.05812)
*Zhanpei Huang,Taochen chen,Fangqing Gu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 提出DSTS框架，通过类感知表示和不平衡感知机制，有效捕捉眼动数据的短期局部依赖，在自闭症谱系障碍检测中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）的诊断需要非侵入性工具，眼动数据具有离散性和短期时间依赖特性，但现有Transformer模型的全局注意力机制对这类数据效果有限，需要专门针对眼动数据特点的建模方法。

Method: 设计了离散短期序列（DSTS）建模框架，包含类感知表示机制和不平衡感知机制，专门针对眼动数据的离散性和短期局部依赖特性进行建模。

Result: 在多个眼动数据集上的实验表明，DSTS框架优于传统机器学习方法和更复杂的深度学习模型，能够有效区分ASD患者和典型发育个体。

Conclusion: 针对眼动数据的离散性和短期依赖特性设计的DSTS框架，通过类感知和不平衡感知机制，能够更有效地捕捉ASD相关的微妙眼动模式，为ASD诊断提供了有效的非侵入性工具。

Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.

</details>


### [46] [A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning](https://arxiv.org/abs/2601.05814)
*Md Sultanul Islam Ovi,Muhsina Tarannum Munfa,Miftahul Alam Adib,Syed Sabbir Hasan*

Main category: cs.LG

TL;DR: 提出双管道机器学习框架，通过统计和包装器两条并行处理流，结合SMOTETomek处理类别不平衡，在睡眠障碍筛查中达到98.67%准确率，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍（如失眠和睡眠呼吸暂停）的准确分类对降低长期健康风险和改善患者生活质量很重要，但临床睡眠研究资源密集且难以扩展到人群筛查规模。

Method: 提出双管道机器学习框架：1）统计管道使用互信息和线性判别分析处理线性可分性；2）包装器管道使用Boruta特征选择和自编码器进行非线性表示学习。采用SMOTETomek混合重采样策略处理类别不平衡。

Result: Extra Trees和K最近邻算法达到98.67%准确率，优于相同数据集上的近期基线。Wilcoxon符号秩检验显示改进具有统计显著性，推理延迟保持在400毫秒以下。

Conclusion: 提出的双管道设计支持准确高效的自动筛查，可用于非侵入性睡眠障碍风险分层，具有临床应用的潜力。

Abstract: Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper presents a Dual Pipeline Machine Learning Framework for multi class sleep disorder screening using the Sleep Health and Lifestyle dataset. The framework consists of two parallel processing streams: a statistical pipeline that targets linear separability using Mutual Information and Linear Discriminant Analysis, and a wrapper based pipeline that applies Boruta feature selection with an autoencoder for non linear representation learning. To address class imbalance, we use the hybrid SMOTETomek resampling strategy. In experiments, Extra Trees and K Nearest Neighbors achieved an accuracy of 98.67%, outperforming recent baselines on the same dataset. Statistical testing using the Wilcoxon Signed Rank Test indicates that the improvement over baseline configurations is significant, and inference latency remains below 400 milliseconds. These results suggest that the proposed dual pipeline design supports accurate and efficient automated screening for non invasive sleep disorder risk stratification.

</details>


### [47] [A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link](https://arxiv.org/abs/2601.05845)
*Eric Weine,Peter Carbonetto,Rafael A. Irizarry,Matthew Stephens*

Main category: cs.LG

TL;DR: 提出带移位对数链接函数的泊松非负矩阵分解，通过可调参数在加性和乘性组合之间灵活转换，改进计数数据的可解释性分解。


<details>
  <summary>Details</summary>
Motivation: 传统泊松NMF假设分解的"部分"以加性方式组合，这在某些场景下不自然。需要一种更灵活的模型，允许部分以更乘性的方式组合，以提高不同数据集的可解释性。

Method: 引入带移位对数链接函数的泊松NMF，通过单个调优参数控制从加性到乘性组合的连续变化。提出最大似然估计算法，并为大型稀疏数据集提供计算高效的近似方法（计算复杂度与数据矩阵非零元素数量成正比）。

Result: 在多个真实数据集上验证了新方法的有效性。结果表明链接函数的选择对分解结果有实质性影响，在某些场景下移位对数链接函数相比标准加性链接能提高可解释性。

Conclusion: 移位对数链接函数为泊松NMF提供了从加性到乘性组合的灵活建模框架，能根据数据特性选择合适组合方式，改进分解结果的可解释性，并为大型稀疏数据集提供高效计算方法。

Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.

</details>


### [48] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: 提出IIB-LPO方法解决RLVR中探索崩溃问题，通过潜在分支和信息瓶颈实现多样化推理，在数学推理基准上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索崩溃问题：随机rollout的语义同质性导致模型陷入狭窄、过度优化的行为。全局熵正则化容易奖励攻击导致无意义冗长，局部token选择性更新则受预训练模型强归纳偏差限制。

Method: 提出IIB-LPO方法：将探索从token分布的统计扰动转向推理轨迹的拓扑分支。在高熵状态触发潜在分支以多样化推理路径，并使用信息瓶颈原则作为轨迹过滤器和自奖励机制，确保简洁且信息丰富的探索。

Result: 在四个数学推理基准上的实验结果表明，IIB-LPO达到最先进性能，在准确率上比先前方法提升高达5.3%，在多样性指标上提升高达7.4%。

Conclusion: IIB-LPO通过将探索从统计扰动转向拓扑分支，有效解决了RLVR中的探索崩溃问题，实现了更高效和多样化的推理路径探索。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


### [49] [GlueNN: gluing patchwise analytic solutions with neural networks](https://arxiv.org/abs/2601.05889)
*Doyoung Kim,Donghee Lee,Hye-Sung Lee,Jiheon Lee,Jaeok Yi*

Main category: cs.LG

TL;DR: 提出一种学习框架，将渐近解析解的积分常数提升为尺度相关函数，通过约束这些系数函数来学习全局有效解，无需边界匹配。


<details>
  <summary>Details</summary>
Motivation: 在物理和工程中，复杂微分方程通常具有强尺度依赖性，难以获得精确解析或数值解。传统分区域简化方法在匹配边界处可能失效，因为近似形式在边界附近可能不成立。

Method: 提出学习框架：将渐近解析解的积分常数提升为尺度相关函数，通过约束这些系数函数满足原始微分方程，神经网络学习全局有效解，平滑插值不同渐近区域。

Result: 在化学动力学和宇宙学代表性问题上验证了框架有效性，准确再现全局解，优于传统匹配方法。

Conclusion: 该框架通过将积分常数推广为尺度相关函数，实现了不同渐近区域间的平滑插值，避免了任意边界匹配，为复杂微分方程求解提供了新方法。

Abstract: In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.

</details>


### [50] [Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates](https://arxiv.org/abs/2601.05909)
*Ayoub Ajarra,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出一个用于机器学习模型在自适应更新下的群体公平性审计框架，通过SP维度量化可允许的策略更新复杂度，实现样本高效的PAC审计。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型会自适应更新（如金融市场的动态调整），这种更新可能改变模型类别但保持某些审计属性不变，需要研究在这种变化下如何进行可靠的公平性审计。

Method: 提出基于经验属性优化（EPO）oracle的通用PAC审计框架。对于统计公平性，建立了由SP维度（一种新的组合度量）表征的分布无关审计界限，该维度捕捉了可允许策略更新的复杂度。

Result: 建立了群体公平性审计的理论框架，能够处理任意模型更新，并通过SP维度量化更新复杂度。框架可扩展到其他审计目标，如预测误差和鲁棒风险。

Conclusion: 该工作为机器学习模型在自适应更新环境下的公平性审计提供了理论基础和实用框架，解决了现实部署中模型动态变化带来的审计挑战。

Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.

</details>


### [51] [Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces](https://arxiv.org/abs/2601.05913)
*Pattarawat Chormai,Ali Hashemi,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: SubDistill是一种新的知识蒸馏算法，专注于仅蒸馏教师模型中与特定子任务相关的组件，在计算资源有限的环境中实现更高效的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 实际应用中通常只需要蒸馏少数类别及其相关中间概念，但现有蒸馏方法很少明确关注相关子任务，导致资源浪费和效率低下。

Method: 提出SubDistill算法，具有改进的数值特性，在每一层仅蒸馏教师模型的相关组件，专注于特定子任务的知识迁移。

Result: 在CIFAR-100和ImageNet数据集上，使用卷积和Transformer模型的实验表明，SubDistill在代表性子任务集上优于现有的逐层蒸馏技术。

Conclusion: SubDistill通过专注于相关子任务的知识蒸馏，不仅性能更优，而且学生模型的决策结构更接近原始教师模型，为实际应用提供了更高效的解决方案。

Abstract: Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.

</details>


### [52] [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)
*Sidney Shapiro,Burhanuddin Panvelwala*

Main category: cs.LG

TL;DR: 该研究评估了Meta开发的Prophet预测框架，将其作为平衡可解释性、标准化工作流程和可访问性的可重复性解决方案，并与ARIMA和随机森林进行对比。


<details>
  <summary>Details</summary>
Motivation: 预测研究与实践中的可重复性挑战，特别是在商业和金融分析领域。传统方法需要大量手动调参且难以在专有环境中复制，机器学习方法虽然灵活但存在可解释性和随机训练过程的问题。

Method: 使用公开可用的金融和零售数据集，在受控且完全文档化的实验设计中，将Prophet的性能和可解释性与多种ARIMA规格（自动选择、手动指定和季节性变体）以及随机森林进行多模型比较。

Result: Prophet在平衡可解释性、标准化工作流程和可访问性方面表现出优势，通过具体Python示例展示了其如何促进高效预测工作流程和分析管道集成。

Conclusion: Prophet作为可重复研究的构建模块，支持验证、可审计性和方法严谨性，为基于Python的研究工作流程提供了实用的可重复预测参考框架。

Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.

</details>


### [53] [On the Robustness of Age for Learning-Based Wireless Scheduling in Unknown Environments](https://arxiv.org/abs/2601.05956)
*Juaren Steiger,Bin Li*

Main category: cs.LG

TL;DR: 提出使用队首年龄代替虚拟队列长度来设计学习型调度策略，在信道条件突变时保持系统稳定


<details>
  <summary>Details</summary>
Motivation: 传统基于虚拟队列长度的约束组合多臂老虎机算法在信道条件突变时，约束可能变得不可行，导致虚拟队列长度无限增长，系统不稳定

Method: 设计基于学习型调度策略，使用队首年龄（虚拟队列中最旧数据包的年龄）代替虚拟队列长度，提高算法鲁棒性

Result: 在i.i.d.网络条件下性能与最先进方法相当；在信道条件突变时系统保持稳定，并能快速从约束不可行期恢复

Conclusion: 队首年龄比虚拟队列长度更适合用于算法设计，能提高系统在动态网络环境中的鲁棒性和稳定性

Abstract: The constrained combinatorial multi-armed bandit model has been widely employed to solve problems in wireless networking and related areas, including the problem of wireless scheduling for throughput optimization under unknown channel conditions. Most work in this area uses an algorithm design strategy that combines a bandit learning algorithm with the virtual queue technique to track the throughput constraint violation. These algorithms seek to minimize the virtual queue length in their algorithm design. However, in networks where channel conditions change abruptly, the resulting constraints may become infeasible, leading to unbounded growth in virtual queue lengths. In this paper, we make the key observation that the dynamics of the head-of-line age, i.e. the age of the oldest packet in the virtual queue, make it more robust when used in algorithm design compared to the virtual queue length. We therefore design a learning-based scheduling policy that uses the head-of-line age in place of the virtual queue length. We show that our policy matches state-of-the-art performance under i.i.d. network conditions. Crucially, we also show that the system remains stable even under abrupt changes in channel conditions and can rapidly recover from periods of constraint infeasibility.

</details>


### [54] [Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks](https://arxiv.org/abs/2601.05984)
*Sahibzada Saadoon Hammad,Joaquín Huerta Guijarro,Francisco Ramos,Michael Gould Carlson,Sergio Trilles Oliver*

Main category: cs.LG

TL;DR: 基于社区兴趣(CoI)的IoT传感器网络异常检测框架，通过融合时空相似性分组传感器，使用自编码器检测温度异常


<details>
  <summary>Details</summary>
Motivation: IoT设备大规模部署形成传感器网络，需要有效组织异构设备并检测异常。社区兴趣(CoI)范式为组织具有相似特征的传感器提供了有前景的方法，但需要开发相应的异常检测框架。

Method: 1) 使用融合相似性矩阵将传感器分组为社区：包含Spearman系数的时间相关性、高斯距离衰减的空间邻近性、海拔相似性；2) 基于最佳轮廓系数选择代表性站点；3) 使用三种自编码器架构(BiLSTM、LSTM、MLP)训练正常温度模式；4) 采用贝叶斯超参数优化和扩展窗口交叉验证；5) 通过重构误差分析检测异常。

Result: 实验结果显示：1) 在评估的配置中，社区内性能稳健；2) 不同社区间观察到性能变化；3) 基于社区的模型共享在减少计算开销方面具有适用性；4) 能够分析模型在IoT传感器网络间的泛化能力。

Conclusion: 基于CoI的异常检测框架有效支持社区内异常检测，社区间模型共享可减少计算开销，为分析IoT传感器网络模型泛化性提供了可行方法。

Abstract: The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.

</details>


### [55] [LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection](https://arxiv.org/abs/2601.06016)
*Þór Sverrisson,Steinn Guðmundsson*

Main category: cs.LG

TL;DR: LookAroundNet是一种基于Transformer的癫痫检测器，利用更宽的时间窗口来建模癫痫活动，通过结合前后EEG信号提高检测性能，在多种临床和家庭监测条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 由于癫痫动态在不同患者、记录条件和临床环境中的巨大变异性，自动癫痫检测仍然具有挑战性。临床医生在解读EEG时通常会参考周围上下文信息，而现有方法往往缺乏这种能力。

Method: 提出LookAroundNet，一种基于Transformer的癫痫检测器，使用更宽的EEG数据时间窗口来建模癫痫活动。该方法结合感兴趣段前后的EEG信号，模拟临床医生使用周围上下文的方式。在多个EEG数据集上进行评估，包括常规临床EEG和长期动态记录，涵盖不同数据分布。

Result: LookAroundNet在多个数据集上表现出色，能够很好地泛化到未见过的记录条件，计算成本适合实际临床部署。结果表明扩展的时间上下文、增加训练数据多样性和模型集成是提高性能的关键因素。

Conclusion: 这项研究通过引入扩展时间上下文和多样化训练数据，推动了自动癫痫检测模型向临床可行解决方案的发展，为在不同临床和家庭监测条件下实现可靠癫痫检测提供了有效方法。

Abstract: Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.

</details>
