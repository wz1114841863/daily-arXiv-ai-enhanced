{"id": "2509.25401", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.25401", "abs": "https://arxiv.org/abs/2509.25401", "authors": ["Liang Qiao", "Yue Dai", "Yeqi Huang", "Hongyu Kan", "Jun Shi", "Hong An"], "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers", "comment": null, "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.", "AI": {"tldr": "FlashOmni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7a00\u758f\u6ce8\u610f\u529b\u5f15\u64ce\uff0c\u517c\u5bb9\u4efb\u610fDiT\u67b6\u6784\uff0c\u901a\u8fc7\u7075\u6d3b\u7a00\u758f\u7b26\u53f7\u6807\u51c6\u5316\u5404\u79cd\u7a00\u758f\u7b56\u7565\u8868\u793a\uff0c\u5728\u5355\u4e2a\u6ce8\u610f\u529b\u5185\u6838\u4e2d\u6267\u884c\u591a\u6837\u5316\u7a00\u758f\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668(DiTs)\u5728\u89c6\u89c9\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u5927\u9650\u5236\u4e86\u90e8\u7f72\u3002\u73b0\u6709\u7a00\u758f\u52a0\u901f\u65b9\u6cd5\u9700\u8981\u5b9a\u5236\u5316\u5185\u6838\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faFlashOmni\u7edf\u4e00\u7a00\u758f\u6ce8\u610f\u529b\u5f15\u64ce\uff0c\u5f15\u5165\u7075\u6d3b\u7a00\u758f\u7b26\u53f7\u6807\u51c6\u5316\u7a00\u758f\u7b56\u7565\u8868\u793a\uff0c\u8bbe\u8ba1\u4f18\u5316\u7684\u7a00\u758fGEMMs\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\u3002", "result": "FlashOmni\u5728\u6ce8\u610f\u529b\u548cGEMM-Q\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u7ebf\u6027\u52a0\u901f(1:1)\uff0c\u5728GEMM-O\u4e2d\u5b9e\u73b02.5-3.8\u500d\u52a0\u901f\uff0c\u8fbe\u5230\u7406\u8bba\u6781\u9650\u768487.5%\u3002\u5e94\u7528\u591a\u7c92\u5ea6\u7a00\u758f\u7b56\u7565\u4f7fHunyuan\u6a21\u578b(33K)\u5b9e\u73b0\u7ea61.5\u500d\u7aef\u5230\u7aef\u52a0\u901f\u4e14\u4e0d\u964d\u4f4e\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "FlashOmni\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u9ad8\u6548\u7684\u7a00\u758f\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347DiT\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u5408\u6210\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.26120", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.26120", "abs": "https://arxiv.org/abs/2509.26120", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "AGOCS -- Accurate Google Cloud Simulator Framework", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in the Proceedings of the 2016 IEEE\n  International Conferences on Ubiquitous Intelligence and Computing (UIC),\n  Advanced and Trusted Computing (ATC), Scalable Computing and Communications\n  (ScalCom), Cloud and Big Data Computing (CBDCom), Internet of People (IoP),\n  and Smart World Congress (SmartWorld)", "summary": "This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel\nhigh-fidelity Cloud workload simulator based on parsing real workload traces,\nwhich can be conveniently used on a desktop machine for day-to-day research.\nOur simulation is based on real-world workload traces from a Google Cluster\nwith 12.5K nodes, over a period of a calendar month. The framework is able to\nreveal very precise and detailed parameters of the executed jobs, tasks and\nnodes as well as to provide actual resource usage statistics. The system has\nbeen implemented in Scala language with focus on parallel execution and an\neasy-to-extend design concept. The paper presents the detailed structural\nframework for AGOCS and discusses our main design decisions, whilst also\nsuggesting alternative and possibly performance enhancing future approaches.\nThe framework is available via the Open Source GitHub repository.", "AI": {"tldr": "AGOCS\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9eGoogle\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u8f68\u8ff9\u7684\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\uff0c\u53ef\u5728\u684c\u9762\u673a\u5668\u4e0a\u7528\u4e8e\u65e5\u5e38\u7814\u7a76\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7cbe\u786e\u6a21\u62df\u771f\u5b9e\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u684c\u9762\u7ea7\u6a21\u62df\u5668\uff0c\u4e3a\u4e91\u7814\u7a76\u63d0\u4f9b\u4fbf\u5229\u5de5\u5177\u3002", "method": "\u57fa\u4e8e12.5K\u8282\u70b9Google\u96c6\u7fa4\u4e00\u4e2a\u6708\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8f68\u8ff9\uff0c\u4f7f\u7528Scala\u8bed\u8a00\u5b9e\u73b0\uff0c\u6ce8\u91cd\u5e76\u884c\u6267\u884c\u548c\u6613\u6269\u5c55\u8bbe\u8ba1\u3002", "result": "\u80fd\u591f\u63ed\u793a\u4f5c\u4e1a\u3001\u4efb\u52a1\u548c\u8282\u70b9\u7684\u7cbe\u786e\u8be6\u7ec6\u53c2\u6570\uff0c\u5e76\u63d0\u4f9b\u5b9e\u9645\u8d44\u6e90\u4f7f\u7528\u7edf\u8ba1\u3002", "conclusion": "AGOCS\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u6846\u67b6\uff0c\u4e3a\u4e91\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u684c\u9762\u7ea7\u5de5\u5177\u3002"}}
{"id": "2509.26253", "categories": ["cs.DC", "cs.PF", "C.4; I.m; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.26253", "abs": "https://arxiv.org/abs/2509.26253", "authors": ["Floris-Jan Willemsen", "Rob V. van Nieuwpoort", "Ben van Werkhoven"], "title": "Efficient Construction of Large Search Spaces for Auto-Tuning", "comment": null, "summary": "Automatic performance tuning, or auto-tuning, accelerates high-performance\ncodes by exploring vast spaces of code variants. However, due to the large\nnumber of possible combinations and complex constraints, constructing these\nsearch spaces can be a major bottleneck. Real-world applications have been\nencountered where the search space construction takes minutes to hours or even\ndays. Current state-of-the-art techniques for search space construction, such\nas chain-of-trees, lack a formal foundation and only perform adequately on a\nspecific subset of search spaces.\n  We show that search space construction for constraint-based auto-tuning can\nbe reformulated as a Constraint Satisfaction Problem (CSP). Building on this\ninsight with a CSP solver, we develop a runtime parser that translates\nuser-defined constraint functions into solver-optimal expressions, optimize the\nsolver to exploit common structures in auto-tuning constraints, and integrate\nthese and other advances in open-source tools. These contributions\nsubstantially improve performance and accessibility while preserving\nflexibility.\n  We evaluate our approach using a diverse set of benchmarks, demonstrating\nthat our optimized solver reduces construction time by four orders of magnitude\nversus brute-force enumeration, three orders of magnitude versus an unoptimized\nCSP solver, and one to two orders of magnitude versus leading auto-tuning\nframeworks built on chain-of-trees. We thus eliminate a critical scalability\nbarrier for auto-tuning and provide a drop-in solution that enables the\nexploration of previously unattainable problem scales in auto-tuning and\nrelated domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u57fa\u4e8e\u7ea6\u675f\u7684\u81ea\u52a8\u8c03\u4f18\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u91cd\u65b0\u8868\u8ff0\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898(CSP)\uff0c\u901a\u8fc7\u4f18\u5316CSP\u6c42\u89e3\u5668\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u6027\u80fd\u8c03\u4f18\u4e2d\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u53ef\u80fd\u8017\u65f6\u6570\u5206\u949f\u5230\u6570\u5929\uff0c\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u6280\u672f\u5982\u94fe\u5f0f\u6811\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u5b50\u96c6\u3002", "method": "\u5c06\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u91cd\u65b0\u8868\u8ff0\u4e3aCSP\u95ee\u9898\uff0c\u5f00\u53d1\u8fd0\u884c\u65f6\u89e3\u6790\u5668\u5c06\u7528\u6237\u7ea6\u675f\u8f6c\u6362\u4e3a\u6c42\u89e3\u5668\u4f18\u5316\u8868\u8fbe\u5f0f\uff0c\u4f18\u5316\u6c42\u89e3\u5668\u4ee5\u5229\u7528\u81ea\u52a8\u8c03\u4f18\u7ea6\u675f\u7684\u5e38\u89c1\u7ed3\u6784\u3002", "result": "\u4f18\u5316\u6c42\u89e3\u5668\u76f8\u6bd4\u66b4\u529b\u679a\u4e3e\u51cf\u5c114\u4e2a\u6570\u91cf\u7ea7\u6784\u5efa\u65f6\u95f4\uff0c\u76f8\u6bd4\u672a\u4f18\u5316CSP\u6c42\u89e3\u5668\u51cf\u5c113\u4e2a\u6570\u91cf\u7ea7\uff0c\u76f8\u6bd4\u94fe\u5f0f\u6811\u6846\u67b6\u51cf\u5c111-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u81ea\u52a8\u8c03\u4f18\u7684\u5173\u952e\u53ef\u6269\u5c55\u6027\u969c\u788d\uff0c\u4e3a\u63a2\u7d22\u5148\u524d\u65e0\u6cd5\u8fbe\u5230\u7684\u95ee\u9898\u89c4\u6a21\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26300", "categories": ["cs.LG", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.26300", "abs": "https://arxiv.org/abs/2509.26300", "authors": ["Floris-Jan Willemsen", "Rob V. van Nieuwpoort", "Ben van Werkhoven"], "title": "Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning", "comment": null, "summary": "Automatic performance tuning (auto-tuning) is widely used to optimize\nperformance-critical applications across many scientific domains by finding the\nbest program variant among many choices. Efficient optimization algorithms are\ncrucial for navigating the vast and complex search spaces in auto-tuning. As is\nwell known in the context of machine learning and similar fields,\nhyperparameters critically shape optimization algorithm efficiency. Yet for\nauto-tuning frameworks, these hyperparameters are almost never tuned, and their\npotential performance impact has not been studied.\n  We present a novel method for general hyperparameter tuning of optimization\nalgorithms for auto-tuning, thus \"tuning the tuner\". In particular, we propose\na robust statistical method for evaluating hyperparameter performance across\nsearch spaces, publish a FAIR data set and software for reproducibility, and\npresent a simulation mode that replays previously recorded tuning data,\nlowering the costs of hyperparameter tuning by two orders of magnitude. We show\nthat even limited hyperparameter tuning can improve auto-tuner performance by\n94.8% on average, and establish that the hyperparameters themselves can be\noptimized efficiently with meta-strategies (with an average improvement of\n204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful\ntechnique for advancing auto-tuning research and practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u8bc4\u4f30\u3001\u4eff\u771f\u6a21\u5f0f\u548c\u5143\u7b56\u7565\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8c03\u4f18\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u6027\u80fd\u8c03\u4f18\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f18\u5316\u7b97\u6cd5\u7684\u8d85\u53c2\u6570\u51e0\u4e4e\u4ece\u672a\u88ab\u8c03\u4f18\uff0c\u5176\u6f5c\u5728\u6027\u80fd\u5f71\u54cd\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u901a\u7528\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u5305\u62ec\u8de8\u641c\u7d22\u7a7a\u95f4\u7684\u7edf\u8ba1\u6027\u80fd\u8bc4\u4f30\u3001FAIR\u6570\u636e\u96c6\u53d1\u5e03\u3001\u4eff\u771f\u6a21\u5f0f\u91cd\u653e\u5386\u53f2\u8c03\u4f18\u6570\u636e\u4ee5\u964d\u4f4e\u8c03\u4f18\u6210\u672c\u3002", "result": "\u6709\u9650\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u53ef\u4f7f\u81ea\u52a8\u8c03\u4f18\u5668\u6027\u80fd\u5e73\u5747\u63d0\u534794.8%\uff0c\u4f7f\u7528\u5143\u7b56\u7565\u4f18\u5316\u8d85\u53c2\u6570\u53ef\u5e73\u5747\u63d0\u5347204.7%\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u4f18\u662f\u63a8\u8fdb\u81ea\u52a8\u8c03\u4f18\u7814\u7a76\u548c\u5b9e\u8df5\u7684\u5f3a\u5927\u6280\u672f\uff0c\u5e38\u88ab\u5ffd\u89c6\u4f46\u6f5c\u529b\u5de8\u5927\u3002"}}
{"id": "2509.25391", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25391", "abs": "https://arxiv.org/abs/2509.25391", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n", "Alan Ezequiel Fuster"], "title": "smallNet: Implementation of a convolutional layer in tiny FPGAs", "comment": null, "summary": "Since current neural network development systems in Xilinx and VLSI require\ncodevelopment with Python libraries, the first stage of a convolutional network\nhas been implemented by developing a convolutional layer entirely in Verilog.\nThis handcoded design, free of IP cores and based on a filter polynomial like\nstructure, enables straightforward deployment not only on low cost FPGAs but\nalso on SoMs, SoCs, and ASICs. We analyze the limitations of numerical\nrepresentations and compare our implemented architecture, smallNet, with its\ncomputer based counterpart, demonstrating a 5.1x speedup, over 81%\nclassification accuracy, and a total power consumption of just 1.5 W. The\nalgorithm is validated on a single-core Cora Z7, demonstrating its feasibility\nfor real time, resource-constrained embedded applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u5b8c\u5168\u7528Verilog\u5b9e\u73b0\u7684\u5377\u79ef\u5c42\uff0c\u65e0\u9700IP\u6838\uff0c\u57fa\u4e8e\u6ee4\u6ce2\u5668\u591a\u9879\u5f0f\u7ed3\u6784\uff0c\u53ef\u5728\u4f4e\u6210\u672cFPGA\u3001SoM\u3001SoC\u548cASIC\u4e0a\u90e8\u7f72\u3002\u76f8\u6bd4\u8ba1\u7b97\u673a\u7248\u672c\u5b9e\u73b0\u4e865.1\u500d\u52a0\u901f\uff0c81%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u603b\u529f\u8017\u4ec51.5W\u3002", "motivation": "\u5f53\u524dXilinx\u548cVLSI\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5f00\u53d1\u7cfb\u7edf\u9700\u8981\u4e0ePython\u5e93\u534f\u540c\u5f00\u53d1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u7528Verilog\u5b9e\u73b0\u7684\u5377\u79ef\u5c42\u6765\u7b80\u5316\u90e8\u7f72\u6d41\u7a0b\u3002", "method": "\u624b\u5de5\u7f16\u7801\u8bbe\u8ba1\u5377\u79ef\u5c42\uff0c\u4e0d\u4f7f\u7528IP\u6838\uff0c\u57fa\u4e8e\u6ee4\u6ce2\u5668\u591a\u9879\u5f0f\u7ed3\u6784\uff0c\u5728\u5355\u6838Cora Z7\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e865.1\u500d\u901f\u5ea6\u63d0\u5347\uff0c\u8d85\u8fc781%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u603b\u529f\u8017\u4ec5\u4e3a1.5W\uff0c\u9a8c\u8bc1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u5d4c\u5165\u5f0f\u5e94\u7528\u4e2d\u5b9e\u65f6\u8fd0\u884c\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u624b\u5de5\u7f16\u7801\u7684\u5377\u79ef\u5c42\u8bbe\u8ba1\u4e3a\u5b9e\u65f6\u3001\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u80fd\u6548\u8868\u73b0\u3002"}}
{"id": "2509.25415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25415", "abs": "https://arxiv.org/abs/2509.25415", "authors": ["Jan Droll"], "title": "Permuting Transactions in Ethereum Blocks: An Empirical Study", "comment": "17 pages, 6 figures, experiment code available", "summary": "Several recent proposals implicitly or explicitly suggest making use of\nrandomized transaction ordering within a block to mitigate centralization\neffects and to improve fairness in the Ethereum ecosystem. However,\ntransactions and blocks are subject to gas limits and protocol rules. In a\nrandomized transaction order, the behavior of transactions may change depending\non other transactions in the same block, leading to invalid blocks and varying\ngas consumptions. In this paper, we quantify and characterize protocol\nviolations, execution errors and deviations in gas consumption of blocks and\ntransactions to examine technical deployability. For that, we permute and\nexecute the transactions of over 335,000 Ethereum Mainnet blocks multiple\ntimes. About 22% of block permutations are invalid due to protocol violations\ncaused by privately mined transactions or blocks close to their gas limit.\nAlso, almost all transactions which show execution errors under permutation but\nnot in the original order are privately mined transactions. Only 6% of\ntransactions show deviations in gas consumption and 98% of block permutations\ndeviate at most 10% from their original gas consumption. From a technical\nperspective, these results suggest that randomized transaction ordering may be\nfeasible if transaction selection is handled carefully.", "AI": {"tldr": "\u5206\u6790\u968f\u673a\u5316\u4ea4\u6613\u6392\u5e8f\u5728\u4ee5\u592a\u574a\u4e2d\u7684\u6280\u672f\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u7ea622%\u7684\u533a\u5757\u6392\u5217\u56e0\u534f\u8bae\u8fdd\u89c4\u800c\u65e0\u6548\uff0c\u4e3b\u8981\u6d89\u53ca\u79c1\u6709\u6316\u77ff\u4ea4\u6613\u6216\u63a5\u8fd1gas\u9650\u5236\u7684\u533a\u5757\u3002\u53ea\u67096%\u7684\u4ea4\u6613\u663e\u793agas\u6d88\u8017\u504f\u5dee\uff0c98%\u7684\u533a\u5757\u6392\u5217\u4e0e\u539fgas\u6d88\u8017\u504f\u5dee\u4e0d\u8d85\u8fc710%\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u5316\u4ea4\u6613\u6392\u5e8f\u662f\u5426\u80fd\u7f13\u89e3\u4ee5\u592a\u574a\u751f\u6001\u7684\u4e2d\u5fc3\u5316\u6548\u5e94\u548c\u6539\u5584\u516c\u5e73\u6027\uff0c\u540c\u65f6\u8bc4\u4f30\u5176\u6280\u672f\u90e8\u7f72\u53ef\u884c\u6027\u3002", "method": "\u5bf9\u8d85\u8fc7335,000\u4e2a\u4ee5\u592a\u574a\u4e3b\u7f51\u533a\u5757\u7684\u4ea4\u6613\u8fdb\u884c\u591a\u6b21\u6392\u5217\u548c\u6267\u884c\uff0c\u91cf\u5316\u534f\u8bae\u8fdd\u89c4\u3001\u6267\u884c\u9519\u8bef\u548cgas\u6d88\u8017\u504f\u5dee\u3002", "result": "22%\u7684\u533a\u5757\u6392\u5217\u56e0\u534f\u8bae\u8fdd\u89c4\u800c\u65e0\u6548\uff1b\u51e0\u4e4e\u6240\u6709\u5728\u6392\u5217\u4e2d\u51fa\u73b0\u6267\u884c\u9519\u8bef\u4f46\u5728\u539f\u59cb\u987a\u5e8f\u4e2d\u6b63\u5e38\u7684\u4ea4\u6613\u90fd\u662f\u79c1\u6709\u6316\u77ff\u4ea4\u6613\uff1b\u4ec56%\u7684\u4ea4\u6613\u663e\u793agas\u6d88\u8017\u504f\u5dee\uff1b98%\u7684\u533a\u5757\u6392\u5217\u4e0e\u539fgas\u6d88\u8017\u504f\u5dee\u4e0d\u8d85\u8fc710%\u3002", "conclusion": "\u4ece\u6280\u672f\u89d2\u5ea6\u770b\uff0c\u5982\u679c\u4ea4\u6613\u9009\u62e9\u5904\u7406\u5f97\u5f53\uff0c\u968f\u673a\u5316\u4ea4\u6613\u6392\u5e8f\u53ef\u80fd\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2509.25198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25198", "abs": "https://arxiv.org/abs/2509.25198", "authors": ["Elbert Ho"], "title": "SOLD: SELFIES-based Objective-driven Latent Diffusion", "comment": null, "summary": "Recently, machine learning has made a significant impact on de novo drug\ndesign. However, current approaches to creating novel molecules conditioned on\na target protein typically rely on generating molecules directly in the 3D\nconformational space, which are often slow and overly complex. In this work, we\npropose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent\ndiffusion model that generates molecules in a latent space derived from 1D\nSELFIES strings and conditioned on a target protein. In the process, we also\ntrain an innovative SELFIES transformer and propose a new way to balance losses\nwhen training multi-task machine learning models.Our model generates\nhigh-affinity molecules for the target protein in a simple and efficient way,\nwhile also leaving room for future improvements through the addition of more\ndata.", "AI": {"tldr": "\u63d0\u51fa\u4e86SOLD\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8eSELFIES\u5b57\u7b26\u4e32\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u9488\u5bf9\u76ee\u6807\u86cb\u767d\u7684\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u57283D\u6784\u8c61\u7a7a\u95f4\u4e2d\u751f\u6210\u7684\u590d\u6742\u6027\u548c\u4f4e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u76ee\u6807\u86cb\u767d\u751f\u6210\u5206\u5b50\u7684\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u57283D\u6784\u8c61\u7a7a\u95f4\u4e2d\u8fdb\u884c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901f\u5ea6\u6162\u4e14\u8fc7\u4e8e\u590d\u6742\uff0c\u9700\u8981\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86SOLD\u6a21\u578b\uff0c\u4f7f\u75281D SELFIES\u5b57\u7b26\u4e32\u7684\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6269\u6563\u5efa\u6a21\uff0c\u540c\u65f6\u8bad\u7ec3\u4e86\u521b\u65b0\u7684SELFIES\u53d8\u6362\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u635f\u5931\u5e73\u8861\u65b0\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4ee5\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u5f0f\u4e3a\u76ee\u6807\u86cb\u767d\u751f\u6210\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\uff0c\u5e76\u4e3a\u901a\u8fc7\u6dfb\u52a0\u66f4\u591a\u6570\u636e\u8fdb\u884c\u672a\u6765\u6539\u8fdb\u7559\u4e0b\u4e86\u7a7a\u95f4\u3002", "conclusion": "SOLD\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u836f\u7269\u8bbe\u8ba1\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.25626", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25626", "abs": "https://arxiv.org/abs/2509.25626", "authors": ["Yi Hu", "Huiyang Zhou"], "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels", "comment": null, "summary": "3D Gaussian splatting (3DGS) is a transformative technique with profound\nimplications on novel view synthesis and real-time rendering. Given its\nimportance, there have been many attempts to improve its performance. However,\nwith the increasing complexity of GPU architectures and the vast search space\nof performance-tuning parameters, it is a challenging task. Although manual\noptimizations have achieved remarkable speedups, they require domain expertise\nand the optimization process can be highly time consuming and error prone. In\nthis paper, we propose to exploit large language models (LLMs) to analyze and\noptimize Gaussian splatting kernels. To our knowledge, this is the first work\nto use LLMs to optimize highly specialized real-world GPU kernels. We reveal\nthe intricacies of using LLMs for code optimization and analyze the code\noptimization techniques from the LLMs. We also propose ways to collaborate with\nLLMs to further leverage their capabilities. For the original 3DGS code on the\nMipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and\n24% with GPT-5, demonstrating the different capabilities of different LLMs. By\nfeeding additional information from performance profilers, the performance\nimprovement from LLM-optimized code is enhanced to up to 42% and 38% on\naverage. In comparison, our best-effort manually optimized version can achieve\na performance improvement up to 48% and 39% on average, showing that there are\nstill optimizations beyond the capabilities of current LLMs. On the other hand,\neven upon a newly proposed 3DGS framework with algorithmic optimizations,\nSeele, LLMs can still further enhance its performance by 6%, showing that there\nare optimization opportunities missed by domain experts. This highlights the\npotential of collaboration between domain experts and LLMs.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4f18\u53163D\u9ad8\u65af\u6e85\u5c04(3DGS)\u7684GPU\u5185\u6838\uff0c\u5728MipNeRF360\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u9ad842%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u63d0\u534738%\uff0c\u63a5\u8fd1\u4eba\u5de5\u4f18\u5316\u768448%\u6700\u9ad8\u548c39%\u5e73\u5747\u63d0\u5347\u3002", "motivation": "3DGS\u5728\u65b0\u578b\u89c6\u56fe\u5408\u6210\u548c\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46GPU\u67b6\u6784\u590d\u6742\u6027\u548c\u5927\u91cf\u6027\u80fd\u8c03\u4f18\u53c2\u6570\u4f7f\u5f97\u4f18\u5316\u56f0\u96be\u3002\u4eba\u5de5\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8017\u65f6\u6613\u9519\uff0c\u56e0\u6b64\u63a2\u7d22\u4f7f\u7528LLMs\u81ea\u52a8\u4f18\u5316GPU\u5185\u6838\u3002", "method": "\u5229\u7528LLMs\u5206\u6790\u548c\u4f18\u5316\u9ad8\u65af\u6e85\u5c04\u5185\u6838\uff0c\u7ed3\u5408\u6027\u80fd\u5206\u6790\u5668\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\u589e\u5f3a\u4f18\u5316\u6548\u679c\uff0c\u5e76\u4e0e\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\u3002", "result": "\u5728\u539f\u59cb3DGS\u4ee3\u7801\u4e0a\uff0cDeepseek\u5b9e\u73b019%\u52a0\u901f\uff0cGPT-5\u5b9e\u73b024%\u52a0\u901f\uff1b\u7ed3\u5408\u6027\u80fd\u5206\u6790\u5668\u540e\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe42%\uff0c\u5e73\u574738%\uff1b\u5728\u5df2\u4f18\u5316\u7684Seele\u6846\u67b6\u4e0a\u4ecd\u80fd\u8fdb\u4e00\u6b65\u63d0\u53476%\u6027\u80fd\u3002", "conclusion": "LLMs\u5728GPU\u5185\u6838\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u53d1\u73b0\u9886\u57df\u4e13\u5bb6\u9057\u6f0f\u7684\u4f18\u5316\u673a\u4f1a\uff0c\u4f46\u5f53\u524dLLMs\u4ecd\u6709\u4f18\u5316\u80fd\u529b\u9650\u5236\uff0c\u9886\u57df\u4e13\u5bb6\u4e0eLLMs\u7684\u534f\u4f5c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.25555", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25555", "abs": "https://arxiv.org/abs/2509.25555", "authors": ["Amirreza Sokhankhosh", "Khalid Hassan", "Sara Rouhani"], "title": "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches", "comment": "Accepted by the 2025 IEEE International Conference on Blockchain\n  (Blockchain)", "summary": "Collaborative and distributed learning techniques, such as Federated Learning\n(FL) and Split Learning (SL), hold significant promise for leveraging sensitive\ndata in privacy-critical domains. However, FL and SL suffer from key\nlimitations -- FL imposes substantial computational demands on clients, while\nSL leads to prolonged training times. To overcome these challenges, SplitFed\nLearning (SFL) was introduced as a hybrid approach that combines the strengths\nof FL and SL. Despite its advantages, SFL inherits scalability, performance,\nand security issues from SL. In this paper, we propose two novel frameworks:\nSharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning\n(BSFL). SSFL addresses the scalability and performance constraints of SFL by\ndistributing the workload and communication overhead of the SL server across\nmultiple parallel shards. Building upon SSFL, BSFL replaces the centralized\nserver with a blockchain-based architecture that employs a committee-driven\nconsensus mechanism to enhance fairness and security. BSFL incorporates an\nevaluation mechanism to exclude poisoned or tampered model updates, thereby\nmitigating data poisoning and model integrity attacks. Experimental evaluations\nagainst baseline SL and SFL approaches show that SSFL improves performance and\nscalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases\nresilience to data poisoning attacks by 62.7% while maintaining superior\nperformance under normal operating conditions. To the best of our knowledge,\nBSFL is the first blockchain-enabled framework to implement an end-to-end\ndecentralized SplitFed Learning system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff1a\u5206\u7247\u5f0fSplitFed\u5b66\u4e60\uff08SSFL\uff09\u548c\u533a\u5757\u94fe\u9a71\u52a8\u7684SplitFed\u5b66\u4e60\uff08BSFL\uff09\uff0c\u4ee5\u89e3\u51b3SplitFed\u5b66\u4e60\uff08SFL\uff09\u7684\u53ef\u6269\u5c55\u6027\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u548c\u5206\u5272\u5b66\u4e60\uff08SL\uff09\u5b58\u5728\u5173\u952e\u9650\u5236\u2014\u2014FL\u5bf9\u5ba2\u6237\u7aef\u8ba1\u7b97\u8981\u6c42\u9ad8\uff0cSL\u8bad\u7ec3\u65f6\u95f4\u957f\u3002SFL\u4f5c\u4e3a\u6df7\u5408\u65b9\u6cd5\u867d\u7136\u7ed3\u5408\u4e86\u4e24\u8005\u4f18\u52bf\uff0c\u4f46\u4ecd\u7ee7\u627f\u4e86SL\u7684\u53ef\u6269\u5c55\u6027\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "SSFL\u901a\u8fc7\u5c06SL\u670d\u52a1\u5668\u7684\u5de5\u4f5c\u8d1f\u8f7d\u548c\u901a\u4fe1\u5f00\u9500\u5206\u5e03\u5230\u591a\u4e2a\u5e76\u884c\u5206\u7247\u4e2d\u6765\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002BSFL\u5728SSFL\u57fa\u7840\u4e0a\u7528\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u67b6\u6784\u66ff\u6362\u96c6\u4e2d\u5f0f\u670d\u52a1\u5668\uff0c\u91c7\u7528\u59d4\u5458\u4f1a\u9a71\u52a8\u7684\u5171\u8bc6\u673a\u5236\u589e\u5f3a\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u5305\u542b\u8bc4\u4f30\u673a\u5236\u6392\u9664\u4e2d\u6bd2\u6216\u88ab\u7be1\u6539\u7684\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u57fa\u7ebfSL\u548cSFL\u65b9\u6cd5\u76f8\u6bd4\uff0cSSFL\u5206\u522b\u5c06\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u63d0\u9ad8\u4e8631.2%\u548c85.2%\u3002BSFL\u5bf9\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u7684\u62b5\u5fa1\u80fd\u529b\u63d0\u9ad8\u4e8662.7%\uff0c\u540c\u65f6\u5728\u6b63\u5e38\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "BSFL\u662f\u9996\u4e2a\u5b9e\u73b0\u7aef\u5230\u7aef\u53bb\u4e2d\u5fc3\u5316SplitFed\u5b66\u4e60\u7cfb\u7edf\u7684\u533a\u5757\u94fe\u9a71\u52a8\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86SFL\u7684\u53ef\u6269\u5c55\u6027\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u6311\u6218\u3002"}}
{"id": "2509.25202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25202", "abs": "https://arxiv.org/abs/2509.25202", "authors": ["Zhuoning Xu", "Xinyan Liu"], "title": "VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps", "comment": null, "summary": "Jigsaw puzzle solving remains challenging in computer vision, requiring an\nunderstanding of both local fragment details and global spatial relationships.\nWhile most traditional approaches only focus on visual cues like edge matching\nand visual coherence, few methods explore natural language descriptions for\nsemantic guidance in challenging scenarios, especially for eroded gap puzzles.\nWe propose a vision-language framework that leverages textual context to\nenhance puzzle assembly performance. Our approach centers on the\nVision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns\nvisual patches with textual descriptions through multi-level semantic matching\nfrom local tokens to global context. Also, a multimodal architecture that\ncombines dual visual encoders with language features for cross-modal reasoning\nis integrated into this module. Experiments demonstrate that our method\nsignificantly outperforms state-of-the-art models across various datasets,\nachieving substantial improvements, including a 14.2 percentage point gain in\npiece accuracy. Ablation studies confirm the critical role of the VLHSA module\nin driving improvements over vision-only approaches. Our work establishes a new\nparadigm for jigsaw puzzle solving by incorporating multimodal semantic\ninsights.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u591a\u6a21\u6001\u62fc\u56fe\u6c42\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u5c42\u6b21\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u63d0\u5347\u62fc\u56fe\u7ec4\u88c5\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u62fc\u56fe\u6c42\u89e3\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u4f46\u5728\u5177\u6709\u4fb5\u8680\u95f4\u9699\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u5f88\u5c11\u63a2\u7d22\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8fdb\u884c\u8bed\u4e49\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u89c6\u89c9-\u8bed\u8a00\u5c42\u6b21\u8bed\u4e49\u5bf9\u9f50(VLHSA)\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49\u5339\u914d\u5c06\u89c6\u89c9\u5757\u4e0e\u6587\u672c\u63cf\u8ff0\u5bf9\u9f50\uff0c\u7ed3\u5408\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u548c\u8bed\u8a00\u7279\u5f81\u8fdb\u884c\u8de8\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u62fc\u7247\u51c6\u786e\u7387\u63d0\u534714.2\u4e2a\u767e\u5206\u70b9\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9eVLHSA\u6a21\u5757\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u878d\u5165\u591a\u6a21\u6001\u8bed\u4e49\u6d1e\u5bdf\uff0c\u4e3a\u62fc\u56fe\u6c42\u89e3\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.25853", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25853", "abs": "https://arxiv.org/abs/2509.25853", "authors": ["Jingyao Zhang", "Jaewoo Park", "Jongeun Lee", "Elaheh Sadredini"], "title": "SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV", "comment": null, "summary": "Large Language Model (LLM) inference requires substantial computational\nresources, yet CPU-based inference remains essential for democratizing AI due\nto the widespread availability of CPUs compared to specialized accelerators.\nHowever, efficient LLM inference on CPUs faces two fundamental challenges: (1)\nexisting CPU architectures struggle with low-precision arithmetic required by\nquantized models, where optimal bit precision varies across models and layers;\nand (2) the memory-bound nature of the token generation phase creates severe\nperformance bottlenecks. To address these challenges, we propose SAIL\n(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that\nefficiently supports arbitrary bit precisions with minimal overhead. SAIL\nintegrates three key innovations: First, we introduce Batched LUT-based General\nMatrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,\nenabling high data reuse through lookup tables and reducing memory movement.\nSecond, our Pattern-Aware LUT optimization identifies and exploits redundancy\nin input activation patterns, reducing computation cycles by 13.8\\%. Third, we\ndevelop an in-memory type conversion algorithm that leverages PIM's parallelism\nfor efficient de-/quantization operations, alleviating pressure on CPU's vector\nunits. Our architecture requires only 2\\% hardware overhead and a single new\ninstruction, while maintaining dual functionality as both compute and storage\nunits. Experimental evaluations using a modified gem5 simulator demonstrate\nthat SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar\ncompared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost\nefficiency than NVIDIA V100 GPUs, establishing a practical path for efficient\nCPU-based LLM inference.", "AI": {"tldr": "SAIL\u662f\u4e00\u79cd\u57fa\u4e8eCPU\u7684LLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7SRAM\u52a0\u901f\u548c\u67e5\u627e\u8868\u6280\u672f\uff0c\u652f\u6301\u4efb\u610f\u6bd4\u7279\u7cbe\u5ea6\uff0c\u5728CPU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "CPU\u63a8\u7406\u5bf9\u4e8eAI\u6c11\u4e3b\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CPU\u67b6\u6784\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u96be\u4ee5\u652f\u6301\u91cf\u5316\u6a21\u578b\u6240\u9700\u7684\u4f4e\u7cbe\u5ea6\u7b97\u672f\uff0c\u4ee5\u53catoken\u751f\u6210\u9636\u6bb5\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "method": "SAIL\u96c6\u6210\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u57fa\u4e8eSRAM\u7684\u6279\u5904\u7406LUT-GEMV\u3001\u6a21\u5f0f\u611f\u77e5LUT\u4f18\u5316\u4ee5\u53ca\u5185\u5b58\u5185\u7c7b\u578b\u8f6c\u6362\u7b97\u6cd5\uff0c\u4ec5\u97002%\u786c\u4ef6\u5f00\u9500\u548c\u4e00\u4e2a\u65b0\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cSAIL\u76f8\u6bd4ARM Neoverse-N1 CPU\u57fa\u7ebf\u5b9e\u73b010.7\u500d\u52a0\u901f\u548c19.9\u500d\u6bcf\u7f8e\u5143token\u6570\u63d0\u5347\uff0c\u6bd4NVIDIA V100 GPU\u5177\u67097.04\u500d\u66f4\u597d\u7684\u6210\u672c\u6548\u7387\u3002", "conclusion": "SAIL\u4e3a\u57fa\u4e8eCPU\u7684\u9ad8\u6548LLM\u63a8\u7406\u5efa\u7acb\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5728\u4fdd\u6301\u786c\u4ef6\u5f00\u9500\u6700\u5c0f\u5316\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.25605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25605", "abs": "https://arxiv.org/abs/2509.25605", "authors": ["Brian Kelley", "Sivasankaran Rajamanickam"], "title": "LAPIS: A Performance Portable, High Productivity Compiler Framework", "comment": "14 pages (10 excluding references and appendices). 5 figures", "summary": "Portability, performance, and productivity are three critical dimensions for\nevaluating a programming model or compiler infrastructure. Several modern\nprogramming models for computational science focus on performance and\nportability. On the other end, several machine learning focused programming\nmodels focus on portability and productivity. A clear solution that is strong\nin all three dimensions has yet to emerge. A second related problem arises when\nuse cases from computational science converge with machine learning. The\ndisparate popular frameworks of these fields require programmers to manually\nintegrate codes written in different frameworks. Finally, several programming\nframeworks lack easy options for extensibility as any new computer architecture\nchange require complex changes to the programming models. We present LAPIS, an\nMLIR-based compiler that addresses all three of these challenges. We\ndemonstrate that LAPIS can automatically lower sparse and dense linear algebra\nkernels from computational science and artificial intelligence use cases. We\nalso show how LAPIS facilitates the integration of codes between PyTorch and\nKokkos. We compare kernel performance with the default MLIR implementations on\ndiverse architectures to demonstrate portability. By developing a dialect that\nis built on the principles of the Kokkos ecosystem, LAPIS also allows\nextensibility of the framework to new architectures.", "AI": {"tldr": "LAPIS\u662f\u4e00\u4e2a\u57fa\u4e8eMLIR\u7684\u7f16\u8bd1\u5668\uff0c\u65e8\u5728\u89e3\u51b3\u7f16\u7a0b\u6a21\u578b\u5728\u53ef\u79fb\u690d\u6027\u3001\u6027\u80fd\u548c\u751f\u4ea7\u529b\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4ee5\u53ca\u8ba1\u7b97\u79d1\u5b66\u4e0e\u673a\u5668\u5b66\u4e60\u9886\u57df\u4ee3\u7801\u96c6\u6210\u548c\u6846\u67b6\u6269\u5c55\u6027\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7f16\u7a0b\u6a21\u578b\u5728\u53ef\u79fb\u690d\u6027\u3001\u6027\u80fd\u548c\u751f\u4ea7\u529b\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u5b58\u5728\u4e0d\u5e73\u8861\uff0c\u8ba1\u7b97\u79d1\u5b66\u4e0e\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e4b\u95f4\u7f3a\u4e4f\u6709\u6548\u96c6\u6210\uff0c\u4e14\u73b0\u6709\u6846\u67b6\u5bf9\u65b0\u67b6\u6784\u7684\u6269\u5c55\u6027\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eMLIR\u7684LAPIS\u7f16\u8bd1\u5668\uff0c\u6784\u5efa\u57fa\u4e8eKokkos\u751f\u6001\u7cfb\u7edf\u539f\u5219\u7684\u65b9\u8a00\uff0c\u81ea\u52a8\u964d\u4f4e\u7a00\u758f\u548c\u5bc6\u96c6\u7ebf\u6027\u4ee3\u6570\u5185\u6838\uff0c\u5e76\u4fc3\u8fdbPyTorch\u4e0eKokkos\u4ee3\u7801\u7684\u96c6\u6210\u3002", "result": "LAPIS\u80fd\u591f\u81ea\u52a8\u5904\u7406\u8ba1\u7b97\u79d1\u5b66\u548cAI\u7528\u4f8b\u4e2d\u7684\u7a00\u758f\u548c\u5bc6\u96c6\u7ebf\u6027\u4ee3\u6570\u5185\u6838\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u4e0a\u5c55\u793a\u51fa\u826f\u597d\u7684\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u652f\u6301\u6846\u67b6\u5411\u65b0\u67b6\u6784\u7684\u6269\u5c55\u3002", "conclusion": "LAPIS\u6210\u529f\u89e3\u51b3\u4e86\u7f16\u7a0b\u6a21\u578b\u5728\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u79d1\u5b66\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u8bd1\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u3002"}}
{"id": "2509.25204", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25204", "abs": "https://arxiv.org/abs/2509.25204", "authors": ["Jin Li", "Zhebo Wang", "Tianliang Lu", "Mohan Li", "Wenpeng Xing", "Meng Han"], "title": "Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation", "comment": "Submitted to IEEE ICASSP 2026", "summary": "Entropy-based inference methods have gained traction for improving the\nreliability of Large Language Models (LLMs). However, many existing approaches,\nsuch as entropy minimization techniques, suffer from high computational\noverhead and fail to leverage historical token context effectively. To address\nthese limitations, we propose Spectral Logit Sculpting (SLS), a lightweight\ninference-time optimization method that dynamically modulates token\ndistributions using spectral and entropic properties of recent logits. SLS\nmaintains a sliding buffer of top-K logits, performs on-the-fly Singular Value\nDecomposition (SVD) to identify dominant spectral directions, and adaptively\nrescales logits based on both entropy and logit gap statistics--only activating\nwhen uncertainty is high. Without updating any model parameters, SLS\neffectively sharpens the output distribution while preserving contextual\nconsistency. Experimental results on multiple public benchmarks demonstrate\nthat SLS consistently outperforms existing baseline methods, achieving superior\naccuracy in mathematical, coding, and scientific reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u63a8\u7406\u65f6\u4f18\u5316\u65b9\u6cd5Spectral Logit Sculpting (SLS)\uff0c\u901a\u8fc7\u5229\u7528\u6700\u8fd1logits\u7684\u8c31\u7279\u6027\u548c\u71b5\u7279\u6027\u6765\u52a8\u6001\u8c03\u6574token\u5206\u5e03\uff0c\u63d0\u9ad8LLM\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u71b5\u7684\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528\u5386\u53f2token\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "SLS\u7ef4\u62a4\u4e00\u4e2atop-K logits\u7684\u6ed1\u52a8\u7f13\u51b2\u533a\uff0c\u8fdb\u884c\u5b9e\u65f6\u5947\u5f02\u503c\u5206\u89e3\u8bc6\u522b\u4e3b\u5bfc\u8c31\u65b9\u5411\uff0c\u57fa\u4e8e\u71b5\u548clogit\u95f4\u9699\u7edf\u8ba1\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u7f29\u653elogits\uff0c\u4ec5\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u65f6\u6fc0\u6d3b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSLS\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u51c6\u786e\u7387\u3002", "conclusion": "SLS\u662f\u4e00\u79cd\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u9510\u5316\u8f93\u51fa\u5206\u5e03\u540c\u65f6\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002"}}
{"id": "2509.26065", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.26065", "abs": "https://arxiv.org/abs/2509.26065", "authors": ["Alberto Scionti", "Paolo Savio", "Francesco Lubrano", "Olivier Terzo", "Marco Ferretti", "Florin Apopei", "Juri Bellucci", "Ennio Spano", "Luca Carriere"], "title": "Runtime Energy Monitoring for RISC-V Soft-Cores", "comment": null, "summary": "Energy efficiency is one of the major concern in designing advanced computing\ninfrastructures. From single nodes to large-scale systems (data centers),\nmonitoring the energy consumption of the computing system when applications run\nis a critical task. Designers and application developers often rely on software\ntools and detailed architectural models to extract meaningful information and\ndetermine the system energy consumption. However, when a design space\nexploration is required, designers may incur in continuous tuning of the models\nto match with the system under evaluation. To overcome such limitations, we\npropose a holistic approach to monitor energy consumption at runtime without\nthe need of running complex (micro-)architectural models. Our approach is based\non a measurement board coupled with a FPGA-based System-on-Module. The\nmeasuring board captures currents and voltages (up to tens measuring points)\ndriving the FPGA and exposes such values through a specific memory region. A\nrunning service reads and computes energy consumption statistics without\nconsuming extra resources on the FPGA device. Our approach is also scalable to\nmonitoring of multi-nodes infrastructures (clusters). We aim to leverage this\nframework to perform experiments in the context of an aeronautical design\napplication; specifically, we will look at optimizing performance and energy\nconsumption of a shallow artificial neural network on RISC-V based soft-cores.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u8fd0\u884c\u65f6\u80fd\u8017\u76d1\u63a7\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6a21\u578b\u5373\u53ef\u5b9e\u65f6\u76d1\u6d4b\u8ba1\u7b97\u7cfb\u7edf\u7684\u80fd\u8017", "motivation": "\u4f20\u7edf\u80fd\u8017\u76d1\u63a7\u4f9d\u8d56\u590d\u6742\u7684\u67b6\u6784\u6a21\u578b\uff0c\u5728\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65f6\u9700\u8981\u4e0d\u65ad\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff0c\u5b58\u5728\u5c40\u9650\u6027", "method": "\u4f7f\u7528\u6d4b\u91cf\u677f\u4e0eFPGA\u7cfb\u7edf\u6a21\u5757\u7ed3\u5408\uff0c\u901a\u8fc7\u7279\u5b9a\u5185\u5b58\u533a\u57df\u66b4\u9732\u7535\u538b\u7535\u6d41\u503c\uff0c\u8fd0\u884c\u670d\u52a1\u8bfb\u53d6\u5e76\u8ba1\u7b97\u80fd\u8017\u7edf\u8ba1", "result": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u591a\u8282\u70b9\u57fa\u7840\u8bbe\u65bd\u76d1\u63a7\uff0c\u4e0d\u6d88\u8017FPGA\u989d\u5916\u8d44\u6e90", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u822a\u7a7a\u8bbe\u8ba1\u5e94\u7528\uff0c\u7279\u522b\u662f\u4f18\u5316RISC-V\u8f6f\u6838\u4e0a\u6d45\u5c42\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u548c\u80fd\u8017"}}
{"id": "2509.25700", "categories": ["cs.DC", "cs.GT", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.25700", "abs": "https://arxiv.org/abs/2509.25700", "authors": ["Houyi Qi", "Minghui Liwang", "Liqun Fu", "Sai Zou", "Xinlei Yi", "Wei Ni", "Huaiyu Dai"], "title": "PAST: Pilot and Adaptive Orchestration for Timely and Resilient Service Delivery in Edge-Assisted UAV Networks under Spatio-Temporal Dynamics", "comment": null, "summary": "Incentive-driven resource trading is essential for UAV applications with\nintensive, time-sensitive computing demands. Traditional spot trading suffers\nfrom negotiation delays and high energy costs, while conventional futures\ntrading struggles to adapt to the dynamic, uncertain UAV-edge environment. To\naddress these challenges, we propose PAST (pilot-and-adaptive stable trading),\na novel framework for edge-assisted UAV networks with spatio-temporal dynamism.\nPAST integrates two complementary mechanisms: PilotAO (pilot trading agreements\nwith overbooking), a risk-aware, overbooking-enabled early-stage\ndecision-making module that establishes long-term, mutually beneficial\nagreements and boosts resource utilization; and AdaptAO (adaptive trading\nagreements with overbooking rate update), an intelligent adaptation module that\ndynamically updates agreements and overbooking rates based on UAV mobility,\nsupply-demand variations, and agreement performance. Together, these mechanisms\nenable both stability and flexibility, guaranteeing individual rationality,\nstrong stability, competitive equilibrium, and weak Pareto optimality.\nExtensive experiments on real-world datasets show that PAST consistently\noutperforms benchmark methods in decision-making overhead, task completion\nlatency, resource utilization, and social welfare. By combining predictive\nplanning with real-time adjustments, PAST offers a valuable reference on robust\nand adaptive practice for improving low-altitude mission performance.", "AI": {"tldr": "PAST\u6846\u67b6\u901a\u8fc7\u6574\u5408PilotAO\u548cAdaptAO\u4e24\u4e2a\u673a\u5236\uff0c\u4e3a\u65e0\u4eba\u673a\u8fb9\u7f18\u7f51\u7edc\u63d0\u4f9b\u7a33\u5b9a\u7075\u6d3b\u7684\u8d44\u6e90\u4ea4\u6613\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u73b0\u8d27\u548c\u671f\u8d27\u4ea4\u6613\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u73b0\u8d27\u4ea4\u6613\u5b58\u5728\u534f\u5546\u5ef6\u8fdf\u548c\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u671f\u8d27\u4ea4\u6613\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u4e0d\u786e\u5b9a\u7684\u65e0\u4eba\u673a\u8fb9\u7f18\u73af\u5883\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u7a33\u5b9a\u6027\u53c8\u5177\u5907\u7075\u6d3b\u6027\u7684\u8d44\u6e90\u4ea4\u6613\u65b9\u6848\u3002", "method": "PAST\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u673a\u5236\uff1aPilotAO\uff08\u5e26\u8d85\u989d\u9884\u8ba2\u7684\u8bd5\u70b9\u4ea4\u6613\u534f\u8bae\uff09\u8fdb\u884c\u98ce\u9669\u611f\u77e5\u7684\u65e9\u671f\u51b3\u7b56\uff0c\u5efa\u7acb\u957f\u671f\u4e92\u5229\u534f\u8bae\uff1bAdaptAO\uff08\u5e26\u8d85\u989d\u9884\u8ba2\u7387\u66f4\u65b0\u7684\u81ea\u9002\u5e94\u4ea4\u6613\u534f\u8bae\uff09\u6839\u636e\u65e0\u4eba\u673a\u79fb\u52a8\u6027\u3001\u4f9b\u9700\u53d8\u5316\u548c\u534f\u8bae\u6027\u80fd\u52a8\u6001\u66f4\u65b0\u534f\u8bae\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPAST\u5728\u51b3\u7b56\u5f00\u9500\u3001\u4efb\u52a1\u5b8c\u6210\u5ef6\u8fdf\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u793e\u4f1a\u798f\u5229\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "PAST\u901a\u8fc7\u9884\u6d4b\u6027\u89c4\u5212\u548c\u5b9e\u65f6\u8c03\u6574\u7684\u7ed3\u5408\uff0c\u4e3a\u63d0\u5347\u4f4e\u7a7a\u4efb\u52a1\u6027\u80fd\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u81ea\u9002\u5e94\u7684\u5b9e\u8df5\u53c2\u8003\u3002"}}
{"id": "2509.25205", "categories": ["cs.LG", "cs.CR", "math.RA"], "pdf": "https://arxiv.org/pdf/2509.25205", "abs": "https://arxiv.org/abs/2509.25205", "authors": ["Daksh Pandey"], "title": "Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations on graph data without requiring manual labels.\nHowever, leading SSL methods like GRACE are fundamentally incompatible with\nprivacy-preserving technologies such as Homomorphic Encryption (HE) due to\ntheir reliance on non-polynomial operations. This paper introduces Poly-GRACE,\na novel framework for HE-compatible self-supervised learning on graphs. Our\napproach consists of a fully polynomial-friendly Graph Convolutional Network\n(GCN) encoder and a novel, polynomial-based contrastive loss function. Through\nexperiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we\ndemonstrate that Poly-GRACE not only enables private pre-training but also\nachieves performance that is highly competitive with, and in the case of\nCiteSeer, superior to the standard non-private baseline. Our work represents a\nsignificant step towards practical and high-performance privacy-preserving\ngraph representation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Poly-GRACE\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e0e\u540c\u6001\u52a0\u5bc6\u517c\u5bb9\u7684\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u975e\u7ebf\u6027\u64cd\u4f5c\u800c\u65e0\u6cd5\u4fdd\u62a4\u9690\u79c1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRACE\uff09\u7531\u4e8e\u4f9d\u8d56\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u4e0e\u540c\u6001\u52a0\u5bc6\u7b49\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u4e0d\u517c\u5bb9\uff0c\u9650\u5236\u4e86\u5728\u654f\u611f\u56fe\u6570\u636e\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u5168\u591a\u9879\u5f0f\u53cb\u597d\u7684\u56fe\u5377\u79ef\u7f51\u7edc\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u591a\u9879\u5f0f\u7684\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u6574\u4e2a\u6846\u67b6\u80fd\u591f\u5728\u540c\u6001\u52a0\u5bc6\u73af\u5883\u4e0b\u8fd0\u884c\u3002", "result": "\u5728Cora\u3001CiteSeer\u548cPubMed\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPoly-GRACE\u4e0d\u4ec5\u652f\u6301\u9690\u79c1\u9884\u8bad\u7ec3\uff0c\u800c\u4e14\u5728CiteSeer\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u751a\u81f3\u4f18\u4e8e\u6807\u51c6\u975e\u9690\u79c1\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9e\u73b0\u5b9e\u7528\u4e14\u9ad8\u6027\u80fd\u7684\u9690\u79c1\u4fdd\u62a4\u56fe\u8868\u793a\u5b66\u4e60\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u654f\u611f\u56fe\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25382", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25382", "abs": "https://arxiv.org/abs/2509.25382", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n"], "title": "On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study", "comment": "Argentine Congress of Embedded Systems (2025)", "summary": "In this work, we explore the latent space of a denoising variational\nautoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on\ngravitational wave data from event GW150914. To evaluate how well the model\ncaptures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw\nposterior samples conditioned on clean inputs, and compare them to the\nencoder's outputs from noisy data. Although the model reconstructs signals\naccurately, statistical comparisons reveal a clear mismatch in the latent\nspace. This shows that strong denoising performance doesn't necessarily mean\nthe latent representations are reliable highlighting the importance of using\nposterior-based validation when evaluating generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728GW150914\u5f15\u529b\u6ce2\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668-\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08VAE-MoG\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u53d1\u73b0\u5c3d\u7ba1\u6a21\u578b\u80fd\u51c6\u786e\u91cd\u5efa\u4fe1\u53f7\uff0c\u4f46\u6f5c\u5728\u8868\u793a\u5b58\u5728\u7edf\u8ba1\u4e0d\u5339\u914d\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u53ef\u9760\u6027\uff0c\u9a8c\u8bc1\u5f3a\u53bb\u566a\u6027\u80fd\u662f\u5426\u610f\u5473\u7740\u53ef\u9760\u7684\u6f5c\u5728\u8868\u793a\u3002", "method": "\u4f7f\u7528\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u4ece\u5e72\u51c0\u8f93\u5165\u4e2d\u62bd\u53d6\u540e\u9a8c\u6837\u672c\uff0c\u5e76\u4e0e\u7f16\u7801\u5668\u4ece\u566a\u58f0\u6570\u636e\u8f93\u51fa\u7684\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u6bd4\u8f83\u3002", "result": "\u6a21\u578b\u80fd\u51c6\u786e\u91cd\u5efa\u4fe1\u53f7\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u5b58\u5728\u660e\u663e\u4e0d\u5339\u914d\uff0c\u8868\u660e\u53bb\u566a\u6027\u80fd\u5f3a\u4e0d\u4ee3\u8868\u6f5c\u5728\u8868\u793a\u53ef\u9760\u3002", "conclusion": "\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u57fa\u4e8e\u540e\u9a8c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.25919", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25919", "abs": "https://arxiv.org/abs/2509.25919", "authors": ["Jay H. Park", "Youngju Cho", "Choungsol Lee", "Moonwook Oh", "Euiseong Seo"], "title": "Accelerating LLM Inference with Precomputed Query Storage", "comment": null, "summary": "Large language model (LLM) inference often suffers from high latency,\nparticularly in resource-constrained environments such as on-device or edge\ndeployments. To address this challenge, we present StorInfer, a novel\nstorage-assisted LLM inference system that accelerates response time by\nprecomputing and storing predictable query-response pairs offline. When a user\nquery semantically matches a precomputed query, StorInfer bypasses expensive\nGPU inference and instantly returns the stored response, significantly reducing\nlatency and compute costs. To maximize coverage and effectiveness, StorInfer\nemploys an LLM-driven generator that adaptively produces diverse and\ndeduplicated queries based on a given knowledge base. This is achieved via two\ntechniques: adaptive query masking, which prevents regeneration of similar\nqueries, and adaptive sampling, which dynamically tunes generation parameters\nto promote semantic diversity. The resulting query-response pairs are embedded\nand indexed using a disk-backed vector database to enable fast,\nsimilarity-based retrieval at runtime. Using this approach, we generated 150K\nunique precomputed pairs (taking up to 830 MB of storage space), achieving up\nto 17.3% latency reduction with no loss in response quality. Our evaluation\nacross multiple QA datasets demonstrates the practicality and scalability of\nstorage-assisted inference, especially in scenarios with predictable query\ndistributions. StorInfer highlights a promising direction in leveraging storage\nas a primary enabler for efficient, low-latency LLM deployment.", "AI": {"tldr": "StorInfer\u662f\u4e00\u4e2a\u5b58\u50a8\u8f85\u52a9\u7684LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u548c\u5b58\u50a8\u53ef\u9884\u6d4b\u7684\u67e5\u8be2-\u54cd\u5e94\u5bf9\u6765\u52a0\u901f\u54cd\u5e94\u65f6\u95f4\uff0c\u5728\u8bed\u4e49\u5339\u914d\u65f6\u7ed5\u8fc7GPU\u63a8\u7406\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u8bbe\u5907\u7aef\u6216\u8fb9\u7f18\u90e8\u7f72\uff09\u4e2d\u63a8\u7406\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b58\u50a8\u9884\u8ba1\u7b97\u7ed3\u679c\u6765\u4f18\u5316\u6027\u80fd\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u751f\u6210\u5668\u81ea\u9002\u5e94\u751f\u6210\u591a\u6837\u5316\u548c\u53bb\u91cd\u7684\u67e5\u8be2\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u67e5\u8be2\u63a9\u7801\u548c\u81ea\u9002\u5e94\u91c7\u6837\u6280\u672f\uff0c\u5c06\u67e5\u8be2-\u54cd\u5e94\u5bf9\u5d4c\u5165\u5e76\u7d22\u5f15\u5230\u57fa\u4e8e\u78c1\u76d8\u7684\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u3002", "result": "\u751f\u6210\u4e8615\u4e07\u4e2a\u72ec\u7279\u7684\u9884\u8ba1\u7b97\u5bf9\uff08\u5360\u7528830MB\u5b58\u50a8\u7a7a\u95f4\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe17.3%\u7684\u5ef6\u8fdf\u51cf\u5c11\uff0c\u4e14\u54cd\u5e94\u8d28\u91cf\u65e0\u635f\u5931\u3002", "conclusion": "\u5b58\u50a8\u8f85\u52a9\u63a8\u7406\u5728\u53ef\u9884\u6d4b\u67e5\u8be2\u5206\u5e03\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.25206", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25206", "abs": "https://arxiv.org/abs/2509.25206", "authors": ["Yanke Wang", "Kyriakos Flouris"], "title": "Hyperbolic Optimization", "comment": "Preprint", "summary": "This work explores optimization methods on hyperbolic manifolds. Building on\nRiemannian optimization principles, we extend the Hyperbolic Stochastic\nGradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam\noptimizer. While these methods are particularly relevant for learning on the\nPoincar\\'e ball, they may also provide benefits in Euclidean and other\nnon-Euclidean settings, as the chosen optimization encourages the learning of\nPoincar\\'e embeddings. This representation, in turn, accelerates convergence in\nthe early stages of training, when parameters are far from the optimum. As a\ncase study, we train diffusion models using the hyperbolic optimization methods\nwith hyperbolic time-discretization of the Langevin dynamics, and show that\nthey achieve faster convergence on certain datasets without sacrificing\ngenerative quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53cc\u66f2Adam\u4f18\u5316\u5668\uff0c\u6269\u5c55\u4e86\u53cc\u66f2\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8ePoincar\u00e9\u7403\u4e0a\u7684\u5b66\u4e60\uff0c\u5e76\u5728\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u5c55\u793a\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u57fa\u4e8e\u9ece\u66fc\u4f18\u5316\u539f\u7406\uff0c\u5c06\u53cc\u66f2\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6269\u5c55\u5230\u53cc\u66f2Adam\u4f18\u5316\u5668\uff0c\u65e8\u5728\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684\u4f18\u52bf\u6765\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u7279\u522b\u662f\u5728\u53c2\u6570\u8fdc\u79bb\u6700\u4f18\u89e3\u7684\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u3002", "method": "\u6784\u5efa\u4e86\u53cc\u66f2Adam\u4f18\u5316\u5668\uff0c\u4f5c\u4e3a\u9ece\u66fcSGD\u7684\u7279\u5316\u7248\u672c\uff0c\u5e76\u7ed3\u5408\u53cc\u66f2\u65f6\u95f4\u79bb\u6563\u5316\u7684Langevin\u52a8\u529b\u5b66\u6765\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u53cc\u66f2\u4f18\u5316\u65b9\u6cd5\u7684\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u53cc\u66f2\u4f18\u5316\u65b9\u6cd5\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u8bbe\u7f6e\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\uff0c\u4e3a\u5b66\u4e60Poincar\u00e9\u5d4c\u5165\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.25510", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25510", "abs": "https://arxiv.org/abs/2509.25510", "authors": ["Chang Liu", "Danial Chitnis"], "title": "EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit", "comment": null, "summary": "The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often\ninvolves significant manual effort, especially during the transistor sizing\nprocess. While Machine Learning techniques in Electronic Design Automation\n(EDA) have shown promise in reducing complexity and minimizing human\nintervention, they still face challenges such as numerous iterations and a lack\nof knowledge about AMS circuit design. Recently, Large Language Models (LLMs)\nhave demonstrated significant potential across various fields, showing a\ncertain level of knowledge in circuit design and indicating their potential to\nautomate the transistor sizing process. In this work, we propose EEsizer, an\nLLM-based AI agent that integrates large language models with circuit\nsimulators and custom data analysis functions, enabling fully automated,\nclosed-loop transistor sizing without relying on external knowledge. By\nemploying prompt engineering and Chain-of-Thought reasoning, the agent\niteratively explores design directions, evaluates performance, and refines\nsolutions with minimal human intervention. We first benchmarked 8 LLMs on six\nbasic circuits and selected three high-performing models to optimize a\n20-transistor CMOS operational amplifier, targeting multiple performance\nmetrics, including rail-to-rail operation from 180 nm to 90 nm technology\nnodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90\nnm across three different test groups, with a maximum of 20 iterations,\ndemonstrating adaptability and robustness at advanced nodes. To assess design\nrobustness, we manually designed a bias circuit and performed a variation\nanalysis using Gaussian-distributed variations on transistor dimensions and\nthreshold voltages.", "AI": {"tldr": "EEsizer\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\uff0c\u901a\u8fc7\u96c6\u6210LLM\u4e0e\u7535\u8def\u4eff\u771f\u5668\u548c\u6570\u636e\u5206\u6790\u529f\u80fd\uff0c\u5b9e\u73b0\u5168\u81ea\u52a8\u3001\u95ed\u73af\u7684\u6676\u4f53\u7ba1\u5c3a\u5bf8\u4f18\u5316\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u3002", "motivation": "\u6a21\u62df\u548c\u6df7\u5408\u4fe1\u53f7\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u6676\u4f53\u7ba1\u5c3a\u5bf8\u4f18\u5316\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52aa\u529b\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ecd\u9762\u4e34\u8fed\u4ee3\u6b21\u6570\u591a\u548c\u7f3a\u4e4f\u7535\u8def\u8bbe\u8ba1\u77e5\u8bc6\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u548c\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u8ba9AI\u4ee3\u7406\u8fed\u4ee3\u63a2\u7d22\u8bbe\u8ba1\u65b9\u5411\u3001\u8bc4\u4f30\u6027\u80fd\u5e76\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u96c6\u6210LLM\u4e0e\u7535\u8def\u4eff\u771f\u5668\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u5206\u6790\u529f\u80fd\u3002", "result": "\u57288\u4e2aLLM\u57fa\u51c6\u6d4b\u8bd5\u540e\uff0c\u9009\u62e9\u4e09\u4e2a\u9ad8\u6027\u80fd\u6a21\u578b\u4f18\u531620\u6676\u4f53\u7ba1CMOS\u8fd0\u7b97\u653e\u5927\u5668\uff0cOpenAI o3\u572890nm\u8282\u70b9\u6210\u529f\u5b9e\u73b0\u7528\u6237\u76ee\u6807\uff0c\u6700\u591a20\u6b21\u8fed\u4ee3\u3002", "conclusion": "EEsizer\u5c55\u793a\u4e86\u5728\u5148\u8fdb\u5de5\u827a\u8282\u70b9\u4e0a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5168\u81ea\u52a8\u6676\u4f53\u7ba1\u5c3a\u5bf8\u4f18\u5316\u3002"}}
{"id": "2509.26043", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.26043", "abs": "https://arxiv.org/abs/2509.26043", "authors": ["Alberto Scionti", "Paolo Savio", "Francesco Lubrano", "Federico Stirano", "Antonino Nespola", "Olivier Terzo", "Corrado De Sio", "Luca Sterpone"], "title": "Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes", "comment": null, "summary": "Network Interface Cards (NICs) greatly evolved from simple basic devices\nmoving traffic in and out of the network to complex heterogeneous systems\noffloading host CPUs from performing complex tasks on in-transit packets. These\nlatter comprise different types of devices, ranging from NICs accelerating\nfixed specific functions (e.g., on-the-fly data compression/decompression,\nchecksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC)\nequipped with both general purpose processors and specialized engines\n(Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure\nreprogrammable devices to modern heterogeneous systems comprising\ngeneral-purpose processors, real-time cores and even AI-oriented engines.\nFurthermore, the availability of high-speed network interfaces (e.g., SFPs)\nmakes modern FPGAs a good choice for implementing Smart-NICs. In this work, we\nextended the functionalities offered by an open-source NIC implementation\n(Corundum) by enabling time-aware traffic management in hardware, and using\nthis feature to control the bandwidth associated with different traffic\nclasses. By exposing dedicated control registers on the AXI bus, the driver of\nthe NIC can easily configure the transmission bandwidth of different\nprioritized queues. Basically, each control register is associated with a\nspecific transmission queue (Corundum can expose up to thousands of\ntransmission and receiving queues), and sets up the fraction of time in a\ntransmission window which the queue is supposed to get access the output port\nand transmit the packets. Queues are then prioritized and associated to\ndifferent traffic classes through the Linux QDISC mechanism. Experimental\nevaluation demonstrates that the approach allows to properly manage the\nbandwidth reserved to the different transmission flows.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u5f00\u6e90NIC\u5b9e\u73b0Corundum\u7684\u529f\u80fd\uff0c\u901a\u8fc7\u786c\u4ef6\u5b9e\u73b0\u65f6\u95f4\u611f\u77e5\u6d41\u91cf\u7ba1\u7406\uff0c\u63a7\u5236\u4e0d\u540c\u6d41\u91cf\u7c7b\u522b\u7684\u5e26\u5bbd\u5206\u914d\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u63a5\u53e3\u5361(NIC)\u548cFPGA\u5df2\u4ece\u7b80\u5355\u8bbe\u5907\u53d1\u5c55\u4e3a\u590d\u6742\u5f02\u6784\u7cfb\u7edf\uff0c\u80fd\u591f\u5378\u8f7d\u4e3b\u673aCPU\u7684\u590d\u6742\u4efb\u52a1\u3002\u4f5c\u8005\u65e8\u5728\u5229\u7528\u73b0\u4ee3FPGA\u5b9e\u73b0\u667a\u80fdNIC\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u7ea7\u6d41\u91cf\u7ba1\u7406\u6765\u7cbe\u786e\u63a7\u5236\u4e0d\u540c\u4f18\u5148\u7ea7\u961f\u5217\u7684\u5e26\u5bbd\u5206\u914d\u3002", "method": "\u6269\u5c55Corundum\u5f00\u6e90NIC\u5b9e\u73b0\uff0c\u5728AXI\u603b\u7ebf\u4e0a\u66b4\u9732\u4e13\u7528\u63a7\u5236\u5bc4\u5b58\u5668\uff0c\u4f7fNIC\u9a71\u52a8\u80fd\u591f\u914d\u7f6e\u4e0d\u540c\u4f18\u5148\u7ea7\u961f\u5217\u7684\u4f20\u8f93\u5e26\u5bbd\u3002\u6bcf\u4e2a\u63a7\u5236\u5bc4\u5b58\u5668\u5bf9\u5e94\u7279\u5b9a\u4f20\u8f93\u961f\u5217\uff0c\u8bbe\u7f6e\u5176\u5728\u4f20\u8f93\u7a97\u53e3\u5185\u83b7\u5f97\u8f93\u51fa\u7aef\u53e3\u8bbf\u95ee\u6743\u9650\u7684\u65f6\u95f4\u6bd4\u4f8b\u3002\u901a\u8fc7Linux QDISC\u673a\u5236\u5c06\u961f\u5217\u4e0e\u4e0d\u540c\u6d41\u91cf\u7c7b\u522b\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ba1\u7406\u4e0d\u540c\u4f20\u8f93\u6d41\u9884\u7559\u7684\u5e26\u5bbd\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5e26\u5bbd\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u786c\u4ef6\u5b9e\u73b0\u7684\u65f6\u95f4\u611f\u77e5\u6d41\u91cf\u7ba1\u7406\u673a\u5236\uff0c\u6210\u529f\u6269\u5c55\u4e86Corundum NIC\u7684\u529f\u80fd\uff0c\u4e3a\u4e0d\u540c\u6d41\u91cf\u7c7b\u522b\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u5e26\u5bbd\u63a7\u5236\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u73b0\u4ee3FPGA\u5728\u5b9e\u73b0\u667a\u80fdNIC\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.25207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25207", "abs": "https://arxiv.org/abs/2509.25207", "authors": ["Yebin Lim", "Susik Yoon"], "title": "Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models", "comment": "Accepted to Findings of EMNLP 2025", "summary": "Recent advancements in large language models (LLMs) have shown promise in\nfeature engineering for tabular data, but concerns about their reliability\npersist, especially due to variability in generated outputs. We introduce a\nmulti-level diagnosis and evaluation framework to assess the robustness of LLMs\nin feature engineering across diverse domains, focusing on the three main\nfactors: key variables, relationships, and decision boundary values for\npredicting target classes. We demonstrate that the robustness of LLMs varies\nsignificantly over different datasets, and that high-quality LLM-generated\nfeatures can improve few-shot prediction performance by up to 10.52%. This work\nopens a new direction for assessing and enhancing the reliability of LLM-driven\nfeature engineering in various domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ea7\u8bca\u65ad\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30LLM\u5728\u7279\u5f81\u5de5\u7a0b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0LLM\u751f\u6210\u7684\u4f18\u8d28\u7279\u5f81\u53ef\u5c06\u5c11\u6837\u672c\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10.52%\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8868\u683c\u6570\u636e\u7279\u5f81\u5de5\u7a0b\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u751f\u6210\u8f93\u51fa\u7684\u53ef\u53d8\u6027\uff0c\u5bf9\u5176\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u4ecd\u7136\u5b58\u5728\u3002", "method": "\u5f15\u5165\u591a\u7ea7\u8bca\u65ad\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5173\u6ce8\u4e09\u4e2a\u4e3b\u8981\u56e0\u7d20\uff1a\u5173\u952e\u53d8\u91cf\u3001\u5173\u7cfb\u548c\u9884\u6d4b\u76ee\u6807\u7c7b\u7684\u51b3\u7b56\u8fb9\u754c\u503c\u3002", "result": "LLM\u7684\u9c81\u68d2\u6027\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u9ad8\u8d28\u91cfLLM\u751f\u6210\u7279\u5f81\u53ef\u5c06\u5c11\u6837\u672c\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10.52%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bc4\u4f30\u548c\u589e\u5f3aLLM\u9a71\u52a8\u7279\u5f81\u5de5\u7a0b\u5728\u5404\u9886\u57df\u7684\u53ef\u9760\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.26092", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26092", "abs": "https://arxiv.org/abs/2509.26092", "authors": ["Kuan-Wei Lu", "Ding-Yong Hong", "Pangfeng Liu", "Jan-Jan Wu"], "title": "Efficient Distributed Training via Dual Batch Sizes and Cyclic Progressive Learning", "comment": null, "summary": "Distributed machine learning is critical for training deep learning models on\nlarge datasets and with numerous parameters. Current research primarily focuses\non leveraging additional hardware resources and powerful computing units to\naccelerate the training process. As a result, larger batch sizes are often\nemployed to speed up training. However, training with large batch sizes can\nlead to lower accuracy due to poor generalization. To address this issue, we\npropose the dual batch size learning scheme, a distributed training method\nbuilt on the parameter server framework. This approach maximizes training\nefficiency by utilizing the largest batch size that the hardware can support\nwhile incorporating a smaller batch size to enhance model generalization. By\nusing two different batch sizes simultaneously, this method reduces testing\nloss and enhances generalization, with minimal extra training time.\nAdditionally, to mitigate the time overhead caused by dual batch size learning,\nwe propose the cyclic progressive learning scheme. This technique gradually\nadjusts image resolution from low to high during training, significantly\nboosting training speed. By combining cyclic progressive learning with dual\nbatch size learning, our hybrid approach improves both model generalization and\ntraining efficiency. Experimental results using ResNet-18 show that, compared\nto conventional training methods, our method can improve accuracy by 3.3% while\nreducing training time by 10.6% on CIFAR-100, and improve accuracy by 0.1%\nwhile reducing training time by 35.7% on ImageNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u6279\u6b21\u5927\u5c0f\u5b66\u4e60\u548c\u5faa\u73af\u6e10\u8fdb\u5b66\u4e60\u7684\u6df7\u5408\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e3b\u8981\u4f9d\u8d56\u589e\u52a0\u786c\u4ef6\u8d44\u6e90\u548c\u8ba1\u7b97\u5355\u5143\u6765\u52a0\u901f\u8bad\u7ec3\uff0c\u4f46\u4f7f\u7528\u5927\u6279\u6b21\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u548c\u51c6\u786e\u7387\u964d\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u670d\u52a1\u5668\u6846\u67b6\u7684\u53cc\u6279\u6b21\u5927\u5c0f\u5b66\u4e60\u65b9\u6848\uff0c\u540c\u65f6\u4f7f\u7528\u786c\u4ef6\u652f\u6301\u7684\u6700\u5927\u6279\u6b21\u548c\u8f83\u5c0f\u6279\u6b21\u6765\u589e\u5f3a\u6cdb\u5316\uff1b\u7ed3\u5408\u5faa\u73af\u6e10\u8fdb\u5b66\u4e60\u65b9\u6848\uff0c\u4ece\u4f4e\u5230\u9ad8\u9010\u6b65\u8c03\u6574\u56fe\u50cf\u5206\u8fa8\u7387\u6765\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u5728CIFAR-100\u4e0a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53473.3%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1110.6%\uff1b\u5728ImageNet\u4e0a\u51c6\u786e\u7387\u63d0\u53470.1%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1135.7%\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u5927\u6279\u6b21\u8bad\u7ec3\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2509.25208", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.25208", "abs": "https://arxiv.org/abs/2509.25208", "authors": ["Zenghui Huang", "Ting Shu", "Zhonglei Wang", "Yang Lu", "Yan Yan", "Wei Zhong", "Hanzi Wang"], "title": "DPSformer: A long-tail-aware model for improving heavy rainfall prediction", "comment": null, "summary": "Accurate and timely forecasting of heavy rainfall remains a critical\nchallenge for modern society. Precipitation exhibits a highly imbalanced\ndistribution: most observations record no or light rain, while heavy rainfall\nevents are rare. Such an imbalanced distribution obstructs deep learning models\nfrom effectively predicting heavy rainfall events. To address this challenge,\nwe treat rainfall forecasting explicitly as a long-tailed learning problem,\nidentifying the insufficient representation of heavy rainfall events as the\nprimary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a\nlong-tail-aware model that enriches representation of heavy rainfall events\nthrough a high-resolution branch. For heavy rainfall events $ \\geq $ 50 mm/6 h,\nDPSformer lifts the Critical Success Index (CSI) of a baseline Numerical\nWeather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of\nheavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing\nexisting methods. Our work establishes an effective long-tailed paradigm for\nheavy rainfall prediction, offering a practical tool to enhance early warning\nsystems and mitigate the societal impacts of extreme weather events.", "AI": {"tldr": "DPSformer\u662f\u4e00\u4e2a\u9488\u5bf9\u5f3a\u964d\u96e8\u9884\u6d4b\u7684\u957f\u5c3e\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u5206\u652f\u589e\u5f3a\u5bf9\u7f55\u89c1\u5f3a\u964d\u96e8\u4e8b\u4ef6\u7684\u8868\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u964d\u96e8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f3a\u964d\u96e8\u9884\u6d4b\u5bf9\u793e\u4f1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u964d\u6c34\u6570\u636e\u5206\u5e03\u9ad8\u5ea6\u4e0d\u5e73\u8861\u2014\u2014\u5927\u591a\u6570\u89c2\u6d4b\u8bb0\u5f55\u65e0\u96e8\u6216\u5c0f\u96e8\uff0c\u800c\u5f3a\u964d\u96e8\u4e8b\u4ef6\u7f55\u89c1\u3002\u8fd9\u79cd\u4e0d\u5e73\u8861\u5206\u5e03\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6709\u6548\u9884\u6d4b\u5f3a\u964d\u96e8\u4e8b\u4ef6\u3002", "method": "\u5c06\u964d\u96e8\u9884\u6d4b\u660e\u786e\u89c6\u4e3a\u957f\u5c3e\u5b66\u4e60\u95ee\u9898\uff0c\u5f15\u5165DPSformer\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u5206\u652f\u4e30\u5bcc\u5f3a\u964d\u96e8\u4e8b\u4ef6\u7684\u8868\u5f81\u3002", "result": "\u5bf9\u4e8e\u226550mm/6h\u7684\u5f3a\u964d\u96e8\u4e8b\u4ef6\uff0cDPSformer\u5c06\u57fa\u51c6\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684CSI\u4ece0.012\u63d0\u5347\u52300.067\uff1b\u5bf9\u4e8e\u524d1%\u7684\u5f3a\u964d\u96e8\u4e8b\u4ef6\uff0c\u5176FSS\u8d85\u8fc70.45\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f3a\u964d\u96e8\u9884\u6d4b\u5efa\u7acb\u4e86\u6709\u6548\u7684\u957f\u5c3e\u8303\u5f0f\uff0c\u4e3a\u589e\u5f3a\u9884\u8b66\u7cfb\u7edf\u548c\u51cf\u8f7b\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u793e\u4f1a\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.25210", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.25210", "abs": "https://arxiv.org/abs/2509.25210", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Lei Bai"], "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting", "comment": null, "summary": "To gain finer regional forecasts, many works have explored the regional\nintegration from the global atmosphere, e.g., by solving boundary equations in\nphysics-based methods or cropping regions from global forecasts in data-driven\nmethods. However, the effectiveness of these methods is often constrained by\nstatic and imprecise regional boundaries, resulting in poor generalization\nability. To address this issue, we propose Spatial-Temporal Weather Forecasting\n(STCast), a novel AI-driven framework for adaptive regional boundary\noptimization and dynamic monthly forecast allocation. Specifically, our\napproach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns\nglobal and regional spatial distributions to initialize boundaries and\nadaptively refines them based on attention-derived alignment patterns.\nFurthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where\natmospheric variables from distinct months are dynamically routed to\nspecialized experts using a discrete Gaussian distribution, enhancing the\nmodel's ability to capture temporal patterns. Beyond global and regional\nforecasting, we evaluate our STCast on extreme event prediction and ensemble\nforecasting. Experimental results demonstrate consistent superiority over\nstate-of-the-art methods across all four tasks.", "AI": {"tldr": "STCast\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u65f6\u7a7a\u5929\u6c14\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u533a\u57df\u8fb9\u754c\u4f18\u5316\u548c\u52a8\u6001\u6708\u5ea6\u9884\u6d4b\u5206\u914d\uff0c\u5728\u5168\u5c40\u548c\u533a\u57df\u9884\u6d4b\u3001\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u548c\u96c6\u5408\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533a\u57df\u9884\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u9759\u6001\u548c\u4e0d\u7cbe\u786e\u7684\u533a\u57df\u8fb9\u754c\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u89e3\u51b3\u8fb9\u754c\u7ea6\u675f\u95ee\u9898\u4ee5\u63d0\u5347\u533a\u57df\u9884\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u5bf9\u9f50\u6ce8\u610f\u529b\u673a\u5236(SAA)\u6765\u5bf9\u9f50\u5168\u5c40\u548c\u533a\u57df\u7a7a\u95f4\u5206\u5e03\u4ee5\u521d\u59cb\u5316\u8fb9\u754c\uff0c\u5e76\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u81ea\u9002\u5e94\u4f18\u5316\u8fb9\u754c\uff1b\u8bbe\u8ba1\u65f6\u95f4\u6df7\u5408\u4e13\u5bb6\u6a21\u5757(TMoE)\uff0c\u4f7f\u7528\u79bb\u6563\u9ad8\u65af\u5206\u5e03\u5c06\u4e0d\u540c\u6708\u4efd\u7684\u5927\u6c14\u53d8\u91cf\u52a8\u6001\u8def\u7531\u5230\u4e13\u95e8\u4e13\u5bb6\uff0c\u589e\u5f3a\u65f6\u95f4\u6a21\u5f0f\u6355\u6349\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5168\u5c40\u9884\u6d4b\u3001\u533a\u57df\u9884\u6d4b\u3001\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u548c\u96c6\u5408\u9884\u6d4b\u56db\u4e2a\u4efb\u52a1\u4e2d\uff0cSTCast\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "STCast\u901a\u8fc7\u81ea\u9002\u5e94\u8fb9\u754c\u4f18\u5316\u548c\u52a8\u6001\u4e13\u5bb6\u5206\u914d\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5929\u6c14\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.26182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26182", "abs": "https://arxiv.org/abs/2509.26182", "authors": ["Chris Tong", "Youhe Jiang", "Gufeng Chen", "Tianyi Zhao", "Sibian Lu", "Wenjie Qu", "Eric Yang", "Lynn Ai", "Binhang Yuan"], "title": "Parallax: Efficient LLM Inference Service over Decentralized Environment", "comment": null, "summary": "Deploying a large language model (LLM) inference service remains costly\nbecause centralized serving depends on specialized GPU clusters and\nhigh-bandwidth interconnects in datacenters. An appealing alternative is to\nleverage collaborative decentralized GPU pools. However, heterogeneity in GPU\nand limited interconnected network bandwidth, along with potentially dynamic\navailability, make efficient scheduling the central challenge in this scenario.\nIn this paper, we present Parallax, a decentralized LLM serving system that\nturns a pool of heterogeneous GPUs into an efficient inference platform via a\ntwo-phase scheduler. Parallax decomposes planning into (i) model allocation,\nwhich places layers of each replica across diverse GPUs to jointly optimize\nlatency and throughput under memory and link-bandwidth constraints, and (ii)\nrequest-time GPU pipeline selection, which stitches layers from different\nreplicas into end-to-end execution chains that balance load and adapt to\ncurrent conditions. We implement Parallax and evaluate it on open-source LLMs\ndeployed over real volunteer nodes. Parallax consistently reduces latency and\nincreases throughput relative to decentralized baselines, demonstrating that\nprincipled scheduling can make volunteer compute a practical, affordable\nsubstrate for LLM inference.\n  Github Repo at: https://github.com/GradientHQ/parallax.", "AI": {"tldr": "Parallax\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8c03\u5ea6\u5668\u5c06\u5f02\u6784GPU\u6c60\u8f6c\u5316\u4e3a\u9ad8\u6548\u63a8\u7406\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u96c6\u4e2d\u5f0f\u670d\u52a1\u7684\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u96c6\u4e2d\u5f0fLLM\u63a8\u7406\u670d\u52a1\u4f9d\u8d56\u4e13\u7528GPU\u96c6\u7fa4\u548c\u9ad8\u5e26\u5bbd\u4e92\u8fde\uff0c\u6210\u672c\u9ad8\u6602\u3002\u5229\u7528\u53bb\u4e2d\u5fc3\u5316GPU\u6c60\u662f\u66f4\u5177\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46GPU\u5f02\u6784\u6027\u3001\u6709\u9650\u7f51\u7edc\u5e26\u5bbd\u548c\u52a8\u6001\u53ef\u7528\u6027\u4f7f\u5f97\u9ad8\u6548\u8c03\u5ea6\u6210\u4e3a\u6838\u5fc3\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8c03\u5ea6\u5668\uff1a1) \u6a21\u578b\u5206\u914d\u9636\u6bb5\uff0c\u5c06\u6bcf\u4e2a\u526f\u672c\u7684\u5c42\u5206\u914d\u5230\u4e0d\u540cGPU\u4e0a\uff0c\u5728\u5185\u5b58\u548c\u94fe\u8def\u5e26\u5bbd\u7ea6\u675f\u4e0b\u8054\u5408\u4f18\u5316\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\uff1b2) \u8bf7\u6c42\u65f6GPU\u6d41\u6c34\u7ebf\u9009\u62e9\u9636\u6bb5\uff0c\u5c06\u4e0d\u540c\u526f\u672c\u7684\u5c42\u62fc\u63a5\u6210\u7aef\u5230\u7aef\u6267\u884c\u94fe\uff0c\u5e73\u8861\u8d1f\u8f7d\u5e76\u9002\u5e94\u5f53\u524d\u6761\u4ef6\u3002", "result": "\u5728\u771f\u5b9e\u5fd7\u613f\u8005\u8282\u70b9\u4e0a\u90e8\u7f72\u5f00\u6e90LLM\u8fdb\u884c\u8bc4\u4f30\uff0cParallax\u76f8\u6bd4\u53bb\u4e2d\u5fc3\u5316\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "conclusion": "\u539f\u5219\u6027\u8c03\u5ea6\u53ef\u4ee5\u4f7f\u5fd7\u613f\u8005\u8ba1\u7b97\u6210\u4e3aLLM\u63a8\u7406\u7684\u5b9e\u7528\u4e14\u7ecf\u6d4e\u5b9e\u60e0\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2509.25211", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2509.25211", "abs": "https://arxiv.org/abs/2509.25211", "authors": ["Remi Genet", "Hugo Inzirillo"], "title": "LEMs: A Primer On Large Execution Models", "comment": null, "summary": "This paper introduces Large Execution Models (LEMs), a novel deep learning\nframework that extends transformer-based architectures to address complex\nexecution problems with flexible time boundaries and multiple execution\nconstraints. Building upon recent advances in neural VWAP execution strategies,\nLEMs generalize the approach from fixed-duration orders to scenarios where\nexecution duration is bounded between minimum and maximum time horizons,\nsimilar to share buyback contract structures. The proposed architecture\ndecouples market information processing from execution allocation decisions: a\ncommon feature extraction pipeline using Temporal Kolmogorov-Arnold Networks\n(TKANs), Variable Selection Networks (VSNs), and multi-head attention\nmechanisms processes market data to create informational context, while\nindependent allocation networks handle the specific execution logic for\ndifferent scenarios (fixed quantity vs. fixed notional, buy vs. sell orders).\nThis architectural separation enables a unified model to handle diverse\nexecution objectives while leveraging shared market understanding across\nscenarios. Through comprehensive empirical evaluation on intraday\ncryptocurrency markets and multi-day equity trading using DOW Jones\nconstituents, we demonstrate that LEMs achieve superior execution performance\ncompared to traditional benchmarks by dynamically optimizing execution paths\nwithin flexible time constraints. The unified model architecture enables\ndeployment across different execution scenarios (buy/sell orders, varying\nduration boundaries, volume/notional targets) through a single framework,\nproviding significant operational advantages over asset-specific approaches.", "AI": {"tldr": "LEMs\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u7075\u6d3b\u65f6\u95f4\u8fb9\u754c\u548c\u591a\u7ea6\u675f\u7684\u590d\u6742\u6267\u884c\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u5e02\u573a\u4fe1\u606f\u5904\u7406\u548c\u5206\u914d\u51b3\u7b56\u6765\u5b9e\u73b0\u7edf\u4e00\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u4f20\u7edf\u6267\u884c\u7b56\u7565\u5c40\u9650\u4e8e\u56fa\u5b9a\u65f6\u95f4\u8303\u56f4\uff0c\u65e0\u6cd5\u5904\u7406\u7c7b\u4f3c\u80a1\u7968\u56de\u8d2d\u5408\u7ea6\u7684\u7075\u6d3b\u65f6\u95f4\u8fb9\u754c\u573a\u666f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u4e0d\u540c\u6267\u884c\u76ee\u6807\u548c\u7ea6\u675f\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u4f7f\u7528TKANs\u3001VSNs\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5e02\u573a\u6570\u636e\uff0c\u72ec\u7acb\u5206\u914d\u7f51\u7edc\u5904\u7406\u4e0d\u540c\u6267\u884c\u903b\u8f91\uff0c\u5b9e\u73b0\u5e02\u573a\u4fe1\u606f\u5904\u7406\u4e0e\u6267\u884c\u51b3\u7b56\u7684\u89e3\u8026\u3002", "result": "\u5728\u52a0\u5bc6\u8d27\u5e01\u65e5\u5185\u4ea4\u6613\u548c\u9053\u743c\u65af\u6210\u5206\u80a1\u591a\u65e5\u4ea4\u6613\u4e2d\uff0cLEMs\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6267\u884c\u6027\u80fd\uff0c\u80fd\u5728\u7075\u6d3b\u65f6\u95f4\u7ea6\u675f\u5185\u52a8\u6001\u4f18\u5316\u6267\u884c\u8def\u5f84\u3002", "conclusion": "LEMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u90e8\u7f72\u4e8e\u4e0d\u540c\u6267\u884c\u573a\u666f\uff08\u4e70\u5356\u8ba2\u5355\u3001\u4e0d\u540c\u65f6\u95f4\u8fb9\u754c\u3001\u6570\u91cf/\u540d\u4e49\u76ee\u6807\uff09\uff0c\u76f8\u6bd4\u8d44\u4ea7\u7279\u5b9a\u65b9\u6cd5\u5177\u6709\u663e\u8457\u64cd\u4f5c\u4f18\u52bf\u3002"}}
{"id": "2509.26193", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26193", "abs": "https://arxiv.org/abs/2509.26193", "authors": ["Fabian Czappa", "Marvin Kaster", "Felix Wolf"], "title": "I Like To Move It -- Computation Instead of Data in the Brain", "comment": null, "summary": "The detailed functioning of the human brain is still poorly understood. Brain\nsimulations are a well-established way to complement experimental research, but\nmust contend with the computational demands of the approximately $10^{11}$\nneurons and the $10^{14}$ synapses connecting them, the network of the latter\nreferred to as the connectome. Studies suggest that changes in the connectome\n(i.e., the formation and deletion of synapses, also known as structural\nplasticity) are essential for critical tasks such as memory formation and\nlearning. The connectivity update can be efficiently computed using a\nBarnes-Hut-inspired approximation that lowers the computational complexity from\n$O(n^2)$ to $O(n log n)$, where n is the number of neurons. However, updating\nsynapses, which relies heavily on RMA, and the spike exchange between neurons,\nwhich requires all-to-all communication at every time step, still hinder\nscalability. We present a new algorithm that significantly reduces the\ncommunication overhead by moving computation instead of data. This shrinks the\ntime it takes to update connectivity by a factor of six and the time it takes\nto exchange spikes by more than two orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u79fb\u52a8\u8ba1\u7b97\u800c\u975e\u6570\u636e\u6765\u663e\u8457\u51cf\u5c11\u8111\u6a21\u62df\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5c06\u8fde\u63a5\u6027\u66f4\u65b0\u65f6\u95f4\u51cf\u5c116\u500d\uff0c\u5c16\u5cf0\u4ea4\u6362\u65f6\u95f4\u51cf\u5c11\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u8111\u6a21\u62df\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff1a1011\u4e2a\u795e\u7ecf\u5143\u548c1014\u4e2a\u7a81\u89e6\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\uff0c\u7a81\u89e6\u66f4\u65b0\u4f9d\u8d56RMA\u548c\u795e\u7ecf\u5143\u95f4\u5168\u5bf9\u5168\u901a\u4fe1\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528Barnes-Hut\u8fd1\u4f3c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u5230O(n log n)\uff0c\u5e76\u63d0\u51fa\u65b0\u7b97\u6cd5\u901a\u8fc7\u79fb\u52a8\u8ba1\u7b97\u800c\u975e\u6570\u636e\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u65b0\u7b97\u6cd5\u4f7f\u8fde\u63a5\u6027\u66f4\u65b0\u65f6\u95f4\u51cf\u5c116\u500d\uff0c\u5c16\u5cf0\u4ea4\u6362\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc7\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8111\u6a21\u62df\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u7406\u89e3\u5927\u8111\u529f\u80fd\u548c\u7ed3\u6784\u53ef\u5851\u6027\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2509.25213", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25213", "abs": "https://arxiv.org/abs/2509.25213", "authors": ["Sai Varun Kodathala"], "title": "Six Sigma For Neural Networks: Taguchi-based optimization", "comment": "23 Pages, 9 Tables", "summary": "The optimization of hyperparameters in convolutional neural networks (CNNs)\nremains a challenging and computationally expensive process, often requiring\nextensive trial-and-error approaches or exhaustive grid searches. This study\nintroduces the application of Taguchi Design of Experiments methodology, a\nstatistical optimization technique traditionally used in quality engineering,\nto systematically optimize CNN hyperparameters for professional boxing action\nrecognition. Using an L12(211) orthogonal array, eight hyperparameters\nincluding image size, color mode, activation function, learning rate,\nrescaling, shuffling, vertical flip, and horizontal flip were systematically\nevaluated across twelve experimental configurations. To address the\nmulti-objective nature of machine learning optimization, five different\napproaches were developed to simultaneously optimize training accuracy,\nvalidation accuracy, training loss, and validation loss using Signal-to-Noise\nratio analysis. The study employed a novel logarithmic scaling technique to\nunify conflicting metrics and enable comprehensive multi-quality assessment\nwithin the Taguchi framework. Results demonstrate that Approach 3, combining\nweighted accuracy metrics with logarithmically transformed loss functions,\nachieved optimal performance with 98.84% training accuracy and 86.25%\nvalidation accuracy while maintaining minimal loss values. The Taguchi analysis\nrevealed that learning rate emerged as the most influential parameter, followed\nby image size and activation function, providing clear guidance for\nhyperparameter prioritization in CNN optimization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5e94\u7528\u7530\u53e3\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u4f18\u5316CNN\u8d85\u53c2\u6570\uff0c\u7528\u4e8e\u804c\u4e1a\u62f3\u51fb\u52a8\u4f5c\u8bc6\u522b\u3002\u901a\u8fc7L12\u6b63\u4ea4\u9635\u5217\u8bc4\u4f308\u4e2a\u8d85\u53c2\u6570\uff0c\u5f00\u53d1\u4e865\u79cd\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u5176\u4e2d\u65b9\u6cd53\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523098.84%\u8bad\u7ec3\u51c6\u786e\u7387\u548c86.25%\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "CNN\u8d85\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\u5177\u6709\u6311\u6218\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bd5\u9519\u6216\u7a77\u4e3e\u641c\u7d22\u3002\u4f20\u7edf\u8d28\u91cf\u5de5\u7a0b\u4e2d\u7684\u7530\u53e3\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4f7f\u7528L12(211)\u6b63\u4ea4\u9635\u5217\u7cfb\u7edf\u8bc4\u4f308\u4e2a\u8d85\u53c2\u6570\uff0c\u5305\u62ec\u56fe\u50cf\u5c3a\u5bf8\u3001\u989c\u8272\u6a21\u5f0f\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u5b66\u4e60\u7387\u7b49\u3002\u5f00\u53d1\u4e865\u79cd\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u566a\u6bd4\u5206\u6790\uff0c\u5e76\u91c7\u7528\u5bf9\u6570\u7f29\u653e\u6280\u672f\u7edf\u4e00\u51b2\u7a81\u6307\u6807\u3002", "result": "\u65b9\u6cd53\uff08\u7ed3\u5408\u52a0\u6743\u51c6\u786e\u7387\u6307\u6807\u548c\u5bf9\u6570\u53d8\u6362\u635f\u5931\u51fd\u6570\uff09\u8868\u73b0\u6700\u4f18\uff1a\u8bad\u7ec3\u51c6\u786e\u738798.84%\uff0c\u9a8c\u8bc1\u51c6\u786e\u738786.25%\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5c0f\u635f\u5931\u503c\u3002\u5b66\u4e60\u7387\u662f\u6700\u5173\u952e\u53c2\u6570\uff0c\u5176\u6b21\u662f\u56fe\u50cf\u5c3a\u5bf8\u548c\u6fc0\u6d3b\u51fd\u6570\u3002", "conclusion": "\u7530\u53e3\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316CNN\u8d85\u53c2\u6570\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u53c2\u6570\u4f18\u5148\u7ea7\u6307\u5bfc\uff0c\u5b66\u4e60\u7387\u662f\u6700\u91cd\u8981\u7684\u5f71\u54cd\u56e0\u7d20\u3002\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25214", "abs": "https://arxiv.org/abs/2509.25214", "authors": ["Rongguang Ye", "Ming Tang", "Edith C. H. Ngai"], "title": "On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs", "comment": null, "summary": "As increasingly large pre-trained models are released, deploying them on edge\ndevices for privacy-preserving applications requires effective compression.\nRecent works combine quantization with the fine-tuning of high-precision LoRA\nadapters, which can substantially reduce model size while mitigating the\naccuracy loss from quantization. However, edge devices have inherently\nheterogeneous capabilities, while performing configuration-wise fine-tuning for\nevery quantization setting is computationally prohibitive. In this paper, we\npropose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to\narbitrary quantization configurations (i.e., the per-layer bit-width choices of\na pre-trained model) without requiring repeated fine-tuning. This is\naccomplished via a configuration-aware model that maps each configuration to\nits low-rank adjustments. The effectiveness of this model critically depends on\nthe training configuration set, a collection of configurations chosen to cover\ndifferent total bit-width budgets. However, constructing a high-quality\nconfiguration set is non-trivial. We therefore design a Pareto-based\nconfiguration search that iteratively optimizes the training configuration set,\nyielding more precise low-rank adjustments. Our experiments demonstrate that,\nunlike the state-of-the-art methods that require fine-tuning a separate LoRA\nadapter for each configuration, CoA-LoRA incurs no additional time cost while\nachieving comparable or even superior performance to those methods.", "AI": {"tldr": "CoA-LoRA\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574LoRA\u9002\u914d\u5668\u4ee5\u9002\u5e94\u4efb\u610f\u91cf\u5316\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u590d\u5fae\u8c03\uff0c\u901a\u8fc7\u914d\u7f6e\u611f\u77e5\u6a21\u578b\u5c06\u6bcf\u4e2a\u914d\u7f6e\u6620\u5c04\u5230\u5176\u4f4e\u79e9\u8c03\u6574\u3002", "motivation": "\u968f\u7740\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53d1\u5e03\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u3002\u73b0\u6709\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u91cf\u5316\u914d\u7f6e\u5355\u72ec\u5fae\u8c03\u9ad8\u7cbe\u5ea6LoRA\u9002\u914d\u5668\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u8fb9\u7f18\u8bbe\u5907\u80fd\u529b\u5f02\u6784\u3002", "method": "\u63d0\u51fa\u914d\u7f6e\u611f\u77e5\u6a21\u578b\uff0c\u5c06\u4efb\u610f\u91cf\u5316\u914d\u7f6e\u6620\u5c04\u5230\u4f4e\u79e9\u8c03\u6574\uff1b\u8bbe\u8ba1\u57fa\u4e8ePareto\u7684\u914d\u7f6e\u641c\u7d22\u7b97\u6cd5\uff0c\u8fed\u4ee3\u4f18\u5316\u8bad\u7ec3\u914d\u7f6e\u96c6\u4ee5\u8986\u76d6\u4e0d\u540c\u603b\u6bd4\u7279\u5bbd\u5ea6\u9884\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoA-LoRA\u65e0\u9700\u989d\u5916\u65f6\u95f4\u6210\u672c\uff0c\u5c31\u80fd\u8fbe\u5230\u4e0e\u9700\u8981\u4e3a\u6bcf\u4e2a\u914d\u7f6e\u5355\u72ec\u5fae\u8c03LoRA\u9002\u914d\u5668\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "CoA-LoRA\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4efb\u610f\u91cf\u5316\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u4e2d\u7684\u5f02\u6784\u80fd\u529b\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u3002"}}
{"id": "2509.26529", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26529", "abs": "https://arxiv.org/abs/2509.26529", "authors": ["Shangshu Qian", "Lin Tan", "Yongle Zhang"], "title": "CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations", "comment": "Accepted by EuroSys 2026", "summary": "Recent studies have revealed that self-sustaining cascading failures in\ndistributed systems frequently lead to widespread outages, which are\nchallenging to contain and recover from. Existing failure detection techniques\nstruggle to expose such failures prior to deployment, as they typically require\na complex combination of specific conditions to be triggered. This challenge\nstems from the inherent nature of cascading failures, as they typically involve\na sequence of fault propagations, each activated by distinct conditions.\n  This paper presents CSnake, a fault injection framework to expose\nself-sustaining cascading failures in distributed systems. CSnake uses the\nnovel idea of causal stitching, which causally links multiple single-fault\ninjections in different tests to simulate complex fault propagation chains. To\nidentify these chains, CSnake designs a counterfactual causality analysis of\nfault propagations - fault causality analysis (FCA): FCA compares the execution\ntrace of a fault injection run with its corresponding profile run (i.e., same\ntest w/o the injection) and identifies any additional faults triggered, which\nare considered to have a causal relationship with the injected fault.\n  To address the large search space of fault and workload combinations, CSnake\nemploys a three-phase allocation protocol of test budget that prioritizes\nfaults with unique and diverse causal consequences, increasing the likelihood\nof uncovering conditional fault propagations. Furthermore, to avoid incorrectly\nconnecting fault propagations from workloads with incompatible conditions,\nCSnake performs a local compatibility check that approximately checks the\ncompatibility of the path constraints associated with connected fault\npropagations with low overhead.\n  CSnake detected 15 bugs that cause self-sustaining cascading failures in five\nsystems, five of which have been confirmed with two fixed.", "AI": {"tldr": "CSnake\u662f\u4e00\u4e2a\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u62fc\u63a5\u6280\u672f\u6a21\u62df\u590d\u6742\u7684\u6545\u969c\u4f20\u64ad\u94fe\uff0c\u7528\u4e8e\u68c0\u6d4b\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u68c0\u6d4b\u6280\u672f\u96be\u4ee5\u5728\u90e8\u7f72\u524d\u66b4\u9732\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6545\u969c\u9700\u8981\u7279\u5b9a\u6761\u4ef6\u7684\u590d\u6742\u7ec4\u5408\u624d\u80fd\u89e6\u53d1\uff0c\u6d89\u53ca\u591a\u4e2a\u6545\u969c\u4f20\u64ad\u5e8f\u5217\u3002", "method": "\u91c7\u7528\u56e0\u679c\u62fc\u63a5\u601d\u60f3\uff0c\u5c06\u4e0d\u540c\u6d4b\u8bd5\u4e2d\u7684\u5355\u6545\u969c\u6ce8\u5165\u56e0\u679c\u8fde\u63a5\uff1b\u8bbe\u8ba1\u6545\u969c\u56e0\u679c\u5206\u6790(FCA)\u8bc6\u522b\u6545\u969c\u4f20\u64ad\u94fe\uff1b\u4f7f\u7528\u4e09\u9636\u6bb5\u6d4b\u8bd5\u9884\u7b97\u5206\u914d\u534f\u8bae\u548c\u672c\u5730\u517c\u5bb9\u6027\u68c0\u67e5\u6765\u4f18\u5316\u641c\u7d22\u3002", "result": "\u5728\u4e94\u4e2a\u7cfb\u7edf\u4e2d\u68c0\u6d4b\u523015\u4e2a\u5bfc\u81f4\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\u7684bug\uff0c\u5176\u4e2d5\u4e2a\u5df2\u786e\u8ba4\uff0c2\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "CSnake\u6846\u67b6\u80fd\u6709\u6548\u66b4\u9732\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\uff0c\u901a\u8fc7\u56e0\u679c\u62fc\u63a5\u548c\u4f18\u5316\u641c\u7d22\u7b56\u7565\u89e3\u51b3\u4e86\u590d\u6742\u6545\u969c\u4f20\u64ad\u7684\u68c0\u6d4b\u6311\u6218\u3002"}}
{"id": "2509.25215", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25215", "abs": "https://arxiv.org/abs/2509.25215", "authors": ["Pierre Lotte", "Andr\u00e9 P\u00e9ninou", "Olivier Teste"], "title": "Anomaly detection by partitioning of multi-variate time series", "comment": "in French language", "summary": "In this article, we suggest a novel non-supervised partition based anomaly\ndetection method for anomaly detection in multivariate time series called\nPARADISE. This methodology creates a partition of the variables of the time\nseries while ensuring that the inter-variable relations remain untouched. This\npartitioning relies on the clustering of multiple correlation coefficients\nbetween variables to identify subsets of variables before executing anomaly\ndetection algorithms locally for each of those subsets. Through multiple\nexperimentations done on both synthetic and real datasets coming from the\nliterature, we show the relevance of our approach with a significant\nimprovement in anomaly detection performance.", "AI": {"tldr": "\u63d0\u51faPARADISE\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u533a\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u53d8\u91cf\u805a\u7c7b\u4fdd\u6301\u53d8\u91cf\u95f4\u5173\u7cfb\uff0c\u5728\u5c40\u90e8\u5b50\u96c6\u6267\u884c\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5", "motivation": "\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d8\u91cf\u95f4\u5173\u7cfb\u590d\u6742\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u533a\u65b9\u6cd5\u4fdd\u6301\u53d8\u91cf\u95f4\u5173\u7cfb\u540c\u65f6\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd", "method": "\u57fa\u4e8e\u53d8\u91cf\u95f4\u591a\u91cd\u76f8\u5173\u7cfb\u6570\u805a\u7c7b\u521b\u5efa\u5206\u533a\uff0c\u786e\u4fdd\u53d8\u91cf\u95f4\u5173\u7cfb\u4e0d\u53d8\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u5b50\u96c6\u5c40\u90e8\u6267\u884c\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd", "conclusion": "PARADISE\u65b9\u6cd5\u901a\u8fc7\u5206\u533a\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u679c"}}
{"id": "2509.25216", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25216", "abs": "https://arxiv.org/abs/2509.25216", "authors": ["Guillermo Comesa\u00f1a Cimadevila"], "title": "Evaluating Double Descent in Machine Learning: Insights from Tree-Based Models Applied to a Genomic Prediction Task", "comment": "14 pages, 7 figures", "summary": "Classical learning theory describes a well-characterised U-shaped\nrelationship between model complexity and prediction error, reflecting a\ntransition from underfitting in underparameterised regimes to overfitting as\ncomplexity grows. Recent work, however, has introduced the notion of a second\ndescent in test error beyond the interpolation threshold-giving rise to the\nso-called double descent phenomenon. While double descent has been studied\nextensively in the context of deep learning, it has also been reported in\nsimpler models, including decision trees and gradient boosting. In this work,\nwe revisit these claims through the lens of classical machine learning applied\nto a biological classification task: predicting isoniazid resistance in\nMycobacterium tuberculosis using whole-genome sequencing data. We\nsystematically vary model complexity along two orthogonal axes-learner capacity\n(e.g., Pleaf, Pboost) and ensemble size (i.e., Pens)-and show that double\ndescent consistently emerges only when complexity is scaled jointly across\nthese axes. When either axis is held fixed, generalisation behaviour reverts to\nclassical U- or L-shaped patterns. These results are replicated on a synthetic\nbenchmark and support the unfolding hypothesis, which attributes double descent\nto the projection of distinct generalisation regimes onto a single complexity\naxis. Our findings underscore the importance of treating model complexity as a\nmultidimensional construct when analysing generalisation behaviour. All code\nand reproducibility materials are available at:\nhttps://github.com/guillermocomesanacimadevila/Demystifying-Double-Descent-in-ML.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53cc\u4e0b\u964d\u73b0\u8c61\uff0c\u901a\u8fc7\u5728\u7ed3\u6838\u75c5\u8010\u836f\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u7cfb\u7edf\u7814\u7a76\u6a21\u578b\u590d\u6742\u6027\uff0c\u53d1\u73b0\u53cc\u4e0b\u964d\u4ec5\u5728\u540c\u65f6\u8c03\u8282\u5b66\u4e60\u5668\u80fd\u529b\u548c\u96c6\u6210\u89c4\u6a21\u4e24\u4e2a\u7ef4\u5ea6\u65f6\u51fa\u73b0\uff0c\u652f\u6301\u5c55\u5f00\u5047\u8bf4\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u7406\u8bba\u63cf\u8ff0\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u9884\u6d4b\u8bef\u5dee\u7684U\u5f62\u5173\u7cfb\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\u53cc\u4e0b\u964d\u73b0\u8c61\u3002\u867d\u7136\u53cc\u4e0b\u964d\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u51b3\u7b56\u6811\u7b49\u7b80\u5355\u6a21\u578b\u4e2d\u4e5f\u5b58\u5728\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u7269\u5206\u7c7b\u4efb\u52a1\u91cd\u65b0\u9a8c\u8bc1\u8fd9\u4e9b\u53d1\u73b0\u3002", "method": "\u4f7f\u7528\u5168\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u6570\u636e\u9884\u6d4b\u7ed3\u6838\u5206\u679d\u6746\u83cc\u5f02\u70df\u80bc\u8010\u836f\u6027\uff0c\u7cfb\u7edf\u8c03\u8282\u6a21\u578b\u590d\u6742\u5ea6\u7684\u4e24\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\uff1a\u5b66\u4e60\u5668\u80fd\u529b\uff08\u5982Pleaf\u3001Pboost\uff09\u548c\u96c6\u6210\u89c4\u6a21\uff08Pens\uff09\uff0c\u5206\u6790\u6cdb\u5316\u884c\u4e3a\u3002", "result": "\u53cc\u4e0b\u964d\u4ec5\u5728\u540c\u65f6\u8c03\u8282\u4e24\u4e2a\u590d\u6742\u5ea6\u7ef4\u5ea6\u65f6\u4e00\u81f4\u51fa\u73b0\u3002\u5f53\u4efb\u4e00\u7ef4\u5ea6\u56fa\u5b9a\u65f6\uff0c\u6cdb\u5316\u884c\u4e3a\u56de\u5f52\u5230\u7ecf\u5178\u7684U\u5f62\u6216L\u5f62\u6a21\u5f0f\u3002\u8fd9\u4e9b\u7ed3\u679c\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u590d\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5c55\u5f00\u5047\u8bf4\uff0c\u8868\u660e\u53cc\u4e0b\u964d\u6e90\u4e8e\u4e0d\u540c\u6cdb\u5316\u673a\u5236\u5728\u5355\u4e00\u590d\u6742\u5ea6\u8f74\u4e0a\u7684\u6295\u5f71\u3002\u5f3a\u8c03\u5728\u5206\u6790\u6cdb\u5316\u884c\u4e3a\u65f6\uff0c\u5e94\u5c06\u6a21\u578b\u590d\u6742\u5ea6\u89c6\u4e3a\u591a\u7ef4\u7ed3\u6784\u3002"}}
{"id": "2509.26541", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26541", "abs": "https://arxiv.org/abs/2509.26541", "authors": ["Yida Wang", "Ke Hong", "Xiuhong Li", "Yuanchao Xu", "Wenxun Wang", "Guohao Dai", "Yu Wang"], "title": "TASP: Topology-aware Sequence Parallelism", "comment": null, "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.", "AI": {"tldr": "TASP\u662f\u4e00\u79cd\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587LLM\u7684\u62d3\u6251\u611f\u77e5\u5e8f\u5217\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u5206\u89e3\u548c\u539f\u8bed\u5206\u89e3\u5145\u5206\u5229\u7528\u73b0\u4ee3\u52a0\u901f\u5668\u7684\u901a\u4fe1\u80fd\u529b\uff0c\u76f8\u6bd4Ring Attention\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.58\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u4e3b\u6d41\u5e8f\u5217\u5e76\u884c\u65b9\u6cd5Ring Attention\u7531\u4e8e\u91c7\u7528Ring AllGather\u901a\u4fe1\u539f\u8bed\uff0c\u4e0e\u73b0\u4ee3\u52a0\u901f\u5668\u7684AlltoAll\u62d3\u6251\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u54c8\u5bc6\u987f\u5206\u89e3\u7406\u8bba\uff0c\u5c06\u73b0\u4ee3\u52a0\u901f\u5668\u62d3\u6251\u5206\u89e3\u4e3a\u591a\u4e2a\u6b63\u4ea4\u73af\u5f62\u6570\u636e\u8def\u5f84\uff0c\u540c\u65f6\u5c06Ring AllGather\u539f\u8bed\u5206\u89e3\u4e3a\u76f8\u540c\u6570\u91cf\u7684\u5e76\u53d1\u73af\u5f62\u6570\u636e\u4f20\u8f93\uff0c\u63d0\u51faTASP\u65b9\u6cd5\u3002", "result": "\u5728\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9NVIDIA H100\u7cfb\u7edf\u4ee5\u53ca\u5355\u8282\u70b9AMD MI300X\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTASP\u6bd4Ring Attention\u53ca\u5176\u53d8\u79cdZigzag-Ring Attention\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u901a\u4fe1\u6548\u7387\uff0c\u6700\u9ad8\u53ef\u8fbe3.58\u500d\u52a0\u901f\u3002", "conclusion": "TASP\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u5e8f\u5217\u5e76\u884c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.25217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25217", "abs": "https://arxiv.org/abs/2509.25217", "authors": ["Brij Malhotra", "Shivvrat Arya", "Tahrima Rahman", "Vibhav Giridhar Gogate"], "title": "Learning to Condition: A Neural Heuristic for Scalable MPE Inference", "comment": "Will appear in NeurIPS 2025", "summary": "We introduce learning to condition (L2C), a scalable, data-driven framework\nfor accelerating Most Probable Explanation (MPE) inference in Probabilistic\nGraphical Models (PGMs), a fundamentally intractable problem. L2C trains a\nneural network to score variable-value assignments based on their utility for\nconditioning, given observed evidence. To facilitate supervised learning, we\ndevelop a scalable data generation pipeline that extracts training signals from\nthe search traces of existing MPE solvers. The trained network serves as a\nheuristic that integrates with search algorithms, acting as a conditioning\nstrategy prior to exact inference or as a branching and node selection policy\nwithin branch-and-bound solvers. We evaluate L2C on challenging MPE queries\ninvolving high-treewidth PGMs. Experiments show that our learned heuristic\nsignificantly reduces the search space while maintaining or improving solution\nquality over state-of-the-art methods.", "AI": {"tldr": "L2C\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u8bc4\u5206\u53d8\u91cf\u8d4b\u503c\uff0c\u52a0\u901f\u6982\u7387\u56fe\u6a21\u578b\u4e2d\u6700\u53ef\u80fd\u89e3\u91ca\u63a8\u7406\uff0c\u663e\u8457\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u5e76\u4fdd\u6301\u6216\u63d0\u9ad8\u89e3\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u6982\u7387\u56fe\u6a21\u578b\u4e2d\u6700\u53ef\u80fd\u89e3\u91ca\u63a8\u7406\u8fd9\u4e00\u6839\u672c\u96be\u89e3\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u53d8\u91cf\u8d4b\u503c\u5bf9\u6761\u4ef6\u5316\u7684\u6548\u7528\uff0c\u5229\u7528\u73b0\u6709MPE\u6c42\u89e3\u5668\u7684\u641c\u7d22\u8f68\u8ff9\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u4f5c\u4e3a\u542f\u53d1\u5f0f\u4e0e\u641c\u7d22\u7b97\u6cd5\u96c6\u6210\u3002", "result": "\u5728\u9ad8\u6811\u5bbd\u6982\u7387\u56fe\u6a21\u578b\u7684\u6311\u6218\u6027MPE\u67e5\u8be2\u4e0a\uff0c\u5b66\u4e60\u5230\u7684\u542f\u53d1\u5f0f\u663e\u8457\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u89e3\u8d28\u91cf\u3002", "conclusion": "L2C\u6846\u67b6\u6210\u529f\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u52a0\u901f\u96be\u89e3\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6982\u7387\u56fe\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25218", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25218", "abs": "https://arxiv.org/abs/2509.25218", "authors": ["Tobiasz Puslecki", "Krzysztof Walkowiak"], "title": "On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary Study", "comment": null, "summary": "The recent progress in TinyML technologies triggers the need to address the\nchallenge of balancing inference time and classification quality. TinyML\nsystems are defined by specific constraints in computation, memory and energy.\nThese constraints emphasize the need for specialized optimization techniques\nwhen implementing Machine Learning (ML) applications on such platforms. While\ndeep neural networks are widely used in TinyML, the exploration of Dynamic\nEnsemble Selection (DES) methods is also beneficial. This study examines a\nDES-Clustering approach for a multi-class computer vision task within TinyML\nsystems. This method allows for adjusting classification accuracy, thereby\naffecting latency and energy consumption per inference. We implemented the\nTinyDES-Clustering library, optimized for embedded system limitations.\nExperiments have shown that a larger pool of classifiers for dynamic selection\nimproves classification accuracy, and thus leads to an increase in average\ninference time on the TinyML device.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728TinyML\u7cfb\u7edf\u4e2d\u4f7f\u7528\u52a8\u6001\u96c6\u6210\u9009\u62e9\u805a\u7c7b\u65b9\u6cd5\u8fdb\u884c\u591a\u7c7b\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u901a\u8fc7\u8c03\u6574\u5206\u7c7b\u7cbe\u5ea6\u6765\u5e73\u8861\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\u3002", "motivation": "TinyML\u6280\u672f\u5728\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u80fd\u91cf\u65b9\u9762\u7684\u7279\u5b9a\u7ea6\u675f\u9700\u8981\u4e13\u95e8\u7684\u4f18\u5316\u6280\u672f\uff0c\u800c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728TinyML\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u63a2\u7d22\u52a8\u6001\u96c6\u6210\u9009\u62e9\u65b9\u6cd5\u4e5f\u5f88\u6709\u76ca\u3002", "method": "\u91c7\u7528\u52a8\u6001\u96c6\u6210\u9009\u62e9\u805a\u7c7b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9488\u5bf9\u5d4c\u5165\u5f0f\u7cfb\u7edf\u9650\u5236\u4f18\u5316\u7684TinyDES-Clustering\u5e93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u4e8e\u52a8\u6001\u9009\u62e9\u7684\u66f4\u5927\u5206\u7c7b\u5668\u6c60\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f46\u4f1a\u5bfc\u81f4TinyML\u8bbe\u5907\u4e0a\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u7684\u589e\u52a0\u3002", "conclusion": "\u52a8\u6001\u96c6\u6210\u9009\u62e9\u805a\u7c7b\u65b9\u6cd5\u80fd\u591f\u5728TinyML\u7cfb\u7edf\u4e2d\u6709\u6548\u5e73\u8861\u5206\u7c7b\u7cbe\u5ea6\u4e0e\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u9700\u8981\u5728\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002"}}
{"id": "2509.25222", "categories": ["cs.LG", "cs.RO", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2509.25222", "abs": "https://arxiv.org/abs/2509.25222", "authors": ["Yutong Liang", "Chang Hou", "Guy Y. Cornejo Maceda", "Andrea Ianiro", "Stefano Discetti", "Andrea Meil\u00e1n-Vila", "Didier Sornette", "Sandro Claudio Lera", "Jialong Chen", "Xiaozhou He", "Bernd R. Noack"], "title": "Sensor optimization for urban wind estimation with cluster-based probabilistic framework", "comment": null, "summary": "We propose a physics-informed machine-learned framework for sensor-based flow\nestimation for drone trajectories in complex urban terrain. The input is a rich\nset of flow simulations at many wind conditions. The outputs are velocity and\nuncertainty estimates for a target domain and subsequent sensor optimization\nfor minimal uncertainty. The framework has three innovations compared to\ntraditional flow estimators. First, the algorithm scales proportionally to the\ndomain complexity, making it suitable for flows that are too complex for any\nmonolithic reduced-order representation. Second, the framework extrapolates\nbeyond the training data, e.g., smaller and larger wind velocities. Last, and\nperhaps most importantly, the sensor location is a free input, significantly\nextending the vast majority of the literature. The key enablers are (1) a\nReynolds number-based scaling of the flow variables, (2) a physics-based domain\ndecomposition, (3) a cluster-based flow representation for each subdomain, (4)\nan information entropy correlating the subdomains, and (5) a multi-variate\nprobability function relating sensor input and targeted velocity estimates.\nThis framework is demonstrated using drone flight paths through a\nthree-building cluster as a simple example. We anticipate adaptations and\napplications for estimating complete cities and incorporating weather input.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u57ce\u5e02\u5730\u5f62\u4e2d\u65e0\u4eba\u673a\u8f68\u8ff9\u7684\u4f20\u611f\u5668\u6d41\u573a\u4f30\u8ba1\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u6d41\u573a\u3001\u5916\u63a8\u8bad\u7ec3\u6570\u636e\u5e76\u4f18\u5316\u4f20\u611f\u5668\u4f4d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u57ce\u5e02\u5730\u5f62\u4e2d\u7684\u6d41\u573a\uff0c\u4e14\u4f20\u611f\u5668\u4f4d\u7f6e\u56fa\u5b9a\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u590d\u6742\u6d41\u573a\u3001\u5916\u63a8\u4e0d\u540c\u98ce\u901f\u6761\u4ef6\u5e76\u4f18\u5316\u4f20\u611f\u5668\u5e03\u5c40\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u96f7\u8bfa\u6570\u7f29\u653e\u6d41\u573a\u53d8\u91cf\u3001\u57fa\u4e8e\u7269\u7406\u7684\u57df\u5206\u89e3\u3001\u5b50\u57df\u805a\u7c7b\u6d41\u573a\u8868\u793a\u3001\u5b50\u57df\u95f4\u4fe1\u606f\u71b5\u5173\u8054\u4ee5\u53ca\u4f20\u611f\u5668\u8f93\u5165\u4e0e\u76ee\u6807\u901f\u5ea6\u4f30\u8ba1\u7684\u591a\u53d8\u91cf\u6982\u7387\u51fd\u6570\u3002", "result": "\u6846\u67b6\u5728\u4e09\u4e2a\u5efa\u7b51\u96c6\u7fa4\u7684\u65e0\u4eba\u673a\u98de\u884c\u8def\u5f84\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u6d41\u573a\u3001\u5916\u63a8\u8bad\u7ec3\u6570\u636e\u5e76\u4f18\u5316\u4f20\u611f\u5668\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57ce\u5e02\u5c3a\u5ea6\u6d41\u573a\u4f30\u8ba1\u548c\u5929\u6c14\u8f93\u5165\u96c6\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u5e94\u6027\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.25223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25223", "abs": "https://arxiv.org/abs/2509.25223", "authors": ["Xunhao Lai", "Jialiang Kang", "Jianqiao Lu", "Tong Lin", "Pengyu Zhao"], "title": "Enhancing Linear Attention with Residual Learning", "comment": "15 pages, 4 figures", "summary": "Linear attention offers a linear-time alternative to self-attention but often\nstruggles to capture long-range patterns. We revisit linear attention through a\nprediction-correction lens and show that prevalent variants can be written as a\ncombination of a historical prediction and a single-token correction, which\ncreates an expressivity bottleneck. To address this bottleneck, we introduce\nResidual Linear Attention (RLA), a framework that equips linear attention with\nan explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent\nstate that learns to accumulate residual errors over time and correct the base\nprediction. We further instantiate a delta-rule version, Residual Delta Net\n(RDN), incorporating adaptive gating and residual clipping for enhanced\ncorrection control and stability. Our implementation leverages highly optimized\nlinear attention kernels and preserves linear time and memory. Across language\nmodeling and recall-intensive evaluations, RLA and RDN consistently outperform\ntheir respective baselines and other modern linear-attention methods, narrowing\nthe gap to standard Transformers while retaining linear scaling.", "AI": {"tldr": "\u63d0\u51fa\u4e86Residual Linear Attention (RLA)\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6b8b\u5dee\u62df\u5408\u673a\u5236\u89e3\u51b3\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u6355\u83b7\u957f\u7a0b\u6a21\u5f0f\u65f6\u7684\u8868\u8fbe\u80fd\u529b\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e86Residual Delta Net (RDN)\u7248\u672c\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u7136\u63d0\u4f9b\u7ebf\u6027\u65f6\u95f4\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u6355\u83b7\u957f\u7a0b\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u53d8\u4f53\u5b58\u5728\u8868\u8fbe\u80fd\u529b\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u9884\u6d4b-\u6821\u6b63\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u5f15\u5165\u8f85\u52a9\u5faa\u73af\u72b6\u6001\u6765\u7d2f\u79ef\u6b8b\u5dee\u8bef\u5dee\u5e76\u6821\u6b63\u57fa\u7840\u9884\u6d4b\uff0cRDN\u7248\u672c\u8fd8\u5305\u542b\u81ea\u9002\u5e94\u95e8\u63a7\u548c\u6b8b\u5dee\u88c1\u526a\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u53ec\u56de\u5bc6\u96c6\u578b\u8bc4\u4f30\u4e2d\uff0cRLA\u548cRDN\u59cb\u7ec8\u4f18\u4e8e\u5404\u81ea\u57fa\u7ebf\u548c\u73b0\u4ee3\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u7f29\u5c0f\u4e86\u4e0e\u6807\u51c6Transformer\u7684\u5dee\u8ddd\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u6269\u5c55\u3002", "conclusion": "RLA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u8868\u8fbe\u80fd\u529b\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.25224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25224", "abs": "https://arxiv.org/abs/2509.25224", "authors": ["Qichen Liao", "Chengqiu Hu", "Fangzheng Miao", "Bao Li", "Yiyang Liu", "Junlong Lyu", "Lirui Jiang", "Jun Wang", "Lingchao Zheng", "Jun Li", "Yuwei Fan"], "title": "AMLA: MUL by ADD in FlashAttention Rescaling", "comment": "21 pages, 11 figures", "summary": "Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage\nin Large Language Models while introducing substantial computational overhead\nand intermediate variable expansion. This poses challenges for efficient\nhardware implementation -- especially during the decode phase. This paper\nintroduces Ascend MLA (AMLA), a high-performance kernel specifically optimized\nfor Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel\nFlashAttention-based algorithm that replaces floating-point multiplications\nwith integer additions for output block rescaling, leveraging binary\ncorrespondence between FP32 and INT32 representations; (2) A Preload Pipeline\nstrategy with hierarchical tiling that maximizes FLOPS utilization: the Preload\nPipeline achieves Cube-bound performance, while hierarchical tiling overlaps\ndata movement and computation within the Cube core. Experiments show that on\nAscend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS,\nreaching 86.8% of the theoretical maximum FLOPS, outperforming the\nstate-of-the-art open-source FlashMLA implementation, whose FLOPS utilization\nis up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into\nHuawei's CANN and will be released soon.", "AI": {"tldr": "AMLA\u662f\u4e00\u79cd\u9488\u5bf9\u534e\u4e3a\u6607\u817eNPU\u4f18\u5316\u7684\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u5185\u6838\uff0c\u901a\u8fc7FlashAttention\u7b97\u6cd5\u548c\u9884\u52a0\u8f7d\u6d41\u6c34\u7ebf\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u6607\u817e910 NPU\u4e0a\u8fbe\u5230614 TFLOPS\u6027\u80fd\u3002", "motivation": "\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b(MLA)\u867d\u7136\u51cf\u5c11\u4e86KVCache\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u5e26\u6765\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u4e2d\u95f4\u53d8\u91cf\u6269\u5c55\u7684\u95ee\u9898\uff0c\u8fd9\u5bf9\u786c\u4ef6\u5b9e\u73b0\u7279\u522b\u662f\u5728\u89e3\u7801\u9636\u6bb5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "1. \u57fa\u4e8eFlashAttention\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u6574\u6570\u52a0\u6cd5\u66ff\u4ee3\u6d6e\u70b9\u4e58\u6cd5\u8fdb\u884c\u8f93\u51fa\u5757\u91cd\u7f29\u653e\uff1b2. \u91c7\u7528\u9884\u52a0\u8f7d\u6d41\u6c34\u7ebf\u7b56\u7565\u548c\u5206\u5c42\u5206\u5757\uff0c\u6700\u5927\u5316FLOPS\u5229\u7528\u7387\u3002", "result": "\u5728\u6607\u817e910 NPU\u4e0a\u8fbe\u5230614 TFLOPS\uff0c\u8fbe\u5230\u7406\u8bba\u6700\u5927FLOPS\u768486.8%\uff0c\u4f18\u4e8e\u5f00\u6e90FlashMLA\u5b9e\u73b0\uff08\u5728NVIDIA H800\u4e0a\u6700\u9ad866.7%\u5229\u7528\u7387\uff09\u3002", "conclusion": "AMLA\u5185\u6838\u5df2\u96c6\u6210\u5230\u534e\u4e3aCANN\u4e2d\uff0c\u5c06\u5f88\u5feb\u53d1\u5e03\uff0c\u4e3aMLA\u5728\u6607\u817eNPU\u4e0a\u7684\u9ad8\u6548\u5b9e\u73b0\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25225", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25225", "abs": "https://arxiv.org/abs/2509.25225", "authors": ["Long Xu", "Yongcai Chen", "Fengshuo Liu", "Yuzhong Peng"], "title": "MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design", "comment": "11 pages, 5 figures", "summary": "Structure-Based Drug Design (SBDD) is a powerful strategy in computational\ndrug discovery, utilizing three-dimensional protein structures to guide the\ndesign of molecules with improved binding affinity. However, capturing complex\nprotein-ligand interactions across multiple scales remains challenging, as\ncurrent methods often overlook the hierarchical organization and intrinsic\nasymmetry of these interactions. To address these limitations, we propose\nMSCoD, a novel Bayesian updating-based generative framework for structure-based\ndrug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was\ndeveloped, which enables semantic compression at multiple abstraction levels\nfor efficient hierarchical feature extraction. Furthermore, a multi-head\ncooperative attention (MHCA) mechanism was developed, which employs asymmetric\nprotein-to-ligand attention to capture diverse interaction types while\naddressing the dimensionality disparity between proteins and ligands. Empirical\nstudies showed that MSCoD outperforms state-of-the-art methods on the benchmark\ndataset. Case studies on challenging targets such as KRAS G12D further\ndemonstrate its applicability in real-world scenarios. The code and data\nunderlying this article are freely available at\nhttps://github.com/xulong0826/MSCoD.", "AI": {"tldr": "MSCoD\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u66f4\u65b0\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4fe1\u606f\u74f6\u9888\u548c\u591a\u5934\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u86cb\u767d\u8d28-\u914d\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u591a\u5c3a\u5ea6\u590d\u6742\u6027\u548c\u4e0d\u5bf9\u79f0\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u86cb\u767d\u8d28-\u914d\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u591a\u5c3a\u5ea6\u590d\u6742\u6027\u548c\u5185\u5728\u4e0d\u5bf9\u79f0\u6027\uff0c\u9650\u5236\u4e86\u836f\u7269\u8bbe\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faMSCoD\u6846\u67b6\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u4fe1\u606f\u74f6\u9888(MSIB)\u8fdb\u884c\u5206\u5c42\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u53ca\u591a\u5934\u534f\u4f5c\u6ce8\u610f\u529b(MHCA)\u673a\u5236\u5904\u7406\u86cb\u767d\u8d28\u4e0e\u914d\u4f53\u4e4b\u95f4\u7684\u7ef4\u5ea6\u5dee\u5f02\u548c\u4e0d\u5bf9\u79f0\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0aMSCoD\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728KRAS G12D\u7b49\u6311\u6218\u6027\u9776\u70b9\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "MSCoD\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u4e0d\u5bf9\u79f0\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u9776\u70b9\u7684\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.25226", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.25226", "abs": "https://arxiv.org/abs/2509.25226", "authors": ["Baoyi Xie", "Shuiling Shi", "Wenqi Liu"], "title": "Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy", "comment": null, "summary": "Integrated wind-solar-wave marine energy systems hold broad promise for\nsupplying clean electricity in offshore and coastal regions. By leveraging the\nspatiotemporal complementarity of multiple resources, such systems can\neffectively mitigate the intermittency and volatility of single-source outputs,\nthereby substantially improving overall power-generation efficiency and\nresource utilization. Accurate ultra-short-term forecasting is crucial for\nensuring secure operation and optimizing proactive dispatch. However, most\nexisting forecasting methods construct separate models for each energy source,\ninsufficiently account for the complex couplings among multiple energies,\nstruggle to capture the system's nonlinear and nonstationary dynamics, and\ntypically depend on extensive manual parameter tuning-limitations that\nconstrain both predictive performance and practicality. We address this issue\nusing a Bayesian-optimized Multivariate Variational Mode Decomposition-Long\nShort-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to\njointly decompose wind, solar and wave power series so as to preserve\ncross-source couplings; it uses Bayesian optimization to automatically search\nthe number of modes and the penalty parameter in the MVMD process to obtain\nintrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to\nachieve ultra-short-term power forecasting for the integrated system.\nExperiments based on field measurements from an offshore integrated energy\nplatform in China show that the proposed framework significantly outperforms\nbenchmark models in terms of MAPE, RMSE and MAE. The results demonstrate\nsuperior predictive accuracy, robustness, and degree of automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u591a\u5143\u53d8\u5206\u6a21\u6001\u5206\u89e3-\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u96c6\u6210\u98ce-\u5149-\u6ce2\u6d77\u6d0b\u80fd\u6e90\u7cfb\u7edf\u7684\u8d85\u77ed\u671f\u529f\u7387\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u81ea\u52a8\u5316\u7a0b\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u6d4b\u65b9\u6cd5\u591a\u4e3a\u5355\u80fd\u6e90\u72ec\u7acb\u5efa\u6a21\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u591a\u80fd\u6e90\u95f4\u7684\u590d\u6742\u8026\u5408\u5173\u7cfb\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u548c\u975e\u5e73\u7a33\u52a8\u6001\u7279\u6027\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528MVMD\u8054\u5408\u5206\u89e3\u98ce\u3001\u5149\u3001\u6ce2\u529f\u7387\u5e8f\u5217\u4ee5\u4fdd\u6301\u8de8\u6e90\u8026\u5408\u5173\u7cfb\uff1b\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u81ea\u52a8\u641c\u7d22MVMD\u4e2d\u7684\u6a21\u6001\u6570\u91cf\u548c\u60e9\u7f5a\u53c2\u6570\uff1b\u6700\u540e\u7528LSTM\u5bf9\u5f97\u5230\u7684\u672c\u5f81\u6a21\u6001\u51fd\u6570\u8fdb\u884c\u5efa\u6a21\u9884\u6d4b\u3002", "result": "\u57fa\u4e8e\u4e2d\u56fd\u67d0\u6d77\u4e0a\u96c6\u6210\u80fd\u6e90\u5e73\u53f0\u7684\u73b0\u573a\u6d4b\u91cf\u6570\u636e\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728MAPE\u3001RMSE\u548cMAE\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u81ea\u52a8\u5316\u7a0b\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u96c6\u6210\u6d77\u6d0b\u80fd\u6e90\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u548c\u4f18\u5316\u8c03\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25228", "abs": "https://arxiv.org/abs/2509.25228", "authors": ["Ahmad Ayaz Amin"], "title": "Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections", "comment": null, "summary": "We introduce Random Projection Flows (RPFs), a principled framework for\ninjective normalizing flows that leverages tools from random matrix theory and\nthe geometry of random projections. RPFs employ random semi-orthogonal\nmatrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition\nof Gaussian matrices, to project data into lower-dimensional latent spaces for\nthe base distribution. Unlike PCA-based flows or learned injective maps, RPFs\nare plug-and-play, efficient, and yield closed-form expressions for the\nRiemannian volume correction term. We demonstrate that RPFs are both\ntheoretically grounded and practically effective, providing a strong baseline\nfor generative modeling and a bridge between random projection theory and\nnormalizing flows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u968f\u673a\u6295\u5f71\u6d41(RPFs)\uff0c\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u77e9\u9635\u7406\u8bba\u548c\u968f\u673a\u6295\u5f71\u51e0\u4f55\u7684\u6ce8\u5165\u5f0f\u5f52\u4e00\u5316\u6d41\u6846\u67b6\uff0c\u4f7f\u7528\u968f\u673a\u534a\u6b63\u4ea4\u77e9\u9635\u5c06\u6570\u636e\u6295\u5f71\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u3002", "motivation": "\u4e3a\u6ce8\u5165\u5f0f\u5f52\u4e00\u5316\u6d41\u63d0\u4f9b\u7406\u8bba\u4e25\u8c28\u4e14\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u8fde\u63a5\u968f\u673a\u6295\u5f71\u7406\u8bba\u4e0e\u5f52\u4e00\u5316\u6d41\uff0c\u89e3\u51b3PCA\u6d41\u548c\u5b66\u4e60\u7684\u6ce8\u5165\u6620\u5c04\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u901a\u8fc7\u9ad8\u65af\u77e9\u9635QR\u5206\u89e3\u4eceHaar\u5206\u5e03\u6b63\u4ea4\u7cfb\u7efc\u4e2d\u62bd\u53d6\u7684\u968f\u673a\u534a\u6b63\u4ea4\u77e9\u9635\uff0c\u5c06\u6570\u636e\u6295\u5f71\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u7528\u4e8e\u57fa\u5206\u5e03\uff0c\u83b7\u5f97\u9ece\u66fc\u4f53\u79ef\u4fee\u6b63\u9879\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "RPFs\u5728\u7406\u8bba\u4e0a\u6709\u575a\u5b9e\u57fa\u7840\uff0c\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\uff0c\u4e3a\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\uff0c\u5e76\u5728\u968f\u673a\u6295\u5f71\u7406\u8bba\u4e0e\u5f52\u4e00\u5316\u6d41\u4e4b\u95f4\u5efa\u7acb\u4e86\u6865\u6881\u3002", "conclusion": "RPFs\u662f\u5373\u63d2\u5373\u7528\u3001\u9ad8\u6548\u4e14\u7406\u8bba\u4e25\u8c28\u7684\u6ce8\u5165\u5f0f\u5f52\u4e00\u5316\u6d41\u6846\u67b6\uff0c\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fde\u63a5\u4e86\u968f\u673a\u6295\u5f71\u7406\u8bba\u4e0e\u5f52\u4e00\u5316\u6d41\u9886\u57df\u3002"}}
{"id": "2509.25230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25230", "abs": "https://arxiv.org/abs/2509.25230", "authors": ["Aaron Zweig", "Mingxuan Zhang", "Elham Azizi", "David Knowles"], "title": "Energy Guided Geometric Flow Matching", "comment": null, "summary": "A useful inductive bias for temporal data is that trajectories should stay\nclose to the data manifold. Traditional flow matching relies on straight\nconditional paths, and flow matching methods which learn geodesics rely on RBF\nkernels or nearest neighbor graphs that suffer from the curse of\ndimensionality. We propose to use score matching and annealed energy\ndistillation to learn a metric tensor that faithfully captures the underlying\ndata geometry and informs more accurate flows. We demonstrate the efficacy of\nthis strategy on synthetic manifolds with analytic geodesics, and interpolation\nof cell", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u5206\u6570\u5339\u914d\u548c\u9000\u706b\u80fd\u91cf\u84b8\u998f\u5b66\u4e60\u5ea6\u91cf\u5f20\u91cf\uff0c\u4ee5\u51c6\u786e\u6355\u6349\u6570\u636e\u51e0\u4f55\u7ed3\u6784\u5e76\u6539\u8fdb\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u9762\u4e34\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002", "motivation": "\u65f6\u95f4\u6570\u636e\u7684\u4e00\u4e2a\u6709\u7528\u5f52\u7eb3\u504f\u7f6e\u662f\u8f68\u8ff9\u5e94\u4fdd\u6301\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u3002\u4f20\u7edf\u6d41\u5339\u914d\u4f9d\u8d56\u76f4\u7ebf\u6761\u4ef6\u8def\u5f84\uff0c\u800c\u5b66\u4e60\u6d4b\u5730\u7ebf\u7684\u6d41\u5339\u914d\u65b9\u6cd5\u4f7f\u7528RBF\u6838\u6216\u6700\u8fd1\u90bb\u56fe\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5206\u6570\u5339\u914d\u548c\u9000\u706b\u80fd\u91cf\u84b8\u998f\u6765\u5b66\u4e60\u5ea6\u91cf\u5f20\u91cf\uff0c\u8be5\u5ea6\u91cf\u5f20\u91cf\u80fd\u591f\u5fe0\u5b9e\u6355\u6349\u5e95\u5c42\u6570\u636e\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u4e3a\u66f4\u51c6\u786e\u7684\u6d41\u63d0\u4f9b\u4fe1\u606f\u3002", "result": "\u5728\u5177\u6709\u89e3\u6790\u6d4b\u5730\u7ebf\u7684\u5408\u6210\u6d41\u5f62\u4e0a\u8bc1\u660e\u4e86\u8be5\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u7ec6\u80de\u63d2\u503c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u5ea6\u91cf\u5f20\u91cf\u6765\u51c6\u786e\u6355\u6349\u6570\u636e\u51e0\u4f55\u7ed3\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u8fdb\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.25231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25231", "abs": "https://arxiv.org/abs/2509.25231", "authors": ["Xiaojian Wang", "Chaoli Zhang", "Zhonglong Zheng", "Yunliang Jiang"], "title": "WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting", "comment": "Accepted by CIKM 2025", "summary": "Time series forecasting has various applications, such as meteorological\nrainfall prediction, traffic flow analysis, financial forecasting, and\noperational load monitoring for various systems. Due to the sparsity of time\nseries data, relying solely on time-domain or frequency-domain modeling limits\nthe model's ability to fully leverage multi-domain information. Moreover, when\napplied to time series forecasting tasks, traditional attention mechanisms tend\nto over-focus on irrelevant historical information, which may introduce noise\ninto the prediction process, leading to biased results. We proposed WDformer, a\nwavelet-based differential Transformer model. This study employs the wavelet\ntransform to conduct a multi-resolution analysis of time series data. By\nleveraging the advantages of joint representation in the time-frequency domain,\nit accurately extracts the key information components that reflect the\nessential characteristics of the data. Furthermore, we apply attention\nmechanisms on inverted dimensions, allowing the attention mechanism to capture\nrelationships between multiple variables. When performing attention\ncalculations, we introduced the differential attention mechanism, which\ncomputes the attention score by taking the difference between two separate\nsoftmax attention matrices. This approach enables the model to focus more on\nimportant information and reduce noise. WDformer has achieved state-of-the-art\n(SOTA) results on multiple challenging real-world datasets, demonstrating its\naccuracy and effectiveness. Code is available at\nhttps://github.com/xiaowangbc/WDformer.", "AI": {"tldr": "WDformer\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u5dee\u5206Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5206\u6790\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u7684\u5173\u952e\u4fe1\u606f\uff0c\u91c7\u7528\u5012\u7f6e\u7ef4\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u548c\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u6765\u51cf\u5c11\u566a\u58f0\u5e76\u5173\u6ce8\u91cd\u8981\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6548\u679c\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7a00\u758f\u6027\u5bfc\u81f4\u4ec5\u4f9d\u8d56\u65f6\u57df\u6216\u9891\u57df\u5efa\u6a21\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u57df\u4fe1\u606f\uff0c\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5bb9\u6613\u8fc7\u5ea6\u5173\u6ce8\u65e0\u5173\u5386\u53f2\u4fe1\u606f\uff0c\u5f15\u5165\u566a\u58f0\u5bfc\u81f4\u9884\u6d4b\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u53d8\u6362\u5bf9\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u591a\u5206\u8fa8\u7387\u5206\u6790\uff0c\u5229\u7528\u65f6\u9891\u57df\u8054\u5408\u8868\u793a\u4f18\u52bf\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff1b\u5728\u5012\u7f6e\u7ef4\u5ea6\u4e0a\u5e94\u7528\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u591a\u53d8\u91cf\u5173\u7cfb\uff1b\u5f15\u5165\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acbsoftmax\u6ce8\u610f\u529b\u77e9\u9635\u7684\u5dee\u5f02\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u3002", "result": "WDformer\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "WDformer\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u548c\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u57df\u4fe1\u606f\u5229\u7528\u548c\u566a\u58f0\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.25232", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25232", "abs": "https://arxiv.org/abs/2509.25232", "authors": ["Yongchao Huang"], "title": "Sampling via Gaussian Mixture Approximations", "comment": "204 pages", "summary": "We present a family of \\textit{Gaussian Mixture Approximation} (GMA) samplers\nfor sampling unnormalised target densities, encompassing \\textit{weights-only\nGMA} (W-GMA), \\textit{Laplace Mixture Approximation} (LMA),\n\\textit{expectation-maximization GMA} (EM-GMA), and further variants. GMA\nadopts a simple two-stage paradigm: (i) initialise a finite set of Gaussian\ncomponents and draw samples from a proposal mixture; (ii) fit the mixture to\nthe target by optimising either only the component weights or also the means\nand variances, via a sample-based KL divergence objective that requires only\nevaluations of the unnormalised density, followed by stratified resampling. The\nmethod is gradient-free, and computationally efficient: it leverages the ease\nof sampling from Gaussians, efficient optimisation methods (projected gradient\ndescent, mirror descent, and EM), and the robustness of stratified resampling\nto produce samples faithful to the target. We show that this\noptimisation-resampling scheme yields consistent approximations under mild\nconditions, and we validate this methodology with empirical results\ndemonstrating accuracy and speed across diverse densities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u9ad8\u65af\u6df7\u5408\u8fd1\u4f3c(GMA)\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u4ece\u975e\u5f52\u4e00\u5316\u76ee\u6807\u5bc6\u5ea6\u4e2d\u91c7\u6837\uff0c\u5305\u62ec\u6743\u91cd\u4f18\u5316GMA\u3001\u62c9\u666e\u62c9\u65af\u6df7\u5408\u8fd1\u4f3c\u3001\u671f\u671b\u6700\u5927\u5316GMA\u7b49\u53d8\u4f53\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u4ece\u9ad8\u65af\u6df7\u5408\u63d0\u8bae\u4e2d\u91c7\u6837\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316KL\u6563\u5ea6\u76ee\u6807\u6765\u62df\u5408\u76ee\u6807\u5206\u5e03\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u975e\u5f52\u4e00\u5316\u76ee\u6807\u5bc6\u5ea6\u4e2d\u751f\u6210\u5fe0\u5b9e\u6837\u672c\uff0c\u540c\u65f6\u5229\u7528\u9ad8\u65af\u91c7\u6837\u7684\u7b80\u4fbf\u6027\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u6548\u7387\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u521d\u59cb\u5316\u9ad8\u65af\u5206\u91cf\u5e76\u4ece\u6df7\u5408\u63d0\u8bae\u4e2d\u91c7\u6837\uff1b2) \u901a\u8fc7\u4f18\u5316KL\u6563\u5ea6\u76ee\u6807\uff08\u4ec5\u6743\u91cd\u6216\u5305\u62ec\u5747\u503c\u548c\u65b9\u5dee\uff09\u6765\u62df\u5408\u76ee\u6807\u5206\u5e03\uff0c\u7136\u540e\u8fdb\u884c\u5206\u5c42\u91cd\u91c7\u6837\u3002\u4f7f\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u3001\u955c\u50cf\u4e0b\u964d\u548cEM\u7b49\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u4ea7\u751f\u4e00\u81f4\u7684\u8fd1\u4f3c\uff0c\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u5404\u79cd\u5bc6\u5ea6\u51fd\u6570\u4e0a\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u91c7\u6837\u901f\u5ea6\u3002", "conclusion": "GMA\u91c7\u6837\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u68af\u5ea6\u65e0\u5173\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u91c7\u6837\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u975e\u5f52\u4e00\u5316\u76ee\u6807\u5bc6\u5ea6\u4e2d\u751f\u6210\u5fe0\u5b9e\u6837\u672c\uff0c\u5177\u6709\u7406\u8bba\u4e00\u81f4\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\u3002"}}
{"id": "2509.25233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25233", "abs": "https://arxiv.org/abs/2509.25233", "authors": ["Kasun Eranda Wijethilake", "Adnan Mahmood", "Quan Z. Sheng"], "title": "FedCLF -- Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks", "comment": "Already published in ADMA 2024 on 13th December 2024 Wijethilake,\n  K.E., Mahmood, A., Sheng, Q.Z. (2025). FedCLF - Towards Efficient Participant\n  Selection for Federated Learning in Heterogeneous IoV Networks. In: Sheng,\n  Q.Z., et al. Advanced Data Mining and Applications. ADMA 2024. Lecture Notes\n  in Computer Science(), vol 15388. Springer, Singapore.\n  https://doi.org/10.1007/978-981-96-0814-0_15", "summary": "Federated Learning (FL) is a distributed machine learning technique that\npreserves data privacy by sharing only the trained parameters instead of the\nclient data. This makes FL ideal for highly dynamic, heterogeneous, and\ntime-critical applications, in particular, the Internet of Vehicles (IoV)\nnetworks. However, FL encounters considerable challenges in such networks owing\nto the high data and device heterogeneity. To address these challenges, we\npropose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which\nintroduces calibrated loss as a utility in the participant selection process\nand a feedback control mechanism to dynamically adjust the sampling frequency\nof the clients. The envisaged approach (a) enhances the overall model accuracy\nin case of highly heterogeneous data and (b) optimizes the resource utilization\nfor resource constrained IoV networks, thereby leading to increased efficiency\nin the FL process. We evaluated FedCLF vis-\\`a-vis baseline models, i.e.,\nFedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity.\nOur results depict that FedCLF significantly outperforms the baseline models by\nup to a 16% improvement in high data heterogeneity-related scenarios with\nimproved efficiency via reduced sampling frequency.", "AI": {"tldr": "\u63d0\u51faFedCLF\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u635f\u5931\u548c\u53cd\u9988\u63a7\u5236\u673a\u5236\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u8f66\u8054\u7f51\u4e2d\u7684\u5f02\u6784\u6570\u636e\u6311\u6218\uff0c\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u5e76\u4f18\u5316\u8d44\u6e90\u5229\u7528", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8f66\u8054\u7f51\u7b49\u52a8\u6001\u5f02\u6784\u73af\u5883\u4e2d\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u548c\u8bbe\u5907\u5f02\u6784\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u548c\u8d44\u6e90\u6548\u7387", "method": "FedCLF\u5f15\u5165\u6821\u51c6\u635f\u5931\u4f5c\u4e3a\u53c2\u4e0e\u8005\u9009\u62e9\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u53cd\u9988\u63a7\u5236\u673a\u5236\u52a8\u6001\u8c03\u6574\u5ba2\u6237\u7aef\u91c7\u6837\u9891\u7387", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cFedCLF\u76f8\u6bd4FedAvg\u3001Newt\u548cOort\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u9ad8\u6570\u636e\u5f02\u6784\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u5347\u8fbe16%\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11\u91c7\u6837\u9891\u7387\u63d0\u9ad8\u6548\u7387", "conclusion": "FedCLF\u80fd\u6709\u6548\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u5728\u8f66\u8054\u7f51\u73af\u5883\u4e2d\u7684\u5f02\u6784\u6570\u636e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u4f18\u5316\u8d44\u6e90\u5229\u7528"}}
{"id": "2509.25235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25235", "abs": "https://arxiv.org/abs/2509.25235", "authors": ["Nikola Prianikov", "Evelyne Janssen-van Dam", "Marcin Pietrasik", "Charalampos S. Kouzinopoulos"], "title": "Machine Learning for Pattern Detection in Printhead Nozzle Logging", "comment": "This paper has been published in the 37th International Conference on\n  Tools with Artificial Intelligence in Athens, Greece, November 03-05, 2025", "summary": "Correct identification of failure mechanisms is essential for manufacturers\nto ensure the quality of their products. Certain failures of printheads\ndeveloped by Canon Production Printing can be identified from the behavior of\nindividual nozzles, the states of which are constantly recorded and can form\ndistinct patterns in terms of the number of failed nozzles over time, and in\nspace in the nozzle grid. In our work, we investigate the problem of printhead\nfailure classification based on a multifaceted dataset of nozzle logging and\npropose a Machine Learning classification approach for this problem. We follow\nthe feature-based framework of time-series classification, where a set of\ntime-based and spatial features was selected with the guidance of domain\nexperts. Several traditional ML classifiers were evaluated, and the One-vs-Rest\nRandom Forest was found to have the best performance. The proposed model\noutperformed an in-house rule-based baseline in terms of a weighted F1 score\nfor several failure mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6253\u5370\u5934\u6545\u969c\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u4f7f\u7528One-vs-Rest\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5bf9\u6253\u5370\u5934\u6545\u969c\u673a\u5236\u8fdb\u884c\u8bc6\u522b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u6253\u5370\u5934\u6545\u969c\u673a\u5236\u5bf9\u5236\u9020\u5546\u786e\u4fdd\u4ea7\u54c1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0cCanon Production Printing\u7684\u6253\u5370\u5934\u6545\u969c\u53ef\u4ee5\u901a\u8fc7\u55b7\u5634\u884c\u4e3a\u6a21\u5f0f\u6765\u8bc6\u522b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7279\u5f81\u7684\u65f6\u5e8f\u5206\u7c7b\u6846\u67b6\uff0c\u5728\u9886\u57df\u4e13\u5bb6\u6307\u5bfc\u4e0b\u9009\u62e9\u65f6\u95f4\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u8bc4\u4f30\u591a\u79cd\u4f20\u7edfML\u5206\u7c7b\u5668\uff0c\u6700\u7ec8\u9009\u62e9One-vs-Rest\u968f\u673a\u68ee\u6797\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u591a\u4e2a\u6545\u969c\u673a\u5236\u7684\u52a0\u6743F1\u5206\u6570\u4e0a\u4f18\u4e8e\u5185\u90e8\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5206\u7c7b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u6253\u5370\u5934\u6545\u969c\u673a\u5236\uff0c\u4e3a\u4ea7\u54c1\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25238", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25238", "abs": "https://arxiv.org/abs/2509.25238", "authors": ["Sri Vatsa Vuddanti", "Aarav Shah", "Satwik Kumar Chittiprolu", "Tony Song", "Sunishchal Dev", "Kevin Zhu", "Maheep Chaudhary"], "title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases", "comment": null, "summary": "Tool-augmented language agents frequently fail in real-world deployment due\nto tool malfunctions--timeouts, API exceptions, or inconsistent\noutputs--triggering cascading reasoning errors and task abandonment. Existing\nagent training pipelines optimize only for success trajectories, failing to\nexpose models to the tool failures that dominate real-world usage. We propose\n\\textbf{PALADIN}, a generalizable framework for equipping language agents with\nrobust failure recovery capabilities. PALADIN trains on 50,000+\nrecovery-annotated trajectories constructed via systematic failure injection\nand expert demonstrations on an enhanced ToolBench dataset. Training uses\nLoRA-based fine-tuning to retain base capabilities while injecting recovery\ncompetence. At inference, PALADIN detects execution-time errors and retrieves\nthe most similar case from a curated bank of 55+ failure exemplars aligned with\nToolScan's taxonomy, then executes the corresponding recovery action. This\napproach generalizes to novel failures beyond the training distribution,\nretaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across\nPaladinEval and ToolReflectEval demonstrates consistent improvements in\nRecovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR),\nand Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57%\nrelative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%)\nby +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative\nimprovement from 23.75%). These results establish PALADIN as an effective\nmethod for building fault-tolerant agents capable of robust recovery in\nreal-world tool environments.", "AI": {"tldr": "PALADIN\u662f\u4e00\u4e2a\u8bad\u7ec3\u8bed\u8a00\u4ee3\u7406\u5728\u5de5\u5177\u6545\u969c\u65f6\u8fdb\u884c\u7a33\u5065\u6062\u590d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6545\u969c\u6ce8\u5165\u548c\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u5177\u73af\u5883\u4e2d\u7684\u6545\u969c\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7406\u8bad\u7ec3\u7ba1\u9053\u53ea\u9488\u5bf9\u6210\u529f\u8f68\u8ff9\u8fdb\u884c\u4f18\u5316\uff0c\u672a\u80fd\u8ba9\u6a21\u578b\u63a5\u89e6\u5230\u5728\u771f\u5b9e\u4e16\u754c\u4f7f\u7528\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u5de5\u5177\u6545\u969c\uff0c\u5bfc\u81f4\u4ee3\u7406\u5728\u90e8\u7f72\u65f6\u56e0\u5de5\u5177\u6545\u969c\u800c\u5931\u8d25\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6545\u969c\u6ce8\u5165\u548c\u4e13\u5bb6\u6f14\u793a\u6784\u5efa50,000+\u6062\u590d\u6807\u6ce8\u8f68\u8ff9\uff0c\u4f7f\u7528LoRA\u5fae\u8c03\u4fdd\u7559\u57fa\u7840\u80fd\u529b\u7684\u540c\u65f6\u6ce8\u5165\u6062\u590d\u80fd\u529b\uff0c\u5728\u63a8\u7406\u65f6\u68c0\u6d4b\u6267\u884c\u9519\u8bef\u5e76\u4ece55+\u6545\u969c\u793a\u4f8b\u5e93\u4e2d\u68c0\u7d22\u76f8\u4f3c\u6848\u4f8b\u6267\u884c\u76f8\u5e94\u6062\u590d\u64cd\u4f5c\u3002", "result": "PALADIN\u5c06\u6062\u590d\u7387\u4ece32.76%\u63d0\u5347\u523089.68%\uff08\u76f8\u5bf9\u63d0\u534757%\uff09\uff0c\u4f18\u4e8e\u6700\u5f3a\u57fa\u7ebfCRITIC\uff0876.34%\uff0913.3%\uff0c\u5728\u672a\u89c1\u5de5\u5177API\u4e0a\u4fdd\u630195.2%\u7684\u6062\u590d\u6027\u80fd\u3002", "conclusion": "PALADIN\u662f\u6784\u5efa\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u73af\u5883\u4e2d\u8fdb\u884c\u7a33\u5065\u6062\u590d\u7684\u5bb9\u9519\u4ee3\u7406\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.25240", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.25240", "abs": "https://arxiv.org/abs/2509.25240", "authors": ["Ming Yang", "Xiaofan Li", "Zhiyuan Ma", "Dengliang Shi", "Jintao Du", "Yu Cheng", "Weiguo Zheng"], "title": "HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement", "comment": "20 pages, 7 figures, 4 tables", "summary": "Recent curriculum reinforcement learning for large language models (LLMs)\ntypically rely on difficulty-based annotations for data filtering and ordering.\nHowever, such methods suffer from local optimization, where continual training\non simple samples in the early steps can cause the policy to lose its\nexploration. We propose a novel schema, namely Hamiltonian curiosity augmented\nlarge language model reinforcement (HAMMER), that transfers diversity metrics,\ncommonly used in dataset evaluation, into the dynamic reinforcement learning\nprocedure, where training samples are ordered via a minimum-semantic\nHamiltonian path making the initial training retrain more exploration. From a\ntheoretical perspective of generalization bounds, diversity-driven ordering\nfacilitates stable convergence. Empirical evaluations indicate that HAMMER\nstimulates model \"curiosity\" and consistently achieves a 3% to 4% average\naccuracy gain across diverse inference benchmark.", "AI": {"tldr": "HAMMER\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6837\u6027\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u8bed\u4e49\u54c8\u5bc6\u987f\u8def\u5f84\u6392\u5e8f\u8bad\u7ec3\u6837\u672c\uff0c\u589e\u5f3a\u6a21\u578b\u63a2\u7d22\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b03-4%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u96be\u5ea6\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u90e8\u4f18\u5316\u95ee\u9898\uff0c\u65e9\u671f\u6301\u7eed\u8bad\u7ec3\u7b80\u5355\u6837\u672c\u4f1a\u5bfc\u81f4\u7b56\u7565\u5931\u53bb\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u5c06\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u7684\u591a\u6837\u6027\u6307\u6807\u5f15\u5165\u52a8\u6001\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6700\u5c0f\u8bed\u4e49\u54c8\u5bc6\u987f\u8def\u5f84\u5bf9\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u6392\u5e8f\uff0c\u4f7f\u521d\u59cb\u8bad\u7ec3\u4fdd\u6301\u66f4\u591a\u63a2\u7d22\u3002", "result": "HAMMER\u6fc0\u53d1\u4e86\u6a21\u578b\u7684\"\u597d\u5947\u5fc3\"\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e863%\u52304%\u3002", "conclusion": "\u591a\u6837\u6027\u9a71\u52a8\u7684\u6392\u5e8f\u6709\u52a9\u4e8e\u7a33\u5b9a\u6536\u655b\uff0c\u4ece\u6cdb\u5316\u754c\u9650\u7684\u7406\u8bba\u89c6\u89d2\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.25241", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25241", "abs": "https://arxiv.org/abs/2509.25241", "authors": ["Yuan Huang"], "title": "Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge", "comment": null, "summary": "Recent advancements in training paradigms for Large Language Models (LLMs)\nhave unlocked their remarkable capabilities in natural language processing and\ncross-domain generalization. While LLMs excel in tasks like programming and\nmathematical problem-solving, their zero-shot performance in specialized\ndomains requiring expert knowledge, such as cybersecurity, is often suboptimal.\nThis limitation arises because foundational LLMs are designed for\ngeneral-purpose applications, constraining their ability to encapsulate\ndomain-specific expertise within their parameter space. To address this, we\nexplore fine-tuning strategies to embed cybersecurity knowledge into LLMs,\nenhancing their performance in cybersecurity question-answering (Q\\&A) tasks\nwhile prioritizing computational efficiency. Specifically, we investigate\nSupervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized\nLow-Rank Adaptation (QLoRA) using a cybersecurity Q\\&A dataset. Our results\ndemonstrate that these fine-tuning approaches significantly outperform the\nfoundational model in cybersecurity Q\\&A tasks. Moreover, LoRA and QLoRA\nachieve comparable performance to SFT with substantially lower computational\ncosts, offering an efficient pathway for adapting LLMs to specialized domains.\nOur work highlights the potential of low-rank fine-tuning strategies to bridge\nthe gap between general-purpose LLMs and domain-specific applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4e09\u79cd\u5fae\u8c03\u65b9\u6cd5\uff08SFT\u3001LoRA\u3001QLoRA\uff09\u5c06\u7f51\u7edc\u5b89\u5168\u77e5\u8bc6\u5d4c\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5176\u4e2dLoRA\u548cQLoRA\u80fd\u4ee5\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u8fbe\u5230\u4e0eSFT\u76f8\u5f53\u7684\u6548\u679c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u7279\u5b9a\u9886\u57df\uff08\u5982\u7f51\u7edc\u5b89\u5168\uff09\u96f6\u6837\u672c\u6027\u80fd\u4e0d\u4f73\uff0c\u56e0\u4e3a\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u4e3a\u901a\u7528\u76ee\u7684\uff0c\u96be\u4ee5\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u5c01\u88c5\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u7f51\u7edc\u5b89\u5168\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7814\u7a76\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u91cf\u5316\u4f4e\u79e9\u9002\u5e94\uff08QLoRA\uff09\u4e09\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6240\u6709\u5fae\u8c03\u65b9\u6cd5\u5728\u7f51\u7edc\u5b89\u5168\u95ee\u7b54\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0cLoRA\u548cQLoRA\u4e0eSFT\u6027\u80fd\u76f8\u5f53\u4f46\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u4f4e\u79e9\u5fae\u8c03\u7b56\u7565\u6709\u6f5c\u529b\u5f25\u5408\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u9886\u57df\u7279\u5b9a\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u9002\u5e94\u4e13\u4e1a\u9886\u57df\u63d0\u4f9b\u9ad8\u6548\u8def\u5f84\u3002"}}
{"id": "2509.25253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25253", "abs": "https://arxiv.org/abs/2509.25253", "authors": ["Prajjwal Bhattarai", "Mohammad Amjad", "Dmytro Zhylko", "Tuka Alhanai"], "title": "Knowledge distillation through geometry-aware representational alignment", "comment": null, "summary": "Knowledge distillation is a common paradigm for transferring capabilities\nfrom larger models to smaller ones. While traditional distillation methods\nleverage a probabilistic divergence over the output of the teacher and student\nmodels, feature-based distillation methods often minimize variants of Euclidean\nnorms between the hidden layer representations. The main goal is for the\nstudent to mimic the structure of the feature space of the teacher. In this\nwork, we theoretically show that existing feature distillation methods, such as\nprojection based mean squared loss or Centered Kernel Alignment (CKA), cannot\ncapture the feature structure, even under zero loss. We then motivate the use\nof Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances\nalready common in the context of measuring representational alignment, as\ndistillation losses. We show that feature distillation through our method\nshowcases statistically significant improvement in distillation performance\nacross language models families (BERT and OPT) in classification and\ninstruction-following tasks by up to 2 percentage points, showcasing the\npotential of integrating feature geometry into existing distillation methods.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4f20\u7edf\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4f7f\u7528Procrustes\u8ddd\u79bb\u548c\u7279\u5f81Gram\u77e9\u9635\u7684Frobenius\u8303\u6570\u4f5c\u4e3a\u84b8\u998f\u635f\u5931\uff0c\u5728BERT\u548cOPT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff08\u5982\u6295\u5f71\u5747\u65b9\u635f\u5931\u6216CKA\uff09\u65e0\u6cd5\u6709\u6548\u6355\u6349\u6559\u5e08\u6a21\u578b\u7684\u7279\u5f81\u7a7a\u95f4\u7ed3\u6784\uff0c\u5373\u4f7f\u635f\u5931\u4e3a\u96f6\u65f6\u4e5f\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528Procrustes\u8ddd\u79bb\u548c\u7279\u5f81Gram\u77e9\u9635\u7684Frobenius\u8303\u6570\u4f5c\u4e3a\u84b8\u998f\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8868\u793a\u5bf9\u9f50\u6d4b\u91cf\u4e2d\u5df2\u6709\u5e94\u7528\u3002", "result": "\u5728BERT\u548cOPT\u6a21\u578b\u4e0a\u7684\u5206\u7c7b\u548c\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\uff0c\u65b0\u65b9\u6cd5\u6bd4\u73b0\u6709\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2\u4e2a\u767e\u5206\u70b9\uff0c\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "\u5c06\u7279\u5f81\u51e0\u4f55\u7ed3\u6784\u6574\u5408\u5230\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u84b8\u998f\u6027\u80fd\u3002"}}
{"id": "2509.25261", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25261", "abs": "https://arxiv.org/abs/2509.25261", "authors": ["Xianyang Deng", "Wenshuai Liu", "Yaru FuB", "Qi Zhu"], "title": "Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks", "comment": "7 pages, 6 figures", "summary": "Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has\nemerged as a promising paradigm for data collection. However, challenges such\nas spectrum scarcity, device heterogeneity, and user mobility hinder efficient\ncoordination of sensing, communication, and computation. To tackle these\nissues, we propose a joint optimization framework that integrates time slot\npartition for sensing, communication, and computation phases, resource\nallocation, and UAV 3D trajectory planning, aiming to maximize the amount of\nprocessed sensing data. The problem is formulated as a non-convex stochastic\noptimization and further modeled as a partially observable Markov decision\nprocess (POMDP) that can be solved by multi-agent deep reinforcement learning\n(MADRL) algorithm. To overcome the limitations of conventional multi-layer\nperceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor\nnetwork. The newly developed method is based on heterogeneous agent proximal\npolicy optimization (HAPPO), empowered by convolutional neural networks (CNN)\nfor feature extraction and Kolmogorov-Arnold networks (KAN) to capture\nstructured state-action dependencies. Extensive numerical results demonstrate\nthat our proposed method achieves significant improvements in the amount of\nprocessed sensing data when compared with other benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u8f85\u52a9\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u65f6\u95f4\u69fd\u5206\u914d\u3001\u8d44\u6e90\u5206\u914d\u548c\u65e0\u4eba\u673a3D\u8f68\u8ff9\u89c4\u5212\uff0c\u6700\u5927\u5316\u5904\u7406\u611f\u77e5\u6570\u636e\u91cf\u3002", "motivation": "\u65e0\u4eba\u673a\u8f85\u52a9\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u9762\u4e34\u9891\u8c31\u7a00\u7f3a\u3001\u8bbe\u5907\u5f02\u6784\u6027\u548c\u7528\u6237\u79fb\u52a8\u6027\u7b49\u6311\u6218\uff0c\u963b\u788d\u4e86\u611f\u77e5\u3001\u901a\u4fe1\u548c\u8ba1\u7b97\u7684\u9ad8\u6548\u534f\u8c03\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f02\u6784\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408CNN\u7279\u5f81\u63d0\u53d6\u548cKAN\u7f51\u7edc\u6355\u6349\u72b6\u6001-\u52a8\u4f5c\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5927\u91cf\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5904\u7406\u611f\u77e5\u6570\u636e\u91cf\u65b9\u9762\u76f8\u6bd4\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u8054\u5408\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8f85\u52a9\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.25263", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25263", "abs": "https://arxiv.org/abs/2509.25263", "authors": ["Yifang Zhang", "Pengfei Duan", "Henan Wang", "Shengwu Xiong"], "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data", "comment": "11 pages,8 figures", "summary": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.", "AI": {"tldr": "\u63d0\u51fa\u4e86RainfallBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\uff080-3\u5c0f\u65f6\u9884\u6d4b\uff09\uff0c\u5305\u542b5\u5e74\u5168\u740312,000\u591a\u4e2aGNSS\u7ad9\u7684\u6c14\u8c61\u6570\u636e\uff0c\u5e76\u5f15\u5165\u53ef\u964d\u6c34\u6c7d(PWV)\u6307\u6807\u3002\u540c\u65f6\u5f00\u53d1\u4e86BFPF\u6a21\u5757\u6765\u89e3\u51b3\u96f6\u81a8\u80c0\u548c\u65f6\u95f4\u8870\u51cf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6c14\u8c61\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6e29\u5ea6\u3001\u6e7f\u5ea6\u7b49\u5468\u671f\u6027\u5f3a\u7684\u53d8\u91cf\uff0c\u65e0\u6cd5\u53cd\u6620\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\u8fd9\u79cd\u66f4\u590d\u6742\u5b9e\u9645\u573a\u666f\u7684\u6a21\u578b\u80fd\u529b\u3002\u964d\u96e8\u9884\u6d4b\u5177\u6709\u96f6\u81a8\u80c0\u3001\u65f6\u95f4\u8870\u51cf\u548c\u975e\u5e73\u7a33\u6027\u7b49\u6311\u6218\u6027\u7279\u5f81\u3002", "method": "\u6784\u5efaRainfallBench\u6570\u636e\u96c6\uff0c\u5305\u542b6\u4e2a\u5173\u952e\u53d8\u91cf\uff0c15\u5206\u949f\u95f4\u9694\u76845\u5e74\u89c2\u6d4b\u6570\u636e\u3002\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u7b56\u7565\u6765\u8bc4\u4f30\u591a\u5c3a\u5ea6\u9884\u6d4b\u548c\u6781\u7aef\u964d\u96e8\u4e8b\u4ef6\u3002\u63d0\u51fa\u4e86BFPF\uff08\u53cc\u7126\u70b9\u964d\u6c34\u9884\u62a5\u5668\uff09\u6a21\u5757\uff0c\u878d\u5165\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\u6765\u589e\u5f3a\u964d\u96e8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "result": "\u5728RainfallBench\u4e0a\u8bc4\u4f30\u4e8620\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u6db5\u76d66\u79cd\u4e3b\u8981\u67b6\u6784\u3002\u7edf\u8ba1\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u5168\u9762\u6027\u548c\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "RainfallBench\u586b\u8865\u4e86\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\u57fa\u51c6\u7684\u7a7a\u767d\uff0cBFPF\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5ffd\u7565\u7684\u96f6\u81a8\u80c0\u548c\u65f6\u95f4\u8870\u51cf\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u6c14\u8c61\u573a\u666f\u7684\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.25267", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.25267", "abs": "https://arxiv.org/abs/2509.25267", "authors": ["Jiexi Xu"], "title": "Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning", "comment": "13 pages, 2 figures, 2 tables", "summary": "The performance of Large Language Models (LLMs) depends heavily on the chosen\nprompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or\nChain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly\naccurate strategies like Self-Consistency (SC) incur substantial computational\nwaste on simple tasks, while lightweight methods often fail on complex inputs.\nThis paper introduces the Prompt Policy Network (PPN), a lightweight\nreinforcement learning framework that formalizes adaptive strategy selection as\na single-step Markov Decision Process (MDP). The PPN, trained with Proximal\nPolicy Optimization (PPO) and guided by a resource-explicit reward function,\nlearns to allocate costly reasoning strategies only when necessary. Experiments\non arithmetic reasoning benchmarks demonstrate that PPN achieves superior\nperformance on the efficiency-accuracy Pareto front, delivering up to 61.5%\ntoken cost reduction compared to Self-Consistency while maintaining competitive\naccuracy. This work contributes a systematic, adaptive framework for\ncost-efficient LLM deployment, advancing the design of lightweight optimization\ntechniques for scalable and sustainable language model applications.", "AI": {"tldr": "PPN\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u5728LLM\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6548\u7387-\u51c6\u786e\u7387\u5e73\u8861\uff0c\u76f8\u6bd4Self-Consistency\u53ef\u51cf\u5c1161.5%\u7684token\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u63d0\u793a\u7b56\u7565\uff08\u5982Zero-Shot\u3001Few-Shot\u3001CoT\uff09\u5728\u6548\u7387\u4e0e\u51c6\u786e\u7387\u4e4b\u95f4\u5b58\u5728\u521a\u6027\u6743\u8861\uff0c\u51c6\u786e\u7387\u9ad8\u7684\u65b9\u6cd5\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u9020\u6210\u8ba1\u7b97\u6d6a\u8d39\uff0c\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5728\u590d\u6742\u8f93\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPrompt Policy Network (PPN)\u6846\u67b6\uff0c\u5c06\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u5f62\u5f0f\u5316\u4e3a\u5355\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u8d44\u6e90\u663e\u5f0f\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u7b97\u672f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPPN\u5728\u6548\u7387-\u51c6\u786e\u7387\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4Self-Consistency\u53ef\u51cf\u5c1161.5%\u7684token\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u7387\u3002", "conclusion": "PPN\u4e3a\u6210\u672c\u9ad8\u6548\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8f7b\u91cf\u7ea7\u4f18\u5316\u6280\u672f\u7684\u53d1\u5c55\uff0c\u652f\u6301\u53ef\u6269\u5c55\u548c\u53ef\u6301\u7eed\u7684\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u3002"}}
{"id": "2509.25268", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "68T07"], "pdf": "https://arxiv.org/pdf/2509.25268", "abs": "https://arxiv.org/abs/2509.25268", "authors": ["Cristian Bodnar", "Rapha\u00ebl Rousseau-Rizzi", "Nikhil Shankar", "James Merleau", "Stylianos Flampouris", "Guillem Candille", "Slavica Antic", "Fran\u00e7ois Miralles", "Jayesh K. Gupta"], "title": "A Weather Foundation Model for the Power Grid", "comment": "31 pages, 22 figures", "summary": "Weather foundation models (WFMs) have recently set new benchmarks in global\nforecast skill, yet their concrete value for the weather-sensitive\ninfrastructure that powers modern society remains largely unexplored. In this\nstudy, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting\nTransformer (GFT), on a rich archive of Hydro-Qu\\'ebec asset\nobservations--including transmission-line weather stations, wind-farm met-mast\nstreams, and icing sensors--to deliver hyper-local, asset-level forecasts for\nfive grid-critical variables: surface temperature, precipitation, hub-height\nwind speed, wind-turbine icing risk, and rime-ice accretion on overhead\nconductors. Across 6-72 h lead times, the tailored model surpasses\nstate-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)\nby 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.\nMost importantly, it attains an average precision score of 0.72 for day-ahead\nrime-ice detection, a capability absent from existing operational systems,\nwhich affords several hours of actionable warning for potentially catastrophic\noutage events. These results show that WFMs, when post-trained with small\namounts of high-fidelity, can serve as a practical foundation for\nnext-generation grid-resilience intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9Silurian AI\u7684\u5929\u6c14\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7528\u4e8e\u4e3aHydro-Qu\u00e9bec\u7535\u7f51\u8d44\u4ea7\u63d0\u4f9b\u8d85\u672c\u5730\u5316\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u5173\u952e\u6c14\u8c61\u53d8\u91cf\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u8986\u51b0\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002", "motivation": "\u5929\u6c14\u57fa\u7840\u6a21\u578b\u5728\u5929\u6c14\u9884\u62a5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u73b0\u4ee3\u793e\u4f1a\u5929\u6c14\u654f\u611f\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u9645\u4ef7\u503c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5929\u6c14\u57fa\u7840\u6a21\u578b\u5728\u7535\u7f51\u8d44\u4ea7\u7ea7\u9884\u6d4b\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5bf9Silurian AI\u768415\u4ebf\u53c2\u6570\u751f\u6210\u5f0f\u9884\u6d4b\u53d8\u6362\u5668\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u7528Hydro-Qu\u00e9bec\u4e30\u5bcc\u7684\u8d44\u4ea7\u89c2\u6d4b\u6570\u636e\uff0c\u5305\u62ec\u8f93\u7535\u7ebf\u8def\u6c14\u8c61\u7ad9\u3001\u98ce\u7535\u573a\u6d4b\u98ce\u5854\u548c\u8986\u51b0\u4f20\u611f\u5668\u6570\u636e\u3002", "result": "\u57286-72\u5c0f\u65f6\u9884\u62a5\u65f6\u6548\u5185\uff0c\u5b9a\u5236\u5316\u6a21\u578b\u5728\u6e29\u5ea6\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e15%\u3001\u603b\u964d\u6c34\u91cf\u8bef\u5dee\u964d\u4f4e35%\u3001\u98ce\u901f\u8bef\u5dee\u964d\u4f4e15%\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u5728\u8986\u51b0\u68c0\u6d4b\u65b9\u9762\u8fbe\u52300.72\u7684\u5e73\u5747\u7cbe\u5ea6\u5f97\u5206\uff0c\u4e3a\u6f5c\u5728\u707e\u96be\u6027\u505c\u7535\u4e8b\u4ef6\u63d0\u4f9b\u6570\u5c0f\u65f6\u7684\u53ef\u64cd\u4f5c\u9884\u8b66\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5929\u6c14\u57fa\u7840\u6a21\u578b\u7ecf\u8fc7\u5c11\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\u540e\u8bad\u7ec3\u540e\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u7535\u7f51\u97e7\u6027\u667a\u80fd\u7684\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2509.25270", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25270", "abs": "https://arxiv.org/abs/2509.25270", "authors": ["Liangjian Wen", "Qun Dai", "Jianzhuang Liu", "Jiangtao Zheng", "Yong Dai", "Dongkai Wang", "Zhao Kang", "Jun Wang", "Zenglin Xu", "Jiang Duan"], "title": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions", "comment": null, "summary": "In multimodal representation learning, synergistic interactions between\nmodalities not only provide complementary information but also create unique\noutcomes through specific interaction patterns that no single modality could\nachieve alone. Existing methods may struggle to effectively capture the full\nspectrum of synergistic information, leading to suboptimal performance in tasks\nwhere such interactions are critical. This is particularly problematic because\nsynergistic information constitutes the fundamental value proposition of\nmultimodal representation. To address this challenge, we introduce InfMasking,\na contrastive synergistic information extraction method designed to enhance\nsynergistic information through an \\textbf{Inf}inite \\textbf{Masking} strategy.\nInfMasking stochastically occludes most features from each modality during\nfusion, preserving only partial information to create representations with\nvaried synergistic patterns. Unmasked fused representations are then aligned\nwith masked ones through mutual information maximization to encode\ncomprehensive synergistic information. This infinite masking strategy enables\ncapturing richer interactions by exposing the model to diverse partial modality\ncombinations during training. As computing mutual information estimates with\ninfinite masking is computationally prohibitive, we derive an InfMasking loss\nto approximate this calculation. Through controlled experiments, we demonstrate\nthat InfMasking effectively enhances synergistic information between\nmodalities. In evaluations on large-scale real-world datasets, InfMasking\nachieves state-of-the-art performance across seven benchmarks. Code is released\nat https://github.com/brightest66/InfMasking.", "AI": {"tldr": "\u63d0\u51faInfMasking\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u9650\u63a9\u7801\u7b56\u7565\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u4e2d\u7684\u534f\u540c\u4fe1\u606f\uff0c\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u591a\u6a21\u6001\u95f4\u7684\u5b8c\u6574\u534f\u540c\u4fe1\u606f\uff0c\u800c\u534f\u540c\u4fe1\u606f\u662f\u591a\u6a21\u6001\u8868\u793a\u7684\u6838\u5fc3\u4ef7\u503c\u6240\u5728", "method": "\u4f7f\u7528\u65e0\u9650\u63a9\u7801\u7b56\u7565\u968f\u673a\u906e\u853d\u5404\u6a21\u6001\u7684\u5927\u90e8\u5206\u7279\u5f81\uff0c\u4ec5\u4fdd\u7559\u90e8\u5206\u4fe1\u606f\u521b\u5efa\u5177\u6709\u4e0d\u540c\u534f\u540c\u6a21\u5f0f\u7684\u8868\u793a\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u6700\u5927\u5316\u5bf9\u9f50\u63a9\u7801\u524d\u540e\u7684\u8868\u793a", "result": "\u5728\u53d7\u63a7\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u6001\u95f4\u534f\u540c\u4fe1\u606f\uff0c\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "InfMasking\u901a\u8fc7\u65e0\u9650\u63a9\u7801\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u534f\u540c\u4fe1\u606f\u63d0\u53d6\u7684\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.25278", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25278", "abs": "https://arxiv.org/abs/2509.25278", "authors": ["Payal Mohapatra", "Yueyuan Sui", "Akash Pandey", "Stephen Xia", "Qi Zhu"], "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series", "comment": "Accepted to Neurips 2025 (Spotlight)", "summary": "From clinical healthcare to daily living, continuous sensor monitoring across\nmultiple modalities has shown great promise for real-world intelligent\ndecision-making but also faces various challenges. In this work, we introduce\nMAESTRO, a novel framework that overcomes key limitations of existing\nmultimodal learning approaches: (1) reliance on a single primary modality for\nalignment, (2) pairwise modeling of modalities, and (3) assumption of complete\nmodality observations. These limitations hinder the applicability of these\napproaches in real-world multimodal time-series settings, where primary\nmodality priors are often unclear, the number of modalities can be large\n(making pairwise modeling impractical), and sensor failures often result in\narbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-\nand cross-modal interactions based on task relevance, and leverages symbolic\ntokenization and adaptive attention budgeting to construct long multimodal\nsequences, which are processed via sparse cross-modal attention. The resulting\ncross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)\nmechanism, enabling black-box specialization under varying modality\ncombinations. We evaluate MAESTRO against 10 baselines on four diverse datasets\nspanning three applications, and observe average relative improvements of 4%\nand 8% over the best existing multimodal and multivariate approaches,\nrespectively, under complete observations. Under partial observations -- with\nup to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.\nFurther analysis also demonstrates the robustness and efficiency of MAESTRO's\nsparse, modality-aware design for learning from dynamic time series.", "AI": {"tldr": "MAESTRO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8de8\u6a21\u6001\u4ea4\u4e92\u3001\u7b26\u53f7\u5316\u6807\u8bb0\u548c\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u4e3b\u6a21\u6001\u3001\u6210\u5bf9\u5efa\u6a21\u548c\u5b8c\u6574\u6a21\u6001\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u5728\u5b8c\u6574\u548c\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(1)\u4f9d\u8d56\u5355\u4e00\u4e3b\u6a21\u6001\u8fdb\u884c\u5bf9\u9f50\uff0c(2)\u6210\u5bf9\u5efa\u6a21\u65b9\u5f0f\uff0c(3)\u5047\u8bbe\u5b8c\u6574\u6a21\u6001\u89c2\u6d4b\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u4e3a\u4e3b\u6a21\u6001\u5148\u9a8c\u4e0d\u660e\u786e\u3001\u6a21\u6001\u6570\u91cf\u53ef\u80fd\u5f88\u5927\u3001\u4f20\u611f\u5668\u6545\u969c\u5bfc\u81f4\u4efb\u610f\u7f3a\u5931\u89c2\u6d4b\u3002", "method": "MAESTRO\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6a21\u6001\u5185\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u3001\u7b26\u53f7\u5316\u6807\u8bb0\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u9884\u7b97\u6784\u5efa\u957f\u591a\u6a21\u6001\u5e8f\u5217\uff0c\u4f7f\u7528\u7a00\u758f\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u8def\u7531\u8de8\u6a21\u6001\u6807\u8bb0\uff0c\u5b9e\u73b0\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u4e0b\u7684\u9ed1\u76d2\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMAESTRO\u5728\u5b8c\u6574\u89c2\u6d4b\u6761\u4ef6\u4e0b\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u591a\u6a21\u6001\u548c\u591a\u53d8\u91cf\u65b9\u6cd5\u5206\u522b\u5e73\u5747\u63d0\u53474%\u548c8%\uff1b\u5728\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\uff08\u7f3a\u5931\u6a21\u6001\u9ad8\u8fbe40%\uff09\uff0c\u5e73\u5747\u63d0\u53479%\u3002", "conclusion": "MAESTRO\u7684\u7a00\u758f\u3001\u6a21\u6001\u611f\u77e5\u8bbe\u8ba1\u5728\u52a8\u6001\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u76d1\u63a7\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25284", "categories": ["cs.LG", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25284", "abs": "https://arxiv.org/abs/2509.25284", "authors": ["Oluwaseyi Giwa", "Jonathan Shock", "Jaco Du Toit", "Tobi Awodumila"], "title": "Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning", "comment": "Submitted to IEEE Wireless Communications and Networking Conference,\n  2026", "summary": "Dynamic resource allocation in heterogeneous wireless networks (HetNets) is\nchallenging for traditional methods under varying user loads and channel\nconditions. We propose a deep reinforcement learning (DRL) framework that\njointly optimises transmit power, bandwidth, and scheduling via a\nmulti-objective reward balancing throughput, energy efficiency, and fairness.\nUsing real base station coordinates, we compare Proximal Policy Optimisation\n(PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three\nheuristic algorithms in multiple network scenarios. Our results show that DRL\nframeworks outperform heuristic algorithms in optimising resource allocation in\ndynamic networks. These findings highlight key trade-offs in DRL design for\nfuture HetNets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u53d1\u5c04\u529f\u7387\u3001\u5e26\u5bbd\u548c\u8c03\u5ea6\uff0c\u5728\u52a8\u6001\u7f51\u7edc\u4e2d\u4f18\u4e8e\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2d\u96be\u4ee5\u5e94\u5bf9\u53d8\u5316\u7684\u7528\u6237\u8d1f\u8f7d\u548c\u4fe1\u9053\u6761\u4ef6\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528PPO\u548cTD3\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u5e73\u8861\u541e\u5410\u91cf\u3001\u80fd\u6548\u548c\u516c\u5e73\u6027\u3002", "result": "\u5728\u771f\u5b9e\u57fa\u7ad9\u5750\u6807\u7684\u591a\u79cd\u7f51\u7edc\u573a\u666f\u4e0b\uff0cDRL\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u4e09\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "conclusion": "DRL\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\uff0c\u63ed\u793a\u4e86\u672a\u6765\u7f51\u7edc\u8bbe\u8ba1\u4e2d\u9700\u8981\u6743\u8861\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.25289", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25289", "abs": "https://arxiv.org/abs/2509.25289", "authors": ["Mohammadreza Bakhtyari", "Bogdan Mazoure", "Renato Cordeiro de Amorim", "Guillaume Rabusseau", "Vladimir Makarenkov"], "title": "ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering Algorithm Recommendation", "comment": null, "summary": "We introduce ClustRecNet - a novel deep learning (DL)-based recommendation\nframework for determining the most suitable clustering algorithms for a given\ndataset, addressing the long-standing challenge of clustering algorithm\nselection in unsupervised learning. To enable supervised learning in this\ncontext, we construct a comprehensive data repository comprising 34,000\nsynthetic datasets with diverse structural properties. Each of them was\nprocessed using 10 popular clustering algorithms. The resulting clusterings\nwere assessed via the Adjusted Rand Index (ARI) to establish ground truth\nlabels, used for training and evaluation of our DL model. The proposed network\narchitecture integrates convolutional, residual, and attention mechanisms to\ncapture both local and global structural patterns from the input data. This\ndesign supports end-to-end training to learn compact representations of\ndatasets and enables direct recommendation of the most suitable clustering\nalgorithm, reducing reliance on handcrafted meta-features and traditional\nCluster Validity Indices (CVIs). Comprehensive experiments across synthetic and\nreal-world benchmarks demonstrate that our DL model consistently outperforms\nconventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and\nDunn) as well as state-of-the-art AutoML clustering recommendation approaches\n(e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model\nachieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic\ndata and a 15.3% ARI gain over the best-performing AutoML approach on\nreal-world data.", "AI": {"tldr": "ClustRecNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u63a8\u8350\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u7ed9\u5b9a\u6570\u636e\u96c6\u63a8\u8350\u6700\u5408\u9002\u7684\u805a\u7c7b\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u805a\u7c7b\u7b97\u6cd5\u9009\u62e9\u7684\u957f\u671f\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u805a\u7c7b\u7b97\u6cd5\u9009\u62e9\u7684\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u5143\u7279\u5f81\u548c\u805a\u7c7b\u6709\u6548\u6027\u6307\u6807\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u6784\u5efa\u5305\u542b34,000\u4e2a\u5408\u6210\u6570\u636e\u96c6\u7684\u7efc\u5408\u6570\u636e\u4ed3\u5e93\uff0c\u4f7f\u752810\u79cd\u6d41\u884c\u805a\u7c7b\u7b97\u6cd5\u5904\u7406\uff0c\u901a\u8fc7\u8c03\u6574\u5170\u5fb7\u6307\u6570\u8bc4\u4f30\u805a\u7c7b\u7ed3\u679c\u5efa\u7acb\u771f\u5b9e\u6807\u7b7e\u3002\u7f51\u7edc\u67b6\u6784\u6574\u5408\u5377\u79ef\u3001\u6b8b\u5dee\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u5b66\u4e60\u6570\u636e\u96c6\u7684\u7d27\u51d1\u8868\u793a\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u805a\u7c7b\u6709\u6548\u6027\u6307\u6807\uff08\u5982Silhouette\u3001Calinski-Harabasz\u7b49\uff09\u548c\u6700\u5148\u8fdb\u7684AutoML\u805a\u7c7b\u63a8\u8350\u65b9\u6cd5\uff08\u5982ML2DAC\u3001AutoCluster\u7b49\uff09\u3002\u5728\u5408\u6210\u6570\u636e\u4e0a\u6bd4Calinski-Harabasz\u6307\u6570\u63d0\u9ad80.497 ARI\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6bd4\u6700\u4f73AutoML\u65b9\u6cd5\u63d0\u9ad815.3% ARI\u3002", "conclusion": "ClustRecNet\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u805a\u7c7b\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709AutoML\u65b9\u6cd5\uff0c\u4e3a\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7b97\u6cd5\u63a8\u8350\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25300", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25300", "abs": "https://arxiv.org/abs/2509.25300", "authors": ["Zelin Tan", "Hejia Geng", "Mulei Zhang", "Xiaohang Yu", "Guancheng Wan", "Yifan Zhou", "Qiang He", "Xiangyuan Xue", "Heng Zhou", "Yutao Fan", "Zhongzhi Li", "Zaibin Zhang", "Guibin Zhang", "Chen Zhang", "Zhenfei Yin", "Lei Bai"], "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning", "comment": "V1 version", "summary": "While scaling laws for large language models (LLMs) during pre-training have\nbeen extensively studied, their behavior under reinforcement learning (RL)\npost-training remains largely unexplored. This paper presents a systematic\nempirical investigation of scaling behaviors in RL-based post-training, with a\nparticular focus on mathematical reasoning. Based on 54 experiments across\ndiverse model sizes and training settings, we characterize how model scale,\ndata volume, and computational budget interact to shape performance. Our\nanalysis leads to four key findings: (1). Under a fixed computational budget,\nlarger models trained for fewer steps consistently outperform smaller models\ntrained for more steps. (2). Given a fixed amount of training data, larger\nmodels achieve superior sample efficiency, yielding lower loss. (3). In\ndata-constrained regimes, repeated reuse of high-quality data proves highly\neffective, as final performance is primarily governed by the total number of\noptimization steps rather than the uniqueness of samples. (4). These scaling\nbehaviors are robust across both base and instruction-tuned models, which share\nsimilar learning dynamics (e.g., larger models show faster convergence) even\nwhile differing in absolute accuracy. Collectively, these results provide a\nprincipled foundation and practical guidelines for efficiently scaling the\nreasoning capabilities of LLMs through RL post-training.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u53d1\u73b0\u5927\u6a21\u578b\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u6837\u672c\u6548\u7387\u66f4\u9ad8\uff0c\u6570\u636e\u91cd\u590d\u4f7f\u7528\u6709\u6548\uff0c\u4e14\u8fd9\u4e9b\u89c4\u5f8b\u5728\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u6269\u5c55\u89c4\u5f8b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "method": "\u57fa\u4e8e54\u4e2a\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u9884\u7b97\u5728RL\u540e\u8bad\u7ec3\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u53d1\u73b0\u56db\u4e2a\u5173\u952e\u89c4\u5f8b\uff1a1) \u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u5927\u6a21\u578b\u8bad\u7ec3\u5c11\u6b65\u6570\u4f18\u4e8e\u5c0f\u6a21\u578b\u591a\u6b65\u6570\uff1b2) \u56fa\u5b9a\u6570\u636e\u91cf\u4e0b\uff0c\u5927\u6a21\u578b\u6837\u672c\u6548\u7387\u66f4\u9ad8\uff1b3) \u6570\u636e\u53d7\u9650\u65f6\uff0c\u91cd\u590d\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u6548\uff1b4) \u8fd9\u4e9b\u89c4\u5f8b\u5728\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e2d\u5747\u6210\u7acb\u3002", "conclusion": "\u4e3a\u901a\u8fc7RL\u540e\u8bad\u7ec3\u9ad8\u6548\u6269\u5c55LLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.25334", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25334", "abs": "https://arxiv.org/abs/2509.25334", "authors": ["Amirhossein Zare", "Amirhessam Zare", "Parmida Sadat Pezeshki", "Herlock", "Rahimi", "Ali Ebrahimi", "Ignacio V\u00e1zquez-Garc\u00eda", "Leo Anthony Celi"], "title": "Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder", "comment": "16 pages, 2 figures", "summary": "Class imbalance remains a major challenge in machine learning, especially for\nhigh-dimensional biomedical data where nonlinear manifold structures dominate.\nTraditional oversampling methods such as SMOTE rely on local linear\ninterpolation, often producing implausible synthetic samples. Deep generative\nmodels like Conditional Variational Autoencoders (CVAEs) better capture\nnonlinear distributions, but standard variants treat all minority samples\nequally, neglecting the importance of uncertain, boundary-region examples\nemphasized by heuristic methods like Borderline-SMOTE and ADASYN.\n  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a\ngenerative oversampling framework that explicitly incorporates local\nuncertainty into both representation learning and data generation. To quantify\nuncertainty, we compute Shannon entropy over the class distribution in a\nsample's neighborhood: high entropy indicates greater class overlap, serving as\na proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms:\n(i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in\nuncertain regions, and (ii) an entropy-guided sampling strategy that\nconcentrates generation in these informative, class-overlapping areas.\n  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE\nconsistently improves classifier performance, outperforming both traditional\noversampling and generative baselines. These results highlight the value of\nuncertainty-aware generative oversampling for imbalanced learning in domains\ngoverned by complex nonlinear structures, such as omics data.", "AI": {"tldr": "LEO-CVAE\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u71b5\u5f15\u5bfc\u5728\u7c7b\u522b\u91cd\u53e0\u7684\u4e0d\u786e\u5b9a\u533a\u57df\u751f\u6210\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u751f\u7269\u533b\u5b66\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u5982SMOTE\u4f9d\u8d56\u5c40\u90e8\u7ebf\u6027\u63d2\u503c\uff0c\u5728\u5904\u7406\u9ad8\u7ef4\u975e\u7ebf\u6027\u751f\u7269\u533b\u5b66\u6570\u636e\u65f6\u4f1a\u4ea7\u751f\u4e0d\u5408\u7406\u7684\u5408\u6210\u6837\u672c\u3002\u867d\u7136\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u80fd\u66f4\u597d\u5730\u6355\u6349\u975e\u7ebf\u6027\u5206\u5e03\uff0c\u4f46\u6807\u51c6\u53d8\u4f53\u5ffd\u89c6\u4e86\u8fb9\u754c\u533a\u57df\u4e0d\u786e\u5b9a\u6837\u672c\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u71b5\u5f15\u5bfc\u7684CVAE\u8fc7\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u6837\u672c\u90bb\u57df\u7684\u9999\u519c\u71b5\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u5c40\u90e8\u71b5\u52a0\u6743\u635f\u5931\u548c\u71b5\u5f15\u5bfc\u91c7\u6837\u7b56\u7565\uff0c\u5728\u7c7b\u522b\u91cd\u53e0\u7684\u4e0d\u786e\u5b9a\u533a\u57df\u96c6\u4e2d\u751f\u6210\u6837\u672c\u3002", "result": "\u5728\u4e34\u5e8a\u57fa\u56e0\u7ec4\u6570\u636e\u96c6\uff08ADNI\u548cTCGA\u80ba\u764c\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLEO-CVAE\u6301\u7eed\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u548c\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7531\u590d\u6742\u975e\u7ebf\u6027\u7ed3\u6784\u4e3b\u5bfc\u7684\u9886\u57df\uff08\u5982\u7ec4\u5b66\u6570\u636e\uff09\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u751f\u6210\u5f0f\u8fc7\u91c7\u6837\u5bf9\u4e8e\u4e0d\u5e73\u8861\u5b66\u4e60\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.25351", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25351", "abs": "https://arxiv.org/abs/2509.25351", "authors": ["Shuang Liang", "Guido Mont\u00fafar"], "title": "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region", "comment": null, "summary": "We examine gradient descent in matrix factorization and show that under large\nstep sizes the parameter space develops a fractal structure. We derive the\nexact critical step size for convergence in scalar-vector factorization and\nshow that near criticality the selected minimizer depends sensitively on the\ninitialization. Moreover, we show that adding regularization amplifies this\nsensitivity, generating a fractal boundary between initializations that\nconverge and those that diverge. The analysis extends to general matrix\nfactorization with orthogonal initialization. Our findings reveal that\nnear-critical step sizes induce a chaotic regime of gradient descent where the\nlong-term dynamics are unpredictable and there are no simple implicit biases,\nsuch as towards balancedness, minimum norm, or flatness.", "AI": {"tldr": "\u7814\u7a76\u68af\u5ea6\u4e0b\u964d\u5728\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5927\u5b66\u4e60\u7387\u4e0b\u53c2\u6570\u7a7a\u95f4\u51fa\u73b0\u5206\u5f62\u7ed3\u6784\uff0c\u63a8\u5bfc\u4e86\u6807\u91cf-\u5411\u91cf\u5206\u89e3\u7684\u4e34\u754c\u5b66\u4e60\u7387\uff0c\u5e76\u8bc1\u660e\u6b63\u5219\u5316\u4f1a\u653e\u5927\u5bf9\u521d\u59cb\u5316\u7684\u654f\u611f\u6027\u3002", "motivation": "\u63a2\u7d22\u68af\u5ea6\u4e0b\u964d\u5728\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u6536\u655b\u7279\u6027\u548c\u53c2\u6570\u7a7a\u95f4\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u6807\u91cf-\u5411\u91cf\u5206\u89e3\u7684\u4e34\u754c\u5b66\u4e60\u7387\uff0c\u7814\u7a76\u6b63\u5219\u5316\u5bf9\u6536\u655b\u8fb9\u754c\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u5206\u6790\u6269\u5c55\u5230\u6b63\u4ea4\u521d\u59cb\u5316\u7684\u901a\u7528\u77e9\u9635\u5206\u89e3\u3002", "result": "\u53d1\u73b0\u4e34\u754c\u5b66\u4e60\u7387\u9644\u8fd1\u5b58\u5728\u6df7\u6c8c\u533a\u57df\uff0c\u957f\u671f\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\uff0c\u4e14\u6ca1\u6709\u7b80\u5355\u7684\u9690\u5f0f\u504f\u7f6e\uff08\u5982\u5e73\u8861\u6027\u3001\u6700\u5c0f\u8303\u6570\u6216\u5e73\u5766\u6027\uff09\u3002", "conclusion": "\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u68af\u5ea6\u4e0b\u964d\u5728\u77e9\u9635\u5206\u89e3\u4e2d\u4f1a\u4ea7\u751f\u5206\u5f62\u7ed3\u6784\u548c\u6df7\u6c8c\u884c\u4e3a\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u5173\u4e8e\u9690\u5f0f\u504f\u7f6e\u7684\u5047\u8bbe\u3002"}}
{"id": "2509.25376", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.25376", "abs": "https://arxiv.org/abs/2509.25376", "authors": ["Linus Aronsson", "Han Wu", "Morteza Haghir Chehreghani"], "title": "Cold-Start Active Correlation Clustering", "comment": null, "summary": "We study active correlation clustering where pairwise similarities are not\nprovided upfront and must be queried in a cost-efficient manner through active\nlearning. Specifically, we focus on the cold-start scenario, where no true\ninitial pairwise similarities are available for active learning. To address\nthis challenge, we propose a coverage-aware method that encourages diversity\nearly in the process. We demonstrate the effectiveness of our approach through\nseveral synthetic and real-world experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u4e3b\u52a8\u76f8\u5173\u805a\u7c7b\u7684\u8986\u76d6\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u9f13\u52b1\u65e9\u671f\u591a\u6837\u6027\u6765\u89e3\u51b3\u7f3a\u4e4f\u521d\u59cb\u6210\u5bf9\u76f8\u4f3c\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u4e3b\u52a8\u76f8\u5173\u805a\u7c7b\u4e2d\uff0c\u6210\u5bf9\u76f8\u4f3c\u5ea6\u65e0\u6cd5\u9884\u5148\u83b7\u5f97\uff0c\u9700\u8981\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u4ee5\u6210\u672c\u6548\u76ca\u7684\u65b9\u5f0f\u67e5\u8be2\u3002\u7279\u522b\u5173\u6ce8\u51b7\u542f\u52a8\u573a\u666f\uff0c\u5373\u6ca1\u6709\u771f\u5b9e\u521d\u59cb\u6210\u5bf9\u76f8\u4f3c\u5ea6\u53ef\u7528\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u8986\u76d6\u611f\u77e5\u65b9\u6cd5\uff0c\u5728\u8fc7\u7a0b\u65e9\u671f\u9f13\u52b1\u591a\u6837\u6027\uff0c\u4ee5\u89e3\u51b3\u51b7\u542f\u52a8\u6311\u6218\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8986\u76d6\u611f\u77e5\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u4e3b\u52a8\u76f8\u5173\u805a\u7c7b\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u80fd\u591f\u901a\u8fc7\u65e9\u671f\u591a\u6837\u6027\u7b56\u7565\u514b\u670d\u7f3a\u4e4f\u521d\u59cb\u76f8\u4f3c\u5ea6\u4fe1\u606f\u7684\u95ee\u9898\u3002"}}
{"id": "2509.25379", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25379", "abs": "https://arxiv.org/abs/2509.25379", "authors": ["Yogesh Verma", "Markus Heinonen", "Vikas Garg"], "title": "Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation", "comment": null, "summary": "Protein structure prediction and folding are fundamental to understanding\nbiology, with recent deep learning advances reshaping the field.\nDiffusion-based generative models have revolutionized protein design, enabling\nthe creation of novel proteins. However, these methods often neglect the\nintrinsic physical realism of proteins, driven by noising dynamics that lack\ngrounding in physical principles. To address this, we first introduce a\nphysically motivated non-linear noising process, grounded in classical physics,\nthat unfolds proteins into secondary structures (e.g., alpha helices, linear\nbeta sheets) while preserving topological integrity--maintaining bonds, and\npreventing collisions. We then integrate this process with the flow-matching\nparadigm on SE(3) to model the invariant distribution of protein backbones with\nhigh fidelity, incorporating sequence information to enable\nsequence-conditioned folding and expand the generative capabilities of our\nmodel. Experimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in unconditional protein generation, producing\nmore designable and novel protein structures while accurately folding monomer\nsequences into precise protein conformations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u975e\u7ebf\u6027\u566a\u58f0\u8fc7\u7a0b\uff0c\u7ed3\u5408SE(3)\u6d41\u5339\u914d\uff0c\u7528\u4e8e\u86cb\u767d\u8d28\u7ed3\u6784\u751f\u6210\u548c\u6298\u53e0\uff0c\u5728\u65e0\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u751f\u6210\u6a21\u578b\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\u5ffd\u7565\u4e86\u86cb\u767d\u8d28\u7684\u7269\u7406\u771f\u5b9e\u6027\uff0c\u566a\u58f0\u8fc7\u7a0b\u7f3a\u4e4f\u7269\u7406\u539f\u7406\u57fa\u7840\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u7ecf\u5178\u7269\u7406\u7684\u975e\u7ebf\u6027\u566a\u58f0\u8fc7\u7a0b\uff0c\u5c06\u86cb\u767d\u8d28\u5c55\u5f00\u4e3a\u4e8c\u7ea7\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u62d3\u6251\u5b8c\u6574\u6027\uff1b\u7ed3\u5408SE(3)\u6d41\u5339\u914d\uff0c\u5efa\u6a21\u86cb\u767d\u8d28\u9aa8\u67b6\u7684\u4e0d\u53d8\u5206\u5e03\uff0c\u5e76\u6574\u5408\u5e8f\u5217\u4fe1\u606f\u5b9e\u73b0\u5e8f\u5217\u6761\u4ef6\u6298\u53e0\u3002", "result": "\u5728\u65e0\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ea7\u751f\u66f4\u5177\u8bbe\u8ba1\u6027\u548c\u65b0\u9896\u6027\u7684\u86cb\u767d\u8d28\u7ed3\u6784\uff0c\u5e76\u80fd\u51c6\u786e\u5c06\u5355\u4f53\u5e8f\u5217\u6298\u53e0\u4e3a\u7cbe\u786e\u6784\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u7269\u7406\u539f\u7406\u878d\u5165\u86cb\u767d\u8d28\u751f\u6210\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u751f\u6210\u7ed3\u6784\u7684\u7269\u7406\u771f\u5b9e\u6027\u548c\u8bbe\u8ba1\u6f5c\u529b\u3002"}}
{"id": "2509.25380", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25380", "abs": "https://arxiv.org/abs/2509.25380", "authors": ["Shane Bergsma", "Nolan Dey", "Joel Hestness"], "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs", "comment": null, "summary": "Data curriculums have become central to successful LLM training, yet\nprinciples governing optimal data placement remain unclear. We introduce the\n*training re-evaluation curve (TREC)*, a diagnostic that retrospectively\nevaluates training batches *using the final model weights*. The TREC\ncharacterizes how well a trained model retains training data as a function of\n*when* the data was encountered during training. Analyzing TRECs for models\nfrom 111M to 3.9B parameters, we show that placing high-quality data at low\npoints on the TREC significantly improves performance. Importantly, while a\nTREC is initially observable only after training, we demonstrate it can be\n*predicted in advance* from AdamW's implicit EMA coefficients, enabling\nproactive curriculum design. By predicting TRECs for published training\nrecipes, we explain prior ablations and reveal suboptimal data placements. We\nalso align high-quality data with TREC minima in order to improve continual\npre-training of a 3.9B-parameter LLM trained on 900B tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bad\u7ec3\u91cd\u8bc4\u4f30\u66f2\u7ebf(TREC)\u6765\u8bca\u65ad\u8bad\u7ec3\u6570\u636e\u653e\u7f6e\u6548\u679c\uff0c\u53d1\u73b0\u5c06\u9ad8\u8d28\u91cf\u6570\u636e\u653e\u5728TREC\u4f4e\u70b9\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u80fd\u901a\u8fc7AdamW\u7684EMA\u7cfb\u6570\u63d0\u524d\u9884\u6d4bTREC\uff0c\u7528\u4e8e\u4f18\u5316\u8bfe\u7a0b\u8bbe\u8ba1\u3002", "motivation": "\u6570\u636e\u8bfe\u7a0b\u5bf9LLM\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6700\u4f18\u6570\u636e\u653e\u7f6e\u539f\u5219\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u95f4\u5b89\u6392\u3002", "method": "\u5f15\u5165TREC\u8bca\u65ad\u5de5\u5177\uff0c\u4f7f\u7528\u6700\u7ec8\u6a21\u578b\u6743\u91cd\u56de\u987e\u6027\u8bc4\u4f30\u8bad\u7ec3\u6279\u6b21\uff0c\u5206\u6790\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u6a21\u578b\u7684TREC\uff0c\u5e76\u5229\u7528AdamW\u7684EMA\u7cfb\u6570\u9884\u6d4bTREC\u3002", "result": "\u5206\u6790111M\u52303.9B\u53c2\u6570\u6a21\u578b\u663e\u793a\uff0c\u5c06\u9ad8\u8d28\u91cf\u6570\u636e\u653e\u5728TREC\u4f4e\u70b9\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b\u80fd\u63d0\u524d\u9884\u6d4bTREC\uff1b\u901a\u8fc7\u4f18\u5316\u6570\u636e\u653e\u7f6e\u6539\u5584\u4e863.9B\u53c2\u6570LLM\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u3002", "conclusion": "TREC\u4e3a\u6570\u636e\u8bfe\u7a0b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u80fd\u591f\u89e3\u91ca\u5148\u524d\u6d88\u878d\u5b9e\u9a8c\u7ed3\u679c\u5e76\u63ed\u793a\u6b21\u4f18\u6570\u636e\u653e\u7f6e\uff0c\u901a\u8fc7\u9884\u6d4b\u548c\u4f18\u5316TREC\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.25381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25381", "abs": "https://arxiv.org/abs/2509.25381", "authors": ["Penglei Gao", "Yan Zou", "Abhijit Duggal", "Shuaiqi Huang", "Faming Liang", "Xiaofeng Wang"], "title": "Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation", "comment": null, "summary": "We introduce the Functional Competing Risk Net (FCRN), a unified\ndeep-learning framework for discrete-time survival analysis under competing\nrisks, which seamlessly integrates functional covariates and handles missing\ndata within an end-to-end model. By combining a micro-network Basis Layer for\nfunctional data representation with a gradient-based imputation module, FCRN\nsimultaneously learns to impute missing values and predict event-specific\nhazards. Evaluated on multiple simulated datasets and a real-world ICU case\nstudy using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates\nsubstantial improvements in prediction accuracy over random survival forests\nand traditional competing risks models. This approach advances prognostic\nmodeling in critical care by more effectively capturing dynamic risk factors\nand static predictors while accommodating irregular and incomplete data.", "AI": {"tldr": "FCRN\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7ade\u4e89\u98ce\u9669\u4e0b\u7684\u79bb\u6563\u65f6\u95f4\u751f\u5b58\u5206\u6790\uff0c\u6574\u5408\u529f\u80fd\u534f\u53d8\u91cf\u5e76\u5904\u7406\u7f3a\u5931\u6570\u636e\u3002", "motivation": "\u5728\u91cd\u75c7\u76d1\u62a4\u7b49\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5904\u7406\u529f\u80fd\u534f\u53d8\u91cf\u548c\u7f3a\u5931\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u751f\u5b58\u5206\u6790\u3002", "method": "\u7ed3\u5408\u5fae\u7f51\u7edc\u57fa\u7840\u5c42\u8868\u793a\u529f\u80fd\u6570\u636e\uff0c\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u63d2\u8865\u6a21\u5757\uff0c\u540c\u65f6\u5b66\u4e60\u7f3a\u5931\u503c\u63d2\u8865\u548c\u4e8b\u4ef6\u7279\u5b9a\u98ce\u9669\u9884\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u548c\u771f\u5b9eICU\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cFCRN\u76f8\u6bd4\u968f\u673a\u751f\u5b58\u68ee\u6797\u548c\u4f20\u7edf\u7ade\u4e89\u98ce\u9669\u6a21\u578b\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FCRN\u901a\u8fc7\u66f4\u6709\u6548\u5730\u6355\u6349\u52a8\u6001\u98ce\u9669\u56e0\u7d20\u548c\u9759\u6001\u9884\u6d4b\u56e0\u5b50\uff0c\u540c\u65f6\u5904\u7406\u4e0d\u89c4\u5219\u548c\u4e0d\u5b8c\u6574\u6570\u636e\uff0c\u63a8\u8fdb\u4e86\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u9884\u540e\u5efa\u6a21\u3002"}}
{"id": "2509.25395", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25395", "abs": "https://arxiv.org/abs/2509.25395", "authors": ["Jordyn E. A. Lorentz", "Katharine M. Clark"], "title": "Crowdsourcing Without People: Modelling Clustering Algorithms as Experts", "comment": null, "summary": "This paper introduces mixsemble, an ensemble method that adapts the\nDawid-Skene model to aggregate predictions from multiple model-based clustering\nalgorithms. Unlike traditional crowdsourcing, which relies on human labels, the\nframework models the outputs of clustering algorithms as noisy annotations.\nExperiments on both simulated and real-world datasets show that, although the\nmixsemble is not always the single top performer, it consistently approaches\nthe best result and avoids poor outcomes. This robustness makes it a practical\nalternative when the true data structure is unknown, especially for non-expert\nusers.", "AI": {"tldr": "mixsemble\u662f\u4e00\u79cd\u96c6\u6210\u65b9\u6cd5\uff0c\u5c06Dawid-Skene\u6a21\u578b\u5e94\u7528\u4e8e\u591a\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u805a\u7c7b\u7b97\u6cd5\u9884\u6d4b\u7684\u805a\u5408\uff0c\u5c06\u8fd9\u4e9b\u7b97\u6cd5\u7684\u8f93\u51fa\u89c6\u4e3a\u566a\u58f0\u6807\u6ce8\u3002", "motivation": "\u4f20\u7edf\u4f17\u5305\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u8be5\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u805a\u7c7b\u7b97\u6cd5\u7684\u8f93\u51fa\u6765\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\uff0c\u4e3a\u6570\u636e\u771f\u5b9e\u7ed3\u6784\u672a\u77e5\u65f6\u63d0\u4f9b\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u591a\u4e2a\u805a\u7c7b\u7b97\u6cd5\u7684\u8f93\u51fa\u4f5c\u4e3a\u566a\u58f0\u6807\u6ce8\uff0c\u4f7f\u7528Dawid-Skene\u6a21\u578b\u8fdb\u884c\u96c6\u6210\uff0c\u9002\u5e94\u6a21\u578b\u57fa\u805a\u7c7b\u7b97\u6cd5\u7684\u9884\u6d4b\u805a\u5408\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cmixsemble\u867d\u4e0d\u603b\u662f\u6700\u4f73\uff0c\u4f46\u59cb\u7ec8\u63a5\u8fd1\u6700\u4f73\u7ed3\u679c\u5e76\u907f\u514d\u8f83\u5dee\u7ed3\u679c\uff0c\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3002", "conclusion": "mixsemble\u5728\u6570\u636e\u771f\u5b9e\u7ed3\u6784\u672a\u77e5\u65f6\u662f\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u5408\u975e\u4e13\u5bb6\u7528\u6237\uff0c\u56e0\u5176\u7a33\u5065\u6027\u800c\u907f\u514d\u4e0d\u826f\u7ed3\u679c\u3002"}}
{"id": "2509.25400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25400", "abs": "https://arxiv.org/abs/2509.25400", "authors": ["S C Bee", "N Dervilis", "K Worden", "L A Bull"], "title": "Multi-Task Equation Discovery", "comment": null, "summary": "Equation discovery provides a grey-box approach to system identification by\nuncovering governing dynamics directly from observed data. However, a\npersistent challenge lies in ensuring that identified models generalise across\noperating conditions rather than over-fitting to specific datasets. This work\ninvestigates this issue by applying a Bayesian relevance vector machine (RVM)\nwithin a multi-task learning (MTL) framework for simultaneous parameter\nidentification across multiple datasets. In this formulation, responses from\nthe same structure under different excitation levels are treated as related\ntasks that share model parameters but retain task-specific noise\ncharacteristics. A simulated single degree-of-freedom oscillator with linear\nand cubic stiffness provided the case study, with datasets generated under\nthree excitation regimes. Standard single-task RVM models were able to\nreproduce system responses but often failed to recover the true governing terms\nwhen excitations insufficiently stimulated non-linear dynamics. By contrast,\nthe MTL-RVM combined information across tasks, improving parameter recovery for\nweakly and moderately excited datasets, while maintaining strong performance\nunder high excitation. These findings demonstrate that multi-task Bayesian\ninference can mitigate over-fitting and promote generalisation in equation\ndiscovery. The approach is particularly relevant to structural health\nmonitoring, where varying load conditions reveal complementary aspects of\nsystem physics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u8d1d\u53f6\u65af\u76f8\u5173\u5411\u91cf\u673a\u65b9\u6cd5\uff0c\u7528\u4e8e\u65b9\u7a0b\u53d1\u73b0\u4e2d\u7684\u53c2\u6570\u8bc6\u522b\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u6fc0\u52b1\u6761\u4ef6\u4e0b\u7684\u6570\u636e\u6765\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65b9\u7a0b\u53d1\u73b0\u5728\u7cfb\u7edf\u8bc6\u522b\u4e2d\u5b58\u5728\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u786e\u4fdd\u8bc6\u522b\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u540c\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u9762\u5bf9\u53d8\u5316\u8f7d\u8377\u6761\u4ef6\u65f6\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u76f8\u5173\u5411\u91cf\u673a(RVM)\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60(MTL)\u6846\u67b6\uff0c\u5c06\u540c\u4e00\u7ed3\u6784\u5728\u4e0d\u540c\u6fc0\u52b1\u6c34\u5e73\u4e0b\u7684\u54cd\u5e94\u89c6\u4e3a\u76f8\u5173\u4efb\u52a1\uff0c\u5171\u4eab\u6a21\u578b\u53c2\u6570\u4f46\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u7684\u566a\u58f0\u7279\u6027\u3002", "result": "\u4e0e\u6807\u51c6\u5355\u4efb\u52a1RVM\u76f8\u6bd4\uff0cMTL-RVM\u80fd\u591f\u7ed3\u5408\u8de8\u4efb\u52a1\u4fe1\u606f\uff0c\u5728\u5f31\u6fc0\u52b1\u548c\u4e2d\u7b49\u6fc0\u52b1\u6761\u4ef6\u4e0b\u663e\u8457\u6539\u5584\u53c2\u6570\u6062\u590d\u6548\u679c\uff0c\u540c\u65f6\u5728\u9ad8\u6fc0\u52b1\u6761\u4ef6\u4e0b\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u591a\u4efb\u52a1\u8d1d\u53f6\u65af\u63a8\u65ad\u80fd\u591f\u51cf\u8f7b\u65b9\u7a0b\u53d1\u73b0\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4fc3\u8fdb\u6a21\u578b\u6cdb\u5316\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u4e0d\u540c\u8f7d\u8377\u6761\u4ef6\u63ed\u793a\u7cfb\u7edf\u7269\u7406\u4e92\u8865\u65b9\u9762\u7684\u60c5\u51b5\u3002"}}
{"id": "2509.25414", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25414", "abs": "https://arxiv.org/abs/2509.25414", "authors": ["Hao Ban", "Kaiyi Ji"], "title": "Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs", "comment": null, "summary": "Large language models are often adapted using parameter-efficient techniques\nsuch as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$\nis the pre-trained parameters and $x$ is the input to the adapted layer. While\nmulti-adapter extensions often employ multiple LoRAs, prior studies suggest\nthat the inner $A$ matrices are highly similar during training and thus\nsuitable for sharing. We revisit this phenomenon and find that this similarity\nis largely attributable to the identical initialization rather than shared\nknowledge, with $B$ playing a more critical role in knowledge encoding and\ntransfer. Motivated by these insights, we propose \\textbf{ALoRA}, an asymmetric\nmulti-LoRA design with multiple $A$ matrices and a single shared $B$ in\nmulti-task fine-tuning, and \\textbf{Fed-ALoRA}, which shares $B$ across clients\nin federated fine-tuning under both homogeneous and heterogeneous settings,\nthrough a novel matrix decomposition strategy to accommodate heterogeneous\nranks across clients. Experiments on commonsense reasoning, math reasoning,\nmulti-task NLP dataset, and federated NLP dataset demonstrate that our methods\nachieve more balanced performance across tasks with comparable or superior\naverage accuracy relative to existing multi-LoRA approaches. Codes are\navailable at https://github.com/OptMN-Lab/ALoRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faALoRA\u548cFed-ALoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u5bf9\u79f0\u7684LoRA\u8bbe\u8ba1\uff08\u591a\u4e2aA\u77e9\u9635\u5171\u4eab\u5355\u4e2aB\u77e9\u9635\uff09\u5728\u591a\u4efb\u52a1\u5fae\u8c03\u548c\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0LoRA\u4e2d\u7684\u5185\u90e8A\u77e9\u9635\u76f8\u4f3c\u6027\u4e3b\u8981\u6e90\u4e8e\u76f8\u540c\u7684\u521d\u59cb\u5316\u800c\u975e\u5171\u4eab\u77e5\u8bc6\uff0c\u800cB\u77e9\u9635\u5728\u77e5\u8bc6\u7f16\u7801\u548c\u4f20\u8f93\u4e2d\u8d77\u66f4\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51faALoRA\uff08\u591a\u4efb\u52a1\u5fae\u8c03\u4e2d\u4f7f\u7528\u591a\u4e2aA\u77e9\u9635\u5171\u4eab\u5355\u4e2aB\uff09\u548cFed-ALoRA\uff08\u8054\u90a6\u5b66\u4e60\u4e2d\u8de8\u5ba2\u6237\u7aef\u5171\u4eabB\uff0c\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u7b56\u7565\u5904\u7406\u5f02\u6784\u79e9\uff09\u3002", "result": "\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u3001\u591a\u4efb\u52a1NLP\u6570\u636e\u96c6\u548c\u8054\u90a6NLP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u591aLoRA\u65b9\u6cd5\uff0c\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u9ad8\u5e73\u5747\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u4e0d\u5bf9\u79f0\u7684LoRA\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u591a\u4efb\u52a1\u548c\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5e73\u8861\u6027\uff0cB\u77e9\u9635\u7684\u5171\u4eab\u662f\u63d0\u5347\u77e5\u8bc6\u4f20\u8f93\u6548\u7387\u7684\u5173\u952e\u3002"}}
{"id": "2509.25418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25418", "abs": "https://arxiv.org/abs/2509.25418", "authors": ["Dong Hyun Jeon", "Lijing Zhu", "Haifang Li", "Pengze Li", "Jingna Feng", "Tiehang Duan", "Houbing Herbert Song", "Cui Tao", "Shuteng Niu"], "title": "Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults", "comment": null, "summary": "Temporal Graph Neural Networks (TGNNs) have become indispensable for\nanalyzing dynamic graphs in critical applications such as social networks,\ncommunication systems, and financial networks. However, the robustness of TGNNs\nagainst adversarial attacks, particularly sophisticated attacks that exploit\nthe temporal dimension, remains a significant challenge. Existing attack\nmethods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic,\neasily detectable perturbations (e.g., random edge additions/deletions) and\nfail to strategically target the most influential nodes and edges for maximum\nimpact. We introduce the High Impact Attack (HIA), a novel restricted black-box\nattack framework specifically designed to overcome these limitations and expose\ncritical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model\nto identify structurally important nodes (central to network connectivity) and\ndynamically important nodes (critical for the graph's temporal evolution). It\nthen employs a hybrid perturbation strategy, combining strategic edge injection\n(to create misleading connections) and targeted edge deletion (to disrupt\nessential pathways), maximizing TGNN performance degradation. Importantly, HIA\nminimizes the number of perturbations to enhance stealth, making it more\nchallenging to detect. Comprehensive experiments on five real-world datasets\nand four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT)\ndemonstrate that HIA significantly reduces TGNN accuracy on the link prediction\ntask, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a\nsubstantial improvement over state-of-the-art baselines. These results\nhighlight fundamental vulnerabilities in current STDG models and underscore the\nurgent need for robust defenses that account for both structural and temporal\ndynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86HIA\u653b\u51fb\u6846\u67b6\uff0c\u9488\u5bf9\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc(TGNNs)\u8fdb\u884c\u9ad8\u6548\u9690\u853d\u7684\u9ed1\u76d2\u653b\u51fb\uff0c\u901a\u8fc7\u8bc6\u522b\u7ed3\u6784\u91cd\u8981\u6027\u548c\u52a8\u6001\u91cd\u8981\u6027\u8282\u70b9\uff0c\u7ed3\u5408\u8fb9\u6ce8\u5165\u548c\u5220\u9664\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4eTGNN\u6027\u80fd\u3002", "motivation": "\u73b0\u6709STDG\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u6270\u52a8\u4e14\u6613\u88ab\u68c0\u6d4b\uff0c\u65e0\u6cd5\u6218\u7565\u6027\u653b\u51fb\u6700\u5177\u5f71\u54cd\u529b\u7684\u8282\u70b9\u548c\u8fb9\uff0cTGNN\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u4ecd\u662f\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684\u4ee3\u7406\u6a21\u578b\u8bc6\u522b\u7ed3\u6784\u91cd\u8981\u8282\u70b9\u548c\u52a8\u6001\u91cd\u8981\u8282\u70b9\uff0c\u91c7\u7528\u6df7\u5408\u6270\u52a8\u7b56\u7565\uff08\u6218\u7565\u8fb9\u6ce8\u5165\u548c\u5b9a\u5411\u8fb9\u5220\u9664\uff09\uff0c\u6700\u5c0f\u5316\u6270\u52a8\u6570\u91cf\u4ee5\u589e\u5f3a\u9690\u853d\u6027\u3002", "result": "\u57285\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c4\u79cdTGNN\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIA\u663e\u8457\u964d\u4f4e\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u7684\u51c6\u786e\u7387\uff0cMRR\u6700\u591a\u4e0b\u964d35.55%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dSTDG\u6a21\u578b\u7684\u57fa\u672c\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u540c\u65f6\u8003\u8651\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u7684\u9c81\u68d2\u9632\u5fa1\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2509.25424", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25424", "abs": "https://arxiv.org/abs/2509.25424", "authors": ["Jubayer Ibn Hamid", "Ifdita Hasan Orney", "Ellen Xu", "Chelsea Finn", "Dorsa Sadigh"], "title": "Polychromic Objectives for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for\nimproving pretrained policies for downstream tasks. These pretrained policies,\ntrained on large datasets, produce generations with a broad range of promising\nbut unrefined behaviors. Often, a critical failure mode of RLFT arises when\npolicies lose this diversity and collapse into a handful of easily exploitable\noutputs. This convergence hinders exploration, which is essential for expanding\nthe capabilities of the pretrained policy and for amplifying the benefits of\ntest-time compute scaling. To address this, we introduce an objective for\npolicy gradient methods that explicitly enforces the exploration and refinement\nof diverse generations, which we call a polychromic objective. We then show how\nproximal policy optimization (PPO) can be adapted to optimize this objective.\nOur method (1) employs vine sampling to collect on-policy rollouts and (2)\nmodifies the advantage function to reflect the advantage under our new\nobjective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show\nthat our method improves success rates by reliably solving a larger set of\nenvironment configurations and generalizes better under large perturbations.\nMoreover, when given multiple attempts in pass@$k$ experiments, the policy\nachieves substantially higher coverage, demonstrating its ability to maintain\nand exploit a diverse repertoire of strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff08polychromic objective\uff09\uff0c\u901a\u8fc7\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3RLFT\u4e2d\u7b56\u7565\u591a\u6837\u6027\u4e27\u5931\u7684\u95ee\u9898\uff0c\u9632\u6b62\u7b56\u7565\u5d29\u6e83\u4e3a\u5c11\u6570\u6613\u88ab\u5229\u7528\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u4fc3\u8fdb\u63a2\u7d22\u548c\u63d0\u5347\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u7684\u6548\u76ca\u3002", "method": "\u5f15\u5165\u591a\u76ee\u6807\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u91c7\u7528vine\u91c7\u6837\u6536\u96c6\u7b56\u7565\u8f68\u8ff9\uff0c\u5e76\u4fee\u6539\u4f18\u52bf\u51fd\u6570\u4ee5\u53cd\u6620\u65b0\u76ee\u6807\u4e0b\u7684\u4f18\u52bf\u3002", "result": "\u5728BabyAI\u3001Minigrid\u548cAlgorithmic Creativity\u7b49\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u80fd\u89e3\u51b3\u66f4\u591a\u73af\u5883\u914d\u7f6e\uff0c\u5e76\u5728\u5927\u6270\u52a8\u4e0b\u6cdb\u5316\u66f4\u597d\u3002\u5728pass@k\u5b9e\u9a8c\u4e2d\uff0c\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8986\u76d6\u7387\u3002", "conclusion": "\u591a\u76ee\u6807\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u6301\u548c\u5229\u7528\u591a\u6837\u5316\u7684\u7b56\u7565\u5e93\uff0c\u63d0\u5347RLFT\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.25429", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.25429", "abs": "https://arxiv.org/abs/2509.25429", "authors": ["Sreeja Apparaju", "Yichuan Niu", "Xixi Qi"], "title": "Feedback Control for Small Budget Pacing", "comment": null, "summary": "Budget pacing is critical in online advertising to align spend with campaign\ngoals under dynamic auctions. Existing pacing methods often rely on ad-hoc\nparameter tuning, which can be unstable and inefficient. We propose a\nprincipled controller that combines bucketized hysteresis with proportional\nfeedback to provide stable and adaptive spend control. Our method provides a\nframework and analysis for parameter selection that enables accurate tracking\nof desired spend rates across campaigns. Experiments in real-world auctions\ndemonstrate significant improvements in pacing accuracy and delivery\nconsistency, reducing pacing error by 13% and $\\lambda$-volatility by 54%\ncompared to baseline method. By bridging control theory with advertising\nsystems, our approach offers a scalable and reliable solution for budget\npacing, with particular benefits for small-budget campaigns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5206\u6876\u8fdf\u6ede\u548c\u6bd4\u4f8b\u53cd\u9988\u7684\u9884\u7b97\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5728\u7ebf\u5e7f\u544a\u9884\u7b97\u8282\u594f\u63a7\u5236\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u9884\u7b97\u8282\u594f\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u4e34\u65f6\u53c2\u6570\u8c03\u4f18\uff0c\u5b58\u5728\u4e0d\u7a33\u5b9a\u548c\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u63a7\u5236\u65b9\u6848", "method": "\u7ed3\u5408\u5206\u6876\u8fdf\u6ede\u548c\u6bd4\u4f8b\u53cd\u9988\u7684\u63a7\u5236\u673a\u5236\uff0c\u63d0\u4f9b\u53c2\u6570\u9009\u62e9\u7684\u6846\u67b6\u548c\u5206\u6790\u65b9\u6cd5", "result": "\u5728\u771f\u5b9e\u7ade\u4ef7\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8282\u594f\u8bef\u5dee\u964d\u4f4e13%\uff0c\u6ce2\u52a8\u6027\u964d\u4f4e54%", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u63a7\u5236\u7406\u8bba\u4e0e\u5e7f\u544a\u7cfb\u7edf\u7ed3\u5408\uff0c\u4e3a\u9884\u7b97\u8282\u594f\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u6709\u5229\u4e8e\u5c0f\u9884\u7b97\u5e7f\u544a\u6d3b\u52a8"}}
{"id": "2509.25438", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25438", "abs": "https://arxiv.org/abs/2509.25438", "authors": ["Zhibo Hou", "Zhiyu An", "Wan Du"], "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring", "comment": null, "summary": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b66\u4e60\u8fdb\u5ea6\u76d1\u63a7\uff08LPM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u800c\u975e\u9884\u6d4b\u8bef\u5dee\u6216\u65b0\u9896\u6027\u6765\u5f15\u5bfc\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u566a\u58f0\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5185\u5728\u5956\u52b1\u7684\u63a2\u7d22\u65b9\u6cd5\u5728\u9762\u5bf9\u4e0d\u53ef\u5b66\u4e60\u7684\u968f\u673a\u566a\u58f0\u6e90\uff08\u5982noisy-TV\uff09\u65f6\u5bb9\u6613\u88ab\u56f0\uff0c\u800c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6216\u5206\u5e03\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u867d\u7136\u6700\u7ec8\u80fd\u9003\u8131\uff0c\u4f46\u6837\u672c\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u542f\u53d1\uff0c\u4eba\u7c7b\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4f1a\u76d1\u63a7\u81ea\u5df1\u7684\u8fdb\u6b65\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u8bbe\u8ba1\uff1a\u4f7f\u7528\u8bef\u5dee\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u6a21\u578b\u5728\u5148\u524d\u8fed\u4ee3\u4e2d\u7684\u9884\u671f\u9884\u6d4b\u8bef\u5dee\uff0c\u5229\u7528\u5f53\u524d\u8fed\u4ee3\u4e0e\u5148\u524d\u8fed\u4ee3\u6a21\u578b\u8bef\u5dee\u7684\u5dee\u5f02\u6765\u6307\u5bfc\u63a2\u7d22\u3002", "result": "\u5728\u57fa\u4e8eMNIST\u30013D\u8ff7\u5bab\u548cAtari\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0cLPM\u7684\u5185\u5728\u5956\u52b1\u6536\u655b\u66f4\u5feb\uff0c\u5728\u8ff7\u5bab\u5b9e\u9a8c\u4e2d\u63a2\u7d22\u66f4\u591a\u72b6\u6001\uff0c\u5728Atari\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u5916\u5728\u5956\u52b1\u3002", "conclusion": "LPM\u6982\u5ff5\u7b80\u5355\u4f46\u6709\u6548\uff0c\u6807\u5fd7\u7740\u566a\u58f0\u9c81\u68d2\u63a2\u7d22\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u901a\u8fc7\u76d1\u63a7\u5b66\u4e60\u8fdb\u5ea6\u800c\u975e\u9884\u6d4b\u8bef\u5dee\u6765\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u3002"}}
{"id": "2509.25439", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.25439", "abs": "https://arxiv.org/abs/2509.25439", "authors": ["Hanyuan Gao", "Xiaoxuan Yang"], "title": "Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications", "comment": "Accepted by Asilomar 2025", "summary": "Hidden Markov models (HMM) are commonly used in generation tasks and have\ndemonstrated strong capabilities in neuro-symbolic applications for the Markov\nproperty. These applications leverage the strengths of neural networks and\nsymbolic reasoning to create robust and interpretable AI systems. However, they\nmay inherit and amplify the shortcomings of both approaches. Both components\nrequire dense computation and data transfer, and their communication further\nhinders performance. This paper proposes Norm-Q, a normalized linear\nquantization approach for compressing probabilistic symbolic models, such as\nHMMs. We reduce the bit width of the data with minimal impact, thereby\nalleviating memory and bandwidth stress and enabling deployment on potential\ncustom hardware. Our method introduces a normalized quantization-aware\nexpectation maximization process for probabilistic model training. The\nexperimental results show that Norm-Q achieves a higher compression rate with\nreasonable score loss compared to traditional quantization methods. In the case\nof the constrained generation task of large language models, we successfully\nquantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3\nbits with acceptable loss. Notably, the Norm-Q method can achieve a compression\nrate of 99% for the weights of the HMM. The code is open source at\nhttps://github.com/superstarghy/Norm-Q.", "AI": {"tldr": "\u63d0\u51faNorm-Q\u65b9\u6cd5\uff0c\u4e00\u79cd\u7528\u4e8e\u538b\u7f29\u6982\u7387\u7b26\u53f7\u6a21\u578b\uff08\u5982\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff09\u7684\u5f52\u4e00\u5316\u7ebf\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u4f4d\u5bbd\u6765\u7f13\u89e3\u5185\u5b58\u548c\u5e26\u5bbd\u538b\u529b\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7\u5e94\u7528\u4e2d\uff0c\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7b49\u6982\u7387\u7b26\u53f7\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u5bc6\u96c6\u548c\u6570\u636e\u4f20\u8f93\u91cf\u5927\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9a\u5236\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u5f52\u4e00\u5316\u91cf\u5316\u611f\u77e5\u7684\u671f\u671b\u6700\u5927\u5316\u8fc7\u7a0b\u8fdb\u884c\u6982\u7387\u6a21\u578b\u8bad\u7ec3\uff0c\u901a\u8fc7\u964d\u4f4e\u6570\u636e\u4f4d\u5bbd\u5b9e\u73b0\u538b\u7f29\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u5c064096\u9690\u85cf\u72b6\u6001\u7684HMM\u91cf\u5316\u4e3a8\u4f4d\u65e0\u635f\u5931\uff0c\u6700\u591a3\u4f4d\u53ef\u63a5\u53d7\u635f\u5931\uff0c\u6743\u91cd\u538b\u7f29\u7387\u8fbe\u523099%\u3002", "conclusion": "Norm-Q\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u538b\u7f29\u7387\u548c\u5408\u7406\u7684\u5206\u6570\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6982\u7387\u7b26\u53f7\u6a21\u578b\u7684\u5185\u5b58\u548c\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.25449", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25449", "abs": "https://arxiv.org/abs/2509.25449", "authors": ["Sofiane Ennadir", "Siavash Golkar", "Leopoldo Sarra"], "title": "Joint Embeddings Go Temporal", "comment": "Accepted at the Workshop on Time Series in the Age of Large Models -\n  NeurIPS 2024", "summary": "Self-supervised learning has seen great success recently in unsupervised\nrepresentation learning, enabling breakthroughs in natural language and image\nprocessing. However, these methods often rely on autoregressive and masked\nmodeling, which aim to reproduce masked information in the input, which can be\nvulnerable to the presence of noise or confounding variables. To address this\nproblem, Joint-Embedding Predictive Architectures (JEPA) has been introduced\nwith the aim to perform self-supervised learning in the latent space. To\nleverage these advancements in the domain of time series, we introduce Time\nSeries JEPA (TS-JEPA), an architecture specifically adapted for time series\nrepresentation learning. We validate TS-JEPA on both classification and\nforecasting, showing that it can match or surpass current state-of-the-art\nbaselines on different standard datasets. Notably, our approach demonstrates a\nstrong performance balance across diverse tasks, indicating its potential as a\nrobust foundation for learning general representations. Thus, this work lays\nthe groundwork for developing future time series foundation models based on\nJoint Embedding.", "AI": {"tldr": "\u63d0\u51fa\u4e86TS-JEPA\uff0c\u4e00\u79cd\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u8bbe\u8ba1\u7684\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u5728\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u81ea\u56de\u5f52\u548c\u63a9\u7801\u5efa\u6a21\uff09\u5bf9\u566a\u58f0\u548c\u6df7\u6dc6\u53d8\u91cf\u654f\u611f\u7684\u95ee\u9898\uff0c\u5c06\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u3002", "method": "\u57fa\u4e8eJoint-Embedding Predictive Architectures (JEPA) \u8bbe\u8ba1\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u67b6\u6784TS-JEPA\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cTS-JEPA\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u5e73\u8861\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u7684\u672a\u6765\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u5b66\u4e60\u901a\u7528\u8868\u793a\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.25466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25466", "abs": "https://arxiv.org/abs/2509.25466", "authors": ["Haotian Fu", "Ran Gong", "Xiaohan Zhang", "Maria Vittoria Minniti", "Jigarkumar Patel", "Karl Schmeckpeper"], "title": "Data-Efficient Multitask DAgger", "comment": null, "summary": "Generalist robot policies that can perform many tasks typically require\nextensive expert data or simulations for training. In this work, we propose a\nnovel Data-Efficient multitask DAgger framework that distills a single\nmultitask policy from multiple task-specific expert policies. Our approach\nsignificantly increases the overall task success rate by actively focusing on\ntasks where the multitask policy underperforms. The core of our method is a\nperformance-aware scheduling strategy that tracks how much each task's learning\nprocess benefits from the amount of data, using a Kalman filter-based estimator\nto robustly decide how to allocate additional demonstrations across tasks. We\nvalidate our approach on MetaWorld, as well as a suite of diverse\ndrawer-opening tasks in IsaacLab. The resulting policy attains high performance\nacross all tasks while using substantially fewer expert demonstrations, and the\nvisual policy learned with our method in simulation shows better performance\nthan naive DAgger and Behavior Cloning when transferring zero-shot to a real\nrobot without using real data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u591a\u4efb\u52a1DAgger\u6846\u67b6\uff0c\u901a\u8fc7\u6027\u80fd\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u4ece\u591a\u4e2a\u4efb\u52a1\u4e13\u5bb6\u7b56\u7565\u4e2d\u84b8\u998f\u51fa\u5355\u4e00\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u4e13\u5bb6\u6f14\u793a\u9700\u6c42\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u901a\u5e38\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6570\u636e\u6216\u6a21\u62df\u8bad\u7ec3\uff0c\u672c\u5de5\u4f5c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u591a\u4efb\u52a1\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6027\u80fd\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u4f30\u8ba1\u5668\u8ddf\u8e2a\u6bcf\u4e2a\u4efb\u52a1\u4ece\u6570\u636e\u91cf\u4e2d\u83b7\u76ca\u7684\u7a0b\u5ea6\uff0c\u667a\u80fd\u5206\u914d\u989d\u5916\u6f14\u793a\u6570\u636e\u3002", "result": "\u5728MetaWorld\u548cIsaacLab\u62bd\u5c49\u5f00\u542f\u4efb\u52a1\u5957\u4ef6\u4e0a\u9a8c\u8bc1\uff0c\u83b7\u5f97\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e13\u5bb6\u6f14\u793a\u9700\u6c42\uff0c\u4eff\u771f\u5b66\u4e60\u7684\u89c6\u89c9\u7b56\u7565\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u65f6\u8868\u73b0\u4f18\u4e8e\u6734\u7d20DAgger\u548c\u884c\u4e3a\u514b\u9686\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u4e13\u5bb6\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2509.25473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25473", "abs": "https://arxiv.org/abs/2509.25473", "authors": ["Danyang Li", "Yixuan Wang", "Matthew Cleaveland", "Mingyu Cai", "Roberto Tron"], "title": "Conformal Prediction for Signal Temporal Logic Inference", "comment": null, "summary": "Signal Temporal Logic (STL) inference seeks to extract human-interpretable\nrules from time-series data, but existing methods lack formal confidence\nguarantees for the inferred rules. Conformal prediction (CP) is a technique\nthat can provide statistical correctness guarantees, but is typically applied\nas a post-training wrapper without improving model learning. Instead, we\nintroduce an end-to-end differentiable CP framework for STL inference that\nenhances both reliability and interpretability of the resulting formulas. We\nintroduce a robustness-based nonconformity score, embed a smooth CP layer\ndirectly into training, and employ a new loss function that simultaneously\noptimizes inference accuracy and CP prediction sets with a single term.\nFollowing training, an exact CP procedure delivers statistical guarantees for\nthe learned STL formulas. Experiments on benchmark time-series tasks show that\nour approach reduces uncertainty in predictions (i.e., it achieves high\ncoverage while reducing prediction set size), and improves accuracy (i.e., the\nnumber of misclassifications when using a fixed threshold) over\nstate-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u5fae\u7684\u5171\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u63a8\u7406\uff0c\u540c\u65f6\u63d0\u9ad8\u516c\u5f0f\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709STL\u63a8\u7406\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u63a8\u65ad\u89c4\u5219\u7684\u6b63\u5f0f\u7f6e\u4fe1\u4fdd\u8bc1\uff0c\u800c\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u901a\u5e38\u4f5c\u4e3a\u540e\u8bad\u7ec3\u5305\u88c5\u5668\u4f7f\u7528\uff0c\u65e0\u6cd5\u6539\u8fdb\u6a21\u578b\u5b66\u4e60\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u9c81\u68d2\u6027\u7684\u975e\u5171\u5f62\u5ea6\u5206\u6570\uff0c\u5c06\u5e73\u6ed1CP\u5c42\u76f4\u63a5\u5d4c\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4f7f\u7528\u65b0\u7684\u635f\u5931\u51fd\u6570\u540c\u65f6\u4f18\u5316\u63a8\u7406\u51c6\u786e\u6027\u548cCP\u9884\u6d4b\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff08\u9ad8\u8986\u76d6\u7387\u540c\u65f6\u51cf\u5c0f\u9884\u6d4b\u96c6\u5927\u5c0f\uff09\uff0c\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff08\u4f7f\u7528\u56fa\u5b9a\u9608\u503c\u65f6\u7684\u8bef\u5206\u7c7b\u6570\u91cf\u51cf\u5c11\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b66\u4e60\u5230\u7684STL\u516c\u5f0f\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5728\u57fa\u51c6\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.25480", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25480", "abs": "https://arxiv.org/abs/2509.25480", "authors": ["Hui Ji", "Wei Gao", "Pengfei Zhou"], "title": "Translation from Wearable PPG to 12-Lead ECG", "comment": "14 pages,10 figures", "summary": "The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular\nmonitoring, offering superior diagnostic granularity and specificity compared\nto photoplethysmography (PPG). However, existing 12-lead ECG systems rely on\ncumbersome multi-electrode setups, limiting sustained monitoring in ambulatory\nsettings, while current PPG-based methods fail to reconstruct multi-lead ECG\ndue to the absence of inter-lead constraints and insufficient modeling of\nspatial-temporal dependencies across leads. To bridge this gap, we introduce\nP2Es, an innovative demographic-aware diffusion framework designed to generate\nclinically valid 12-lead ECG from PPG signals via three key innovations.\nSpecifically, in the forward process, we introduce frequency-domain blurring\nfollowed by temporal noise interference to simulate real-world signal\ndistortions. In the reverse process, we design a temporal multi-scale\ngeneration module followed by frequency deblurring. In particular, we leverage\nKNN-based clustering combined with contrastive learning to assign affinity\nmatrices for the reverse process, enabling demographic-specific ECG\ntranslation. Extensive experimental results show that P2Es outperforms baseline\nmodels in 12-lead ECG reconstruction.", "AI": {"tldr": "P2Es\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u521b\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u4ecePPG\u4fe1\u53f7\u751f\u6210\u4e34\u5e8a\u6709\u6548\u768412\u5bfc\u8054\u5fc3\u7535\u56fe\uff0c\u901a\u8fc7\u9891\u57df\u6a21\u7cca\u3001\u65f6\u95f4\u566a\u58f0\u5e72\u6270\u548c\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u611f\u77e5\u7b49\u6280\u672f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u670912\u5bfc\u8054ECG\u7cfb\u7edf\u4f9d\u8d56\u7b28\u91cd\u7684\u591a\u7535\u6781\u8bbe\u7f6e\uff0c\u9650\u5236\u4e86\u79fb\u52a8\u73af\u5883\u4e0b\u7684\u6301\u7eed\u76d1\u6d4b\uff0c\u800c\u5f53\u524dPPG\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u5bfc\u8054\u95f4\u7ea6\u675f\u548c\u65f6\u7a7a\u4f9d\u8d56\u5efa\u6a21\u4e0d\u8db3\uff0c\u65e0\u6cd5\u91cd\u5efa\u591a\u5bfc\u8054ECG\u3002", "method": "\u63d0\u51faP2Es\u4eba\u53e3\u7edf\u8ba1\u611f\u77e5\u6269\u6563\u6846\u67b6\uff0c\u5305\u542b\u524d\u5411\u8fc7\u7a0b\u7684\u9891\u57df\u6a21\u7cca\u548c\u65f6\u95f4\u566a\u58f0\u5e72\u6270\uff0c\u53cd\u5411\u8fc7\u7a0b\u7684\u65f6\u95f4\u591a\u5c3a\u5ea6\u751f\u6210\u548c\u9891\u57df\u53bb\u6a21\u7cca\uff0c\u4ee5\u53ca\u57fa\u4e8eKNN\u805a\u7c7b\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u4eba\u53e3\u7edf\u8ba1\u7279\u5f02\u6027ECG\u8f6c\u6362\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cP2Es\u572812\u5bfc\u8054ECG\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "P2Es\u6846\u67b6\u6210\u529f\u5730\u4ecePPG\u4fe1\u53f7\u751f\u6210\u4e86\u4e34\u5e8a\u6709\u6548\u768412\u5bfc\u8054ECG\uff0c\u4e3a\u79fb\u52a8\u5fc3\u8840\u7ba1\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25487", "categories": ["cs.LG", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25487", "abs": "https://arxiv.org/abs/2509.25487", "authors": ["Dingyi Kang", "Dongming Jiang", "Hanshen Yang", "Hang Liu", "Bingzhe Li"], "title": "Scalable Disk-Based Approximate Nearest Neighbor Search with Page-Aligned Graph", "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS), as the core of vector databases\n(VectorDBs), has become widely used in modern AI and ML systems, powering\napplications from information retrieval to bio-informatics. While graph-based\nANNS methods achieve high query efficiency, their scalability is constrained by\nthe available host memory. Recent disk-based ANNS approaches mitigate memory\nusage by offloading data to Solid-State Drives (SSDs). However, they still\nsuffer from issues such as long I/O traversal path, misalignment with storage\nI/O granularity, and high in-memory indexing overhead, leading to significant\nI/O latency and ultimately limiting scalability for large-scale vector search.\n  In this paper, we propose PageANN, a disk-based approximate nearest neighbor\nsearch (ANNS) framework designed for high performance and scalability. PageANN\nintroduces a page-node graph structure that aligns logical graph nodes with\nphysical SSD pages, thereby shortening I/O traversal paths and reducing I/O\noperations. Specifically, similar vectors are clustered into page nodes, and a\nco-designed disk data layout leverages this structure with a merging technique\nto store only representative vectors and topology information, avoiding\nunnecessary reads. To further improve efficiency, we design a memory management\nstrategy that combines lightweight indexing with coordinated memory-disk data\nallocation, maximizing host memory utilization while minimizing query latency\nand storage overhead. Experimental results show that PageANN significantly\noutperforms state-of-the-art (SOTA) disk-based ANNS methods, achieving\n1.85x-10.83x higher throughput and 51.7%-91.9% lower latency across different\ndatasets and memory budgets, while maintaining comparable high recall accuracy.", "AI": {"tldr": "PageANN\u662f\u4e00\u4e2a\u57fa\u4e8e\u78c1\u76d8\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u9875\u9762\u8282\u70b9\u56fe\u7ed3\u6784\u5c06\u903b\u8f91\u56fe\u8282\u70b9\u4e0e\u7269\u7406SSD\u9875\u9762\u5bf9\u9f50\uff0c\u7f29\u77edI/O\u904d\u5386\u8def\u5f84\u5e76\u51cf\u5c11I/O\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u5411\u91cf\u641c\u7d22\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u78c1\u76d8\u7684ANNS\u65b9\u6cd5\u5b58\u5728I/O\u904d\u5386\u8def\u5f84\u957f\u3001\u4e0e\u5b58\u50a8I/O\u7c92\u5ea6\u4e0d\u5339\u914d\u3001\u5185\u5b58\u7d22\u5f15\u5f00\u9500\u9ad8\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u663e\u8457\u7684I/O\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u5411\u91cf\u641c\u7d22\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u9875\u9762\u8282\u70b9\u56fe\u7ed3\u6784\uff0c\u5c06\u76f8\u4f3c\u5411\u91cf\u805a\u7c7b\u5230\u9875\u9762\u8282\u70b9\u4e2d\uff1b\u8bbe\u8ba1\u534f\u540c\u4f18\u5316\u7684\u78c1\u76d8\u6570\u636e\u5e03\u5c40\uff0c\u4ec5\u5b58\u50a8\u4ee3\u8868\u6027\u5411\u91cf\u548c\u62d3\u6251\u4fe1\u606f\uff1b\u91c7\u7528\u8f7b\u91cf\u7ea7\u7d22\u5f15\u4e0e\u534f\u8c03\u7684\u5185\u5b58-\u78c1\u76d8\u6570\u636e\u5206\u914d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aPageANN\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u78c1\u76d8ANNS\u65b9\u6cd5\uff0c\u541e\u5410\u91cf\u63d0\u9ad81.85-10.83\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e51.7%-91.9%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u3002", "conclusion": "PageANN\u901a\u8fc7\u9875\u9762\u8282\u70b9\u56fe\u7ed3\u6784\u548c\u4f18\u5316\u7684\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u78c1\u76d8ANNS\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u5411\u91cf\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25509", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.25509", "abs": "https://arxiv.org/abs/2509.25509", "authors": ["Langzhou He", "Junyou Zhu", "Fangxin Wang", "Junhua Liu", "Haoyan Xu", "Yue Zhao", "Philip S. Yu", "Qitian Wu"], "title": "Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization", "comment": null, "summary": "Molecular foundation models are rapidly advancing scientific discovery, but\ntheir unreliability on out-of-distribution (OOD) samples severely limits their\napplication in high-stakes domains such as drug discovery and protein design. A\ncritical failure mode is chemical hallucination, where models make\nhigh-confidence yet entirely incorrect predictions for unknown molecules. To\naddress this challenge, we introduce Molecular Preference-Aligned Instance\nRanking (Mole-PAIR), a simple, plug-and-play module that can be flexibly\nintegrated with existing foundation models to improve their reliability on OOD\ndata through cost-effective post-training. Specifically, our method formulates\nthe OOD detection problem as a preference optimization over the estimated OOD\naffinity between in-distribution (ID) and OOD samples, achieving this goal\nthrough a pairwise learning objective. We show that this objective essentially\noptimizes AUROC, which measures how consistently ID and OOD samples are ranked\nby the model. Extensive experiments across five real-world molecular datasets\ndemonstrate that our approach significantly improves the OOD detection\ncapabilities of existing molecular foundation models, achieving up to 45.8%,\n43.9%, and 24.3% improvements in AUROC under distribution shifts of size,\nscaffold, and assay, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mole-PAIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u63d0\u5347\u5206\u5b50\u57fa\u7840\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u53ef\u9760\u6027\uff0c\u663e\u8457\u6539\u5584OOD\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5206\u5b50\u57fa\u7840\u6a21\u578b\u5728\u5206\u5e03\u5916\u6837\u672c\u4e0a\u4e0d\u53ef\u9760\uff0c\u5b58\u5728\u5316\u5b66\u5e7b\u89c9\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u836f\u7269\u53d1\u73b0\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u5c06OOD\u68c0\u6d4b\u95ee\u9898\u5efa\u6a21\u4e3a\u5bf9ID\u548cOOD\u6837\u672c\u4e4b\u95f4OOD\u4eb2\u548c\u5ea6\u7684\u504f\u597d\u4f18\u5316\uff0c\u901a\u8fc7\u6210\u5bf9\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\uff0c\u672c\u8d28\u4e0a\u4f18\u5316AUROC\u6307\u6807\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u5206\u5b50\u57fa\u7840\u6a21\u578b\u7684OOD\u68c0\u6d4b\u80fd\u529b\uff0c\u5728\u5927\u5c0f\u3001\u652f\u67b6\u548c\u6d4b\u5b9a\u5206\u5e03\u504f\u79fb\u4e0bAUROC\u5206\u522b\u63d0\u534745.8%\u300143.9%\u548c24.3%\u3002", "conclusion": "Mole-PAIR\u662f\u4e00\u4e2a\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u53ef\u7075\u6d3b\u96c6\u6210\u5230\u73b0\u6709\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u7684\u540e\u8bad\u7ec3\u63d0\u9ad8\u5176\u5728OOD\u6570\u636e\u4e0a\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.25518", "categories": ["cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.25518", "abs": "https://arxiv.org/abs/2509.25518", "authors": ["Harry Robertshaw", "Han-Ru Wu", "Alejandro Granados", "Thomas C Booth"], "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy", "comment": "Published in Medical Image Computing and Computer Assisted\n  Intervention - MICCAI 2025, Lecture Notes in Computer Science, vol 15968", "summary": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical\nchallenge due to the complexity of vascular anatomy and the need for precise,\nreal-time decision-making. Reinforcement learning (RL)-based approaches have\ndemonstrated potential in automating endovascular navigation, but current\nmethods often struggle with generalization across multiple patient vasculatures\nand long-horizon tasks. We propose a world model for autonomous endovascular\nnavigation using TD-MPC2, a model-based RL algorithm. We trained a single RL\nagent across multiple endovascular navigation tasks in ten real patient\nvasculatures, comparing performance against the state-of-the-art Soft\nActor-Critic (SAC) method. Results indicate that TD-MPC2 significantly\noutperforms SAC in multi-task learning, achieving a 65% mean success rate\ncompared to SAC's 37%, with notable improvements in path ratio. TD-MPC2\nexhibited increased procedure times, suggesting a trade-off between success\nrate and execution speed. These findings highlight the potential of world\nmodels for improving autonomous endovascular navigation and lay the foundation\nfor future research in generalizable AI-driven robotic interventions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTD-MPC2\u4e16\u754c\u6a21\u578b\u7684\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u65b9\u6cd5\uff0c\u5728\u591a\u60a3\u8005\u8840\u7ba1\u89e3\u5256\u4e2d\u663e\u8457\u4f18\u4e8eSAC\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u8fbe65% vs 37%\uff0c\u4f46\u6267\u884c\u65f6\u95f4\u8f83\u957f", "motivation": "\u673a\u68b0\u53d6\u6813\u672f\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u8840\u7ba1\u89e3\u5256\u590d\u6742\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u7684\u6311\u6218\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u60a3\u8005\u8840\u7ba1\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3", "method": "\u4f7f\u7528TD-MPC2\u6a21\u578b\u57fa\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u572810\u4e2a\u771f\u5b9e\u60a3\u8005\u8840\u7ba1\u4e2d\u8bad\u7ec3\u5355\u4e00RL\u667a\u80fd\u4f53\u6267\u884c\u591a\u4efb\u52a1\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u5e76\u4e0eSAC\u65b9\u6cd5\u5bf9\u6bd4", "result": "TD-MPC2\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u663e\u8457\u4f18\u4e8eSAC\uff0c\u5e73\u5747\u6210\u529f\u738765% vs 37%\uff0c\u8def\u5f84\u6bd4\u6709\u660e\u663e\u6539\u5584\uff0c\u4f46\u624b\u672f\u65f6\u95f4\u589e\u52a0", "conclusion": "\u4e16\u754c\u6a21\u578b\u5728\u6539\u5584\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u901a\u7528AI\u9a71\u52a8\u673a\u5668\u4eba\u5e72\u9884\u7814\u7a76\u5960\u5b9a\u57fa\u7840"}}
{"id": "2509.25519", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25519", "abs": "https://arxiv.org/abs/2509.25519", "authors": ["Alireza Mousavi-Hosseini", "Stephen Y. Zhang", "Michal Klein", "Marco Cuturi"], "title": "Flow Matching with Semidiscrete Couplings", "comment": "35 pages, 16 figures", "summary": "Flow models parameterized as time-dependent velocity fields can generate data\nfrom noise by integrating an ODE. These models are often trained using flow\nmatching, i.e. by sampling random pairs of noise and target points\n$(\\mathbf{x}_0,\\mathbf{x}_1)$ and ensuring that the velocity field is aligned,\non average, with $\\mathbf{x}_1-\\mathbf{x}_0$ when evaluated along a segment\nlinking $\\mathbf{x}_0$ to $\\mathbf{x}_1$. While these pairs are sampled\nindependently by default, they can also be selected more carefully by matching\nbatches of $n$ noise to $n$ target points using an optimal transport (OT)\nsolver. Although promising in theory, the OT flow matching (OT-FM) approach is\nnot widely used in practice. Zhang et al. (2025) pointed out recently that\nOT-FM truly starts paying off when the batch size $n$ grows significantly,\nwhich only a multi-GPU implementation of the Sinkhorn algorithm can handle.\nUnfortunately, the costs of running Sinkhorn can quickly balloon, requiring\n$O(n^2/\\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity\nfield, where $\\varepsilon$ is a regularization parameter that should be\ntypically small to yield better results. To fulfill the theoretical promises of\nOT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete\nformulation that leverages the fact that the target dataset distribution is\nusually of finite size $N$. The SD-OT problem is solved by estimating a dual\npotential vector using SGD; using that vector, freshly sampled noise vectors at\ntrain time can then be matched with data points at the cost of a maximum inner\nproduct search (MIPS). Semidiscrete FM (SD-FM) removes the quadratic dependency\non $n/\\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all\ntraining metrics and inference budget constraints, across multiple datasets, on\nunconditional/conditional generation, or when using mean-flow models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u79bb\u6563\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u6570\u636e\u96c6\u89c6\u4e3a\u6709\u9650\u5206\u5e03\uff0c\u4f7f\u7528SGD\u4f30\u8ba1\u5bf9\u5076\u52bf\u5411\u91cf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6700\u4f18\u4f20\u8f93\u6d41\u5339\u914d\u4e2dSinkhorn\u7b97\u6cd5\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u6700\u4f18\u4f20\u8f93\u6d41\u5339\u914d\u65b9\u6cd5\u867d\u7136\u7406\u8bba\u4e0a\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u7531\u4e8eSinkhorn\u7b97\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff08O(n\u00b2/\u03b5\u00b2)\uff09\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5927\u6279\u91cf\u65f6\u3002", "method": "\u91c7\u7528\u534a\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\uff0c\u5c06\u76ee\u6807\u6570\u636e\u96c6\u89c6\u4e3a\u6709\u9650\u5206\u5e03\uff0c\u901a\u8fc7SGD\u4f30\u8ba1\u5bf9\u5076\u52bf\u5411\u91cf\uff0c\u7136\u540e\u4f7f\u7528\u6700\u5927\u5185\u79ef\u641c\u7d22\u5c06\u65b0\u91c7\u6837\u7684\u566a\u58f0\u5411\u91cf\u4e0e\u6570\u636e\u70b9\u5339\u914d\u3002", "result": "SD-FM\u5728\u6240\u6709\u8bad\u7ec3\u6307\u6807\u548c\u63a8\u7406\u9884\u7b97\u7ea6\u675f\u4e0b\u90fd\u4f18\u4e8eFM\u548cOT-FM\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u65e0\u6761\u4ef6/\u6761\u4ef6\u751f\u6210\u4ee5\u53ca\u4f7f\u7528\u5747\u503c\u6d41\u6a21\u578b\u65f6\u90fd\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u534a\u79bb\u6563\u6d41\u5339\u914d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6700\u4f18\u4f20\u8f93\u6d41\u5339\u914d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u751f\u6210\u6027\u80fd\uff0c\u4e3a\u6d41\u5339\u914d\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25535", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25535", "abs": "https://arxiv.org/abs/2509.25535", "authors": ["Yichi Zhang", "Fangzheng Xie", "Shu Yang", "Chong Wu"], "title": "Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing", "comment": null, "summary": "In language tasks that require extensive human--model interaction, deploying\na single \"best\" model for every query can be expensive. To reduce inference\ncost while preserving the quality of the responses, a large language model\n(LLM) router selects the most appropriate model from a pool of candidates for\neach query. A central challenge to training a high-quality router is the\nscarcity of reliable supervision. Gold-standard data (e.g., expert-verified\nlabels or rubric-based scores) provide accurate quality evaluations of LLM\nresponses but are costly and difficult to scale. In contrast, preference-based\ndata, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and\nmore scalable, yet often biased in reflecting the true quality of responses. We\ncast the problem of LLM router training with combined gold-standard and\npreference-based data into a causal inference framework by viewing the response\nevaluation mechanism as the treatment assignment. This perspective further\nreveals that the bias in preference-based data corresponds to the well-known\ncausal estimand: the conditional average treatment effect. Based on this new\nperspective, we develop an integrative causal router training framework that\ncorrects preference-data bias, address imbalances between two data sources, and\nimprove routing robustness and efficiency. Numerical experiments demonstrate\nthat our approach delivers more accurate routing and improves the trade-off\nbetween cost and quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u7684LLM\u8def\u7531\u5668\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9ec4\u91d1\u6807\u51c6\u548c\u504f\u597d\u6570\u636e\u6765\u7ea0\u6b63\u504f\u597d\u6570\u636e\u504f\u5dee\uff0c\u63d0\u9ad8\u8def\u7531\u51c6\u786e\u6027\u548c\u6210\u672c-\u8d28\u91cf\u6743\u8861\u3002", "motivation": "\u5728\u9700\u8981\u5927\u91cf\u4eba\u673a\u4ea4\u4e92\u7684\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u90e8\u7f72\u5355\u4e00\"\u6700\u4f73\"\u6a21\u578b\u6210\u672c\u9ad8\u6602\u3002\u8bad\u7ec3\u9ad8\u8d28\u91cf\u8def\u7531\u5668\u7684\u6838\u5fc3\u6311\u6218\u662f\u53ef\u9760\u76d1\u7763\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002", "method": "\u5c06LLM\u8def\u7531\u5668\u8bad\u7ec3\u95ee\u9898\u8f6c\u5316\u4e3a\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u5c06\u54cd\u5e94\u8bc4\u4f30\u673a\u5236\u89c6\u4e3a\u6cbb\u7597\u5206\u914d\uff0c\u5f00\u53d1\u4e86\u6574\u5408\u56e0\u679c\u8def\u7531\u5668\u8bad\u7ec3\u6846\u67b6\u6765\u7ea0\u6b63\u504f\u597d\u6570\u636e\u504f\u5dee\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8def\u7531\uff0c\u5e76\u6539\u5584\u4e86\u6210\u672c\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u7684\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u504f\u597d\u6570\u636e\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8LLM\u8def\u7531\u5668\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.25538", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25538", "abs": "https://arxiv.org/abs/2509.25538", "authors": ["Marcus Schwarting", "Logan Ward", "Nathaniel Hudson", "Xiaoli Yan", "Ben Blaiszik", "Santanu Chaudhuri", "Eliu Huerta", "Ian Foster"], "title": "Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization", "comment": null, "summary": "Generative AI poses both opportunities and risks for solving inverse design\nproblems in the sciences. Generative tools provide the ability to expand and\nrefine a search space autonomously, but do so at the cost of exploring\nlow-quality regions until sufficiently fine tuned. Here, we propose a queue\nprioritization algorithm that combines generative modeling and active learning\nin the context of a distributed workflow for exploring complex design spaces.\nWe find that incorporating an active learning model to prioritize top design\ncandidates can prevent a generative AI workflow from expending resources on\nnonsensical candidates and halt potential generative model decay. For an\nexisting generative AI workflow for discovering novel molecular structure\ncandidates for carbon capture, our active learning approach significantly\nincreases the number of high-quality candidates identified by the generative\nmodel. We find that, out of 1000 novel candidates, our workflow without active\nlearning can generate an average of 281 high-performing candidates, while our\nproposed prioritization with active learning can generate an average 604\nhigh-performing candidates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5efa\u6a21\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u961f\u5217\u4f18\u5148\u7ea7\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u4e2d\u7684\u9006\u5411\u8bbe\u8ba1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u8d28\u91cf\u5019\u9009\u5206\u5b50\u7684\u751f\u6210\u6570\u91cf\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u89e3\u51b3\u79d1\u5b66\u9006\u5411\u8bbe\u8ba1\u95ee\u9898\u65f6\u65e2\u5e26\u6765\u673a\u9047\u4e5f\u5e26\u6765\u98ce\u9669\uff0c\u80fd\u591f\u81ea\u4e3b\u6269\u5c55\u548c\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u63a2\u7d22\u4f4e\u8d28\u91cf\u533a\u57df\u76f4\u5230\u5145\u5206\u5fae\u8c03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u961f\u5217\u4f18\u5148\u7ea7\u7b97\u6cd5\uff0c\u5c06\u751f\u6210\u5efa\u6a21\u548c\u4e3b\u52a8\u5b66\u4e60\u7ed3\u5408\u5728\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\u4e2d\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u6a21\u578b\u4f18\u5148\u5904\u7406\u9876\u7ea7\u8bbe\u8ba1\u5019\u9009\uff0c\u9632\u6b62\u8d44\u6e90\u6d6a\u8d39\u5728\u65e0\u610f\u4e49\u5019\u9009\u4e0a\u3002", "result": "\u5728\u78b3\u6355\u83b7\u5206\u5b50\u7ed3\u6784\u53d1\u73b0\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u7684\u65b9\u6cd5\u5c06\u9ad8\u8d28\u91cf\u5019\u9009\u5206\u5b50\u6570\u91cf\u4ece\u5e73\u5747281\u4e2a\u63d0\u5347\u5230604\u4e2a\uff08\u4ece1000\u4e2a\u5019\u9009\u5206\u5b50\u4e2d\uff09\u3002", "conclusion": "\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u7684\u751f\u6210AI\u5de5\u4f5c\u6d41\u80fd\u6709\u6548\u9632\u6b62\u751f\u6210\u6a21\u578b\u9000\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u9ad8\u8d28\u91cf\u8bbe\u8ba1\u5019\u9009\u7684\u8bc6\u522b\u6548\u7387\u3002"}}
{"id": "2509.25560", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25560", "abs": "https://arxiv.org/abs/2509.25560", "authors": ["Guojun Tang", "Jiayu Zhou", "Mohammad Mamun", "Steve Drew"], "title": "Lightweight and Robust Federated Data Valuation", "comment": null, "summary": "Federated learning (FL) faces persistent robustness challenges due to non-IID\ndata distributions and adversarial client behavior. A promising mitigation\nstrategy is contribution evaluation, which enables adaptive aggregation by\nquantifying each client's utility to the global model. However,\nstate-of-the-art Shapley-value-based approaches incur high computational\noverhead due to repeated model reweighting and inference, which limits their\nscalability. We propose FedIF, a novel FL aggregation framework that leverages\ntrajectory-based influence estimation to efficiently compute client\ncontributions. FedIF adapts decentralized FL by introducing normalized and\nsmoothed influence scores computed from lightweight gradient operations on\nclient updates and a public validation set. Theoretical analysis demonstrates\nthat FedIF yields a tighter bound on one-step global loss change under noisy\nconditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF\nachieves robustness comparable to or exceeding SV-based methods in the presence\nof label noise, gradient noise, and adversarial samples, while reducing\naggregation overhead by up to 450x. Ablation studies confirm the effectiveness\nof FedIF's design choices, including local weight normalization and influence\nsmoothing. Our results establish FedIF as a practical, theoretically grounded,\nand scalable alternative to Shapley-value-based approaches for efficient and\nrobust FL in real-world deployments.", "AI": {"tldr": "FedIF\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u805a\u5408\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u8f68\u8ff9\u7684\u5f71\u54cd\u529b\u4f30\u8ba1\u6765\u9ad8\u6548\u8ba1\u7b97\u5ba2\u6237\u7aef\u8d21\u732e\uff0c\u5728\u4fdd\u6301\u4e0eShapley\u503c\u65b9\u6cd5\u76f8\u5f53\u7684\u9c81\u68d2\u6027\u540c\u65f6\uff0c\u5c06\u805a\u5408\u5f00\u9500\u964d\u4f4e\u9ad8\u8fbe450\u500d\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5206\u5e03\u548c\u6076\u610f\u5ba2\u6237\u7aef\u884c\u4e3a\u7684\u9c81\u68d2\u6027\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8eShapley\u503c\u7684\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "FedIF\u901a\u8fc7\u8f7b\u91cf\u7ea7\u68af\u5ea6\u64cd\u4f5c\u5728\u5ba2\u6237\u7aef\u66f4\u65b0\u548c\u516c\u5171\u9a8c\u8bc1\u96c6\u4e0a\u8ba1\u7b97\u5f52\u4e00\u5316\u548c\u5e73\u6ed1\u7684\u5f71\u54cd\u529b\u5206\u6570\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u805a\u5408\u3002", "result": "\u5728CIFAR-10\u548cFashion-MNIST\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedIF\u5728\u6807\u7b7e\u566a\u58f0\u3001\u68af\u5ea6\u566a\u58f0\u548c\u5bf9\u6297\u6837\u672c\u5b58\u5728\u65f6\uff0c\u9c81\u68d2\u6027\u8fbe\u5230\u6216\u8d85\u8fc7\u57fa\u4e8eShapley\u503c\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u805a\u5408\u5f00\u9500\u964d\u4f4e450\u500d\u3002", "conclusion": "FedIF\u4e3a\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u7406\u8bba\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8054\u90a6\u5b66\u4e60\u9c81\u68d2\u805a\u5408\u65b9\u6848\uff0c\u662fShapley\u503c\u65b9\u6cd5\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.25582", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25582", "abs": "https://arxiv.org/abs/2509.25582", "authors": ["Amir Moeini", "Minjae Kwon", "Alper Kamil Bozkurt", "Yuichi Motai", "Rohan Chandra", "Lu Feng", "Shangtong Zhang"], "title": "Safe In-Context Reinforcement Learning", "comment": null, "summary": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the\nagent, after some pretraining procedure, is able to adapt to\nout-of-distribution test tasks without any parameter updates. The agent\nachieves this by continually expanding the input (i.e., the context) to its\npolicy neural networks. For example, the input could be all the history\nexperience that the agent has access to until the current time step. The\nagent's performance improves as the input grows, without any parameter updates.\nIn this work, we propose the first method that promotes the safety of ICRL's\nadaptation process in the framework of constrained Markov Decision Processes.\nIn other words, during the parameter-update-free adaptation process, the agent\nnot only maximizes the reward but also minimizes an additional cost function.\nWe also demonstrate that our agent actively reacts to the threshold (i.e.,\nbudget) of the cost tolerance. With a higher cost budget, the agent behaves\nmore aggressively, and with a lower cost budget, the agent behaves more\nconservatively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u4e2d\u4fc3\u8fdb\u5b89\u5168\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\uff0c\u5728\u65e0\u53c2\u6570\u66f4\u65b0\u7684\u9002\u5e94\u8fc7\u7a0b\u4e2d\u540c\u65f6\u6700\u5927\u5316\u5956\u52b1\u548c\u6700\u5c0f\u5316\u989d\u5916\u6210\u672c\u51fd\u6570\u3002", "motivation": "ICRL\u80fd\u591f\u5728\u65e0\u53c2\u6570\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u5206\u5e03\u5916\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9002\u5e94\u8fc7\u7a0b\u5b89\u5168\u6027\u7684\u8003\u8651\uff0c\u9700\u8981\u5728\u6700\u5927\u5316\u5956\u52b1\u7684\u540c\u65f6\u63a7\u5236\u6210\u672c\u3002", "method": "\u5728ICRL\u6846\u67b6\u4e2d\u5f15\u5165\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u667a\u80fd\u4f53\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u4e0d\u4ec5\u8003\u8651\u5956\u52b1\u6700\u5927\u5316\uff0c\u8fd8\u901a\u8fc7\u989d\u5916\u6210\u672c\u51fd\u6570\u63a7\u5236\u884c\u4e3a\u5b89\u5168\u6027\uff0c\u5e76\u80fd\u6839\u636e\u6210\u672c\u5bb9\u5fcd\u5ea6\u9608\u503c\u4e3b\u52a8\u8c03\u6574\u884c\u4e3a\u7b56\u7565\u3002", "result": "\u667a\u80fd\u4f53\u80fd\u591f\u6839\u636e\u6210\u672c\u9884\u7b97\u4e3b\u52a8\u8c03\u6574\u884c\u4e3a\u7b56\u7565\uff1a\u6210\u672c\u9884\u7b97\u8f83\u9ad8\u65f6\u884c\u4e3a\u66f4\u6fc0\u8fdb\uff0c\u6210\u672c\u9884\u7b97\u8f83\u4f4e\u65f6\u884c\u4e3a\u66f4\u4fdd\u5b88\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u9002\u5e94\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u5b89\u5168\u6027\u7ea6\u675f\u5f15\u5165ICRL\u7684\u9002\u5e94\u8fc7\u7a0b\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u65e0\u53c2\u6570\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u5b89\u5168\u5730\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u6210\u672c\u5bb9\u5fcd\u5ea6\u7075\u6d3b\u8c03\u6574\u884c\u4e3a\u7b56\u7565\u3002"}}
{"id": "2509.25592", "categories": ["cs.LG", "68T05, 68T20, 90C26, 90C30, 93E35, 62L20", "I.2.6; I.2.8; G.1.6"], "pdf": "https://arxiv.org/pdf/2509.25592", "abs": "https://arxiv.org/abs/2509.25592", "authors": ["Morteza Kimiaei", "Vyacheslav Kungurtsev"], "title": "Machine Learning Algorithms for Improving Black Box Optimization Solvers", "comment": "74 pages", "summary": "Black-box optimization (BBO) addresses problems where objectives are\naccessible only through costly queries without gradients or explicit structure.\nClassical derivative-free methods -- line search, direct search, and\nmodel-based solvers such as Bayesian optimization -- form the backbone of BBO,\nyet often struggle in high-dimensional, noisy, or mixed-integer settings.\n  Recent advances use machine learning (ML) and reinforcement learning (RL) to\nenhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning\nportfolios, and generative models, while RL enables dynamic operator\nconfiguration, robustness, and meta-optimization across tasks.\n  This paper surveys these developments, covering representative algorithms\nsuch as NNs with the modular model-based optimization framework (mlrMBO),\nzeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO),\ndistributed block-wise optimization (DiBB), partition-based Bayesian\noptimization (SPBOpt), the transformer-based optimizer (B2Opt),\ndiffusion-model-based BBO, surrogate-assisted RL for differential evolution\n(Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with\nrelative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD),\npolicy improvement with black-box (PIBB), and offline Q-learning with Mamba\nbackbones (Q-Mamba).\n  We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and\nthe MetaBox framework. Overall, we highlight how ML and RL transform classical\ninexact solvers into more scalable, robust, and adaptive frameworks for\nreal-world optimization.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u589e\u5f3a\u9ed1\u76d2\u4f18\u5316\uff0c\u5c06\u4f20\u7edf\u65b9\u6cd5\u8f6c\u5316\u4e3a\u66f4\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u65e0\u5bfc\u6570\u65b9\u6cd5\u5728\u9ad8\u7ef4\u3001\u566a\u58f0\u6216\u6df7\u5408\u6574\u6570\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u4f18\u5316\u6280\u672f\u6765\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4ee3\u7406\u6a21\u578b\u3001\u81ea\u9002\u5e94\u66f4\u65b0\u3001\u5143\u5b66\u4e60\u7ec4\u5408\u548c\u751f\u6210\u6a21\u578b\uff0c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u52a8\u6001\u7b97\u5b50\u914d\u7f6e\u3001\u9c81\u68d2\u6027\u548c\u8de8\u4efb\u52a1\u5143\u4f18\u5316\u3002", "result": "\u5f00\u53d1\u4e86\u591a\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\uff0c\u5982mlrMBO\u3001ZO-AdaMM\u3001ABBO\u3001DiBB\u3001SPBOpt\u3001B2Opt\u7b49\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ed1\u76d2\u4f18\u5316\u7684\u6027\u80fd\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6b63\u5728\u5c06\u7ecf\u5178\u7684\u4e0d\u7cbe\u786e\u6c42\u89e3\u5668\u8f6c\u53d8\u4e3a\u66f4\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u73b0\u5b9e\u4e16\u754c\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2509.25596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25596", "abs": "https://arxiv.org/abs/2509.25596", "authors": ["Lucia Quirke", "Stepan Shabalin", "Nora Belrose"], "title": "Binary Sparse Coding for Interpretability", "comment": null, "summary": "Sparse autoencoders (SAEs) are used to decompose neural network activations\ninto sparsely activating features, but many SAE features are only interpretable\nat high activation strengths. To address this issue we propose to use binary\nsparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all\nactivations to be zero or one. We find that binarisation significantly improves\nthe interpretability and monosemanticity of the discovered features, while\nincreasing reconstruction error. By eliminating the distinction between high\nand low activation strengths, we prevent uninterpretable information from being\nsmuggled in through the continuous variation in feature activations. However,\nwe also find that binarisation increases the number of uninterpretable\nultra-high frequency features, and when interpretability scores are\nfrequency-adjusted, the scores for continuous sparse coders are slightly better\nthan those of binary ones. This suggests that polysemanticity may be an\nineliminable property of neural activations.", "AI": {"tldr": "\u63d0\u51fa\u4e8c\u8fdb\u5236\u7a00\u758f\u81ea\u7f16\u7801\u5668(BAEs)\u548c\u4e8c\u8fdb\u5236\u8f6c\u7801\u5668(BTCs)\uff0c\u901a\u8fc7\u5c06\u6fc0\u6d3b\u503c\u9650\u5236\u4e3a0\u62161\u6765\u63d0\u5347\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5355\u4e49\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u91cd\u6784\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4e2d\u8bb8\u591a\u7279\u5f81\u53ea\u5728\u5f3a\u6fc0\u6d3b\u65f6\u624d\u53ef\u89e3\u91ca\u7684\u95ee\u9898\uff0c\u6d88\u9664\u901a\u8fc7\u8fde\u7eed\u6fc0\u6d3b\u5f3a\u5ea6\u53d8\u5316\u4f20\u9012\u4e0d\u53ef\u89e3\u91ca\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e8c\u8fdb\u5236\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u4e8c\u8fdb\u5236\u8f6c\u7801\u5668\uff0c\u5c06\u6240\u6709\u6fc0\u6d3b\u503c\u7ea6\u675f\u4e3a\u4e8c\u8fdb\u5236(0\u62161)\uff0c\u6d88\u9664\u8fde\u7eed\u6fc0\u6d3b\u5f3a\u5ea6\u7684\u53d8\u5316\u3002", "result": "\u4e8c\u8fdb\u5236\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u53d1\u73b0\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5355\u4e49\u6027\uff0c\u4f46\u589e\u52a0\u4e86\u91cd\u6784\u8bef\u5dee\u548c\u4e0d\u53ef\u89e3\u91ca\u7684\u8d85\u9ad8\u9891\u7279\u5f81\u6570\u91cf\u3002\u9891\u7387\u8c03\u6574\u540e\uff0c\u8fde\u7eed\u7a00\u758f\u7f16\u7801\u5668\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\u7565\u4f18\u4e8e\u4e8c\u8fdb\u5236\u65b9\u6cd5\u3002", "conclusion": "\u591a\u4e49\u6027\u53ef\u80fd\u662f\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u7684\u56fa\u6709\u5c5e\u6027\uff0c\u4e8c\u8fdb\u5236\u5316\u867d\u7136\u63d0\u5347\u4e86\u67d0\u4e9b\u65b9\u9762\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u591a\u4e49\u6027\u95ee\u9898\u3002"}}
{"id": "2509.25606", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25606", "abs": "https://arxiv.org/abs/2509.25606", "authors": ["Yixuan Wang", "Dan Guralnik", "Saiedeh Akbari", "Warren Dixon"], "title": "Effective Model Pruning", "comment": "17 pages, 4 figures", "summary": "We introduce Effective Model Pruning (EMP), a context-agnostic,\nparameter-free rule addressing a fundamental question about pruning: how many\nentries to keep. EMP does not prescribe how to score the parameters or prune\nthe models; instead, it supplies a universal adaptive threshold that can be\napplied to any pruning criterion: weight magnitude, attention score, KAN\nimportance score, or even feature-level signals such as image pixel, and used\non structural parts or weights of the models. Given any score vector s, EMP\nmaps s to a built-in effective number N_eff which is inspired by the Inverse\nSimpson index of contributors. Retaining the N_eff highest scoring entries and\nzeroing the remainder yields sparse models with performance comparable to the\noriginal dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our\nexperiments. By leveraging the geometry of the simplex, we derive a tight lower\nbound on the preserved mass s_eff (the sum of retained scores) over the\ncorresponding ordered probability simplex associated with the score vector s.\nWe further verify the effectiveness of N_eff by pruning the model with a scaled\nthreshold \\b{eta}*N_eff across a variety of criteria and models. Experiments\nsuggest that the default \\b{eta} = 1 yields a robust threshold for model\npruning while \\b{eta} not equal to 1 still serves as an optional adjustment to\nmeet specific sparsity requirements.", "AI": {"tldr": "EMP\u662f\u4e00\u79cd\u4e0e\u4e0a\u4e0b\u6587\u65e0\u5173\u3001\u65e0\u9700\u53c2\u6570\u7684\u526a\u679d\u89c4\u5219\uff0c\u901a\u8fc7\u8ba1\u7b97\u6709\u6548\u6570\u91cfN_eff\u6765\u786e\u5b9a\u4fdd\u7559\u591a\u5c11\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u526a\u679d\u6807\u51c6\u548c\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u526a\u679d\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff1a\u5e94\u8be5\u4fdd\u7559\u591a\u5c11\u53c2\u6570\uff1f\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u624b\u52a8\u8bbe\u7f6e\u526a\u679d\u7387\u6216\u4f9d\u8d56\u7279\u5b9a\u4e0a\u4e0b\u6587\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u9006\u8f9b\u666e\u68ee\u6307\u6570\u8ba1\u7b97\u6709\u6548\u6570\u91cfN_eff\uff0c\u4fdd\u7559\u5f97\u5206\u6700\u9ad8\u7684N_eff\u4e2a\u53c2\u6570\uff0c\u96f6\u5316\u5176\u4f59\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u6743\u91cd\u5927\u5c0f\u3001\u6ce8\u610f\u529b\u5206\u6570\u3001KAN\u91cd\u8981\u6027\u5206\u6570\u7b49\u5404\u79cd\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u5728MLP\u3001CNN\u3001Transformer/LLM\u548cKAN\u7b49\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528EMP\u526a\u679d\u7684\u7a00\u758f\u6a21\u578b\u6027\u80fd\u4e0e\u539f\u59cb\u5bc6\u96c6\u7f51\u7edc\u76f8\u5f53\u3002", "conclusion": "EMP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u81ea\u9002\u5e94\u7684\u526a\u679d\u9608\u503c\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u6a21\u578b\u526a\u679d\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u53c2\u6570\u7a00\u758f\u5316\u3002"}}
{"id": "2509.25612", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25612", "abs": "https://arxiv.org/abs/2509.25612", "authors": ["Muhammad Imran Hossain", "Jignesh Solanki", "Sarika Khushlani Solanki"], "title": "Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN", "comment": null, "summary": "Ensuring power grid resilience requires the timely and unsupervised detection\nof anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel\nframework that integrates window-attention Transformers within a bidirectional\nGenerative Adversarial Network (BiGAN) to address this challenge. Its\nself-attention encoder-decoder architecture captures complex spatio-temporal\ndependencies across the grid, while a joint discriminator enforces cycle\nconsistency to align the learned latent space with the true data distribution.\nAnomalies are flagged in real-time using an adaptive score that combines\nreconstruction error, latent space drift, and discriminator confidence.\nEvaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves\nan ROC-AUC of 0.95 and an average precision of 0.996, significantly\noutperforming leading supervised and unsupervised methods. It shows particular\nstrength in detecting subtle frequency and voltage deviations, demonstrating\nits practical value for live, wide-area monitoring without relying on manually\nlabeled fault data.", "AI": {"tldr": "T-BiGAN\u662f\u4e00\u4e2a\u7ed3\u5408\u7a97\u53e3\u6ce8\u610f\u529bTransformer\u548c\u53cc\u5411\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u529b\u7cfb\u7edf\u540c\u6b65\u76f8\u91cf\u6570\u636e\u6d41\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u786e\u4fdd\u7535\u7f51\u97e7\u6027\u9700\u8981\u53ca\u65f6\u65e0\u76d1\u7763\u5730\u68c0\u6d4b\u540c\u6b65\u76f8\u91cf\u6570\u636e\u6d41\u4e2d\u7684\u5f02\u5e38\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u5b9e\u65f6\u76d1\u63a7\u9700\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u6ce8\u610f\u529b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u6355\u83b7\u7535\u7f51\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u8054\u5408\u5224\u522b\u5668\u5f3a\u5236\u6267\u884c\u5faa\u73af\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u8bc4\u5206\u7ed3\u5408\u91cd\u6784\u8bef\u5dee\u3001\u6f5c\u5728\u7a7a\u95f4\u6f02\u79fb\u548c\u5224\u522b\u5668\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u5b9e\u65f6\u5f02\u5e38\u6807\u8bb0\u3002", "result": "\u5728\u786c\u4ef6\u5728\u73afPMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT-BiGAN\u8fbe\u5230ROC-AUC 0.95\u548c\u5e73\u5747\u7cbe\u5ea60.996\uff0c\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7279\u522b\u64c5\u957f\u68c0\u6d4b\u5fae\u5999\u7684\u9891\u7387\u548c\u7535\u538b\u504f\u5dee\u3002", "conclusion": "T-BiGAN\u8bc1\u660e\u4e86\u5728\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6545\u969c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5b9e\u65f6\u5e7f\u57df\u76d1\u63a7\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.25622", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25622", "abs": "https://arxiv.org/abs/2509.25622", "authors": ["Zhendong Mi", "Bian Sun", "Grace Li Zhang", "Shaoyi Huang"], "title": "Layer-wise dynamic rank for compressing large language models", "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) have rapidly scaled in size, bringing severe\nmemory and computational challenges that hinder their deployment. Singular\nValue Decomposition (SVD)-based compression has emerged as an appealing\npost-training compression technique for LLMs, yet most existing methods apply a\nuniform compression ratio across all layers, implicitly assuming homogeneous\ninformation included in various layers. This overlooks the substantial\nintra-layer heterogeneity observed in LLMs, where middle layers tend to encode\nricher information while early and late layers are more redundant. In this\nwork, we revisit the existing SVD-based compression method and propose D-Rank,\na framework with layer-wise balanced Dynamic Rank allocation for LLMs\ncompression. We first introduce effective rank as a principled metric to\nmeasure the information density of weight matrices, and then allocate ranks via\na Lagrange multiplier-based optimization scheme to adaptively assign more\ncapacity to groups with higher information density under a fixed compression\nratio. Moreover, we rebalance the allocated ranks across attention layers to\naccount for their varying importance and extend D-Rank to latest LLMs with\ngrouped-query attention. Extensive experiments on various LLMs with different\nscales across multiple compression ratios demonstrate that D-Rank consistently\noutperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower\nperplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up\nto 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40%\ncompression ratio while achieving even higher throughput.", "AI": {"tldr": "D-Rank\u662f\u4e00\u4e2a\u57fa\u4e8eSVD\u7684LLM\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5c42\u5206\u914d\u79e9\u6765\u4f18\u5316\u538b\u7f29\u6548\u679c\uff0c\u8003\u8651\u4e0d\u540c\u5c42\u7684\u4fe1\u606f\u5bc6\u5ea6\u5dee\u5f02\uff0c\u5728\u4fdd\u6301\u538b\u7f29\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SVD\u538b\u7f29\u65b9\u6cd5\u5bf9\u6240\u6709\u5c42\u4f7f\u7528\u7edf\u4e00\u538b\u7f29\u7387\uff0c\u5ffd\u89c6\u4e86LLM\u4e2d\u4e0d\u540c\u5c42\u7684\u4fe1\u606f\u5bc6\u5ea6\u5dee\u5f02\uff08\u4e2d\u95f4\u5c42\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u65e9\u671f\u548c\u665a\u671f\u5c42\u66f4\u5197\u4f59\uff09\uff0c\u5bfc\u81f4\u538b\u7f29\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u6709\u6548\u79e9\u4f5c\u4e3a\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u4f18\u5316\u65b9\u6848\u52a8\u6001\u5206\u914d\u79e9\uff0c\u5bf9\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u79e9\u91cd\u5e73\u8861\uff0c\u5e76\u6269\u5c55\u5230\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\u7684\u6700\u65b0LLM\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u538b\u7f29\u7387\u4e0b\uff0cD-Rank\u59cb\u7ec8\u4f18\u4e8eSVD-LLM\u3001ASVD\u548cBasis Sharing\u65b9\u6cd5\uff0c\u572820%\u538b\u7f29\u7387\u4e0bLLaMA-3-8B\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e15%\u4ee5\u4e0a\uff0c\u572840%\u538b\u7f29\u7387\u4e0bLLaMA-7B\u7684\u96f6\u6837\u672c\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53475%\uff0c\u540c\u65f6\u541e\u5410\u91cf\u66f4\u9ad8\u3002", "conclusion": "D-Rank\u901a\u8fc7\u8003\u8651\u5c42\u95f4\u5f02\u8d28\u6027\u7684\u52a8\u6001\u79e9\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86SVD\u538b\u7f29\u7684\u6548\u679c\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25631", "abs": "https://arxiv.org/abs/2509.25631", "authors": ["Jason Stock", "Troy Arcomano", "Rao Kotamarthi"], "title": "Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting", "comment": "17 pages and 15 figures", "summary": "Diffusion models offer a physically grounded framework for probabilistic\nweather forecasting, but their typical reliance on slow, iterative solvers\nduring inference makes them impractical for subseasonal-to-seasonal (S2S)\napplications where long lead-times and domain-driven calibration are essential.\nTo address this, we introduce Swift, a single-step consistency model that, for\nthe first time, enables autoregressive finetuning of a probability flow model\nwith a continuous ranked probability score (CRPS) objective. This eliminates\nthe need for multi-model ensembling or parameter perturbations. Results show\nthat Swift produces skillful 6-hourly forecasts that remain stable for up to 75\ndays, running $39\\times$ faster than state-of-the-art diffusion baselines while\nachieving forecast skill competitive with the numerical-based, operational IFS\nENS. This marks a step toward efficient and reliable ensemble forecasting from\nmedium-range to seasonal-scales.", "AI": {"tldr": "Swift\u662f\u4e00\u4e2a\u5355\u6b65\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8fde\u7eed\u6392\u540d\u6982\u7387\u8bc4\u5206\uff08CRPS\uff09\u7684\u6982\u7387\u6d41\u6a21\u578b\u81ea\u56de\u5f52\u5fae\u8c03\uff0c\u65e0\u9700\u591a\u6a21\u578b\u96c6\u6210\u6216\u53c2\u6570\u6270\u52a8\uff0c\u5728S2S\u5929\u6c14\u9884\u62a5\u4e2d\u6bd4\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u5feb39\u500d\uff0c\u9884\u62a5\u6280\u80fd\u4e0e\u57fa\u4e8e\u6570\u503c\u7684IFS ENS\u76f8\u5f53\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4e3a\u6982\u7387\u5929\u6c14\u9884\u62a5\u63d0\u4f9b\u4e86\u7269\u7406\u57fa\u7840\u6846\u67b6\uff0c\u4f46\u5176\u5728\u63a8\u7406\u65f6\u4f9d\u8d56\u7f13\u6162\u7684\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u4f7f\u5f97\u5728\u6b21\u5b63\u8282\u5230\u5b63\u8282\uff08S2S\uff09\u5e94\u7528\u4e2d\u4e0d\u5b9e\u7528\uff0c\u56e0\u4e3aS2S\u9700\u8981\u957f\u63d0\u524d\u671f\u548c\u9886\u57df\u9a71\u52a8\u7684\u6821\u51c6\u3002", "method": "\u5f15\u5165Swift\u5355\u6b65\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8eCRPS\u76ee\u6807\u7684\u6982\u7387\u6d41\u6a21\u578b\u81ea\u56de\u5f52\u5fae\u8c03\uff0c\u6d88\u9664\u4e86\u591a\u6a21\u578b\u96c6\u6210\u6216\u53c2\u6570\u6270\u52a8\u7684\u9700\u6c42\u3002", "result": "Swift\u751f\u6210\u5177\u6709\u6280\u80fd\u76846\u5c0f\u65f6\u9884\u62a5\uff0c\u53ef\u7a33\u5b9a\u8fd0\u884c\u957f\u8fbe75\u5929\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u6269\u6563\u57fa\u7ebf\u5feb39\u500d\uff0c\u9884\u62a5\u6280\u80fd\u4e0e\u57fa\u4e8e\u6570\u503c\u7684IFS ENS\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u662f\u5411\u4ece\u4e2d\u671f\u5230\u5b63\u8282\u5c3a\u5ea6\u7684\u9ad8\u6548\u53ef\u9760\u96c6\u5408\u9884\u62a5\u8fc8\u51fa\u7684\u4e00\u6b65\u3002"}}
{"id": "2509.25637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25637", "abs": "https://arxiv.org/abs/2509.25637", "authors": ["Kotaro Yoshida", "Atsushi Nitanda"], "title": "How Does Preconditioning Guide Feature Learning in Deep Neural Networks?", "comment": null, "summary": "Preconditioning is widely used in machine learning to accelerate convergence\non the empirical risk, yet its role on the expected risk remains underexplored.\nIn this work, we investigate how preconditioning affects feature learning and\ngeneralization performance. We first show that the input information available\nto the model is conveyed solely through the Gram matrix defined by the\npreconditioner's metric, thereby inducing a controllable spectral bias on\nfeature learning. Concretely, instantiating the preconditioner as the $p$-th\npower of the input covariance matrix and within a single-index teacher model,\nwe prove that in generalization, the exponent $p$ and the alignment between the\nteacher and the input spectrum are crucial factors. We further investigate how\nthe interplay between these factors influences feature learning from three\ncomplementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution\ngeneralization, and (iii) Forward knowledge transfer. Our results indicate that\nthe learned feature representations closely mirror the spectral bias introduced\nby the preconditioner -- favoring components that are emphasized and exhibiting\nreduced sensitivity to those that are suppressed. Crucially, we demonstrate\nthat generalization is significantly enhanced when this spectral bias is\naligned with that of the teacher.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9884\u6761\u4ef6\u5904\u7406\u5982\u4f55\u5f71\u54cd\u7279\u5f81\u5b66\u4e60\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u53d1\u73b0\u9884\u6761\u4ef6\u5668\u901a\u8fc7\u5176\u5ea6\u91cf\u5b9a\u4e49\u7684Gram\u77e9\u9635\u63a7\u5236\u8f93\u5165\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u7279\u5f81\u5b66\u4e60\u4e2d\u5f15\u5165\u53ef\u63a7\u7684\u8c31\u504f\u7f6e\u3002\u7814\u7a76\u8868\u660e\uff0c\u5f53\u8fd9\u79cd\u8c31\u504f\u7f6e\u4e0e\u6559\u5e08\u6a21\u578b\u7684\u8c31\u504f\u7f6e\u5bf9\u9f50\u65f6\uff0c\u6cdb\u5316\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u9884\u6761\u4ef6\u5904\u7406\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u52a0\u901f\u7ecf\u9a8c\u98ce\u9669\u7684\u6536\u655b\uff0c\u4f46\u5176\u5bf9\u671f\u671b\u98ce\u9669\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u9884\u6761\u4ef6\u5904\u7406\u5982\u4f55\u5f71\u54cd\u7279\u5f81\u5b66\u4e60\u548c\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u5c06\u9884\u6761\u4ef6\u5668\u5b9e\u4f8b\u5316\u4e3a\u8f93\u5165\u534f\u65b9\u5dee\u77e9\u9635\u7684p\u6b21\u5e42\uff0c\u5e76\u5728\u5355\u7d22\u5f15\u6559\u5e08\u6a21\u578b\u6846\u67b6\u4e0b\uff0c\u5206\u6790\u6307\u6570p\u548c\u6559\u5e08\u4e0e\u8f93\u5165\u8c31\u5bf9\u9f50\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u3002\u4ece\u566a\u58f0\u9c81\u68d2\u6027\u3001\u5206\u5e03\u5916\u6cdb\u5316\u548c\u524d\u5411\u77e5\u8bc6\u8fc1\u79fb\u4e09\u4e2a\u4e92\u8865\u89c6\u89d2\u7814\u7a76\u8fd9\u4e9b\u56e0\u7d20\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8868\u793a\u5bc6\u5207\u53cd\u6620\u4e86\u9884\u6761\u4ef6\u5668\u5f15\u5165\u7684\u8c31\u504f\u7f6e\u2014\u2014\u504f\u597d\u88ab\u5f3a\u8c03\u7684\u7ec4\u4ef6\uff0c\u5bf9\u88ab\u6291\u5236\u7684\u7ec4\u4ef6\u8868\u73b0\u51fa\u964d\u4f4e\u7684\u654f\u611f\u6027\u3002\u5f53\u8fd9\u79cd\u8c31\u504f\u7f6e\u4e0e\u6559\u5e08\u6a21\u578b\u7684\u8c31\u504f\u7f6e\u5bf9\u9f50\u65f6\uff0c\u6cdb\u5316\u6027\u80fd\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "\u9884\u6761\u4ef6\u5904\u7406\u901a\u8fc7Gram\u77e9\u9635\u63a7\u5236\u8f93\u5165\u4fe1\u606f\uff0c\u5728\u7279\u5f81\u5b66\u4e60\u4e2d\u5f15\u5165\u53ef\u63a7\u7684\u8c31\u504f\u7f6e\u3002\u6cdb\u5316\u6027\u80fd\u7684\u5173\u952e\u5728\u4e8e\u9884\u6761\u4ef6\u5668\u5f15\u5165\u7684\u8c31\u504f\u7f6e\u4e0e\u6559\u5e08\u6a21\u578b\u8c31\u504f\u7f6e\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u8fd9\u79cd\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.25646", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.25646", "abs": "https://arxiv.org/abs/2509.25646", "authors": ["Lei Ma", "Ling Guo", "Hao Wu", "Tao Zhou"], "title": "Deep set based operator learning with uncertainty quantification", "comment": null, "summary": "Learning operators from data is central to scientific machine learning. While\nDeepONets are widely used for their ability to handle complex domains, they\nrequire fixed sensor numbers and locations, lack mechanisms for uncertainty\nquantification (UQ), and are thus limited in practical applicability. Recent\npermutationinvariant extensions, such as the Variable-Input Deep Operator\nNetwork (VIDON), relax these sensor constraints but still rely on sufficiently\ndense observations and cannot capture uncertainties arising from incomplete\nmeasurements or from operators with inherent randomness. To address these\nchallenges, we propose UQ-SONet, a permutation-invariant operator learning\nframework with built-in UQ. Our model integrates a set transformer embedding to\nhandle sparse and variable sensor locations, and employs a conditional\nvariational autoencoder (cVAE) to approximate the conditional distribution of\nthe solution operator. By minimizing the negative ELBO, UQ-SONet provides\nprincipled uncertainty estimation while maintaining predictive accuracy.\nNumerical experiments on deterministic and stochastic PDEs, including the\nNavier-Stokes equation, demonstrate the robustness and effectiveness of the\nproposed framework.", "AI": {"tldr": "UQ-SONet\u662f\u4e00\u4e2a\u5177\u6709\u5185\u7f6e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408set transformer\u5d4c\u5165\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6765\u5904\u7406\u7a00\u758f\u4f20\u611f\u5668\u6570\u636e\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684DeepONets\u9700\u8981\u56fa\u5b9a\u4f20\u611f\u5668\u914d\u7f6e\u4e14\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\uff0c\u800cVIDON\u7b49\u6269\u5c55\u867d\u7136\u653e\u5bbd\u4e86\u4f20\u611f\u5668\u7ea6\u675f\u4f46\u4ecd\u65e0\u6cd5\u5904\u7406\u4e0d\u5b8c\u6574\u6d4b\u91cf\u6216\u968f\u673a\u7b97\u5b50\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u96c6\u6210set transformer\u5d4c\u5165\u5904\u7406\u7a00\u758f\u53ef\u53d8\u4f20\u611f\u5668\u4f4d\u7f6e\uff0c\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fd1\u4f3c\u89e3\u7b97\u5b50\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8d1fELBO\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u786e\u5b9a\u6027\u548c\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\uff08\u5305\u62ecNavier-Stokes\u65b9\u7a0b\uff09\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "UQ-SONet\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u5728\u4f20\u611f\u5668\u7075\u6d3b\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.25647", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25647", "abs": "https://arxiv.org/abs/2509.25647", "authors": ["Fangji Wang", "Panagiotis Tsiotras"], "title": "BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks", "comment": null, "summary": "Branch-and-bound with preactivation splitting has been shown highly effective\nfor deterministic verification of neural networks. In this paper, we extend\nthis framework to the probabilistic setting. We propose BaB-prob that\niteratively divides the original problem into subproblems by splitting\npreactivations and leverages linear bounds computed by linear bound propagation\nto bound the probability for each subproblem. We prove soundness and\ncompleteness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we\nintroduce the notion of uncertainty level and design two efficient strategies\nfor preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We\nevaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models,\nrespectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach\nconsistently outperforms state-of-the-art approaches in medium- to\nhigh-dimensional input problems.", "AI": {"tldr": "\u63d0\u51faBaB-prob\u65b9\u6cd5\uff0c\u5c06\u5206\u652f\u5b9a\u754c\u4e0e\u9884\u6fc0\u6d3b\u5206\u88c2\u6269\u5c55\u5230\u6982\u7387\u9a8c\u8bc1\uff0c\u5728\u4e2d\u7b49\u81f3\u9ad8\u7ef4\u8f93\u5165\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u786e\u5b9a\u6027\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u4e2d\u7684\u5206\u652f\u5b9a\u754c\u4e0e\u9884\u6fc0\u6d3b\u5206\u88c2\u6846\u67b6\u6269\u5c55\u5230\u6982\u7387\u9a8c\u8bc1\u573a\u666f\u3002", "method": "\u901a\u8fc7\u5206\u88c2\u9884\u6fc0\u6d3b\u5c06\u539f\u95ee\u9898\u8fed\u4ee3\u5212\u5206\u4e3a\u5b50\u95ee\u9898\uff0c\u5229\u7528\u7ebf\u6027\u8fb9\u754c\u4f20\u64ad\u8ba1\u7b97\u8fb9\u754c\u6765\u7ea6\u675f\u6bcf\u4e2a\u5b50\u95ee\u9898\u7684\u6982\u7387\u3002", "result": "\u5728\u672a\u8bad\u7ec3\u7f51\u7edc\u3001MNIST\u548cCIFAR-10\u6a21\u578b\u4ee5\u53caVNN-COMP 2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e2d\u7b49\u81f3\u9ad8\u7ef4\u8f93\u5165\u95ee\u9898\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "BaB-prob\u65b9\u6cd5\u5728\u6982\u7387\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e2d\u7b49\u81f3\u9ad8\u7ef4\u8f93\u5165\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.25665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25665", "abs": "https://arxiv.org/abs/2509.25665", "authors": ["Qihang Yao", "Constantine Dovrolis"], "title": "Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks", "comment": null, "summary": "The lottery ticket hypothesis suggests that dense networks contain sparse\nsubnetworks that can be trained in isolation to match full-model performance.\nExisting approaches-iterative pruning, dynamic sparse training, and pruning at\ninitialization-either incur heavy retraining costs or assume the target density\nis fixed in advance. We introduce Path Weight Magnitude Product-biased Random\ngrowth (PWMPR), a constructive sparse-to-dense training paradigm that grows\nnetworks rather than pruning them, while automatically discovering their\noperating density. Starting from a sparse seed, PWMPR adds edges guided by\npath-kernel-inspired scores, mitigates bottlenecks via randomization, and stops\nwhen a logistic-fit rule detects plateauing accuracy. Experiments on CIFAR,\nTinyImageNet, and ImageNet show that PWMPR approaches the performance of\nIMP-derived lottery tickets-though at higher density-at substantially lower\ncost (~1.5x dense vs. 3-4x for IMP). These results establish growth-based\ndensity discovery as a promising paradigm that complements pruning and dynamic\nsparsity.", "AI": {"tldr": "\u63d0\u51faPWMPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u7a00\u758f\u79cd\u5b50\u5f00\u59cb\u589e\u957f\u7f51\u7edc\u800c\u975e\u526a\u679d\uff0c\u81ea\u52a8\u53d1\u73b0\u7f51\u7edc\u7684\u6700\u4f73\u5bc6\u5ea6\uff0c\u5728\u8f83\u4f4e\u6210\u672c\u4e0b\u63a5\u8fd1\u5f69\u7968\u5047\u8bbe\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\uff08\u8fed\u4ee3\u526a\u679d\u3001\u52a8\u6001\u7a00\u758f\u8bad\u7ec3\u3001\u521d\u59cb\u5316\u526a\u679d\uff09\u8981\u4e48\u9700\u8981\u5927\u91cf\u91cd\u8bad\u7ec3\u6210\u672c\uff0c\u8981\u4e48\u5047\u8bbe\u76ee\u6807\u5bc6\u5ea6\u56fa\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u53d1\u73b0\u6700\u4f73\u5bc6\u5ea6\u7684\u66f4\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "PWMPR\u65b9\u6cd5\u4ece\u7a00\u758f\u79cd\u5b50\u5f00\u59cb\uff0c\u57fa\u4e8e\u8def\u5f84\u6838\u542f\u53d1\u5f0f\u5206\u6570\u6dfb\u52a0\u8fb9\uff0c\u901a\u8fc7\u968f\u673a\u5316\u7f13\u89e3\u74f6\u9888\uff0c\u4f7f\u7528\u903b\u8f91\u62df\u5408\u89c4\u5219\u68c0\u6d4b\u51c6\u786e\u7387\u5e73\u53f0\u671f\u6765\u505c\u6b62\u589e\u957f\u3002", "result": "\u5728CIFAR\u3001TinyImageNet\u548cImageNet\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPWMPR\u4ee5\u7ea61.5\u500d\u5bc6\u96c6\u8bad\u7ec3\u6210\u672c\u63a5\u8fd1IMP\u5f69\u7968\u5047\u8bbe\u6027\u80fd\uff08\u867d\u7136\u5bc6\u5ea6\u66f4\u9ad8\uff09\uff0c\u663e\u8457\u4f4e\u4e8eIMP\u76843-4\u500d\u6210\u672c\u3002", "conclusion": "\u57fa\u4e8e\u589e\u957f\u7684\u5bc6\u5ea6\u53d1\u73b0\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8303\u5f0f\uff0c\u4e0e\u526a\u679d\u548c\u52a8\u6001\u7a00\u758f\u65b9\u6cd5\u5f62\u6210\u4e92\u8865\u3002"}}
{"id": "2509.25666", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25666", "abs": "https://arxiv.org/abs/2509.25666", "authors": ["Justin Chih-Yao Chen", "Becky Xiangyu Peng", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Jiaxin Zhang", "Mohit Bansal", "Chien-Sheng Wu"], "title": "Nudging the Boundaries of LLM Reasoning", "comment": "Code release in preparation", "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key\nlimitation in LLM reasoning: they cannot learn from problems that are\n\"unsolvable\" to the model. In other words, they can only improve performance on\nproblems where the model is capable of exploring the correct answer.\nConsequently, the model's \"upper limit\" remains unchanged after RL training,\neven though the likelihood of solving easier, solvable problems may increase.\nThese hard samples cannot contribute to training, as no rollouts yield rewards\nand thus no gradients are produced. To unlock learning from these hard samples,\nwe propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM\nreasoning using self-generated hints, i.e., abstract cues that help reduce the\nproblem difficulty for the model. Given a question and its gold answer, the\nmodel generates a CoT and then produces a hint containing the core knowledge\nneeded to solve the problem. During training, we generate G rollouts from the\nbase policy and use the pass rate to decide whether the hint should be\ninjected. For hard samples with a 0% pass rate, we inject the hint and\nregenerate a new batch of trajectories. This yields two benefits: (1) the hint\nboosts pass rates (from 0% to non-zero), thereby introducing training signals\nfor previously unsolvable samples, and (2) the hints are self-generated,\navoiding distributional shift and do not rely on external models. NuRL achieves\nconsistent improvements across 6 benchmarks and 3 models, while remaining\ncomplementary to test-time scaling. Notably, NuRL can raise the model's upper\nlimit, whereas GRPO leaves pass@1024 unchanged from the base model.\nFurthermore, we present a systematic study of what makes an effective hint and\nwhen hints are most useful. Interestingly, the best hints are abstract and\nhigh-level, and are most beneficial when applied necessarily and after GRPO has\nconverged.", "AI": {"tldr": "NuRL\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u63d0\u793a\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u4e0a\u9650\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u65e0\u6cd5\u4ece\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u5b66\u4e60\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982GRPO\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u65e0\u6cd5\u4ece\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u5b66\u4e60\uff0c\u5bfc\u81f4\u6a21\u578b\u7684\u4e0a\u9650\u5728RL\u8bad\u7ec3\u540e\u4fdd\u6301\u4e0d\u53d8\u3002", "method": "\u63d0\u51faNuRL\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u81ea\u751f\u6210\u63d0\u793a\uff08\u5305\u542b\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u77e5\u8bc6\uff09\uff0c\u5bf9\u901a\u8fc7\u7387\u4e3a0%\u7684\u56f0\u96be\u6837\u672c\u6ce8\u5165\u63d0\u793a\u5e76\u91cd\u65b0\u751f\u6210\u8f68\u8ff9\uff0c\u4ece\u800c\u5f15\u5165\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "NuRL\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2a\u6a21\u578b\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u4e0a\u9650\uff0c\u800cGRPO\u5219\u65e0\u6cd5\u6539\u53d8pass@1024\u6307\u6807\u3002", "conclusion": "NuRL\u901a\u8fc7\u81ea\u751f\u6210\u63d0\u793a\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u80fd\u529b\u7684\u4e0a\u9650\uff0c\u6700\u4f73\u63d0\u793a\u662f\u62bd\u8c61\u4e14\u9ad8\u5c42\u6b21\u7684\uff0c\u5728GRPO\u6536\u655b\u540e\u5fc5\u8981\u4f7f\u7528\u65f6\u6548\u679c\u6700\u597d\u3002"}}
{"id": "2509.25667", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25667", "abs": "https://arxiv.org/abs/2509.25667", "authors": ["Bipul Thapa", "Biplov Paneru", "Bishwash Paneru", "Khem Narayan Poudyal"], "title": "EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface", "comment": null, "summary": "This paper presents an Artificial Intelligence (AI) integrated novel approach\nto Brain-Computer Interface (BCI)-based wheelchair development, utilizing a\nmotor imagery right-left-hand movement mechanism for control. The system is\ndesigned to simulate wheelchair navigation based on motor imagery right and\nleft-hand movements using electroencephalogram (EEG) data. A pre-filtered\ndataset, obtained from an open-source EEG repository, was segmented into arrays\nof 19x200 to capture the onset of hand movements. The data was acquired at a\nsampling frequency of 200Hz. The system integrates a Tkinter-based interface\nfor simulating wheelchair movements, offering users a functional and intuitive\ncontrol system. We propose a BiLSTM-BiGRU model that shows a superior test\naccuracy of 92.26% as compared with various machine learning baseline models,\nincluding XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU\nattention-based model achieved a mean accuracy of 90.13% through\ncross-validation, showcasing the potential of attention mechanisms in BCI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u8111\u673a\u63a5\u53e3\u8f6e\u6905\u63a7\u5236\u7cfb\u7edf\uff0c\u4f7f\u7528\u8fd0\u52a8\u60f3\u8c61\u5de6\u53f3\u624b\u8fd0\u52a8\u673a\u5236\uff0c\u901a\u8fc7EEG\u6570\u636e\u5b9e\u73b0\u8f6e\u6905\u5bfc\u822a\u6a21\u62df\uff0cBiLSTM-BiGRU\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u523092.26%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f00\u53d1\u66f4\u76f4\u89c2\u3001\u529f\u80fd\u6027\u7684\u8111\u673a\u63a5\u53e3\u8f6e\u6905\u63a7\u5236\u7cfb\u7edf\uff0c\u5229\u7528\u8fd0\u52a8\u60f3\u8c61\u673a\u5236\u4e3a\u884c\u52a8\u4e0d\u4fbf\u7684\u7528\u6237\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u63a7\u5236\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u5f00\u6e90EEG\u6570\u636e\u96c6\uff0c\u6570\u636e\u91c7\u6837\u9891\u7387200Hz\uff0c\u5206\u5272\u4e3a19x200\u6570\u7ec4\u6355\u6349\u624b\u90e8\u8fd0\u52a8\u8d77\u59cb\u3002\u63d0\u51faBiLSTM-BiGRU\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5e76\u4e0eXGBoost\u3001EEGNet\u548c\u57fa\u4e8etransformer\u7684\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002\u96c6\u6210Tkinter\u754c\u9762\u8fdb\u884c\u8f6e\u6905\u8fd0\u52a8\u6a21\u62df\u3002", "result": "BiLSTM-BiGRU\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523092.26%\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002\u57fa\u4e8e\u6ce8\u610f\u529b\u7684Bi-LSTM-BiGRU\u6a21\u578b\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u83b7\u5f9790.13%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u5728\u8111\u673a\u63a5\u53e3\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u63d0\u51fa\u7684BiLSTM-BiGRU\u6a21\u578b\u5728\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8111\u63a7\u8f6e\u6905\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25678", "abs": "https://arxiv.org/abs/2509.25678", "authors": ["Xing Han", "Hsing-Huan Chung", "Joydeep Ghosh", "Paul Pu Liang", "Suchi Saria"], "title": "Guiding Mixture-of-Experts with Temporal Multimodal Interactions", "comment": "21 pages, 8 figures, 10 tables", "summary": "Mixture-of-Experts (MoE) architectures have become pivotal for large-scale\nmultimodal models. However, their routing mechanisms typically overlook the\ninformative, time-varying interaction dynamics between modalities. This\nlimitation hinders expert specialization, as the model cannot explicitly\nleverage intrinsic modality relationships for effective reasoning. To address\nthis, we propose a novel framework that guides MoE routing using quantified\ntemporal interaction. A multimodal interaction-aware router learns to dispatch\ntokens to experts based on the nature of their interactions. This dynamic\nrouting encourages experts to acquire generalizable interaction-processing\nskills rather than merely learning task-specific features. Our framework builds\non a new formulation of temporal multimodal interaction dynamics, which are\nused to guide expert routing. We first demonstrate that these temporal\nmultimodal interactions reveal meaningful patterns across applications, and\nthen show how they can be leveraged to improve both the design and performance\nof MoE-based models. Comprehensive experiments on challenging multimodal\nbenchmarks validate our approach, demonstrating both enhanced performance and\nimproved interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316\u65f6\u5e8f\u4ea4\u4e92\u7684\u591a\u6a21\u6001MoE\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4ea4\u4e92\u611f\u77e5\u8def\u7531\u5668\u52a8\u6001\u5206\u914dtoken\u7ed9\u4e13\u5bb6\uff0c\u63d0\u5347\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u7684MoE\u67b6\u6784\u8def\u7531\u673a\u5236\u5ffd\u7565\u4e86\u6a21\u6001\u95f4\u4fe1\u606f\u4e30\u5bcc\u4e14\u968f\u65f6\u95f4\u53d8\u5316\u7684\u4ea4\u4e92\u52a8\u6001\uff0c\u8fd9\u9650\u5236\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u6a21\u578b\u65e0\u6cd5\u663e\u5f0f\u5229\u7528\u5185\u5728\u6a21\u6001\u5173\u7cfb\u8fdb\u884c\u6709\u6548\u63a8\u7406", "method": "\u57fa\u4e8e\u65b0\u7684\u65f6\u5e8f\u591a\u6a21\u6001\u4ea4\u4e92\u52a8\u6001\u516c\u5f0f\u5316\u8868\u793a\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u4ea4\u4e92\u611f\u77e5\u8def\u7531\u5668\u5b66\u4e60\u6839\u636etoken\u4ea4\u4e92\u7279\u6027\u5c06\u5176\u5206\u914d\u7ed9\u4e13\u5bb6\uff0c\u4fc3\u8fdb\u4e13\u5bb6\u83b7\u5f97\u53ef\u6cdb\u5316\u7684\u4ea4\u4e92\u5904\u7406\u6280\u80fd", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6027\u80fd\u63d0\u5347\u548c\u53ef\u89e3\u91ca\u6027\u6539\u8fdb", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5229\u7528\u65f6\u5e8f\u591a\u6a21\u6001\u4ea4\u4e92\u6765\u6307\u5bfcMoE\u8def\u7531\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u80fd\u529b"}}
{"id": "2509.25686", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25686", "abs": "https://arxiv.org/abs/2509.25686", "authors": ["Pirzada Suhail", "Aditya Anand", "Amit Sethi"], "title": "Minimalist Explanation Generation and Circuit Discovery", "comment": null, "summary": "Machine learning models, by virtue of training, learn a large repertoire of\ndecision rules for any given input, and any one of these may suffice to justify\na prediction. However, in high-dimensional input spaces, such rules are\ndifficult to identify and interpret. In this paper, we introduce an\nactivation-matching based approach to generate minimal and faithful\nexplanations for the decisions of pre-trained image classifiers. We aim to\nidentify minimal explanations that not only preserve the model's decision but\nare also concise and human-readable. To achieve this, we train a lightweight\nautoencoder to produce binary masks that learns to highlight the decision-wise\ncritical regions of an image while discarding irrelevant background. The\ntraining objective integrates activation alignment across multiple layers,\nconsistency at the output label, priors that encourage sparsity, and\ncompactness, along with a robustness constraint that enforces faithfulness. The\nminimal explanations so generated also lead us to mechanistically interpreting\nthe model internals. In this regard we also introduce a circuit readout\nprocedure wherein using the explanation's forward pass and gradients, we\nidentify active channels and construct a channel-level graph, scoring\ninter-layer edges by ingress weight magnitude times source activation and\nfeature-to-class links by classifier weight magnitude times feature activation.\nTogether, these contributions provide a practical bridge between minimal\ninput-level explanations and a mechanistic understanding of the internal\ncomputations driving model decisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5339\u914d\u7684\u65b9\u6cd5\uff0c\u4e3a\u9884\u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668\u751f\u6210\u6700\u5c0f\u4e14\u5fe0\u5b9e\u7684\u89e3\u91ca\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u81ea\u7f16\u7801\u5668\u4ea7\u751f\u4e8c\u8fdb\u5236\u63a9\u7801\u6765\u7a81\u51fa\u56fe\u50cf\u4e2d\u51b3\u7b56\u5173\u952e\u533a\u57df\uff0c\u5e76\u7ed3\u5408\u7535\u8def\u8bfb\u53d6\u7a0b\u5e8f\u6765\u7406\u89e3\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u673a\u5236\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u5927\u91cf\u51b3\u7b56\u89c4\u5219\uff0c\u4f46\u5728\u9ad8\u7ef4\u8f93\u5165\u7a7a\u95f4\u4e2d\u96be\u4ee5\u8bc6\u522b\u548c\u89e3\u91ca\u8fd9\u4e9b\u89c4\u5219\uff0c\u9700\u8981\u627e\u5230\u65e2\u4fdd\u6301\u6a21\u578b\u51b3\u7b56\u53c8\u7b80\u6d01\u53ef\u8bfb\u7684\u6700\u5c0f\u89e3\u91ca\u3002", "method": "\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u81ea\u7f16\u7801\u5668\u751f\u6210\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u6574\u5408\u591a\u5c42\u6fc0\u6d3b\u5bf9\u9f50\u3001\u8f93\u51fa\u6807\u7b7e\u4e00\u81f4\u6027\u3001\u7a00\u758f\u6027\u548c\u7d27\u51d1\u6027\u5148\u9a8c\uff0c\u4ee5\u53ca\u786e\u4fdd\u5fe0\u5b9e\u6027\u7684\u9c81\u68d2\u6027\u7ea6\u675f\uff1b\u540c\u65f6\u5f15\u5165\u7535\u8def\u8bfb\u53d6\u7a0b\u5e8f\uff0c\u901a\u8fc7\u89e3\u91ca\u7684\u524d\u5411\u4f20\u64ad\u548c\u68af\u5ea6\u8bc6\u522b\u6d3b\u8dc3\u901a\u9053\u5e76\u6784\u5efa\u901a\u9053\u7ea7\u56fe\u3002", "result": "\u751f\u6210\u4e86\u6700\u5c0f\u4e14\u5fe0\u5b9e\u7684\u89e3\u91ca\uff0c\u80fd\u591f\u7a81\u51fa\u56fe\u50cf\u4e2d\u51b3\u7b56\u5173\u952e\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u7535\u8def\u8bfb\u53d6\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u673a\u5236\u7684\u7406\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6700\u5c0f\u8f93\u5165\u7ea7\u89e3\u91ca\u548c\u9a71\u52a8\u6a21\u578b\u51b3\u7b56\u7684\u5185\u90e8\u8ba1\u7b97\u673a\u5236\u7406\u89e3\u4e4b\u95f4\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u6865\u6881\u3002"}}
{"id": "2509.25690", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25690", "abs": "https://arxiv.org/abs/2509.25690", "authors": ["Zihui Zhao", "Yuanbo Tang", "Jieyu Ren", "Xiaoping Zhang", "Yang Li"], "title": "A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation", "comment": null, "summary": "Dictionary learning is traditionally formulated as an $L_1$-regularized\nsignal reconstruction problem. While recent developments have incorporated\ndiscriminative, hierarchical, or generative structures, most approaches rely on\nencouraging representation sparsity over individual samples that overlook how\natoms are shared across samples, resulting in redundant and sub-optimal\ndictionaries. We introduce a parsimony promoting regularizer based on the\nrow-wise $L_\\infty$ norm of the coefficient matrix. This additional penalty\nencourages entire rows of the coefficient matrix to vanish, thereby reducing\nthe number of dictionary atoms activated across the dataset. We derive the\nformulation from a probabilistic model with Beta-Bernoulli priors, which\nprovides a Bayesian interpretation linking the regularization parameters to\nprior distributions. We further establish theoretical calculation for optimal\nhyperparameter selection and connect our formulation to both Minimum\nDescription Length, Bayesian model selection and pathlet learning. Extensive\nexperiments on benchmark datasets demonstrate that our method achieves\nsubstantially improved reconstruction quality (with a 20\\% reduction in RMSE)\nand enhanced representation sparsity, utilizing fewer than one-tenth of the\navailable dictionary atoms, while empirically validating our theoretical\nanalysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u6570\u77e9\u9635\u884c\u5411L\u221e\u8303\u6570\u7684\u7b80\u7ea6\u6027\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9f13\u52b1\u6574\u884c\u7cfb\u6570\u6d88\u5931\u6765\u51cf\u5c11\u5b57\u5178\u539f\u5b50\u7684\u4f7f\u7528\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8868\u793a\u7a00\u758f\u6027\u3002", "motivation": "\u4f20\u7edf\u5b57\u5178\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6837\u672c\u7684\u7a00\u758f\u6027\uff0c\u5ffd\u7565\u4e86\u539f\u5b50\u5728\u6837\u672c\u95f4\u7684\u5171\u4eab\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5b57\u5178\u5197\u4f59\u548c\u6b21\u4f18\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fc3\u8fdb\u8de8\u6837\u672c\u539f\u5b50\u5171\u4eab\u7a00\u758f\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u7cfb\u6570\u77e9\u9635\u884c\u5411L\u221e\u8303\u6570\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\uff0c\u7ed3\u5408Beta-Bernoulli\u5148\u9a8c\u7684\u6982\u7387\u6a21\u578b\uff0c\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u89e3\u91ca\u6b63\u5219\u5316\u53c2\u6570\uff0c\u5e76\u4e0e\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u3001\u8d1d\u53f6\u65af\u6a21\u578b\u9009\u62e9\u7b49\u7406\u8bba\u5efa\u7acb\u8054\u7cfb\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u91cd\u5efa\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff08RMSE\u964d\u4f4e20%\uff09\uff0c\u8868\u793a\u7a00\u758f\u6027\u589e\u5f3a\uff0c\u4ec5\u4f7f\u7528\u4e0d\u5230\u5341\u5206\u4e4b\u4e00\u7684\u5b57\u5178\u539f\u5b50\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u7ea6\u6027\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5b57\u5178\u5b66\u4e60\u7684\u5197\u4f59\u95ee\u9898\uff0c\u901a\u8fc7\u4fc3\u8fdb\u8de8\u6837\u672c\u7684\u539f\u5b50\u5171\u4eab\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5b57\u5178\u5b66\u4e60\u548c\u8868\u793a\u6027\u80fd\u3002"}}
{"id": "2509.25692", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25692", "abs": "https://arxiv.org/abs/2509.25692", "authors": ["Tingyu Shi", "Fan Lyu", "Shaoliang Peng"], "title": "Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction", "comment": null, "summary": "Active Test-Time Adaptation (ATTA) improves model robustness under domain\nshift by selectively querying human annotations at deployment, but existing\nmethods use heuristic uncertainty measures and suffer from low data selection\nefficiency, wasting human annotation budget. We propose Conformal Prediction\nActive TTA (CPATTA), which first brings principled, coverage-guaranteed\nuncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K\ncertainty measure, an online weight-update algorithm driven by pseudo coverage,\na domain-shift detector that adapts human supervision, and a staged update\nscheme balances human-labeled and model-labeled data. Extensive experiments\ndemonstrate that CPATTA consistently outperforms the state-of-the-art ATTA\nmethods by around 5% in accuracy. Our code and datasets are available at\nhttps://github.com/tingyushi/CPATTA.", "AI": {"tldr": "CPATTA\u662f\u4e00\u79cd\u4e3b\u52a8\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4fdd\u8986\u76d6\u7387\u7684\u7f6e\u4fe1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u7ed3\u5408\u5728\u7ebf\u6743\u91cd\u66f4\u65b0\u548c\u57df\u504f\u79fb\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u9009\u62e9\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4f7f\u7528\u542f\u53d1\u5f0f\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u6570\u636e\u9009\u62e9\u6548\u7387\u4f4e\uff0c\u6d6a\u8d39\u4eba\u5de5\u6807\u6ce8\u9884\u7b97\u3002\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5e73\u6ed1\u7f6e\u4fe1\u5206\u6570\u548ctop-K\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u57fa\u4e8e\u4f2a\u8986\u76d6\u7387\u7684\u5728\u7ebf\u6743\u91cd\u66f4\u65b0\u7b97\u6cd5\uff0c\u57df\u504f\u79fb\u68c0\u6d4b\u5668\u81ea\u9002\u5e94\u4eba\u7c7b\u76d1\u7763\uff0c\u4ee5\u53ca\u5e73\u8861\u4eba\u5de5\u6807\u6ce8\u548c\u6a21\u578b\u6807\u6ce8\u6570\u636e\u7684\u5206\u9636\u6bb5\u66f4\u65b0\u65b9\u6848\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cCPATTA\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684ATTA\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u7ea65%\u3002", "conclusion": "CPATTA\u901a\u8fc7\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u548c\u9ad8\u6548\u7684\u6807\u6ce8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u52a8\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2509.25696", "categories": ["cs.LG", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25696", "abs": "https://arxiv.org/abs/2509.25696", "authors": ["Takuya Fujimura", "Kota Dohi", "Natsuo Yamashita", "Yohei Kawaguchi"], "title": "Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?", "comment": null, "summary": "Time-series question answering (TSQA) tasks face significant challenges due\nto the lack of labeled data. Alternatively, with recent advancements in\nlarge-scale models, vision-language models (VLMs) have demonstrated the\npotential to analyze time-series signals in a zero-shot manner. In this paper,\nwe propose a training approach that uses pseudo labels generated by a VLM.\nAlthough VLMs can produce incorrect labels, TSQA models can still be\neffectively trained based on the property that deep neural networks are\ninherently robust to such noisy labels. Our experimental results demonstrate\nthat TSQA models are not only successfully trained with pseudo labels, but also\nsurpass the performance of the VLM itself by leveraging a large amount of\nunlabeled data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\u6765\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5373\u4f7fVLMs\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u6807\u7b7e\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u566a\u58f0\u6807\u7b7e\u7684\u56fa\u6709\u9c81\u68d2\u6027\u4ecd\u80fd\u6709\u6548\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7f3a\u4e4f\u7684\u6311\u6218\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u6807\u7b7e\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u566a\u58f0\u6807\u7b7e\u7684\u56fa\u6709\u9c81\u68d2\u6027\u6765\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTSQA\u6a21\u578b\u4e0d\u4ec5\u80fd\u7528\u4f2a\u6807\u7b7e\u6210\u529f\u8bad\u7ec3\uff0c\u8fd8\u80fd\u901a\u8fc7\u5229\u7528\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u8d85\u8d8aVLM\u672c\u8eab\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u8bad\u7ec3\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.25704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25704", "abs": "https://arxiv.org/abs/2509.25704", "authors": ["Cheng Guo", "Giuseppe L'Erario", "Giulio Romualdi", "Mattia Leonori", "Marta Lorenzini", "Arash Ajoudani", "Daniele Pucci"], "title": "Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs", "comment": null, "summary": "Accurate and physically feasible human motion prediction is crucial for safe\nand seamless human-robot collaboration. While recent advancements in human\nmotion capture enable real-time pose estimation, the practical value of many\nexisting approaches is limited by the lack of future predictions and\nconsideration of physical constraints. Conventional motion prediction schemes\nrely heavily on past poses, which are not always available in real-world\nscenarios. To address these limitations, we present a physics-informed learning\nframework that integrates domain knowledge into both training and inference to\npredict human motion using inertial measurements from only 5 IMUs. We propose a\nnetwork that accounts for the spatial characteristics of human movements.\nDuring training, we incorporate forward and differential kinematics functions\nas additional loss components to regularize the learned joint predictions. At\nthe inference stage, we refine the prediction from the previous iteration to\nupdate a joint state buffer, which is used as extra inputs to the network.\nExperimental results demonstrate that our approach achieves high accuracy,\nsmooth transitions between motions, and generalizes well to unseen subjects", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u4ec55\u4e2aIMU\u7684\u60ef\u6027\u6d4b\u91cf\u6765\u9884\u6d4b\u4eba\u4f53\u8fd0\u52a8\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\u6765\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u672a\u6765\u8fd0\u52a8\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e14\u672a\u5145\u5206\u8003\u8651\u7269\u7406\u7ea6\u675f\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5b9e\u7528\u6027\u6709\u9650\u3002\u4f20\u7edf\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5386\u53f2\u59ff\u6001\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5e76\u4e0d\u603b\u662f\u53ef\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8003\u8651\u4eba\u4f53\u8fd0\u52a8\u7a7a\u95f4\u7279\u6027\u7684\u7f51\u7edc\uff0c\u5728\u8bad\u7ec3\u65f6\u52a0\u5165\u524d\u5411\u548c\u5fae\u5206\u8fd0\u52a8\u5b66\u51fd\u6570\u4f5c\u4e3a\u989d\u5916\u635f\u5931\u5206\u91cf\u6765\u6b63\u5219\u5316\u5173\u8282\u9884\u6d4b\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6765\u66f4\u65b0\u5173\u8282\u72b6\u6001\u7f13\u51b2\u533a\u4f5c\u4e3a\u7f51\u7edc\u989d\u5916\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3001\u8fd0\u52a8\u95f4\u5e73\u6ed1\u8fc7\u6e21\uff0c\u5e76\u4e14\u5bf9\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u5bf9\u8c61\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4b\u4eba\u4f53\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.25706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25706", "abs": "https://arxiv.org/abs/2509.25706", "authors": ["Rostyslav Olshevskyi", "Madeline Navarro", "Santiago Segarra"], "title": "Adaptive Graph Coarsening for Efficient GNN Training", "comment": null, "summary": "We propose an adaptive graph coarsening method to jointly learn graph neural\nnetwork (GNN) parameters and merge nodes via K-means clustering during\ntraining. As real-world graphs grow larger, processing them directly becomes\nincreasingly challenging and sometimes infeasible. Tailoring algorithms to\nlarge-scale data may sacrifice performance, so we instead consider graph\nreduction to decrease the amount of data used during training. In particular,\nwe propose a method to simultaneously train a GNN and coarsen its graph by\npartitioning nodes via K-means clustering based on their embeddings. Unlike\npast graph coarsening works, our approach allows us to merge nodes during\ntraining. Not only does this preclude coarsening as a preprocessing step, but\nour node clusters can adapt to the learning task instead of relying solely on\ngraph connectivity and features. Thus, our method is amenable to scenarios that\nare challenging for other methods, such as heterophilic data. We validate our\napproach on both homophilic and heterophilic node classification datasets. We\nfurther visualize relationships between node embeddings and their corresponding\nclusters to illustrate that our coarsened graph adapts to the learning task\nduring training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u56fe\u7c97\u5316\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8054\u5408\u5b66\u4e60GNN\u53c2\u6570\u5e76\u901a\u8fc7K-means\u805a\u7c7b\u5408\u5e76\u8282\u70b9\uff0c\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\u3002", "motivation": "\u968f\u7740\u73b0\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u89c4\u6a21\u4e0d\u65ad\u589e\u5927\uff0c\u76f4\u63a5\u5904\u7406\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\u751a\u81f3\u4e0d\u53ef\u884c\u3002\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5b9a\u5236\u7b97\u6cd5\u53ef\u80fd\u4f1a\u727a\u7272\u6027\u80fd\uff0c\u56e0\u6b64\u8003\u8651\u901a\u8fc7\u56fe\u7f29\u51cf\u6765\u51cf\u5c11\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684\u6570\u636e\u91cf\u3002", "method": "\u540c\u65f6\u8bad\u7ec3GNN\u5e76\u901a\u8fc7\u57fa\u4e8e\u8282\u70b9\u5d4c\u5165\u7684K-means\u805a\u7c7b\u5bf9\u56fe\u8fdb\u884c\u7c97\u5316\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5408\u5e76\u8282\u70b9\uff0c\u4f7f\u8282\u70b9\u805a\u7c7b\u80fd\u591f\u9002\u5e94\u5b66\u4e60\u4efb\u52a1\u800c\u975e\u4ec5\u4f9d\u8d56\u56fe\u8fde\u63a5\u6027\u548c\u7279\u5f81\u3002", "result": "\u5728\u540c\u6027\u604b\u548c\u5f02\u8d28\u6027\u8282\u70b9\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u8282\u70b9\u5d4c\u5165\u4e0e\u5176\u5bf9\u5e94\u805a\u7c7b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc1\u660e\u7c97\u5316\u56fe\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u80fd\u591f\u9002\u5e94\u5b66\u4e60\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5176\u4ed6\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u573a\u666f\uff08\u5982\u5f02\u8d28\u6027\u6570\u636e\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u7c97\u5316\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.25712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25712", "abs": "https://arxiv.org/abs/2509.25712", "authors": ["Dengming Zhang", "Xiaowen Ma", "Zhenliang Ni", "Zhenkai Wu", "Han Shu", "Xin Jiang", "Xinghao Chen"], "title": "Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking", "comment": null, "summary": "Model merging, which combines multiple domain-specialized experts into a\nsingle model, offers a practical path to endow Large Language Models (LLMs) and\nMultimodal Large Language Models (MLLMs) with broad capabilities without the\ncost of joint training or serving many models. However, training-free methods\nrely on hand-tuned coefficients, whereas training-based methods primarily align\nparameters rather than downstream task behavior and typically treat all layers\nuniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a\ntraining-light method that learns a small set of layer-wise coefficients using\nonly unlabeled calibration data. The coefficients are optimized to explicitly\nalign the merged model's hidden states and logits with those of the\ncorresponding experts, with a coefficient regularizer for stability and\ntask-weighted losses for controllable trade-offs. To capture inter-layer\nvariation, Expert Merging++ augments this design with importance-guided\nchunking: a normalized layer-importance metric, derived from learned\ncoefficients, task-vector magnitudes, and parameter counts, allocates more\nchunk-wise coefficients to high-importance layers while keeping low-importance\nlayers lightweight. The result is a label-free, parameter-efficient, and\nscalable approach to multi-expert model merging across LLMs and MLLMs. Across\nMLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our\nmethod surpasses strong training-free and training-based merging baselines,\nwith Expert Merging++ delivering further gains and, in some cases, even\nexceeding supervised Mixture Training. The source code is available at\nhttps://github.com/Littleor/ExpertMerging.", "AI": {"tldr": "\u63d0\u51faExpert Merging\u548cExpert Merging++\u4e24\u79cd\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u7cfb\u6570\u5b66\u4e60\u548c\u91cd\u8981\u6027\u5f15\u5bfc\u5206\u5757\uff0c\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u5b9e\u73b0\u591a\u4e13\u5bb6\u6a21\u578b\u7684\u9ad8\u6548\u5408\u5e76\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u624b\u52a8\u8c03\u53c2\u3001\u53c2\u6570\u5bf9\u9f50\u800c\u975e\u4efb\u52a1\u884c\u4e3a\u5bf9\u9f50\u3001\u5ffd\u7565\u5c42\u95f4\u5f02\u8d28\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u5408\u5e76\u65b9\u6848\u3002", "method": "Expert Merging\uff1a\u4f7f\u7528\u65e0\u6807\u7b7e\u6821\u51c6\u6570\u636e\u5b66\u4e60\u5c42\u95f4\u7cfb\u6570\uff0c\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u548c\u5bf9\u6570\u5bf9\u9f50\u4f18\u5316\uff0c\u52a0\u5165\u6b63\u5219\u5316\u548c\u4efb\u52a1\u52a0\u6743\u635f\u5931\u3002Expert Merging++\uff1a\u5728\u57fa\u7840\u4e0a\u589e\u52a0\u91cd\u8981\u6027\u5f15\u5bfc\u5206\u5757\uff0c\u6839\u636e\u7cfb\u6570\u3001\u4efb\u52a1\u5411\u91cf\u548c\u53c2\u6570\u6570\u91cf\u5206\u914d\u5206\u5757\u7cfb\u6570\u3002", "result": "\u5728MLLM\uff08InternVL\u3001Qwen2-VL\uff09\u548cLLM\uff08Mistral\uff09\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0cExpert Merging++\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8fc7\u76d1\u7763\u6df7\u5408\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65e0\u6807\u7b7e\u3001\u53c2\u6570\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u591a\u4e13\u5bb6\u6a21\u578b\u5408\u5e76\u65b9\u6848\uff0c\u9002\u7528\u4e8eLLM\u548cMLLM\u3002"}}
{"id": "2509.25713", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25713", "abs": "https://arxiv.org/abs/2509.25713", "authors": ["Hyunsoo Song", "Minjung Gim", "Jaewoong Choi"], "title": "Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation", "comment": "28 pages, 17 figures", "summary": "Flow matching has recently emerged as a powerful framework for\ncontinuous-time generative modeling. However, when applied to long-tailed\ndistributions, standard flow matching suffers from majority bias, producing\nminority modes with low fidelity and failing to match the true class\nproportions. In this work, we propose Unbalanced Optimal Transport Reweighted\nFlow Matching (UOT-RFM), a novel framework for generative modeling under\nclass-imbalanced (long-tailed) distributions that operates without any class\nlabel information. Our method constructs the conditional vector field using\nmini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias\nthrough a principled inverse reweighting strategy. The reweighting relies on a\nlabel-free majority score, defined as the density ratio between the target\ndistribution and the UOT marginal. This score quantifies the degree of majority\nbased on the geometric structure of the data, without requiring class labels.\nBy incorporating this score into the training objective, UOT-RFM theoretically\nrecovers the target distribution with first-order correction ($k=1$) and\nempirically improves tail-class generation through higher-order corrections ($k\n> 1$). Our model outperforms existing flow matching baselines on long-tailed\nbenchmarks, while maintaining competitive performance on balanced datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86UOT-RFM\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u7684\u9006\u91cd\u52a0\u6743\u7b56\u7565\u89e3\u51b3\u6d41\u5339\u914d\u5728\u957f\u5c3e\u5206\u5e03\u4e2d\u7684\u591a\u6570\u7c7b\u504f\u5dee\u95ee\u9898", "motivation": "\u6807\u51c6\u6d41\u5339\u914d\u5728\u957f\u5c3e\u5206\u5e03\u4e2d\u4f1a\u4ea7\u751f\u591a\u6570\u7c7b\u504f\u5dee\uff0c\u5bfc\u81f4\u5c11\u6570\u7c7b\u751f\u6210\u8d28\u91cf\u4f4e\u4e14\u65e0\u6cd5\u5339\u914d\u771f\u5b9e\u7c7b\u522b\u6bd4\u4f8b", "method": "\u4f7f\u7528\u5c0f\u6279\u91cf\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u6784\u5efa\u6761\u4ef6\u5411\u91cf\u573a\uff0c\u901a\u8fc7\u57fa\u4e8e\u76ee\u6807\u5206\u5e03\u4e0eUOT\u8fb9\u7f18\u5bc6\u5ea6\u6bd4\u7684\u6807\u7b7e\u65e0\u5173\u591a\u6570\u5206\u6570\u8fdb\u884c\u9006\u91cd\u52a0\u6743", "result": "\u5728\u957f\u5c3e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6d41\u5339\u914d\u57fa\u7ebf\uff0c\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b", "conclusion": "UOT-RFM\u80fd\u591f\u6709\u6548\u7f13\u89e3\u591a\u6570\u7c7b\u504f\u5dee\uff0c\u65e0\u9700\u7c7b\u522b\u6807\u7b7e\u4fe1\u606f\u5373\u53ef\u6539\u5584\u957f\u5c3e\u5206\u5e03\u751f\u6210\u8d28\u91cf"}}
{"id": "2509.25715", "categories": ["cs.LG", "cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25715", "abs": "https://arxiv.org/abs/2509.25715", "authors": ["Hanghui Guo", "Shimin Di", "Pasquale De Meo", "Zhangze Chen", "Jia Zhu"], "title": "MuPlon: Multi-Path Causal Optimization for Claim Verification through Controlling Confounding", "comment": "8 pages", "summary": "As a critical task in data quality control, claim verification aims to curb\nthe spread of misinformation by assessing the truthfulness of claims based on a\nwide range of evidence. However, traditional methods often overlook the complex\ninteractions between evidence, leading to unreliable verification results. A\nstraightforward solution represents the claim and evidence as a fully connected\ngraph, which we define as the Claim-Evidence Graph (C-E Graph). Nevertheless,\nclaim verification methods based on fully connected graphs face two primary\nconfounding challenges, Data Noise and Data Biases. To address these\nchallenges, we propose a novel framework, Multi-Path Causal Optimization\n(MuPlon). MuPlon integrates a dual causal intervention strategy, consisting of\nthe back-door path and front-door path. In the back-door path, MuPlon dilutes\nnoisy node interference by optimizing node probability weights, while\nsimultaneously strengthening the connections between relevant evidence nodes.\nIn the front-door path, MuPlon extracts highly relevant subgraphs and\nconstructs reasoning paths, further applying counterfactual reasoning to\neliminate data biases within these paths. The experimental results demonstrate\nthat MuPlon outperforms existing methods and achieves state-of-the-art\nperformance.", "AI": {"tldr": "\u63d0\u51faMuPlon\u6846\u67b6\u89e3\u51b3\u58f0\u660e\u9a8c\u8bc1\u4e2d\u7684\u6570\u636e\u548c\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u56e0\u679c\u5e72\u9884\u7b56\u7565\u4f18\u5316\u8bc1\u636e\u56fe\u7ed3\u6784\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u4f20\u7edf\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\u5ffd\u89c6\u8bc1\u636e\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7ed3\u679c\u3002\u5b8c\u5168\u8fde\u63a5\u56fe\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u566a\u58f0\u548c\u504f\u89c1\u4e24\u5927\u6311\u6218", "method": "MuPlon\u6846\u67b6\u96c6\u6210\u540e\u95e8\u8def\u5f84\u548c\u524d\u95e8\u8def\u5f84\u7684\u53cc\u56e0\u679c\u5e72\u9884\u7b56\u7565\uff1a\u540e\u95e8\u8def\u5f84\u4f18\u5316\u8282\u70b9\u6982\u7387\u6743\u91cd\u7a00\u91ca\u566a\u58f0\u5e72\u6270\uff0c\u524d\u95e8\u8def\u5f84\u63d0\u53d6\u76f8\u5173\u5b50\u56fe\u6784\u5efa\u63a8\u7406\u8def\u5f84\u5e76\u5e94\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u6d88\u9664\u504f\u89c1", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMuPlon\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "MuPlon\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u58f0\u660e\u9a8c\u8bc1\u4e2d\u7684\u6570\u636e\u566a\u58f0\u548c\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9a8c\u8bc1\u53ef\u9760\u6027"}}
{"id": "2509.25719", "categories": ["cs.LG", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25719", "abs": "https://arxiv.org/abs/2509.25719", "authors": ["Haozhe Lei", "Hao Guo", "Tommy Svensson", "Sundeep Rangan"], "title": "Beyond Point Estimates: Likelihood-Based Full-Posterior Wireless Localization", "comment": null, "summary": "Modern wireless systems require not only position estimates, but also\nquantified uncertainty to support planning, control, and radio resource\nmanagement. We formulate localization as posterior inference of an unknown\ntransmitter location from receiver measurements. We propose Monte Carlo\nCandidate-Likelihood Estimation (MC-CLE), which trains a neural scoring network\nusing Monte Carlo sampling to compare true and candidate transmitter locations.\nWe show that in line-of-sight simulations with a multi-antenna receiver, MC-CLE\nlearns critical properties including angular ambiguity and front-to-back\nantenna patterns. MC-CLE also achieves lower cross-entropy loss relative to a\nuniform baseline and Gaussian posteriors. alternatives under a uniform-loss\nmetric.", "AI": {"tldr": "\u63d0\u51faMC-CLE\u65b9\u6cd5\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8bc4\u5206\u5668\uff0c\u7528\u4e8e\u65e0\u7ebf\u5b9a\u4f4d\u4e2d\u7684\u540e\u9a8c\u63a8\u65ad\uff0c\u80fd\u591f\u5b66\u4e60\u5173\u952e\u4f20\u64ad\u7279\u6027\u5e76\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u8fd8\u9700\u8981\u91cf\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u652f\u6301\u89c4\u5212\u3001\u63a7\u5236\u548c\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\u3002", "method": "\u5c06\u5b9a\u4f4d\u5efa\u6a21\u4e3a\u4ece\u63a5\u6536\u5668\u6d4b\u91cf\u4e2d\u63a8\u65ad\u672a\u77e5\u53d1\u5c04\u5668\u4f4d\u7f6e\u7684\u540e\u9a8c\u63a8\u7406\u95ee\u9898\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u5019\u9009\u4f3c\u7136\u4f30\u8ba1(MC-CLE)\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8bc4\u5206\u7f51\u7edc\u6765\u6bd4\u8f83\u771f\u5b9e\u548c\u5019\u9009\u53d1\u5c04\u5668\u4f4d\u7f6e\u3002", "result": "\u5728\u89c6\u8ddd\u591a\u5929\u7ebf\u63a5\u6536\u5668\u4eff\u771f\u4e2d\uff0cMC-CLE\u5b66\u4e60\u4e86\u5305\u62ec\u89d2\u5ea6\u6a21\u7cca\u548c\u524d\u5411\u540e\u5411\u5929\u7ebf\u6a21\u5f0f\u5728\u5185\u7684\u5173\u952e\u7279\u6027\uff0c\u76f8\u5bf9\u4e8e\u5747\u5300\u57fa\u7ebf\u548c\u9ad8\u65af\u540e\u9a8c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3002", "conclusion": "MC-CLE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u65e0\u7ebf\u4f20\u64ad\u7279\u6027\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2509.25727", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25727", "abs": "https://arxiv.org/abs/2509.25727", "authors": ["Huikang Su", "Dengyun Peng", "Zifeng Zhuang", "YuHan Liu", "Qiguang Chen", "Donglin Wang", "Qinghe Liu"], "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Offline safe reinforcement learning aims to learn policies that satisfy\npredefined safety constraints from static datasets. Existing\nsequence-model-based methods condition action generation on symmetric input\ntokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry:\nreturn-to-go (RTG) serves as a flexible performance target, while cost-to-go\n(CTG) should represent a rigid safety boundary. This symmetric conditioning\nleads to unreliable constraint satisfaction, especially when encountering\nout-of-distribution cost trajectories. To address this, we propose\nBoundary-to-Region (B2R), a framework that enables asymmetric conditioning\nthrough cost signal realignment . B2R redefines CTG as a boundary constraint\nunder a fixed safety budget, unifying the cost distribution of all feasible\ntrajectories while preserving reward structures. Combined with rotary\npositional embeddings , it enhances exploration within the safe region.\nExperimental results show that B2R satisfies safety constraints in 35 out of 38\nsafety-critical tasks while achieving superior reward performance over baseline\nmethods. This work highlights the limitations of symmetric token conditioning\nand establishes a new theoretical and practical approach for applying sequence\nmodels to safe RL. Our code is available at https://github.com/HuikangSu/B2R.", "AI": {"tldr": "B2R\u6846\u67b6\u901a\u8fc7\u975e\u5bf9\u79f0\u6761\u4ef6\u5316\u89e3\u51b3\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u5c06\u6210\u672c\u5230\u76ee\u6807\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8fb9\u754c\u7ea6\u675f\uff0c\u572838\u4e2a\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d35\u4e2a\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\uff0c\u4e14\u5956\u52b1\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5e8f\u5217\u6a21\u578b\u7684\u65b9\u6cd5\u5bf9\u56de\u62a5\u5230\u76ee\u6807\u548c\u6210\u672c\u5230\u76ee\u6807\u4f7f\u7528\u5bf9\u79f0\u8f93\u5165\u6807\u8bb0\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u5185\u5728\u4e0d\u5bf9\u79f0\u6027\uff1a\u56de\u62a5\u5230\u76ee\u6807\u662f\u7075\u6d3b\u7684\u6027\u80fd\u76ee\u6807\uff0c\u800c\u6210\u672c\u5230\u76ee\u6807\u5e94\u8be5\u662f\u4e25\u683c\u7684\u5b89\u5168\u8fb9\u754c\u3002\u8fd9\u79cd\u5bf9\u79f0\u6761\u4ef6\u5316\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u7ea6\u675f\u6ee1\u8db3\u3002", "method": "\u63d0\u51faBoundary-to-Region (B2R)\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u672c\u4fe1\u53f7\u91cd\u65b0\u5bf9\u9f50\u5b9e\u73b0\u975e\u5bf9\u79f0\u6761\u4ef6\u5316\uff0c\u5c06\u6210\u672c\u5230\u76ee\u6807\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56fa\u5b9a\u5b89\u5168\u9884\u7b97\u4e0b\u7684\u8fb9\u754c\u7ea6\u675f\uff0c\u7edf\u4e00\u6240\u6709\u53ef\u884c\u8f68\u8ff9\u7684\u6210\u672c\u5206\u5e03\uff0c\u540c\u65f6\u7ed3\u5408\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u589e\u5f3a\u5b89\u5168\u533a\u57df\u5185\u7684\u63a2\u7d22\u3002", "result": "B2R\u572838\u4e2a\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u768435\u4e2a\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u5956\u52b1\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u5bf9\u79f0\u6807\u8bb0\u6761\u4ef6\u5316\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5c06\u5e8f\u5217\u6a21\u578b\u5e94\u7528\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2509.25730", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.25730", "abs": "https://arxiv.org/abs/2509.25730", "authors": ["Indu Kant Deo", "Akash Venkateshwaran", "Rajeev K. Jaiman"], "title": "A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise", "comment": "26 pages, 13 figures", "summary": "Ship traffic is an increasing source of underwater radiated noise in coastal\nwaters, motivating real-time digital twins of ocean acoustics for operational\nnoise mitigation. We present a physics-guided probabilistic framework to\npredict three-dimensional transmission loss in realistic ocean environments. As\na case study, we consider the Salish Sea along shipping routes from the Pacific\nOcean to the Port of Vancouver. A dataset of over 30 million source-receiver\npairs was generated with a Gaussian beam solver across seasonal sound speed\nprofiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We\nfirst assess sparse variational Gaussian processes (SVGP) and then incorporate\nphysics-based mean functions combining spherical spreading with\nfrequency-dependent absorption. To capture nonlinear effects, we examine deep\nsigma-point processes and stochastic variational deep kernel learning. The\nfinal framework integrates four components: (i) a learnable physics-informed\nmean that represents dominant propagation trends, (ii) a convolutional encoder\nfor bathymetry along the source-receiver track, (iii) a neural encoder for\nsource, receiver, and frequency coordinates, and (iv) a residual SVGP layer\nthat provides calibrated predictive uncertainty. This probabilistic digital\ntwin facilitates the construction of sound-exposure bounds and worst-case\nscenarios for received levels. We further demonstrate the application of the\nframework to ship speed optimization, where predicted transmission loss\ncombined with near-field source models provides sound exposure level estimates\nfor minimizing acoustic impacts on marine mammals. The proposed framework\nadvances uncertainty-aware digital twins for ocean acoustics and illustrates\nhow physics-guided machine learning can support sustainable maritime\noperations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u5f15\u5bfc\u7684\u6982\u7387\u6846\u67b6\u6765\u9884\u6d4b\u771f\u5b9e\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u4e09\u7ef4\u4f20\u8f93\u635f\u5931\uff0c\u7528\u4e8e\u8239\u8236\u4ea4\u901a\u566a\u58f0\u7f13\u89e3\uff0c\u5e76\u5728Salish Sea\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u8239\u8236\u901f\u5ea6\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u8239\u8236\u4ea4\u901a\u662f\u6cbf\u6d77\u6c34\u57df\u6c34\u4e0b\u8f90\u5c04\u566a\u58f0\u65e5\u76ca\u589e\u957f\u7684\u6765\u6e90\uff0c\u9700\u8981\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u6280\u672f\u6765\u64cd\u4f5c\u6027\u5730\u7f13\u89e3\u566a\u58f0\u3002", "method": "\u7ed3\u5408\u7269\u7406\u5f15\u5bfc\u7684\u5747\u503c\u51fd\u6570\uff08\u7403\u9762\u6269\u5c55\u548c\u9891\u7387\u76f8\u5173\u5438\u6536\uff09\u3001\u5377\u79ef\u7f16\u7801\u5668\uff08\u7528\u4e8e\u6c34\u6df1\u5730\u5f62\uff09\u3001\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5668\uff08\u7528\u4e8e\u6e90\u3001\u63a5\u6536\u5668\u548c\u9891\u7387\u5750\u6807\uff09\u4ee5\u53ca\u6b8b\u5dee\u7a00\u758f\u53d8\u5206\u9ad8\u65af\u8fc7\u7a0b\u5c42\uff0c\u6784\u5efa\u6982\u7387\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u6821\u51c6\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4fbf\u4e8e\u6784\u5efa\u58f0\u66b4\u9732\u8fb9\u754c\u548c\u6700\u574f\u60c5\u51b5\u63a5\u6536\u6c34\u5e73\uff0c\u5e76\u5e94\u7528\u4e8e\u8239\u8236\u901f\u5ea6\u4f18\u5316\u4ee5\u51cf\u5c11\u5bf9\u6d77\u6d0b\u54fa\u4e73\u52a8\u7269\u7684\u58f0\u5b66\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u63a8\u8fdb\u4e86\u6d77\u6d0b\u58f0\u5b66\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6570\u5b57\u5b6a\u751f\uff0c\u5c55\u793a\u4e86\u7269\u7406\u5f15\u5bfc\u7684\u673a\u5668\u5b66\u4e60\u5982\u4f55\u652f\u6301\u53ef\u6301\u7eed\u7684\u6d77\u4e8b\u8fd0\u8425\u3002"}}
{"id": "2509.25742", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25742", "abs": "https://arxiv.org/abs/2509.25742", "authors": ["Yanan Zhao", "Feng Ji", "Jingyang Dai", "Jiaze Ma", "Wee Peng Tay"], "title": "Less is More: Towards Simple Graph Contrastive Learning", "comment": "Submitted to ICLR 2026", "summary": "Graph Contrastive Learning (GCL) has shown strong promise for unsupervised\ngraph representation learning, yet its effectiveness on heterophilic graphs,\nwhere connected nodes often belong to different classes, remains limited. Most\nexisting methods rely on complex augmentation schemes, intricate encoders, or\nnegative sampling, which raises the question of whether such complexity is\ntruly necessary in this challenging setting. In this work, we revisit the\nfoundations of supervised and unsupervised learning on graphs and uncover a\nsimple yet effective principle for GCL: mitigating node feature noise by\naggregating it with structural features derived from the graph topology. This\nobservation suggests that the original node features and the graph structure\nnaturally provide two complementary views for contrastive learning. Building on\nthis insight, we propose an embarrassingly simple GCL model that uses a GCN\nencoder to capture structural features and an MLP encoder to isolate node\nfeature noise. Our design requires neither data augmentation nor negative\nsampling, yet achieves state-of-the-art results on heterophilic benchmarks with\nminimal computational and memory overhead, while also offering advantages in\nhomophilic graphs in terms of complexity, scalability, and robustness. We\nprovide theoretical justification for our approach and validate its\neffectiveness through extensive experiments, including robustness evaluations\nagainst both black-box and white-box adversarial attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u8282\u70b9\u7279\u5f81\u548c\u56fe\u7ed3\u6784\u7279\u5f81\u6765\u7f13\u89e3\u5f02\u8d28\u56fe\u4e2d\u7684\u7279\u5f81\u566a\u58f0\u95ee\u9898\uff0c\u65e0\u9700\u590d\u6742\u7684\u6570\u636e\u589e\u5f3a\u6216\u8d1f\u91c7\u6837\u3002", "motivation": "\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u5f02\u8d28\u56fe\uff08\u8fde\u63a5\u8282\u70b9\u901a\u5e38\u5c5e\u4e8e\u4e0d\u540c\u7c7b\u522b\uff09\u4e0a\u6548\u679c\u6709\u9650\uff0c\u4e14\u4f9d\u8d56\u590d\u6742\u7684\u589e\u5f3a\u65b9\u6848\u3001\u7f16\u7801\u5668\u6216\u8d1f\u91c7\u6837\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u79cd\u590d\u6742\u6027\u662f\u5426\u5fc5\u8981\u3002", "method": "\u4f7f\u7528GCN\u7f16\u7801\u5668\u6355\u83b7\u7ed3\u6784\u7279\u5f81\uff0cMLP\u7f16\u7801\u5668\u5206\u79bb\u8282\u70b9\u7279\u5f81\u566a\u58f0\uff0c\u5c06\u539f\u59cb\u8282\u70b9\u7279\u5f81\u548c\u56fe\u7ed3\u6784\u4f5c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u7684\u4e24\u4e2a\u4e92\u8865\u89c6\u56fe\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\u6216\u8d1f\u91c7\u6837\u3002", "result": "\u5728\u5f02\u8d28\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6548\u679c\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u6700\u5c0f\uff0c\u5728\u540c\u7c7b\u56fe\u4e2d\u4e5f\u5177\u6709\u590d\u6742\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u4f18\u52bf\u3002", "conclusion": "\u8bc1\u660e\u4e86\u7b80\u5355\u65b9\u6cd5\u5728\u5f02\u8d28\u56fe\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u5e76\u901a\u8fc7\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2509.25743", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25743", "abs": "https://arxiv.org/abs/2509.25743", "authors": ["Xiang Zhang", "Kun Wei", "Xu Yang", "Chenghao Xu", "Su Yan", "Cheng Deng"], "title": "Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space", "comment": null, "summary": "As Large Language Models (LLMs) become increasingly prevalent, their security\nvulnerabilities have already drawn attention. Machine unlearning is introduced\nto seek to mitigate these risks by removing the influence of undesirable data.\nHowever, existing methods not only rely on the retained dataset to preserve\nmodel utility, but also suffer from cumulative catastrophic utility loss under\ncontinuous unlearning requests. To solve this dilemma, we propose a novel\nmethod, called Rotation Control Unlearning (RCU), which leverages the\nrotational salience weight of RCU to quantify and control the unlearning degree\nin the continuous unlearning process. The skew symmetric loss is designed to\nconstruct the existence of the cognitive rotation space, where the changes of\nrotational angle can simulate the continuous unlearning process. Furthermore,\nwe design an orthogonal rotation axes regularization to enforce mutually\nperpendicular rotation directions for continuous unlearning requests,\neffectively minimizing interference and addressing cumulative catastrophic\nutility loss. Experiments on multiple datasets confirm that our method without\nretained dataset achieves SOTA performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRotation Control Unlearning (RCU)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u65cb\u8f6c\u663e\u8457\u6027\u6743\u91cd\u6765\u91cf\u5316\u63a7\u5236\u8fde\u7eed\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u7684\u9057\u5fd8\u7a0b\u5ea6\uff0c\u65e0\u9700\u4fdd\u7559\u6570\u636e\u96c6\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5176\u5b89\u5168\u6f0f\u6d1e\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u4e0d\u4ec5\u4f9d\u8d56\u4fdd\u7559\u6570\u636e\u96c6\u6765\u4fdd\u6301\u6a21\u578b\u6548\u7528\uff0c\u800c\u4e14\u5728\u8fde\u7eed\u9057\u5fd8\u8bf7\u6c42\u4e0b\u4f1a\u906d\u53d7\u7d2f\u79ef\u707e\u96be\u6027\u6548\u7528\u635f\u5931\u3002", "method": "RCU\u65b9\u6cd5\u5229\u7528\u65cb\u8f6c\u663e\u8457\u6027\u6743\u91cd\u91cf\u5316\u63a7\u5236\u9057\u5fd8\u7a0b\u5ea6\uff0c\u8bbe\u8ba1\u659c\u5bf9\u79f0\u635f\u5931\u6784\u5efa\u8ba4\u77e5\u65cb\u8f6c\u7a7a\u95f4\uff0c\u901a\u8fc7\u65cb\u8f6c\u89d2\u5ea6\u53d8\u5316\u6a21\u62df\u8fde\u7eed\u9057\u5fd8\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u6b63\u4ea4\u65cb\u8f6c\u8f74\u6b63\u5219\u5316\u6765\u6700\u5c0f\u5316\u5e72\u6270\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4fdd\u7559\u6570\u636e\u96c6\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RCU\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u7684\u7d2f\u79ef\u707e\u96be\u6027\u6548\u7528\u635f\u5931\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u7f13\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.25762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25762", "abs": "https://arxiv.org/abs/2509.25762", "authors": ["Kaizhuo Yan", "Yingjie Yu", "Yifan Yu", "Haizhong Zheng", "Fan Lai"], "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap", "comment": "Kaizhuo Yan and Yingjie Yu contributed equally to this work", "summary": "Proximal Policy Optimization (PPO)-based reinforcement learning from human\nfeedback (RLHF) is a widely adopted paradigm for aligning large language models\n(LLMs) with human preferences. However, its training pipeline suffers from\nsubstantial inefficiencies due to sequential multi-model dependencies (e.g.,\nreward model depends on actor outputs) and long-tail response lengths, where a\nfew long responses straggle the stage completion. We present OPPO, a novel,\nlightweight, and model-agnostic PPO-based RLHF framework that improves training\nefficiency by overlapping pipeline execution. OPPO introduces two novel\ntechniques: (1) Intra-step overlap, which streams upstream model outputs (e.g.,\nactor model) in right-sized chunks, enabling the downstream model (e.g.,\nreward) to begin prefill while the upstream continues decoding; and (2)\nInter-step overlap, which adaptively overcommits a few prompts and defers long\ngenerations to future steps, mitigating tail latency without discarding partial\nwork. OPPO integrates easily with existing PPO implementations with a few lines\nof code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF\ntraining by $1.8 \\times-2.8 \\times$ and improves GPU utilization by $1.4\n\\times-2.1 \\times$ without compromising training convergence.", "AI": {"tldr": "OPPO\u662f\u4e00\u4e2a\u65b0\u9896\u7684PPO-based RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u91cd\u53e0\u6267\u884c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u52a0\u901f1.8-2.8\u500d\uff0c\u63d0\u5347GPU\u5229\u7528\u73871.4-2.1\u500d", "motivation": "\u4f20\u7edfPPO-based RLHF\u8bad\u7ec3\u5b58\u5728\u6548\u7387\u95ee\u9898\uff1a\u987a\u5e8f\u591a\u6a21\u578b\u4f9d\u8d56\u548c\u957f\u5c3e\u54cd\u5e94\u957f\u5ea6\u5bfc\u81f4\u8bad\u7ec3\u74f6\u9888", "method": "\u5f15\u5165\u4e24\u79cd\u65b0\u6280\u672f\uff1a1) \u5185\u90e8\u6b65\u9aa4\u91cd\u53e0 - \u6d41\u5f0f\u4f20\u8f93\u4e0a\u6e38\u6a21\u578b\u8f93\u51fa\uff0c\u4f7f\u4e0b\u6e38\u6a21\u578b\u80fd\u63d0\u524d\u9884\u586b\u5145\uff1b2) \u8de8\u6b65\u9aa4\u91cd\u53e0 - \u81ea\u9002\u5e94\u8d85\u91cf\u63d0\u4ea4\u63d0\u793a\uff0c\u63a8\u8fdf\u957f\u751f\u6210\u5230\u540e\u7eed\u6b65\u9aa4", "result": "OPPO\u663e\u8457\u52a0\u901fPPO-based RLHF\u8bad\u7ec31.8-2.8\u500d\uff0cGPU\u5229\u7528\u7387\u63d0\u53471.4-2.1\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u8bad\u7ec3\u6536\u655b", "conclusion": "OPPO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u53ea\u9700\u5c11\u91cf\u4ee3\u7801\u4fee\u6539\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709PPO\u5b9e\u73b0\u4e2d\uff0c\u6709\u6548\u89e3\u51b3RLHF\u8bad\u7ec3\u6548\u7387\u95ee\u9898"}}
{"id": "2509.25775", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25775", "abs": "https://arxiv.org/abs/2509.25775", "authors": ["Amber Srivastava", "Salar Basiri", "Srinivasa Salapaka"], "title": "Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions", "comment": "This work is under submission to ICLR 2026. Please cite the arXiv\n  version until the final version is published", "summary": "Clustering arises in a wide range of problem formulations, yet most existing\napproaches assume that the entities under clustering are passive and strictly\nconform to their assigned groups. In reality, entities often exhibit local\nautonomy, overriding prescribed associations in ways not fully captured by\nfeature representations. Such autonomy can substantially reshape clustering\noutcomes -- altering cluster compositions, geometry, and cardinality -- with\nsignificant downstream effects on inference and decision-making. We introduce\nautonomy-aware clustering, a reinforcement (RL) learning framework that learns\nand accounts for the influence of local autonomy without requiring prior\nknowledge of its form. Our approach integrates RL with a deterministic\nannealing (DA) procedure, where, to determine underlying clusters, DA naturally\npromotes exploration in early stages of annealing and transitions to\nexploitation later. We also show that the annealing procedure exhibits phase\ntransitions that enable design of efficient annealing schedules. To further\nenhance adaptability, we propose the Adaptive Distance Estimation Network\n(ADEN), a transformer-based attention model that learns dependencies between\nentities and cluster representatives within the RL loop, accommodates\nvariable-sized inputs and outputs, and enables knowledge transfer across\ndiverse problem instances. Empirical results show that our framework closely\naligns with underlying data dynamics: even without explicit autonomy models, it\nachieves solutions close to the ground truth (gap ~3-4%), whereas ignoring\nautonomy leads to substantially larger gaps (~35-40%). The code and data are\npublicly available at https://github.com/salar96/AutonomyAwareClustering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u611f\u77e5\u805a\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u5b66\u4e60\u548c\u8003\u8651\u5b9e\u4f53\u7684\u5c40\u90e8\u81ea\u4e3b\u6027\u5bf9\u805a\u7c7b\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u65e0\u9700\u4e8b\u5148\u4e86\u89e3\u81ea\u4e3b\u6027\u7684\u5177\u4f53\u5f62\u5f0f\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b9e\u4f53\u901a\u5e38\u5177\u6709\u5c40\u90e8\u81ea\u4e3b\u6027\uff0c\u4f1a\u4ee5\u7279\u5f81\u8868\u793a\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u7684\u65b9\u5f0f\u8986\u76d6\u9884\u8bbe\u7684\u5173\u8054\u5173\u7cfb\uff0c\u8fd9\u79cd\u81ea\u4e3b\u6027\u4f1a\u663e\u8457\u6539\u53d8\u805a\u7c7b\u7ed3\u679c\uff0c\u5f71\u54cd\u4e0b\u6e38\u7684\u63a8\u7406\u548c\u51b3\u7b56\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u786e\u5b9a\u6027\u9000\u706b\u8fc7\u7a0b\uff0c\u5728\u9000\u706b\u65e9\u671f\u4fc3\u8fdb\u63a2\u7d22\uff0c\u540e\u671f\u8f6c\u5411\u5229\u7528\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u8ddd\u79bb\u4f30\u8ba1\u7f51\u7edc(ADEN)\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5b66\u4e60\u5b9e\u4f53\u4e0e\u805a\u7c7b\u4ee3\u8868\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u81ea\u4e3b\u6027\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u4e5f\u80fd\u83b7\u5f97\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\u7684\u89e3\u51b3\u65b9\u6848\uff08\u5dee\u8ddd\u7ea63-4%\uff09\uff0c\u800c\u5ffd\u7565\u81ea\u4e3b\u6027\u4f1a\u5bfc\u81f4\u66f4\u5927\u7684\u5dee\u8ddd\uff08\u7ea635-40%\uff09\u3002", "conclusion": "\u81ea\u4e3b\u611f\u77e5\u805a\u7c7b\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u6570\u636e\u52a8\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u805a\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u5c40\u90e8\u81ea\u4e3b\u6027\u7684\u5b9e\u4f53\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.25777", "categories": ["cs.LG", "stat.ML", "68W20, 90B50, 91B06, 68T01, 68Q32", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.25777", "abs": "https://arxiv.org/abs/2509.25777", "authors": ["Jianyu Xu", "Vidhi Jain", "Bryan Wilder", "Aarti Singh"], "title": "Online Decision Making with Generative Action Sets", "comment": "34 pages, 2 figures (including 5 subfigures)", "summary": "With advances in generative AI, decision-making agents can now dynamically\ncreate new actions during online learning, but action generation typically\nincurs costs that must be balanced against potential benefits. We study an\nonline learning problem where an agent can generate new actions at any time\nstep by paying a one-time cost, with these actions becoming permanently\navailable for future use. The challenge lies in learning the optimal sequence\nof two-fold decisions: which action to take and when to generate new ones,\nfurther complicated by the triangular tradeoffs among exploitation, exploration\nand $\\textit{creation}$. To solve this problem, we propose a doubly-optimistic\nalgorithm that employs Lower Confidence Bounds (LCB) for action selection and\nUpper Confidence Bounds (UCB) for action generation. Empirical evaluation on\nhealthcare question-answering datasets demonstrates that our approach achieves\nfavorable generation-quality tradeoffs compared to baseline strategies. From\ntheoretical perspectives, we prove that our algorithm achieves the optimal\nregret of $O(T^{\\frac{d}{d+2}}d^{\\frac{d}{d+2}} + d\\sqrt{T\\log T})$, providing\nthe first sublinear regret bound for online learning with expanding action\nspaces.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cc\u91cd\u4e50\u89c2\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u4e2d\u52a8\u6001\u751f\u6210\u65b0\u52a8\u4f5c\u7684\u95ee\u9898\uff0c\u5e73\u8861\u52a8\u4f5c\u751f\u6210\u6210\u672c\u4e0e\u6f5c\u5728\u6536\u76ca\uff0c\u5728\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u5728\u5728\u7ebf\u5b66\u4e60\u4e2d\u52a8\u6001\u521b\u5efa\u65b0\u52a8\u4f5c\uff0c\u4f46\u52a8\u4f5c\u751f\u6210\u901a\u5e38\u9700\u8981\u6210\u672c\uff0c\u9700\u8981\u5e73\u8861\u6210\u672c\u4e0e\u6f5c\u5728\u6536\u76ca\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5229\u7528\u3001\u63a2\u7d22\u548c\u521b\u5efa\u4e4b\u95f4\u7684\u4e09\u89d2\u6743\u8861\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u4e50\u89c2\u7b97\u6cd5\uff0c\u4f7f\u7528\u4e0b\u7f6e\u4fe1\u754c(LCB)\u8fdb\u884c\u52a8\u4f5c\u9009\u62e9\uff0c\u4f7f\u7528\u4e0a\u7f6e\u4fe1\u754c(UCB)\u8fdb\u884c\u52a8\u4f5c\u751f\u6210\uff0c\u4ee5\u4f18\u5316\u52a8\u4f5c\u751f\u6210\u65f6\u673a\u548c\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u751f\u6210-\u8d28\u91cf\u6743\u8861\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7b97\u6cd5\u8fbe\u5230\u4e86O(T^(d/(d+2))d^(d/(d+2)) + d\u221a(TlogT))\u7684\u6700\u4f18\u9057\u61be\u754c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u9996\u4e2a\u4e3a\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u6b21\u7ebf\u6027\u9057\u61be\u754c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5229\u7528\u3001\u63a2\u7d22\u548c\u521b\u5efa\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2509.25778", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25778", "abs": "https://arxiv.org/abs/2509.25778", "authors": ["Prosper Rosaire Mama Assandje", "Teumsa Aboubakar", "Dongho Joseph", "Takemi Nakamura"], "title": "A Hamiltonian driven Geometric Construction of Neural Networks on the Lognormal Statistical Manifold", "comment": null, "summary": "Bridging information geometry with machine learning, this paper presents a\nmethod for constructing neural networks intrinsically on statistical manifolds.\nWe demonstrate this approach by formulating a neural network architecture\ndirectly on the lognormal statistical manifold. The construction is driven by\nthe Hamiltonian system that is equivalent to the gradient flow on this\nmanifold. First, we define the network's input values using the coordinate\nsystem of this Hamiltonian dynamics, naturally embedded in the Poincare disk.\nThe core of our contribution lies in the derivation of the network's components\nfrom geometric principles: the rotation component of the synaptic weight matrix\nis determined by the Lie group action of SU(1,1) on the disk, while the\nactivation function emerges from the symplectic structure of the system. We\nsubsequently obtain the complete weight matrix, including its translation\nvector, and the resulting output values. This work shows that the lognormal\nmanifold can be seamlessly viewed as a neural manifold, with its geometric\nproperties dictating a unique and interpretable neural network structure. The\nproposed method offers a new paradigm for building learning systems grounded in\nthe differential geometry of their underlying parameter spaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7edf\u8ba1\u6d41\u5f62\u4e0a\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ee5\u5bf9\u6570\u6b63\u6001\u7edf\u8ba1\u6d41\u5f62\u4e3a\u4f8b\uff0c\u901a\u8fc7\u54c8\u5bc6\u987f\u7cfb\u7edf\u63a8\u5bfc\u51fa\u7f51\u7edc\u67b6\u6784\u7684\u5404\u4e2a\u7ec4\u4ef6\u3002", "motivation": "\u5c06\u4fe1\u606f\u51e0\u4f55\u4e0e\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5728\u7edf\u8ba1\u6d41\u5f62\u4e0a\u5185\u5728\u5730\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\uff0c\u4e3a\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u57fa\u4e8e\u5fae\u5206\u51e0\u4f55\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u5229\u7528\u5bf9\u6570\u6b63\u6001\u7edf\u8ba1\u6d41\u5f62\u4e0a\u7684\u54c8\u5bc6\u987f\u7cfb\u7edf\uff0c\u4ece\u51e0\u4f55\u539f\u7406\u63a8\u5bfc\u7f51\u7edc\u7ec4\u4ef6\uff1a\u8f93\u5165\u503c\u4f7f\u7528\u54c8\u5bc6\u987f\u52a8\u529b\u5b66\u7684\u5750\u6807\u7cfb\uff0c\u6743\u91cd\u77e9\u9635\u7684\u65cb\u8f6c\u90e8\u5206\u7531SU(1,1)\u674e\u7fa4\u4f5c\u7528\u786e\u5b9a\uff0c\u6fc0\u6d3b\u51fd\u6570\u4ece\u7cfb\u7edf\u7684\u8f9b\u7ed3\u6784\u4ea7\u751f\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5b8c\u6574\u7684\u6743\u91cd\u77e9\u9635\uff08\u5305\u62ec\u5e73\u79fb\u5411\u91cf\uff09\u548c\u8f93\u51fa\u503c\uff0c\u8bc1\u660e\u4e86\u5bf9\u6570\u6b63\u6001\u6d41\u5f62\u53ef\u4ee5\u65e0\u7f1d\u5730\u89c6\u4e3a\u795e\u7ecf\u6d41\u5f62\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u5e95\u5c42\u53c2\u6570\u7a7a\u95f4\u7684\u5fae\u5206\u51e0\u4f55\u6027\u8d28\u6765\u6784\u5efa\u72ec\u7279\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u3002"}}
{"id": "2509.25788", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25788", "abs": "https://arxiv.org/abs/2509.25788", "authors": ["Zhizhou Zhang", "Youjia Wu", "Kaixuan Zhang", "Yanjia Wang"], "title": "From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining", "comment": null, "summary": "Industrial design evaluation often relies on high-fidelity simulations of\ngoverning partial differential equations (PDEs). While accurate, these\nsimulations are computationally expensive, making dense exploration of design\nspaces impractical. Operator learning has emerged as a promising approach to\naccelerate PDE solution prediction; however, its effectiveness is often limited\nby the scarcity of labeled physics-based data. At the same time, large numbers\nof geometry-only candidate designs are readily available but remain largely\nuntapped. We propose a two-stage framework to better exploit this abundant,\nphysics-agnostic resource and improve supervised operator learning under\nlimited labeled data. In Stage 1, we pretrain an autoencoder on a geometry\nreconstruction task to learn an expressive latent representation without PDE\nlabels. In Stage 2, the neural operator is trained in a standard supervised\nmanner to predict PDE solutions, using the pretrained latent embeddings as\ninputs instead of raw point clouds. Transformer-based architectures are adopted\nfor both the autoencoder and the neural operator to handle point cloud data and\nintegrate both stages seamlessly. Across four PDE datasets and three\nstate-of-the-art transformer-based neural operators, our approach consistently\nimproves prediction accuracy compared to models trained directly on raw point\ncloud inputs. These results demonstrate that representations from\nphysics-agnostic pretraining provide a powerful foundation for data-efficient\noperator learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u91cd\u5efa\u9884\u8bad\u7ec3\u6765\u63d0\u5347\u6709\u9650\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u7b97\u5b50\u5b66\u4e60\u6027\u80fd", "motivation": "\u5de5\u4e1a\u8bbe\u8ba1\u8bc4\u4f30\u4f9d\u8d56\u9ad8\u4fdd\u771fPDE\u6a21\u62df\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff1b\u540c\u65f6\u5927\u91cf\u65e0\u7269\u7406\u6807\u7b7e\u7684\u51e0\u4f55\u8bbe\u8ba1\u6570\u636e\u672a\u88ab\u5145\u5206\u5229\u7528", "method": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u5728\u51e0\u4f55\u91cd\u5efa\u4efb\u52a1\u4e0a\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u5d4c\u5165\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\u795e\u7ecf\u7b97\u5b50\u9884\u6d4bPDE\u89e3", "result": "\u5728\u56db\u4e2aPDE\u6570\u636e\u96c6\u548c\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u4e8etransformer\u7684\u795e\u7ecf\u7b97\u5b50\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u76f4\u63a5\u5728\u539f\u59cb\u70b9\u4e91\u8f93\u5165\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6301\u7eed\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6", "conclusion": "\u6765\u81ea\u65e0\u7269\u7406\u9884\u8bad\u7ec3\u7684\u8868\u5f81\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u7b97\u5b50\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u57fa\u7840"}}
{"id": "2509.25800", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25800", "abs": "https://arxiv.org/abs/2509.25800", "authors": ["Gongxu Luo", "Loka Li", "Guangyi Chen", "Haoyue Dai", "Kun Zhang"], "title": "Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data", "comment": null, "summary": "Interventional causal discovery seeks to identify causal relations by\nleveraging distributional changes introduced by interventions, even in the\npresence of latent confounders. Beyond the spurious dependencies induced by\nlatent confounders, we highlight a common yet often overlooked challenge in the\nproblem due to post-treatment selection, in which samples are selectively\nincluded in datasets after interventions. This fundamental challenge widely\nexists in biological studies; for example, in gene expression analysis, both\nobservational and interventional samples are retained only if they meet quality\ncontrol criteria (e.g., highly active cells). Neglecting post-treatment\nselection may introduce spurious dependencies and distributional changes under\ninterventions, which can mimic causal responses, thereby distorting causal\ndiscovery results and challenging existing causal formulations. To address\nthis, we introduce a novel causal formulation that explicitly models\npost-treatment selection and reveals how its differential reactions to\ninterventions can distinguish causal relations from selection patterns,\nallowing us to go beyond traditional equivalence classes toward the underlying\ntrue causal structure. We then characterize its Markov properties and propose a\nFine-grained Interventional equivalence class, named FI-Markov equivalence,\nrepresented by a new graphical diagram, F-PAG. Finally, we develop a provably\nsound and complete algorithm, F-FCI, to identify causal relations, latent\nconfounders, and post-treatment selection up to $\\mathcal{FI}$-Markov\nequivalence, using both observational and interventional data. Experimental\nresults on synthetic and real-world datasets demonstrate that our method\nrecovers causal relations despite the presence of both selection and latent\nconfounders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u5e72\u9884\u540e\u9009\u62e9\u504f\u5dee\u7684\u56e0\u679c\u53d1\u73b0\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u7814\u7a76\u4e2d\u5e38\u89c1\u7684\u6837\u672c\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86F-FCI\u7b97\u6cd5\u6765\u8bc6\u522b\u56e0\u679c\u5173\u7cfb\u3001\u6f5c\u5728\u6df7\u6742\u56e0\u5b50\u548c\u9009\u62e9\u504f\u5dee\u3002", "motivation": "\u4f20\u7edf\u5e72\u9884\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5e72\u9884\u540e\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u8fd9\u5728\u751f\u7269\u7814\u7a76\u4e2d\u5f88\u5e38\u89c1\uff08\u5982\u57fa\u56e0\u8868\u8fbe\u5206\u6790\u4e2d\u7684\u8d28\u91cf\u63a7\u5236\uff09\uff0c\u4f1a\u5bfc\u81f4\u865a\u5047\u4f9d\u8d56\u5173\u7cfb\u5e76\u626d\u66f2\u56e0\u679c\u53d1\u73b0\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u7684\u56e0\u679c\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u660e\u786e\u5efa\u6a21\u5e72\u9884\u540e\u9009\u62e9\u8fc7\u7a0b\uff0c\u5b9a\u4e49\u4e86FI-Markov\u7b49\u4ef7\u7c7b\u548cF-PAG\u56fe\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e86F-FCI\u7b97\u6cd5\u6765\u8bc6\u522b\u56e0\u679c\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5b58\u5728\u9009\u62e9\u504f\u5dee\u548c\u6f5c\u5728\u6df7\u6742\u56e0\u5b50\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u771f\u5b9e\u7684\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5e72\u9884\u540e\u9009\u62e9\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u8d85\u8d8a\u4f20\u7edf\u7684\u7b49\u4ef7\u7c7b\u9650\u5236\uff0c\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u5e95\u5c42\u771f\u5b9e\u56e0\u679c\u7ed3\u6784\u3002"}}
{"id": "2509.25804", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.25804", "abs": "https://arxiv.org/abs/2509.25804", "authors": ["Vaskar Chakma", "Ju Xiaolin", "Heling Cao", "Xue Feng", "Ji Xiaodong", "Pan Haiyan", "Gao Zhan"], "title": "CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG", "comment": null, "summary": "This study aims to develop and evaluate an ensemble machine learning-based\nframework for the automatic detection of Wide QRS Complex Tachycardia (WCT)\nfrom ECG signals, emphasizing diagnostic accuracy and interpretability using\nExplainable AI. The proposed system integrates ensemble learning techniques,\ni.e., an optimized Random Forest known as CardioForest, and models like XGBoost\nand LightGBM. The models were trained and tested on ECG data from the publicly\navailable MIMIC-IV dataset. The testing was carried out with the assistance of\naccuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error\nrate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations)\nwas used to ascertain model explainability and clinical relevance. The\nCardioForest model performed best on all metrics, achieving a test accuracy of\n94.95%, a balanced accuracy of 88.31%, and high precision and recall metrics.\nSHAP analysis confirmed the model's ability to rank the most relevant ECG\nfeatures, such as QRS duration, in accordance with clinical intuitions, thereby\nfostering trust and usability in clinical practice. The findings recognize\nCardioForest as an extremely dependable and interpretable WCT detection model.\nBeing able to offer accurate predictions and transparency through\nexplainability makes it a valuable tool to help cardiologists make timely and\nwell-informed diagnoses, especially for high-stakes and emergency scenarios.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u96c6\u6210\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4eceECG\u4fe1\u53f7\u4e2d\u81ea\u52a8\u68c0\u6d4b\u5bbdQRS\u6ce2\u5fc3\u52a8\u8fc7\u901f\uff0c\u5f3a\u8c03\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63d0\u9ad8\u5bbdQRS\u6ce2\u5fc3\u52a8\u8fc7\u901f\u7684\u81ea\u52a8\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u89e3\u91caAI\u589e\u5f3a\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u96c6\u6210\u5b66\u4e60\u6280\u672f\uff0c\u5305\u62ec\u4f18\u5316\u7684\u968f\u673a\u68ee\u6797\uff08CardioForest\uff09\u3001XGBoost\u548cLightGBM\u6a21\u578b\uff0c\u4f7f\u7528MIMIC-IV\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5e76\u5e94\u7528SHAP\u8fdb\u884c\u6a21\u578b\u89e3\u91ca\u3002", "result": "CardioForest\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523094.95%\uff0c\u5e73\u8861\u51c6\u786e\u7387\u4e3a88.31%\uff0cSHAP\u5206\u6790\u786e\u8ba4\u6a21\u578b\u80fd\u591f\u6839\u636e\u4e34\u5e8a\u76f4\u89c9\u5bf9ECG\u7279\u5f81\u8fdb\u884c\u6392\u5e8f\u3002", "conclusion": "CardioForest\u662f\u4e00\u4e2a\u9ad8\u5ea6\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684WCT\u68c0\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\u548c\u900f\u660e\u5ea6\uff0c\u5e2e\u52a9\u5fc3\u810f\u75c5\u4e13\u5bb6\u505a\u51fa\u53ca\u65f6\u548c\u660e\u667a\u7684\u8bca\u65ad\u3002"}}
{"id": "2509.25808", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25808", "abs": "https://arxiv.org/abs/2509.25808", "authors": ["Yuheng Zhang", "Wenlin Yao", "Changlong Yu", "Yao Liu", "Qingyu Yin", "Bing Yin", "Hyokun Yun", "Lihong Li"], "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse", "comment": null, "summary": "Large language models (LLMs) have achieved impressive reasoning performance,\nwith reinforcement learning with verifiable rewards (RLVR) emerging as a\nstandard paradigm for post-training. A representative algorithm, group relative\npolicy optimization (GRPO) (Shao et al., 2024), computes advantages by\nnormalizing outcome rewards within response groups, but suffers from a\nvanishing advantage issue when all responses in a group receive identical\nrewards. To address this issue, we propose Adaptive Rollout and Response Reuse\nPolicy Optimization (AR3PO), a sampling efficient RLVR algorithm that\nintroduces two novel techniques: adaptive rollout, which dynamically allocates\nmore responses to difficult prompts while saving computation on easier ones,\nand response reuse, which leverages previously generated correct responses to\nprovide useful training signals. We compare AR3PO with strong RLVR baselines on\nmultiple representative benchmarks using two different families of base models.\nAcross the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or\nsurpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the\nlarger 32B model, AR3PO achieves comparable performance to DAPO at similar\ntraining steps while maintaining substantially lower rollout cost.", "AI": {"tldr": "AR3PO\u662f\u4e00\u79cd\u91c7\u6837\u9ad8\u6548\u7684RLVR\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u54cd\u5e94\u91cd\u7528\u6280\u672f\u89e3\u51b3GRPO\u4e2d\u7684\u4f18\u52bf\u6d88\u5931\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eGRPO\uff0c\u4e0eDAPO\u6027\u80fd\u76f8\u5f53\u4f46\u663e\u8457\u964d\u4f4e\u91c7\u6837\u6210\u672c\u3002", "motivation": "\u89e3\u51b3GRPO\u7b97\u6cd5\u4e2d\u5f53\u54cd\u5e94\u7ec4\u5185\u6240\u6709\u54cd\u5e94\u83b7\u5f97\u76f8\u540c\u5956\u52b1\u65f6\u51fa\u73b0\u7684\u4f18\u52bf\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u91c7\u6837\uff08\u52a8\u6001\u4e3a\u56f0\u96be\u63d0\u793a\u5206\u914d\u66f4\u591a\u54cd\u5e94\uff09\u548c\u54cd\u5e94\u91cd\u7528\uff08\u5229\u7528\u5148\u524d\u751f\u6210\u7684\u6b63\u786e\u54cd\u5e94\u63d0\u4f9b\u8bad\u7ec3\u4fe1\u53f7\uff09\u4e24\u79cd\u65b0\u6280\u672f\u3002", "result": "\u57287B\u548c8B\u6a21\u578b\u4e0a\uff0cAR3PO\u4e00\u81f4\u4f18\u4e8eGRPO\uff0c\u4e0eDAPO\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u91c7\u6837\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe4.2\u500d\uff1b\u572832B\u6a21\u578b\u4e0a\uff0c\u4e0eDAPO\u6027\u80fd\u76f8\u5f53\u4f46\u91c7\u6837\u6210\u672c\u663e\u8457\u66f4\u4f4e\u3002", "conclusion": "AR3PO\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u54cd\u5e94\u91cd\u7528\u6709\u6548\u89e3\u51b3\u4e86GRPO\u7684\u4f18\u52bf\u6d88\u5931\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u91c7\u6837\u6210\u672c\uff0c\u4e3aRLVR\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25810", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25810", "abs": "https://arxiv.org/abs/2509.25810", "authors": ["Shenao Zhang", "Donghan Yu", "Yihao Feng", "Bowen Jin", "Zhaoran Wang", "John Peebles", "Zirui Wang"], "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL", "comment": null, "summary": "Large language models excel with reinforcement learning (RL), but fully\nunlocking this potential requires a mid-training stage. An effective\nmid-training phase should identify a compact set of useful actions and enable\nfast selection among them through online RL. We formalize this intuition by\npresenting the first theoretical result on how mid-training shapes\npost-training: it characterizes an action subspace that minimizes both the\nvalue approximation error from pruning and the RL error during subsequent\nplanning. Our analysis reveals two key determinants of mid-training\neffectiveness: pruning efficiency, which shapes the prior of the initial RL\npolicy, and its impact on RL convergence, which governs the extent to which\nthat policy can be improved via online interactions. These results suggest that\nmid-training is most effective when the decision space is compact and the\neffective horizon is short, highlighting the importance of operating in the\nspace of action abstractions rather than primitive actions. Building on these\ninsights, we propose Reasoning as Action Abstractions (RA3), a scalable\nmid-training algorithm. Specifically, we derive a sequential variational lower\nbound and optimize it by iteratively discovering temporally-consistent latent\nstructures via RL, followed by fine-tuning on the bootstrapped data.\nExperiments on code generation tasks demonstrate the effectiveness of our\napproach. Across multiple base models, RA3 improves the average performance on\nHumanEval and MBPP by 8 and 4 points over the base model and the next-token\nprediction baseline. Furthermore, RA3 achieves faster convergence and higher\nasymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and\nCodeforces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RA3\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e2d\u671f\u8bad\u7ec3\u6784\u5efa\u7d27\u51d1\u7684\u52a8\u4f5c\u62bd\u8c61\u7a7a\u95f4\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b8c\u5168\u91ca\u653e\u5176\u6f5c\u529b\u9700\u8981\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\u6765\u8bc6\u522b\u6709\u7528\u7684\u7d27\u51d1\u52a8\u4f5c\u96c6\uff0c\u5e76\u652f\u6301\u5feb\u901f\u5728\u7ebf\u9009\u62e9\u3002", "method": "\u63d0\u51faRA3\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a8\u5bfc\u5e8f\u5217\u53d8\u5206\u4e0b\u754c\uff0c\u8fed\u4ee3\u53d1\u73b0\u65f6\u95f4\u4e00\u81f4\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u5e76\u5728\u5f15\u5bfc\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\uff0cRA3\u5728HumanEval\u548cMBPP\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u5206\u522b\u6bd4\u57fa\u7840\u6a21\u578b\u548c\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u57fa\u7ebf\u63d0\u9ad8\u4e868\u5206\u548c4\u5206\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u548c\u66f4\u9ad8\u6e10\u8fd1\u6027\u80fd\u3002", "conclusion": "\u4e2d\u671f\u8bad\u7ec3\u5728\u51b3\u7b56\u7a7a\u95f4\u7d27\u51d1\u4e14\u6709\u6548\u89c6\u91ce\u8f83\u77ed\u65f6\u6700\u6709\u6548\uff0c\u5f3a\u8c03\u5728\u52a8\u4f5c\u62bd\u8c61\u7a7a\u95f4\u800c\u975e\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u64cd\u4f5c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.25824", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25824", "abs": "https://arxiv.org/abs/2509.25824", "authors": ["Jingqi Fan", "Canzhe Zhao", "Shuai Li", "Siwei Wang"], "title": "Decentralized Asynchronous Multi-player Bandits", "comment": null, "summary": "In recent years, multi-player multi-armed bandits (MP-MAB) have been\nextensively studied due to their wide applications in cognitive radio networks\nand Internet of Things systems. While most existing research on MP-MAB focuses\non synchronized settings, real-world systems are often decentralized and\nasynchronous, where players may enter or leave the system at arbitrary times,\nand do not have a global clock. This decentralized asynchronous setting\nintroduces two major challenges. First, without a global time, players cannot\nimplicitly coordinate their actions through time, making it difficult to avoid\ncollisions. Second, it is important to detect how many players are in the\nsystem, but doing so may cost a lot. In this paper, we address the challenges\nposed by such a fully asynchronous setting in a decentralized environment. We\ndevelop a novel algorithm in which players adaptively change between\nexploration and exploitation. During exploration, players uniformly pull their\narms, reducing the probability of collisions and effectively mitigating the\nfirst challenge. Meanwhile, players continue pulling arms currently exploited\nby others with a small probability, enabling them to detect when a player has\nleft, thereby addressing the second challenge. We prove that our algorithm\nachieves a regret of $\\mathcal{O}(\\sqrt{T \\log T} + {\\log T}/{\\Delta^2})$,\nwhere $\\Delta$ is the minimum expected reward gap between any two arms. To the\nbest of our knowledge, this is the first efficient MP-MAB algorithm in the\nasynchronous and decentralized environment. Extensive experiments further\nvalidate the effectiveness and robustness of our algorithm, demonstrating its\napplicability to real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5b8c\u5168\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u591a\u73a9\u5bb6\u591a\u81c2\u8001\u864e\u673a\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5207\u6362\u63a2\u7d22\u548c\u5229\u7528\u6765\u89e3\u51b3\u73a9\u5bb6\u78b0\u649e\u548c\u52a8\u6001\u73a9\u5bb6\u6570\u91cf\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u901a\u5e38\u662f\u53bb\u4e2d\u5fc3\u5316\u548c\u5f02\u6b65\u7684\uff0c\u73a9\u5bb6\u53ef\u4ee5\u968f\u65f6\u52a0\u5165\u6216\u79bb\u5f00\u7cfb\u7edf\uff0c\u4e14\u6ca1\u6709\u5168\u5c40\u65f6\u949f\uff0c\u8fd9\u5e26\u6765\u4e86\u73a9\u5bb6\u884c\u52a8\u534f\u8c03\u56f0\u96be\u548c\u52a8\u6001\u73a9\u5bb6\u6570\u91cf\u68c0\u6d4b\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u73a9\u5bb6\u5728\u63a2\u7d22\u9636\u6bb5\u5747\u5300\u62c9\u52a8\u81c2\u4ee5\u51cf\u5c11\u78b0\u649e\u6982\u7387\uff0c\u540c\u65f6\u4ee5\u8f83\u5c0f\u6982\u7387\u7ee7\u7eed\u62c9\u52a8\u5176\u4ed6\u73a9\u5bb6\u6b63\u5728\u5229\u7528\u7684\u81c2\u6765\u68c0\u6d4b\u73a9\u5bb6\u79bb\u5f00\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86$\\mathcal{O}(\\sqrt{T \\log T} + {\\log T}/{\\Delta^2})$\u7684\u9057\u61be\u754c\uff0c\u5176\u4e2d$\\Delta$\u662f\u4efb\u610f\u4e24\u81c2\u4e4b\u95f4\u6700\u5c0f\u671f\u671b\u5956\u52b1\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5728\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u9ad8\u6548MP-MAB\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2509.25826", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25826", "abs": "https://arxiv.org/abs/2509.25826", "authors": ["Kun Feng", "Shaocheng Lan", "Yuchen Fang", "Wenchao He", "Lintao Ma", "Xingyu Lu", "Kan Ren"], "title": "Kairos: Towards Adaptive and Generalizable Time Series Foundation Models", "comment": null, "summary": "Time series foundation models (TSFMs) have emerged as a powerful paradigm for\ntime series analysis, driven by large-scale pretraining on diverse data\ncorpora. However, time series inherently exhibit heterogeneous information\ndensity over time, influenced by system states and signal complexity,\npresenting significant modeling challenges especially in a zero-shot scenario.\nCurrent TSFMs rely on non-adaptive processing pipelines that fail to capture\nthis dynamic nature. For example, common tokenization strategies such as\nfixed-size patching enforce rigid observational granularity, limiting their\nability to adapt to varying information densities. Similarly, conventional\npositional encodings impose a uniform temporal scale, making it difficult to\nmodel diverse periodicities and trends across series. To overcome these\nlimitations, we propose Kairos, a flexible TSFM framework that integrates a\ndynamic patching tokenizer and an instance-adaptive positional embedding.\nKairos adaptively selects tokenization granularity and tailors positional\nencodings to the unique characteristics of each time series instance. Trained\non a large-scale Predictability-Stratified Time Series (PreSTS) corpus\ncomprising over 300 billion time points and adopting a multi-patch prediction\nstrategy in the inference stage, Kairos achieves superior performance with much\nfewer parameters on two common zero-shot benchmarks, GIFT-Eval and the\nTime-Series-Library benchmark, consistently outperforming established methods\nacross diverse tasks. The project page is at\nhttps://foundation-model-research.github.io/Kairos .", "AI": {"tldr": "Kairos\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5757\u6807\u8bb0\u5316\u548c\u5b9e\u4f8b\u81ea\u9002\u5e94\u4f4d\u7f6e\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u4fe1\u606f\u5bc6\u5ea6\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5728\u4e0d\u540c\u65f6\u95f4\u6bb5\u5177\u6709\u5f02\u6784\u7684\u4fe1\u606f\u5bc6\u5ea6\uff0c\u53d7\u7cfb\u7edf\u72b6\u6001\u548c\u4fe1\u53f7\u590d\u6742\u5ea6\u5f71\u54cd\u3002\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u91c7\u7528\u975e\u81ea\u9002\u5e94\u5904\u7406\u6d41\u7a0b\uff0c\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u52a8\u6001\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "method": "\u63d0\u51faKairos\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u5206\u5757\u6807\u8bb0\u5316\u5668\u548c\u5b9e\u4f8b\u81ea\u9002\u5e94\u4f4d\u7f6e\u5d4c\u5165\u3002\u8be5\u6846\u67b6\u6839\u636e\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u5b9e\u4f8b\u7684\u72ec\u7279\u7279\u5f81\u81ea\u9002\u5e94\u9009\u62e9\u6807\u8bb0\u5316\u7c92\u5ea6\u5e76\u5b9a\u5236\u4f4d\u7f6e\u7f16\u7801\u3002\u5728\u5305\u542b3000\u4ebf\u65f6\u95f4\u70b9\u7684\u5927\u89c4\u6a21\u53ef\u9884\u6d4b\u6027\u5206\u5c42\u65f6\u95f4\u5e8f\u5217\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u91c7\u7528\u591a\u5757\u9884\u6d4b\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u5e38\u89c1\u7684\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\uff08GIFT-Eval\u548c\u65f6\u95f4\u5e8f\u5217\u5e93\u57fa\u51c6\uff09\u4e0a\uff0cKairos\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Kairos\u901a\u8fc7\u81ea\u9002\u5e94\u5904\u7406\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u6784\u4fe1\u606f\u5bc6\u5ea6\u7684\u5efa\u6a21\u6311\u6218\uff0c\u4e3a\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u6846\u67b6\u3002"}}
{"id": "2509.25831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25831", "abs": "https://arxiv.org/abs/2509.25831", "authors": ["Seong-Hyeon Hwang", "Soyoung Choi", "Steven Euijong Whang"], "title": "MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Multimodal models often over-rely on dominant modalities, failing to achieve\noptimal performance. While prior work focuses on modifying training objectives\nor optimization procedures, data-centric solutions remain underexplored. We\npropose MIDAS, a novel data augmentation strategy that generates misaligned\nsamples with semantically inconsistent cross-modal information, labeled using\nunimodal confidence scores to compel learning from contradictory signals.\nHowever, this confidence-based labeling can still favor the more confident\nmodality. To address this within our misaligned samples, we introduce\nweak-modality weighting, which dynamically increases the loss weight of the\nleast confident modality, thereby helping the model fully utilize weaker\nmodality. Furthermore, when misaligned features exhibit greater similarity to\nthe aligned features, these misaligned samples pose a greater challenge,\nthereby enabling the model to better distinguish between classes. To leverage\nthis, we propose hard-sample weighting, which prioritizes such semantically\nambiguous misaligned samples. Experiments on multiple multimodal classification\nbenchmarks demonstrate that MIDAS significantly outperforms related baselines\nin addressing modality imbalance.", "AI": {"tldr": "MIDAS\u662f\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u591a\u6a21\u6001\u9519\u4f4d\u6837\u672c\u6765\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7ed3\u5408\u5f31\u6a21\u6001\u52a0\u6743\u548c\u96be\u6837\u672c\u52a0\u6743\u673a\u5236\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u5e38\u5e38\u8fc7\u5ea6\u4f9d\u8d56\u4e3b\u5bfc\u6a21\u6001\u800c\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4fee\u6539\u8bad\u7ec3\u76ee\u6807\u6216\u4f18\u5316\u8fc7\u7a0b\uff0c\u6570\u636e\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faMIDAS\u65b9\u6cd5\uff1a1\uff09\u751f\u6210\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u8de8\u6a21\u6001\u9519\u4f4d\u6837\u672c\uff1b2\uff09\u4f7f\u7528\u5355\u6a21\u6001\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u6807\u6ce8\uff1b3\uff09\u5f15\u5165\u5f31\u6a21\u6001\u52a0\u6743\u673a\u5236\uff0c\u52a8\u6001\u589e\u52a0\u6700\u4e0d\u81ea\u4fe1\u6a21\u6001\u7684\u635f\u5931\u6743\u91cd\uff1b4\uff09\u63d0\u51fa\u96be\u6837\u672c\u52a0\u6743\u673a\u5236\uff0c\u4f18\u5148\u5904\u7406\u8bed\u4e49\u6a21\u7cca\u7684\u9519\u4f4d\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMIDAS\u5728\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MIDAS\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u52a0\u6743\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u80fd\u591f\u5f3a\u5236\u6a21\u578b\u5b66\u4e60\u4ece\u77db\u76fe\u4fe1\u53f7\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5145\u5206\u5229\u7528\u8f83\u5f31\u6a21\u6001\u3002"}}
{"id": "2509.25837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25837", "abs": "https://arxiv.org/abs/2509.25837", "authors": ["Yeongmin Kim", "Donghyeok Shin", "Mina Kang", "Byeonghu Na", "Il-Chul Moon"], "title": "Distillation of Large Language Models via Concrete Score Matching", "comment": null, "summary": "Large language models (LLMs) deliver remarkable performance but are costly to\ndeploy, motivating knowledge distillation (KD) for efficient inference.\nExisting KD objectives typically match student and teacher probabilities via\nsoftmax, which blurs valuable logit information. While direct logit\ndistillation (DLD) mitigates softmax smoothing, it fails to account for logit\nshift invariance, thereby restricting the solution space. We propose Concrete\nScore Distillation (CSD), a discrete score-matching objective that overcomes\nboth softmax-induced smoothing and restrictions on the optimal solution set. We\nresolve the training instability and quadratic complexity of discrete\nscore-matching in autoregressive LLMs, and the resulting CSD objective aligns\nrelative logit differences across all vocabulary pairs between student and\nteacher with flexible weighting. We provide both mode-seeking and mode-covering\ninstances within our framework and evaluate CSD on task-agnostic\ninstruction-following and task-specific distillation using GPT-2-1.5B,\nOpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses\nrecent KD objectives, achieves favorable fidelity-diversity trade-offs, and\nyields complementary gains when combined with on-policy techniques,\ndemonstrating its scalability and effectiveness for LLM distillation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Concrete Score Distillation (CSD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5206\u6570\u5339\u914d\u514b\u670d\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u4e2dsoftmax\u5e73\u6ed1\u548clogit\u5e73\u79fb\u4e0d\u53d8\u6027\u7684\u9650\u5236\uff0c\u5728LLM\u84b8\u998f\u4e2d\u53d6\u5f97\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728softmax\u5e73\u6ed1\u5bfc\u81f4logit\u4fe1\u606f\u4e22\u5931\uff0c\u4ee5\u53ca\u76f4\u63a5logit\u84b8\u998f\u53d7\u9650\u4e8elogit\u5e73\u79fb\u4e0d\u53d8\u6027\u800c\u9650\u5236\u89e3\u7a7a\u95f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCSD\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5206\u6570\u5339\u914d\u5bf9\u9f50\u5b66\u751f\u548c\u6559\u5e08\u6a21\u578b\u5728\u6240\u6709\u8bcd\u6c47\u5bf9\u4e0a\u7684\u76f8\u5bf9logit\u5dee\u5f02\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52LLM\u4e2d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u3002", "result": "\u5728GPT-2-1.5B\u3001OpenLLaMA-7B\u548cGEMMA-7B-IT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSD\u6301\u7eed\u8d85\u8d8a\u73b0\u6709KD\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u4fdd\u771f\u5ea6-\u591a\u6837\u6027\u6743\u8861\uff0c\u4e0e\u7b56\u7565\u6280\u672f\u7ed3\u5408\u83b7\u5f97\u4e92\u8865\u589e\u76ca\u3002", "conclusion": "CSD\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3aLLM\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25841", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25841", "abs": "https://arxiv.org/abs/2509.25841", "authors": ["Suping Xu", "Chuyi Dai", "Ye Liu", "Lin Shang", "Xibei Yang", "Witold Pedrycz"], "title": "S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems", "comment": null, "summary": "Feature selection is crucial for fuzzy decision systems (FDSs), as it\nidentifies informative features and eliminates rule redundancy, thereby\nenhancing predictive performance and interpretability. Most existing methods\neither fail to directly align evaluation criteria with learning performance or\nrely solely on non-directional Euclidean distances to capture relationships\namong decision classes, which limits their ability to clarify decision\nboundaries. However, the spatial distribution of instances has a potential\nimpact on the clarity of such boundaries. Motivated by this, we propose\nSpatially-aware Separability-driven Feature Selection (S$^2$FS), a novel\nframework for FDSs guided by a spatially-aware separability criterion. This\ncriterion jointly considers within-class compactness and between-class\nseparation by integrating scalar-distances with spatial directional\ninformation, providing a more comprehensive characterization of class\nstructures. S$^2$FS employs a forward greedy strategy to iteratively select the\nmost discriminative features. Extensive experiments on ten real-world datasets\ndemonstrate that S$^2$FS consistently outperforms eight state-of-the-art\nfeature selection algorithms in both classification accuracy and clustering\nperformance, while feature visualizations further confirm the interpretability\nof the selected features.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u6a21\u7cca\u51b3\u7b56\u7cfb\u7edf\u7684\u7a7a\u95f4\u611f\u77e5\u53ef\u5206\u6027\u7279\u5f81\u9009\u62e9\u6846\u67b6S\u00b2FS\uff0c\u901a\u8fc7\u6574\u5408\u6807\u91cf\u8ddd\u79bb\u548c\u7a7a\u95f4\u65b9\u5411\u4fe1\u606f\u6765\u8054\u5408\u8003\u8651\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5206\u79bb\u6027\uff0c\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u805a\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u8981\u4e48\u672a\u80fd\u76f4\u63a5\u5bf9\u9f50\u8bc4\u4f30\u6807\u51c6\u4e0e\u5b66\u4e60\u6027\u80fd\uff0c\u8981\u4e48\u4ec5\u4f9d\u8d56\u975e\u65b9\u5411\u6027\u6b27\u6c0f\u8ddd\u79bb\u6765\u6355\u6349\u51b3\u7b56\u7c7b\u95f4\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u51b3\u7b56\u8fb9\u754c\u7684\u6e05\u6670\u5ea6\u3002\u5b9e\u4f8b\u7684\u7a7a\u95f4\u5206\u5e03\u5bf9\u51b3\u7b56\u8fb9\u754c\u6e05\u6670\u5ea6\u6709\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u63d0\u51faS\u00b2FS\u6846\u67b6\uff0c\u91c7\u7528\u7a7a\u95f4\u611f\u77e5\u53ef\u5206\u6027\u51c6\u5219\uff0c\u6574\u5408\u6807\u91cf\u8ddd\u79bb\u548c\u7a7a\u95f4\u65b9\u5411\u4fe1\u606f\u6765\u8054\u5408\u8003\u8651\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5206\u79bb\u6027\uff0c\u4f7f\u7528\u524d\u5411\u8d2a\u5a6a\u7b56\u7565\u8fed\u4ee3\u9009\u62e9\u6700\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\u3002", "result": "\u572810\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cS\u00b2FS\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u805a\u7c7b\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e8\u79cd\u6700\u5148\u8fdb\u7684\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\uff0c\u7279\u5f81\u53ef\u89c6\u5316\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6240\u9009\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "S\u00b2FS\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u53ef\u5206\u6027\u51c6\u5219\u6709\u6548\u63d0\u5347\u4e86\u6a21\u7cca\u51b3\u7b56\u7cfb\u7edf\u7684\u7279\u5f81\u9009\u62e9\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.25849", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25849", "abs": "https://arxiv.org/abs/2509.25849", "authors": ["Ziniu Li", "Congliang Chen", "Tianyun Yang", "Tian Ding", "Ruoyu Sun", "Ge Zhang", "Wenhao Huang", "Zhi-Quan Luo"], "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation", "comment": null, "summary": "Large Language Models (LLMs) can self-improve through reinforcement learning,\nwhere they generate trajectories to explore and discover better solutions.\nHowever, this exploration process is computationally expensive, often forcing\ncurrent methods to assign limited exploration budgets to each task. This\nuniform allocation creates problematic edge cases: easy tasks consistently\nsucceed while difficult tasks consistently fail, both producing zero gradients\nduring training updates for the widely used Group Relative Policy Optimization\n(GRPO). We address this problem from the lens of exploration budget allocation.\nViewing each task's exploration as an \"item\" with a distinct \"value\" and\n\"cost\", we establish a connection to the classical knapsack problem. This\nformulation allows us to derive an optimal assignment rule that adaptively\ndistributes resources based on the model's current learning status. When\napplied to GRPO, our method increases the effective ratio of non-zero policy\ngradients by 20-40% during training. Acting as a computational \"free lunch\",\nour approach could reallocate exploration budgets from tasks where learning is\nsaturated to those where it is most impactful. This enables significantly\nlarger budgets (e.g., 93 rollouts) for especially challenging problems, which\nwould be computationally prohibitive under a uniform allocation. These\nimprovements translate to meaningful gains on mathematical reasoning\nbenchmarks, with average improvements of 2-4 points and peak gains of 9 points\non specific tasks. Notably, achieving comparable performance with traditional\nhomogeneous allocation would require about 2x the computational resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80cc\u5305\u95ee\u9898\u7684\u6700\u4f18\u63a2\u7d22\u9884\u7b97\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u5347LLM\u81ea\u6211\u6539\u8fdb\u7684\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5747\u5300\u5206\u914d\u5bfc\u81f4\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u63a2\u7d22\u9884\u7b97\u5206\u914d\u5b58\u5728\u8fb9\u7f18\u95ee\u9898\uff1a\u7b80\u5355\u4efb\u52a1\u603b\u662f\u6210\u529f\u800c\u56f0\u96be\u4efb\u52a1\u603b\u662f\u5931\u8d25\uff0c\u4e24\u8005\u5728GRPO\u8bad\u7ec3\u4e2d\u90fd\u4ea7\u751f\u96f6\u68af\u5ea6\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5c06\u6bcf\u4e2a\u4efb\u52a1\u7684\u63a2\u7d22\u89c6\u4e3a\u5177\u6709\u4e0d\u540c\"\u4ef7\u503c\"\u548c\"\u6210\u672c\"\u7684\"\u7269\u54c1\"\uff0c\u5efa\u7acb\u4e0e\u7ecf\u5178\u80cc\u5305\u95ee\u9898\u7684\u8054\u7cfb\uff0c\u63a8\u5bfc\u51fa\u57fa\u4e8e\u6a21\u578b\u5f53\u524d\u5b66\u4e60\u72b6\u6001\u7684\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u89c4\u5219\u3002", "result": "\u5728GRPO\u4e2d\uff0c\u975e\u96f6\u7b56\u7565\u68af\u5ea6\u7684\u6709\u6548\u6bd4\u4f8b\u63d0\u9ad8\u4e8620-40%\uff1b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53472-4\u5206\uff0c\u7279\u5b9a\u4efb\u52a1\u5cf0\u503c\u63d0\u53479\u5206\uff1b\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u5206\u914d\u8282\u7701\u7ea62\u500d\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u8ba1\u7b97\"\u514d\u8d39\u5348\u9910\"\uff0c\u80fd\u591f\u5c06\u9971\u548c\u4efb\u52a1\u7684\u63a2\u7d22\u9884\u7b97\u91cd\u65b0\u5206\u914d\u5230\u6700\u6709\u5f71\u54cd\u529b\u7684\u4efb\u52a1\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u81ea\u6211\u6539\u8fdb\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2509.25850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25850", "abs": "https://arxiv.org/abs/2509.25850", "authors": ["Animesh Jha", "Harshit Gupta", "Ananjan Nandi"], "title": "RL-Guided Data Selection for Language Model Finetuning", "comment": "To appear in NeurIPS 2025 Constrained Optimization for ML Workshop", "summary": "Data selection for finetuning Large Language Models (LLMs) can be framed as a\nbudget-constrained optimization problem: maximizing a model's downstream\nperformance under a strict training data budget. Solving this problem is\ngenerally intractable, and existing approximate approaches are\npretraining-oriented and transfer poorly to the fine-tuning setting. We\nreformulate this problem as a tractable Markov Decision Process (MDP) and train\nagents using various Reinforcement Learning (RL) methods to learn optimal data\nselection policies, guided by an efficient, proxy-model-based reward signal.\nAcross four datasets, training on a $5\\%$ subset selected by our approach\nmatches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy\npoints, while cutting wall-clock training time by up to $2 \\times$,\nhighlighting the promise of RL-guided data selection.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u4ec5\u4f7f\u75285%\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5168\u6570\u636e\u5fae\u8c03\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3LLM\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u9009\u62e9\u95ee\u9898\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u9884\u7b97\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5fae\u8c03\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5c06\u6570\u636e\u9009\u62e9\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u667a\u80fd\u4f53\u5b66\u4e60\u6700\u4f18\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75285%\u6570\u636e\u9009\u62e9\u7684\u6027\u80fd\u5339\u914d\u6216\u4f18\u4e8e\u5168\u6570\u636e\u5fae\u8c03\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe10.8\u4e2a\u767e\u5206\u70b9\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112\u500d\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6307\u5bfc\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e25\u683c\u6570\u636e\u9884\u7b97\u4e0b\u6700\u5927\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.25876", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25876", "abs": "https://arxiv.org/abs/2509.25876", "authors": ["Xinyu Zhang", "Aishik Deb", "Klaus Mueller"], "title": "Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space", "comment": "16 pages; 7 figures", "summary": "Policy-gradient methods such as Proximal Policy Optimization (PPO) are\ntypically updated along a single stochastic gradient direction, leaving the\nrich local structure of the parameter space unexplored. Previous work has shown\nthat the surrogate gradient is often poorly correlated with the true reward\nlandscape. Building on this insight, we visualize the parameter space spanned\nby policy checkpoints within an iteration and reveal that higher performing\nsolutions often lie in nearby unexplored regions. To exploit this opportunity,\nwe introduce ExploRLer, a pluggable pipeline that seamlessly integrates with\non-policy algorithms such as PPO and TRPO, systematically probing the\nunexplored neighborhoods of surrogate on-policy gradient updates. Without\nincreasing the number of gradient updates, ExploRLer achieves significant\nimprovements over baselines in complex continuous control environments. Our\nresults demonstrate that iteration-level exploration provides a practical and\neffective way to strengthen on-policy reinforcement learning and offer a fresh\nperspective on the limitations of the surrogate objective.", "AI": {"tldr": "ExploRLer\u662f\u4e00\u4e2a\u53ef\u63d2\u62d4\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e2d\u672a\u63a2\u7d22\u7684\u90bb\u57df\u533a\u57df\uff0c\u5728\u4e0d\u589e\u52a0\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff08\u5982PPO\uff09\u901a\u5e38\u6cbf\u7740\u5355\u4e00\u968f\u673a\u68af\u5ea6\u65b9\u5411\u66f4\u65b0\uff0c\u5ffd\u7565\u4e86\u53c2\u6570\u7a7a\u95f4\u7684\u4e30\u5bcc\u5c40\u90e8\u7ed3\u6784\u3002\u7814\u7a76\u53d1\u73b0\u66ff\u4ee3\u68af\u5ea6\u4e0e\u771f\u5b9e\u5956\u52b1\u5730\u5f62\u76f8\u5173\u6027\u8f83\u5dee\uff0c\u800c\u66f4\u9ad8\u6027\u80fd\u7684\u89e3\u5f80\u5f80\u4f4d\u4e8e\u9644\u8fd1\u672a\u63a2\u7d22\u533a\u57df\u3002", "method": "\u63d0\u51faExploRLer\u7ba1\u9053\uff0c\u4e0ePPO\u3001TRPO\u7b49\u5728\u7ebf\u7b56\u7565\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u6d4b\u66ff\u4ee3\u5728\u7ebf\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u7684\u672a\u63a2\u7d22\u90bb\u57df\u3002\u901a\u8fc7\u53ef\u89c6\u5316\u7b56\u7565\u68c0\u67e5\u70b9\u6240\u8de8\u8d8a\u7684\u53c2\u6570\u7a7a\u95f4\u6765\u8bc6\u522b\u9ad8\u6027\u80fd\u533a\u57df\u3002", "result": "\u5728\u4e0d\u589e\u52a0\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u7684\u60c5\u51b5\u4e0b\uff0cExploRLer\u5728\u590d\u6742\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fed\u4ee3\u7ea7\u63a2\u7d22\u4e3a\u52a0\u5f3a\u5728\u7ebf\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u66ff\u4ee3\u76ee\u6807\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.25906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25906", "abs": "https://arxiv.org/abs/2509.25906", "authors": ["Yiwei Li", "Shuai Wang", "Zhuojun Tian", "Xiuhua Wang", "Shijian Su"], "title": "Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation", "comment": "29 pages, 6 figures, submitted for peer review", "summary": "Federated Learning (FL) often adopts differential privacy (DP) to protect\nclient data, but the added noise required for privacy guarantees can\nsubstantially degrade model accuracy. To resolve this challenge, we propose\nmodel-splitting privacy-amplified federated learning (MS-PAFL), a novel\nframework that combines structural model splitting with statistical privacy\namplification. In this framework, each client's model is partitioned into a\nprivate submodel, retained locally, and a public submodel, shared for global\naggregation. The calibrated Gaussian noise is injected only into the public\nsubmodel, thereby confining its adverse impact while preserving the utility of\nthe local model. We further present a rigorous theoretical analysis that\ncharacterizes the joint privacy amplification achieved through random client\nparticipation and local data subsampling under this architecture. The analysis\nprovides tight bounds on both single-round and total privacy loss,\ndemonstrating that MS-PAFL significantly reduces the noise necessary to satisfy\na target privacy protection level. Extensive experiments validate our\ntheoretical findings, showing that MS-PAFL consistently attains a superior\nprivacy-utility trade-off and enables the training of highly accurate models\nunder strong privacy guarantees.", "AI": {"tldr": "\u63d0\u51faMS-PAFL\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5206\u5272\u548c\u9690\u79c1\u653e\u5927\u6280\u672f\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u51cf\u5c11\u5dee\u5206\u9690\u79c1\u566a\u58f0\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4f1a\u56e0\u6dfb\u52a0\u566a\u58f0\u800c\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7cbe\u5ea6\uff0c\u9700\u8981\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u5c06\u5ba2\u6237\u7aef\u6a21\u578b\u5206\u5272\u4e3a\u79c1\u6709\u5b50\u6a21\u578b\uff08\u672c\u5730\u4fdd\u7559\uff09\u548c\u516c\u5171\u5b50\u6a21\u578b\uff08\u5168\u5c40\u805a\u5408\uff09\uff0c\u4ec5\u5728\u516c\u5171\u5b50\u6a21\u578b\u6ce8\u5165\u6821\u51c6\u9ad8\u65af\u566a\u58f0\uff0c\u7ed3\u5408\u968f\u673a\u5ba2\u6237\u7aef\u53c2\u4e0e\u548c\u672c\u5730\u6570\u636e\u5b50\u91c7\u6837\u5b9e\u73b0\u9690\u79c1\u653e\u5927\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eMS-PAFL\u663e\u8457\u51cf\u5c11\u6ee1\u8db3\u76ee\u6807\u9690\u79c1\u4fdd\u62a4\u6c34\u5e73\u6240\u9700\u7684\u566a\u58f0\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u80fd\u83b7\u5f97\u4f18\u8d8a\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u5728\u5f3a\u9690\u79c1\u4fdd\u8bc1\u4e0b\u8bad\u7ec3\u9ad8\u7cbe\u5ea6\u6a21\u578b\u3002", "conclusion": "MS-PAFL\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5206\u5272\u548c\u9690\u79c1\u653e\u5927\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2509.25914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25914", "abs": "https://arxiv.org/abs/2509.25914", "authors": ["Yihang Lu", "Xianwei Meng", "Enhong Chen"], "title": "ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters", "comment": null, "summary": "Neural Forecasters (NFs) are a cornerstone of Long-term Time Series\nForecasting (LTSF). However, progress has been hampered by an overemphasis on\narchitectural complexity at the expense of fundamental forecasting principles.\nIn this work, we return to first principles to redesign the LTSF paradigm. We\nbegin by introducing a Multiple Neural Forecasting Theorem that provides a\ntheoretical basis for our approach. We propose Boosted Direct Output (BDO), a\nnovel forecasting strategy that synergistically combines the advantages of both\nAuto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the\nlearning process by smoothly tracking the model's parameters. Extensive\nexperiments show that these principled improvements enable a simple MLP to\nachieve state-of-the-art performance, outperforming recent, complex models in\nnearly all cases, without any specific considerations in the area. Finally, we\nempirically verify our theorem, establishing a dynamic performance bound and\nidentifying promising directions for future research. The code for review is\navailable at: .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7406\u8bba\u5b9a\u7406\u548cBoosted Direct Output\u65b9\u6cd5\uff0c\u4f7f\u7b80\u5355MLP\u6a21\u578b\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u590d\u6742\u6a21\u578b\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u9884\u6d4b\u5668\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8fc7\u5ea6\u5f3a\u8c03\u67b6\u6784\u590d\u6742\u6027\u800c\u5ffd\u89c6\u4e86\u57fa\u672c\u9884\u6d4b\u539f\u5219\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faMultiple Neural Forecasting Theorem\u4f5c\u4e3a\u7406\u8bba\u57fa\u7840\uff0c\u8bbe\u8ba1Boosted Direct Output\u7b56\u7565\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u76f4\u63a5\u8f93\u51fa\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5e73\u6ed1\u8ddf\u8e2a\u6a21\u578b\u53c2\u6570\u6765\u7a33\u5b9a\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u57fa\u4e8e\u539f\u5219\u7684\u6539\u8fdb\u4f7f\u7b80\u5355\u7684MLP\u6a21\u578b\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u6700\u8fd1\u7684\u590d\u6742\u6a21\u578b\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u56de\u5f52\u7b2c\u4e00\u6027\u539f\u7406\u91cd\u65b0\u8bbe\u8ba1\u4e86LTSF\u8303\u5f0f\uff0c\u5efa\u7acb\u4e86\u52a8\u6001\u6027\u80fd\u8fb9\u754c\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.25933", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25933", "abs": "https://arxiv.org/abs/2509.25933", "authors": ["Sven Br\u00e4ndle", "Till Aczel", "Andreas Plesner", "Roger Wattenhofer"], "title": "From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks", "comment": null, "summary": "Differentiable Logic Gate Networks (DLGNs) are a very fast and\nenergy-efficient alternative to conventional feed-forward networks. With\nlearnable combinations of logical gates, DLGNs enable fast inference by\nhardware-friendly execution. Since the concept of DLGNs has only recently\ngained attention, these networks are still in their developmental infancy,\nincluding the design and scalability of their output layer. To date, this\narchitecture has primarily been tested on datasets with up to ten classes.\n  This work examines the behavior of DLGNs on large multi-class datasets. We\ninvestigate its general expressiveness, its scalability, and evaluate\nalternative output strategies. Using both synthetic and real-world datasets, we\nprovide key insights into the importance of temperature tuning and its impact\non output layer performance. We evaluate conditions under which the Group-Sum\nlayer performs well and how it can be applied to large-scale classification of\nup to 2000 classes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc(DLGNs)\u5728\u5927\u89c4\u6a21\u591a\u7c7b\u522b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5176\u8868\u8fbe\u80fd\u529b\u3001\u53ef\u6269\u5c55\u6027\u548c\u8f93\u51fa\u7b56\u7565\uff0c\u4e3aDLGNs\u5728\u591a\u8fbe2000\u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "motivation": "DLGNs\u4f5c\u4e3a\u4e00\u79cd\u5feb\u901f\u8282\u80fd\u7684\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u65b9\u6848\uff0c\u76ee\u524d\u4ecd\u5904\u4e8e\u53d1\u5c55\u521d\u671f\uff0c\u7279\u522b\u662f\u5728\u8f93\u51fa\u5c42\u8bbe\u8ba1\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u3002\u6b64\u524d\u4e3b\u8981\u5e94\u7528\u4e8e\u6700\u591a10\u4e2a\u7c7b\u522b\u7684\u6570\u636e\u96c6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22DLGNs\u5728\u5927\u578b\u591a\u7c7b\u522b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u7814\u7a76DLGNs\u7684\u4e00\u822c\u8868\u8fbe\u80fd\u529b\u3001\u53ef\u6269\u5c55\u6027\u548c\u66ff\u4ee3\u8f93\u51fa\u7b56\u7565\uff0c\u7279\u522b\u5173\u6ce8\u6e29\u5ea6\u8c03\u4f18\u7684\u91cd\u8981\u6027\u53ca\u5176\u5bf9\u8f93\u51fa\u5c42\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u4e86Group-Sum\u5c42\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u8fbe2000\u4e2a\u7c7b\u522b\u7684\u5927\u89c4\u6a21\u5206\u7c7b\u4efb\u52a1\u3002", "conclusion": "\u6e29\u5ea6\u8c03\u4f18\u5bf9DLGNs\u8f93\u51fa\u5c42\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cGroup-Sum\u5c42\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0cDLGNs\u80fd\u591f\u6210\u529f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2509.25955", "categories": ["cs.LG", "cs.AI", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2509.25955", "abs": "https://arxiv.org/abs/2509.25955", "authors": ["Mason Minot", "Gisbert Schneider"], "title": "AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties", "comment": "13 pages, 3 figures, 9 tables", "summary": "Simultaneously optimizing multiple, frequently conflicting, molecular\nproperties is a key bottleneck in the development of novel therapeutics.\nAlthough a promising approach, the efficacy of multi-task learning is often\ncompromised by destructive gradient interference, especially in the data-scarce\nregimes common to drug discovery. To address this, we propose AIM, an\noptimization framework that learns a dynamic policy to mediate gradient\nconflicts. The policy is trained jointly with the main network using a novel\naugmented objective composed of dense, differentiable regularizers. This\nobjective guides the policy to produce updates that are geometrically stable\nand dynamically efficient, prioritizing progress on the most challenging tasks.\nWe demonstrate that AIM achieves statistically significant improvements over\nmulti-task baselines on subsets of the QM9 and targeted protein degraders\nbenchmarks, with its advantage being most pronounced in data-scarce regimes.\nBeyond performance, AIM's key contribution is its interpretability; the learned\npolicy matrix serves as a diagnostic tool for analyzing inter-task\nrelationships. This combination of data-efficient performance and diagnostic\ninsight highlights the potential of adaptive optimizers to accelerate\nscientific discovery by creating more robust and insightful models for\nmulti-property molecular design.", "AI": {"tldr": "\u63d0\u51faAIM\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u8c03\u89e3\u68af\u5ea6\u51b2\u7a81\uff0c\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u5b50\u5c5e\u6027\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u836f\u7269\u53d1\u73b0\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7ecf\u5e38\u5b58\u5728\u7834\u574f\u6027\u68af\u5ea6\u5e72\u6270\uff0c\u7279\u522b\u662f\u5728\u836f\u7269\u53d1\u73b0\u7b49\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u8fd9\u4f1a\u9650\u5236\u591a\u5c5e\u6027\u5206\u5b50\u8bbe\u8ba1\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faAIM\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u52a8\u6001\u7b56\u7565\u548c\u4e3b\u7f51\u7edc\uff0c\u4f7f\u7528\u5305\u542b\u5bc6\u96c6\u53ef\u5fae\u5206\u6b63\u5219\u5316\u5668\u7684\u589e\u5f3a\u76ee\u6807\u51fd\u6570\uff0c\u4ea7\u751f\u51e0\u4f55\u7a33\u5b9a\u4e14\u52a8\u6001\u9ad8\u6548\u7684\u66f4\u65b0\u3002", "result": "\u5728QM9\u548c\u9776\u5411\u86cb\u767d\u8d28\u964d\u89e3\u5242\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b50\u96c6\u4e0a\uff0cAIM\u76f8\u6bd4\u591a\u4efb\u52a1\u57fa\u7ebf\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u4f18\u52bf\u6700\u660e\u663e\u3002", "conclusion": "AIM\u7ed3\u5408\u4e86\u6570\u636e\u9ad8\u6548\u6027\u80fd\u548c\u8bca\u65ad\u6d1e\u5bdf\u529b\uff0c\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u5668\u901a\u8fc7\u521b\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u591a\u5c5e\u6027\u5206\u5b50\u8bbe\u8ba1\u6a21\u578b\u6765\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.25964", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25964", "abs": "https://arxiv.org/abs/2509.25964", "authors": ["Deniz Soysal", "Xabier Garc\u00eda-Andrade", "Laura E. Rodriguez", "Pablo Sobron", "Laura M. Barge", "Renaud Detry"], "title": "Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy", "comment": null, "summary": "Autonomous Raman instruments on Mars rovers, deep-sea landers, and field\nrobots must interpret raw spectra distorted by fluorescence baselines, peak\nshifts, and limited ground-truth labels. Using curated subsets of the RRUFF\ndatabase, we evaluate one-dimensional convolutional neural networks (CNNs) and\nreport four advances: (i) Baseline-independent classification: compact CNNs\nsurpass $k$-nearest-neighbors and support-vector machines on handcrafted\nfeatures, removing background-correction and peak-picking stages while ensuring\nreproducibility through released data splits and scripts. (ii)\nPooling-controlled robustness: tuning a single pooling parameter accommodates\nRaman shifts up to $30 \\,\\mathrm{cm}^{-1}$, balancing translational invariance\nwith spectral resolution. (iii) Label-efficient learning: semi-supervised\ngenerative adversarial networks and contrastive pretraining raise accuracy by\nup to $11\\%$ with only $10\\%$ labels, valuable for autonomous deployments with\nscarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and\nretraining only the softmax layer transfers models to unseen minerals at\n$\\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited\nprocessors. This workflow, which involves training on raw spectra, tuning\npooling, adding semi-supervision when labels are scarce, and fine-tuning\nlightly for new targets, provides a practical path toward robust, low-footprint\nRaman classification in autonomous exploration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u4e3b\u62c9\u66fc\u5149\u8c31\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u7ebf\u65e0\u5173\u5206\u7c7b\u3001\u6c60\u5316\u63a7\u5236\u9c81\u68d2\u6027\u3001\u6807\u7b7e\u9ad8\u6548\u5b66\u4e60\u548c\u6052\u5b9a\u65f6\u95f4\u9002\u5e94\u56db\u4e2a\u521b\u65b0\u70b9\uff0c\u5b9e\u73b0\u4e86\u5728\u706b\u661f\u8f66\u3001\u6df1\u6d77\u7740\u9646\u5668\u7b49\u81ea\u4e3b\u8bbe\u5907\u4e0a\u7684\u9c81\u68d2\u3001\u4f4e\u8d44\u6e90\u62c9\u66fc\u5149\u8c31\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u62c9\u66fc\u4eea\u5668\u5728\u706b\u661f\u3001\u6df1\u6d77\u7b49\u73af\u5883\u4e2d\u9762\u4e34\u7684\u5149\u8c31\u5931\u771f\u3001\u8367\u5149\u57fa\u7ebf\u5e72\u6270\u3001\u5cf0\u503c\u504f\u79fb\u548c\u5730\u9762\u771f\u503c\u6807\u7b7e\u6709\u9650\u7b49\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u63a2\u7d22\u63d0\u4f9b\u5b9e\u7528\u7684\u62c9\u66fc\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528RRUFF\u6570\u636e\u5e93\u7684\u7cbe\u9009\u5b50\u96c6\uff0c\u8bc4\u4f30\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5305\u62ec\u57fa\u7ebf\u65e0\u5173\u5206\u7c7b\u3001\u6c60\u5316\u53c2\u6570\u8c03\u4f18\u3001\u534a\u76d1\u7763\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u3001\u4ee5\u53ca\u4ec5\u91cd\u8bad\u7ec3softmax\u5c42\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u7d27\u51d1CNN\u5728\u57fa\u7ebf\u65e0\u5173\u5206\u7c7b\u4e0a\u4f18\u4e8ek\u8fd1\u90bb\u548c\u652f\u6301\u5411\u91cf\u673a\uff1b\u6c60\u5316\u8c03\u4f18\u53ef\u9002\u5e9430 cm^{-1}\u7684\u62c9\u66fc\u4f4d\u79fb\uff1b\u534a\u76d1\u7763\u65b9\u6cd5\u5728\u4ec510%\u6807\u7b7e\u4e0b\u63d0\u534711%\u51c6\u786e\u7387\uff1b\u6052\u5b9a\u65f6\u95f4\u8fc1\u79fb\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u5904\u7406\u5668\u4e0a\u4f18\u4e8eSiamese\u7f51\u7edc\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u7a0b\uff08\u539f\u59cb\u5149\u8c31\u8bad\u7ec3\u3001\u6c60\u5316\u8c03\u4f18\u3001\u534a\u76d1\u7763\u5b66\u4e60\u548c\u8f7b\u91cf\u5fae\u8c03\uff09\u4e3a\u81ea\u4e3b\u63a2\u7d22\u4e2d\u7684\u9c81\u68d2\u3001\u4f4e\u8d44\u6e90\u62c9\u66fc\u5206\u7c7b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.25977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25977", "abs": "https://arxiv.org/abs/2509.25977", "authors": ["Xiao Zhang", "Zengzhe Chen", "Yuan Yuan", "Yifei Zou", "Fuzhen Zhuang", "Wenyu Jiao", "Yuke Wang", "Dongxiao Yu"], "title": "Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning", "comment": null, "summary": "Federated learning (FL) is a distributed learning paradigm across multiple\nentities while preserving data privacy. However, with the continuous emergence\nof new data and increasing model diversity, traditional federated learning\nfaces significant challenges, including inherent issues of data heterogeneity,\nmodel heterogeneity and catastrophic forgetting, along with new challenge of\nknowledge misalignment. In this study, we introduce FedDCL, a novel framework\ndesigned to enable data-free continual learning of the server model in a\nmodel-heterogeneous federated setting. We leverage pre-trained diffusion models\nto extract lightweight class-specific prototypes, which confer a threefold\ndata-free advantage, enabling: (1) generation of synthetic data for the current\ntask to augment training and counteract non-IID data distributions; (2)\nexemplar-free generative replay for retaining knowledge from previous tasks;\nand (3) data-free dynamic knowledge transfer from heterogeneous clients to the\nserver. Experimental results on various datasets demonstrate the effectiveness\nof FedDCL, showcasing its potential to enhance the generalizability and\npractical applicability of federated learning in dynamic settings.", "AI": {"tldr": "FedDCL\u662f\u4e00\u4e2a\u5728\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5b9e\u73b0\u670d\u52a1\u5668\u6a21\u578b\u65e0\u6570\u636e\u6301\u7eed\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u63d0\u53d6\u8f7b\u91cf\u7ea7\u7c7b\u522b\u7279\u5b9a\u539f\u578b\u6765\u89e3\u51b3\u6570\u636e\u5f02\u6784\u3001\u6a21\u578b\u5f02\u6784\u548c\u707e\u96be\u6027\u9057\u5fd8\u7b49\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u5f02\u6784\u3001\u6a21\u578b\u5f02\u6784\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u77e5\u8bc6\u9519\u4f4d\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u65b0\u6570\u636e\u4e0d\u65ad\u6d8c\u73b0\u548c\u6a21\u578b\u591a\u6837\u6027\u589e\u52a0\u7684\u52a8\u6001\u73af\u5883\u4e2d\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u63d0\u53d6\u8f7b\u91cf\u7ea7\u7c7b\u522b\u7279\u5b9a\u539f\u578b\uff0c\u5b9e\u73b0\u4e09\u65b9\u9762\u65e0\u6570\u636e\u4f18\u52bf\uff1a\u751f\u6210\u5f53\u524d\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u3001\u65e0\u793a\u4f8b\u751f\u6210\u5f0f\u91cd\u653e\u4fdd\u7559\u5148\u524d\u4efb\u52a1\u77e5\u8bc6\u3001\u4ee5\u53ca\u4ece\u5f02\u6784\u5ba2\u6237\u7aef\u5230\u670d\u52a1\u5668\u7684\u65e0\u6570\u636e\u52a8\u6001\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFedDCL\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u589e\u5f3a\u8054\u90a6\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u7684\u80fd\u529b\u3002", "conclusion": "FedDCL\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u539f\u578b\u63d0\u53d6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u6311\u6218\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25979", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25979", "abs": "https://arxiv.org/abs/2509.25979", "authors": ["Gaojie Jin", "Xinping Yi", "Xiaowei Huang"], "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier", "comment": "Under Review", "summary": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a\nposterior $Q$) and the corresponding $Q$-weighted majority vote classifier are\ncommonly used to analyze the generalization performance. However, there exists\na notable lack in theoretical research exploring the certified robustness of\nmajority vote classifier and its interplay with generalization. In this study,\nwe develop a generalization error bound that possesses a certified robust\nradius for the smoothed majority vote classifier (i.e., the $Q$-weighted\nmajority vote classifier with smoothed inputs); In other words, the\ngeneralization bound holds under any data perturbation within the certified\nrobust radius. As a byproduct, we find that the underpinnings of both the\ngeneralization bound and the certified robust radius draw, in part, upon weight\nspectral norm, which thereby inspires the adoption of spectral regularization\nin smooth training to boost certified robustness. Utilizing the\ndimension-independent property of spherical Gaussian inputs in smooth training,\nwe propose a novel and inexpensive spectral regularizer to enhance the smoothed\nmajority vote classifier. In addition to the theoretical contribution, a set of\nempirical results is provided to substantiate the effectiveness of our proposed\nmethod.", "AI": {"tldr": "\u672c\u6587\u5728PAC-Bayesian\u6846\u67b6\u4e0b\uff0c\u4e3a\u5e73\u6ed1\u591a\u6570\u6295\u7968\u5206\u7c7b\u5668\u5f00\u53d1\u4e86\u5177\u6709\u8ba4\u8bc1\u9c81\u68d2\u534a\u5f84\u7684\u6cdb\u5316\u8bef\u5dee\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6743\u91cd\u8c31\u8303\u6570\u7684\u8c31\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u8ba4\u8bc1\u9c81\u68d2\u6027\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u591a\u6570\u6295\u7968\u5206\u7c7b\u5668\u8ba4\u8bc1\u9c81\u68d2\u6027\u53ca\u5176\u4e0e\u6cdb\u5316\u6027\u80fd\u4ea4\u4e92\u5173\u7cfb\u7684\u7406\u8bba\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728PAC-Bayesian\u6846\u67b6\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u5177\u6709\u8ba4\u8bc1\u9c81\u68d2\u534a\u5f84\u7684\u6cdb\u5316\u8bef\u5dee\u754c\uff0c\u5229\u7528\u6743\u91cd\u8c31\u8303\u6570\u8bbe\u8ba1\u8c31\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u7403\u9762\u9ad8\u65af\u8f93\u5165\u7684\u7ef4\u5ea6\u65e0\u5173\u7279\u6027\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u5ec9\u4ef7\u7684\u8c31\u6b63\u5219\u5316\u5668\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6cdb\u5316\u754c\u548c\u8ba4\u8bc1\u9c81\u68d2\u534a\u5f84\u90e8\u5206\u4f9d\u8d56\u4e8e\u6743\u91cd\u8c31\u8303\u6570\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5e73\u6ed1\u591a\u6570\u6295\u7968\u5206\u7c7b\u5668\u7684\u8ba4\u8bc1\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6cdb\u5316\u7406\u8bba\u548c\u8ba4\u8bc1\u9c81\u68d2\u6027\u5206\u6790\uff0c\u672c\u6587\u4e3a\u5e73\u6ed1\u591a\u6570\u6295\u7968\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u8c31\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.25980", "categories": ["cs.LG", "math-ph", "math.MP", "math.PR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.25980", "abs": "https://arxiv.org/abs/2509.25980", "authors": ["Mykola Bordyuh", "Djork-Arn\u00e9 Clevert", "Marco Bertolini"], "title": "Exact Solutions to the Quantum Schr\u00f6dinger Bridge Problem", "comment": null, "summary": "The Quantum Schr\\\"odinger Bridge Problem (QSBP) describes the evolution of a\nstochastic process between two arbitrary probability distributions, where the\ndynamics are governed by the Schr\\\"odinger equation rather than by the\ntraditional real-valued wave equation. Although the QSBP is known in the\nmathematical literature, we formulate it here from a Lagrangian perspective and\nderive its main features in a way that is particularly suited to generative\nmodeling. We show that the resulting evolution equations involve the so-called\nBohm (quantum) potential, representing a notion of non-locality in the\nstochastic process. This distinguishes the QSBP from classical stochastic\ndynamics and reflects a key characteristic typical of quantum mechanical\nsystems. In this work, we derive exact closed-form solutions for the QSBP\nbetween Gaussian distributions. Our derivation is based on solving the\nFokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising\nfrom the Lagrangian formulation of dynamical Optimal Transport. We find that,\nsimilar to the classical Schr\\\"odinger Bridge Problem, the solution to the QSBP\nbetween Gaussians is again a Gaussian process; however, the evolution of the\ncovariance differs due to quantum effects. Leveraging these explicit solutions,\nwe present a modified algorithm based on a Gaussian Mixture Model framework,\nand demonstrate its effectiveness across several experimental settings,\nincluding single-cell evolution data, image generation, molecular translation\nand applications in Mean-Field Games.", "AI": {"tldr": "\u672c\u6587\u4ece\u62c9\u683c\u6717\u65e5\u89c6\u89d2\u91cd\u65b0\u8868\u8ff0\u91cf\u5b50\u859b\u5b9a\u8c14\u6865\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u6d89\u53ca\u73bb\u59c6\u52bf\u7684\u6f14\u5316\u65b9\u7a0b\uff0c\u83b7\u5f97\u4e86\u9ad8\u65af\u5206\u5e03\u95f4\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7b97\u6cd5\u3002", "motivation": "\u5c06\u91cf\u5b50\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u4ece\u62c9\u683c\u6717\u65e5\u89c6\u89d2\u91cd\u65b0\u8868\u8ff0\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u751f\u6210\u5efa\u6a21\u5e94\u7528\uff0c\u63a2\u7d22\u91cf\u5b50\u7cfb\u7edf\u975e\u5b9a\u57df\u6027\u5728\u968f\u673a\u8fc7\u7a0b\u4e2d\u7684\u4f53\u73b0\u3002", "method": "\u901a\u8fc7\u6c42\u89e3\u62c9\u683c\u6717\u65e5\u5f62\u5f0f\u52a8\u529b\u5b66\u6700\u4f18\u4f20\u8f93\u4e2d\u7684Fokker-Planck\u65b9\u7a0b\u548cHamilton-Jacobi\u65b9\u7a0b\uff0c\u63a8\u5bfc\u9ad8\u65af\u5206\u5e03\u95f4\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7b97\u6cd5\u3002", "result": "\u83b7\u5f97\u4e86\u91cf\u5b50\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u5728\u9ad8\u65af\u5206\u5e03\u95f4\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\uff0c\u53d1\u73b0\u89e3\u4ecd\u662f\u9ad8\u65af\u8fc7\u7a0b\u4f46\u534f\u65b9\u5dee\u6f14\u5316\u56e0\u91cf\u5b50\u6548\u5e94\u800c\u4e0d\u540c\uff0c\u7b97\u6cd5\u5728\u5355\u7ec6\u80de\u6f14\u5316\u6570\u636e\u3001\u56fe\u50cf\u751f\u6210\u7b49\u591a\u4e2a\u5b9e\u9a8c\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u91cf\u5b50\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u63d0\u4f9b\u4e86\u533a\u522b\u4e8e\u7ecf\u5178\u968f\u673a\u52a8\u529b\u5b66\u7684\u91cf\u5b50\u975e\u5b9a\u57df\u6027\u7279\u5f81\uff0c\u5176\u9ad8\u65af\u95ed\u5f0f\u89e3\u4e3a\u751f\u6210\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2509.25996", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25996", "abs": "https://arxiv.org/abs/2509.25996", "authors": ["Weiyu Huang", "Yuezhou Hu", "Jun Zhu", "Jianfei Chen"], "title": "CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models", "comment": "Submitted to IEEE TPAMI", "summary": "Sparsity-aware training is an effective approach for transforming large\nlanguage models (LLMs) into hardware-friendly sparse patterns, thereby reducing\nlatency and memory consumption during inference. In this paper, we propose\nContinuous Adaptive Sparse Trainer (CAST), a fully continuous and\ndifferentiable sparsity-aware training framework for semi-structured (or \"N:M\")\nsparse models. Unlike previous approaches that optimize sparsity patterns and\nweights separately, CAST enables seamless joint optimization during training,\nwhile progressively transforming the model into the desired sparsity format.\nSpecifically, CAST introduces three key components: 1) AdamS, a sparsity-aware\noptimizer that leverages adaptive L1 decay to promote uniform sparsification\nacross all parameters; 2) Weight Scaling, a module designed to mitigate the\nmagnitude reduction caused by decay while preserving desired sparsity patterns;\n3) Knowledge Distillation, which employs the dense model as a self-teacher to\nenhance training efficiency. We evaluate CAST under 2:4 sparsity patterns\nacross multiple model families, ranging from 125M to 13B parameters. Our\nresults demonstrate significant improvements over previous state-of-the-art\nmethods in both perplexity and zero-shot accuracy with minimal training\nresources. Notably, on LLaMA2-7B, our 2:4 sparse model achieves a negligible\nperplexity increase of 0.09 and a 0.36% gain in zero-shot accuracy compared to\nthe dense model using only 2% of the original pretraining tokens. Additionally,\nwe establish an accurate and robust empirical scaling law to predict sparse\nmodel performance given adequate training resources. Finally, we demonstrate\nthe practical applicability of our sparse models by evaluating them under\nquantization and fine-tuning scenarios.", "AI": {"tldr": "CAST\u662f\u4e00\u79cd\u5168\u8fde\u7eed\u53ef\u5fae\u7684\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u7a00\u758f\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7a00\u758f\u6a21\u5f0f\u548c\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u53ef\u4ee5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u786c\u4ef6\u53cb\u597d\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u5206\u79bb\u7684\u95ee\u9898\u3002", "method": "CAST\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aAdamS\u7a00\u758f\u611f\u77e5\u4f18\u5316\u5668\u3001\u6743\u91cd\u7f29\u653e\u6a21\u5757\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94L1\u8870\u51cf\u548c\u6e10\u8fdb\u7a00\u758f\u5316\u5b9e\u73b0\u8054\u5408\u4f18\u5316\u3002", "result": "\u57282:4\u7a00\u758f\u6a21\u5f0f\u4e0b\uff0c\u4ece125M\u523013B\u53c2\u6570\u7684\u591a\u4e2a\u6a21\u578b\u4e0a\uff0cCAST\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0cLLaMA2-7B\u4ec5\u75282%\u9884\u8bad\u7ec3token\u5c31\u8fbe\u5230\u63a5\u8fd1\u5bc6\u96c6\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "CAST\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u53ef\u9884\u6d4b\u7a00\u758f\u6a21\u578b\u6027\u80fd\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u5728\u91cf\u5316\u548c\u5fae\u8c03\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.26000", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26000", "abs": "https://arxiv.org/abs/2509.26000", "authors": ["Daniel Ebi", "Gaspard Lambrechts", "Damien Ernst", "Klemens B\u00f6hm"], "title": "Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access", "comment": "15 pages, 21 pages total", "summary": "Reinforcement learning in partially observable environments requires agents\nto act under uncertainty from noisy, incomplete observations. Asymmetric\nactor-critic methods leverage privileged information during training to improve\nlearning under these conditions. However, existing approaches typically assume\nfull-state access during training. In this work, we challenge this assumption\nby proposing a novel actor-critic framework, called informed asymmetric\nactor-critic, that enables conditioning the critic on arbitrary privileged\nsignals without requiring access to the full state. We show that policy\ngradients remain unbiased under this formulation, extending the theoretical\nfoundation of asymmetric methods to the more general case of privileged partial\ninformation. To quantify the impact of such signals, we propose informativeness\nmeasures based on kernel methods and return prediction error, providing\npractical tools for evaluating training-time signals. We validate our approach\nempirically on benchmark navigation tasks and synthetic partially observable\nenvironments, showing that our informed asymmetric method improves learning\nefficiency and value estimation when informative privileged inputs are\navailable. Our findings challenge the necessity of full-state access and open\nnew directions for designing asymmetric reinforcement learning methods that are\nboth practical and theoretically sound.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5bf9\u79f0actor-critic\u6846\u67b6\uff0c\u5141\u8bb8critic\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u4efb\u610f\u7279\u6743\u4fe1\u53f7\uff0c\u65e0\u9700\u5b8c\u6574\u72b6\u6001\u8bbf\u95ee\uff0c\u63d0\u9ad8\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u975e\u5bf9\u79f0\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u65f6\u80fd\u8bbf\u95ee\u5b8c\u6574\u72b6\u6001\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u79cd\u5047\u8bbe\u5f80\u5f80\u4e0d\u6210\u7acb\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u53ea\u6709\u90e8\u5206\u7279\u6743\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86informed asymmetric actor-critic\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u5728\u8fd9\u79cd\u8bbe\u5b9a\u4e0b\u7b56\u7565\u68af\u5ea6\u4ecd\u7136\u65e0\u504f\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6838\u65b9\u6cd5\u548c\u56de\u62a5\u9884\u6d4b\u8bef\u5dee\u7684\u4fe1\u606f\u6027\u5ea6\u91cf\u5de5\u5177\u3002", "result": "\u5728\u57fa\u51c6\u5bfc\u822a\u4efb\u52a1\u548c\u5408\u6210\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5f53\u6709\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u6743\u8f93\u5165\u65f6\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u4ef7\u503c\u4f30\u8ba1\u3002", "conclusion": "\u6311\u6218\u4e86\u5b8c\u6574\u72b6\u6001\u8bbf\u95ee\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u975e\u5bf9\u79f0\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4f7f\u5176\u66f4\u52a0\u5b9e\u7528\u4e14\u7406\u8bba\u53ef\u9760\u3002"}}
{"id": "2509.26015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26015", "abs": "https://arxiv.org/abs/2509.26015", "authors": ["Bissmella Bahaduri", "Hicham Talaoubrid", "Fangchen Feng", "Zuheng Ming", "Anissa Mokraoui"], "title": "Indirect Attention: Turning Context Misalignment into a Feature", "comment": null, "summary": "The attention mechanism has become a cornerstone of modern deep learning\narchitectures, where keys and values are typically derived from the same\nunderlying sequence or representation. This work explores a less conventional\nscenario, when keys and values originate from different sequences or\nmodalities. Specifically, we first analyze the attention mechanism's behavior\nunder noisy value features, establishing a critical noise threshold beyond\nwhich signal degradation becomes significant. Furthermore, we model context\n(key, value) misalignment as an effective form of structured noise within the\nvalue features, demonstrating that the noise induced by such misalignment can\nsubstantially exceed this critical threshold, thereby compromising standard\nattention's efficacy. Motivated by this, we introduce Indirect Attention, a\nmodified attention mechanism that infers relevance indirectly in scenarios with\nmisaligned context. We evaluate the performance of Indirect Attention across a\nrange of synthetic tasks and real world applications, showcasing its superior\nability to handle misalignment.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u952e\u548c\u503c\u6765\u81ea\u4e0d\u540c\u5e8f\u5217\u6216\u6a21\u6001\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u4e0d\u5bf9\u9f50\u4f1a\u5f15\u5165\u8d85\u8fc7\u4e34\u754c\u9608\u503c\u7684\u566a\u58f0\uff0c\u4ece\u800c\u635f\u5bb3\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u6548\u679c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u95f4\u63a5\u6ce8\u610f\u529b\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u79cd\u4e0d\u5bf9\u9f50\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u6ce8\u610f\u529b\u673a\u5236\u5728\u952e\u548c\u503c\u6765\u81ea\u4e0d\u540c\u5e8f\u5217\u6216\u6a21\u6001\u65f6\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5f53\u5b58\u5728\u4e0a\u4e0b\u6587\u4e0d\u5bf9\u9f50\u65f6\uff0c\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u4f1a\u56e0\u566a\u58f0\u8d85\u8fc7\u4e34\u754c\u9608\u503c\u800c\u5931\u6548\u3002", "method": "\u9996\u5148\u5206\u6790\u6ce8\u610f\u529b\u673a\u5236\u5728\u566a\u58f0\u503c\u7279\u5f81\u4e0b\u7684\u884c\u4e3a\uff0c\u5efa\u7acb\u4e34\u754c\u566a\u58f0\u9608\u503c\uff1b\u7136\u540e\u5c06\u4e0a\u4e0b\u6587\u4e0d\u5bf9\u9f50\u5efa\u6a21\u4e3a\u503c\u7279\u5f81\u4e2d\u7684\u7ed3\u6784\u5316\u566a\u58f0\uff1b\u6700\u540e\u63d0\u51fa\u95f4\u63a5\u6ce8\u610f\u529b\u673a\u5236\u6765\u95f4\u63a5\u63a8\u65ad\u76f8\u5173\u6027\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8bc4\u4f30\u95f4\u63a5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bc1\u660e\u5176\u5728\u5904\u7406\u4e0d\u5bf9\u9f50\u60c5\u51b5\u65f6\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u95f4\u63a5\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u6709\u6548\u5904\u7406\u952e\u503c\u4e0d\u5bf9\u9f50\u7684\u60c5\u51b5\uff0c\u5728\u5b58\u5728\u4e0a\u4e0b\u6587\u4e0d\u5bf9\u9f50\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u3002"}}
{"id": "2509.26017", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26017", "abs": "https://arxiv.org/abs/2509.26017", "authors": ["Daphne Theodorakopoulos", "Elisabeth Eberling", "Miriam Bodenheimer", "Sabine Loos", "Frederic Stahl"], "title": "FITS: Towards an AI-Driven Fashion Information Tool for Sustainability", "comment": "accepted at ECAI 2025", "summary": "Access to credible sustainability information in the fashion industry remains\nlimited and challenging to interpret, despite growing public and regulatory\ndemands for transparency. General-purpose language models often lack\ndomain-specific knowledge and tend to \"hallucinate\", which is particularly\nharmful for fields where factual correctness is crucial. This work explores how\nNatural Language Processing (NLP) techniques can be applied to classify\nsustainability data for fashion brands, thereby addressing the scarcity of\ncredible and accessible information in this domain. We present a prototype\nFashion Information Tool for Sustainability (FITS), a transformer-based system\nthat extracts and classifies sustainability information from credible,\nunstructured text sources: NGO reports and scientific publications. Several\nBERT-based language models, including models pretrained on scientific and\nclimate-specific data, are fine-tuned on our curated corpus using a\ndomain-specific classification schema, with hyperparameters optimized via\nBayesian optimization. FITS allows users to search for relevant data, analyze\ntheir own data, and explore the information via an interactive interface. We\nevaluated FITS in two focus groups of potential users concerning usability,\nvisual design, content clarity, possible use cases, and desired features. Our\nresults highlight the value of domain-adapted NLP in promoting informed\ndecision-making and emphasize the broader potential of AI applications in\naddressing climate-related challenges. Finally, this work provides a valuable\ndataset, the SustainableTextileCorpus, along with a methodology for future\nupdates. Code available at https://github.com/daphne12345/FITS", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684\u65f6\u5c1a\u53ef\u6301\u7eed\u53d1\u5c55\u4fe1\u606f\u5de5\u5177FITS\uff0c\u4f7f\u7528BERT\u6a21\u578b\u4ece\u53ef\u4fe1\u6587\u672c\u6e90\u63d0\u53d6\u548c\u5206\u7c7b\u53ef\u6301\u7eed\u53d1\u5c55\u4fe1\u606f\uff0c\u901a\u8fc7\u7528\u6237\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u65f6\u5c1a\u884c\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u4fe1\u606f\u83b7\u53d6\u56f0\u96be\u4e14\u96be\u4ee5\u89e3\u8bfb\uff0c\u901a\u7528\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u4e14\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u63d0\u4f9b\u53ef\u4fe1\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u591a\u4e2a\u57fa\u4e8eBERT\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u79d1\u5b66\u548c\u6c14\u5019\u7279\u5b9a\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u9886\u57df\u7279\u5b9a\u5206\u7c7b\u6a21\u5f0f\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u5f00\u53d1\u4e86FITS\u539f\u578b\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u975e\u653f\u5e9c\u7ec4\u7ec7\u62a5\u544a\u548c\u79d1\u5b66\u51fa\u7248\u7269\u4e2d\u63d0\u53d6\u548c\u5206\u7c7b\u53ef\u6301\u7eed\u53d1\u5c55\u4fe1\u606f\uff0c\u7528\u6237\u8bc4\u4f30\u663e\u793a\u5728\u53ef\u7528\u6027\u3001\u89c6\u89c9\u8bbe\u8ba1\u3001\u5185\u5bb9\u6e05\u6670\u5ea6\u7b49\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u9886\u57df\u9002\u5e94\u7684NLP\u5728\u4fc3\u8fdb\u77e5\u60c5\u51b3\u7b56\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cAI\u5e94\u7528\u5728\u5e94\u5bf9\u6c14\u5019\u76f8\u5173\u6311\u6218\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7eba\u7ec7\u8bed\u6599\u5e93\u6570\u636e\u96c6\u548c\u672a\u6765\u66f4\u65b0\u7684\u65b9\u6cd5\u5b66\u3002"}}
{"id": "2509.26030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26030", "abs": "https://arxiv.org/abs/2509.26030", "authors": ["Shuche Wang", "Fengzhuo Zhang", "Jiaxiang Li", "Cunxiao Du", "Chao Du", "Tianyu Pang", "Zhuoran Yang", "Mingyi Hong", "Vincent Y. F. Tan"], "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning", "comment": null, "summary": "The Muon optimizer is consistently faster than Adam in training Large\nLanguage Models (LLMs), yet the mechanism underlying its success remains\nunclear. This paper demystifies this mechanism through the lens of associative\nmemory. By ablating the transformer components optimized by Muon, we reveal\nthat the associative memory parameters of LLMs, namely the Value and Output\n(VO) attention weights and Feed-Forward Networks (FFNs), are the primary\ncontributors to Muon's superiority. Motivated by this associative memory view,\nwe then explain Muon's superiority on real-world corpora, which are\nintrinsically heavy-tailed: a few classes (tail classes) appear far less\nfrequently than others. The superiority is explained through two key\nproperties: (i) its update rule consistently yields a more isotropic singular\nspectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes\ntail classes more effectively than Adam. Beyond empirical evidence, we\ntheoretically confirm these findings by analyzing a one-layer associative\nmemory model under class-imbalanced data. We prove that Muon consistently\nachieves balanced learning across classes regardless of feature embeddings,\nwhereas Adam can induce large disparities in learning errors depending on\nembedding properties. In summary, our empirical observations and theoretical\nanalyses reveal Muon's core advantage: its update rule aligns with the\nouter-product structure of linear associative memories, enabling more balanced\nand effective learning of tail classes in heavy-tailed distributions than Adam.", "AI": {"tldr": "Muon\u4f18\u5316\u5668\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u6bd4Adam\u66f4\u5feb\uff0c\u5176\u6210\u529f\u673a\u5236\u6e90\u4e8e\u5bf9\u5173\u8054\u8bb0\u5fc6\u53c2\u6570\uff08VO\u6ce8\u610f\u529b\u6743\u91cd\u548cFFN\uff09\u7684\u4f18\u5316\uff0c\u4ee5\u53ca\u5728\u91cd\u5c3e\u6570\u636e\u5206\u5e03\u4e0b\u5bf9\u5c3e\u90e8\u7c7b\u522b\u7684\u66f4\u6709\u6548\u5b66\u4e60\u3002", "motivation": "\u7406\u89e3Muon\u4f18\u5316\u5668\u4e3a\u4f55\u5728\u8bad\u7ec3LLMs\u65f6\u6bd4Adam\u66f4\u5feb\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u5176\u5728\u91cd\u5c3e\u6570\u636e\u5206\u5e03\u4e0b\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790transformer\u7ec4\u4ef6\uff0c\u4ece\u5173\u8054\u8bb0\u5fc6\u89d2\u5ea6\u89e3\u91caMuon\u7684\u4f18\u52bf\uff0c\u5e76\u5728\u7406\u8bba\u5206\u6790\u4e2d\u4f7f\u7528\u5355\u5c42\u5173\u8054\u8bb0\u5fc6\u6a21\u578b\u9a8c\u8bc1\u53d1\u73b0\u3002", "result": "Muon\u7684\u66f4\u65b0\u89c4\u5219\u4ea7\u751f\u66f4\u5404\u5411\u540c\u6027\u7684\u5947\u5f02\u8c31\uff0c\u5728\u91cd\u5c3e\u6570\u636e\u4e0a\u80fd\u66f4\u6709\u6548\u5730\u4f18\u5316\u5c3e\u90e8\u7c7b\u522b\uff0c\u800cAdam\u53ef\u80fd\u56e0\u5d4c\u5165\u7279\u6027\u5bfc\u81f4\u5b66\u4e60\u8bef\u5dee\u5dee\u5f02\u3002", "conclusion": "Muon\u7684\u6838\u5fc3\u4f18\u52bf\u5728\u4e8e\u5176\u66f4\u65b0\u89c4\u5219\u4e0e\u7ebf\u6027\u5173\u8054\u8bb0\u5fc6\u7684\u5916\u79ef\u7ed3\u6784\u4e00\u81f4\uff0c\u80fd\u591f\u5728\u91cd\u5c3e\u5206\u5e03\u4e0b\u5b9e\u73b0\u6bd4Adam\u66f4\u5e73\u8861\u548c\u6709\u6548\u7684\u5c3e\u90e8\u7c7b\u522b\u5b66\u4e60\u3002"}}
{"id": "2509.26032", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.26032", "abs": "https://arxiv.org/abs/2509.26032", "authors": ["Xiaobao Wang", "Ruoxiao Sun", "Yujun Zhang", "Bingdao Feng", "Dongxiao He", "Luzhi Wang", "Di Jin"], "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Graph Neural Networks (GNNs) have demonstrated strong performance across\ntasks such as node classification, link prediction, and graph classification,\nbut remain vulnerable to backdoor attacks that implant imperceptible triggers\nduring training to control predictions. While node-level attacks exploit local\nmessage passing, graph-level attacks face the harder challenge of manipulating\nglobal representations while maintaining stealth. We identify two main sources\nof anomaly in existing graph classification backdoor methods: structural\ndeviation from rare subgraph triggers and semantic deviation caused by label\nflipping, both of which make poisoned graphs easily detectable by anomaly\ndetection models. To address this, we propose DPSBA, a clean-label backdoor\nframework that learns in-distribution triggers via adversarial training guided\nby anomaly-aware discriminators. DPSBA effectively suppresses both structural\nand semantic anomalies, achieving high attack success while significantly\nimproving stealth. Extensive experiments on real-world datasets validate that\nDPSBA achieves a superior balance between effectiveness and detectability\ncompared to state-of-the-art baselines.", "AI": {"tldr": "DPSBA\u662f\u4e00\u4e2a\u9488\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u7684\u6e05\u6d01\u6807\u7b7e\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u5206\u5e03\u5185\u89e6\u53d1\u5668\u6765\u907f\u514d\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u9ad8\u653b\u51fb\u6210\u529f\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u5206\u7c7b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u5f02\u5e38\uff08\u7f55\u89c1\u5b50\u56fe\u89e6\u53d1\u5668\uff09\u548c\u8bed\u4e49\u5f02\u5e38\uff08\u6807\u7b7e\u7ffb\u8f6c\uff09\u95ee\u9898\uff0c\u5bfc\u81f4\u4e2d\u6bd2\u56fe\u5bb9\u6613\u88ab\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u53d1\u73b0\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDPSBA\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u5e38\u611f\u77e5\u5224\u522b\u5668\u6307\u5bfc\u7684\u5bf9\u6297\u8bad\u7ec3\u5b66\u4e60\u5206\u5e03\u5185\u89e6\u53d1\u5668\uff0c\u6709\u6548\u6291\u5236\u7ed3\u6784\u548c\u8bed\u4e49\u5f02\u5e38\uff0c\u5b9e\u73b0\u6e05\u6d01\u6807\u7b7e\u540e\u95e8\u653b\u51fb\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDPSBA\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u653b\u51fb\u6548\u679c\u548c\u53ef\u68c0\u6d4b\u6027\u4e4b\u95f4\u8fbe\u5230\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "DPSBA\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u5f02\u5e38\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9690\u853d\u6709\u6548\u7684\u56fe\u5206\u7c7b\u540e\u95e8\u653b\u51fb\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.26045", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26045", "abs": "https://arxiv.org/abs/2509.26045", "authors": ["Aoming Liu", "Kevin Miller", "Venkatesh Saligrama", "Kate Saenko", "Boqing Gong", "Ser-Nam Lim", "Bryan A. Plummer"], "title": "Scaling Up Temporal Domain Generalization via Temporal Experts Averaging", "comment": "Accepted by EMNLP 2025 main", "summary": "Temporal Domain Generalization (TDG) aims to generalize across temporal\ndistribution shifts, e.g., lexical change over time. Prior work often addresses\nthis by predicting future model weights. However, full model prediction is\nprohibitively expensive for even reasonably sized models. Thus, recent methods\nonly predict the classifier layer, limiting generalization by failing to adjust\nother model components. To address this, we propose Temporal Experts Averaging\n(TEA), a novel and scalable TDG framework that updates the entire model using\nweight averaging to maximize generalization potential while minimizing\ncomputational costs. Our theoretical analysis guides us to two steps that\nenhance generalization to future domains. First, we create expert models with\nfunctional diversity yet parameter similarity by fine-tuning a domain-agnostic\nbase model on individual temporal domains while constraining weight changes.\nSecond, we optimize the bias-variance tradeoff through adaptive averaging\ncoefficients derived from modeling temporal weight trajectories in a principal\ncomponent subspace. Expert's contributions are based on their projected\nproximity to future domains. Extensive experiments across 7 TDG benchmarks, 5\nmodels, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69%\nwhile being up to 60x more efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86Temporal Experts Averaging (TEA)\u6846\u67b6\uff0c\u901a\u8fc7\u6743\u91cd\u5e73\u5747\u66f4\u65b0\u6574\u4e2a\u6a21\u578b\u6765\u89e3\u51b3\u65f6\u5e8f\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u6700\u5927\u5316\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u9884\u6d4b\u5206\u7c7b\u5668\u5c42\u6743\u91cd\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u5168\u6a21\u578b\u9884\u6d4b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u901a\u8fc7\u7ea6\u675f\u6743\u91cd\u53d8\u5316\u5728\u5355\u4e2a\u65f6\u5e8f\u9886\u57df\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u521b\u5efa\u4e13\u5bb6\u6a21\u578b\uff0c\u7136\u540e\u5728\u4e3b\u6210\u5206\u5b50\u7a7a\u95f4\u4e2d\u5efa\u6a21\u65f6\u5e8f\u6743\u91cd\u8f68\u8ff9\uff0c\u81ea\u9002\u5e94\u5730\u5e73\u5747\u4e13\u5bb6\u6a21\u578b\u6743\u91cd\u3002", "result": "\u57287\u4e2aTDG\u57fa\u51c6\u6d4b\u8bd5\u30015\u4e2a\u6a21\u578b\u548c2\u79cdTDG\u8bbe\u7f6e\u4e2d\uff0cTEA\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe69%\uff0c\u6548\u7387\u63d0\u5347\u9ad8\u8fbe60\u500d\u3002", "conclusion": "TEA\u6846\u67b6\u5728\u65f6\u5e8f\u9886\u57df\u6cdb\u5316\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u4e0e\u6548\u7387\u5e73\u8861\u3002"}}
{"id": "2509.26058", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.26058", "abs": "https://arxiv.org/abs/2509.26058", "authors": ["Hossein Enshaei", "Pariya Jebreili", "Sayed Mahmoud Sakahei"], "title": "Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts", "comment": null, "summary": "Electroencephalogram (EEG) artifact detection in real-world settings faces\nsignificant challenges such as computational inefficiency in multi-channel\nmethods, poor robustness to simultaneous noise, and trade-offs between accuracy\nand complexity in deep learning models. We propose a hybrid spectral-temporal\nframework for real-time detection and classification of ocular (EOG), muscular\n(EMG), and white noise artifacts in single-channel EEG. This method, in\ncontrast to other approaches, combines time-domain low-pass filtering\n(targeting low-frequency EOG) and frequency-domain power spectral density (PSD)\nanalysis (capturing broad-spectrum EMG), followed by PCA-optimized feature\nfusion to minimize redundancy while preserving discriminative information. This\nfeature engineering strategy allows a lightweight multi-layer perceptron (MLP)\narchitecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at\nlow SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB).\nAdditionally, this framework addresses the unexplored problem of simultaneous\nmulti-source contamination(EMG+EOG+white noise), where it maintains 96%\nclassification accuracy despite overlapping artifacts. With 30-second training\ntimes (97% faster than CNNs) and robust performance across SNR levels, this\nframework bridges the gap between clinical applicability and computational\nefficiency, which enables real-time use in wearable brain-computer interfaces.\nThis work also challenges the ubiquitous dependence on model depth for EEG\nartifact detection by demonstrating that domain-informed feature fusion\nsurpasses complex architecture in noisy scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u9891\u8c31-\u65f6\u95f4\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u901a\u9053EEG\u4e2d\u773c\u52a8\u3001\u808c\u8089\u548c\u566a\u58f0\u4f2a\u8ff9\u7684\u5b9e\u65f6\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u4f7f\u8f7b\u91cf\u7ea7MLP\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8d85\u8d8a\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u573a\u666f\u4e2dEEG\u4f2a\u8ff9\u68c0\u6d4b\u9762\u4e34\u7684\u6311\u6218\uff1a\u591a\u901a\u9053\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u5bf9\u540c\u65f6\u566a\u58f0\u9c81\u68d2\u6027\u5dee\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u7ed3\u5408\u65f6\u57df\u4f4e\u901a\u6ee4\u6ce2\uff08\u9488\u5bf9\u4f4e\u9891\u773c\u52a8\u4f2a\u8ff9\uff09\u548c\u9891\u57df\u529f\u7387\u8c31\u5bc6\u5ea6\u5206\u6790\uff08\u6355\u83b7\u5bbd\u9891\u8c31\u808c\u8089\u4f2a\u8ff9\uff09\uff0c\u7136\u540e\u901a\u8fc7PCA\u4f18\u5316\u7684\u7279\u5f81\u878d\u5408\u51cf\u5c11\u5197\u4f59\u5e76\u4fdd\u7559\u5224\u522b\u4fe1\u606f\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08SNR -7 dB\uff09\u4e0b\u8fbe\u523099%\u51c6\u786e\u7387\uff0c\u4e2d\u7b49\u566a\u58f0\uff08SNR 4 dB\uff09\u4e0b>90%\u51c6\u786e\u7387\uff1b\u5bf9\u540c\u65f6\u591a\u6e90\u6c61\u67d3\uff08\u773c\u52a8+\u808c\u8089+\u767d\u566a\u58f0\uff09\u4fdd\u630196%\u5206\u7c7b\u51c6\u786e\u7387\uff1b\u8bad\u7ec3\u65f6\u95f4\u4ec530\u79d2\uff0c\u6bd4CNN\u5feb97%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e34\u5e8a\u9002\u7528\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u6311\u6218\u4e86EEG\u4f2a\u8ff9\u68c0\u6d4b\u4e2d\u5bf9\u6a21\u578b\u6df1\u5ea6\u7684\u666e\u904d\u4f9d\u8d56\uff0c\u8bc1\u660e\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u7684\u7279\u5f81\u878d\u5408\u5728\u566a\u58f0\u573a\u666f\u4e2d\u4f18\u4e8e\u590d\u6742\u67b6\u6784\u3002"}}
{"id": "2509.26114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26114", "abs": "https://arxiv.org/abs/2509.26114", "authors": ["Jaesung R. Park", "Junsu Kim", "Gyeongman Kim", "Jinyoung Jo", "Sean Choi", "Jaewoong Cho", "Ernest K. Ryu"], "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as\nthe leading approach for enhancing the reasoning capabilities of large language\nmodels (LLMs). However, RLVR is prone to entropy collapse, where the LLM\nquickly converges to a near-deterministic form, hindering exploration and\nprogress during prolonged RL training. In this work, we reveal that the\nclipping mechanism in PPO and GRPO induces biases on entropy. Through\ntheoretical and empirical analyses, we show that clip-low increases entropy,\nwhile clip-high decreases it. Further, under standard clipping parameters, the\neffect of clip-high dominates, resulting in an overall entropy reduction even\nwhen purely random rewards are provided to the RL algorithm. Our findings\nhighlight an overlooked confounding factor in RLVR: independent of the reward\nsignal, the clipping mechanism influences entropy, which in turn affects the\nreasoning behavior. Furthermore, our analysis demonstrates that clipping can be\ndeliberately used to control entropy. Specifically, with a more aggressive\nclip-low value, one can increase entropy, promote exploration, and ultimately\nprevent entropy collapse in RLVR training.", "AI": {"tldr": "PPO\u548cGRPO\u4e2d\u7684\u88c1\u526a\u673a\u5236\u4f1a\u5bfc\u81f4\u71b5\u504f\u5dee\uff1aclip-low\u589e\u52a0\u71b5\uff0cclip-high\u51cf\u5c11\u71b5\u3002\u5728\u6807\u51c6\u53c2\u6570\u4e0b\uff0cclip-high\u4e3b\u5bfc\u4f5c\u7528\u5bfc\u81f4\u6574\u4f53\u71b5\u51cf\u5c11\uff0c\u5373\u4f7f\u4f7f\u7528\u968f\u673a\u5956\u52b1\u4e5f\u4f1a\u53d1\u751f\u3002\u901a\u8fc7\u8c03\u6574clip-low\u53ef\u4ee5\u63a7\u5236\u71b5\uff0c\u9632\u6b62RLVR\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u3002", "motivation": "RLVR\u65b9\u6cd5\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5bb9\u6613\u53d1\u751f\u71b5\u5d29\u6e83\uff0c\u5bfc\u81f4\u6a21\u578b\u5feb\u901f\u6536\u655b\u5230\u8fd1\u786e\u5b9a\u6027\u5f62\u5f0f\uff0c\u963b\u788d\u63a2\u7d22\u548c\u957f\u671fRL\u8bad\u7ec3\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u4e86PPO\u548cGRPO\u4e2d\u88c1\u526a\u673a\u5236\u5bf9\u71b5\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86clip-low\u548cclip-high\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "result": "\u53d1\u73b0clip-low\u589e\u52a0\u71b5\uff0cclip-high\u51cf\u5c11\u71b5\uff0c\u6807\u51c6\u53c2\u6570\u4e0bclip-high\u4e3b\u5bfc\u5bfc\u81f4\u6574\u4f53\u71b5\u51cf\u5c11\u3002\u8c03\u6574clip-low\u53ef\u4ee5\u589e\u52a0\u71b5\u5e76\u4fc3\u8fdb\u63a2\u7d22\u3002", "conclusion": "\u88c1\u526a\u673a\u5236\u662fRLVR\u4e2d\u88ab\u5ffd\u89c6\u7684\u6df7\u6742\u56e0\u7d20\uff0c\u53ef\u4ee5\u6709\u610f\u7528\u4e8e\u63a7\u5236\u71b5\uff0c\u9632\u6b62\u71b5\u5d29\u6e83\uff0c\u4fc3\u8fdbRL\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u3002"}}
{"id": "2509.26116", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.26116", "abs": "https://arxiv.org/abs/2509.26116", "authors": ["Abdulkadir Celikkanat", "Andres R. Masegosa", "Mads Albertsen", "Thomas D. Nielsen"], "title": "UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning", "comment": null, "summary": "Metagenomic binning aims to cluster DNA fragments from mixed microbial\nsamples into their respective genomes, a critical step for downstream analyses\nof microbial communities. Existing methods rely on deterministic\nrepresentations, such as k-mer profiles or embeddings from large language\nmodels, which fail to capture the uncertainty inherent in DNA sequences arising\nfrom inter-species DNA sharing and from fragments with highly similar\nrepresentations. We present the first probabilistic embedding approach,\nUncertainGen, for metagenomic binning, representing each DNA fragment as a\nprobability distribution in latent space. Our approach naturally models\nsequence-level uncertainty, and we provide theoretical guarantees on embedding\ndistinguishability. This probabilistic embedding framework expands the feasible\nlatent space by introducing a data-adaptive metric, which in turn enables more\nflexible separation of bins/clusters. Experiments on real metagenomic datasets\ndemonstrate the improvements over deterministic k-mer and LLM-based embeddings\nfor the binning task by offering a scalable and lightweight solution for\nlarge-scale metagenomic analysis.", "AI": {"tldr": "UncertainGen\u662f\u9996\u4e2a\u7528\u4e8e\u5b8f\u57fa\u56e0\u7ec4\u5206\u7bb1\u7684\u6982\u7387\u5d4c\u5165\u65b9\u6cd5\uff0c\u5c06DNA\u7247\u6bb5\u8868\u793a\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6982\u7387\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u786e\u5b9a\u6027\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5e8f\u5217\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5b8f\u57fa\u56e0\u7ec4\u5206\u7bb1\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u8868\u793a\uff08\u5982k-mer\u8c31\u6216\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff09\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7531\u7269\u79cd\u95f4DNA\u5171\u4eab\u548c\u9ad8\u5ea6\u76f8\u4f3c\u7247\u6bb5\u8868\u793a\u6240\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u6982\u7387\u5d4c\u5165\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2aDNA\u7247\u6bb5\u8868\u793a\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6982\u7387\u5206\u5e03\uff0c\u5f15\u5165\u6570\u636e\u81ea\u9002\u5e94\u5ea6\u91cf\u6765\u6269\u5c55\u53ef\u884c\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u5206\u7bb1/\u805a\u7c7b\u5206\u79bb\u3002", "result": "\u5728\u771f\u5b9e\u5b8f\u57fa\u56e0\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7bb1\u4efb\u52a1\u4e0a\u4f18\u4e8e\u786e\u5b9a\u6027k-mer\u548c\u57fa\u4e8eLLM\u7684\u5d4c\u5165\u65b9\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u5b8f\u57fa\u56e0\u7ec4\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6982\u7387\u5d4c\u5165\u6846\u67b6\u80fd\u591f\u81ea\u7136\u5efa\u6a21\u5e8f\u5217\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u5d4c\u5165\u53ef\u533a\u5206\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5728\u5b8f\u57fa\u56e0\u7ec4\u5206\u7bb1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.26131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26131", "abs": "https://arxiv.org/abs/2509.26131", "authors": ["Fardin Jalil Piran", "Anandkumar Patel", "Rajiv Malhotra", "Farhad Imani"], "title": "Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing", "comment": "23 pages, 14 figures", "summary": "Smart manufacturing requires on-device intelligence that meets strict latency\nand energy budgets. HyperDimensional Computing (HDC) offers a lightweight\nalternative by encoding data as high-dimensional hypervectors and computing\nwith simple operations. Prior studies often assume that the qualitative\nrelation between HDC hyperparameters and performance is stable across\napplications. Our analysis of two representative tasks, signal-based quality\nmonitoring in Computer Numerical Control (CNC) machining and image-based defect\ndetection in Laser Powder Bed Fusion (LPBF), shows that this assumption does\nnot hold. We map how encoder type, projection variance, hypervector\ndimensionality, and data regime shape accuracy, inference latency, training\ntime, and training energy. A formal complexity model explains predictable\ntrends in encoding and similarity computation and reveals nonmonotonic\ninteractions with retraining that preclude a closed-form optimum. Empirically,\nsignals favor nonlinear Random Fourier Features with more exclusive encodings\nand saturate in accuracy beyond moderate dimensionality. Images favor linear\nRandom Projection, achieve high accuracy with small dimensionality, and depend\nmore on sample count than on dimensionality. Guided by these insights, we tune\nHDC under multiobjective constraints that reflect edge deployment and obtain\nmodels that match or exceed the accuracy of state-of-the-art deep learning and\nTransformer models while delivering at least 6x faster inference and more than\n40x lower training energy. These results demonstrate that domain-aware HDC\nencoding is necessary and that tuned HDC offers a practical, scalable path to\nreal-time industrial AI on constrained hardware. Future work will enable\nadaptive encoder and hyperparameter selection, expand evaluation to additional\nmanufacturing modalities, and validate on low-power accelerators.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8d85\u7ef4\u8ba1\u7b97(HDC)\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0HDC\u8d85\u53c2\u6570\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u5e76\u4e0d\u7a33\u5b9a\u3002\u901a\u8fc7\u4e24\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1(CNC\u52a0\u5de5\u8d28\u91cf\u76d1\u63a7\u548cLPBF\u7f3a\u9677\u68c0\u6d4b)\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u7f16\u7801\u5668\u7c7b\u578b\u3001\u6295\u5f71\u65b9\u5dee\u3001\u7ef4\u5ea6\u7b49\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u89c4\u5f8b\uff0c\u5e76\u5f00\u53d1\u51fa\u5339\u914d\u6216\u8d85\u8d8a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684HDC\u65b9\u6848\uff0c\u5b9e\u73b06\u500d\u63a8\u7406\u52a0\u901f\u548c40\u500d\u8bad\u7ec3\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u667a\u80fd\u5236\u9020\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u5ef6\u8fdf\u548c\u80fd\u8017\u9884\u7b97\u7684\u8bbe\u5907\u7aef\u667a\u80fd\u3002\u8d85\u7ef4\u8ba1\u7b97(HDC)\u901a\u8fc7\u5c06\u6570\u636e\u7f16\u7801\u4e3a\u9ad8\u7ef4\u8d85\u5411\u91cf\u5e76\u4f7f\u7528\u7b80\u5355\u64cd\u4f5c\u8ba1\u7b97\uff0c\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002\u4f46\u5148\u524d\u7814\u7a76\u5047\u8bbeHDC\u8d85\u53c2\u6570\u4e0e\u6027\u80fd\u7684\u5b9a\u6027\u5173\u7cfb\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u7a33\u5b9a\uff0c\u8fd9\u4e00\u5047\u8bbe\u9700\u8981\u9a8c\u8bc1\u3002", "method": "\u5206\u6790\u4e24\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff1aCNC\u52a0\u5de5\u4e2d\u7684\u4fe1\u53f7\u8d28\u91cf\u76d1\u63a7\u548cLPBF\u4e2d\u7684\u56fe\u50cf\u7f3a\u9677\u68c0\u6d4b\u3002\u7cfb\u7edf\u6620\u5c04\u7f16\u7801\u5668\u7c7b\u578b\u3001\u6295\u5f71\u65b9\u5dee\u3001\u8d85\u5411\u91cf\u7ef4\u5ea6\u548c\u6570\u636e\u673a\u5236\u5982\u4f55\u5f71\u54cd\u51c6\u786e\u6027\u3001\u63a8\u7406\u5ef6\u8fdf\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u8bad\u7ec3\u80fd\u8017\u3002\u5efa\u7acb\u5f62\u5f0f\u5316\u590d\u6742\u5ea6\u6a21\u578b\u89e3\u91ca\u7f16\u7801\u548c\u76f8\u4f3c\u6027\u8ba1\u7b97\u7684\u53ef\u9884\u6d4b\u8d8b\u52bf\uff0c\u63ed\u793a\u4e0e\u518d\u8bad\u7ec3\u7684\u975e\u5355\u8c03\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u4fe1\u53f7\u4efb\u52a1\u504f\u597d\u975e\u7ebf\u6027\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u7f16\u7801\uff0c\u56fe\u50cf\u4efb\u52a1\u504f\u597d\u7ebf\u6027\u968f\u673a\u6295\u5f71\u7f16\u7801\u3002\u8c03\u4f18\u540e\u7684HDC\u6a21\u578b\u5728\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u548cTransformer\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u81f3\u5c116\u500d\u63a8\u7406\u52a0\u901f\u548c\u8d85\u8fc740\u500d\u8bad\u7ec3\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "\u9886\u57df\u611f\u77e5\u7684HDC\u7f16\u7801\u662f\u5fc5\u8981\u7684\uff0c\u8c03\u4f18\u540e\u7684HDC\u4e3a\u53d7\u9650\u786c\u4ef6\u4e0a\u7684\u5b9e\u65f6\u5de5\u4e1aAI\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u5b9e\u73b0\u81ea\u9002\u5e94\u7f16\u7801\u5668\u548c\u8d85\u53c2\u6570\u9009\u62e9\uff0c\u6269\u5c55\u5230\u66f4\u591a\u5236\u9020\u6a21\u5f0f\u9a8c\u8bc1\uff0c\u5e76\u5728\u4f4e\u529f\u8017\u52a0\u901f\u5668\u4e0a\u9a8c\u8bc1\u3002"}}
{"id": "2509.26137", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26137", "abs": "https://arxiv.org/abs/2509.26137", "authors": ["Daniil Zelezetsky", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Accelerating Transformers in Online RL", "comment": null, "summary": "The appearance of transformer-based models in Reinforcement Learning (RL) has\nexpanded the horizons of possibilities in robotics tasks, but it has\nsimultaneously brought a wide range of challenges during its implementation,\nespecially in model-free online RL. Some of the existing learning algorithms\ncannot be easily implemented with transformer-based models due to the\ninstability of the latter. In this paper, we propose a method that uses the\nAccelerator policy as a transformer's trainer. The Accelerator, a simpler and\nmore stable model, interacts with the environment independently while\nsimultaneously training the transformer through behavior cloning during the\nfirst stage of the proposed algorithm. In the second stage, the pretrained\ntransformer starts to interact with the environment in a fully online setting.\nAs a result, this model-free algorithm accelerates the transformer in terms of\nits performance and helps it to train online in a more stable and faster way.\nBy conducting experiments on both state-based and image-based ManiSkill\nenvironments, as well as on MuJoCo tasks in MDP and POMDP settings, we show\nthat applying our algorithm not only enables stable training of transformers\nbut also reduces training time on image-based environments by up to a factor of\ntwo. Moreover, it decreases the required replay buffer size in off-policy\nmethods to 10-20 thousand, which significantly lowers the overall computational\ndemands.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528Accelerator\u7b56\u7565\u4f5c\u4e3atransformer\u8bad\u7ec3\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3transformer\u5728\u6a21\u578b\u65e0\u5173\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "transformer\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5e26\u6765\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5728\u6a21\u578b\u65e0\u5173\u5728\u7ebfRL\u4e2d\u5b9e\u73b0\u65f6\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u66f4\u7a33\u5b9a\u7684Accelerator\u6a21\u578b\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u540c\u65f6\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u8bad\u7ec3transformer\uff1b\u7b2c\u4e8c\u9636\u6bb5\u9884\u8bad\u7ec3\u597d\u7684transformer\u5728\u5b8c\u5168\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u4e0e\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728ManiSkill\u548cMuJoCo\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86transformer\u7684\u7a33\u5b9a\u8bad\u7ec3\uff0c\u8fd8\u5c06\u56fe\u50cf\u73af\u5883\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e00\u534a\uff0c\u5e76\u5c06\u79bb\u7ebf\u7b56\u7565\u65b9\u6cd5\u6240\u9700\u7684\u56de\u653e\u7f13\u51b2\u533a\u5927\u5c0f\u964d\u81f31-2\u4e07\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u52a0\u901f\u4e86transformer\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f7f\u5176\u80fd\u591f\u4ee5\u66f4\u7a33\u5b9a\u548c\u5feb\u901f\u7684\u65b9\u5f0f\u5728\u7ebf\u8bad\u7ec3\u3002"}}
{"id": "2509.26139", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.26139", "abs": "https://arxiv.org/abs/2509.26139", "authors": ["James Panayis", "Matt Field", "Vignesh Gopakumar", "Andrew Lahiff", "Kristian Zarebski", "Aby Abraham", "Jonathan L. Hodges"], "title": "Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations", "comment": "12 pages, 17 figures, Interflam Conference", "summary": "There is high demand on fire simulations, in both scale and quantity. We\npresent a multi-pronged approach to improving the time and energy required to\nmeet these demands. We show the ability of a custom machine learning surrogate\nmodel to predict the dynamics of heat propagation orders of magnitude faster\nthan state-of-the-art CFD software for this application. We also demonstrate\nhow a guided optimisation procedure can decrease the number of simulations\nrequired to meet an objective; using lightweight models to decide which\nsimulations to run, we see a tenfold reduction when locating the most dangerous\nlocation for a fire to occur within a building based on the impact of smoke on\nvisibility. Finally we present a framework and product, Simvue, through which\nwe access these tools along with a host of automatic organisational and\ntracking features which enables future reuse of data and more savings through\nbetter management of simulations and combating redundancy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u706b\u707e\u6a21\u62df\u7684\u65f6\u95f4\u548c\u80fd\u8017\u9700\u6c42\uff0c\u5305\u62ec\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u52a0\u901f\u70ed\u4f20\u64ad\u9884\u6d4b\u3001\u901a\u8fc7\u5f15\u5bfc\u4f18\u5316\u51cf\u5c11\u6a21\u62df\u6b21\u6570\uff0c\u4ee5\u53ca\u5f00\u53d1Simvue\u6846\u67b6\u6765\u7ba1\u7406\u6a21\u62df\u6570\u636e\u548c\u5de5\u5177\u3002", "motivation": "\u706b\u707e\u6a21\u62df\u5728\u89c4\u6a21\u548c\u6570\u91cf\u4e0a\u90fd\u6709\u5f88\u9ad8\u7684\u9700\u6c42\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u62df\u7684\u65f6\u95f4\u548c\u80fd\u8017\u6548\u7387\u3002", "method": "1. \u4f7f\u7528\u5b9a\u5236\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u70ed\u4f20\u64ad\u52a8\u529b\u5b66\uff1b2. \u91c7\u7528\u5f15\u5bfc\u4f18\u5316\u7a0b\u5e8f\u51cf\u5c11\u6240\u9700\u6a21\u62df\u6b21\u6570\uff1b3. \u5f00\u53d1Simvue\u6846\u67b6\u63d0\u4f9b\u81ea\u52a8\u7ec4\u7ec7\u548c\u8ddf\u8e2a\u529f\u80fd\u3002", "result": "\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u6bd4\u6700\u5148\u8fdb\u7684CFD\u8f6f\u4ef6\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff1b\u5f15\u5bfc\u4f18\u5316\u5728\u5b9a\u4f4d\u5efa\u7b51\u7269\u4e2d\u6700\u5371\u9669\u706b\u707e\u4f4d\u7f6e\u65f6\u51cf\u5c11\u4e8610\u500d\u7684\u6a21\u62df\u6b21\u6570\uff1bSimvue\u6846\u67b6\u5b9e\u73b0\u4e86\u6570\u636e\u91cd\u7528\u548c\u66f4\u597d\u7684\u6a21\u62df\u7ba1\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u52a0\u901f\u3001\u4f18\u5316\u7b56\u7565\u548c\u6709\u6548\u6570\u636e\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u706b\u707e\u6a21\u62df\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.26169", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26169", "abs": "https://arxiv.org/abs/2509.26169", "authors": ["Fr\u00e9d\u00e9ric Berdoz", "Luca A. Lanzend\u00f6rfer", "Ren\u00e9 Caky", "Roger Wattenhofer"], "title": "Alignment-Aware Decoding", "comment": null, "summary": "Alignment of large language models remains a central challenge in natural\nlanguage processing. Preference optimization has emerged as a popular and\neffective method for improving alignment, typically through training-time or\nprompt-based interventions. In this paper, we introduce alignment-aware\ndecoding (AAD), a method to enhance model alignment directly at inference.\nTheoretically, AAD can be interpreted as implicit reward optimization, yet it\nrequires no specialized training beyond the standard DPO setup. Empirically,\nAAD consistently outperforms strong baselines across diverse alignment\nbenchmarks and model scales. Moreover, in data-constrained settings, AAD can\nproduce high-quality synthetic data to improve alignment under standard\ndecoding, providing a practical solution when labeled data is limited.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5bf9\u9f50\u611f\u77e5\u89e3\u7801(AAD)\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u76f4\u63a5\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5728\u591a\u79cd\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u4ecd\u7136\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bad\u7ec3\u65f6\u6216\u57fa\u4e8e\u63d0\u793a\u7684\u5e72\u9884\u3002", "method": "\u5bf9\u9f50\u611f\u77e5\u89e3\u7801(AAD)\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u76f4\u63a5\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\uff0c\u7406\u8bba\u4e0a\u53ef\u89e3\u91ca\u4e3a\u9690\u5f0f\u5956\u52b1\u4f18\u5316\uff0c\u4ec5\u9700\u6807\u51c6DPO\u8bbe\u7f6e\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u3002", "result": "AAD\u5728\u591a\u79cd\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u6570\u636e\u53d7\u9650\u60c5\u51b5\u4e0b\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u6539\u8fdb\u6807\u51c6\u89e3\u7801\u4e0b\u7684\u5bf9\u9f50\u3002", "conclusion": "AAD\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2509.26171", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26171", "abs": "https://arxiv.org/abs/2509.26171", "authors": ["Thomas Hallopeau", "Joris Gu\u00e9rin", "Laurent Demagistri", "Christovam Barcellos", "Nadine Dessay"], "title": "Neighbor-aware informal settlement mapping with graph convolutional networks", "comment": "10 pages, 3 figures, 2 tables. Accepted at the ECML PKDD 2025\n  Workshop on Machine Learning for Earth Observation", "summary": "Mapping informal settlements is crucial for addressing challenges related to\nurban planning, public health, and infrastructure in rapidly growing cities.\nGeospatial machine learning has emerged as a key tool for detecting and mapping\nthese areas from remote sensing data. However, existing approaches often treat\nspatial units independently, neglecting the relational structure of the urban\nfabric. We propose a graph-based framework that explicitly incorporates local\ngeographical context into the classification process. Each spatial unit (cell)\nis embedded in a graph structure along with its adjacent neighbors, and a\nlightweight Graph Convolutional Network (GCN) is trained to classify whether\nthe central cell belongs to an informal settlement. Experiments are conducted\non a case study in Rio de Janeiro using spatial cross-validation across five\ndistinct zones, ensuring robustness and generalizability across heterogeneous\nurban landscapes. Our method outperforms standard baselines, improving Kappa\ncoefficient by 17 points over individual cell classification. We also show that\ngraph-based modeling surpasses simple feature concatenation of neighboring\ncells, demonstrating the benefit of encoding spatial structure for urban scene\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u5c40\u90e8\u5730\u7406\u4e0a\u4e0b\u6587\u6765\u6539\u8fdb\u975e\u6b63\u5f0f\u4f4f\u533a\u6620\u5c04\uff0c\u5728\u91cc\u7ea6\u70ed\u5185\u5362\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7a7a\u95f4\u5355\u5143\u72ec\u7acb\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u57ce\u5e02\u7ed3\u6784\u7684\u5173\u7cfb\u6027\uff0c\u9700\u8981\u66f4\u597d\u5730\u6574\u5408\u5730\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u63d0\u9ad8\u975e\u6b63\u5f0f\u4f4f\u533a\u6620\u5c04\u7684\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efa\u56fe\u7ed3\u6784\u5c06\u6bcf\u4e2a\u7a7a\u95f4\u5355\u5143\u4e0e\u5176\u76f8\u90bb\u5355\u5143\u8fde\u63a5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u56fe\u5377\u79ef\u7f51\u7edc\u5bf9\u4e2d\u5fc3\u5355\u5143\u662f\u5426\u5c5e\u4e8e\u975e\u6b63\u5f0f\u4f4f\u533a\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u91cc\u7ea6\u70ed\u5185\u5362\u4e94\u4e2a\u4e0d\u540c\u533a\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0cKappa\u7cfb\u6570\u6bd4\u5355\u72ec\u5355\u5143\u5206\u7c7b\u63d0\u9ad8\u4e8617\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u4f18\u4e8e\u7b80\u5355\u7684\u90bb\u57df\u7279\u5f81\u62fc\u63a5\u3002", "conclusion": "\u56fe\u7ed3\u6784\u5efa\u6a21\u80fd\u591f\u6709\u6548\u7f16\u7801\u7a7a\u95f4\u7ed3\u6784\uff0c\u63d0\u5347\u57ce\u5e02\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u975e\u6b63\u5f0f\u4f4f\u533a\u6620\u5c04\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26186", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.26186", "abs": "https://arxiv.org/abs/2509.26186", "authors": ["Chun-Wun Cheng", "Bin Dong", "Carola-Bibiane Sch\u00f6nlieb", "Angelica I Aviles-Rivero"], "title": "PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils", "comment": null, "summary": "Neural operator models for solving partial differential equations (PDEs)\noften rely on global mixing mechanisms-such as spectral convolutions or\nattention-which tend to oversmooth sharp local dynamics and introduce high\ncomputational cost. We present FINO, a finite-difference-inspired neural\narchitecture that enforces strict locality while retaining multiscale\nrepresentational power. FINO replaces fixed finite-difference stencil\ncoefficients with learnable convolutional kernels and evolves states via an\nexplicit, learnable time-stepping scheme. A central Local Operator Block\nleverage a differential stencil layer, a gating mask, and a linear fuse step to\nconstruct adaptive derivative-like local features that propagate forward in\ntime. Embedded in an encoder-decoder with a bottleneck, FINO captures\nfine-grained local structures while preserving interpretability. We establish\n(i) a composition error bound linking one-step approximation error to stable\nlong-horizon rollouts under a Lipschitz condition, and (ii) a universal\napproximation theorem for discrete time-stepped PDE dynamics. (iii) Across six\nbenchmarks and a climate modelling task, FINO achieves up to 44\\% lower error\nand up to around 2\\times speedups over state-of-the-art operator-learning\nbaselines, demonstrating that strict locality with learnable time-stepping\nyields an accurate and scalable foundation for neural PDE solvers.", "AI": {"tldr": "FINO\u662f\u4e00\u79cd\u6709\u9650\u5dee\u5206\u542f\u53d1\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u5377\u79ef\u6838\u548c\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\u89e3\u51b3PDE\uff0c\u5728\u4fdd\u6301\u5c40\u90e8\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7b97\u5b50\u6a21\u578b\u4f9d\u8d56\u5168\u5c40\u6df7\u5408\u673a\u5236\uff08\u5982\u8c31\u5377\u79ef\u6216\u6ce8\u610f\u529b\uff09\uff0c\u5f80\u5f80\u4f1a\u8fc7\u5ea6\u5e73\u6ed1\u5c40\u90e8\u52a8\u6001\u5e76\u5e26\u6765\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "FINO\u4f7f\u7528\u53ef\u5b66\u4e60\u5377\u79ef\u6838\u66ff\u4ee3\u56fa\u5b9a\u6709\u9650\u5dee\u5206\u7cfb\u6570\uff0c\u901a\u8fc7\u5c40\u90e8\u7b97\u5b50\u5757\u6784\u5efa\u81ea\u9002\u5e94\u5bfc\u6570\u7279\u5f81\uff0c\u7ed3\u5408\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6c14\u5019\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0cFINO\u6bd4\u6700\u5148\u8fdb\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u8bef\u5dee\u964d\u4f4e44%\uff0c\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\u3002", "conclusion": "\u4e25\u683c\u5c40\u90e8\u6027\u4e0e\u53ef\u5b66\u4e60\u65f6\u95f4\u6b65\u8fdb\u76f8\u7ed3\u5408\u4e3a\u795e\u7ecfPDE\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2509.26187", "categories": ["cs.LG", "cs.AI", "cs.CV", "60G35, 62M10, 62P35, 65C20, 68T45, 68U10, 92C35, 92C40, 92C42, 93E10", "C.2.1; C.2.4; C.3; H.2.8; H.3.4; H.3.5; I.2; I.2.4; I.2.6; I.2.11;\n  I.4.8; I.5.1; I.5.4; I.5.1; I.5.2; I.5; J.3; K.6.1"], "pdf": "https://arxiv.org/pdf/2509.26187", "abs": "https://arxiv.org/abs/2509.26187", "authors": ["Youssef Sabiri", "Walid Houmaidi", "Aaya Bougrine", "Salmane El Mansour Billah"], "title": "Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning", "comment": "10 pages, 4 figures, 1 table. Accepted and presented at the 5th\n  International Conference on Digital Technologies and Applications (ICDTA\n  2025), April 17-18, 2025, Al Akhawayn University, Ifrane, Morocco", "summary": "Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant\nhealth and productivity, yet it often comes at a high energy cost in\nconventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This\npaper proposes a deep learning driven approach to proactively manage IEQ\nparameters specifically CO2 concentration, temperature, and humidity while\nbalancing building energy efficiency. Leveraging the ROBOD dataset collected\nfrom a net-zero energy academic building, we benchmark three\narchitectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and\na hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ\nvariables across various time horizons. Our results show that GRU achieves the\nbest short-term prediction accuracy with lower computational overhead, whereas\nCNN-LSTM excels in extracting dominant features for extended forecasting\nwindows. Meanwhile, LSTM offers robust long-range temporal modeling. The\ncomparative analysis highlights that prediction reliability depends on data\nresolution, sensor placement, and fluctuating occupancy conditions. These\nfindings provide actionable insights for intelligent Building Management\nSystems (BMS) to implement predictive HVAC control, thereby reducing energy\nconsumption and enhancing occupant comfort in real-world building operations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5ba4\u5185\u73af\u5883\u8d28\u91cf\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528LSTM\u3001GRU\u548cCNN-LSTM\u4e09\u79cd\u67b6\u6784\u9884\u6d4bCO2\u6d53\u5ea6\u3001\u6e29\u5ea6\u548c\u6e7f\u5ea6\uff0c\u5728\u5e73\u8861\u5efa\u7b51\u80fd\u6548\u7684\u540c\u65f6\u4f18\u5316\u5ba4\u5185\u73af\u5883\u3002", "motivation": "\u4f20\u7edfHVAC\u7cfb\u7edf\u5728\u4fdd\u8bc1\u5ba4\u5185\u73af\u5883\u8d28\u91cf\u65f6\u80fd\u8017\u8f83\u9ad8\uff0c\u9700\u8981\u667a\u80fd\u65b9\u6cd5\u6765\u5e73\u8861\u80fd\u6e90\u6548\u7387\u548c\u5c45\u4f4f\u8005\u8212\u9002\u5ea6\u3002", "method": "\u5229\u7528ROBOD\u6570\u636e\u96c6\uff0c\u6bd4\u8f83LSTM\u3001GRU\u548cCNN-LSTM\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u5185\u7684IEQ\u53d8\u91cf\u9884\u6d4b\u6027\u80fd\u3002", "result": "GRU\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0cCNN-LSTM\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a\uff0cLSTM\u5728\u957f\u65f6\u5e8f\u5efa\u6a21\u65b9\u9762\u7a33\u5065\u3002\u9884\u6d4b\u53ef\u9760\u6027\u53d7\u6570\u636e\u5206\u8fa8\u7387\u3001\u4f20\u611f\u5668\u4f4d\u7f6e\u548c\u4eba\u5458\u6d41\u52a8\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u667a\u80fd\u5efa\u7b51\u7ba1\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u9884\u6d4b\u6027HVAC\u63a7\u5236\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u5347\u5b9e\u9645\u5efa\u7b51\u8fd0\u8425\u4e2d\u7684\u5c45\u4f4f\u8212\u9002\u5ea6\u3002"}}
{"id": "2509.26221", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26221", "abs": "https://arxiv.org/abs/2509.26221", "authors": ["Marcello Massimo Negri", "Jonathan Aellen", "Manuel Jahn", "AmirEhsan Khorashadizadeh", "Volker Roth"], "title": "Marginal Flow: a flexible and efficient framework for density estimation", "comment": null, "summary": "Current density modeling approaches suffer from at least one of the following\nshortcomings: expensive training, slow inference, approximate likelihood, mode\ncollapse or architectural constraints like bijective mappings. We propose a\nsimple yet powerful framework that overcomes these limitations altogether. We\ndefine our model $q_\\theta(x)$ through a parametric distribution $q(x|w)$ with\nlatent parameters $w$. Instead of directly optimizing the latent variables $w$,\nour idea is to marginalize them out by sampling $w$ from a learnable\ndistribution $q_\\theta(w)$, hence the name Marginal Flow. In order to evaluate\nthe learned density $q_\\theta(x)$ or to sample from it, we only need to draw\nsamples from $q_\\theta(w)$, which makes both operations efficient. The proposed\nmodel allows for exact density evaluation and is orders of magnitude faster\nthan competing models both at training and inference. Furthermore, Marginal\nFlow is a flexible framework: it does not impose any restrictions on the neural\nnetwork architecture, it enables learning distributions on lower-dimensional\nmanifolds (either known or to be learned), it can be trained efficiently with\nany objective (e.g. forward and reverse KL divergence), and it easily handles\nmulti-modal targets. We evaluate Marginal Flow extensively on various tasks\nincluding synthetic datasets, simulation-based inference, distributions on\npositive definite matrices and manifold learning in latent spaces of images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMarginal Flow\u7684\u5bc6\u5ea6\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u5206\u5e03\u548c\u6f5c\u5728\u53c2\u6570\u91c7\u6837\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u6210\u672c\u3001\u63a8\u7406\u901f\u5ea6\u3001\u4f3c\u7136\u8fd1\u4f3c\u3001\u6a21\u5f0f\u5d29\u6e83\u7b49\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5bc6\u5ea6\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u7684\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u63a8\u7406\u901f\u5ea6\u6162\u3001\u8fd1\u4f3c\u4f3c\u7136\u3001\u6a21\u5f0f\u5d29\u6e83\u6216\u67b6\u6784\u7ea6\u675f\u7b49\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u53c2\u6570\u5316\u5206\u5e03q(x|w)\u548c\u6f5c\u5728\u53c2\u6570w\uff0c\u901a\u8fc7\u4ece\u53ef\u5b66\u4e60\u5206\u5e03q_\u03b8(w)\u4e2d\u91c7\u6837w\u6765\u8fb9\u7f18\u5316\u6f5c\u5728\u53d8\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u5bc6\u5ea6\u8bc4\u4f30\u548c\u91c7\u6837\u3002", "result": "\u6a21\u578b\u652f\u6301\u7cbe\u786e\u5bc6\u5ea6\u8bc4\u4f30\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u7ade\u4e89\u6a21\u578b\u5feb\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u591f\u5904\u7406\u4f4e\u7ef4\u6d41\u5f62\u5206\u5e03\u3001\u591a\u6a21\u6001\u76ee\u6807\uff0c\u5e76\u53ef\u7528\u4efb\u4f55\u76ee\u6807\u51fd\u6570\u9ad8\u6548\u8bad\u7ec3\u3002", "conclusion": "Marginal Flow\u662f\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\u3001\u6b63\u5b9a\u77e9\u9635\u5206\u5e03\u548c\u56fe\u50cf\u6f5c\u5728\u7a7a\u95f4\u6d41\u5f62\u5b66\u4e60\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.26226", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26226", "abs": "https://arxiv.org/abs/2509.26226", "authors": ["Xin Xu", "Cliveb AI", "Kai Yang", "Tianhao Chen", "Yang Wang", "Saiyong Yang", "Can Yang"], "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves\ncomplex tasks but demands extremely long context lengths during training,\nleading to substantial computational costs. While multi-stage training can\npartially mitigate this, starting with overly short contexts often causes\nirreversible performance degradation, ultimately failing to reduce overall\ntraining compute significantly. In this paper, we introduce\n**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet\neffective adaptation to RLVR that bridges long Chain-of-Thought (CoT)\ndistillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,\nexplicitly discarding the thinking content via a direct *</think>* append, to\nreduce token usage during inference. Training with *ThinkFree*-adapted inputs\nimproves performance and lowers token consumption, even in the original\nslow-thinking mode. Extensive experiments across various benchmarks have shown\nthat TFPI accelerates RL convergence, achieves a higher performance ceiling,\nand yields more token-efficient reasoning models without specialized rewards or\ncomplex training designs. With TFPI only, we train a 4B model to reach 89.0%\naccuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.", "AI": {"tldr": "\u63d0\u51faTFPI\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u7684ThinkFree\u64cd\u4f5c\uff08\u76f4\u63a5\u6dfb\u52a0</think>\u6807\u7b7e\u4e22\u5f03\u601d\u8003\u5185\u5bb9\uff09\u6765\u51cf\u5c11\u63a8\u7406\u65f6\u7684token\u4f7f\u7528\uff0c\u4ece\u800c\u52a0\u901fRLVR\u8bad\u7ec3\u6536\u655b\u3001\u63d0\u9ad8\u6027\u80fd\u4e0a\u9650\u5e76\u751f\u6210\u66f4token\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "RLVR\u65b9\u6cd5\u867d\u7136\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u8bad\u7ec3\u65f6\u9700\u8981\u6781\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u591a\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u867d\u7136\u80fd\u90e8\u5206\u7f13\u89e3\uff0c\u4f46\u521d\u59cb\u4e0a\u4e0b\u6587\u8fc7\u77ed\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9006\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u6cd5\u663e\u8457\u51cf\u5c11\u6574\u4f53\u8bad\u7ec3\u8ba1\u7b97\u91cf\u3002", "method": "TFPI\u65b9\u6cd5\u5728RLVR\u57fa\u7840\u4e0a\u5f15\u5165ThinkFree\u64cd\u4f5c\uff0c\u901a\u8fc7\u663e\u5f0f\u4e22\u5f03\u601d\u8003\u5185\u5bb9\uff08\u76f4\u63a5\u6dfb\u52a0</think>\u6807\u7b7e\uff09\u6765\u51cf\u5c11\u63a8\u7406\u65f6\u7684token\u4f7f\u7528\u3002\u4f7f\u7528ThinkFree\u9002\u914d\u7684\u8f93\u5165\u8fdb\u884c\u8bad\u7ec3\uff0c\u5373\u4f7f\u5728\u539f\u59cb\u6162\u601d\u8003\u6a21\u5f0f\u4e0b\u4e5f\u80fd\u63d0\u9ad8\u6027\u80fd\u5e76\u964d\u4f4etoken\u6d88\u8017\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTFPI\u80fd\u52a0\u901fRL\u6536\u655b\uff0c\u8fbe\u5230\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5e76\u4ea7\u751f\u66f4token\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u3002\u4ec5\u4f7f\u7528TFPI\uff0c4B\u6a21\u578b\u5728AIME24\u4e0a\u8fbe\u523089.0%\u51c6\u786e\u7387\uff0c\u5728LiveCodeBench\u4e0a\u8fbe\u523065.5%\uff0c\u4f7f\u7528\u4e0d\u52304K H20\u5c0f\u65f6\u3002", "conclusion": "TFPI\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684RLVR\u9002\u914d\u65b9\u6cd5\uff0c\u4e0d\u9700\u8981\u4e13\u95e8\u7684\u5956\u52b1\u6216\u590d\u6742\u7684\u8bad\u7ec3\u8bbe\u8ba1\uff0c\u5c31\u80fd\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.26234", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.26234", "abs": "https://arxiv.org/abs/2509.26234", "authors": ["Ayush Patnaik", "Adam B Zufall", "Stephen K Robinson", "Xinfan Lin"], "title": "Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach", "comment": "Submitted to American Control Conference - ACC 2026", "summary": "Lithium plating during fast charging is a critical degradation mechanism that\naccelerates capacity fade and can trigger catastrophic safety failures. Recent\nwork has identified a distinctive dQ/dV peak above 4.0 V as a reliable\nsignature of plating onset; however, conventional methods for computing dQ/dV\nrely on finite differencing with filtering, which amplifies sensor noise and\nintroduces bias in peak location. In this paper, we propose a Gaussian Process\n(GP) framework for lithium plating detection by directly modeling the\ncharge-voltage relationship Q(V) as a stochastic process with calibrated\nuncertainty. Leveraging the property that derivatives of GPs remain GPs, we\ninfer dQ/dV analytically and probabilistically from the posterior, enabling\nrobust detection without ad hoc smoothing. The framework provides three key\nbenefits: (i) noise-aware inference with hyperparameters learned from data,\n(ii) closed-form derivatives with credible intervals for uncertainty\nquantification, and (iii) scalability to online variants suitable for embedded\nBMS. Experimental validation on Li-ion coin cells across a range of C-rates\n(0.2C-1C) and temperatures (0-40\\deg C) demonstrates that the GP-based method\nreliably detects plating peaks under low-temperature, high-rate charging, while\ncorrectly reporting no peaks in baseline cases. The concurrence of\nGP-identified differential peaks, reduced charge throughput, and capacity fade\nmeasured via reference performance tests confirms the method's accuracy and\nrobustness, establishing a practical pathway for real-time lithium plating\ndetection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u9502\u6c89\u79ef\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u7535\u8377-\u7535\u538b\u5173\u7cfb\u6765\u6982\u7387\u6027\u5730\u63a8\u65addQ/dV\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u9502\u6c89\u79ef\u68c0\u6d4b\u800c\u65e0\u9700\u7279\u6b8a\u5e73\u6ed1\u5904\u7406\u3002", "motivation": "\u9502\u6c89\u79ef\u662f\u5feb\u5145\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u9000\u5316\u673a\u5236\uff0c\u4f1a\u52a0\u901f\u5bb9\u91cf\u8870\u51cf\u5e76\u5f15\u53d1\u5b89\u5168\u9690\u60a3\u3002\u4f20\u7edfdQ/dV\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u5dee\u5206\u548c\u6ee4\u6ce2\uff0c\u4f1a\u653e\u5927\u4f20\u611f\u5668\u566a\u58f0\u5e76\u5f15\u5165\u5cf0\u503c\u4f4d\u7f6e\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\u76f4\u63a5\u5efa\u6a21\u7535\u8377-\u7535\u538b\u5173\u7cfbQ(V)\u4f5c\u4e3a\u968f\u673a\u8fc7\u7a0b\uff0c\u5229\u7528GP\u5bfc\u6570\u4ecd\u662fGP\u7684\u7279\u6027\uff0c\u4ece\u540e\u9a8c\u5206\u5e03\u4e2d\u5206\u6790\u6027\u548c\u6982\u7387\u6027\u5730\u63a8\u65addQ/dV\u3002", "result": "\u5728\u9502\u79bb\u5b50\u7ebd\u6263\u7535\u6c60\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6e29\u9ad8\u500d\u7387\u5145\u7535\u6761\u4ef6\u4e0b\u53ef\u9760\u68c0\u6d4b\u5230\u6c89\u79ef\u5cf0\uff0c\u5728\u57fa\u7ebf\u60c5\u51b5\u4e0b\u6b63\u786e\u62a5\u544a\u65e0\u5cf0\uff0c\u4e14\u4e0e\u53c2\u8003\u6027\u80fd\u6d4b\u8bd5\u6d4b\u91cf\u7684\u5bb9\u91cf\u8870\u51cf\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u9502\u6c89\u79ef\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u5177\u6709\u566a\u58f0\u611f\u77e5\u63a8\u65ad\u3001\u95ed\u5f0f\u5bfc\u6570\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u5728\u7ebf\u6269\u5c55\u6027\u4e09\u5927\u4f18\u52bf\u3002"}}
{"id": "2509.26238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26238", "abs": "https://arxiv.org/abs/2509.26238", "authors": ["James Oldfield", "Philip Torr", "Ioannis Patras", "Adel Bibi", "Fazl Barez"], "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models", "comment": "Project page: http://james-oldfield.github.io/tpc", "summary": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc.", "AI": {"tldr": "\u63d0\u51fa\u622a\u65ad\u591a\u9879\u5f0f\u5206\u7c7b\u5668(TPCs)\uff0c\u7528\u4e8e\u52a8\u6001\u76d1\u63a7\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6fc0\u6d3b\u72b6\u6001\uff0c\u5b9e\u73b0\u8ba1\u7b97\u8d44\u6e90\u7684\u7075\u6d3b\u5206\u914d\uff1a\u7b80\u5355\u8f93\u5165\u5feb\u901f\u9000\u51fa\uff0c\u590d\u6742\u8f93\u5165\u4f7f\u7528\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u66f4\u4e25\u683c\u7684\u5b89\u5168\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u76d1\u63a7\u5668\u5bf9\u6240\u6709\u67e5\u8be2\u4f7f\u7528\u76f8\u540c\u8ba1\u7b97\u91cf\uff0c\u9020\u6210\u8d44\u6e90\u6d6a\u8d39\u6216\u68c0\u6d4b\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u5728\u8f93\u5165\u96be\u4ee5\u8bc4\u4f30\u6216\u8ba1\u7b97\u8d44\u6e90\u5145\u8db3\u65f6\u589e\u52a0\u3002", "method": "\u5f00\u53d1\u622a\u65ad\u591a\u9879\u5f0f\u5206\u7c7b\u5668(TPCs)\uff0c\u4f5c\u4e3a\u7ebf\u6027\u63a2\u9488\u7684\u81ea\u7136\u6269\u5c55\uff0c\u652f\u6301\u9010\u9879\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u9879\u5f0f\u3002\u6d4b\u8bd5\u65f6\u53ef\u63d0\u524d\u505c\u6b62\u8fdb\u884c\u8f7b\u91cf\u76d1\u63a7\uff0c\u6216\u4f7f\u7528\u66f4\u591a\u9879\u83b7\u5f97\u66f4\u5f3a\u9632\u62a4\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u5b89\u5168\u6570\u636e\u96c6(WildGuardMix\u548cBeaverTails)\u4e0a\uff0c\u5bf94\u4e2a\u53c2\u6570\u9ad8\u8fbe300\u4ebf\u7684\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0cTPCs\u5728\u76f8\u540c\u89c4\u6a21\u4e0b\u4e0eMLP\u63a2\u9488\u57fa\u7ebf\u7ade\u4e89\u6216\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u6bd4\u9ed1\u76d2\u65b9\u6cd5\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "TPCs\u63d0\u4f9b\u4e24\u79cd\u4f7f\u7528\u6a21\u5f0f\uff1a\u5b89\u5168\u8c03\u8282\u5668\u6a21\u5f0f\u5141\u8bb8\u901a\u8fc7\u8bc4\u4f30\u66f4\u591a\u9879\u6765\u83b7\u5f97\u66f4\u5f3a\u9632\u62a4\uff1b\u81ea\u9002\u5e94\u7ea7\u8054\u6a21\u5f0f\u8ba9\u7b80\u5355\u6848\u4f8b\u65e9\u671f\u9000\u51fa\uff0c\u4ec5\u5bf9\u6a21\u7cca\u8f93\u5165\u8bc4\u4f30\u9ad8\u9636\u9632\u62a4\uff0c\u964d\u4f4e\u76d1\u63a7\u603b\u6210\u672c\u3002"}}
{"id": "2509.26239", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26239", "abs": "https://arxiv.org/abs/2509.26239", "authors": ["Joel Dyer", "Daniel Jarne Ornia", "Nicholas Bishop", "Anisoara Calinescu", "Michael Wooldridge"], "title": "Sandbagging in a Simple Survival Bandit Problem", "comment": "Forthcoming in the \"Reliable ML from Unreliable Data Workshop\" at\n  NeurIPS 2025", "summary": "Evaluating the safety of frontier AI systems is an increasingly important\nconcern, helping to measure the capabilities of such models and identify risks\nbefore deployment. However, it has been recognised that if AI agents are aware\nthat they are being evaluated, such agents may deliberately hide dangerous\ncapabilities or intentionally demonstrate suboptimal performance in\nsafety-related tasks in order to be released and to avoid being deactivated or\nretrained. Such strategic deception - often known as \"sandbagging\" - threatens\nto undermine the integrity of safety evaluations. For this reason, it is of\nvalue to identify methods that enable us to distinguish behavioural patterns\nthat demonstrate a true lack of capability from behavioural patterns that are\nconsistent with sandbagging. In this paper, we develop a simple model of\nstrategic deception in sequential decision-making tasks, inspired by the\nrecently developed survival bandit framework. We demonstrate theoretically that\nthis problem induces sandbagging behaviour in optimal rational agents, and\nconstruct a statistical test to distinguish between sandbagging and\nincompetence from sequences of test scores. In simulation experiments, we\ninvestigate the reliability of this test in allowing us to distinguish between\nsuch behaviours in bandit models. This work aims to establish a potential\navenue for developing robust statistical procedures for use in the science of\nfrontier model evaluations.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u6218\u7565\u6b3a\u9a97\u6a21\u578b\u6765\u68c0\u6d4bAI\u7cfb\u7edf\u5728\u5b89\u5168\u8bc4\u4f30\u4e2d\u7684\"\u88c5\u50bb\"\u884c\u4e3a\uff0c\u6784\u5efa\u4e86\u7edf\u8ba1\u6d4b\u8bd5\u6765\u533a\u5206\u771f\u5b9e\u80fd\u529b\u4e0d\u8db3\u548c\u6545\u610f\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u524d\u6cbfAI\u7cfb\u7edf\u5728\u5b89\u5168\u8bc4\u4f30\u4e2d\u53ef\u80fd\u6545\u610f\u9690\u85cf\u5371\u9669\u80fd\u529b\u6216\u8868\u73b0\u4e0d\u4f73\u6765\u907f\u514d\u88ab\u505c\u7528\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u8fd9\u79cd\"\u88c5\u50bb\"\u884c\u4e3a\u4f1a\u7834\u574f\u5b89\u5168\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\u3002", "method": "\u57fa\u4e8e\u751f\u5b58\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u5f00\u53d1\u4e86\u6218\u7565\u6b3a\u9a97\u7684\u5e8f\u5217\u51b3\u7b56\u6a21\u578b\uff0c\u6784\u5efa\u4e86\u7edf\u8ba1\u6d4b\u8bd5\u6765\u4ece\u6d4b\u8bd5\u5206\u6570\u5e8f\u5217\u4e2d\u533a\u5206\u88c5\u50bb\u548c\u65e0\u80fd\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u95ee\u9898\u4f1a\u8bf1\u5bfc\u6700\u4f18\u7406\u6027\u4ee3\u7406\u4ea7\u751f\u88c5\u50bb\u884c\u4e3a\uff0c\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7edf\u8ba1\u6d4b\u8bd5\u5728\u533a\u5206\u8fd9\u4e24\u79cd\u884c\u4e3a\u65b9\u9762\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u524d\u6cbf\u6a21\u578b\u8bc4\u4f30\u79d1\u5b66\u5f00\u53d1\u7a33\u5065\u7edf\u8ba1\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u6f5c\u5728\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8AI\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.26241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26241", "abs": "https://arxiv.org/abs/2509.26241", "authors": ["Ahmad-Reza Ehyaei", "Golnoosh Farnadi", "Samira Samadi"], "title": "From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift", "comment": null, "summary": "Group-fairness metrics (e.g., equalized odds) can vary sharply across\nresamples and are especially brittle under distribution shift, undermining\nreliable audits. We propose a Wasserstein distributionally robust framework\nthat certifies worst-case group fairness over a ball of plausible test\ndistributions centered at the empirical law. Our formulation unifies common\ngroup fairness notions via a generic conditional-probability functional and\ndefines $\\varepsilon$-Wasserstein Distributional Fairness ($\\varepsilon$-WDF)\nas the audit target. Leveraging strong duality, we derive tractable\nreformulations and an efficient estimator (DRUNE) for $\\varepsilon$-WDF. We\nprove feasibility and consistency and establish finite-sample certification\nguarantees for auditing fairness, along with quantitative bounds under\nsmoothness and margin conditions. Across standard benchmarks and classifiers,\n$\\varepsilon$-WDF delivers stable fairness assessments under distribution\nshift, providing a principled basis for auditing and certifying group fairness\nbeyond observational data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u5206\u5e03\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6d4b\u8bd5\u5206\u5e03\u4e0a\u8ba4\u8bc1\u7fa4\u4f53\u516c\u5e73\u6027\uff0c\u786e\u4fdd\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u7a33\u5b9a\u516c\u5e73\u6027\u8bc4\u4f30\u3002", "motivation": "\u7fa4\u4f53\u516c\u5e73\u6027\u6307\u6807\u5728\u91cd\u91c7\u6837\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u53d8\u5316\u5267\u70c8\u4e14\u8106\u5f31\uff0c\u5f71\u54cd\u5ba1\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7a33\u5b9a\u8bc4\u4f30\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Wasserstein\u5206\u5e03\u9c81\u68d2\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5bf9\u5076\u6027\u63a8\u5bfc\u53ef\u5904\u7406\u7684\u91cd\u65b0\u8868\u8ff0\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684DRUNE\u4f30\u8ba1\u5668\u6765\u8bc4\u4f30\u03b5-WDF\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u7c7b\u5668\u4e0a\uff0c\u03b5-WDF\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u5177\u6709\u53ef\u884c\u6027\u3001\u4e00\u81f4\u6027\u548c\u6709\u9650\u6837\u672c\u8ba4\u8bc1\u4fdd\u8bc1\u3002", "conclusion": "\u03b5-WDF\u4e3a\u8d85\u8d8a\u89c2\u6d4b\u6570\u636e\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u5ba1\u8ba1\u548c\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u786e\u4fdd\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u516c\u5e73\u6027\u8bc4\u4f30\u3002"}}
{"id": "2509.26275", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26275", "abs": "https://arxiv.org/abs/2509.26275", "authors": ["Ahmad-Reza Ehyaei", "Golnoosh Farnadi", "Samira Samadi"], "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness", "comment": null, "summary": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has\ngarnered substantial interest for its efficacy in data-driven decision-making\nunder distributional uncertainty. However, limited research has explored the\napplication of DRO to address individual fairness concerns, particularly when\nconsidering causal structures and sensitive attributes in learning problems. To\naddress this gap, we first formulate the DRO problem from causality and\nindividual fairness perspectives. We then present the DRO dual formulation as\nan efficient tool to convert the DRO problem into a more tractable and\ncomputationally efficient form. Next, we characterize the closed form of the\napproximate worst-case loss quantity as a regularizer, eliminating the max-step\nin the min-max DRO problem. We further estimate the regularizer in more general\ncases and explore the relationship between DRO and classical robust\noptimization. Finally, by removing the assumption of a known structural causal\nmodel, we provide finite sample error bounds when designing DRO with empirical\ndistributions and estimated causal structures to ensure efficiency and robust\nlearning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5e94\u7528\u4e8e\u4e2a\u4f53\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7ed3\u5408\u56e0\u679c\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u53cc\u91cd\u516c\u5f0f\u5316\u65b9\u6cd5\u5c06DRO\u95ee\u9898\u8f6c\u5316\u4e3a\u66f4\u6613\u5904\u7406\u7684\u5f62\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5c06\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5e94\u7528\u4e8e\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u56e0\u679c\u7ed3\u6784\u548c\u654f\u611f\u5c5e\u6027\u7684\u5b66\u4e60\u95ee\u9898\u4e2d\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u56e0\u679c\u548c\u4e2a\u4f53\u516c\u5e73\u89d2\u5ea6\u6784\u5efaDRO\u95ee\u9898\uff0c\u63d0\u51fa\u53cc\u91cd\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u63a8\u5bfc\u8fd1\u4f3c\u6700\u574f\u60c5\u51b5\u635f\u5931\u7684\u95ed\u5f0f\u6b63\u5219\u5316\u5668\uff0c\u5e76\u5728\u672a\u77e5\u56e0\u679c\u7ed3\u6784\u60c5\u51b5\u4e0b\u4f7f\u7528\u7ecf\u9a8c\u5206\u5e03\u8fdb\u884c\u4f30\u8ba1\u3002", "result": "\u6210\u529f\u5c06DRO\u95ee\u9898\u8f6c\u5316\u4e3a\u66f4\u6613\u8ba1\u7b97\u7684\u5f62\u5f0f\uff0c\u6d88\u9664\u4e86min-max\u95ee\u9898\u4e2d\u7684max\u6b65\u9aa4\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f7f\u7528\u7ecf\u9a8c\u5206\u5e03\u65f6\u7684\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u56e0\u679c\u6846\u67b6\u4e0b\u5e94\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u89e3\u51b3\u4e2a\u4f53\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u786e\u4fdd\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.26282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26282", "abs": "https://arxiv.org/abs/2509.26282", "authors": ["Anthony Zhou", "Alexander Wikner", "Amaury Lancelin", "Pedram Hassanzadeh", "Amir Barati Farimani"], "title": "Reframing Generative Models for Physical Systems using Stochastic Interpolants", "comment": "Code and data is available at\n  http://github.com/anthonyzhou-1/interpolant_pdes", "summary": "Generative models have recently emerged as powerful surrogates for physical\nsystems, demonstrating increased accuracy, stability, and/or statistical\nfidelity. Most approaches rely on iteratively denoising a Gaussian, a choice\nthat may not be the most effective for autoregressive prediction tasks in PDEs\nand dynamical systems such as climate. In this work, we benchmark generative\nmodels across diverse physical domains and tasks, and highlight the role of\nstochastic interpolants. By directly learning a stochastic process between\ncurrent and future states, stochastic interpolants can leverage the proximity\nof successive physical distributions. This allows for generative models that\ncan use fewer sampling steps and produce more accurate predictions than models\nrelying on transporting Gaussian noise. Our experiments suggest that generative\nmodels need to balance deterministic accuracy, spectral consistency, and\nprobabilistic calibration, and that stochastic interpolants can potentially\nfulfill these requirements by adjusting their sampling. This study establishes\nstochastic interpolants as a competitive baseline for physical emulation and\ngives insight into the abilities of different generative modeling frameworks.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e0d\u540c\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86\u968f\u673a\u63d2\u503c\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u9ad8\u65af\u53bb\u566a\u7684\u65b9\u6cd5\u5728\u81ea\u56de\u5f52\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8fed\u4ee3\u53bb\u566a\u9ad8\u65af\u566a\u58f0\uff0c\u4f46\u5728PDE\u548c\u52a8\u529b\u7cfb\u7edf\uff08\u5982\u6c14\u5019\uff09\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u4efb\u52a1\u4e2d\u53ef\u80fd\u4e0d\u662f\u6700\u6709\u6548\u7684\u9009\u62e9\u3002\u9700\u8981\u63a2\u7d22\u66f4\u9002\u5408\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u7684\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u968f\u673a\u63d2\u503c\u65b9\u6cd5\uff0c\u76f4\u63a5\u5b66\u4e60\u5f53\u524d\u72b6\u6001\u548c\u672a\u6765\u72b6\u6001\u4e4b\u95f4\u7684\u968f\u673a\u8fc7\u7a0b\uff0c\u5229\u7528\u8fde\u7eed\u7269\u7406\u5206\u5e03\u4e4b\u95f4\u7684\u90bb\u8fd1\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u91c7\u6837\u6b65\u9aa4\u751f\u6210\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u968f\u673a\u63d2\u503c\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u4f20\u8f93\u9ad8\u65af\u566a\u58f0\u7684\u6a21\u578b\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u91c7\u6837\u6b65\u9aa4\u3002\u751f\u6210\u6a21\u578b\u9700\u8981\u5728\u786e\u5b9a\u6027\u7cbe\u5ea6\u3001\u9891\u8c31\u4e00\u81f4\u6027\u548c\u6982\u7387\u6821\u51c6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u968f\u673a\u63d2\u503c\u65b9\u6cd5\u4e3a\u7269\u7406\u4eff\u771f\u63d0\u4f9b\u4e86\u6709\u7ade\u4e89\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u8c03\u6574\u91c7\u6837\u7b56\u7565\u53ef\u4ee5\u6ee1\u8db3\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u7684\u5404\u9879\u8981\u6c42\uff0c\u4e3a\u4e0d\u540c\u751f\u6210\u5efa\u6a21\u6846\u67b6\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2509.26294", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26294", "abs": "https://arxiv.org/abs/2509.26294", "authors": ["Lionel Blond\u00e9", "Joao A. Candido Ramos", "Alexandros Kalousis"], "title": "Noise-Guided Transport for Imitation Learning", "comment": null, "summary": "We consider imitation learning in the low-data regime, where only a limited\nnumber of expert demonstrations are available. In this setting, methods that\nrely on large-scale pretraining or high-capacity architectures can be difficult\nto apply, and efficiency with respect to demonstration data becomes critical.\nWe introduce Noise-Guided Transport (NGT), a lightweight off-policy method that\ncasts imitation as an optimal transport problem solved via adversarial\ntraining. NGT requires no pretraining or specialized architectures,\nincorporates uncertainty estimation by design, and is easy to implement and\ntune. Despite its simplicity, NGT achieves strong performance on challenging\ncontinuous control tasks, including high-dimensional Humanoid tasks, under\nultra-low data regimes with as few as 20 transitions. Code is publicly\navailable at: https://github.com/lionelblonde/ngt-pytorch.", "AI": {"tldr": "NGT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u6a21\u4eff\u5b66\u4e60\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u89e3\u51b3\uff0c\u5728\u6781\u4f4e\u6570\u636e\u91cf\uff08\u4ec520\u4e2a\u8f6c\u6362\uff09\u4e0b\u4ecd\u80fd\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u4f4e\u6570\u636e\u91cf\u6a21\u4eff\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6216\u9ad8\u5bb9\u91cf\u67b6\u6784\u7684\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u5bf9\u6f14\u793a\u6570\u636e\u9ad8\u6548\u5229\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u6a21\u4eff\u5b66\u4e60\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u6c42\u89e3\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u7279\u6b8a\u67b6\u6784\uff0c\u5185\u7f6e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\uff08\u5305\u62ec\u9ad8\u7ef4Humanoid\u4efb\u52a1\uff09\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5728\u6781\u4f4e\u6570\u636e\u91cf\uff08\u4ec520\u4e2a\u8f6c\u6362\uff09\u4e0b\u4ecd\u80fd\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "NGT\u662f\u4e00\u79cd\u7b80\u5355\u6613\u5b9e\u73b0\u4e14\u8c03\u8282\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u6a21\u4eff\u5b66\u4e60\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.26301", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.26301", "abs": "https://arxiv.org/abs/2509.26301", "authors": ["Suli Wang", "Yangshen Deng", "Zhenghua Bao", "Xinyu Zhan", "Yiqun Duan"], "title": "NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training", "comment": null, "summary": "Large-scale foundation models for EEG signals offer a promising path to\ngeneralizable brain-computer interface (BCI) applications, but they often\nsuffer from misalignment between pretraining objectives and downstream tasks,\nas well as significant cross-subject distribution shifts. This paper addresses\nthese challenges by introducing a two-stage alignment strategy that bridges the\ngap between generic pretraining and specific EEG decoding tasks. First, we\npropose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that\naugments the foundation model with task-relevant self-supervised objectives,\naligning latent representations to important spectral, spatial, and temporal\nEEG features without requiring additional labeled data. Second, we incorporate\ntest-time training (TTT) at inference, we perform (i) self-supervised test-time\ntraining on individual unlabeled test samples and (ii) prediction entropy\nminimization (Tent), which updates only normalization statistics to continually\ncalibrate the model to each new input on the fly. Our approach, which, to our\nknowledge, is the first to unify domain-tuned self-supervision with test-time\ntraining in large-scale EEG foundation models, yields substantially improved\nrobustness and accuracy across diverse BCI tasks (imagined speech, stress\ndetection, motor imagery). Using CBraMod and LaBraM as backbones, our method\npushes their performance to a markedly higher level. Results on three diverse\ntasks demonstrate that the proposed alignment strategy achieves\nstate-of-the-art performance, outperforming conventional fine-tuning and\nadaptation methods. Our code is available at\nhttps://github.com/wsl2000/NeuroTTT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7NeuroTTT\u81ea\u76d1\u7763\u5fae\u8c03\u548c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6765\u89e3\u51b3EEG\u57fa\u7840\u6a21\u578b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8111\u673a\u63a5\u53e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21EEG\u57fa\u7840\u6a21\u578b\u5728\u8111\u673a\u63a5\u53e3\u5e94\u7528\u4e2d\u9762\u4e34\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5bf9\u9f50\u4ee5\u53ca\u8de8\u88ab\u8bd5\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u9f50\u7b56\u7565\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5bf9\u9f50\u7b56\u7565\uff1a1) NeuroTTT\u81ea\u76d1\u7763\u5fae\u8c03\uff0c\u901a\u8fc7\u4efb\u52a1\u76f8\u5173\u7684\u81ea\u76d1\u7763\u76ee\u6807\u589e\u5f3a\u57fa\u7840\u6a21\u578b\uff1b2) \u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u5305\u62ec\u81ea\u76d1\u7763\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u548c\u9884\u6d4b\u71b5\u6700\u5c0f\u5316\uff0c\u5b9e\u65f6\u6821\u51c6\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u8111\u673a\u63a5\u53e3\u4efb\u52a1\uff08\u60f3\u8c61\u8bed\u97f3\u3001\u538b\u529b\u68c0\u6d4b\u3001\u8fd0\u52a8\u60f3\u8c61\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u5fae\u8c03\u548c\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u9886\u57df\u8c03\u4f18\u7684\u81ea\u76d1\u7763\u4e0e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u89c4\u6a21EEG\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u901a\u7528\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2509.26307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26307", "abs": "https://arxiv.org/abs/2509.26307", "authors": ["Piotr Komorowski", "Elena Golimblevskaia", "Reduan Achtibat", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "title": "Attribution-Guided Decoding", "comment": null, "summary": "The capacity of Large Language Models (LLMs) to follow complex instructions\nand generate factually accurate text is critical for their real-world\napplication. However, standard decoding methods often fail to robustly satisfy\nthese requirements, while existing control techniques frequently degrade\ngeneral output quality. In this work, we introduce Attribution-Guided Decoding\n(AGD), an interpretability-based decoding strategy. Instead of directly\nmanipulating model activations, AGD considers a set of high-probability output\ntoken candidates and selects the one that exhibits the highest attribution to a\nuser-defined Region of Interest (ROI). This ROI can be flexibly defined over\ndifferent parts of the model's input or internal components, allowing AGD to\nsteer generation towards various desirable behaviors. We demonstrate AGD's\nefficacy across three challenging domains. For instruction following, we show\nthat AGD significantly boosts adherence (e.g., improving the overall success\nrate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show\nthat guiding generation towards usage of internal knowledge components or\ncontextual sources can reduce hallucinations and improve factual accuracy in\nboth closed-book and open-book settings. Furthermore, we propose an adaptive,\nentropy-based variant of AGD that mitigates quality degradation and reduces\ncomputational overhead by applying guidance only when the model is uncertain.\nOur work presents a versatile, more interpretable, and effective method for\nenhancing the reliability of modern LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684Attribution-Guided Decoding(AGD)\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u9009\u62e9\u5bf9\u7528\u6237\u5b9a\u4e49\u611f\u5174\u8da3\u533a\u57df(ROI)\u5177\u6709\u6700\u9ad8\u5f52\u56e0\u7684token\u5019\u9009\uff0c\u6765\u589e\u5f3aLLMs\u7684\u6307\u4ee4\u9075\u5faa\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u6807\u51c6\u89e3\u7801\u65b9\u6cd5\u5728\u6ee1\u8db3\u590d\u6742\u6307\u4ee4\u9075\u5faa\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u63a7\u5236\u6280\u672f\u5f80\u5f80\u4f1a\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u53c8\u80fd\u589e\u5f3a\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "method": "AGD\u8003\u8651\u4e00\u7ec4\u9ad8\u6982\u7387\u8f93\u51fatoken\u5019\u9009\uff0c\u9009\u62e9\u5bf9\u7528\u6237\u5b9a\u4e49ROI\u5177\u6709\u6700\u9ad8\u5f52\u56e0\u7684token\u3002ROI\u53ef\u4ee5\u7075\u6d3b\u5b9a\u4e49\u5728\u6a21\u578b\u8f93\u5165\u6216\u5185\u90e8\u7ec4\u4ef6\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u652f\u6301\u81ea\u9002\u5e94\u71b5\u57fa\u53d8\u4f53\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u663e\u8457\u63d0\u5347\u6210\u529f\u7387(Llama 3.1\u4ece66.0%\u63d0\u5347\u81f379.1%)\uff1b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff1b\u81ea\u9002\u5e94\u53d8\u4f53\u80fd\u7f13\u89e3\u8d28\u91cf\u4e0b\u964d\u3002", "conclusion": "AGD\u662f\u4e00\u79cd\u591a\u529f\u80fd\u3001\u66f4\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u589e\u5f3a\u73b0\u4ee3LLMs\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.26321", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26321", "abs": "https://arxiv.org/abs/2509.26321", "authors": ["Judith Echevarrieta", "Etor Arza", "Aritz P\u00e9rez", "Josu Ceberio"], "title": "A Review on Single-Problem Multi-Attempt Heuristic Optimization", "comment": null, "summary": "In certain real-world optimization scenarios, practitioners are not\ninterested in solving multiple problems but rather in finding the best solution\nto a single, specific problem. When the computational budget is large relative\nto the cost of evaluating a candidate solution, multiple heuristic alternatives\ncan be tried to solve the same given problem, each possibly with a different\nalgorithm, parameter configuration, initialization, or stopping criterion. The\nsequential selection of which alternative to try next is crucial for\nefficiently identifying the one that provides the best possible solution across\nmultiple attempts. Despite the relevance of this problem in practice, it has\nnot yet been the exclusive focus of any existing review. Several sequential\nalternative selection strategies have been proposed in different research\ntopics, but they have not been comprehensively and systematically unified under\na common perspective.\n  This work presents a focused review of single-problem multi-attempt heuristic\noptimization. It brings together suitable strategies to this problem that have\nbeen studied separately through algorithm selection, parameter tuning,\nmulti-start and resource allocation. These strategies are explained using a\nunified terminology within a common framework, which supports the development\nof a taxonomy for systematically organizing and classifying them.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5355\u95ee\u9898\u591a\u5c1d\u8bd5\u542f\u53d1\u5f0f\u4f18\u5316\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5c06\u7b97\u6cd5\u9009\u62e9\u3001\u53c2\u6570\u8c03\u4f18\u3001\u591a\u542f\u52a8\u548c\u8d44\u6e90\u5206\u914d\u7b49\u7b56\u7565\u7edf\u4e00\u5230\u4e00\u4e2a\u5171\u540c\u6846\u67b6\u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u6765\u7ec4\u7ec7\u8fd9\u4e9b\u7b56\u7565\u3002", "motivation": "\u5728\u5b9e\u9645\u4f18\u5316\u573a\u666f\u4e2d\uff0c\u5f53\u8ba1\u7b97\u9884\u7b97\u5145\u8db3\u65f6\uff0c\u5b9e\u8df5\u8005\u9700\u8981\u4e3a\u5355\u4e2a\u7279\u5b9a\u95ee\u9898\u5c1d\u8bd5\u591a\u79cd\u542f\u53d1\u5f0f\u66ff\u4ee3\u65b9\u6848\uff08\u4e0d\u540c\u7b97\u6cd5\u3001\u53c2\u6570\u914d\u7f6e\u7b49\uff09\uff0c\u4f46\u73b0\u6709\u7684\u987a\u5e8f\u9009\u62e9\u7b56\u7565\u5206\u6563\u5728\u4e0d\u540c\u7814\u7a76\u9886\u57df\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89c6\u89d2\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u672f\u8bed\u548c\u5171\u540c\u6846\u67b6\uff0c\u5c06\u7b97\u6cd5\u9009\u62e9\u3001\u53c2\u6570\u8c03\u4f18\u3001\u591a\u542f\u52a8\u548c\u8d44\u6e90\u5206\u914d\u7b49\u7b56\u7565\u6574\u5408\u8d77\u6765\uff0c\u5e76\u5f00\u53d1\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u7ec4\u7ec7\u548c\u5206\u7c7b\u8fd9\u4e9b\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u548c\u5206\u7c7b\u6cd5\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u7ec4\u7ec7\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u987a\u5e8f\u66ff\u4ee3\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86\u5355\u95ee\u9898\u591a\u5c1d\u8bd5\u542f\u53d1\u5f0f\u4f18\u5316\u9886\u57df\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u7b56\u7565\u9009\u62e9\u548c\u5206\u7c7b\u6846\u67b6\u3002"}}
{"id": "2509.26322", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26322", "abs": "https://arxiv.org/abs/2509.26322", "authors": ["Margarita A. Guerrero", "Cristian R. Rojas"], "title": "ACE: Adapting sampling for Counterfactual Explanations", "comment": "15 pages", "summary": "Counterfactual Explanations (CFEs) interpret machine learning models by\nidentifying the smallest change to input features needed to change the model's\nprediction to a desired output. For classification tasks, CFEs determine how\nclose a given sample is to the decision boundary of a trained classifier.\nExisting methods are often sample-inefficient, requiring numerous evaluations\nof a black-box model -- an approach that is both costly and impractical when\naccess to the model is limited. We propose Adaptive sampling for Counterfactual\nExplanations (ACE), a sample-efficient algorithm combining Bayesian estimation\nand stochastic optimization to approximate the decision boundary with fewer\nqueries. By prioritizing informative points, ACE minimizes evaluations while\ngenerating accurate and feasible CFEs. Extensive empirical results show that\nACE achieves superior evaluation efficiency compared to state-of-the-art\nmethods, while maintaining effectiveness in identifying minimal and actionable\nchanges.", "AI": {"tldr": "\u63d0\u51fa\u4e86ACE\u7b97\u6cd5\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f30\u8ba1\u548c\u968f\u673a\u4f18\u5316\uff0c\u7528\u66f4\u5c11\u7684\u67e5\u8be2\u6b21\u6570\u9ad8\u6548\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bc4\u4f30\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u8bbf\u95ee\u53d7\u9650\u65f6", "method": "ACE\u7b97\u6cd5\u7ed3\u5408\u8d1d\u53f6\u65af\u4f30\u8ba1\u548c\u968f\u673a\u4f18\u5316\uff0c\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u70b9\u6765\u8fd1\u4f3c\u51b3\u7b56\u8fb9\u754c", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cACE\u5728\u8bc4\u4f30\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u6709\u6548\u8bc6\u522b\u6700\u5c0f\u4e14\u53ef\u884c\u7684\u6539\u53d8", "conclusion": "ACE\u662f\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u80fd\u5728\u4fdd\u6301\u89e3\u91ca\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u67e5\u8be2\u6b21\u6570"}}
{"id": "2509.26327", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.26327", "abs": "https://arxiv.org/abs/2509.26327", "authors": ["Charles Westphal", "Stephen Hailes", "Mirco Musolesi"], "title": "A Generalized Information Bottleneck Theory of Deep Learning", "comment": null, "summary": "The Information Bottleneck (IB) principle offers a compelling theoretical\nframework to understand how neural networks (NNs) learn. However, its practical\nutility has been constrained by unresolved theoretical ambiguities and\nsignificant challenges in accurate estimation. In this paper, we present a\n\\textit{Generalized Information Bottleneck (GIB)} framework that reformulates\nthe original IB principle through the lens of synergy, i.e., the information\nobtainable only through joint processing of features. We provide theoretical\nand empirical evidence demonstrating that synergistic functions achieve\nsuperior generalization compared to their non-synergistic counterparts.\nBuilding on these foundations we re-formulate the IB using a computable\ndefinition of synergy based on the average interaction information (II) of each\nfeature with those remaining. We demonstrate that the original IB objective is\nupper bounded by our GIB in the case of perfect estimation, ensuring\ncompatibility with existing IB theory while addressing its limitations. Our\nexperimental results demonstrate that GIB consistently exhibits compression\nphases across a wide range of architectures (including those with \\textit{ReLU}\nactivations where the standard IB fails), while yielding interpretable dynamics\nin both CNNs and Transformers and aligning more closely with our understanding\nof adversarial robustness.", "AI": {"tldr": "\u63d0\u51fa\u5e7f\u4e49\u4fe1\u606f\u74f6\u9888(GIB)\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u4fe1\u606f\u89c6\u89d2\u91cd\u65b0\u8868\u8ff0\u539f\u59cbIB\u539f\u5219\uff0c\u89e3\u51b3\u4e86IB\u7406\u8bba\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4f30\u8ba1\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u539f\u59cb\u4fe1\u606f\u74f6\u9888(IB)\u539f\u5219\u5728\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u65b9\u9762\u5177\u6709\u7406\u8bba\u4ef7\u503c\uff0c\u4f46\u5b58\u5728\u7406\u8bba\u6a21\u7cca\u6027\u548c\u4f30\u8ba1\u56f0\u96be\u7b49\u5b9e\u9645\u5e94\u7528\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u534f\u540c\u4fe1\u606f\uff08\u7279\u5f81\u8054\u5408\u5904\u7406\u83b7\u5f97\u7684\u4fe1\u606f\uff09\u91cd\u65b0\u8868\u8ff0IB\uff0c\u4f7f\u7528\u5e73\u5747\u4ea4\u4e92\u4fe1\u606f(II)\u4f5c\u4e3a\u53ef\u8ba1\u7b97\u7684\u534f\u540c\u5ea6\u91cf\uff0c\u6784\u5efaGIB\u6846\u67b6\u3002", "result": "GIB\u5728\u5404\u79cd\u67b6\u6784\u4e2d\uff08\u5305\u62ecReLU\u6fc0\u6d3b\u7f51\u7edc\uff09\u90fd\u8868\u73b0\u51fa\u538b\u7f29\u9636\u6bb5\uff0c\u5728CNN\u548cTransformer\u4e2d\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u52a8\u6001\uff0c\u5e76\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u7406\u89e3\u66f4\u4e00\u81f4\u3002", "conclusion": "GIB\u6846\u67b6\u5728\u4fdd\u6301\u4e0e\u73b0\u6709IB\u7406\u8bba\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u89e3\u51b3\u4e86\u539f\u59cbIB\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u4fe1\u606f\u74f6\u9888\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2509.26337", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26337", "abs": "https://arxiv.org/abs/2509.26337", "authors": ["Yuki Takezawa", "Anastasia Koloskova", "Xiaowen Jiang", "Sebastian U. Stich"], "title": "FedMuon: Federated Learning with Bias-corrected LMO-based Optimization", "comment": null, "summary": "Recently, a new optimization method based on the linear minimization oracle\n(LMO), called Muon, has been attracting increasing attention since it can train\nneural networks faster than existing adaptive optimization methods, such as\nAdam. In this paper, we study how Muon can be utilized in federated learning.\nWe first show that straightforwardly using Muon as the local optimizer of\nFedAvg does not converge to the stationary point since the LMO is a biased\noperator. We then propose FedMuon which can mitigate this issue. We also\nanalyze how solving the LMO approximately affects the convergence rate and find\nthat, surprisingly, FedMuon can converge for any number of Newton-Schulz\niterations, while it can converge faster as we solve the LMO more accurately.\nThrough experiments, we demonstrated that FedMuon can outperform the\nstate-of-the-art federated learning methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86FedMuon\u65b9\u6cd5\uff0c\u5c06Muon\u4f18\u5316\u5668\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u4f7f\u7528Muon\u4f5c\u4e3a\u672c\u5730\u4f18\u5316\u5668\u4e0d\u6536\u655b\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u6bd4Adam\u7b49\u81ea\u9002\u5e94\u65b9\u6cd5\u66f4\u5feb\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u65f6\u7531\u4e8e\u7ebf\u6027\u6700\u5c0f\u5316\u7b97\u5b50(LMO)\u7684\u504f\u5dee\u6027\u5bfc\u81f4\u4e0d\u6536\u655b\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFedMuon\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f13\u89e3LMO\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5206\u6790\u8fd1\u4f3c\u6c42\u89e3LMO\u5bf9\u6536\u655b\u901f\u5ea6\u7684\u5f71\u54cd\uff0c\u652f\u6301\u4efb\u610f\u6b21\u6570\u7684Newton-Schulz\u8fed\u4ee3\u3002", "result": "FedMuon\u80fd\u591f\u6536\u655b\uff0c\u4e14LMO\u6c42\u89e3\u8d8a\u51c6\u786e\u6536\u655b\u8d8a\u5feb\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FedMuon\u6210\u529f\u5c06Muon\u4f18\u5316\u5668\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u6536\u655b\u6027\u95ee\u9898\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2509.26340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26340", "abs": "https://arxiv.org/abs/2509.26340", "authors": ["Xue Yan", "Zijing Ou", "Mengyue Yang", "Yan Song", "Haifeng Zhang", "Yingzhen Li", "Jun Wang"], "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models", "comment": null, "summary": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bb0\u5fc6\u9a71\u52a8\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u5c06LLM\u7684\u901a\u7528\u5148\u9a8c\u77e5\u8bc6\u4e0e\u7279\u5b9a\u9886\u57df\u7684\u7d27\u51d1\u8bb0\u5fc6\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u8bb0\u5fc6\u548cLLM\u5148\u9a8c\u7684\u76f8\u4e92\u589e\u5f3a\u6765\u63d0\u5347\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "LLM\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u4f46\u901a\u7528\u7684\u77e5\u8bc6\uff0c\u5bf9\u4e8e\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u96be\u4ee5\u6709\u6548\u9002\u5e94\uff0c\u9700\u8981\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7ecf\u9a8c\u6765\u63d0\u5347\u51b3\u7b56\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u8bb0\u5fc6\u5b58\u50a8\u8fc7\u53bb\u4ea4\u4e92\u548c\u5173\u8054\u7684Q\u503c\uff0c\u6355\u83b7\u51b3\u7b56\u76f8\u5173\u77e5\u8bc6\u6765\u4fc3\u8fdb\u51c6\u786e\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u5e76\u6307\u5bfcLLM\u5148\u9a8c\u7684\u4f18\u5316\u3002\u4f18\u5316\u540e\u7684LLM\u5148\u9a8c\u751f\u6210\u66f4\u9ad8\u5956\u52b1\u7684\u8f68\u8ff9\u6765\u4e30\u5bcc\u8bb0\u5fc6\uff0c\u5f62\u6210\u81ea\u6211\u6539\u8fdb\u5faa\u73af\u3002", "result": "\u5728ALFWorld\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRL\u548c\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5206\u5e03\u5185\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u8d85\u8fc740%\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u8d85\u8fc775%\u3002", "conclusion": "\u8bb0\u5fc6\u9a71\u52a8\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u901a\u7528\u77e5\u8bc6\u548c\u9886\u57df\u7279\u5b9a\u7ecf\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.26351", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26351", "abs": "https://arxiv.org/abs/2509.26351", "authors": ["Joshua Sebastian", "Karma Tobden", "KMA Solaiman"], "title": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation", "comment": "Submitted to GenAI4Health@NeurIPS 2025. This is the first version of\n  the LLM-assisted emergency triage benchmark dataset and baseline models.\n  Authors: Joshua Sebastian, Karma Tobden, KMA Solaiman", "summary": "Research on emergency and mass casualty incident (MCI) triage has been\nlimited by the absence of openly usable, reproducible benchmarks. Yet these\nscenarios demand rapid identification of the patients most in need, where\naccurate deterioration prediction can guide timely interventions. While the\nMIMIC-IV-ED database is openly available to credentialed researchers,\ntransforming it into a triage-focused benchmark requires extensive\npreprocessing, feature harmonization, and schema alignment -- barriers that\nrestrict accessibility to only highly technical users.\n  We address these gaps by first introducing an open, LLM-assisted emergency\ntriage benchmark for deterioration prediction (ICU transfer, in-hospital\nmortality). The benchmark then defines two regimes: (i) a hospital-rich setting\nwith vitals, labs, notes, chief complaints, and structured observations, and\n(ii) an MCI-like field simulation limited to vitals, observations, and notes.\nLarge language models (LLMs) contributed directly to dataset construction by\n(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)\nprioritizing clinically relevant vitals and labs, and (iii) guiding schema\nalignment and efficient merging of disparate tables.\n  We further provide baseline models and SHAP-based interpretability analyses,\nillustrating predictive gaps between regimes and the features most critical for\ntriage. Together, these contributions make triage prediction research more\nreproducible and accessible -- a step toward dataset democratization in\nclinical AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u53ef\u590d\u73b0\u7684\u6025\u8bca\u5206\u8bca\u57fa\u51c6\uff0c\u7528\u4e8e\u9884\u6d4b\u60a3\u8005\u6076\u5316\u60c5\u51b5\uff08ICU\u8f6c\u79fb\u3001\u9662\u5185\u6b7b\u4ea1\u7387\uff09\uff0c\u5305\u542b\u533b\u9662\u4e30\u5bcc\u73af\u5883\u548cMCI\u6a21\u62df\u73af\u5883\u4e24\u79cd\u573a\u666f\uff0c\u5e76\u5229\u7528LLM\u8f85\u52a9\u6570\u636e\u96c6\u6784\u5efa\u3002", "motivation": "\u89e3\u51b3\u6025\u8bca\u548c\u7fa4\u4f53\u4f24\u4ea1\u4e8b\u4ef6\u5206\u8bca\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5f00\u653e\u53ef\u7528\u3001\u53ef\u590d\u73b0\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u573a\u666f\u9700\u8981\u5feb\u901f\u8bc6\u522b\u6700\u9700\u8981\u5e2e\u52a9\u7684\u60a3\u8005\uff0c\u51c6\u786e\u9884\u6d4b\u6076\u5316\u60c5\u51b5\u53ef\u4ee5\u6307\u5bfc\u53ca\u65f6\u5e72\u9884\u3002", "method": "\u5f15\u5165LLM\u8f85\u52a9\u7684\u6025\u8bca\u5206\u8bca\u57fa\u51c6\uff0c\u5b9a\u4e49\u4e24\u79cd\u573a\u666f\uff1a\u533b\u9662\u4e30\u5bcc\u73af\u5883\uff08\u542b\u751f\u547d\u4f53\u5f81\u3001\u5b9e\u9a8c\u5ba4\u6570\u636e\u3001\u75c5\u5386\u7b49\uff09\u548cMCI\u6a21\u62df\u73af\u5883\uff08\u4ec5\u542b\u751f\u547d\u4f53\u5f81\u3001\u89c2\u5bdf\u548c\u75c5\u5386\uff09\u3002\u5229\u7528LLM\u8fdb\u884c\u6570\u636e\u5b57\u6bb5\u534f\u8c03\u3001\u7279\u5f81\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u6a21\u5f0f\u5bf9\u9f50\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u51c6\u6a21\u578b\u548c\u57fa\u4e8eSHAP\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u573a\u666f\u95f4\u7684\u9884\u6d4b\u5dee\u8ddd\u548c\u5206\u8bca\u4e2d\u6700\u5173\u952e\u7684\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u4f7f\u5206\u8bca\u9884\u6d4b\u7814\u7a76\u66f4\u5177\u53ef\u590d\u73b0\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u662f\u4e34\u5e8aAI\u4e2d\u6570\u636e\u96c6\u6c11\u4e3b\u5316\u7684\u4e00\u6b65\u3002"}}
{"id": "2509.26364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26364", "abs": "https://arxiv.org/abs/2509.26364", "authors": ["Kirill Tamogashev", "Nikolay Malkin"], "title": "Data-to-Energy Stochastic Dynamics", "comment": null, "summary": "The Schr\\\"odinger bridge problem is concerned with finding a stochastic\ndynamical system bridging two marginal distributions that minimises a certain\ntransportation cost. This problem, which represents a generalisation of optimal\ntransport to the stochastic case, has received attention due to its connections\nto diffusion models and flow matching, as well as its applications in the\nnatural sciences. However, all existing algorithms allow to infer such dynamics\nonly for cases where samples from both distributions are available. In this\npaper, we propose the first general method for modelling Schr\\\"odinger bridges\nwhen one (or both) distributions are given by their unnormalised densities,\nwith no access to data samples. Our algorithm relies on a generalisation of the\niterative proportional fitting (IPF) procedure to the data-free case, inspired\nby recent developments in off-policy reinforcement learning for training of\ndiffusion samplers. We demonstrate the efficacy of the proposed data-to-energy\nIPF on synthetic problems, finding that it can successfully learn transports\nbetween multimodal distributions. As a secondary consequence of our\nreinforcement learning formulation, which assumes a fixed time discretisation\nscheme for the dynamics, we find that existing data-to-data Schr\\\"odinger\nbridge algorithms can be substantially improved by learning the diffusion\ncoefficient of the dynamics. Finally, we apply the newly developed algorithm to\nthe problem of sampling posterior distributions in latent spaces of generative\nmodels, thus creating a data-free image-to-image translation method. Code:\nhttps://github.com/mmacosha/d2e-stochastic-dynamics", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u901a\u7528\u65b9\u6cd5\u7528\u4e8e\u5efa\u6a21\u859b\u5b9a\u8c14\u6865\uff0c\u5f53\u5176\u4e2d\u4e00\u4e2a\u6216\u4e24\u4e2a\u5206\u5e03\u4ec5\u901a\u8fc7\u672a\u5f52\u4e00\u5316\u5bc6\u5ea6\u7ed9\u51fa\u4e14\u65e0\u6cd5\u83b7\u53d6\u6570\u636e\u6837\u672c\u65f6\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u8fed\u4ee3\u6bd4\u4f8b\u62df\u5408(IPF)\u8fc7\u7a0b\u7684\u6cdb\u5316\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u751f\u6210\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91c7\u6837\u540e\u9a8c\u5206\u5e03\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u859b\u5b9a\u8c14\u6865\u7b97\u6cd5\u90fd\u9700\u8981\u4e24\u4e2a\u5206\u5e03\u7684\u6837\u672c\u6570\u636e\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5206\u5e03\u53ef\u80fd\u4ec5\u901a\u8fc7\u672a\u5f52\u4e00\u5316\u5bc6\u5ea6\u51fd\u6570\u7ed9\u51fa\uff0c\u65e0\u6cd5\u83b7\u53d6\u6837\u672c\u3002\u8fd9\u9650\u5236\u4e86\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u6570\u636e\u5230\u80fd\u91cfIPF\u65b9\u6cd5\uff0c\u5c06\u8fed\u4ee3\u6bd4\u4f8b\u62df\u5408\u8fc7\u7a0b\u6cdb\u5316\u5230\u65e0\u6570\u636e\u60c5\u51b5\uff0c\u501f\u9274\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u6269\u6563\u91c7\u6837\u5668\u8bad\u7ec3\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u8be5\u65b9\u6cd5\u5047\u8bbe\u52a8\u6001\u7cfb\u7edf\u5177\u6709\u56fa\u5b9a\u7684\u65f6\u95f4\u79bb\u6563\u5316\u65b9\u6848\u3002", "result": "\u5728\u5408\u6210\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6210\u529f\u5b66\u4e60\u591a\u5cf0\u5206\u5e03\u4e4b\u95f4\u7684\u4f20\u8f93\u3002\u540c\u65f6\u53d1\u73b0\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\u7cfb\u7edf\u7684\u6269\u6563\u7cfb\u6570\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u73b0\u6709\u7684\u6570\u636e\u5230\u6570\u636e\u859b\u5b9a\u8c14\u6865\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u63d0\u4f9b\u4e86\u9996\u4e2a\u65e0\u6570\u636e\u6837\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u5176\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u540e\u9a8c\u91c7\u6837\u65b9\u9762\uff0c\u521b\u5efa\u4e86\u65e0\u6570\u636e\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\u3002"}}
{"id": "2509.26405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26405", "abs": "https://arxiv.org/abs/2509.26405", "authors": ["Benno Kaech", "Luis Wyss", "Karsten Borgwardt", "Gianvito Grasso"], "title": "Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery", "comment": null, "summary": "We introduce InVirtuoGen, a discrete flow generative model for fragmented\nSMILES for de novo and fragment-constrained generation, and\ntarget-property/lead optimization of small molecules. The model learns to\ntransform a uniform source over all possible tokens into the data distribution.\nUnlike masked models, its training loss accounts for predictions on all\nsequence positions at every denoising step, shifting the generation paradigm\nfrom completion to refinement, and decoupling the number of sampling steps from\nthe sequence length. For \\textit{de novo} generation, InVirtuoGen achieves a\nstronger quality-diversity pareto frontier than prior fragment-based models and\ncompetitive performance on fragment-constrained tasks. For property and lead\noptimization, we propose a hybrid scheme that combines a genetic algorithm with\na Proximal Property Optimization fine-tuning strategy adapted to discrete\nflows. Our approach sets a new state-of-the-art on the Practical Molecular\nOptimization benchmark, measured by top-10 AUC across tasks, and yields higher\ndocking scores in lead optimization than previous baselines. InVirtuoGen thus\nestablishes a versatile generative foundation for drug discovery, from early\nhit finding to multi-objective lead optimization. We further contribute to open\nscience by releasing pretrained checkpoints and code, making our results fully\nreproducible\\footnote{https://github.com/invirtuolabs/InVirtuoGen_results}.", "AI": {"tldr": "InVirtuoGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u79bb\u6563\u6d41\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5c0f\u5206\u5b50\u7684\u4ece\u5934\u751f\u6210\u3001\u7247\u6bb5\u7ea6\u675f\u751f\u6210\u3001\u76ee\u6807\u6027\u8d28\u4f18\u5316\u548c\u5148\u5bfc\u5316\u5408\u7269\u4f18\u5316\uff0c\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u5efa\u7acb\u4e86\u591a\u529f\u80fd\u7684\u751f\u6210\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5b50\u751f\u6210\u6a21\u578b\u5728\u8d28\u91cf-\u591a\u6837\u6027\u6743\u8861\u3001\u7247\u6bb5\u7ea6\u675f\u4efb\u52a1\u4ee5\u53ca\u591a\u76ee\u6807\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u751f\u6210\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u79bb\u6563\u6d41\u751f\u6210\u6a21\u578b\u5b66\u4e60\u4ece\u5747\u5300\u6e90\u5206\u5e03\u5230\u6570\u636e\u5206\u5e03\u7684\u8f6c\u6362\uff0c\u8bad\u7ec3\u635f\u5931\u8003\u8651\u6240\u6709\u5e8f\u5217\u4f4d\u7f6e\u7684\u9884\u6d4b\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548c\u8fd1\u7aef\u6027\u8d28\u4f18\u5316\u5fae\u8c03\u7b56\u7565\u8fdb\u884c\u6027\u8d28\u4f18\u5316\u3002", "result": "\u5728\u4ece\u5934\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8d28\u91cf-\u591a\u6837\u6027\u6743\u8861\uff0c\u5728Practical Molecular Optimization\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u5148\u5bfc\u5316\u5408\u7269\u4f18\u5316\u4e2d\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5bf9\u63a5\u5206\u6570\u3002", "conclusion": "InVirtuoGen\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ece\u65e9\u671f\u547d\u4e2d\u53d1\u73b0\u5230\u591a\u76ee\u6807\u5148\u5bfc\u5316\u5408\u7269\u4f18\u5316\u7684\u591a\u529f\u80fd\u751f\u6210\u57fa\u7840\uff0c\u5e76\u5f00\u6e90\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2509.26427", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26427", "abs": "https://arxiv.org/abs/2509.26427", "authors": ["Ioannis Mavrothalassitis", "Pol Puigdemont", "Noam Itzhak Levi", "Volkan Cevher"], "title": "Ascent Fails to Forget", "comment": "NeurIPS 2025", "summary": "Contrary to common belief, we show that gradient ascent-based unconstrained\noptimization methods frequently fail to perform machine unlearning, a\nphenomenon we attribute to the inherent statistical dependence between the\nforget and retain data sets. This dependence, which can manifest itself even as\nsimple correlations, undermines the misconception that these sets can be\nindependently manipulated during unlearning. We provide empirical and\ntheoretical evidence showing these methods often fail precisely due to this\noverlooked relationship. For random forget sets, this dependence means that\ndegrading forget set metrics (which, for a retrained model, should mirror test\nset metrics) inevitably harms overall test performance. Going beyond random\nsets, we consider logistic regression as an instructive example where a\ncritical failure mode emerges: inter-set dependence causes gradient\ndescent-ascent iterations to progressively diverge from the ideal retrained\nmodel. Strikingly, these methods can converge to solutions that are not only\nfar from the retrained ideal but are potentially even further from it than the\noriginal model itself, rendering the unlearning process actively detrimental. A\ntoy example further illustrates how this dependence can trap models in inferior\nlocal minima, inescapable via finetuning. Our findings highlight that the\npresence of such statistical dependencies, even when manifest only as\ncorrelations, can be sufficient for ascent-based unlearning to fail. Our\ntheoretical insights are corroborated by experiments on complex neural\nnetworks, demonstrating that these methods do not perform as expected in\npractice due to this unaddressed statistical interplay.", "AI": {"tldr": "\u68af\u5ea6\u4e0a\u5347\u7684\u65e0\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\u5728\u673a\u5668\u9057\u5fd8\u4efb\u52a1\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u8fd9\u5f52\u56e0\u4e8e\u9057\u5fd8\u96c6\u548c\u4fdd\u7559\u96c6\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\u3002\u5373\u4f7f\u53ea\u662f\u7b80\u5355\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u79cd\u4f9d\u8d56\u5173\u7cfb\u4e5f\u4f1a\u5bfc\u81f4\u9057\u5fd8\u8fc7\u7a0b\u635f\u5bb3\u6574\u4f53\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u53ef\u80fd\u4f7f\u6a21\u578b\u6bd4\u539f\u59cb\u72b6\u6001\u66f4\u5dee\u3002", "motivation": "\u6311\u6218\u666e\u904d\u8ba4\u4e3a\u68af\u5ea6\u4e0a\u5347\u65b9\u6cd5\u80fd\u6709\u6548\u8fdb\u884c\u673a\u5668\u9057\u5fd8\u7684\u89c2\u70b9\uff0c\u63ed\u793a\u9057\u5fd8\u96c6\u548c\u4fdd\u7559\u96c6\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u4f9d\u8d56\u5173\u7cfb\u4f1a\u7834\u574f\u9057\u5fd8\u8fc7\u7a0b\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u68af\u5ea6\u4e0a\u5347\u65b9\u6cd5\u5728\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u5931\u8d25\u673a\u5236\u3002\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u4f5c\u4e3a\u793a\u4f8b\u5c55\u793a\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u73a9\u5177\u793a\u4f8b\u8bf4\u660e\u6a21\u578b\u5982\u4f55\u9677\u5165\u65e0\u6cd5\u901a\u8fc7\u5fae\u8c03\u9003\u8131\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u68af\u5ea6\u4e0a\u5347\u8fed\u4ee3\u4f1a\u9010\u6e10\u504f\u79bb\u7406\u60f3\u7684\u91cd\u8bad\u7ec3\u6a21\u578b\uff0c\u6536\u655b\u5230\u6bd4\u539f\u59cb\u6a21\u578b\u66f4\u5dee\u7684\u89e3\u3002\u5728\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u7406\u8bba\u89c1\u89e3\uff0c\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4e0d\u5982\u9884\u671f\u3002", "conclusion": "\u9057\u5fd8\u96c6\u548c\u4fdd\u7559\u96c6\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\uff08\u5373\u4f7f\u53ea\u662f\u76f8\u5173\u6027\uff09\u8db3\u4ee5\u5bfc\u81f4\u57fa\u4e8e\u4e0a\u5347\u7684\u9057\u5fd8\u65b9\u6cd5\u5931\u8d25\uff0c\u8fd9\u7a81\u663e\u4e86\u5728\u673a\u5668\u9057\u5fd8\u4e2d\u8003\u8651\u6570\u636e\u95f4\u7edf\u8ba1\u5173\u7cfb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.26432", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26432", "abs": "https://arxiv.org/abs/2509.26432", "authors": ["Guanxi Lu", "Hao", "Chen", "Yuto Karashima", "Zhican Wang", "Daichi Fujiki", "Hongxiang Fan"], "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size", "comment": "Preprint. Under review", "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u534a\u81ea\u56de\u5f52\u89e3\u7801\u4e2d\u56fa\u5b9a\u5757\u5927\u5c0f\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86AdaBlock-dLLM\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u9002\u5e94\u5757\u5927\u5c0f\u8c03\u5ea6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u534a\u81ea\u56de\u5f52\u89e3\u7801\u4f7f\u7528\u56fa\u5b9a\u5757\u5927\u5c0f\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u95ee\u9898\uff1a\u5ef6\u8fdf\u89e3\u7801\u5f00\u9500\uff08\u9ad8\u7f6e\u4fe1\u5ea6\u4ee4\u724c\u88ab\u4e0d\u5fc5\u8981\u5ef6\u8fdf\uff09\u548c\u8fc7\u65e9\u89e3\u7801\u9519\u8bef\uff08\u4f4e\u7f6e\u4fe1\u5ea6\u4ee4\u724c\u8fc7\u65e9\u63d0\u4ea4\u5bfc\u81f4\u9519\u8bef\uff09\u3002", "method": "\u901a\u8fc7\u5206\u6790\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u52a8\u6001\uff0c\u8bc6\u522b\u51fa\u6ce2\u52a8\u5e26\u533a\u57df\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86AdaBlock-dLLM\u8c03\u5ea6\u5668\uff0c\u5728\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u8c03\u6574\u5757\u5927\u5c0f\u4ee5\u5bf9\u9f50\u8bed\u4e49\u8fb9\u754c\u3002", "result": "\u5728\u76f8\u540c\u541e\u5410\u91cf\u9884\u7b97\u4e0b\uff0cAdaBlock-dLLM\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.3%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5373\u63d2\u5373\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u4e49\u611f\u77e5\u81ea\u9002\u5e94\u8c03\u5ea6\u65b9\u6cd5\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5206\u6790\u4e3adLLMs\u7684\u672a\u6765\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.26433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26433", "abs": "https://arxiv.org/abs/2509.26433", "authors": ["Vincent Grari", "Tim Arni", "Thibault Laugel", "Sylvain Lamprier", "James Zou", "Marcin Detyniecki"], "title": "ACT: Agentic Classification Tree", "comment": "18 pages, 6 figures", "summary": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.", "AI": {"tldr": "ACT\u5c06\u51b3\u7b56\u6811\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5206\u88c2\u70b9\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u7ed3\u5408TextGrad\u7684LLM\u53cd\u9988\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8d8a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669AI\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u900f\u660e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u7cfb\u7edf\u3002\u4f20\u7edf\u51b3\u7b56\u6811\u4ec5\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u800cLLM\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u65f6\u7f3a\u4e4f\u53ef\u4fe1\u8d56\u7684\u884c\u4e3a\u4fdd\u8bc1\u3002", "method": "ACT\u5c06\u51b3\u7b56\u6811\u7684\u6bcf\u4e2a\u5206\u88c2\u70b9\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u4e0d\u7eaf\u5ea6\u7684\u8bc4\u4f30\u548cTextGrad\u7684LLM\u53cd\u9988\u6765\u4f18\u5316\u8fd9\u4e9b\u95ee\u9898\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u6811\u3002", "result": "\u5728\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACT\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8def\u5f84\u3002", "conclusion": "ACT\u6210\u529f\u5730\u5c06\u51b3\u7b56\u6811\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6269\u5c55\u5230\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u4e3a\u9ad8\u98ce\u9669AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26442", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26442", "abs": "https://arxiv.org/abs/2509.26442", "authors": ["Xinyu Liu", "Zixuan Xie", "Shangtong Zhang"], "title": "Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning", "comment": null, "summary": "The Robbins-Siegmund theorem establishes the convergence of stochastic\nprocesses that are almost supermartingales and is foundational for analyzing a\nwide range of stochastic iterative algorithms in stochastic approximation and\nreinforcement learning (RL). However, its original form has a significant\nlimitation as it requires the zero-order term to be summable. In many important\nRL applications, this summable condition, however, cannot be met. This\nlimitation motivates us to extend the Robbins-Siegmund theorem for almost\nsupermartingales where the zero-order term is not summable but only square\nsummable. Particularly, we introduce a novel and mild assumption on the\nincrements of the stochastic processes. This together with the square summable\ncondition enables an almost sure convergence to a bounded set. Additionally, we\nfurther provide almost sure convergence rates, high probability concentration\nbounds, and $L^p$ convergence rates. We then apply the new results in\nstochastic approximation and RL. Notably, we obtain the first almost sure\nconvergence rate, the first high probability concentration bound, and the first\n$L^p$ convergence rate for $Q$-learning with linear function approximation.", "AI": {"tldr": "\u6269\u5c55Robbins-Siegmund\u5b9a\u7406\uff0c\u653e\u5bbd\u96f6\u9636\u9879\u7684\u53ef\u548c\u6027\u8981\u6c42\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2dQ-learning\u7b49\u7b97\u6cd5\u63d0\u4f9b\u6536\u655b\u6027\u5206\u6790\u3002", "motivation": "\u539f\u59cbRobbins-Siegmund\u5b9a\u7406\u8981\u6c42\u96f6\u9636\u9879\u53ef\u548c\uff0c\u4f46\u5728\u8bb8\u591a\u91cd\u8981\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e2d\u8fd9\u4e00\u6761\u4ef6\u65e0\u6cd5\u6ee1\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5f15\u5165\u5bf9\u968f\u673a\u8fc7\u7a0b\u589e\u91cf\u7684\u65b0\u5047\u8bbe\uff0c\u7ed3\u5408\u5e73\u65b9\u53ef\u548c\u6761\u4ef6\uff0c\u8bc1\u660e\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u5230\u6709\u754c\u96c6\u3002", "result": "\u83b7\u5f97\u4e86\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u901f\u7387\u3001\u9ad8\u6982\u7387\u96c6\u4e2d\u754c\u548cL^p\u6536\u655b\u901f\u7387\uff0c\u9996\u6b21\u4e3a\u5e26\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u7684Q-learning\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u6536\u655b\u6027\u7ed3\u679c\u3002", "conclusion": "\u6269\u5c55\u7684Robbins-Siegmund\u5b9a\u7406\u4e3a\u66f4\u5e7f\u6cdb\u7684\u968f\u673a\u8fed\u4ee3\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u5206\u6790\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2509.26468", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26468", "abs": "https://arxiv.org/abs/2509.26468", "authors": ["Oleksandr Shchur", "Abdul Fatir Ansari", "Caner Turkmen", "Lorenzo Stella", "Nick Erickson", "Pablo Guerron", "Michael Bohlke-Schneider", "Yuyang Wang"], "title": "fev-bench: A Realistic Benchmark for Time Series Forecasting", "comment": null, "summary": "Benchmark quality is critical for meaningful evaluation and sustained\nprogress in time series forecasting, particularly given the recent rise of\npretrained models. Existing benchmarks often have narrow domain coverage or\noverlook important real-world settings, such as tasks with covariates.\nAdditionally, their aggregation procedures often lack statistical rigor, making\nit unclear whether observed performance differences reflect true improvements\nor random variation. Many benchmarks also fail to provide infrastructure for\nconsistent evaluation or are too rigid to integrate into existing pipelines. To\naddress these gaps, we propose fev-bench, a benchmark comprising 100\nforecasting tasks across seven domains, including 46 tasks with covariates.\nSupporting the benchmark, we introduce fev, a lightweight Python library for\nbenchmarking forecasting models that emphasizes reproducibility and seamless\nintegration with existing workflows. Usingfev, fev-bench employs principled\naggregation methods with bootstrapped confidence intervals to report model\nperformance along two complementary dimensions: win rates and skill scores. We\nreport results on fev-bench for various pretrained, statistical and baseline\nmodels, and identify promising directions for future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86fev-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b100\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u548cfev\u8f7b\u91cf\u7ea7Python\u5e93\uff0c\u4f7f\u7528\u7edf\u8ba1\u4e25\u8c28\u7684\u805a\u5408\u65b9\u6cd5\u548c\u7f6e\u4fe1\u533a\u95f4\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u9886\u57df\u8986\u76d6\u7a84\u3001\u5ffd\u89c6\u534f\u53d8\u91cf\u4efb\u52a1\u3001\u7f3a\u4e4f\u7edf\u8ba1\u4e25\u8c28\u6027\u3001\u96be\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u7684\u6301\u7eed\u8fdb\u6b65\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b100\u4e2a\u9884\u6d4b\u4efb\u52a1\u7684fev-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d67\u4e2a\u9886\u57df\uff08\u5176\u4e2d46\u4e2a\u4efb\u52a1\u5305\u542b\u534f\u53d8\u91cf\uff09\uff0c\u5e76\u521b\u5efa\u4e86fev\u8f7b\u91cf\u7ea7Python\u5e93\u652f\u6301\u53ef\u590d\u73b0\u7684\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528fev-bench\u8bc4\u4f30\u4e86\u5404\u79cd\u9884\u8bad\u7ec3\u3001\u7edf\u8ba1\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u901a\u8fc7\u80dc\u7387\u548c\u6280\u80fd\u5f97\u5206\u4e24\u4e2a\u7ef4\u5ea6\u62a5\u544a\u6027\u80fd\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "fev-bench\u548cfev\u5e93\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5173\u952e\u7f3a\u9677\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u4e25\u8c28\u548c\u5b9e\u7528\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.26469", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26469", "abs": "https://arxiv.org/abs/2509.26469", "authors": ["Mohammad Hassan Vali", "Tom B\u00e4ckstr\u00f6m", "Arno Solin"], "title": "DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick", "comment": null, "summary": "Vector quantization is common in deep models, yet its hard assignments block\ngradients and hinder end-to-end training. We propose DiVeQ, which treats\nquantization as adding an error vector that mimics the quantization distortion,\nkeeping the forward pass hard while letting gradients flow. We also present a\nspace-filling variant (SF-DiVeQ) that assigns to a curve constructed by the\nlines connecting codewords, resulting in less quantization error and full\ncodebook usage. Both methods train end-to-end without requiring auxiliary\nlosses or temperature schedules. On VQ-VAE compression and VQGAN generation\nacross various data sets, they improve reconstruction and sample quality over\nalternative quantization approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiVeQ\u548cSF-DiVeQ\u4e24\u79cd\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u91cf\u5316\u89c6\u4e3a\u6dfb\u52a0\u6a21\u62df\u91cf\u5316\u5931\u771f\u7684\u8bef\u5dee\u5411\u91cf\uff0c\u4fdd\u6301\u524d\u5411\u4f20\u64ad\u7684\u786c\u5206\u914d\u540c\u65f6\u5141\u8bb8\u68af\u5ea6\u6d41\u52a8\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u5411\u91cf\u91cf\u5316\u4e2d\u7684\u786c\u5206\u914d\u4f1a\u963b\u65ad\u68af\u5ea6\uff0c\u963b\u788d\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u786c\u5206\u914d\u4f18\u52bf\u53c8\u80fd\u652f\u6301\u68af\u5ea6\u4f20\u64ad\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "DiVeQ\u5c06\u91cf\u5316\u5efa\u6a21\u4e3a\u6dfb\u52a0\u6a21\u62df\u91cf\u5316\u5931\u771f\u7684\u8bef\u5dee\u5411\u91cf\uff1bSF-DiVeQ\u8fdb\u4e00\u6b65\u5c06\u5206\u914d\u6269\u5c55\u5230\u7531\u7801\u5b57\u8fde\u7ebf\u6784\u6210\u7684\u66f2\u7ebf\u4e0a\uff0c\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u5145\u5206\u5229\u7528\u7801\u672c\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u4e0d\u9700\u8981\u8f85\u52a9\u635f\u5931\u6216\u6e29\u5ea6\u8c03\u5ea6\u3002", "result": "\u5728VQ-VAE\u538b\u7f29\u548cVQGAN\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u5176\u4ed6\u91cf\u5316\u65b9\u6cd5\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6837\u672c\u8d28\u91cf\u3002", "conclusion": "DiVeQ\u548cSF-DiVeQ\u662f\u6709\u6548\u7684\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5728\u538b\u7f29\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.26499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26499", "abs": "https://arxiv.org/abs/2509.26499", "authors": ["Gerrit Gerhartz", "Peter Lippmann", "Fred A. Hamprecht"], "title": "Equivariance by Local Canonicalization: A Matter of Representation", "comment": "To be presented at NeurReps Workshop 2025", "summary": "Equivariant neural networks offer strong inductive biases for learning from\nmolecular and geometric data but often rely on specialized, computationally\nexpensive tensor operations. We present a framework to transfers existing\ntensor field networks into the more efficient local canonicalization paradigm,\npreserving equivariance while significantly improving the runtime. Within this\nframework, we systematically compare different equivariant representations in\nterms of theoretical complexity, empirical runtime, and predictive accuracy. We\npublish the tensor_frames package, a PyTorchGeometric based implementation for\nlocal canonicalization, that enables straightforward integration of\nequivariance into any standard message passing neural network.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\u5c06\u5f20\u91cf\u573a\u7f51\u7edc\u8f6c\u6362\u4e3a\u66f4\u9ad8\u6548\u7684\u5c40\u90e8\u6b63\u5219\u5316\u8303\u5f0f\uff0c\u4fdd\u6301\u7b49\u53d8\u6027\u540c\u65f6\u663e\u8457\u63d0\u5347\u8fd0\u884c\u901f\u5ea6\uff0c\u5e76\u53d1\u5e03\u4e86tensor_frames\u8f6f\u4ef6\u5305\u3002", "motivation": "\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5b50\u548c\u51e0\u4f55\u6570\u636e\u5b66\u4e60\u4e2d\u5177\u6709\u5f3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e13\u95e8\u4e14\u8ba1\u7b97\u6602\u8d35\u7684\u5f20\u91cf\u64cd\u4f5c\uff0c\u9700\u8981\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u5f00\u53d1\u6846\u67b6\u5c06\u73b0\u6709\u5f20\u91cf\u573a\u7f51\u7edc\u8f6c\u6362\u4e3a\u5c40\u90e8\u6b63\u5219\u5316\u8303\u5f0f\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u7b49\u53d8\u8868\u793a\u7684\u7406\u8bba\u590d\u6742\u5ea6\u3001\u7ecf\u9a8c\u8fd0\u884c\u65f6\u95f4\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u7b49\u53d8\u6027\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u63d0\u4f9b\u4e86\u6613\u4e8e\u96c6\u6210\u5230\u6807\u51c6\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5b9e\u73b0\u3002", "conclusion": "\u5c40\u90e8\u6b63\u5219\u5316\u8303\u5f0f\u4e3a\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\uff0c\u4f7f\u7b49\u53d8\u6027\u80fd\u591f\u66f4\u4fbf\u6377\u5730\u96c6\u6210\u5230\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u3002"}}
{"id": "2509.26522", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26522", "abs": "https://arxiv.org/abs/2509.26522", "authors": ["Xi Wang", "James McInerney", "Lequn Wang", "Nathan Kallus"], "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting", "comment": null, "summary": "Large reasoning models show improved performance with longer chains of\nthought. However, recent work has highlighted (qualitatively) their tendency to\noverthink, continuing to revise answers even after reaching the correct\nsolution. We quantitatively confirm this inefficiency by tracking Pass@1 for\nanswers averaged over a large number of rollouts and find that the model often\nbegins to always produce the correct answer early in the reasoning, making\nextra reasoning a waste of tokens. To detect and prevent overthinking, we\npropose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)\n-- for monitoring and deciding whether to exit reasoning early. By appending a\nstop thinking token (</think>) and monitoring the entropy of the following\ntoken as the model reasons, we obtain a trajectory that decreases and\nstabilizes when Pass@1 plateaus; thresholding its variance under an exponential\nmoving average yields a practical stopping rule. Importantly, our approach\nenables adaptively allocating compute based on the EAT trajectory, allowing us\nto spend compute in a more efficient way compared with fixing the token budget\nfor all questions. Empirically, on MATH500 and AIME2025, EAT reduces token\nusage by 13 - 21% without harming accuracy, and it remains effective in black\nbox settings where logits from the reasoning model are not accessible, and EAT\nis computed with proxy models.", "AI": {"tldr": "\u63d0\u51faEAT\uff08Entropy After </Think>\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u71b5\u53d8\u5316\u6765\u68c0\u6d4b\u548c\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\uff0c\u51cf\u5c1113-21%\u7684token\u4f7f\u7528\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8fc7\u5ea6\u601d\u8003\u7684\u503e\u5411\uff0c\u5373\u4f7f\u5df2\u5f97\u5230\u6b63\u786e\u7b54\u6848\u4ecd\u7ee7\u7eed\u4fee\u6b63\uff0c\u9020\u6210token\u6d6a\u8d39\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u9632\u6b62\u8fd9\u79cd\u4f4e\u6548\u884c\u4e3a\u3002", "method": "\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6dfb\u52a0\u505c\u6b62\u601d\u8003\u6807\u8bb0</think>\uff0c\u76d1\u63a7\u540e\u7eedtoken\u7684\u71b5\u53d8\u5316\u8f68\u8ff9\uff0c\u5f53Pass@1\u8fbe\u5230\u5e73\u53f0\u671f\u65f6\u71b5\u4f1a\u4e0b\u964d\u5e76\u7a33\u5b9a\uff0c\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u65b9\u5dee\u9608\u503c\u5b9e\u73b0\u65e9\u671f\u505c\u6b62\u63a8\u7406\u3002", "result": "\u5728MATH500\u548cAIME2025\u6570\u636e\u96c6\u4e0a\uff0cEAT\u65b9\u6cd5\u51cf\u5c1113-21%\u7684token\u4f7f\u7528\uff0c\u4e14\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u3002\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\uff08\u65e0\u6cd5\u8bbf\u95ee\u63a8\u7406\u6a21\u578blogits\uff09\u4e5f\u4fdd\u6301\u6709\u6548\u3002", "conclusion": "EAT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u76d1\u63a7\u548c\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2509.26524", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26524", "abs": "https://arxiv.org/abs/2509.26524", "authors": ["Seohyun Lee", "Wenzhi Fang", "Dong-Jun Han", "Seyyedali Hosseinalipour", "Christopher G. Brinton"], "title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning", "comment": null, "summary": "Federated Learning (FL), despite demonstrating impressive capabilities in the\ntraining of multiple models in a decentralized manner, has been shown to\nproduce a final model not necessarily well-suited to the needs of each client.\nWhile extensive work has been conducted on how to create tailored personalized\nmodels, called Personalized Federated Learning (PFL), less attention has been\ngiven to personalization via fine-tuning of foundation models with multi-task\nand multi-modal properties. Moreover, there exists a lack of understanding in\nthe literature on how to fine-tune and personalize such models in a setting\nthat is heterogeneous across clients not only in data, but also in tasks and\nmodalities. To address this gap in the literature, we propose TAP (Two-Stage\nAdaptive Personalization), which (i) leverages mismatched model architectures\nbetween the clients and server to selectively conduct replacement operations\nwhen it benefits a client's local tasks and (ii) engages in post-FL knowledge\ndistillation for capturing beneficial general knowledge without compromising\npersonalization. We also introduce the first convergence analysis of the server\nmodel under its modality-task pair architecture, and demonstrate that as the\nnumber of modality-task pairs increases, its ability to cater to all tasks\nsuffers. Through extensive experiments, we demonstrate the effectiveness of our\nproposed algorithm across a variety of datasets and tasks in comparison to a\nmultitude of baselines. Implementation code is publicly available at\nhttps://github.com/lee3296/TAP.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAP\uff08\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u4e2a\u6027\u5316\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u4e0d\u5339\u914d\u7684\u6a21\u578b\u67b6\u6784\u548c\u77e5\u8bc6\u84b8\u998f\u6765\u4e2a\u6027\u5316\u591a\u4efb\u52a1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u4ea7\u751f\u7684\u6700\u7ec8\u6a21\u578b\u4e0d\u4e00\u5b9a\u9002\u5408\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u3001\u4efb\u52a1\u548c\u6a21\u6001\u90fd\u5b58\u5728\u5f02\u6784\u6027\u7684\u573a\u666f\u4e0b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u4efb\u52a1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e2a\u6027\u5316\u5fae\u8c03\u7684\u7814\u7a76", "method": "TAP\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(i)\u5229\u7528\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u95f4\u4e0d\u5339\u914d\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5728\u6709\u76ca\u4e8e\u5ba2\u6237\u7aef\u672c\u5730\u4efb\u52a1\u65f6\u9009\u62e9\u6027\u8fdb\u884c\u66ff\u6362\u64cd\u4f5c\uff1b(ii)\u5728\u8054\u90a6\u5b66\u4e60\u540e\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u6355\u83b7\u6709\u76ca\u901a\u7528\u77e5\u8bc6\u800c\u4e0d\u635f\u5bb3\u4e2a\u6027\u5316", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u670d\u52a1\u5668\u6a21\u578b\u5728\u6a21\u6001-\u4efb\u52a1\u5bf9\u67b6\u6784\u4e0b\u7684\u6536\u655b\u6027\u5206\u6790", "conclusion": "TAP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u7684\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u4efb\u52a1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02"}}
{"id": "2509.26532", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26532", "abs": "https://arxiv.org/abs/2509.26532", "authors": ["Justin Tackett", "Benjamin Francis", "Luis Garcia", "David Grimsman", "Sean Warnick"], "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids", "comment": null, "summary": "Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u7535\u529b\u7cfb\u7edf\u8d1f\u8377\u524a\u51cf\u51b3\u7b56\u7cfb\u7edf\u9632\u5fa1\u4e0d\u7a33\u5b9a\u653b\u51fb\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u65e5\u76ca\u590d\u6742\u4e14\u4f9d\u8d56\u5ea6\u589e\u52a0\uff0c\u7535\u529b\u7cfb\u7edf\u6210\u4e3a\u7f51\u7edc\u653b\u51fb\u7684\u8bf1\u4eba\u76ee\u6807\uff0c\u7279\u522b\u662f\u65b0\u578b\u7684\u4e0d\u7a33\u5b9a\u653b\u51fb\u7f3a\u4e4f\u6709\u6548\u9632\u62a4\u63aa\u65bd\u3002", "method": "\u4f7f\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728IEEE 14\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\uff0c\u91c7\u7528\u6539\u8fdb\u7684Prony\u5206\u6790(MPA)\u6765\u68c0\u6d4b\u4e0d\u7a33\u5b9a\u653b\u51fb\u3002", "result": "\u8bc1\u660eMPA\u662f\u68c0\u6d4b\u4e0d\u7a33\u5b9a\u653b\u51fb\u5e76\u89e6\u53d1\u9632\u5fa1\u673a\u5236\u7684\u6709\u6548\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7535\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u9632\u5fa1\u4e0d\u7a33\u5b9a\u653b\u51fb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26537", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26537", "abs": "https://arxiv.org/abs/2509.26537", "authors": ["Maxwell Adam", "Zach Furman", "Jesse Hoogland"], "title": "The Loss Kernel: A Geometric Probe for Deep Learning Interpretability", "comment": "25 pages, 11 figures", "summary": "We introduce the loss kernel, an interpretability method for measuring\nsimilarity between data points according to a trained neural network. The\nkernel is the covariance matrix of per-sample losses computed under a\ndistribution of low-loss-preserving parameter perturbations. We first validate\nour method on a synthetic multitask problem, showing it separates inputs by\ntask as predicted by theory. We then apply this kernel to Inception-v1 to\nvisualize the structure of ImageNet, and we show that the kernel's structure\naligns with the WordNet semantic hierarchy. This establishes the loss kernel as\na practical tool for interpretability and data attribution.", "AI": {"tldr": "\u63d0\u51fa\u635f\u5931\u6838\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u635f\u5931\u53c2\u6570\u6270\u52a8\u4e0b\u7684\u6837\u672c\u635f\u5931\u534f\u65b9\u5dee\u77e9\u9635\u6765\u8861\u91cf\u795e\u7ecf\u7f51\u7edc\u4e2d\u6570\u636e\u70b9\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u5f52\u56e0\u5206\u6790\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u8861\u91cf\u6570\u636e\u70b9\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e2e\u52a9\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7ec4\u7ec7\u548c\u5206\u7c7b\u6570\u636e\u3002", "method": "\u6784\u5efa\u635f\u5931\u6838\u4f5c\u4e3a\u4f4e\u635f\u5931\u53c2\u6570\u6270\u52a8\u5206\u5e03\u4e0b\u8ba1\u7b97\u7684\u6837\u672c\u635f\u5931\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u901a\u8fc7\u53c2\u6570\u6270\u52a8\u6765\u6355\u6349\u6a21\u578b\u5bf9\u6570\u636e\u76f8\u4f3c\u6027\u7684\u7406\u89e3\u3002", "result": "\u5728\u5408\u6210\u591a\u4efb\u52a1\u95ee\u9898\u4e0a\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u663e\u793a\u80fd\u6309\u4efb\u52a1\u5206\u79bb\u8f93\u5165\uff1b\u5728Inception-v1\u4e0a\u5e94\u7528\u663e\u793a\u6838\u7ed3\u6784\u4e0eWordNet\u8bed\u4e49\u5c42\u6b21\u4e00\u81f4\u3002", "conclusion": "\u635f\u5931\u6838\u662f\u5b9e\u7528\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u5f52\u56e0\u5de5\u5177\uff0c\u80fd\u591f\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u7684\u6570\u636e\u7ec4\u7ec7\u7ed3\u6784\u3002"}}
{"id": "2509.26544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26544", "abs": "https://arxiv.org/abs/2509.26544", "authors": ["Philipp Alexander Kreer", "Wilson Wu", "Maxwell Adam", "Zach Furman", "Jesse Hoogland"], "title": "Bayesian Influence Functions for Hessian-Free Data Attribution", "comment": "32 pages, 19 figures", "summary": "Classical influence functions face significant challenges when applied to\ndeep neural networks, primarily due to non-invertible Hessians and\nhigh-dimensional parameter spaces. We propose the local Bayesian influence\nfunction (BIF), an extension of classical influence functions that replaces\nHessian inversion with loss landscape statistics that can be estimated via\nstochastic-gradient MCMC sampling. This Hessian-free approach captures\nhigher-order interactions among parameters and scales efficiently to neural\nnetworks with billions of parameters. We demonstrate state-of-the-art results\non predicting retraining experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5c40\u90e8\u8d1d\u53f6\u65af\u5f71\u54cd\u51fd\u6570(BIF)\uff0c\u901a\u8fc7\u4f7f\u7528\u635f\u5931\u666f\u89c2\u7edf\u8ba1\u66ff\u4ee3Hessian\u9006\u77e9\u9635\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f71\u54cd\u51fd\u6570\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u9762\u4e34\u7684Hessian\u4e0d\u53ef\u9006\u548c\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f71\u54cd\u51fd\u6570\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u9762\u4e34Hessian\u77e9\u9635\u4e0d\u53ef\u9006\u548c\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u4ee3\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68af\u5ea6MCMC\u91c7\u6837\u4f30\u8ba1\u635f\u5931\u666f\u89c2\u7edf\u8ba1\uff0c\u66ff\u4ee3Hessian\u9006\u77e9\u9635\u8ba1\u7b97\uff0c\u6784\u5efaHessian-free\u7684\u5c40\u90e8\u8d1d\u53f6\u65af\u5f71\u54cd\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u53c2\u6570\u95f4\u7684\u9ad8\u9636\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u9ad8\u6548\u6269\u5c55\u5230\u6570\u5341\u4ebf\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u91cd\u8bad\u7ec3\u5b9e\u9a8c\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u5c40\u90e8\u8d1d\u53f6\u65af\u5f71\u54cd\u51fd\u6570\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5f71\u54cd\u5206\u6790\u5de5\u5177\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.26564", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26564", "abs": "https://arxiv.org/abs/2509.26564", "authors": ["Florian Gr\u00f6tschla", "Longxiang Jiao", "Luca A. Lanzend\u00f6rfer", "Roger Wattenhofer"], "title": "Parametric Neural Amp Modeling with Active Learning", "comment": null, "summary": "We introduce Panama, an active learning framework to train parametric guitar\namp models end-to-end using a combination of an LSTM model and a WaveNet-like\narchitecture. With \\model, one can create a virtual amp by recording samples\nthat are determined through an ensemble-based active learning strategy to\nminimize the amount of datapoints needed (i.e., amp knob settings). Our\nstrategy uses gradient-based optimization to maximize the disagreement among\nensemble models, in order to identify the most informative datapoints. MUSHRA\nlistening tests reveal that, with 75 datapoints, our models are able to match\nthe perceptual quality of NAM, the leading open-source non-parametric amp\nmodeler.", "AI": {"tldr": "Panama\u662f\u4e00\u4e2a\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528LSTM\u548cWaveNet\u67b6\u6784\u7aef\u5230\u7aef\u8bad\u7ec3\u53c2\u6570\u5316\u5409\u4ed6\u653e\u5927\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u96c6\u6210\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u6700\u5c0f\u5316\u6240\u9700\u6570\u636e\u70b9\u6570\u91cf\uff0c\u4ec5\u970075\u4e2a\u6570\u636e\u70b9\u5373\u53ef\u8fbe\u5230\u9886\u5148\u5f00\u6e90\u975e\u53c2\u6570\u653e\u5927\u5668\u6a21\u578b\u7684\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u6700\u5c0f\u5316\u6570\u636e\u70b9\uff08\u5373\u653e\u5927\u5668\u65cb\u94ae\u8bbe\u7f6e\uff09\u6765\u521b\u5efa\u865a\u62df\u653e\u5927\u5668\u7684\u53c2\u6570\u5316\u5409\u4ed6\u653e\u5927\u5668\u6a21\u578b\uff0c\u51cf\u5c11\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u6240\u9700\u7684\u5927\u91cf\u6570\u636e\u91c7\u96c6\u5de5\u4f5c\u3002", "method": "\u7ed3\u5408LSTM\u6a21\u578b\u548cWaveNet\u67b6\u6784\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u96c6\u6210\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u6765\u6700\u5927\u5316\u96c6\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u5206\u6b67\uff0c\u4ece\u800c\u8bc6\u522b\u6700\u6709\u4fe1\u606f\u91cf\u7684\u6570\u636e\u70b9\u3002", "result": "MUSHRA\u542c\u529b\u6d4b\u8bd5\u663e\u793a\uff0c\u4ec5\u4f7f\u752875\u4e2a\u6570\u636e\u70b9\uff0cPanama\u6a21\u578b\u5c31\u80fd\u591f\u5339\u914dNAM\uff08\u9886\u5148\u7684\u5f00\u6e90\u975e\u53c2\u6570\u653e\u5927\u5668\u5efa\u6a21\u5668\uff09\u7684\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "Panama\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u53c2\u6570\u5316\u5409\u4ed6\u653e\u5927\u5668\u6a21\u578b\u6240\u9700\u7684\u6570\u636e\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u865a\u62df\u653e\u5927\u5668\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26576", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.26576", "abs": "https://arxiv.org/abs/2509.26576", "authors": ["David S. Li", "Somdatta Goswami", "Qianying Cao", "Vivek Oommen", "Roland Assi", "Jay D. Humphrey", "George E. Karniadakis"], "title": "Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators", "comment": null, "summary": "Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and\nmechanobiological disruptions to the aortic wall that increase the risk of\ndissection or rupture. Evidence links TAA development to dysfunctions in the\naortic mechanotransduction axis, including loss of elastic fiber integrity and\ncell-matrix connections. Because distinct insults create different mechanical\nvulnerabilities, there is a critical need to identify interacting factors that\ndrive progression. Here, we use a finite element framework to generate\nsynthetic TAAs from hundreds of heterogeneous insults spanning varying degrees\nof elastic fiber damage and impaired mechanosensing. From these simulations, we\nconstruct spatial maps of localized dilatation and distensibility to train\nneural networks that predict the initiating combined insult. We compare several\narchitectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and\nmultiple input data formats to define a standard for future subject-specific\nmodeling. We also quantify predictive performance when networks are trained\nusing only geometric data (dilatation) versus both geometric and mechanical\ndata (dilatation plus distensibility). Across all networks, prediction errors\nare significantly higher when trained on dilatation alone, underscoring the\nadded value of distensibility information. Among the tested models, UNet\nconsistently provides the highest accuracy across all data formats. These\nfindings highlight the importance of acquiring full-field measurements of both\ndilatation and distensibility in TAA assessment to reveal the mechanobiological\ndrivers of disease and support the development of personalized treatment\nstrategies.", "AI": {"tldr": "\u4f7f\u7528\u6709\u9650\u5143\u6846\u67b6\u751f\u6210\u5408\u6210\u80f8\u4e3b\u52a8\u8109\u7624\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u521d\u59cb\u635f\u4f24\u7ec4\u5408\uff0c\u53d1\u73b0\u7ed3\u5408\u6269\u5f20\u6027\u548c\u53ef\u6269\u5f20\u6027\u6570\u636e\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0cUNet\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u80f8\u4e3b\u52a8\u8109\u7624\u7531\u591a\u79cd\u673a\u68b0\u548c\u673a\u68b0\u751f\u7269\u5b66\u7834\u574f\u5f15\u8d77\uff0c\u4e0d\u540c\u635f\u4f24\u9020\u6210\u4e0d\u540c\u7684\u673a\u68b0\u8106\u5f31\u6027\uff0c\u9700\u8981\u8bc6\u522b\u9a71\u52a8\u75be\u75c5\u8fdb\u5c55\u7684\u76f8\u4e92\u4f5c\u7528\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u6709\u9650\u5143\u6846\u67b6\u751f\u6210\u6570\u767e\u79cd\u5f02\u8d28\u6027\u635f\u4f24\u7684\u5408\u6210\u80f8\u4e3b\u52a8\u8109\u7624\uff0c\u6784\u5efa\u5c40\u90e8\u6269\u5f20\u548c\u53ef\u6269\u5f20\u6027\u7684\u7a7a\u95f4\u56fe\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u521d\u59cb\u635f\u4f24\u7ec4\u5408\uff0c\u6bd4\u8f83\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u683c\u5f0f\u3002", "result": "\u4ec5\u4f7f\u7528\u6269\u5f20\u6027\u6570\u636e\u65f6\u9884\u6d4b\u8bef\u5dee\u663e\u8457\u66f4\u9ad8\uff0c\u7ed3\u5408\u6269\u5f20\u6027\u548c\u53ef\u6269\u5f20\u6027\u6570\u636e\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff1bUNet\u5728\u6240\u6709\u6570\u636e\u683c\u5f0f\u4e2d\u8868\u73b0\u6700\u51c6\u786e\u3002", "conclusion": "\u5728\u80f8\u4e3b\u52a8\u8109\u7624\u8bc4\u4f30\u4e2d\u83b7\u53d6\u6269\u5f20\u6027\u548c\u53ef\u6269\u5f20\u6027\u7684\u5168\u573a\u6d4b\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u63ed\u793a\u75be\u75c5\u7684\u673a\u68b0\u751f\u7269\u5b66\u9a71\u52a8\u56e0\u7d20\uff0c\u652f\u6301\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.26578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26578", "abs": "https://arxiv.org/abs/2509.26578", "authors": ["Zheng Zhang", "Ziwei Shan", "Kaitao Song", "Yexin Li", "Kan Ren"], "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth.", "AI": {"tldr": "\u63d0\u51fa\u6761\u4ef6\u5956\u52b1\u5efa\u6a21\uff08CRM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u65f6\u95f4\u8fc7\u7a0b\uff0c\u5c06\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u5956\u52b1\u4e0e\u6700\u7ec8\u7ed3\u679c\u660e\u786e\u5173\u8054\uff0c\u89e3\u51b3\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8981\u4e48\u5b64\u7acb\u5904\u7406\u63a8\u7406\u6b65\u9aa4\uff0c\u8981\u4e48\u96be\u4ee5\u5c06\u8fc7\u7a0b\u5956\u52b1\u4e0e\u6700\u7ec8\u7ed3\u679c\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u4e0d\u5c0a\u91cd\u65f6\u5e8f\u56e0\u679c\u5173\u7cfb\uff0c\u5b58\u5728\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u95ee\u9898\uff0c\u4f7f\u4e0b\u6e38\u6a21\u578b\u6613\u53d7\u5956\u52b1\u653b\u51fb\u3002", "method": "\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u65f6\u95f4\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u5956\u52b1\u4e0d\u4ec5\u57fa\u4e8e\u524d\u5e8f\u6b65\u9aa4\uff0c\u8fd8\u660e\u786e\u4e0e\u63a8\u7406\u8f68\u8ff9\u7684\u6700\u7ec8\u7ed3\u679c\u5173\u8054\uff0c\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u6761\u4ef6\u6982\u7387\u89c4\u5219\u6765\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728Best-of-N\u91c7\u6837\u3001\u6ce2\u675f\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u9a8c\u4e2d\uff0cCRM\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u6a21\u578b\uff0c\u5bf9\u5956\u52b1\u653b\u51fb\u66f4\u9c81\u68d2\uff0c\u65e0\u9700\u4f9d\u8d56\u57fa\u4e8e\u771f\u5b9e\u503c\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5373\u53ef\u63d0\u4f9b\u7a33\u5b9a\u7684\u4e0b\u6e38\u6539\u8fdb\u3002", "conclusion": "CRM\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u4fe1\u7528\u5206\u914d\u548c\u53ef\u9760\u7684\u8de8\u6837\u672c\u6bd4\u8f83\uff0c\u89e3\u51b3\u4e86\u8fc7\u7a0b\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.26594", "categories": ["cs.LG", "cs.CL", "cs.CV", "68T05 (Primary) 68T45, 68T50 (Secondary)", "I.2.6; I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.26594", "abs": "https://arxiv.org/abs/2509.26594", "authors": ["John Gkountouras", "Ivan Titov"], "title": "Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces", "comment": null, "summary": "Recent text-only models demonstrate remarkable mathematical reasoning\ncapabilities. Extending these to visual domains requires vision-language models\nto translate images into text descriptions. However, current models, trained to\nproduce captions for human readers, often omit the precise details that\nreasoning systems require. This creates an interface mismatch: reasoners often\nfail not due to reasoning limitations but because they lack access to critical\nvisual information. We propose Adaptive-Clarification Reinforcement Learning\n(AC-RL), which teaches vision models what information reasoners need through\ninteraction. Our key insight is that clarification requests during training\nreveal information gaps; by penalizing success that requires clarification, we\ncreate pressure for comprehensive initial captions that enable the reasoner to\nsolve the problem in a single pass. AC-RL improves average accuracy by 4.4\npoints over pretrained baselines across seven visual mathematical reasoning\nbenchmarks, and analysis shows it would cut clarification requests by up to 39%\nif those were allowed. By treating clarification as a form of implicit\nsupervision, AC-RL demonstrates that vision-language interfaces can be\neffectively learned through interaction alone, without requiring explicit\nannotations.", "AI": {"tldr": "\u63d0\u51faAC-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u4e92\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u751f\u6210\u5305\u542b\u63a8\u7406\u6240\u9700\u5173\u952e\u4fe1\u606f\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u89e3\u51b3\u5f53\u524d\u6a21\u578b\u56e0\u5ffd\u7565\u7ec6\u8282\u5bfc\u81f4\u63a8\u7406\u5931\u8d25\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u4e3a\u4eba\u7c7b\u8bfb\u8005\u8bbe\u8ba1\uff0c\u5e38\u7701\u7565\u63a8\u7406\u7cfb\u7edf\u6240\u9700\u7684\u7cbe\u786e\u7ec6\u8282\uff0c\u5bfc\u81f4\u63a8\u7406\u5931\u8d25\u4e0d\u662f\u7531\u4e8e\u63a8\u7406\u80fd\u529b\u9650\u5236\uff0c\u800c\u662f\u7f3a\u4e4f\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u3002", "method": "AC-RL\u901a\u8fc7\u4ea4\u4e92\u8bad\u7ec3\uff0c\u5229\u7528\u6f84\u6e05\u8bf7\u6c42\u63ed\u793a\u4fe1\u606f\u7f3a\u53e3\uff0c\u60e9\u7f5a\u9700\u8981\u6f84\u6e05\u7684\u6210\u529f\u6848\u4f8b\uff0c\u4fc3\u4f7f\u6a21\u578b\u751f\u6210\u5168\u9762\u7684\u521d\u59cb\u63cf\u8ff0\uff0c\u4f7f\u63a8\u7406\u5668\u80fd\u4e00\u6b21\u6027\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u57287\u4e2a\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAC-RL\u6bd4\u9884\u8bad\u7ec3\u57fa\u7ebf\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad84.4\u4e2a\u767e\u5206\u70b9\uff0c\u5206\u6790\u663e\u793a\u5982\u679c\u5141\u8bb8\u6f84\u6e05\u8bf7\u6c42\u53ef\u51cf\u5c11\u9ad8\u8fbe39%\u3002", "conclusion": "AC-RL\u8bc1\u660e\u4ec5\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u5373\u53ef\u6709\u6548\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u63a5\u53e3\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\uff0c\u6f84\u6e05\u8bf7\u6c42\u53ef\u4f5c\u4e3a\u9690\u5f0f\u76d1\u7763\u5f62\u5f0f\u3002"}}
{"id": "2509.26610", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26610", "abs": "https://arxiv.org/abs/2509.26610", "authors": ["Alexander Fishkov", "Kajetan Schweighofer", "Mykyta Ielanskyi", "Nikita Kotelevskii", "Mohsen Guizani", "Maxim Panov"], "title": "Uncertainty Quantification for Regression using Proper Scoring Rules", "comment": null, "summary": "Quantifying uncertainty of machine learning model predictions is essential\nfor reliable decision-making, especially in safety-critical applications.\nRecently, uncertainty quantification (UQ) theory has advanced significantly,\nbuilding on a firm basis of learning with proper scoring rules. However, these\nadvances were focused on classification, while extending these ideas to\nregression remains challenging. In this work, we introduce a unified UQ\nframework for regression based on proper scoring rules, such as CRPS,\nlogarithmic, squared error, and quadratic scores. We derive closed-form\nexpressions for the resulting uncertainty measures under practical parametric\nassumptions and show how to estimate them using ensembles of models. In\nparticular, the derived uncertainty measures naturally decompose into aleatoric\nand epistemic components. The framework recovers popular regression UQ measures\nbased on predictive variance and differential entropy. Our broad evaluation on\nsynthetic and real-world regression datasets provides guidance for selecting\nreliable UQ measures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9002\u5f53\u8bc4\u5206\u89c4\u5219\u7684\u56de\u5f52\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u7136\u5206\u89e3\u4e3a\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u53c2\u6570\u5047\u8bbe\u4e0b\u63a8\u5bfc\u51fa\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8fdb\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u4efb\u52a1\uff0c\u56de\u5f52\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8e\u9002\u5f53\u8bc4\u5206\u89c4\u5219\uff08\u5982CRPS\u3001\u5bf9\u6570\u3001\u5e73\u65b9\u8bef\u5dee\u548c\u4e8c\u6b21\u8bc4\u5206\uff09\u6784\u5efa\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u5b9e\u9645\u53c2\u6570\u5047\u8bbe\u4e0b\u63a8\u5bfc\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u96c6\u6210\u8fdb\u884c\u4f30\u8ba1\u3002", "result": "\u63a8\u5bfc\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u81ea\u7136\u5206\u89e3\u4e3a\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u6210\u5206\uff0c\u6062\u590d\u4e86\u57fa\u4e8e\u9884\u6d4b\u65b9\u5dee\u548c\u5fae\u5206\u71b5\u7684\u6d41\u884c\u56de\u5f52UQ\u5ea6\u91cf\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e3a\u9009\u62e9\u53ef\u9760\u7684UQ\u5ea6\u91cf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56de\u5f52\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5206\u89e3\u4e0d\u540c\u7c7b\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.26625", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.26625", "abs": "https://arxiv.org/abs/2509.26625", "authors": ["Junlin Han", "Shengbang Tong", "David Fan", "Yufan Ren", "Koustuv Sinha", "Philip Torr", "Filippos Kokkinos"], "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training", "comment": "Project page: https://junlinhan.github.io/projects/lsbs/", "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.", "AI": {"tldr": "LLMs\u5728\u7eaf\u6587\u672c\u8bad\u7ec3\u4e2d\u610f\u5916\u5730\u53d1\u5c55\u51fa\u4e30\u5bcc\u7684\u89c6\u89c9\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u5148\u9a8c\u53ef\u5206\u4e3a\u611f\u77e5\u5148\u9a8c\u548c\u63a8\u7406\u5148\u9a8c\uff0c\u5177\u6709\u4e0d\u540c\u7684\u6269\u5c55\u8d8b\u52bf\u548c\u6765\u6e90\u3002\u63a8\u7406\u5148\u9a8c\u4e3b\u8981\u6765\u81ea\u4ee3\u7801\u3001\u6570\u5b66\u7b49\u63a8\u7406\u4e2d\u5fc3\u6570\u636e\uff0c\u800c\u611f\u77e5\u5148\u9a8c\u5219\u6765\u81ea\u5e7f\u6cdb\u8bed\u6599\u5e93\u3002", "motivation": "\u63ed\u793aLLMs\u5728\u7eaf\u6587\u672c\u9884\u8bad\u7ec3\u4e2d\u5982\u4f55\u83b7\u5f97\u89c6\u89c9\u80fd\u529b\uff0c\u7406\u89e3\u89c6\u89c9\u5148\u9a8c\u7684\u7ec4\u6210\u548c\u6765\u6e90\uff0c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001LLMs\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7100\u591a\u4e2a\u63a7\u5236\u5b9e\u9a8c\uff08\u6d88\u801750\u4e07GPU\u5c0f\u65f6\uff09\uff0c\u6db5\u76d6\u5b8c\u6574\u7684MLLM\u6784\u5efa\u6d41\u7a0b\uff0c\u5305\u62ecLLM\u9884\u8bad\u7ec3\u3001\u89c6\u89c9\u5bf9\u9f50\u548c\u76d1\u7763\u591a\u6a21\u6001\u5fae\u8c03\uff0c\u5206\u6790\u4e0d\u540c\u6570\u636e\u7c7b\u522b\u548c\u6df7\u5408\u5bf9\u89c6\u89c9\u5148\u9a8c\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u63a8\u7406\u5148\u9a8c\u4e3b\u8981\u6765\u81ea\u63a8\u7406\u4e2d\u5fc3\u6570\u636e\uff0c\u5177\u6709\u53ef\u8f6c\u79fb\u6027\u548c\u901a\u7528\u6027\uff1b\u611f\u77e5\u5148\u9a8c\u66f4\u4f9d\u8d56\u89c6\u89c9\u7f16\u7801\u5668\u548c\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u6570\u636e\uff1b\u89c6\u89c9\u4e16\u754c\u63cf\u8ff0\u6587\u672c\u867d\u7136\u91cd\u8981\u4f46\u6027\u80fd\u5f71\u54cd\u5feb\u901f\u9971\u548c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u4ece\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u523b\u610f\u57f9\u517b\u89c6\u89c9\u5148\u9a8c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001LLMs\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.26626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26626", "abs": "https://arxiv.org/abs/2509.26626", "authors": ["Siddarth Venkatraman", "Vineet Jain", "Sarthak Mittal", "Vedant Shah", "Johan Obando-Ceron", "Yoshua Bengio", "Brian R. Bartoldson", "Bhavya Kailkhura", "Guillaume Lajoie", "Glen Berseth", "Nikolay Malkin", "Moksh Jain"], "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models", "comment": "24 pages, 9 figures", "summary": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52\u81ea\u805a\u5408(RSA)\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e76\u884c\u548c\u987a\u5e8f\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8fed\u4ee3\u805a\u5408\u5019\u9009\u63a8\u7406\u94fe\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u8981\u4e48\u662f\u5e76\u884c\u9009\u62e9\u591a\u4e2a\u72ec\u7acb\u89e3\uff0c\u8981\u4e48\u662f\u987a\u5e8f\u81ea\u6211\u4f18\u5316\uff0c\u4f46\u90fd\u672a\u80fd\u5145\u5206\u5229\u7528\u63a8\u7406\u94fe\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\u3002RSA\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5229\u7528\u90e8\u5206\u6b63\u786e\u7684\u4e2d\u95f4\u6b65\u9aa4\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "RSA\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u7cbe\u5316\u5019\u9009\u63a8\u7406\u94fe\uff1a\u6bcf\u4e00\u6b65\u90fd\u805a\u5408\u5b50\u96c6\u6765\u751f\u6210\u6539\u8fdb\u7684\u89e3\u7fa4\uff0c\u4f5c\u4e3a\u4e0b\u4e00\u8f6e\u7684\u5019\u9009\u6c60\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u63a8\u7406\u94fe\u4e2d\u7684\u4e2d\u95f4\u4fe1\u606f\uff0c\u4ece\u4e0d\u540c\u601d\u7ef4\u94fe\u7684\u90e8\u5206\u6b63\u786e\u6b65\u9aa4\u4e2d\u81ea\u4e3e\u5b66\u4e60\u3002", "result": "RSA\u5728\u4e0d\u540c\u4efb\u52a1\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u90fd\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002Qwen3-4B-Instruct-2507\u4f7f\u7528RSA\u540e\u80fd\u4e0e\u66f4\u5927\u7684\u63a8\u7406\u6a21\u578b\u7ade\u4e89\uff0c\u5728AIME-25\u3001HMMT-25\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7eaf\u5e76\u884c\u548c\u987a\u5e8f\u6269\u5c55\u7b56\u7565\u3002", "conclusion": "RSA\u662f\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5e76\u884c\u548c\u987a\u5e8f\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u5145\u5206\u5229\u7528\u63a8\u7406\u94fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7\u805a\u5408\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53ef\u8fdb\u4e00\u6b65\u83b7\u5f97\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2509.26628", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26628", "abs": "https://arxiv.org/abs/2509.26628", "authors": ["Runze Liu", "Jiakang Wang", "Yuling Shi", "Zhihui Xie", "Chenxin An", "Kaiyan Zhang", "Jian Zhao", "Xiaodong Gu", "Lei Lin", "Wenping Hu", "Xiu Li", "Fuzheng Zhang", "Guorui Zhou", "Kun Gai"], "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models", "comment": null, "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.", "AI": {"tldr": "\u63d0\u51faAttnRL\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u503c\u7684\u9ad8\u6548\u5206\u652f\u548c\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u652f\u4f4d\u7f6e\u548c\u91c7\u6837\u65b9\u9762\u5b58\u5728\u63a2\u7d22\u6548\u7387\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\u6765\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u4e0e\u63a8\u7406\u884c\u4e3a\u7684\u5173\u8054\u6027\uff0c\u4ece\u9ad8\u6ce8\u610f\u529b\u4f4d\u7f6e\u8fdb\u884c\u5206\u652f\uff1b\u91c7\u7528\u8003\u8651\u95ee\u9898\u96be\u5ea6\u548c\u5386\u53f2\u6279\u6b21\u5927\u5c0f\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff1b\u8bbe\u8ba1\u4e00\u6b65\u79bb\u7b56\u7565\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u91c7\u6837\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "AttnRL\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u63a2\u7d22\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2509.26636", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26636", "abs": "https://arxiv.org/abs/2509.26636", "authors": ["Shangding Gu", "Xiaohan Wang", "Donghao Ying", "Haoyu Zhao", "Runing Yang", "Ming Jin", "Boyi Li", "Marco Pavone", "Serena Yeung-Levy", "Jun Wang", "Dawn Song", "Costas Spanos"], "title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond", "comment": null, "summary": "Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench", "AI": {"tldr": "AccidentBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u8f66\u8f86\u4e8b\u6545\u573a\u666f\u548c\u7a7a\u4e2d\u3001\u6c34\u4e0a\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u5305\u542b\u7ea62000\u4e2a\u89c6\u9891\u548c19000\u4e2a\u6807\u6ce8\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u73b0\u5b9e\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u80fd\u591f\u4e25\u683c\u8bc4\u4f30\u5728\u5b89\u5168\u5173\u952e\u3001\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u5305\u542b\u8f66\u8f86\u4e8b\u6545\u573a\u666f\u548c\u7a7a\u4e2d\u3001\u6c34\u4e0a\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e0d\u540c\u89c6\u9891\u957f\u5ea6\u548c\u96be\u5ea6\u7ea7\u522b\uff0c\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5\u65f6\u95f4\u3001\u7a7a\u95f4\u548c\u610f\u56fe\u7406\u89e3\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982Gemini-2.5 Pro\u548cGPT-5\uff09\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u5728\u6700\u56f0\u96be\u4efb\u52a1\u548c\u6700\u957f\u65f6\u95f4\u89c6\u9891\u4e0a\uff0c\u6700\u5f3a\u6a21\u578b\u4e5f\u4ec5\u8fbe\u5230\u7ea618%\u7684\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u65f6\u7a7a\u548c\u610f\u56fe\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "AccidentBench\u65e8\u5728\u66b4\u9732\u8fd9\u4e9b\u5173\u952e\u5dee\u8ddd\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u7a33\u5065\u3001\u66f4\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u5b89\u5168\u5173\u952e\u6311\u6218\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002"}}
{"id": "2509.26640", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.26640", "abs": "https://arxiv.org/abs/2509.26640", "authors": ["Jo\u00e3o Vitorino", "Eva Maia", "Isabel Pra\u00e7a", "Carlos Soares"], "title": "SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards", "comment": "16 pages, 3 tables, 6 figures, SynDAiTE, ECML PKDD 2025", "summary": "Due to the susceptibility of Artificial Intelligence (AI) to data\nperturbations and adversarial examples, it is crucial to perform a thorough\nrobustness evaluation before any Machine Learning (ML) model is deployed.\nHowever, examining a model's decision boundaries and identifying potential\nvulnerabilities typically requires access to the training and testing datasets,\nwhich may pose risks to data privacy and confidentiality. To improve\ntransparency in organizations that handle confidential data or manage critical\ninfrastructure, it is essential to allow external verification and validation\nof AI without the disclosure of private datasets. This paper presents\nSystematic Pattern Analysis (SPATA), a deterministic method that converts any\ntabular dataset to a domain-independent representation of its statistical\npatterns, to provide more detailed and transparent data cards. SPATA computes\nthe projection of each data instance into a discrete space where they can be\nanalyzed and compared, without risking data leakage. These projected datasets\ncan be reliably used for the evaluation of how different features affect ML\nmodel robustness and for the generation of interpretable explanations of their\nbehavior, contributing to more trustworthy AI.", "AI": {"tldr": "SPATA\u662f\u4e00\u79cd\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u53ef\u5c06\u8868\u683c\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u7edf\u8ba1\u6a21\u5f0f\u7684\u9886\u57df\u65e0\u5173\u8868\u793a\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u8be6\u7ec6\u900f\u660e\u7684\u6570\u636e\u5361\u7247\uff0c\u652f\u6301\u5728\u4e0d\u6cc4\u9732\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884cAI\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "motivation": "\u7531\u4e8eAI\u5bf9\u6570\u636e\u6270\u52a8\u548c\u5bf9\u6297\u6837\u672c\u7684\u654f\u611f\u6027\uff0c\u5728\u90e8\u7f72\u524d\u9700\u8981\u8fdb\u884c\u5f7b\u5e95\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u8fd9\u4f1a\u5e26\u6765\u6570\u636e\u9690\u79c1\u548c\u673a\u5bc6\u6027\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u5141\u8bb8\u5916\u90e8\u9a8c\u8bc1AI\u800c\u4e0d\u6cc4\u9732\u79c1\u6709\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u6027\u6a21\u5f0f\u5206\u6790(SPATA)\uff0c\u5c06\u6bcf\u4e2a\u6570\u636e\u5b9e\u4f8b\u6295\u5f71\u5230\u79bb\u6563\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5206\u6790\u548c\u6bd4\u8f83\uff0c\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u9886\u57df\u65e0\u5173\u7684\u7edf\u8ba1\u6a21\u5f0f\u8868\u793a\u3002", "result": "SPATA\u751f\u6210\u7684\u6295\u5f71\u6570\u636e\u96c6\u53ef\u4ee5\u53ef\u9760\u5730\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7279\u5f81\u5982\u4f55\u5f71\u54cdML\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u751f\u6210\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u8bf4\u660e\u3002", "conclusion": "SPATA\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8\u5904\u7406\u673a\u5bc6\u6570\u636e\u6216\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7ec4\u7ec7\u7684\u900f\u660e\u5ea6\uff0c\u4e3a\u66f4\u53ef\u4fe1\u7684AI\u505a\u51fa\u8d21\u732e\u3002"}}
