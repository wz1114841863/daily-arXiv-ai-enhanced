{"id": "2509.22951", "categories": ["cs.PF", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22951", "abs": "https://arxiv.org/abs/2509.22951", "authors": ["Jack Cashman", "Jiaqi Nie"], "title": "Tiny-QMoE", "comment": null, "summary": "The QMoE model provides a practical approach for compression of massive\nMixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory\nlimitations that often reach terabyte scales, and it has the advantage of\nworking with high sparsity models which implicitly lend themselves to\ncompression techniques. QMoE also has the advantage of only taking MoE models\ninto account and does not evaluate its use with non mixture of expert systems.\nAlthough this prior attempt focuses on the limitations of large servers with\nthe latest NVIDIA hardware which in the case of the H100 and V100 which have 80\nGB of HBM (High Bandwidth Memory), what is not being considered is a\nsignificantly more constrained environment, such as in the case of mobile\ndevices which may have in the case of the iPhone anywhere from 4 to 8 GB of\nunified memory which also needs to be shared with the operating system and\nadditional processes. Although edge devices such as phones and laptops are\nbecoming increasingly more computationally powerful, they are still not close\nto the level of advanced server machines such as NVIDIA. An additional\nconstraint that we must consider is that of latency. The communication time of\nsending a request to an LLM server and then getting it back is an additional\nwaiting time that can be removed. We may also want to use LLM technology in\nenvironments where there is no reliable network connection.", "AI": {"tldr": "QMoE\u6a21\u578b\u4e13\u6ce8\u4e8e\u538b\u7f29\u5927\u89c4\u6a21\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u4f46\u672a\u8003\u8651\u79fb\u52a8\u8bbe\u5907\u7b49\u53d7\u9650\u73af\u5883\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21MoE\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u6d88\u9664\u7f51\u7edc\u5ef6\u8fdf\u5e76\u652f\u6301\u79bb\u7ebf\u4f7f\u7528\u3002", "method": "\u672a\u660e\u786e\u8bf4\u660e\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u6307\u51faQMoE\u4ec5\u9488\u5bf9\u670d\u52a1\u5668\u73af\u5883\uff0c\u9700\u8981\u9488\u5bf9\u79fb\u52a8\u8bbe\u5907\u7684\u4f18\u5316\u65b9\u6848\u3002", "result": "\u6307\u51fa\u5f53\u524dQMoE\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5185\u5b58\u9650\u5236\u3001\u5ef6\u8fdf\u95ee\u9898\u548c\u7f51\u7edc\u4f9d\u8d56\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u79fb\u52a8\u8bbe\u5907\u7b49\u53d7\u9650\u73af\u5883\u7684MoE\u6a21\u578b\u538b\u7f29\u548c\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2509.25090", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2509.25090", "abs": "https://arxiv.org/abs/2509.25090", "authors": ["Rohan Basu Roy", "Vijay Gadepally", "Devesh Tiwari"], "title": "DarwinGame: Playing Tournaments for Tuning Applications in Noisy Cloud Environments", "comment": null, "summary": "This work introduces a new subarea of performance tuning -- performance\ntuning in a shared interference-prone computing environment. We demonstrate\nthat existing tuners are significantly suboptimal by design because of their\ninability to account for interference during tuning. Our solution, DarwinGame,\nemploys a tournament-based design to systematically compare application\nexecutions with different tunable parameter configurations, enabling it to\nidentify the relative performance of different tunable parameter configurations\nin a noisy environment. Compared to existing solutions, DarwinGame achieves\nmore than 27% reduction in execution time, with less than 0.5% performance\nvariability. DarwinGame is the first performance tuner that will help\ndevelopers tune their applications in shared, interference-prone, cloud\nenvironments.", "AI": {"tldr": "DarwinGame\u662f\u9996\u4e2a\u9488\u5bf9\u5171\u4eab\u5e72\u6270\u73af\u5883\u8bbe\u8ba1\u7684\u6027\u80fd\u8c03\u4f18\u5668\uff0c\u901a\u8fc7\u9526\u6807\u8d5b\u673a\u5236\u5728\u566a\u58f0\u73af\u5883\u4e2d\u6bd4\u8f83\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1127%\u6267\u884c\u65f6\u95f4\u4e14\u6027\u80fd\u53d8\u5f02\u5c0f\u4e8e0.5%\u3002", "motivation": "\u73b0\u6709\u8c03\u4f18\u5668\u5728\u8bbe\u8ba1\u4e0a\u65e0\u6cd5\u8003\u8651\u5171\u4eab\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u5e72\u6270\u56e0\u7d20\uff0c\u5bfc\u81f4\u5728\u5e72\u6270\u6613\u53d1\u7684\u5171\u4eab\u73af\u5883\u4e2d\u8868\u73b0\u663e\u8457\u6b21\u4f18\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9526\u6807\u8d5b\u7684\u8bbe\u8ba1\uff0c\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e0d\u540c\u53ef\u8c03\u53c2\u6570\u914d\u7f6e\u7684\u5e94\u7528\u7a0b\u5e8f\u6267\u884c\uff0c\u4ece\u800c\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8bc6\u522b\u4e0d\u540c\u914d\u7f6e\u7684\u76f8\u5bf9\u6027\u80fd\u3002", "result": "\u4e0e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0cDarwinGame\u5b9e\u73b0\u4e86\u8d85\u8fc727%\u7684\u6267\u884c\u65f6\u95f4\u51cf\u5c11\uff0c\u6027\u80fd\u53d8\u5f02\u5c0f\u4e8e0.5%\u3002", "conclusion": "DarwinGame\u662f\u9996\u4e2a\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u5171\u4eab\u3001\u5e72\u6270\u6613\u53d1\u7684\u4e91\u73af\u5883\u4e2d\u8c03\u4f18\u5e94\u7528\u7a0b\u5e8f\u7684\u6027\u80fd\u8c03\u4f18\u5668\u3002"}}
{"id": "2509.22684", "categories": ["cs.DC", "cs.AR", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.22684", "abs": "https://arxiv.org/abs/2509.22684", "authors": ["Tarunesh Verma", "Yichao Yuan", "Nishil Talati", "Todd Austin"], "title": "ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs", "comment": "To appear at 2025 IEEE International Symposium on Workload\n  Characterization", "summary": "Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic\nproofs to demonstrate knowledge of a secret input in a computation without\nrevealing any information about the secret. ZKPs enable novel applications in\nprivate and verifiable computing such as anonymized cryptocurrencies and\nblockchain scaling and have seen adoption in several real-world systems. Prior\nwork has accelerated ZKPs on GPUs by leveraging the inherent parallelism in\ncore computation kernels like Multi-Scalar Multiplication (MSM). However, we\nfind that a systematic characterization of execution bottlenecks in ZKPs, as\nwell as their scalability on modern GPU architectures, is missing in the\nliterature. This paper presents ZKProphet, a comprehensive performance study of\nZero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that\nZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they\naccount for up to 90% of the proof generation latency on GPUs when paired with\noptimized MSM implementations. Available NTT implementations under-utilize GPU\ncompute resources and often do not employ architectural features like\nasynchronous compute and memory operations. We observe that the arithmetic\noperations underlying ZKPs execute exclusively on the GPU's 32-bit integer\npipeline and exhibit limited instruction-level parallelism due to data\ndependencies. Their performance is thus limited by the available integer\ncompute units. While one way to scale the performance of ZKPs is adding more\ncompute units, we discuss how runtime parameter tuning for optimizations like\nprecomputed inputs and alternative data representations can extract additional\nspeedup. With this work, we provide the ZKP community a roadmap to scale\nperformance on GPUs and construct definitive GPU-accelerated ZKPs for their\napplication requirements and available hardware resources.", "AI": {"tldr": "ZKProphet\uff1a\u5bf9GPU\u4e0a\u96f6\u77e5\u8bc6\u8bc1\u660e\u6027\u80fd\u7684\u5168\u9762\u7814\u7a76\uff0c\u53d1\u73b0NTT\u5185\u6838\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u5360\u8bc1\u660e\u751f\u6210\u5ef6\u8fdf\u768490%\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u53c2\u6570\u8c03\u4f18\u548c\u67b6\u6784\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9ZKP\u6267\u884c\u74f6\u9888\u7684\u7cfb\u7edf\u6027\u5206\u6790\u53ca\u5176\u5728\u73b0\u4ee3GPU\u67b6\u6784\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728MSM\u52a0\u901f\u540e\uff0c\u5176\u4ed6\u5185\u6838\u5982NTT\u6210\u4e3a\u65b0\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6027\u80fd\u5206\u6790\uff0c\u8bc6\u522bZKP\u5728GPU\u4e0a\u7684\u6267\u884c\u74f6\u9888\uff0c\u7279\u522b\u662fNTT\u5185\u6838\u7684\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5f02\u6b65\u8ba1\u7b97\u3001\u5185\u5b58\u64cd\u4f5c\u7b49\u67b6\u6784\u7279\u6027\u7684\u5e94\u7528\u3002", "result": "\u53d1\u73b0ZKP\u7684\u7b97\u672f\u8fd0\u7b97\u4e3b\u8981\u5728GPU\u768432\u4f4d\u6574\u6570\u6d41\u6c34\u7ebf\u4e0a\u6267\u884c\uff0c\u53d7\u9650\u4e8e\u6570\u636e\u4f9d\u8d56\u5bfc\u81f4\u7684\u6307\u4ee4\u7ea7\u5e76\u884c\u6027\u6709\u9650\uff0c\u6027\u80fd\u53d7\u53ef\u7528\u6574\u6570\u8ba1\u7b97\u5355\u5143\u9650\u5236\u3002", "conclusion": "\u63d0\u51fa\u901a\u8fc7\u8fd0\u884c\u65f6\u53c2\u6570\u8c03\u4f18\uff08\u5982\u9884\u8ba1\u7b97\u8f93\u5165\u548c\u66ff\u4ee3\u6570\u636e\u8868\u793a\uff09\u6765\u63d0\u53d6\u989d\u5916\u52a0\u901f\uff0c\u4e3aZKP\u793e\u533a\u63d0\u4f9b\u5728GPU\u4e0a\u6269\u5c55\u6027\u80fd\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2509.23410", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.23410", "abs": "https://arxiv.org/abs/2509.23410", "authors": ["Younes Hourri", "Mohammad Mozaffari", "Maryam Mehri Dehnavi"], "title": "PATCH: Learnable Tile-level Hybrid Sparsity for LLMs", "comment": null, "summary": "Large language models (LLMs) deliver impressive performance but incur\nprohibitive memory and compute costs at deployment. Model pruning is an\neffective way to reduce these overheads, yet existing approaches face\nchallenges: unstructured sparsity, where nonzeros can appear anywhere,\npreserves accuracy but yields irregular access patterns that prevent GPU\nacceleration, while semi-structured 2:4 sparsity is hardware-friendly but\nenforces a rigid 50% pattern that degrades model quality. To bridge this gap,\nwe introduce PATCH, a hybrid sparsity framework that enables a continuous\nsparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,\nassigning each tile to be either dense or 2:4 sparse via a learnable mask\nselection mechanism. This design provides fine-grained control over\naccuracy-acceleration tradeoffs and supports non-uniform sparsity across\nlayers, leading to superior overall quality. Across models from 0.5B to 8B\nparameters, PATCH consistently narrows the gap to dense accuracy while\ndelivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,\nPATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while\nimproving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning\nmethod, MaskLLM.", "AI": {"tldr": "PATCH\u662f\u4e00\u79cd\u6df7\u5408\u7a00\u758f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u5212\u5206\u4e3a\u5757\uff0c\u8ba9\u6bcf\u4e2a\u5757\u9009\u62e9\u4e3a\u5bc6\u96c6\u62162:4\u7a00\u758f\uff0c\u5b9e\u73b00%-50%\u4e4b\u95f4\u7684\u8fde\u7eed\u7a00\u758f\u7387\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u5b9e\u9645\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u526a\u679d\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u975e\u7ed3\u6784\u5316\u7a00\u758f\u4fdd\u6301\u7cbe\u5ea6\u4f46\u65e0\u6cd5GPU\u52a0\u901f\uff0c\u800c2:4\u7a00\u758f\u786c\u4ef6\u53cb\u597d\u4f46\u56fa\u5b9a\u768450%\u6a21\u5f0f\u4f1a\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u7cbe\u5ea6\u4e0e\u52a0\u901f\u7684\u7075\u6d3b\u7a00\u758f\u65b9\u6cd5\u3002", "method": "\u5c06\u6743\u91cd\u77e9\u9635\u5212\u5206\u4e3a\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u9009\u62e9\u673a\u5236\u4e3a\u6bcf\u4e2a\u5757\u5206\u914d\u5bc6\u96c6\u62162:4\u7a00\u758f\u6a21\u5f0f\uff0c\u652f\u6301\u8de8\u5c42\u975e\u5747\u5300\u7a00\u758f\u548c\u7ec6\u7c92\u5ea6\u7cbe\u5ea6-\u52a0\u901f\u6743\u8861\u63a7\u5236\u3002", "result": "\u57280.5B\u52308B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cPATCH\u6301\u7eed\u7f29\u5c0f\u4e0e\u5bc6\u96c6\u6a21\u578b\u7684\u7cbe\u5ea6\u5dee\u8ddd\uff0c\u5728LLaMA-2 7B\u4e0a\u5b9e\u73b01.18x-1.38x\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u76842:4\u526a\u679d\u65b9\u6cd5MaskLLM\u7cbe\u5ea6\u63d0\u53470.37%-2.96%\u3002", "conclusion": "PATCH\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86\u7cbe\u5ea6\u4fdd\u6301\u4e0e\u786c\u4ef6\u52a0\u901f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7a00\u758f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22680", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22680", "abs": "https://arxiv.org/abs/2509.22680", "authors": ["Paul Churnock"], "title": "Cognition Engines: A Row-Scale HVDC Architecture for Computational Continuity of AI", "comment": "17 pages, 2 figures", "summary": "AI training creates synchronized, step-dominant surges with millisecond edges\nthat destabilize constant-power loads (Choukse et al., 2025; arXiv:2508.14318).\nWe propose a physics-anchored row-scale $\\pm 400$ Vdc architecture that makes\nComputational Continuity a structural property. DRUs supply fast energy via\ncontrolled droop; SSTs regulate average power with bounded ramps and no reverse\npower flow and no high-frequency export at the PCC; import is subjected to a\nbounded dP/dt envelope; film capacitance and clamps absorb the first edge. The\ncontract is explicit: $\\pm 1\\%$ steady-band, $\\leq 2\\%$ transient deviation,\n$\\leq 3$ ms recovery, $\\geq 45^{\\circ}$ margin, reserve floors intact, yields\nspine and lowest branches. Recharge is valley-following (admitted only below\nAvg with MW headroom; $\\leq 5$ kW/s per row ramps). Protection is time-graded\n(branch $\\mu$s, row ms, MW seconds). Scaling preserves invariants from row to\npod/hall/campus without retuning. Conformance is by waveform evidence\n(microsecond branch clears, $2\\%/50$ ms holds, FLISR with no reverse power flow\nand no high-frequency export at the PCC). The result is not tuning but a\ncontract for continuity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u00b1400Vdc\u67b6\u6784\uff0c\u901a\u8fc7DRU\u5feb\u901f\u4f9b\u80fd\u3001SST\u8c03\u8282\u5e73\u5747\u529f\u7387\u3001\u8584\u819c\u7535\u5bb9\u5438\u6536\u77ac\u6001\uff0c\u5b9e\u73b0AI\u8bad\u7ec3\u8d1f\u8f7d\u7684\u7a33\u5b9a\u4f9b\u7535\uff0c\u786e\u4fdd\u8ba1\u7b97\u8fde\u7eed\u6027\u3002", "motivation": "AI\u8bad\u7ec3\u4ea7\u751f\u6beb\u79d2\u7ea7\u77ac\u6001\u529f\u7387\u51b2\u51fb\uff0c\u7834\u574f\u6052\u529f\u7387\u8d1f\u8f7d\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u65b0\u7684\u4f9b\u7535\u67b6\u6784\u6765\u4fdd\u8bc1\u8ba1\u7b97\u8fde\u7eed\u6027\u3002", "method": "\u91c7\u7528\u00b1400Vdc\u67b6\u6784\uff0cDRU\u63d0\u4f9b\u5feb\u901f\u80fd\u91cf\uff0cSST\u8c03\u8282\u5e73\u5747\u529f\u7387\uff0c\u8584\u819c\u7535\u5bb9\u548c\u94b3\u4f4d\u7535\u8def\u5438\u6536\u77ac\u6001\uff0c\u4fdd\u62a4\u7cfb\u7edf\u5206\u7ea7\u8bbe\u8ba1\uff0c\u5145\u7535\u91c7\u7528\u8c37\u503c\u8ddf\u968f\u7b56\u7565\u3002", "result": "\u5b9e\u73b0\u00b11%\u7a33\u6001\u5e26\u5bbd\u3001\u22642%\u77ac\u6001\u504f\u5dee\u3001\u22643ms\u6062\u590d\u65f6\u95f4\u3001\u226545\u00b0\u88d5\u5ea6\uff0c\u4fdd\u62a4\u50a8\u5907\u5b8c\u597d\uff0c\u4ece\u884c\u7ea7\u5230\u56ed\u533a\u7ea7\u6269\u5c55\u65e0\u9700\u91cd\u65b0\u8c03\u8c10\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e0d\u662f\u8c03\u8c10\u4f18\u5316\uff0c\u800c\u662f\u901a\u8fc7\u660e\u786e\u5951\u7ea6\u786e\u4fdd\u8ba1\u7b97\u8fde\u7eed\u6027\uff0c\u6ce2\u5f62\u8bc1\u636e\u9a8c\u8bc1\u7cfb\u7edf\u7b26\u5408\u6027\u3002"}}
{"id": "2509.22710", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.22710", "abs": "https://arxiv.org/abs/2509.22710", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise", "comment": "Published, CC BY-NC 4.0; includes 2 figures and 1 table;\n  InceptionV3/ImageNet evaluation", "summary": "Adversarial attacks in machine learning traditionally focus on global\nperturbations to input data, yet the potential of localized adversarial noise\nremains underexplored. This study systematically evaluates localized\nadversarial attacks across widely-used methods, including FGSM, PGD, and C&W,\nto quantify their effectiveness, imperceptibility, and computational\nefficiency. By introducing a binary mask to constrain noise to specific\nregions, localized attacks achieve significantly lower mean pixel\nperturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved\nStructural Similarity Index (SSIM) compared to global attacks. However, these\nbenefits come at the cost of increased computational effort and a modest\nreduction in Attack Success Rate (ASR). Our results highlight that iterative\nmethods, such as PGD and C&W, are more robust to localization constraints than\nsingle-step methods like FGSM, maintaining higher ASR and imperceptibility\nmetrics. This work provides a comprehensive analysis of localized adversarial\nattacks, offering practical insights for advancing attack strategies and\ndesigning robust defensive systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u6548\u679c\uff0c\u53d1\u73b0\u76f8\u6bd4\u5168\u5c40\u653b\u51fb\uff0c\u5c40\u90e8\u653b\u51fb\u5177\u6709\u66f4\u4f4e\u7684\u50cf\u7d20\u6270\u52a8\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u4e14\u653b\u51fb\u6210\u529f\u7387\u7565\u6709\u4e0b\u964d\u3002\u8fed\u4ee3\u65b9\u6cd5\u6bd4\u5355\u6b65\u65b9\u6cd5\u5bf9\u5c40\u90e8\u5316\u7ea6\u675f\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u6270\u52a8\uff0c\u800c\u5c40\u90e8\u5bf9\u6297\u566a\u58f0\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u6709\u6548\u6027\u3001\u4e0d\u53ef\u611f\u77e5\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e8c\u5143\u63a9\u7801\u5c06\u566a\u58f0\u9650\u5236\u5728\u7279\u5b9a\u533a\u57df\uff0c\u5728FGSM\u3001PGD\u548cC&W\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b9\u6cd5\u4e0a\u8bc4\u4f30\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\uff0c\u91cf\u5316\u5176\u6548\u679c\u3001\u4e0d\u53ef\u611f\u77e5\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5c40\u90e8\u653b\u51fb\u76f8\u6bd4\u5168\u5c40\u653b\u51fb\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u5e73\u5747\u50cf\u7d20\u6270\u52a8\u3001\u66f4\u9ad8\u7684\u5cf0\u503c\u4fe1\u566a\u6bd4\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u3002\u4f46\u4ee3\u4ef7\u662f\u8ba1\u7b97\u91cf\u589e\u52a0\u548c\u653b\u51fb\u6210\u529f\u7387\u7565\u6709\u4e0b\u964d\u3002\u8fed\u4ee3\u65b9\u6cd5\u6bd4\u5355\u6b65\u65b9\u6cd5\u5bf9\u5c40\u90e8\u5316\u7ea6\u675f\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u5168\u9762\u5206\u6790\uff0c\u4e3a\u63a8\u8fdb\u653b\u51fb\u7b56\u7565\u548c\u8bbe\u8ba1\u9c81\u68d2\u9632\u5fa1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2509.22679", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22679", "abs": "https://arxiv.org/abs/2509.22679", "authors": ["Abdessalam Benhari", "Yves Denneulin", "Fr\u00e9d\u00e9ric Desprez", "Fanny Dufoss\u00e9", "Denis Trystram"], "title": "Analysis of the carbon footprint of HPC", "comment": null, "summary": "The demand in computing power has never stopped growing over the years.\nToday, the performance of the most powerful systems exceeds the exascale.\nUnfortunately, this growth also comes with ever-increasing energy costs,\nleading to a high carbon footprint. This paper investigates the evolution of\nhigh performance systems in terms of carbon emissions. A lot of studies focus\non Top500 (and Green500) as the tip of an iceberg to identify trends in the\ndomain in terms of computing performance. We propose here to go further in\nconsidering the whole span life of several large scale systems and to link the\nevolution with trajectory toward 2030. More precisely, we introduce the energy\nmix in the analysis of Top500 systems and we derive a predictive model for\nestimating the weight of HPC for the next 5 years.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u78b3\u6392\u653e\u6f14\u53d8\uff0c\u901a\u8fc7\u5f15\u5165\u80fd\u6e90\u7ed3\u6784\u5206\u6790Top500\u7cfb\u7edf\uff0c\u5e76\u5efa\u7acb\u4e86\u9884\u6d4b\u6a21\u578b\u6765\u4f30\u8ba1\u672a\u67655\u5e74HPC\u7684\u78b3\u8db3\u8ff9\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u6301\u7eed\u589e\u957f\uff0c\u4f46\u4f34\u968f\u800c\u6765\u7684\u80fd\u6e90\u6210\u672c\u548c\u78b3\u8db3\u8ff9\u4e5f\u5728\u4e0d\u65ad\u589e\u52a0\uff0c\u9700\u8981\u7814\u7a76\u5176\u78b3\u6392\u653e\u6f14\u53d8\u8d8b\u52bf\u3002", "method": "\u8003\u8651\u591a\u4e2a\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u5168\u751f\u547d\u5468\u671f\uff0c\u5f15\u5165\u80fd\u6e90\u7ed3\u6784\u5206\u6790Top500\u7cfb\u7edf\uff0c\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u6765\u4f30\u8ba1\u672a\u67655\u5e74HPC\u7684\u78b3\u8db3\u8ff9\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u4f30\u8ba1\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u5728\u672a\u67655\u5e74\u7684\u78b3\u6392\u653e\u91cf\u3002", "conclusion": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u5173\u6ce8\u5176\u5168\u751f\u547d\u5468\u671f\u78b3\u8db3\u8ff9\uff0c\u5e76\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u6765\u6307\u5bfc\u672a\u6765\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2509.22980", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22980", "abs": "https://arxiv.org/abs/2509.22980", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "\\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory", "comment": null, "summary": "Processing-in-Memory (PIM) is a promising approach to overcoming the\nmemory-wall bottleneck. However, the PIM community has largely treated its two\nfundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they\nwere interchangeable. This implicit \"one-layout-fits-all\" assumption, often\nhard-coded into existing evaluation frameworks, creates a critical gap:\narchitects lack systematic, workload-driven guidelines for choosing the optimal\ndata layout for their target applications.\n  To address this gap, this paper presents the first systematic,\nworkload-driven characterization of BP and BS PIM architectures. We develop\niso-area, cycle-accurate BP and BS PIM architectural models and conduct a\ncomprehensive evaluation using a diverse set of benchmarks. Our suite includes\nboth fine-grained microworkloads from MIMDRAM to isolate specific operational\ncharacteristics, and large-scale applications from the PIMBench suite, such as\nthe VGG network, to represent realistic end-to-end workloads.\n  Our results quantitatively demonstrate that no single layout is universally\nsuperior; the optimal choice is strongly dependent on workload characteristics.\nBP excels on control-flow-intensive tasks with irregular memory access\npatterns, whereas BS shows substantial advantages in massively parallel,\nlow-precision (e.g., INT4/INT8) computations common in AI. Based on this\ncharacterization, we distill a set of actionable design guidelines for\narchitects. This work challenges the prevailing one-size-fits-all view on PIM\ndata layouts and provides a principled foundation for designing\nnext-generation, workload-aware, and potentially hybrid PIM systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86PIM\u67b6\u6784\u4e2d\u4e24\u79cd\u57fa\u672c\u6570\u636e\u5e03\u5c40\uff08\u4f4d\u5e76\u884cBP\u548c\u4f4d\u4e32\u884cBS\uff09\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u5e03\u5c40\u9002\u7528\u4e8e\u6240\u6709\u573a\u666f\uff0c\u6700\u4f18\u9009\u62e9\u53d6\u51b3\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u3002", "motivation": "PIM\u793e\u533a\u957f\u671f\u4ee5\u6765\u5c06BP\u548cBS\u5e03\u5c40\u89c6\u4e3a\u53ef\u4e92\u6362\u7684\uff0c\u8fd9\u79cd\"\u4e00\u5200\u5207\"\u7684\u5047\u8bbe\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9a71\u52a8\u6307\u5bfc\uff0c\u5bfc\u81f4\u67b6\u6784\u5e08\u65e0\u6cd5\u4e3a\u76ee\u6807\u5e94\u7528\u9009\u62e9\u6700\u4f18\u6570\u636e\u5e03\u5c40\u3002", "method": "\u5f00\u53d1\u4e86\u7b49\u9762\u79ef\u3001\u5468\u671f\u7cbe\u786e\u7684BP\u548cBS PIM\u67b6\u6784\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5305\u62ecMIMDRAM\u7684\u7ec6\u7c92\u5ea6\u5fae\u5de5\u4f5c\u8d1f\u8f7d\u548cPIMBench\u7684\u5927\u89c4\u6a21\u5e94\u7528\uff08\u5982VGG\u7f51\u7edc\uff09\u3002", "result": "BP\u5728\u63a7\u5236\u6d41\u5bc6\u96c6\u3001\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e0d\u89c4\u5219\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800cBS\u5728AI\u4e2d\u5e38\u89c1\u7684\u5927\u89c4\u6a21\u5e76\u884c\u3001\u4f4e\u7cbe\u5ea6\uff08\u5982INT4/INT8\uff09\u8ba1\u7b97\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u6311\u6218\u4e86PIM\u6570\u636e\u5e03\u5c40\u7684\"\u4e00\u5200\u5207\"\u89c2\u70b9\uff0c\u4e3a\u8bbe\u8ba1\u4e0b\u4e00\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u548c\u6f5c\u5728\u6df7\u5408PIM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5e76\u63d0\u70bc\u51fa\u4e00\u5957\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2509.24063", "categories": ["cs.DC", "cs.CE", "cs.MA", "cs.PF", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24063", "abs": "https://arxiv.org/abs/2509.24063", "authors": ["Lukas Breitwieser", "Ahmad Hesam", "Abdullah Giray Ya\u011fl\u0131k\u00e7\u0131", "Mohammad Sadrosadati", "Fons Rademakers", "Onur Mutlu"], "title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents", "comment": null, "summary": "Agent-based simulation is an indispensable paradigm for studying complex\nsystems. These systems can comprise billions of agents, requiring the computing\nresources of multiple servers to simulate. Unfortunately, the state-of-the-art\nplatform, BioDynaMo, does not scale out across servers due to its\nshared-memory-based implementation.\n  To overcome this key limitation, we introduce TeraAgent, a distributed\nagent-based simulation engine. A critical challenge in distributed execution is\nthe exchange of agent information across servers, which we identify as a major\nperformance bottleneck. We propose two solutions: 1) a tailored serialization\nmechanism that allows agents to be accessed and mutated directly from the\nreceive buffer, and 2) leveraging the iterative nature of agent-based\nsimulations to reduce data transfer with delta encoding.\n  Built on our solutions, TeraAgent enables extreme-scale simulations with half\na trillion agents (an 84x improvement), reduces time-to-result with additional\ncompute nodes, improves interoperability with third-party tools, and provides\nusers with more hardware flexibility.", "AI": {"tldr": "TeraAgent\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u4ee3\u7406\u4eff\u771f\u5f15\u64ce\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u5e8f\u5217\u5316\u673a\u5236\u548c\u589e\u91cf\u7f16\u7801\u89e3\u51b3\u4e86\u8de8\u670d\u52a1\u5668\u4ee3\u7406\u4fe1\u606f\u4ea4\u6362\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u4e07\u4ebf\u7ea7\u4ee3\u7406\u7684\u4eff\u771f\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684BioDynaMo\u5e73\u53f0\u7531\u4e8e\u57fa\u4e8e\u5171\u4eab\u5185\u5b58\u5b9e\u73b0\uff0c\u65e0\u6cd5\u8de8\u670d\u52a1\u5668\u6269\u5c55\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u590d\u6742\u7cfb\u7edf\u7684\u4eff\u771f\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u5b9a\u5236\u5e8f\u5217\u5316\u673a\u5236\uff0c\u5141\u8bb8\u76f4\u63a5\u4ece\u63a5\u6536\u7f13\u51b2\u533a\u8bbf\u95ee\u548c\u4fee\u6539\u4ee3\u7406\uff1b2\uff09\u5229\u7528\u4ee3\u7406\u4eff\u771f\u7684\u8fed\u4ee3\u7279\u6027\uff0c\u901a\u8fc7\u589e\u91cf\u7f16\u7801\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u3002", "result": "TeraAgent\u5b9e\u73b0\u4e865000\u4ebf\u4ee3\u7406\u7684\u4eff\u771f\uff0884\u500d\u63d0\u5347\uff09\uff0c\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u8282\u70b9\u51cf\u5c11\u7ed3\u679c\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86\u4e0e\u7b2c\u4e09\u65b9\u5de5\u5177\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u591a\u786c\u4ef6\u7075\u6d3b\u6027\u3002", "conclusion": "TeraAgent\u514b\u670d\u4e86\u73b0\u6709\u5e73\u53f0\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u6781\u7aef\u89c4\u6a21\u7684\u5206\u5e03\u5f0f\u4ee3\u7406\u4eff\u771f\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8ba1\u7b97\u5e73\u53f0\u3002"}}
{"id": "2509.22767", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.22767", "abs": "https://arxiv.org/abs/2509.22767", "authors": ["Christoph Weilenmann", "Hanglin He", "Marko Mladenovi\u0107", "Till Zellweger", "Kevin Portner", "Klemens Bauer", "Guillaume Bellec", "Mathieu Luisier", "Alexandros Emboras"], "title": "Conductance-dependent Photoresponse in a Dynamic SrTiO3 Memristor for Biorealistic Computing", "comment": null, "summary": "Modern computers perform pre-defined operations using static memory\ncomponents, whereas biological systems learn through inherently dynamic,\ntime-dependent processes in synapses and neurons. The biological learning\nprocess also relies on global signals - neuromodulators - who influence many\nsynapses at once depending on their dynamic, internal state. In this study,\nusing optical radiation as a global neuromodulatory signal, we investigate\nnanoscale SrTiO3 (STO) memristors that can act as solid-state synapses. Via\ndiverse sets of measurements, we demonstrate that the memristor's photoresponse\ndepends on the electrical conductance state, following a well-defined square\nroot relation. Additionally, we show that the conductance decays after\nphotoexcitation with time constants in the range of 1 - 10 s and that this\neffect can be reliably controlled using an electrical bias. These properties in\ncombination with our device's low power operation (< 1pJ per optical pulse) and\nsmall measurement variability may pave the way for space- and energy-efficient\nimplementations of complex biological learning processes in electro-optical\nhardware.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5149\u8f90\u5c04\u4f5c\u4e3a\u5168\u5c40\u795e\u7ecf\u8c03\u8282\u4fe1\u53f7\uff0c\u63a2\u7d22\u4e86\u7eb3\u7c73\u7ea7SrTiO3\u5fc6\u963b\u5668\u4f5c\u4e3a\u56fa\u6001\u7a81\u89e6\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5149\u7535\u54cd\u5e94\u4e0e\u7535\u5bfc\u72b6\u6001\u7684\u5173\u7cfb\u4ee5\u53ca\u53ef\u63a7\u7684\u8870\u51cf\u7279\u6027\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u673a\u4f7f\u7528\u9759\u6001\u5185\u5b58\u7ec4\u4ef6\u6267\u884c\u9884\u5b9a\u4e49\u64cd\u4f5c\uff0c\u800c\u751f\u7269\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u7684\u3001\u65f6\u95f4\u4f9d\u8d56\u7684\u7a81\u89e6\u548c\u795e\u7ecf\u5143\u8fc7\u7a0b\u5b66\u4e60\u3002\u751f\u7269\u5b66\u4e60\u8fc7\u7a0b\u8fd8\u4f9d\u8d56\u4e8e\u5168\u5c40\u4fe1\u53f7\uff08\u795e\u7ecf\u8c03\u8282\u5242\uff09\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u6839\u636e\u52a8\u6001\u5185\u90e8\u72b6\u6001\u5f71\u54cd\u591a\u4e2a\u7a81\u89e6\u3002", "method": "\u4f7f\u7528\u5149\u5b66\u8f90\u5c04\u4f5c\u4e3a\u5168\u5c40\u795e\u7ecf\u8c03\u8282\u4fe1\u53f7\uff0c\u7814\u7a76\u7eb3\u7c73\u7ea7SrTiO3\u5fc6\u963b\u5668\u4f5c\u4e3a\u56fa\u6001\u7a81\u89e6\u7684\u529f\u80fd\u3002\u901a\u8fc7\u591a\u79cd\u6d4b\u91cf\u65b9\u6cd5\u5206\u6790\u5fc6\u963b\u5668\u7684\u5149\u7535\u54cd\u5e94\u7279\u6027\u3002", "result": "\u5fc6\u963b\u5668\u7684\u5149\u7535\u54cd\u5e94\u53d6\u51b3\u4e8e\u7535\u5bfc\u72b6\u6001\uff0c\u9075\u5faa\u660e\u786e\u7684\u5e73\u65b9\u6839\u5173\u7cfb\u3002\u7535\u5bfc\u5728\u5149\u6fc0\u53d1\u540e\u4ee51-10\u79d2\u7684\u65f6\u95f4\u5e38\u6570\u8870\u51cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u7535\u504f\u538b\u53ef\u9760\u63a7\u5236\u3002\u5668\u4ef6\u5177\u6709\u4f4e\u529f\u8017\uff08<1pJ/\u5149\u8109\u51b2\uff09\u548c\u5c0f\u6d4b\u91cf\u53d8\u5f02\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7279\u6027\u7ed3\u5408\u4f4e\u529f\u8017\u548c\u5c0f\u53d8\u5f02\u6027\uff0c\u53ef\u80fd\u4e3a\u5728\u7535\u5149\u786c\u4ef6\u4e2d\u5b9e\u73b0\u590d\u6742\u751f\u7269\u5b66\u4e60\u8fc7\u7a0b\u7684\u8282\u80fd\u9ad8\u6548\u5b9e\u73b0\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.22764", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22764", "abs": "https://arxiv.org/abs/2509.22764", "authors": ["Liuwang Kang", "Fan Wang", "Shaoshan Liu", "Hung-Chyun Chou", "Chuan Lin", "Ning Ding"], "title": "In-Context Learning can Perform Continual Learning Like Humans", "comment": null, "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning\n(ICL) without parameter updates, making them powerful learning engines for fast\nadaptation. While extensive research has examined ICL as a few-shot learner,\nwhether it can achieve long-term retention and cross-task knowledge\naccumulation when multitasks arrive sequentially remains underexplored.\nMotivated by human memory studies, we investigate the retention characteristics\nof ICL in multitask settings and extend it to in-context continual learning\n(ICCL), where continual learning ability emerges through task scheduling and\nprompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,\nfor specific large-language models, ICCL benefits from distributed practice\n(DP) in a manner analogous to humans, consistently revealing a spacing \"sweet\nspot\" for retention. Beyond retention performance, we propose a human-retention\nsimilarity metric to quantify how closely a continual-learning (CL) method\naligns with human retention dynamics. Using this metric, we show that\nlinear-attention models such as MAMBA and RWKV exhibit particularly human-like\nretention patterns, despite their retention performance lagging behind that of\nTransformer-based LLMs. Overall, our results establish ICCL as both cognitively\nplausible and practically effective, providing an inference-only CL paradigm\nthat mitigates catastrophic forgetting and addresses the stability-plasticity\ndilemma in conventional CL methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u60c5\u5883\u6301\u7eed\u5b66\u4e60(ICCL)\uff0c\u901a\u8fc7\u4efb\u52a1\u8c03\u5ea6\u548c\u63d0\u793a\u91cd\u6392\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4fdd\u7559\u7279\u6027\u4e0e\u4eba\u7c7b\u5206\u5e03\u5f0f\u7ec3\u4e60\u76f8\u4f3c\uff0c\u5e76\u63d0\u51fa\u4e86\u8861\u91cf\u5b66\u4e60\u7b97\u6cd5\u4e0e\u4eba\u7c7b\u4fdd\u7559\u76f8\u4f3c\u5ea6\u7684\u6307\u6807\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u957f\u671f\u4fdd\u7559\u548c\u8de8\u4efb\u52a1\u77e5\u8bc6\u79ef\u7d2f\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u5206\u5e03\u5f0f\u7ec3\u4e60\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u60c5\u5883\u6301\u7eed\u5b66\u4e60(ICCL)\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u8c03\u5ea6\u548c\u63d0\u793a\u91cd\u6392\u5b9e\u73b0\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u5728\u9a6c\u5c14\u53ef\u592b\u94fe\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86\u4eba\u7c7b\u4fdd\u7559\u76f8\u4f3c\u5ea6\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660eICCL\u53d7\u76ca\u4e8e\u5206\u5e03\u5f0f\u7ec3\u4e60\uff0c\u5b58\u5728\u4fdd\u7559\u7684\"\u751c\u871c\u70b9\"\uff1b\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u5982MAMBA\u548cRWKV\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u4fdd\u7559\u6a21\u5f0f\uff0c\u5c3d\u7ba1\u5176\u4fdd\u7559\u6027\u80fd\u4e0d\u5982\u57fa\u4e8eTransformer\u7684LLMs\u3002", "conclusion": "ICCL\u65e2\u5177\u6709\u8ba4\u77e5\u5408\u7406\u6027\u53c8\u5b9e\u7528\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ec5\u901a\u8fc7\u63a8\u7406\u7684\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u5e76\u89e3\u51b3\u4e86\u4f20\u7edfCL\u65b9\u6cd5\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002"}}
{"id": "2509.22681", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22681", "abs": "https://arxiv.org/abs/2509.22681", "authors": ["Xianwen Guo", "Bin Huang", "Xiaomeng Wu", "Guanlin Wu", "Fangjian Li", "Shijia Wang", "Qiang Xiao", "Chuanjiang Luo", "Yong Li"], "title": "FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency", "comment": null, "summary": "Generative recommendation (GR) models possess greater scaling power compared\nto traditional deep learning recommendation models (DLRMs), yet they also\nimpose a tremendous increase in computational burden. Measured in FLOPs, a\ntypical GR model's workload sits in $10^9 \\sim 10^{11}$ range, roughly four\norders of magnitude higher than traditional DLRMs. Delivering accurate results\nin a few tens of milliseconds while processing billions of such requests per\nday puts extreme demands on the performance of the online serving system.\nTherefore, for industry practitioners, the alluring gains of GR models are\ntempered by the formidable challenge of online deployment at scale in\nproduction services. In this work, we introduce a comprehensive solution of\nonline serving system tailored For Large-scale GenerAtive RecoMmendation with\nEfficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware\nto decouple feature pre-processing and model computation. We encapsulated\nseveral memory optimization features as the Proximal Data Accelerator (PDA)\nmodule to make full use of limited bandwidth and storage resources, which\nachieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the\nFused Kernel Engine (FKE) module based on the functionality and interface of\nNVIDIA TensorRT to boost model computation, delivering a speedup ratio of\n4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we\ndesign the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent\nrequests, enhancing the system throughput performance with 1.3x improvement in\nthroughput and 2.3x speed-up under non-uniform distribution of upstream\ncandidates. Comprehensive evaluations demonstrate that our FLAME effectively\nsupports large-scale online deployment of GR models and achieves remarkable\nimprovements in system performance.", "AI": {"tldr": "FLAME\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5728\u7ebf\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7CPU-GPU\u5f02\u6784\u786c\u4ef6\u89e3\u8026\u7279\u5f81\u9884\u5904\u7406\u548c\u6a21\u578b\u8ba1\u7b97\uff0c\u5305\u542bPDA\u3001FKE\u548cDSO\u4e09\u4e2a\u4f18\u5316\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u63a8\u8350\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u6269\u5c55\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u8d1f\u62c5\u589e\u52a0\u4e864\u4e2a\u6570\u91cf\u7ea7\uff0c\u5728\u4fdd\u6301\u6beb\u79d2\u7ea7\u54cd\u5e94\u65f6\u95f4\u7684\u540c\u65f6\u5904\u7406\u6570\u5341\u4ebf\u8bf7\u6c42\u5bf9\u5728\u7ebf\u670d\u52a1\u7cfb\u7edf\u63d0\u51fa\u4e86\u6781\u9ad8\u8981\u6c42\u3002", "method": "\u4f7f\u7528CPU-GPU\u5f02\u6784\u786c\u4ef6\u67b6\u6784\uff0c\u5f00\u53d1\u4e86\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aPDA\uff08\u8fd1\u7aef\u6570\u636e\u52a0\u901f\u5668\uff09\u4f18\u5316\u5185\u5b58\u4f7f\u7528\uff0cFKE\uff08\u878d\u5408\u5185\u6838\u5f15\u64ce\uff09\u57fa\u4e8eTensorRT\u52a0\u901f\u6a21\u578b\u8ba1\u7b97\uff0cDSO\uff08\u52a8\u6001\u6d41\u7f16\u6392\u5668\uff09\u534f\u8c03\u5e76\u53d1\u8bf7\u6c42\u3002", "result": "PDA\u6a21\u5757\u5b9e\u73b01.9\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c1.7\u500d\u5ef6\u8fdf\u964d\u4f4e\uff1bFKE\u6a21\u5757\u5e26\u67654.6-6.1\u500d\u52a0\u901f\u6bd4\u548c4.7-6.3\u500d\u541e\u5410\u91cf\u63d0\u5347\uff1bDSO\u6a21\u5757\u5728\u975e\u5747\u5300\u5206\u5e03\u4e0b\u5b9e\u73b01.3\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c2.3\u500d\u52a0\u901f\u3002", "conclusion": "FLAME\u7cfb\u7edf\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5728\u7ebf\u90e8\u7f72\uff0c\u5728\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.22999", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22999", "abs": "https://arxiv.org/abs/2509.22999", "authors": ["Sachin Sachdeva", "Jincong Lu", "Wantong Li", "Sheldon X. -D. Tan"], "title": "Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators", "comment": "8 pages", "summary": "This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)\nframework for ultra-low-power hardware accelerators with deterministic\nadditions. Inspired by the recently proposed HTC architecture, which leverages\npulse-rate and temporal data encoding to reduce switching activity and energy\nconsumption but loses accuracy due to its multiplexer (MUX)-based scaled\naddition, we propose two bitstream addition schemes: (1) an Exact\nMultiple-input Binary Accumulator (EMBA), which performs precise binary\naccumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),\nwhich employs threshold logic for scaled addition. These adders are integrated\ninto a multiplier accumulator (MAC) unit supporting both unipolar and bipolar\nencodings. To validate the framework, we implement two accelerators: a Finite\nImpulse Response (FIR) filter and an 8-point Discrete Cosine Transform\n(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC\nmatches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)\nMAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by\n23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In\nbipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over\nMUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings\nof 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,\nboth E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while\nsaving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB\n(70--75% RMSE reduction) while saving area and power over both MUX- and\nCBSC-based designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7cbe\u5ea6\u589e\u5f3a\u7684\u6df7\u5408\u65f6\u5e8f\u8ba1\u7b97\u6846\u67b6E-HTC\uff0c\u901a\u8fc7EMBA\u7cbe\u786e\u4e8c\u8fdb\u5236\u7d2f\u52a0\u5668\u548cDTSA\u786e\u5b9a\u6027\u9608\u503c\u7f29\u653e\u52a0\u6cd5\u5668\uff0c\u5728\u4fdd\u6301\u4f4e\u529f\u8017\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684HTC\u67b6\u6784\u867d\u7136\u901a\u8fc7\u8109\u51b2\u7387\u548c\u65f6\u5e8f\u6570\u636e\u7f16\u7801\u964d\u4f4e\u4e86\u529f\u8017\uff0c\u4f46\u57fa\u4e8eMUX\u7684\u7f29\u653e\u52a0\u6cd5\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\uff0c\u9700\u8981\u6539\u8fdb\u52a0\u6cd5\u65b9\u6848\u6765\u5e73\u8861\u7cbe\u5ea6\u548c\u529f\u8017\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4f4d\u6d41\u52a0\u6cd5\u65b9\u6848\uff1aEMBA\uff08\u7cbe\u786e\u591a\u8f93\u5165\u4e8c\u8fdb\u5236\u7d2f\u52a0\u5668\uff09\u548cDTSA\uff08\u786e\u5b9a\u6027\u9608\u503c\u7f29\u653e\u52a0\u6cd5\u5668\uff09\uff0c\u96c6\u6210\u5230\u652f\u6301\u5355\u6781\u548c\u53cc\u6781\u7f16\u7801\u7684MAC\u5355\u5143\u4e2d\u3002", "result": "\u57284x4 MAC\u4e2d\uff0c\u5355\u6781\u6a21\u5f0f\u4e0bE-HTC\u4e0eCBSC\u7684RMSE\u76f8\u5f53\uff0c\u6bd4MUX-HTC\u7cbe\u5ea6\u63d0\u9ad894%\uff0c\u529f\u8017\u548c\u9762\u79ef\u5206\u522b\u51cf\u5c1123%\u548c7%\uff1b\u53cc\u6781\u6a21\u5f0f\u4e0bRMSE\u4e3a2.09%\uff0c\u6bd4MUX-HTC\u63d0\u9ad883%\u3002\u5728FIR\u548cDCT/iDCT\u5e94\u7528\u4e2d\u5747\u83b7\u5f97\u663e\u8457\u7cbe\u5ea6\u63d0\u5347\u548c\u529f\u8017\u8282\u7701\u3002", "conclusion": "E-HTC\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u529f\u8017\u7684\u826f\u597d\u5e73\u8861\uff0c\u5728\u4fdd\u6301\u8d85\u4f4e\u529f\u8017\u7279\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u4e3a\u786c\u4ef6\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u8bbe\u8ba1\u65b9\u6848\u3002"}}
{"id": "2509.23463", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23463", "abs": "https://arxiv.org/abs/2509.23463", "authors": ["Xiaoke Wang", "Dirk Stroobandt"], "title": "Length-Matching Routing for Programmable Photonic Circuits Using Best-First Strategy", "comment": null, "summary": "In the realm of programmable photonic integrated circuits (PICs), precise\nwire length control is crucial for the performance of on-chip programmable\ncomponents such as optical ring resonators, Mach-Zehnder interferometers, and\noptical true time-delay lines. Unlike conventional routing algorithms that\nprioritize shortest-path solutions, these photonic components require\nexact-length routing to maintain the desired optical properties.\n  To address these challenges, this paper presents different length-matching\nrouting strategies to find exact-length paths while balancing search space and\nruntime efficiently. We propose a novel admissible heuristic estimator and a\npruning method, designed to enhance the accuracy and efficiency of the search\nprocess. The algorithms are derived from the Best-First search with modified\nevaluation functions. For two-pin length-matching routing, we formally prove\nthat the proposed algorithms are complete under monotonic heuristics. For\nmulti-pin length-matching challenges, we introduce a pin-ordering mechanism\nbased on detour margins to reduce the likelihood of prematurely blocking\nfeasible routes. Through evaluations on various length-matching benchmarks, we\nanalyze runtime and heuristic performance, demonstrating the effectiveness of\nthe proposed approaches across different layout scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u53ef\u7f16\u7a0b\u5149\u5b50\u96c6\u6210\u7535\u8def\u7684\u7cbe\u786e\u957f\u5ea6\u5339\u914d\u5e03\u7ebf\u7b56\u7565\uff0c\u5305\u62ec\u53ef\u91c7\u7eb3\u542f\u53d1\u5f0f\u4f30\u8ba1\u5668\u548c\u526a\u679d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5e73\u8861\u641c\u7d22\u7a7a\u95f4\u548c\u8fd0\u884c\u65f6\u95f4\u7684\u540c\u65f6\u627e\u5230\u7cbe\u786e\u957f\u5ea6\u7684\u8def\u5f84\u3002", "motivation": "\u5728\u53ef\u7f16\u7a0b\u5149\u5b50\u96c6\u6210\u7535\u8def\u4e2d\uff0c\u7cbe\u786e\u7684\u5bfc\u7ebf\u957f\u5ea6\u63a7\u5236\u5bf9\u4e8e\u5149\u5b66\u73af\u5f62\u8c10\u632f\u5668\u3001\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u5e72\u6d89\u4eea\u7b49\u7ec4\u4ef6\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u9700\u8981\u7cbe\u786e\u957f\u5ea6\u5e03\u7ebf\u800c\u975e\u4f20\u7edf\u7684\u6700\u77ed\u8def\u5f84\u5e03\u7ebf\u3002", "method": "\u57fa\u4e8e\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7684\u6539\u8fdb\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86\u53ef\u91c7\u7eb3\u542f\u53d1\u5f0f\u4f30\u8ba1\u5668\u548c\u526a\u679d\u65b9\u6cd5\uff1b\u5bf9\u4e8e\u53cc\u5f15\u811a\u957f\u5ea6\u5339\u914d\u5e03\u7ebf\uff0c\u5728\u5355\u8c03\u542f\u53d1\u5f0f\u4e0b\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u5b8c\u5907\u6027\uff1b\u5bf9\u4e8e\u591a\u5f15\u811a\u957f\u5ea6\u5339\u914d\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u7ed5\u884c\u8fb9\u754c\u7684\u5f15\u811a\u6392\u5e8f\u673a\u5236\u3002", "result": "\u5728\u5404\u79cd\u957f\u5ea6\u5339\u914d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86\u8fd0\u884c\u65f6\u95f4\u548c\u542f\u53d1\u5f0f\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u540c\u5e03\u5c40\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u957f\u5ea6\u5339\u914d\u5e03\u7ebf\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u53ef\u7f16\u7a0b\u5149\u5b50\u96c6\u6210\u7535\u8def\u4e2d\u7684\u7cbe\u786e\u5e03\u7ebf\u9700\u6c42\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5e73\u8861\u4e86\u641c\u7d22\u6548\u7387\u548c\u8fd0\u884c\u65f6\u95f4\u3002"}}
{"id": "2509.22823", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22823", "abs": "https://arxiv.org/abs/2509.22823", "authors": ["Mounssif Krouka", "Mehdi Bennis"], "title": "Communication-Efficient and Interoperable Distributed Learning", "comment": "Preprint version. Submitted for peer review", "summary": "Collaborative learning across heterogeneous model architectures presents\nsignificant challenges in ensuring interoperability and preserving privacy. We\npropose a communication-efficient distributed learning framework that supports\nmodel heterogeneity and enables modular composition during inference. To\nfacilitate interoperability, all clients adopt a common fusion-layer output\ndimension, which permits each model to be partitioned into a personalized base\nblock and a generalized modular block. Clients share their fusion-layer\noutputs, keeping model parameters and architectures private. Experimental\nresults demonstrate that the framework achieves superior communication\nefficiency compared to federated learning (FL) and federated split learning\n(FSL) baselines, while ensuring stable training performance across\nheterogeneous architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u6a21\u578b\u5f02\u6784\u7684\u901a\u4fe1\u9ad8\u6548\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u878d\u5408\u5c42\u8f93\u51fa\u6765\u5b9e\u73b0\u4e92\u64cd\u4f5c\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u6a21\u578b\u53c2\u6570\u548c\u67b6\u6784\u9690\u79c1\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u6a21\u578b\u67b6\u6784\u5728\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u6311\u6218\u3002", "method": "\u91c7\u7528\u901a\u7528\u878d\u5408\u5c42\u8f93\u51fa\u7ef4\u5ea6\uff0c\u5c06\u6a21\u578b\u5206\u4e3a\u4e2a\u6027\u5316\u57fa\u7840\u5757\u548c\u901a\u7528\u6a21\u5757\u5757\uff0c\u5ba2\u6237\u7aef\u5171\u4eab\u878d\u5408\u5c42\u8f93\u51fa\u800c\u975e\u6a21\u578b\u53c2\u6570\u3002", "result": "\u76f8\u6bd4\u8054\u90a6\u5b66\u4e60\u548c\u8054\u90a6\u5206\u5272\u5b66\u4e60\u57fa\u7ebf\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u901a\u4fe1\u6548\u7387\uff0c\u540c\u65f6\u5728\u5f02\u6784\u67b6\u6784\u4e0a\u4fdd\u6301\u7a33\u5b9a\u7684\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6a21\u578b\u534f\u4f5c\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.23179", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23179", "abs": "https://arxiv.org/abs/2509.23179", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "A Near-Cache Architectural Framework for Cryptographic Computing", "comment": null, "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aCrypto-Near-Cache (CNC)\u7684\u8fd1\u7f13\u5b58\u5207\u7247\u8ba1\u7b97\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u5177\u6709\u4f4d\u7ebf\u8ba1\u7b97\u80fd\u529b\u7684SRAM\u9635\u5217\u653e\u7f6e\u5728\u7f13\u5b58\u5207\u7247\u9644\u8fd1\uff0c\u6765\u52a0\u901f\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u548c\u5176\u4ed6\u5e94\u7528\u3002", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u7684\u516c\u94a5\u548c\u7b7e\u540d\u957f\u5ea6\u6bd4\u524d\u91cf\u5b50\u5bc6\u7801\u957f3-9\u500d\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u548c\u80fd\u6548\u5f00\u9500\u3002\u5173\u952e\u74f6\u9888\u662f\u7f13\u5b58\u5e26\u5bbd\u9650\u5236\uff0c\u8fd9\u4fc3\u4f7f\u91c7\u7528\u7247\u4e0a\u8fd1\u7f13\u5b58\u8ba1\u7b97\u8303\u5f0f\u3002", "method": "\u8bbe\u8ba1CNC\u8303\u5f0f\uff0c\u5c06\u5177\u6709\u4f4d\u7ebf\u8ba1\u7b97\u80fd\u529b\u7684SRAM\u9635\u5217\u653e\u7f6e\u5728\u7f13\u5b58\u5207\u7247\u9644\u8fd1\uff0c\u5b9e\u73b0\u9ad8\u5185\u90e8\u5e26\u5bbd\u548c\u77ed\u6570\u636e\u79fb\u52a8\uff0c\u652f\u6301\u865a\u62df\u5730\u5740\u5bfb\u5740\uff0c\u5e76\u63d0\u51faISA\u6269\u5c55\u3002", "result": "\u901a\u8fc7CNC\u5b9e\u73b0\u4e86\u9ad8\u5185\u90e8\u5e26\u5bbd\u548c\u77ed\u6570\u636e\u79fb\u52a8\uff0c\u539f\u751f\u652f\u6301\u865a\u62df\u5bfb\u5740\uff0c\u4e3a\u6838\u5fc3/\u7f13\u5b58\u6570\u636e\u8def\u5f84\u63d0\u4f9b\u4e86\u8be6\u7ec6\u5b9e\u73b0\u65b9\u6848\u3002", "conclusion": "CNC\u8303\u5f0f\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\uff0c\u89e3\u51b3\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u80fd\u6548\u95ee\u9898\uff0c\u4e3a\u52a0\u901f\u6b64\u7c7b\u7b97\u6cd5\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24992", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24992", "abs": "https://arxiv.org/abs/2509.24992", "authors": ["Till Aust", "Christoph Karl Heck", "Eduard Buss", "Heiko Hamann"], "title": "Embedded Deep Learning for Bio-hybrid Plant Sensors to Detect Increased Heat and Ozone Levels", "comment": "Submitted to IEEE Sensors 2025", "summary": "We present a bio-hybrid environmental sensor system that integrates natural\nplants and embedded deep learning for real-time, on-device detection of\ntemperature and ozone level changes. Our system, based on the low-power\nPhytoNode platform, records electric differential potential signals from Hedera\nhelix and processes them onboard using an embedded deep learning model. We\ndemonstrate that our sensing device detects changes in temperature and ozone\nwith good sensitivity of up to 0.98. Daily and inter-plant variability, as well\nas limited precision, could be mitigated by incorporating additional training\ndata, which is readily integrable in our data-driven framework. Our approach\nalso has potential to scale to new environmental factors and plant species. By\nintegrating embedded deep learning onboard our biological sensing device, we\noffer a new, low-power solution for continuous environmental monitoring and\npotentially other fields of application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u690d\u7269\u548c\u5d4c\u5165\u5f0f\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u7269\u6df7\u5408\u73af\u5883\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u6e29\u5ea6\u548c\u81ed\u6c27\u53d8\u5316\u3002", "motivation": "\u5f00\u53d1\u4f4e\u529f\u8017\u7684\u8fde\u7eed\u73af\u5883\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u690d\u7269\u7684\u751f\u7269\u7279\u6027\u8fdb\u884c\u73af\u5883\u611f\u77e5\u3002", "method": "\u4f7f\u7528\u4f4e\u529f\u8017PhytoNode\u5e73\u53f0\u8bb0\u5f55\u5e38\u6625\u85e4\u7684\u7535\u52bf\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u5f0f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u6e29\u5ea6\u548c\u81ed\u6c27\u53d8\u5316\uff0c\u7075\u654f\u5ea6\u9ad8\u8fbe0.98\uff0c\u4f46\u5b58\u5728\u65e5\u95f4\u548c\u690d\u7269\u95f4\u53d8\u5f02\u6027\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u4f4e\u529f\u8017\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u6269\u5c55\u5230\u5176\u4ed6\u73af\u5883\u56e0\u7d20\u548c\u690d\u7269\u7269\u79cd\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.22840", "categories": ["cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.22840", "abs": "https://arxiv.org/abs/2509.22840", "authors": ["Micah Adler"], "title": "On the Capacity of Self-Attention", "comment": null, "summary": "While self-attention is known to learn relations among tokens, we lack a\nformal understanding of its capacity: how many distinct relations can a single\nlayer reliably recover for a given budget?\n  To formalize this, we introduce Relational Graph Recognition (RGR), where the\nkey-query channel represents a graph on $m$ items with $m'$ directed edges,\nand, given a context of items, must recover the neighbors of each item. We\nmeasure resources by the total key dimension $D_K = h\\,d_k$. Within this\nframework, we analytically derive a capacity scaling law and validate it\nempirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both\nnecessary (information-theoretic lower bound) and sufficient (explicit\nconstruction) in a broad class of graphs to recover $m'$ relations. This\nscaling law directly leads to a new, capacity-based rationale for multi-head\nattention that applies even when each item only attends to a single target.\nWhen embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a\npermutation, a single head suffices. However, compression ($m >\nd_{\\text{model}}$) forces relations into overlapping subspaces, creating\ninterference that a single large head cannot disentangle. Our analysis shows\nthat allocating a fixed $D_K$ across many small heads mitigates this\ninterference, increasing the number of recoverable relations. Controlled\nsingle-layer experiments mirror the theory, revealing a sharp performance\nthreshold that matches the predicted capacity scaling and confirms the benefit\nof distributing $D_K$ across multiple heads.\n  Altogether, these results provide a concrete scaling law for self-attention\ncapacity and a principled design rule for allocating key-query budget across\nheads.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5bb9\u91cf\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u5173\u7cfb\u56fe\u8bc6\u522b\u6846\u67b6\uff0c\u63a8\u5bfc\u51fa\u5bb9\u91cf\u7f29\u653e\u5b9a\u5f8b\uff1aD_K = \u0398(m' log m' / d_model)\uff0c\u5e76\u8bc1\u660e\u591a\u5934\u6ce8\u610f\u529b\u5728\u538b\u7f29\u5d4c\u5165\u60c5\u51b5\u4e0b\u80fd\u7f13\u89e3\u5173\u7cfb\u5e72\u6270\u3002", "motivation": "\u867d\u7136\u5df2\u77e5\u81ea\u6ce8\u610f\u529b\u80fd\u5b66\u4e60token\u95f4\u7684\u5173\u7cfb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5bb9\u91cf\u7684\u5f62\u5f0f\u5316\u7406\u89e3\uff1a\u5728\u7ed9\u5b9a\u9884\u7b97\u4e0b\uff0c\u5355\u5c42\u80fd\u53ef\u9760\u6062\u590d\u591a\u5c11\u79cd\u4e0d\u540c\u5173\u7cfb\uff1f", "method": "\u5f15\u5165\u5173\u7cfb\u56fe\u8bc6\u522b(RGR)\u6846\u67b6\uff0c\u5c06\u952e-\u67e5\u8be2\u901a\u9053\u8868\u793a\u4e3a\u5177\u6709m'\u6761\u6709\u5411\u8fb9\u7684\u56fe\uff0c\u5206\u6790\u4fe1\u606f\u8bba\u4e0b\u754c\u548c\u663e\u5f0f\u6784\u9020\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u5355\u5c42\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u63a8\u5bfc\u51fa\u5bb9\u91cf\u7f29\u653e\u5b9a\u5f8bD_K = \u0398(m' log m' / d_model)\uff0c\u8bc1\u660e\u5728\u538b\u7f29\u5d4c\u5165(m > d_model)\u65f6\uff0c\u591a\u5934\u6ce8\u610f\u529b\u6bd4\u5355\u5934\u80fd\u6062\u590d\u66f4\u591a\u5173\u7cfb\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u9608\u503c\u4e0e\u9884\u6d4b\u5bb9\u91cf\u7f29\u653e\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u81ea\u6ce8\u610f\u529b\u5bb9\u91cf\u63d0\u4f9b\u4e86\u5177\u4f53\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u4e3a\u8de8\u5934\u5206\u914d\u952e-\u67e5\u8be2\u9884\u7b97\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8bbe\u8ba1\u89c4\u5219\u3002"}}
{"id": "2509.22701", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22701", "abs": "https://arxiv.org/abs/2509.22701", "authors": ["Leszek Sliwko", "Jolanta Mizera-Pietraszko"], "title": "Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization", "comment": "This is the accepted version of the paper published in 2025 IEEE\n  International Parallel and Distributed Processing Symposium Workshops\n  (IPDPSW). The final version is available at:\n  https://doi.org/10.1109/IPDPSW66978.2025.00056", "summary": "This study presents a machine learning-assisted approach to optimize task\nscheduling in cluster systems, focusing on node-affinity constraints.\nTraditional schedulers like Kubernetes struggle with real-time adaptability,\nwhereas the proposed continuous transfer learning model evolves dynamically\nduring operations, minimizing retraining needs. Evaluated on Google Cluster\nData, the model achieves over 99% accuracy, reducing computational overhead and\nimproving scheduling latency for constrained tasks. This scalable solution\nenables real-time optimization, advancing machine learning integration in\ncluster management and paving the way for future adaptive scheduling\nstrategies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u96c6\u7fa4\u4efb\u52a1\u8c03\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u8fc1\u79fb\u5b66\u4e60\u52a8\u6001\u9002\u5e94\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\uff0c\u51cf\u5c11\u91cd\u8bad\u7ec3\u9700\u6c42\uff0c\u5728Google\u96c6\u7fa4\u6570\u636e\u4e0a\u8fbe\u523099%\u4ee5\u4e0a\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u8c03\u5ea6\u5668\u5982Kubernetes\u5728\u5b9e\u65f6\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\u4e0b\u7684\u52a8\u6001\u8c03\u5ea6\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6301\u7eed\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6f14\u5316\uff0c\u6700\u5c0f\u5316\u91cd\u8bad\u7ec3\u9700\u6c42\uff0c\u4f18\u5316\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\u4e0b\u7684\u4efb\u52a1\u8c03\u5ea6\u3002", "result": "\u5728Google\u96c6\u7fa4\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u6539\u5584\u53d7\u9650\u4efb\u52a1\u7684\u8c03\u5ea6\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4f18\u5316\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u96c6\u7fa4\u7ba1\u7406\u4e2d\u7684\u96c6\u6210\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u8c03\u5ea6\u7b56\u7565\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.23674", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23674", "abs": "https://arxiv.org/abs/2509.23674", "authors": ["Hongqin Lyu", "Yonghao Wang", "Yunlin Du", "Mingyu Shi", "Zhiteng Chao", "Wenxing Li", "Tiancheng Wang", "Huawei Li"], "title": "AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging", "comment": "6 pages, 7 figures", "summary": "Assertion-based verification (ABV) serves as a crucial technique for ensuring\nthat register-transfer level (RTL) designs adhere to their specifications.\nWhile Large Language Model (LLM) aided assertion generation approaches have\nrecently achieved remarkable progress, existing methods are still unable to\neffectively identify the relationship between design specifications and RTL\ndesigns, which leads to the insufficiency of the generated assertions. To\naddress this issue, we propose AssertGen, an assertion generation framework\nthat automatically generates SystemVerilog assertions (SVA). AssertGen first\nextracts verification objectives from specifications using a chain-of-thought\n(CoT) reasoning strategy, then bridges corresponding signals between these\nobjectives and the RTL code to construct a cross-layer signal chain, and\nfinally generates SVAs based on the LLM. Experimental results demonstrate that\nAssertGen outperforms the existing state-of-the-art methods across several key\nmetrics, such as pass rate of formal property verification (FPV), cone of\ninfluence (COI), proof core and mutation testing coverage.", "AI": {"tldr": "AssertGen\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65ad\u8a00\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4ece\u89c4\u8303\u4e2d\u63d0\u53d6\u9a8c\u8bc1\u76ee\u6807\uff0c\u6784\u5efa\u8de8\u5c42\u4fe1\u53f7\u94fe\uff0c\u751f\u6210SystemVerilog\u65ad\u8a00\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u8bbe\u8ba1\u89c4\u8303\u4e0eRTL\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u65ad\u8a00\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4ece\u89c4\u8303\u4e2d\u63d0\u53d6\u9a8c\u8bc1\u76ee\u6807\uff0c\u7136\u540e\u5728\u76ee\u6807\u4e0eRTL\u4ee3\u7801\u4e4b\u95f4\u6865\u63a5\u4fe1\u53f7\u6784\u5efa\u8de8\u5c42\u4fe1\u53f7\u94fe\uff0c\u6700\u540e\u57fa\u4e8eLLM\u751f\u6210SystemVerilog\u65ad\u8a00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aAssertGen\u5728\u5f62\u5f0f\u5c5e\u6027\u9a8c\u8bc1\u901a\u8fc7\u7387\u3001\u5f71\u54cd\u9525\u3001\u8bc1\u660e\u6838\u5fc3\u548c\u53d8\u5f02\u6d4b\u8bd5\u8986\u76d6\u7387\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "AssertGen\u901a\u8fc7\u6709\u6548\u8fde\u63a5\u89c4\u8303\u4e0eRTL\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65ad\u8a00\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002"}}
{"id": "2509.25057", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.25057", "abs": "https://arxiv.org/abs/2509.25057", "authors": ["O. Tansel Baydas", "Efe Yatgin", "Ozgur B. Akan"], "title": "Information Transmission in Quorum Sensing for Gut Microbiome", "comment": null, "summary": "Microorganisms employ sophisticated mechanisms for intercellular\ncommunication and environmental sensing, with quorum sensing serving as a\nfundamental regulatory process. Dysregulation of quorum sensing has been\nimplicated in various diseases. While most theoretical studies focus on\nmathematical modeling of quorum sensing dynamics, the communication-theoretic\naspects remain less explored. In this study, we investigate the information\nprocessing capabilities of quorum sensing systems using a stochastic\ndifferential equation framework that links intracellular gene regulation to\nextracellular autoinducer dynamics. We quantify mutual information as a measure\nof signaling efficiency and information fidelity in two major bacterial phyla\nof the gut microbiota: Firmicutes and Bacteroidetes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7fa4\u4f53\u611f\u5e94\u7cfb\u7edf\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\u8fde\u63a5\u7ec6\u80de\u5185\u57fa\u56e0\u8c03\u63a7\u548c\u7ec6\u80de\u5916\u81ea\u8bf1\u5bfc\u7269\u52a8\u6001\uff0c\u91cf\u5316\u4e86\u80a0\u9053\u5fae\u751f\u7269\u7fa4\u4e2d\u539a\u58c1\u83cc\u95e8\u548c\u62df\u6746\u83cc\u95e8\u7684\u4fe1\u53f7\u6548\u7387\u548c\u4fe1\u606f\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5fae\u751f\u7269\u91c7\u7528\u590d\u6742\u7684\u7ec6\u80de\u95f4\u901a\u4fe1\u548c\u73af\u5883\u611f\u77e5\u673a\u5236\uff0c\u7fa4\u4f53\u611f\u5e94\u662f\u57fa\u672c\u8c03\u63a7\u8fc7\u7a0b\u3002\u7fa4\u4f53\u611f\u5e94\u5931\u8c03\u4e0e\u591a\u79cd\u75be\u75c5\u76f8\u5173\u3002\u867d\u7136\u5927\u591a\u6570\u7406\u8bba\u7814\u7a76\u5173\u6ce8\u7fa4\u4f53\u611f\u5e94\u52a8\u529b\u5b66\u7684\u6570\u5b66\u5efa\u6a21\uff0c\u4f46\u5176\u901a\u4fe1\u7406\u8bba\u65b9\u9762\u4ecd\u8f83\u5c11\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u5c06\u7ec6\u80de\u5185\u57fa\u56e0\u8c03\u63a7\u4e0e\u7ec6\u80de\u5916\u81ea\u8bf1\u5bfc\u7269\u52a8\u6001\u8054\u7cfb\u8d77\u6765\uff0c\u91cf\u5316\u4e92\u4fe1\u606f\u4f5c\u4e3a\u4fe1\u53f7\u6548\u7387\u548c\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u5ea6\u91cf\u3002", "result": "\u5728\u80a0\u9053\u5fae\u751f\u7269\u7fa4\u7684\u4e24\u4e2a\u4e3b\u8981\u7ec6\u83cc\u95e8\uff08\u539a\u58c1\u83cc\u95e8\u548c\u62df\u6746\u83cc\u95e8\uff09\u4e2d\u91cf\u5316\u4e86\u7fa4\u4f53\u611f\u5e94\u7cfb\u7edf\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u7fa4\u4f53\u611f\u5e94\u7cfb\u7edf\u7684\u901a\u4fe1\u7406\u8bba\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7ec6\u83cc\u95f4\u4fe1\u606f\u4f20\u9012\u7684\u6548\u7387\u7279\u5f81\u3002"}}
{"id": "2509.22850", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22850", "abs": "https://arxiv.org/abs/2509.22850", "authors": ["Roie Kazoom", "Yuval Ratzabi", "Etamar Rothstein", "Ofer Hadar"], "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data", "comment": null, "summary": "Adversarial robustness in structured data remains an underexplored frontier\ncompared to vision and language domains. In this work, we introduce a novel\nblack-box, decision-based adversarial attack tailored for tabular data. Our\napproach combines gradient-free direction estimation with an iterative boundary\nsearch, enabling efficient navigation of discrete and continuous feature spaces\nunder minimal oracle access. Extensive experiments demonstrate that our method\nsuccessfully compromises nearly the entire test set across diverse models,\nranging from classical machine learning classifiers to large language model\n(LLM)-based pipelines. Remarkably, the attack achieves success rates\nconsistently above 90%, while requiring only a small number of queries per\ninstance. These results highlight the critical vulnerability of tabular models\nto adversarial perturbations, underscoring the urgent need for stronger\ndefenses in real-world decision-making systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u9ed1\u76d2\u51b3\u7b56\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u7ed3\u5408\u65e0\u68af\u5ea6\u65b9\u5411\u4f30\u8ba1\u548c\u8fed\u4ee3\u8fb9\u754c\u641c\u7d22\uff0c\u5728\u6700\u5c0f\u5316\u67e5\u8be2\u6b21\u6570\u7684\u524d\u63d0\u4e0b\u6709\u6548\u653b\u51fb\u8868\u683c\u6a21\u578b\u3002", "motivation": "\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u63ed\u793a\u5176\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u91c7\u7528\u9ed1\u76d2\u51b3\u7b56\u653b\u51fb\uff0c\u7ed3\u5408\u65e0\u68af\u5ea6\u65b9\u5411\u4f30\u8ba1\u548c\u8fed\u4ee3\u8fb9\u754c\u641c\u7d22\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u79bb\u6563\u548c\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u51e0\u4e4e\u80fd\u653b\u7834\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\uff0c\u5305\u62ec\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u6c34\u7ebf\uff0c\u4e14\u6bcf\u4e2a\u5b9e\u4f8b\u4ec5\u9700\u5c11\u91cf\u67e5\u8be2\u3002", "conclusion": "\u8868\u683c\u6a21\u578b\u5bf9\u5bf9\u6297\u6270\u52a8\u5b58\u5728\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u7cfb\u7edf\u8feb\u5207\u9700\u8981\u66f4\u5f3a\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2509.22704", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22704", "abs": "https://arxiv.org/abs/2509.22704", "authors": ["Leszek Sliwko"], "title": "Intelligent Load Balancing in Cloud Computer Systems", "comment": "A thesis submitted in partial fulfilment of the requirements of the\n  University of Westminster for the degree of Doctor of Philosophy", "summary": "Cloud computing is an established technology allowing users to share\nresources on a large scale, never before seen in IT history. A cloud system\nconnects multiple individual servers in order to process related tasks in\nseveral environments at the same time. Clouds are typically more cost-effective\nthan single computers of comparable computing performance. The sheer physical\nsize of the system itself means that thousands of machines may be involved. The\nfocus of this research was to design a strategy to dynamically allocate tasks\nwithout overloading Cloud nodes which would result in system stability being\nmaintained at minimum cost. This research has added the following new\ncontributions to the state of knowledge: (i) a novel taxonomy and\ncategorisation of three classes of schedulers, namely OS-level, Cluster and Big\nData, which highlight their unique evolution and underline their different\nobjectives; (ii) an abstract model of cloud resources utilisation is specified,\nincluding multiple types of resources and consideration of task migration\ncosts; (iii) a virtual machine live migration was experimented with in order to\ncreate a formula which estimates the network traffic generated by this process;\n(iv) a high-fidelity Cloud workload simulator, based on a month-long workload\ntraces from Google's computing cells, was created; (v) two possible approaches\nto resource management were proposed and examined in the practical part of the\nmanuscript: the centralised metaheuristic load balancer and the decentralised\nagent-based system. The project involved extensive experiments run on the\nUniversity of Westminster HPC cluster, and the promising results are presented\ntogether with detailed discussions and a conclusion.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e91\u73af\u5883\u4e2d\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u7c7b\u8c03\u5ea6\u5668\u3001\u5efa\u7acb\u8d44\u6e90\u5229\u7528\u6a21\u578b\u3001\u5b9e\u9a8c\u865a\u62df\u673a\u8fc1\u79fb\u3001\u5f00\u53d1\u9ad8\u4fdd\u771f\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\uff0c\u5e76\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u6210\u672c\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e91\u8ba1\u7b97\u7cfb\u7edf\u89c4\u6a21\u5e9e\u5927\uff0c\u9700\u8981\u8bbe\u8ba1\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\u4ee5\u907f\u514d\u8282\u70b9\u8fc7\u8f7d\uff0c\u5728\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6210\u672c\u3002", "method": "\u63d0\u51fa\u8c03\u5ea6\u5668\u5206\u7c7b\u6cd5\u3001\u5efa\u7acb\u62bd\u8c61\u8d44\u6e90\u5229\u7528\u6a21\u578b\u3001\u5b9e\u9a8c\u865a\u62df\u673a\u8fc1\u79fb\u7f51\u7edc\u6d41\u91cf\u3001\u5f00\u53d1\u57fa\u4e8eGoogle\u5de5\u4f5c\u8d1f\u8f7d\u75d5\u8ff9\u7684\u6a21\u62df\u5668\uff0c\u5e76\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u5143\u542f\u53d1\u5f0f\u8d1f\u8f7d\u5747\u8861\u5668\u548c\u5206\u5e03\u5f0f\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u5728\u5a01\u65af\u654f\u65af\u7279\u5927\u5b66HPC\u96c6\u7fa4\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u865a\u62df\u673a\u8fc1\u79fb\u7f51\u7edc\u6d41\u91cf\u4f30\u7b97\u516c\u5f0f\u548c\u9ad8\u4fdd\u771f\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u4e91\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u8d44\u6e90\u4f18\u5316\uff0c\u80fd\u591f\u5728\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6210\u672c\u6700\u5c0f\u5316\u3002"}}
{"id": "2509.23693", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.23693", "abs": "https://arxiv.org/abs/2509.23693", "authors": ["Tao Lu", "Jiapin Wang", "Yelin Shan", "Xiangping Zhang", "Xiang Chen"], "title": "ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights", "comment": "16 pages", "summary": "Lossless compression imposes significant computational over head on\ndatacenters when performed on CPUs. Hardware compression and decompression\nprocessing units (CDPUs) can alleviate this overhead, but optimal algorithm\nselection, microarchitectural design, and system-level placement of CDPUs are\nstill not well understood. We present the design of an ASIC-based in-storage\nCDPU and provide a comprehensive end-to-end evaluation against two leading ASIC\naccelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant\nCDPU placement regimes: peripheral, on-chip, and in-storage. Our results\nreveal: (i) acute sensitivity of throughput and latency to CDPU placement and\ninterconnection, (ii) strong correlation between compression efficiency and\ndata patterns/layouts, (iii) placement-driven divergences between\nmicrobenchmark gains and real-application speedups, (iv) discrepancies between\nmodule and system-level power efficiency, and (v) scalability and multi-tenant\ninterference is sues of various CDPUs. These findings motivate a\nplacement-aware, cross-layer rethinking of hardware (de)compression for\nhyperscale storage infrastructures.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u786c\u4ef6\u538b\u7f29\u89e3\u538b\u5904\u7406\u5355\u5143(CDPU)\u5728\u6570\u636e\u4e2d\u5fc3\u4e2d\u7684\u4f18\u5316\u8bbe\u8ba1\u3001\u7b97\u6cd5\u9009\u62e9\u548c\u7cfb\u7edf\u7ea7\u90e8\u7f72\u95ee\u9898\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e09\u79cdCDPU\u90e8\u7f72\u65b9\u5f0f(\u5916\u8bbe\u3001\u7247\u4e0a\u3001\u5b58\u50a8\u5185)\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "CPU\u6267\u884c\u65e0\u635f\u538b\u7f29\u5728\u6570\u636e\u4e2d\u5fc3\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u5f00\u9500\uff0c\u786c\u4ef6CDPU\u53ef\u4ee5\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u6700\u4f18\u7b97\u6cd5\u9009\u62e9\u3001\u5fae\u67b6\u6784\u8bbe\u8ba1\u548c\u7cfb\u7edf\u7ea7\u90e8\u7f72\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u8bbe\u8ba1ASIC\u5b58\u50a8\u5185CDPU\uff0c\u5e76\u4e0eIntel QAT 8970\u548cQAT 4xxx\u8fdb\u884c\u7aef\u5230\u7aef\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e09\u79cdCDPU\u90e8\u7f72\u65b9\u5f0f\uff1a\u5916\u8bbe\u3001\u7247\u4e0a\u3001\u5b58\u50a8\u5185\u3002", "result": "\u53d1\u73b0\uff1a(i)\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u5bf9CDPU\u90e8\u7f72\u548c\u4e92\u8fde\u9ad8\u5ea6\u654f\u611f\uff1b(ii)\u538b\u7f29\u6548\u7387\u4e0e\u6570\u636e\u6a21\u5f0f/\u5e03\u5c40\u5f3a\u76f8\u5173\uff1b(iii)\u5fae\u57fa\u51c6\u6d4b\u8bd5\u589e\u76ca\u4e0e\u5b9e\u9645\u5e94\u7528\u52a0\u901f\u5b58\u5728\u90e8\u7f72\u9a71\u52a8\u5dee\u5f02\uff1b(iv)\u6a21\u5757\u7ea7\u4e0e\u7cfb\u7edf\u7ea7\u80fd\u6548\u5b58\u5728\u5dee\u5f02\uff1b(v)\u5404\u79cdCDPU\u7684\u53ef\u6269\u5c55\u6027\u548c\u591a\u79df\u6237\u5e72\u6270\u95ee\u9898\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4fc3\u4f7f\u5bf9\u8d85\u5927\u89c4\u6a21\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u786c\u4ef6\u538b\u7f29\u89e3\u538b\u8fdb\u884c\u90e8\u7f72\u611f\u77e5\u7684\u8de8\u5c42\u91cd\u65b0\u601d\u8003\u3002"}}
{"id": "2509.22851", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22851", "abs": "https://arxiv.org/abs/2509.22851", "authors": ["Yaswanth Chittepu", "Prasann Singhal", "Greg Durrett", "Scott Niekum"], "title": "Adaptive Margin RLHF via Preference over Preferences", "comment": null, "summary": "Margin-based optimization is fundamental to improving generalization and\nrobustness in classification tasks. In the context of reward model learning\nfrom preferences within Reinforcement Learning from Human Feedback (RLHF),\nexisting methods typically rely on no margins, fixed margins, or margins that\nare simplistic functions of preference ratings. However, such formulations\noften fail to account for the varying strengths of different preferences, for\nexample some preferences are associated with larger margins between responses,\nor they rely on noisy margin information derived from ratings. We argue that\nmodeling the strength of preferences can lead to better generalization and more\nfaithful alignment. Furthermore, many existing methods that use adaptive\nmargins assume access to accurate preference scores, which can be difficult for\nhumans to provide reliably. We propose an approach that leverages preferences\nover preferences, that is annotations indicating which of two preferences\nreflects a stronger distinction. We use this ordinal signal to infer adaptive\nmargins on a per-datapoint basis. We introduce an extension to Direct\nPreference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from\npreference-over-preference supervision, enabling improved discriminative and\ngenerative performance. Empirically, our method outperforms vanilla DPO, DPO\nwith fixed margins, and DPO with ground-truth margins on the UltraFeedback\ndataset. Additionally, we show that there is a tradeoff between discriminative\nand generative performance: improving test classification accuracy,\nparticularly by correctly labeling weaker preferences at the expense of\nstronger ones, can lead to a decline in generative quality. To navigate this\ntradeoff, we propose two sampling strategies to gather\npreference-over-preference labels: one favoring discriminative performance and\none favoring generative performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86DPO-PoP\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u4e4b\u95f4\u7684\u504f\u597d\u76d1\u7763\u6765\u63a8\u65ad\u81ea\u9002\u5e94\u8fb9\u754c\uff0c\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\u4f18\u4e8evanilla DPO\u3001\u56fa\u5b9a\u8fb9\u754cDPO\u548c\u771f\u5b9e\u8fb9\u754cDPO\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u65e0\u8fb9\u754c\u3001\u56fa\u5b9a\u8fb9\u754c\u6216\u7b80\u5355\u51fd\u6570\u8fb9\u754c\uff0c\u65e0\u6cd5\u8003\u8651\u4e0d\u540c\u504f\u597d\u7684\u5f3a\u5ea6\u5dee\u5f02\uff0c\u4e14\u4f9d\u8d56\u53ef\u80fd\u5608\u6742\u7684\u8bc4\u5206\u4fe1\u606f\u3002\u5efa\u6a21\u504f\u597d\u5f3a\u5ea6\u53ef\u4ee5\u6539\u5584\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u9f50\u6548\u679c\u3002", "method": "\u6269\u5c55DPO\u65b9\u6cd5\u4e3aDPO-PoP\uff0c\u5229\u7528\u504f\u597d\u4e4b\u95f4\u7684\u504f\u597d\u6807\u6ce8\u6765\u63a8\u65ad\u6bcf\u4e2a\u6570\u636e\u70b9\u7684\u81ea\u9002\u5e94\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u91c7\u6837\u7b56\u7565\u6765\u5e73\u8861\u5224\u522b\u6027\u80fd\u548c\u751f\u6210\u6027\u80fd\u3002", "result": "\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\uff0cDPO-PoP\u5728\u5224\u522b\u548c\u751f\u6210\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u53d1\u73b0\u5224\u522b\u6027\u80fd\u548c\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u504f\u597d\u4e4b\u95f4\u7684\u504f\u597d\u76d1\u7763\u53ef\u4ee5\u6709\u6548\u5730\u63a8\u65ad\u81ea\u9002\u5e94\u8fb9\u754c\uff0c\u63d0\u51fa\u7684\u4e24\u79cd\u91c7\u6837\u7b56\u7565\u80fd\u591f\u6839\u636e\u76ee\u6807\uff08\u5224\u522b\u6216\u751f\u6210\u6027\u80fd\uff09\u6765\u5e73\u8861\u8fd9\u79cd\u6743\u8861\u3002"}}
{"id": "2509.22707", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22707", "abs": "https://arxiv.org/abs/2509.22707", "authors": ["Jinqi Yan", "Fang He", "Qianlong Sang", "Bifeng Tong", "Peng Sun", "Yili Gong", "Chuang Hu", "Dazhao Cheng"], "title": "Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices", "comment": null, "summary": "Dynamic Voltage and Frequency Scaling is essential for enhancing energy\nefficiency in mobile platforms. However, traditional heuristic-based governors\nare increasingly inadequate for managing the complexity of heterogeneous\nSystem-on-Chip designs and diverse application workloads. Although\nreinforcement learning approaches offer improved performance, their poor\ngeneralization capability and reliance on extensive retraining for each\nhardware and application combination leads to significant deployment costs. In\nthis work, we observe that device and application metadata inherently\nencapsulate valuable knowledge for DVFS, presenting an opportunity to overcome\nthese limitations. We formulate DVFS for heterogeneous devices and applications\nas a multi-task reinforcement learning problem. We introduce MetaDVFS, which is\na metadata-guided framework that systematically leverages metadata to discover\nand transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of\nDVFS models with significant generalization capability for various applications\nof heterogeneous devices. Evaluations on five Google Pixel devices running six\napplications show that MetaDVFS achieves up to 17% improvement in\nPerformance-Power Ratio and up to 26% improvement in Quality of Experience.\nCompared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation\nand 5.8-27.6% higher performance over standalone device-application specific\ntraining, while avoiding negative transfer effects. These results establish\nMetaDVFS as an effective and scalable solution for DVFS deployment in\nheterogeneous mobile environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86MetaDVFS\u6846\u67b6\uff0c\u5229\u7528\u5143\u6570\u636e\u5f15\u5bfc\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u5f02\u6784\u8bbe\u5907\u548c\u5e94\u7528\u7684DVFS\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u7528\u6237\u4f53\u9a8c", "motivation": "\u4f20\u7edf\u542f\u53d1\u5f0fDVFS\u8c03\u63a7\u5668\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5f02\u6784SoC\u548c\u591a\u6837\u5316\u5e94\u7528\u8d1f\u8f7d\u7684\u590d\u6742\u6027\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u90e8\u7f72\u6210\u672c\u9ad8", "method": "\u5c06DVFS\u5efa\u6a21\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528\u8bbe\u5907\u548c\u5e94\u7528\u5143\u6570\u636e\u53d1\u73b0\u548c\u8fc1\u79fb\u5171\u4eab\u77e5\u8bc6\uff0c\u5f00\u53d1\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684DVFS\u6a21\u578b\u96c6", "result": "\u57285\u6b3eGoogle Pixel\u8bbe\u5907\u548c6\u4e2a\u5e94\u7528\u4e0a\u6d4b\u8bd5\uff0cPPR\u63d0\u534717%\uff0cQoE\u63d0\u534726%\uff0c\u9002\u5e94\u901f\u5ea6\u63d0\u9ad870.8%\uff0c\u6027\u80fd\u6bd4\u72ec\u7acb\u8bad\u7ec3\u63d0\u53475.8-27.6%", "conclusion": "MetaDVFS\u662f\u5f02\u6784\u79fb\u52a8\u73af\u5883\u4e2dDVFS\u90e8\u7f72\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u8d1f\u8fc1\u79fb\u6548\u5e94"}}
{"id": "2509.23972", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23972", "abs": "https://arxiv.org/abs/2509.23972", "authors": ["Hongqin Lyu", "Yunlin Du", "Yonghao Wang", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "AssertFix: Empowering Automated Assertion Fix via Large Language Models", "comment": "6 pages, 6 figures", "summary": "Assertion-based verification (ABV) is critical in ensuring that\nregister-transfer level (RTL) designs conform to their functional\nspecifications. SystemVerilog Assertions (SVA) effectively specify design\nproperties, but writing and maintaining them manually is challenging and\nerror-prone. Although recent progress of assertion generation methods\nleveraging large language models (LLMs) have shown great potential in improving\nassertion quality, they typically treat assertion generation as a final step,\nleaving the burden of fixing of the incorrect assertions to human effects,\nwhich may significantly limits the application of these methods. To address the\nabove limitation, we propose an automatic assertion fix framework based on\nLLMs, named AssertFix. AsserFix accurately locates the RTL code related to the\nincorrect assertion, systematically identifies the root causes of the assertion\nerrors, classifies the error type and finally applies dedicated fix strategies\nto automatically correct these errors, improving the overall quality of the\ngenerated assertions. Experimental results show that AssertFix achieves\nnoticeable improvements in both fix rate and verification coverage across the\nOpencore benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86AssertFix\u6846\u67b6\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590dSystemVerilog\u65ad\u8a00\u4e2d\u7684\u9519\u8bef\uff0c\u901a\u8fc7\u5b9a\u4f4dRTL\u4ee3\u7801\u3001\u8bc6\u522b\u9519\u8bef\u6839\u6e90\u3001\u5206\u7c7b\u9519\u8bef\u7c7b\u578b\u5e76\u5e94\u7528\u4e13\u7528\u4fee\u590d\u7b56\u7565\u6765\u63d0\u9ad8\u65ad\u8a00\u8d28\u91cf\u3002", "motivation": "\u624b\u52a8\u7f16\u5199\u548c\u7ef4\u62a4SystemVerilog\u65ad\u8a00\u5177\u6709\u6311\u6218\u6027\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\u5c06\u9519\u8bef\u4fee\u590d\u8d1f\u62c5\u7559\u7ed9\u4eba\u5de5\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faAssertFix\u6846\u67b6\uff0c\u81ea\u52a8\u5b9a\u4f4d\u4e0e\u9519\u8bef\u65ad\u8a00\u76f8\u5173\u7684RTL\u4ee3\u7801\uff0c\u7cfb\u7edf\u8bc6\u522b\u65ad\u8a00\u9519\u8bef\u7684\u6839\u672c\u539f\u56e0\uff0c\u5206\u7c7b\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u5e94\u7528\u4e13\u7528\u4fee\u590d\u7b56\u7565\u81ea\u52a8\u7ea0\u6b63\u9519\u8bef\u3002", "result": "\u5728Opencore\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAssertFix\u5728\u4fee\u590d\u7387\u548c\u9a8c\u8bc1\u8986\u76d6\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AssertFix\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u81ea\u52a8\u751f\u6210\u7684SystemVerilog\u65ad\u8a00\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u9519\u8bef\u4fee\u590d\u4f9d\u8d56\u4eba\u5de5\u7684\u95ee\u9898\u3002"}}
{"id": "2509.22855", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22855", "abs": "https://arxiv.org/abs/2509.22855", "authors": ["Sameep Chattopadhyay", "Nikhil Karamchandani", "Sharayu Mohair"], "title": "Observation-Free Attacks on Online Learning to Rank", "comment": null, "summary": "Online learning to rank (OLTR) plays a critical role in information retrieval\nand machine learning systems, with a wide range of applications in search\nengines and content recommenders. However, despite their extensive adoption,\nthe susceptibility of OLTR algorithms to coordinated adversarial attacks\nremains poorly understood. In this work, we present a novel framework for\nattacking some of the widely used OLTR algorithms. Our framework is designed to\npromote a set of target items so that they appear in the list of top-K\nrecommendations for T - o(T) rounds, while simultaneously inducing linear\nregret in the learning algorithm. We propose two novel attack strategies:\nCascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical\nguarantees showing that both strategies require only O(log T) manipulations to\nsucceed. Additionally, we supplement our theoretical analysis with empirical\nresults on real-world data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5728\u7ebf\u5b66\u4e60\u6392\u5e8f\u7b97\u6cd5\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u9700O(log T)\u6b21\u64cd\u7eb5\u5c31\u80fd\u5728T\u8f6e\u4e2d\u8ba9\u76ee\u6807\u7269\u54c1\u51fa\u73b0\u5728top-K\u63a8\u8350\u4e2d\uff0c\u540c\u65f6\u4f7f\u5b66\u4e60\u7b97\u6cd5\u4ea7\u751f\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u5c3d\u7ba1\u5728\u7ebf\u5b66\u4e60\u6392\u5e8f\u7b97\u6cd5\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u534f\u540c\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u8fd9\u6784\u6210\u4e86\u91cd\u8981\u7684\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u653b\u51fb\u7b56\u7565\uff1a\u9488\u5bf9CascadeUCB1\u7684CascadeOFA\u548c\u9488\u5bf9PBM-UCB\u7684PBMOFA\uff0c\u8fd9\u4e9b\u7b56\u7565\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u64cd\u7eb5\u6765\u6b3a\u9a97\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e24\u79cd\u7b56\u7565\u4ec5\u9700O(log T)\u6b21\u64cd\u7eb5\u5373\u53ef\u6210\u529f\uff0c\u5b9e\u8bc1\u7ed3\u679c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u7ebf\u5b66\u4e60\u6392\u5e8f\u7b97\u6cd5\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.22832", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22832", "abs": "https://arxiv.org/abs/2509.22832", "authors": ["Biyao Zhang", "Mingkai Zheng", "Debargha Ganguly", "Xuecen Zhang", "Vikash Singh", "Vipin Chaudhary", "Zhao Zhang"], "title": "Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM", "comment": null, "summary": "Training Large Language Models(LLMs) is one of the most compute-intensive\ntasks in high-performance computing. Predicting end-to-end training time for\nmulti-billion parameter models distributed across hundreds of GPUs remains\nchallenging due to complex interactions between transformer components,\nparallelism strategies(data, model, pipeline, tensor), and multi-tier\ncommunication. Learned models require costly sampling, while analytical models\noften struggle with real-world network and hardware complexities. We address\nthis by decomposing LLMs into core computational primitives and modeling them\nwith: (1) operator-level decomposition for fine-grained analysis; (2)\nlightweight sampling based hardware-aware prediction models for key operations;\n(3) an end-to-end prediction system integrating these components across complex\nparallelization strategies. Crucially, our methodology has been validated on\ntwo large-scale HPC systems. Our framework achieves low average prediction\nerrors-4.98\\% on Perlmutter(A100) and 9.38\\% on Vista(GH200)-for models up to\n20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling\nrapid iteration over hardware configurations and training strategies without\ncostly on-cluster experimentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3LLM\u4e3a\u8ba1\u7b97\u539f\u8bed\uff0c\u7ed3\u5408\u7b97\u5b50\u7ea7\u5206\u89e3\u548c\u8f7b\u91cf\u7ea7\u91c7\u6837\u786c\u4ef6\u611f\u77e5\u9884\u6d4b\u6a21\u578b\uff0c\u5728CPU\u4e0a\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u65e0\u9700\u6602\u8d35\u7684\u96c6\u7fa4\u5b9e\u9a8c\u3002", "motivation": "\u9884\u6d4b\u6570\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u6570\u767eGPU\u4e0a\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u91c7\u6837\uff0c\u800c\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u7f51\u7edc\u548c\u786c\u4ef6\u590d\u6742\u6027\u3002", "method": "\u5c06LLM\u5206\u89e3\u4e3a\u6838\u5fc3\u8ba1\u7b97\u539f\u8bed\uff0c\u91c7\u7528\u7b97\u5b50\u7ea7\u5206\u89e3\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u91c7\u6837\u7684\u786c\u4ef6\u611f\u77e5\u9884\u6d4b\u6a21\u578b\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u9884\u6d4b\u7cfb\u7edf\u6574\u5408\u8fd9\u4e9b\u7ec4\u4ef6\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21HPC\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0cPerlmutter(A100)\u5e73\u5747\u9884\u6d4b\u8bef\u5dee4.98%\uff0cVista(GH200)\u5e73\u5747\u9884\u6d4b\u8bef\u5dee9.38%\uff0c\u9002\u7528\u4e8e\u9ad8\u8fbe200\u4ebf\u53c2\u6570\u6a21\u578b\u548c128\u4e2aGPU\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u80fd\u591f\u5feb\u901f\u8fed\u4ee3\u786c\u4ef6\u914d\u7f6e\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u6602\u8d35\u7684\u96c6\u7fa4\u5b9e\u9a8c\uff0c\u4e3aLLM\u8bad\u7ec3\u65f6\u95f4\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24929", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.24929", "abs": "https://arxiv.org/abs/2509.24929", "authors": ["Hongwei Zhao", "Vianney Lapotre", "Guy Gogniat"], "title": "Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI", "comment": "12 pages, 7 tables", "summary": "Fault injection attacks exploit physical disturbances to compromise the\nfunctionality and security of integrated circuits. As System on Chip (SoC)\narchitectures grow in complexity, the vulnerability of on chip communication\nfabrics has become increasingly prominent. Buses, serving as interconnects\namong various IP cores, represent potential vectors for fault-based\nexploitation. In this study, we perform simulation-driven fault injection\nacross three mainstream bus protocols Wishbone, AXI Lite, and AXI. We\nsystematically examine fault success rates, spatial vulnerability\ndistributions, and timing dependencies to characterize how faults interact with\nbus-level transactions. The results uncover consistent behavioral patterns\nacross protocols, offering practical insights for both attack modeling and the\ndevelopment of resilient SoC designs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4eff\u771f\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\uff0c\u5206\u6790\u4e86\u4e09\u79cd\u4e3b\u6d41\u603b\u7ebf\u534f\u8bae\uff08Wishbone\u3001AXI Lite\u548cAXI\uff09\u7684\u6f0f\u6d1e\uff0c\u63ed\u793a\u4e86\u603b\u7ebf\u534f\u8bae\u5728\u6545\u969c\u653b\u51fb\u4e0b\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740SoC\u67b6\u6784\u590d\u6742\u6027\u589e\u52a0\uff0c\u7247\u4e0a\u901a\u4fe1\u7ed3\u6784\uff08\u7279\u522b\u662f\u603b\u7ebf\uff09\u7684\u6f0f\u6d1e\u65e5\u76ca\u7a81\u51fa\uff0c\u603b\u7ebf\u4f5c\u4e3aIP\u6838\u95f4\u4e92\u8fde\u53ef\u80fd\u6210\u4e3a\u6545\u969c\u653b\u51fb\u7684\u6f5c\u5728\u8f7d\u4f53\u3002", "method": "\u91c7\u7528\u4eff\u771f\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u68c0\u67e5\u6545\u969c\u6210\u529f\u7387\u3001\u7a7a\u95f4\u6f0f\u6d1e\u5206\u5e03\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u8868\u5f81\u6545\u969c\u4e0e\u603b\u7ebf\u7ea7\u4e8b\u52a1\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8de8\u534f\u8bae\u7684\u4e00\u81f4\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e3a\u653b\u51fb\u5efa\u6a21\u548c\u5f39\u6027SoC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u603b\u7ebf\u534f\u8bae\u5728\u6545\u969c\u6ce8\u5165\u653b\u51fb\u4e0b\u5b58\u5728\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff0c\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u6539\u8fdbSoC\u7684\u5b89\u5168\u6027\u548c\u97e7\u6027\u8bbe\u8ba1\u3002"}}
{"id": "2509.22868", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22868", "abs": "https://arxiv.org/abs/2509.22868", "authors": ["Zehao Niu", "Mihai Anitescu", "Jie Chen"], "title": "Neighborhood Sampling Does Not Learn the Same Graph Neural Network", "comment": null, "summary": "Neighborhood sampling is an important ingredient in the training of\nlarge-scale graph neural networks. It suppresses the exponential growth of the\nneighborhood size across network layers and maintains feasible memory\nconsumption and time costs. While it becomes a standard implementation in\npractice, its systemic behaviors are less understood. We conduct a theoretical\nanalysis by using the tool of neural tangent kernels, which characterize the\n(analogous) training dynamics of neural networks based on their infinitely wide\ncounterparts -- Gaussian processes (GPs). We study several established\nneighborhood sampling approaches and the corresponding posterior GP. With\nlimited samples, the posteriors are all different, although they converge to\nthe same one as the sample size increases. Moreover, the posterior covariance,\nwhich lower-bounds the mean squared prediction error, is uncomparable, aligning\nwith observations that no sampling approach dominates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u795e\u7ecf\u6b63\u5207\u6838\u7406\u8bba\u5206\u6790\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u90bb\u57df\u91c7\u6837\u7684\u7cfb\u7edf\u884c\u4e3a\uff0c\u53d1\u73b0\u4e0d\u540c\u91c7\u6837\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u540e\u9a8c\u5206\u5e03\u4e0d\u540c\uff0c\u4f46\u968f\u7740\u6837\u672c\u589e\u52a0\u4f1a\u6536\u655b\u5230\u76f8\u540c\u5206\u5e03\uff0c\u4e14\u6ca1\u6709\u4e00\u79cd\u91c7\u6837\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u5360\u4f18\u3002", "motivation": "\u90bb\u57df\u91c7\u6837\u5728\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u662f\u91cd\u8981\u6280\u672f\uff0c\u80fd\u6291\u5236\u90bb\u57df\u5927\u5c0f\u7684\u6307\u6570\u589e\u957f\u5e76\u63a7\u5236\u5185\u5b58\u548c\u65f6\u95f4\u6210\u672c\u3002\u867d\u7136\u5df2\u6210\u4e3a\u6807\u51c6\u5b9e\u73b0\uff0c\u4f46\u5176\u7cfb\u7edf\u884c\u4e3a\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u6b63\u5207\u6838\u5de5\u5177\u8fdb\u884c\u5206\u6790\uff0c\u8be5\u5de5\u5177\u901a\u8fc7\u65e0\u9650\u5bbd\u7f51\u7edc\u5bf9\u5e94\u7684\u9ad8\u65af\u8fc7\u7a0b\u6765\u8868\u5f81\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u7814\u7a76\u4e86\u51e0\u79cd\u5df2\u5efa\u7acb\u7684\u90bb\u57df\u91c7\u6837\u65b9\u6cd5\u53ca\u5176\u5bf9\u5e94\u7684\u540e\u9a8c\u9ad8\u65af\u8fc7\u7a0b\u3002", "result": "\u5728\u6709\u9650\u6837\u672c\u4e0b\uff0c\u4e0d\u540c\u91c7\u6837\u65b9\u6cd5\u7684\u540e\u9a8c\u5206\u5e03\u5404\u4e0d\u76f8\u540c\uff0c\u4f46\u968f\u7740\u6837\u672c\u91cf\u589e\u52a0\u4f1a\u6536\u655b\u5230\u76f8\u540c\u7684\u5206\u5e03\u3002\u540e\u9a8c\u534f\u65b9\u5dee\uff08\u9884\u6d4b\u8bef\u5dee\u5747\u65b9\u503c\u7684\u4e0b\u754c\uff09\u4e0d\u53ef\u6bd4\u8f83\uff0c\u8fd9\u4e0e\u6ca1\u6709\u91c7\u6837\u65b9\u6cd5\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u89c2\u5bdf\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u90bb\u57df\u91c7\u6837\u65b9\u6cd5\u7684\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u6709\u9650\u6837\u672c\u60c5\u51b5\u4e0b\u4e0d\u540c\u65b9\u6cd5\u4ea7\u751f\u4e0d\u540c\u6548\u679c\uff0c\u4f46\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u6700\u4f18\uff0c\u8fd9\u89e3\u91ca\u4e86\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\u3002"}}
{"id": "2509.22922", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22922", "abs": "https://arxiv.org/abs/2509.22922", "authors": ["Pranjal Naman", "Yogesh Simmhan"], "title": "OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks", "comment": "Extended full-length version of paper that appeared at Euro-Par 2024:\n  \"Optimizing Federated Learning Using Remote Embeddings for Graph Neural\n  Networks\", Pranjal Naman and Yogesh Simmhan, in International European\n  Conference on Parallel and Distributed Computing (Euro-Par), 2024. DOI:\n  https://doi.org/10.1007/978-3-031-69766-1_32", "summary": "Graph Neural Networks (GNNs) have experienced rapid advancements in recent\nyears due to their ability to learn meaningful representations from graph data\nstructures. However, in most real-world settings, such as financial transaction\nnetworks and healthcare networks, this data is localized to different data\nowners and cannot be aggregated due to privacy concerns. Federated Learning\n(FL) has emerged as a viable machine learning approach for training a shared\nmodel that iteratively aggregates local models trained on decentralized data.\nThis addresses privacy concerns while leveraging parallelism. State-of-the-art\nmethods enhance the privacy-respecting convergence accuracy of federated GNN\ntraining by sharing remote embeddings of boundary vertices through a server\n(EmbC). However, they are limited by diminished performance due to large\ncommunication costs. In this article, we propose OptimES, an optimized\nfederated GNN training framework that employs remote neighbourhood pruning,\noverlapping the push of embeddings to the server with local training, and\ndynamic pulling of embeddings to reduce network costs and training time. We\nperform a rigorous evaluation of these strategies for four common graph\ndatasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop\nin per-round accuracy due to the preemptive push of embeddings is out-stripped\nby the reduction in per-round training time for large and dense graphs like\nReddit and Products, converging up to $\\approx 3.5\\times$ faster than EmbC and\ngiving up to $\\approx16\\%$ better accuracy than the default federated GNN\nlearning. While accuracy improvements over default federated GNNs are modest\nfor sparser graphs like Arxiv and Papers, they achieve the target accuracy\nabout $\\approx11\\times$ faster than EmbC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u7684\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6846\u67b6OptimES\uff0c\u901a\u8fc7\u8fdc\u7a0b\u90bb\u5c45\u526a\u679d\u3001\u91cd\u53e0\u63a8\u9001\u5d4c\u5165\u4e0e\u672c\u5730\u8bad\u7ec3\u3001\u52a8\u6001\u62c9\u53d6\u5d4c\u5165\u7b49\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u6570\u636e\u901a\u5e38\u5206\u5e03\u5728\u4e0d\u540c\u7684\u6570\u636e\u6240\u6709\u8005\u4e4b\u95f4\uff0c\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u96c6\u4e2d\u805a\u5408\u3002\u73b0\u6709\u7684\u8054\u90a6GNN\u8bad\u7ec3\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u5171\u4eab\u8fb9\u754c\u9876\u70b9\u7684\u8fdc\u7a0b\u5d4c\u5165\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u4f18\u5316\u7b56\u7565\uff1a1) \u8fdc\u7a0b\u90bb\u5c45\u526a\u679d\u51cf\u5c11\u901a\u4fe1\u91cf\uff1b2) \u5c06\u5d4c\u5165\u63a8\u9001\u5230\u670d\u52a1\u5668\u7684\u64cd\u4f5c\u4e0e\u672c\u5730\u8bad\u7ec3\u91cd\u53e0\u8fdb\u884c\uff1b3) \u52a8\u6001\u62c9\u53d6\u5d4c\u5165\u673a\u5236\u3002\u5728\u56db\u4e2a\u5927\u578b\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5bf9\u4e8e\u5927\u578b\u5bc6\u96c6\u56fe\uff08\u5982Reddit\u548cProducts\uff09\uff0c\u6536\u655b\u901f\u5ea6\u6bd4EmbC\u5feb\u7ea63.5\u500d\uff0c\u6bd4\u9ed8\u8ba4\u8054\u90a6GNN\u5b66\u4e60\u51c6\u786e\u7387\u63d0\u9ad8\u7ea616%\u3002\u5bf9\u4e8e\u7a00\u758f\u56fe\uff08\u5982Arxiv\u548cPapers\uff09\uff0c\u867d\u7136\u51c6\u786e\u7387\u63d0\u5347\u6709\u9650\uff0c\u4f46\u8fbe\u5230\u76ee\u6807\u51c6\u786e\u5ea6\u7684\u901f\u5ea6\u6bd4EmbC\u5feb\u7ea611\u500d\u3002", "conclusion": "OptimES\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.22881", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22881", "abs": "https://arxiv.org/abs/2509.22881", "authors": ["Karim Khamaisi", "Nicolas Keller", "Stefan Krummenacher", "Valentin Huber", "Bernhard F\u00e4ssler", "Bruno Rodrigues"], "title": "From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants", "comment": null, "summary": "In the context of industrial factories and energy producers, unplanned\noutages are highly costly and difficult to service. However, existing\nacoustic-anomaly detection studies largely rely on generic industrial or\nsynthetic datasets, with few focused on hydropower plants due to limited\naccess. This paper presents a comparative analysis of acoustic-based anomaly\ndetection methods, as a way to improve predictive maintenance in hydropower\nplants. We address key challenges in the acoustic preprocessing under highly\nnoisy conditions before extracting time- and frequency-domain features. Then,\nwe benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which\nare tested on two real-world datasets from the Rodundwerk II pumped-storage\nplant in Austria, one with induced anomalies and one with real-world\nconditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC\n0.966-0.998) and minimal training time, while the LSTM autoencoder delivered\nstrong detection (ROC AUC 0.889-0.997) at the expense of higher computational\ncost.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6c34\u7535\u7ad9\u58f0\u5b66\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u5728\u5f3a\u566a\u58f0\u6761\u4ef6\u4e0b\u9884\u5904\u7406\u58f0\u5b66\u6570\u636e\u5e76\u63d0\u53d6\u65f6\u9891\u57df\u7279\u5f81\uff0c\u6bd4\u8f83\u4e86LSTM AE\u3001K-Means\u548cOC-SVM\u4e09\u79cd\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5de5\u4e1a\u5de5\u5382\u548c\u80fd\u6e90\u751f\u4ea7\u5546\u4e2d\u975e\u8ba1\u5212\u505c\u673a\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u7ef4\u62a4\uff0c\u73b0\u6709\u58f0\u5b66\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u5927\u591a\u4f9d\u8d56\u901a\u7528\u5de5\u4e1a\u6216\u5408\u6210\u6570\u636e\u96c6\uff0c\u9488\u5bf9\u6c34\u7535\u7ad9\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5728\u5f3a\u566a\u58f0\u6761\u4ef6\u4e0b\u8fdb\u884c\u58f0\u5b66\u9884\u5904\u7406\uff0c\u63d0\u53d6\u65f6\u9891\u57df\u7279\u5f81\uff0c\u5e76\u57fa\u51c6\u6d4b\u8bd5LSTM AE\u3001K-Means\u548cOC-SVM\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528\u5965\u5730\u5229Rodundwerk II\u62bd\u6c34\u84c4\u80fd\u7535\u7ad9\u7684\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "One-Class SVM\u5728\u51c6\u786e\u7387\uff08ROC AUC 0.966-0.998\uff09\u548c\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff0cLSTM\u81ea\u7f16\u7801\u5668\u68c0\u6d4b\u6027\u80fd\u5f3a\uff08ROC AUC 0.889-0.997\uff09\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "OC-SVM\u662f\u6c34\u7535\u7ad9\u58f0\u5b66\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9e\u7528\u9009\u62e9\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u800cLSTM AE\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002"}}
{"id": "2509.23013", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23013", "abs": "https://arxiv.org/abs/2509.23013", "authors": ["Varad Kulkarni", "Nikhil Reddy", "Tuhin Khare", "Abhinandan S. Prasad", "Chitra Babu", "Yogesh Simmhan"], "title": "Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly", "comment": null, "summary": "Function-as-a-service (FaaS) is a popular serverless computing paradigm for\ndeveloping event-driven functions that elastically scale on public clouds. FaaS\nworkflows, such as AWS Step Functions and Azure Durable Functions, are composed\nfrom FaaS functions, like AWS Lambda and Azure Functions, to build practical\napplications. But, the complex interactions between functions in the workflow\nand the limited visibility into the internals of proprietary FaaS platforms are\nmajor impediments to gaining a deeper understanding of FaaS workflow platforms.\nWhile several works characterize FaaS platforms to derive such insights, there\nis a lack of a principled and rigorous study for FaaS workflow platforms, which\nhave unique scaling, performance and costing behavior influenced by the\nplatform design, dataflow and workloads. In this article, we perform extensive\nevaluations of three popular FaaS workflow platforms from AWS and Azure,\nrunning 25 micro-benchmark and application workflows over 132k invocations. Our\ndetailed analysis confirms some conventional wisdom but also uncovers unique\ninsights on the function execution, workflow orchestration, inter-function\ninteractions, cold-start scaling and monetary costs. Our observations help\ndevelopers better configure and program these platforms, set performance and\nscalability expectations, and identify research gaps on enhancing the\nplatforms.", "AI": {"tldr": "\u5bf9AWS\u548cAzure\u7684\u4e09\u79cd\u6d41\u884cFaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u901a\u8fc713.2\u4e07\u6b21\u8c03\u7528\u8fd0\u884c25\u4e2a\u5fae\u57fa\u51c6\u548c\u5e94\u7528\u5de5\u4f5c\u6d41\uff0c\u63ed\u793a\u4e86\u51fd\u6570\u6267\u884c\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u51b7\u542f\u52a8\u6269\u5c55\u548c\u6210\u672c\u7b49\u65b9\u9762\u7684\u72ec\u7279\u89c1\u89e3\u3002", "motivation": "FaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\uff08\u5982AWS Step Functions\u548cAzure Durable Functions\uff09\u7684\u590d\u6742\u51fd\u6570\u4ea4\u4e92\u548c\u4e13\u6709\u5e73\u53f0\u5185\u90e8\u53ef\u89c1\u6027\u6709\u9650\uff0c\u963b\u788d\u4e86\u5bf9\u8fd9\u4e9b\u5e73\u53f0\u7684\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u6765\u4e86\u89e3\u5176\u72ec\u7279\u7684\u6269\u5c55\u3001\u6027\u80fd\u548c\u6210\u672c\u7279\u6027\u3002", "method": "\u5bf9AWS\u548cAzure\u7684\u4e09\u79cd\u6d41\u884cFaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8fd0\u884c25\u4e2a\u5fae\u57fa\u51c6\u548c\u5e94\u7528\u5de5\u4f5c\u6d41\uff0c\u5171\u8ba1132,000\u6b21\u8c03\u7528\uff0c\u8be6\u7ec6\u5206\u6790\u51fd\u6570\u6267\u884c\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u51fd\u6570\u95f4\u4ea4\u4e92\u7b49\u5173\u952e\u65b9\u9762\u3002", "result": "\u7814\u7a76\u65e2\u8bc1\u5b9e\u4e86\u4e00\u4e9b\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e5f\u53d1\u73b0\u4e86\u5173\u4e8e\u51fd\u6570\u6267\u884c\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u51fd\u6570\u95f4\u4ea4\u4e92\u3001\u51b7\u542f\u52a8\u6269\u5c55\u548c\u8d27\u5e01\u6210\u672c\u7684\u72ec\u7279\u89c1\u89e3\uff0c\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u8005\u66f4\u597d\u5730\u914d\u7f6e\u548c\u7f16\u7a0b\u8fd9\u4e9b\u5e73\u53f0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u914d\u7f6e\u548c\u7f16\u7a0b\u8fd9\u4e9b\u5e73\u53f0\u7684\u6307\u5bfc\uff0c\u5e2e\u52a9\u8bbe\u5b9a\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u9884\u671f\uff0c\u5e76\u8bc6\u522b\u4e86\u589e\u5f3a\u8fd9\u4e9b\u5e73\u53f0\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2509.22907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22907", "abs": "https://arxiv.org/abs/2509.22907", "authors": ["Anutam Srinivasan", "Aditya T. Vadlamani", "Amin Meghrazi", "Srinivasan Parthasarathy"], "title": "FedCF: Fair Federated Conformal Prediction", "comment": "Preprint", "summary": "Conformal Prediction (CP) is a widely used technique for quantifying\nuncertainty in machine learning models. In its standard form, CP offers\nprobabilistic guarantees on the coverage of the true label, but it is agnostic\nto sensitive attributes in the dataset. Several recent works have sought to\nincorporate fairness into CP by ensuring conditional coverage guarantees across\ndifferent subgroups. One such method is Conformal Fairness (CF). In this work,\nwe extend the CF framework to the Federated Learning setting and discuss how we\ncan audit a federated model for fairness by analyzing the fairness-related gaps\nfor different demographic groups. We empirically validate our framework by\nconducting experiments on several datasets spanning multiple domains, fully\nleveraging the exchangeability assumption.", "AI": {"tldr": "\u5c06Conformal Fairness\u6846\u67b6\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u5dee\u8ddd\u6765\u5ba1\u8ba1\u8054\u90a6\u6a21\u578b\u7684\u516c\u5e73\u6027", "motivation": "\u6807\u51c6Conformal Prediction\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u5ffd\u7565\u4e86\u654f\u611f\u5c5e\u6027\uff0c\u9700\u8981\u5c06\u516c\u5e73\u6027\u7eb3\u5165CP\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d", "method": "\u6269\u5c55Conformal Fairness\u6846\u67b6\u5230\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5229\u7528\u53ef\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u5dee\u8ddd\u6765\u5ba1\u8ba1\u6a21\u578b", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u6210\u529f\u5c06\u516c\u5e73\u6027\u8003\u8651\u6574\u5408\u5230\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u4e3a\u8054\u90a6\u6a21\u578b\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.23241", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23241", "abs": "https://arxiv.org/abs/2509.23241", "authors": ["Ankita Dutta", "Nabendu Chaki", "Rajat K. De"], "title": "Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed", "comment": null, "summary": "High resource requirement for Deep Neural Network (DNN) training across\nmultiple GPUs necessitates development of various parallelism techniques. In\nthis paper, we introduce two interconnected DNN training frameworks, namely,\nV-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model\nparallelism. V-TiMePReSt is a completely staleness-free system which enables\nthe DNNs to be trained on the latest updated weights in each stage of all\nforward and backward passes. Developing staleness-aware systems at the expense\nof weight stashing reduces GPU-memory consumption, however, increases the\nnumber of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a\nstaleness-aware system, but not at the expense of weight stashing. It does not\nrely solely on the stale weights or the latest updated weights. I-TiMePReSt\ncomputes an intermediate weight towards the latter and performs backward pass\non it. Additionally, we formulate the significance of the stale weights\nmathematically depending on the degree of staleness. In contrast to\nV-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have\na significant contribution in training, which can be quantified mathematically\nbased on the degree of staleness, although there are other contributory factors\nwhich should not be ignored. Experimental results show that V-TiMePReSt is\nadvantageous over existing models in terms of $1)$ the extent of staleness of\nthe weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt\nis superior in terms of $1)$ removing staleness of the weight parameters\nwithout removing weight stashing and $2)$ maintaining the trade-off between GPU\nmemory consumption and convergence speed (number of epochs).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u6d41\u6c34\u7ebf\u5e76\u884c\u7684DNN\u8bad\u7ec3\u6846\u67b6\uff1aV-TiMePReSt\u548cI-TiMePReSt\u3002V-TiMePReSt\u662f\u5b8c\u5168\u65e0\u9648\u65e7\u6027\u7684\u7cfb\u7edf\uff0c\u5728\u6240\u6709\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u9636\u6bb5\u4f7f\u7528\u6700\u65b0\u6743\u91cd\uff1bI-TiMePReSt\u662f\u9648\u65e7\u611f\u77e5\u7cfb\u7edf\uff0c\u4f46\u4e0d\u4f9d\u8d56\u6743\u91cd\u5b58\u50a8\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e2d\u95f4\u6743\u91cd\u6765\u5e73\u8861\u9648\u65e7\u6743\u91cd\u548c\u6700\u65b0\u6743\u91cd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u591aGPU\u8bad\u7ec3\u4e2dDNN\u7684\u9ad8\u8d44\u6e90\u9700\u6c42\u95ee\u9898\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u5e76\u884c\u8bad\u7ec3\u6280\u672f\uff0c\u7279\u522b\u5173\u6ce8\u6d41\u6c34\u7ebf\u5e76\u884c\u4e2d\u7684\u6743\u91cd\u9648\u65e7\u6027\u95ee\u9898\u3002", "method": "V-TiMePReSt\uff1a\u5b8c\u5168\u65e0\u9648\u65e7\u6027\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u7cfb\u7edf\uff0c\u786e\u4fdd\u6240\u6709\u9636\u6bb5\u4f7f\u7528\u6700\u65b0\u6743\u91cd\uff1bI-TiMePReSt\uff1a\u9648\u65e7\u611f\u77e5\u7cfb\u7edf\uff0c\u57fa\u4e8e\u9648\u65e7\u7a0b\u5ea6\u6570\u5b66\u91cf\u5316\u9648\u65e7\u6743\u91cd\u7684\u8d21\u732e\uff0c\u8ba1\u7b97\u4e2d\u95f4\u6743\u91cd\u8fdb\u884c\u540e\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cV-TiMePReSt\u5728\u6743\u91cd\u53c2\u6570\u9648\u65e7\u7a0b\u5ea6\u548cGPU\u5185\u5b58\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff1bI-TiMePReSt\u5728\u4e0d\u79fb\u9664\u6743\u91cd\u5b58\u50a8\u7684\u60c5\u51b5\u4e0b\u6d88\u9664\u6743\u91cd\u53c2\u6570\u9648\u65e7\u6027\uff0c\u5e76\u5728GPU\u5185\u5b58\u6d88\u8017\u548c\u6536\u655b\u901f\u5ea6\u4e4b\u95f4\u4fdd\u6301\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u4e24\u79cd\u6846\u67b6\u5404\u6709\u4f18\u52bf\uff1aV-TiMePReSt\u63d0\u4f9b\u5b8c\u5168\u65e0\u9648\u65e7\u6027\u7684\u8bad\u7ec3\uff0cI-TiMePReSt\u901a\u8fc7\u6570\u5b66\u91cf\u5316\u9648\u65e7\u6743\u91cd\u8d21\u732e\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5185\u5b58-\u6536\u655b\u5e73\u8861\uff0c\u4e3aDNN\u5e76\u884c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22913", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22913", "abs": "https://arxiv.org/abs/2509.22913", "authors": ["Jake S. Rhodes", "Adam G. Rustad", "Marshall S. Nielsen", "Morgan Chase McClellan", "Dallan Gardner", "Dawson Hedges"], "title": "Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders", "comment": "10 pages, 4 figures, 7 tables. Accepted at the MMAI workshop at ICDM,\n  2025", "summary": "Manifold alignment (MA) involves a set of techniques for learning shared\nrepresentations across domains, yet many traditional MA methods are incapable\nof performing out-of-sample extension, limiting their real-world applicability.\nWe propose a guided representation learning framework leveraging a\ngeometry-regularized twin autoencoder (AE) architecture to enhance MA while\nenabling generalization to unseen data. Our method enforces structured\ncross-modal mappings to maintain geometric fidelity in learned embeddings. By\nincorporating a pre-trained alignment model and a multitask learning\nformulation, we improve cross-domain generalization and representation\nrobustness while maintaining alignment fidelity. We evaluate our approach using\nseveral MA methods, showing improvements in embedding consistency, information\npreservation, and cross-domain transfer. Additionally, we apply our framework\nto Alzheimer's disease diagnosis, demonstrating its ability to integrate\nmulti-modal patient data and enhance predictive accuracy in cases limited to a\nsingle domain by leveraging insights from the multi-modal problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u6b63\u5219\u5316\u53cc\u81ea\u7f16\u7801\u5668\u7684\u6d41\u5f62\u5bf9\u9f50\u6846\u67b6\uff0c\u80fd\u591f\u8fdb\u884c\u6837\u672c\u5916\u6269\u5c55\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u63d0\u5347\u8de8\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6d41\u5f62\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u8fdb\u884c\u6837\u672c\u5916\u6269\u5c55\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u6570\u636e\u7684\u6d41\u5f62\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u6b63\u5219\u5316\u53cc\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8de8\u6a21\u6001\u6620\u5c04\u4fdd\u6301\u5d4c\u5165\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u5bf9\u9f50\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u6d41\u5f62\u5bf9\u9f50\u65b9\u6cd5\u4e0a\u8bc4\u4f30\uff0c\u663e\u793a\u5728\u5d4c\u5165\u4e00\u81f4\u6027\u3001\u4fe1\u606f\u4fdd\u7559\u548c\u8de8\u57df\u8fc1\u79fb\u65b9\u9762\u5747\u6709\u6539\u8fdb\u3002\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u6210\u529f\u6574\u5408\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u6d41\u5f62\u5bf9\u9f50\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8868\u793a\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u5355\u6a21\u6001\u60c5\u51b5\u4e0b\u901a\u8fc7\u591a\u6a21\u6001\u95ee\u9898\u6d1e\u5bdf\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2509.23324", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23324", "abs": "https://arxiv.org/abs/2509.23324", "authors": ["Zixu Hao", "Jianyu Wei", "Tuowei Wang", "Minxing Huang", "Huiqiang Jiang", "Shiqi Jiang", "Ting Cao", "Ju Ren"], "title": "Scaling LLM Test-Time Compute with Mobile NPU on Smartphones", "comment": null, "summary": "Deploying Large Language Models (LLMs) on mobile devices faces the challenge\nof insufficient performance in smaller models and excessive resource\nconsumption in larger ones. This paper highlights that mobile Neural Processing\nUnits (NPUs) have underutilized computational resources, particularly their\nmatrix multiplication units, during typical LLM inference. To leverage this\nwasted compute capacity, we propose applying parallel test-time scaling\ntechniques on mobile NPUs to enhance the performance of smaller LLMs. However,\nthis approach confronts inherent NPU challenges, including inadequate hardware\nsupport for fine-grained quantization and low efficiency in general-purpose\ncomputations. To overcome these, we introduce two key techniques: a\nhardware-aware tile quantization scheme that aligns group quantization with NPU\nmemory access patterns, and efficient LUT-based replacements for complex\noperations such as Softmax and dequantization. We design and implement an\nend-to-end inference system that leverages the NPU's compute capability to\nsupport test-time scaling on Qualcomm Snapdragon platforms. Experiments show\nour approach brings significant speedups: up to 19.0 for mixed-precision GEMM\nand 2.2 for Softmax. More importantly, we demonstrate that smaller models using\ntest-time scaling can match or exceed the accuracy of larger models, achieving\na new performance-cost Pareto frontier.", "AI": {"tldr": "\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u7684\u6743\u8861\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u5229\u7528\u79fb\u52a8NPU\u7684\u672a\u5145\u5206\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u901a\u8fc7\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u786c\u4ef6\u611f\u77e5\u7684\u91cf\u5316\u65b9\u6848\u548c\u9ad8\u6548\u64cd\u4f5c\u66ff\u6362\uff0c\u5728Qualcomm Snapdragon\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72LLM\u65f6\uff0c\u5c0f\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u800c\u5927\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u8fc7\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u79fb\u52a8NPU\u5728\u5178\u578bLLM\u63a8\u7406\u4e2d\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u672a\u5145\u5206\u5229\u7528\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u77e9\u9635\u4e58\u6cd5\u5355\u5143\u3002", "method": "1. \u5e94\u7528\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\u5229\u7528NPU\u6d6a\u8d39\u7684\u8ba1\u7b97\u80fd\u529b\uff1b2. \u786c\u4ef6\u611f\u77e5\u7684\u74e6\u7247\u91cf\u5316\u65b9\u6848\uff0c\u4f7f\u7ec4\u91cf\u5316\u4e0eNPU\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u5bf9\u9f50\uff1b3. \u4f7f\u7528\u57fa\u4e8eLUT\u7684\u9ad8\u6548\u66ff\u6362\u65b9\u6848\u5904\u7406\u590d\u6742\u64cd\u4f5c\u5982Softmax\u548c\u53cd\u91cf\u5316\uff1b4. \u8bbe\u8ba1\u7aef\u5230\u7aef\u63a8\u7406\u7cfb\u7edf\u652f\u6301\u6d4b\u8bd5\u65f6\u7f29\u653e\u3002", "result": "\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\uff1a\u6df7\u5408\u7cbe\u5ea6GEMM\u6700\u9ad8\u52a0\u901f19.0\u500d\uff0cSoftmax\u52a0\u901f2.2\u500d\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u5c0f\u6a21\u578b\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u5927\u6a21\u578b\u7684\u51c6\u786e\u5ea6\uff0c\u5b9e\u73b0\u4e86\u65b0\u7684\u6027\u80fd-\u6210\u672c\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "\u901a\u8fc7\u5145\u5206\u5229\u7528\u79fb\u52a8NPU\u7684\u672a\u4f7f\u7528\u8ba1\u7b97\u80fd\u529b\uff0c\u7ed3\u5408\u786c\u4ef6\u4f18\u5316\u7684\u91cf\u5316\u65b9\u6848\u548c\u9ad8\u6548\u64cd\u4f5c\u66ff\u6362\uff0c\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684LLM\u63a8\u7406\uff0c\u4f7f\u5c0f\u6a21\u578b\u8fbe\u5230\u5927\u6a21\u578b\u7684\u51c6\u786e\u5ea6\u6c34\u5e73\uff0c\u4e3a\u79fb\u52a8\u7aefLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24425", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.24425", "abs": "https://arxiv.org/abs/2509.24425", "authors": ["Jingtao Zhang", "Yi Liu", "Qi Shen", "Changhong Wang"], "title": "BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification", "comment": null, "summary": "The proliferation of Internet-of-Things (IoT) devices has led to an\nunprecedented volume of multivariate time series (MTS) data, requiring\nefficient and accurate processing for timely decision-making in\nresource-constrained edge environments. Hyperdimensional (HD) computing, with\nits inherent efficiency and parallelizability, has shown promise in\nclassification tasks but struggles to capture complex temporal patterns, while\nTransformers excel at sequence modeling but incur high computational and memory\noverhead. We introduce BiHDTrans, an efficient neurosymbolic binary\nhyperdimensional Transformer that integrates self-attention into the HD\ncomputing paradigm, unifying the representational efficiency of HD computing\nwith the temporal modeling power of Transformers. Empirically, BiHDTrans\noutperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and\nachieves 6.67% higher accuracy on average than SOTA binary Transformers. With\nhardware acceleration on FPGA, our pipelined implementation leverages the\nindependent and identically distributed properties of high-dimensional\nrepresentations, delivering 39.4 times lower inference latency than SOTA binary\nTransformers. Theoretical analysis shows that binarizing in holographic\nhigh-dimensional space incurs significantly less information distortion than\ndirectly binarizing neural networks, explaining BiHDTrans's superior accuracy.\nFurthermore, dimensionality experiments confirm that BiHDTrans remains\ncompetitive even with a 64% reduction in hyperspace dimensionality, surpassing\nSOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as\nwell as further reducing the latency by 49.8% compare to the full-dimensional\nbaseline. Together, these contributions bridge the gap between the\nexpressiveness of Transformers and the efficiency of HD computing, enabling\naccurate, scalable, and low-latency MTS classification.", "AI": {"tldr": "BiHDTrans\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u4e8c\u5143\u8d85\u7ef4\u53d8\u6362\u5668\uff0c\u5c06\u81ea\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u5230HD\u8ba1\u7b97\u8303\u5f0f\u4e2d\uff0c\u7ed3\u5408\u4e86HD\u8ba1\u7b97\u7684\u9ad8\u6548\u8868\u793a\u548c\u53d8\u6362\u5668\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u6269\u5c55\u548c\u4f4e\u5ef6\u8fdf\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u4ea7\u751f\u5927\u91cf\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u6548\u51c6\u786e\u5904\u7406\u3002\u73b0\u6709HD\u8ba1\u7b97\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u9ad8\u6548\u4f46\u96be\u4ee5\u6355\u6349\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\uff0c\u800c\u53d8\u6362\u5668\u64c5\u957f\u5e8f\u5217\u5efa\u6a21\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faBiHDTrans\u6a21\u578b\uff0c\u5728HD\u8ba1\u7b97\u8303\u5f0f\u4e2d\u96c6\u6210\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528\u5168\u606f\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e8c\u503c\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4fe1\u606f\u5931\u771f\u3002", "result": "BiHDTrans\u6bd4\u6700\u5148\u8fdb\u7684HD\u8ba1\u7b97\u6a21\u578b\u51c6\u786e\u7387\u81f3\u5c11\u63d0\u9ad814.47%\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u4e8c\u5143\u53d8\u6362\u5668\u5e73\u5747\u51c6\u786e\u7387\u9ad86.67%\u3002\u5728FPGA\u786c\u4ef6\u52a0\u901f\u4e0b\uff0c\u63a8\u7406\u5ef6\u8fdf\u6bd4SOTA\u4e8c\u5143\u53d8\u6362\u5668\u964d\u4f4e39.4\u500d\u3002\u5373\u4f7f\u51cf\u5c1164%\u7684\u8d85\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u4ecd\u6bd4SOTA\u4e8c\u5143\u53d8\u6362\u5668\u51c6\u786e\u7387\u9ad81-2%\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c114.4\u500d\uff0c\u5ef6\u8fdf\u6bd4\u5168\u7ef4\u57fa\u7ebf\u8fdb\u4e00\u6b65\u964d\u4f4e49.8%\u3002", "conclusion": "BiHDTrans\u6210\u529f\u5f25\u5408\u4e86\u53d8\u6362\u5668\u7684\u8868\u8fbe\u80fd\u529b\u548cHD\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u53ef\u6269\u5c55\u548c\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22921", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22921", "abs": "https://arxiv.org/abs/2509.22921", "authors": ["Matthieu Zimmer", "Xiaotong Ji", "Tu Nguyen", "Haitham Bou Ammar"], "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective", "comment": null, "summary": "We introduce a novel approach to large language model (LLM) distillation by\nformulating it as a constrained reinforcement learning problem. While recent\nwork has begun exploring the integration of task-specific rewards into\ndistillation processes, existing methods typically rely on ad-hoc reward\nweighting. We propose a principled optimization framework that maximizes\ntask-specific rewards while constraining the divergence from the teacher model\nto remain below a specified threshold. Our approach adapts constrained state\naugmented reinforcement learning to the distillation setting, introducing a\nmodified reward function that maintains theoretical guarantees of constraint\nsatisfaction without requiring state augmentation or teacher model access\nduring deployment and without the computational overhead of the dual Lagrangian\nmethods. Through extensive experiments on mathematical reasoning tasks, we\ndemonstrate that our method achieves better constraint satisfaction rates and\nbetter reasoning compared to the soft Lagrangian relaxation baselines while\nmaintaining competitive task performance. Our framework provides a\ntheoretically grounded and practically efficient solution for reward-aware\ndistillation in resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u5efa\u6a21\u4e3a\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u540c\u65f6\u7ea6\u675f\u4e0e\u6559\u5e08\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e34\u65f6\u6027\u7684\u5956\u52b1\u52a0\u6743\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\uff0c\u53c8\u80fd\u63a7\u5236\u4e0e\u6559\u5e08\u6a21\u578b\u5dee\u5f02\u7684\u539f\u5219\u6027\u4f18\u5316\u6846\u67b6\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6700\u5927\u5316\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\uff0c\u540c\u65f6\u7ea6\u675f\u4e0e\u6559\u5e08\u6a21\u578b\u7684KL\u6563\u5ea6\u4e0d\u8d85\u8fc7\u6307\u5b9a\u9608\u503c\u3002\u907f\u514d\u4e86\u72b6\u6001\u589e\u5f3a\u548c\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u65b9\u6cd5\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u8f6f\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u7387\u548c\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5956\u52b1\u611f\u77e5\u84b8\u998f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u4e14\u5b9e\u9645\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23384", "abs": "https://arxiv.org/abs/2509.23384", "authors": ["Yue Zhang", "Yuansheng Chen", "Xuan Mo", "Alex Xi", "Jialun Li", "WeiGang Wu"], "title": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving", "comment": null, "summary": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose SynergySched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of SynergySched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nSynergySched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy SynergySched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment.", "AI": {"tldr": "SynergySched\u662f\u4e00\u4e2a\u8de8\u5c42LLM\u63a8\u7406\u670d\u52a1\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u7f16\u6392\u89e3\u51b3\u4f20\u7edf\u4e24\u5c42\u67b6\u6784\u4e2d\u7684\u4fe1\u606f\u9e3f\u6c9f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347SLO\u8fbe\u6210\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edfLLM\u63a8\u7406\u670d\u52a1\u7684\u4e24\u5c42\u67b6\u6784\u5b58\u5728\u4fe1\u606f\u9e3f\u6c9f\uff1a\u96c6\u7fa4\u5c42\u8def\u7531\u5668\u4f9d\u8d56\u6ede\u540e\u6307\u6807\u5bfc\u81f4\u51b3\u7b56\u5ef6\u8fdf\uff0c\u5f15\u64ce\u5c42\u9759\u6001\u8c03\u5ea6\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5bfc\u81f4SLO\u8fdd\u89c4\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u611f\u77e5\u7684\u5728\u7ebf\u6027\u80fd\u6a21\u578b\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u9010\u6b65\u9aa4\u5ef6\u8fdf\u548c\u5bb9\u91cf\u9884\u6d4b\u3002\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5f15\u64ce\u5c42\u7684LENS\u8fdb\u884cSLO\u611f\u77e5\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u96c6\u7fa4\u5c42\u7684PRISM\u6267\u884c\u72b6\u6001\u9a71\u52a8\u8def\u7531\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5f02\u6784\u573a\u666f\u4e2d\uff0cSLO\u8fbe\u6210\u7387\u5e73\u5747\u63d0\u534743%\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53473\u500d\uff0c\u5e76\u5728FlowGPT\u751f\u4ea7\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "SynergySched\u901a\u8fc7\u8de8\u5c42\u9884\u6d4b\u6027\u7f16\u6392\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u63a8\u7406\u670d\u52a1\u4e2d\u7684\u7cfb\u7edf\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u53cd\u5e94\u5f0f\u8d1f\u8f7d\u5747\u8861\u5230\u9884\u6d4b\u5f0f\u7f16\u6392\u7684\u8f6c\u53d8\u3002"}}
{"id": "2509.22931", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22931", "abs": "https://arxiv.org/abs/2509.22931", "authors": ["Shreyas Gokhale"], "title": "MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints", "comment": "16 pages, 7 figures", "summary": "Learning high-quality, robust, efficient, and disentangled representations is\na central challenge in artificial intelligence (AI). Deep metric learning\nframeworks tackle this challenge primarily using architectural and optimization\nconstraints. Here, we introduce a third approach that instead relies on\n$\\textit{functional}$ constraints. Specifically, we present MonoCon, a simple\nframework that uses a small monotonic multi-layer perceptron (MLP) head\nattached to any pre-trained encoder. Due to co-adaptation between encoder and\nhead guided by contrastive loss and monotonicity constraints, MonoCon learns\nrobust, disentangled, and highly compact embeddings at a practically negligible\nperformance cost. On the CIFAR-100 image classification task, MonoCon yields\nrepresentations that are nearly 9x more compact and 1.5x more robust than the\nfine-tuned encoder baseline, while retaining 99\\% of the baseline's 5-NN\nclassification accuracy. We also report a 3.4x more compact and 1.4x more\nrobust representation on an SNLI sentence similarity task for a marginal\nreduction in the STSb score, establishing MonoCon as a general domain-agnostic\nframework. Crucially, these robust, ultra-compact representations learned via\nfunctional constraints offer a unified solution to critical challenges in\ndisparate contexts ranging from edge computing to cloud-scale retrieval.", "AI": {"tldr": "MonoCon\u662f\u4e00\u4e2a\u901a\u8fc7\u529f\u80fd\u6027\u7ea6\u675f\u5b66\u4e60\u9ad8\u8d28\u91cf\u8868\u5f81\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u8c03MLP\u5934\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u548c\u5355\u8c03\u6027\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b09\u500d\u538b\u7f29\u548c1.5\u500d\u9c81\u68d2\u6027\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3AI\u4e2d\u5b66\u4e60\u9ad8\u8d28\u91cf\u3001\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u53ef\u89e3\u8026\u8868\u5f81\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u67b6\u6784\u548c\u4f18\u5316\u7ea6\u675f\uff0c\u672c\u6587\u5f15\u5165\u529f\u80fd\u6027\u7ea6\u675f\u4f5c\u4e3a\u7b2c\u4e09\u79cd\u9014\u5f84\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e0a\u9644\u52a0\u5c0f\u578b\u5355\u8c03\u591a\u5c42\u611f\u77e5\u673a\u5934\uff0c\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u5355\u8c03\u6027\u7ea6\u675f\u5f15\u5bfc\u7f16\u7801\u5668\u4e0e\u5934\u7684\u534f\u540c\u9002\u5e94\u3002", "result": "\u5728CIFAR-100\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8868\u5f81\u6bd4\u5fae\u8c03\u7f16\u7801\u5668\u57fa\u7ebf\u538b\u7f29\u8fd19\u500d\u3001\u9c81\u68d2\u6027\u63d0\u53471.5\u500d\uff0c\u540c\u65f6\u4fdd\u630199%\u76845-NN\u5206\u7c7b\u51c6\u786e\u7387\uff1b\u5728SNLI\u53e5\u5b50\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\uff0c\u8868\u5f81\u538b\u7f293.4\u500d\u3001\u9c81\u68d2\u6027\u63d0\u53471.4\u500d\u3002", "conclusion": "\u901a\u8fc7\u529f\u80fd\u6027\u7ea6\u675f\u5b66\u4e60\u7684\u9c81\u68d2\u8d85\u7d27\u51d1\u8868\u5f81\u4e3a\u4ece\u8fb9\u7f18\u8ba1\u7b97\u5230\u4e91\u89c4\u6a21\u68c0\u7d22\u7b49\u4e0d\u540c\u573a\u666f\u7684\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23419", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23419", "abs": "https://arxiv.org/abs/2509.23419", "authors": ["Asadullah Tariq", "Tariq Qayyum", "Mohamed Adel Serhani", "Farag Sallabi", "Ikbal Taleb", "Ezedin S. Barka"], "title": "Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization", "comment": null, "summary": "Federated Learning (FL) enables participant devices to collaboratively train\ndeep learning models without sharing their data with the server or other\ndevices, effectively addressing data privacy and computational concerns.\nHowever, FL faces a major bottleneck due to high communication overhead from\nfrequent model updates between devices and the server, limiting deployment in\nresource-constrained wireless networks. In this paper, we propose a three-fold\nstrategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less\nimportant features while retaining high-value ones; secondly, Adaptive Gradient\nInnovation and Error Sensitivity-Based Quantization, which dynamically adjusts\nthe quantization level for innovative gradient compression; and thirdly,\nCommunication Frequency Optimization to enhance communication efficiency. We\nevaluated our proposed model's performance through extensive experiments,\nassessing accuracy, loss, and convergence compared to baseline techniques. The\nresults show that our model achieves high communication efficiency in the\nframework while maintaining accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u8054\u90a6\u5b66\u4e60\u901a\u4fe1\u4f18\u5316\u7b56\u7565\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u7279\u5f81\u6d88\u9664\u3001\u52a8\u6001\u68af\u5ea6\u91cf\u5316\u548c\u901a\u4fe1\u9891\u7387\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u901a\u4fe1\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u9891\u7e41\u7684\u6a21\u578b\u66f4\u65b0\u5bfc\u81f4\u9ad8\u901a\u4fe1\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7b56\u7565\uff1a1) \u81ea\u9002\u5e94\u7279\u5f81\u6d88\u9664\u7b56\u7565\u5254\u9664\u4e0d\u91cd\u8981\u7279\u5f81\uff1b2) \u57fa\u4e8e\u68af\u5ea6\u521b\u65b0\u548c\u8bef\u5dee\u654f\u611f\u6027\u7684\u81ea\u9002\u5e94\u91cf\u5316\uff1b3) \u901a\u4fe1\u9891\u7387\u4f18\u5316\u63d0\u5347\u901a\u4fe1\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u901a\u4fe1\u6548\u7387\uff0c\u5728\u51c6\u786e\u7387\u3001\u635f\u5931\u548c\u6536\u655b\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u4e09\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.22935", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22935", "abs": "https://arxiv.org/abs/2509.22935", "authors": ["Aleksandr Dremov", "David Grangier", "Angelos Katharopoulos", "Awni Hannun"], "title": "Compute-Optimal Quantization-Aware Training", "comment": null, "summary": "Quantization-aware training (QAT) is a leading technique for improving the\naccuracy of quantized neural networks. Previous work has shown that decomposing\ntraining into a full-precision (FP) phase followed by a QAT phase yields\nsuperior accuracy compared to QAT alone. However, the optimal allocation of\ncompute between the FP and QAT phases remains unclear. We conduct extensive\nexperiments with various compute budgets, QAT bit widths, and model sizes from\n86.0M to 2.2B to investigate how different QAT durations impact final\nperformance. We demonstrate that, contrary to previous findings, the\nloss-optimal ratio of QAT to FP training increases with the total amount of\ncompute. Moreover, the optimal fraction can be accurately predicted for a wide\nrange of model sizes and quantization widths using the\ntokens-per-parameter-byte statistic. From experimental data, we derive a loss\nscaling law that predicts both optimal QAT ratios and final model performance\nacross different QAT/FP compute allocation strategies and QAT bit widths. We\nuse the scaling law to make further predictions, which we verify\nexperimentally, including which QAT bit width is optimal under a given memory\nconstraint and how QAT accuracy with different bit widths compares to\nfull-precision model accuracy. Additionally, we propose a novel cooldown and\nQAT fusion approach that performs learning rate decay jointly with\nquantization-aware training, eliminating redundant full-precision model updates\nand achieving significant compute savings. These findings provide practical\ninsights into efficient QAT planning and enable the training of higher-quality\nquantized models with the same compute budget.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5316\u611f\u77e5\u8bad\u7ec3(QAT)\u4e2dFP\u9636\u6bb5\u548cQAT\u9636\u6bb5\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7684\u6700\u4f18\u6bd4\u4f8b\uff0c\u53d1\u73b0QAT\u4e0eFP\u8bad\u7ec3\u7684\u6700\u4f18\u6bd4\u4f8b\u968f\u603b\u8ba1\u7b97\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8etokens-per-parameter-byte\u7edf\u8ba1\u7684\u9884\u6d4b\u65b9\u6cd5\u548c\u635f\u5931\u7f29\u653e\u5b9a\u5f8b\u3002", "motivation": "\u867d\u7136\u5148\u524d\u7814\u7a76\u8868\u660e\u5c06\u8bad\u7ec3\u5206\u89e3\u4e3a\u5168\u7cbe\u5ea6(FP)\u9636\u6bb5\u548cQAT\u9636\u6bb5\u6bd4\u5355\u72ec\u4f7f\u7528QAT\u80fd\u83b7\u5f97\u66f4\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u4e24\u4e2a\u9636\u6bb5\u95f4\u6700\u4f18\u8ba1\u7b97\u5206\u914d\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6db5\u76d6\u4e0d\u540c\u8ba1\u7b97\u9884\u7b97\u3001QAT\u4f4d\u5bbd(\u4ece86.0M\u52302.2B)\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u5206\u6790\u4e0d\u540cQAT\u6301\u7eed\u65f6\u95f4\u5bf9\u6700\u7ec8\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u5bfc\u51fa\u635f\u5931\u7f29\u653e\u5b9a\u5f8b\u3002", "result": "\u53d1\u73b0QAT\u4e0eFP\u8bad\u7ec3\u7684\u6700\u4f18\u6bd4\u4f8b\u968f\u603b\u8ba1\u7b97\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u8be5\u6bd4\u4f8b\u53ef\u901a\u8fc7tokens-per-parameter-byte\u7edf\u8ba1\u51c6\u786e\u9884\u6d4b\uff0c\u63d0\u51fa\u7684\u7f29\u653e\u5b9a\u5f8b\u80fd\u9884\u6d4b\u6700\u4f18QAT\u6bd4\u4f8b\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u9ad8\u6548QAT\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u80fd\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u8bad\u7ec3\u66f4\u9ad8\u8d28\u91cf\u7684\u91cf\u5316\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u51b7\u5374\u548cQAT\u878d\u5408\u65b9\u6cd5\u4ee5\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2509.23448", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23448", "abs": "https://arxiv.org/abs/2509.23448", "authors": ["Hao Hao", "Dahlia Malkhi", "Maofan Yin", "Lizan Zhou"], "title": "Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice", "comment": null, "summary": "This paper introduces Lyquor, a decentralized platform that reimagines\nblockchain infrastructure through a service-centric model where nodes\nselectively host smart contracts (called Lyquids) while preserving global\ncomposability. We present three key innovations: (1) Fate-Constrained Ordering\n(FCO), which decouples consensus from execution to enable selective hosting\nwithout sacrificing Layer-1 grade composability; (2) Direct Memory Architecture\n(DMA), which eliminates state access bottlenecks by providing each contract\nwith persistent, byte-addressable virtual memory; and (3) Universal Procedure\nCall (UPC), which enables fault-tolerant, programmable coordination across\ndistributed off-chain computation. Together, these components are powered by a\nRust-macroed unified programming model where on-chain and off-chain logic\ncoexist seamlessly, supporting both traditional smart contract patterns and\nnovel distributed applications. Lyquor addresses critical limitations in\nexisting systems while maintaining compatibility with Ethereum APIs, offering a\npath toward truly scalable decentralized computation.", "AI": {"tldr": "Lyquor\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u5e73\u53f0\uff0c\u901a\u8fc7\u670d\u52a1\u4e2d\u5fc3\u6a21\u578b\u91cd\u65b0\u6784\u60f3\u533a\u5757\u94fe\u57fa\u7840\u8bbe\u65bd\uff0c\u8282\u70b9\u53ef\u9009\u62e9\u6027\u5730\u6258\u7ba1\u667a\u80fd\u5408\u7ea6\uff08\u79f0\u4e3aLyquids\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u53ef\u7ec4\u5408\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u7279\u522b\u662f\u72b6\u6001\u8bbf\u95ee\u74f6\u9888\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4ee5\u592a\u574aAPI\u7684\u517c\u5bb9\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1aFate-Constrained Ordering\uff08FCO\uff09\u5c06\u5171\u8bc6\u4e0e\u6267\u884c\u89e3\u8026\uff1bDirect Memory Architecture\uff08DMA\uff09\u6d88\u9664\u72b6\u6001\u8bbf\u95ee\u74f6\u9888\uff1bUniversal Procedure Call\uff08UPC\uff09\u5b9e\u73b0\u8de8\u5206\u5e03\u5f0f\u94fe\u4e0b\u8ba1\u7b97\u7684\u5bb9\u9519\u53ef\u7f16\u7a0b\u534f\u8c03\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u4f20\u7edf\u667a\u80fd\u5408\u7ea6\u6a21\u5f0f\u548c\u65b0\u578b\u5206\u5e03\u5f0f\u5e94\u7528\u7684\u65e0\u7f1d\u7edf\u4e00\u7f16\u7a0b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u53ef\u6269\u5c55\u7684\u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97\u3002", "conclusion": "Lyquor\u901a\u8fc7\u5176\u521b\u65b0\u7684\u670d\u52a1\u4e2d\u5fc3\u6a21\u578b\u548c\u4e09\u4e2a\u6838\u5fc3\u6280\u672f\u7ec4\u4ef6\uff0c\u4e3a\u533a\u5757\u94fe\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u517c\u5bb9\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.22938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22938", "abs": "https://arxiv.org/abs/2509.22938", "authors": ["Yanqing Lu", "Letao Wang", "Jinbo Liu"], "title": "Understanding SOAP from the Perspective of Gradient Whitening", "comment": null, "summary": "Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) has recently\nemerged as a promising optimization algorithm for neural network training,\nachieving superior training efficiency over both Adam and Shampoo in language\nmodeling tasks. In this work, we analyze Adam, Shampoo, and SOAP from the\nperspective of gradient whitening, interpreting their preconditioners as\napproximations to the whitening matrix, which captures second-order curvature\ninformation. We further establish a theoretical equivalence between idealized\nversions of SOAP and Shampoo under the Kronecker product assumption. To\nempirically evaluate these insights, we reproduce the language modeling\nexperiments using nanoGPT and grayscale image colorization. Our results show\nthat SOAP exhibits similar convergence rate as Shampoo, and no significant\nadvantage over both Adam and Shampoo in the final loss achieved, which aligns\nwith their equivalence in theory.", "AI": {"tldr": "SOAP\u4f18\u5316\u7b97\u6cd5\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eAdam\u548cShampoo\uff0c\u4f46\u7406\u8bba\u5206\u6790\u663e\u793aSOAP\u4e0eShampoo\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u7b49\u4ef7\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u4e24\u8005\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u635f\u5931\u76f8\u4f3c\u3002", "motivation": "\u4ece\u68af\u5ea6\u767d\u5316\u89d2\u5ea6\u5206\u6790Adam\u3001Shampoo\u548cSOAP\u4f18\u5316\u7b97\u6cd5\uff0c\u7406\u89e3\u5176\u9884\u8c03\u8282\u5668\u4f5c\u4e3a\u767d\u5316\u77e9\u9635\u7684\u8fd1\u4f3c\uff0c\u5e76\u5efa\u7acbSOAP\u4e0eShampoo\u7684\u7406\u8bba\u7b49\u4ef7\u6027\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u767d\u5316\u7406\u8bba\u6846\u67b6\u5206\u6790\u4e09\u79cd\u4f18\u5316\u7b97\u6cd5\uff0c\u5efa\u7acbSOAP\u4e0eShampoo\u5728Kronecker\u4e58\u79ef\u5047\u8bbe\u4e0b\u7684\u7406\u8bba\u7b49\u4ef7\u6027\uff0c\u5e76\u4f7f\u7528nanoGPT\u548c\u7070\u5ea6\u56fe\u50cf\u7740\u8272\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSOAP\u4e0eShampoo\u5177\u6709\u76f8\u4f3c\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5728\u6700\u7ec8\u635f\u5931\u65b9\u9762\u76f8\u6bd4Adam\u548cShampoo\u6ca1\u6709\u663e\u8457\u4f18\u52bf\uff0c\u8fd9\u4e0e\u7406\u8bba\u7b49\u4ef7\u6027\u4e00\u81f4\u3002", "conclusion": "SOAP\u867d\u7136\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7406\u8bba\u4e0a\u4e0eShampoo\u7b49\u4ef7\uff0c\u5b9e\u9645\u6027\u80fd\u5dee\u5f02\u4e0d\u5927\uff0c\u8fd9\u4e3a\u7406\u89e3\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2509.23706", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.23706", "abs": "https://arxiv.org/abs/2509.23706", "authors": ["Bogdan-Ioan Popa", "Adrian-Marius Dumitran", "Livia Magureanu"], "title": "Parallel Algorithms for the One Sided Crossing Minimization Problem", "comment": null, "summary": "The One Sided Crossing Minimization (OSCM) problem is an optimization problem\nin graph drawing that aims to minimize the number of edge crossings in\nbipartite graph layouts. It has practical applications in areas such as network\nvisualization and VLSI (Very Large Scale Integration) design, where reducing\nedge crossings improves the arrangement of circuit components and their\ninterconnections. Despite the rise of multi-core systems, the parallelization\nof exact and fixed-parameter tractable (FPT) algorithms for OSCM remains\nlargely unexplored. Parallel variants offer significant potential for scaling\nto larger graphs but require careful handling of synchronization and memory\nmanagement. In this paper, we explore various previously studied exact and FPT\nalgorithms for OSCM, implementing and analyzing them in both sequential and\nparallel forms. Our main contribution lies in empirically proving that these\nalgorithms can achieve close to linear speedup under parallelization. In\nparticular, our best result achieves a speedup of nearly 19 on a 16-core,\n32-thread machine. We further investigate and discuss the reasons why linear\nspeedup is not always attained.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5355\u8fb9\u4ea4\u53c9\u6700\u5c0f\u5316\u95ee\u9898\u7684\u5e76\u884c\u5316\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u7cbe\u786e\u548c\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7b97\u6cd5\uff0c\u572816\u6838\u673a\u5668\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd119\u500d\u7684\u52a0\u901f\u6bd4\u3002", "motivation": "\u5355\u8fb9\u4ea4\u53c9\u6700\u5c0f\u5316\u95ee\u9898\u5728\u56fe\u5f62\u7ed8\u5236\u548cVLSI\u8bbe\u8ba1\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7cbe\u786e\u548c\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7b97\u6cd5\u7684\u5e76\u884c\u5316\u7814\u7a76\u4e0d\u8db3\uff0c\u5e76\u884c\u5316\u5177\u6709\u6269\u5c55\u5230\u66f4\u5927\u56fe\u7684\u6f5c\u529b\u3002", "method": "\u63a2\u7d22\u4e86\u591a\u79cd\u5148\u524d\u7814\u7a76\u7684\u7cbe\u786e\u548c\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5b83\u4eec\u7684\u987a\u5e8f\u548c\u5e76\u884c\u7248\u672c\uff0c\u5e76\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5b9e\u8bc1\u8bc1\u660e\u8fd9\u4e9b\u7b97\u6cd5\u5728\u5e76\u884c\u5316\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u7ebf\u6027\u7684\u52a0\u901f\u6bd4\uff0c\u572816\u683832\u7ebf\u7a0b\u673a\u5668\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd119\u500d\u7684\u6700\u4f73\u52a0\u901f\u6bd4\u3002", "conclusion": "\u5e76\u884c\u5316\u80fd\u591f\u663e\u8457\u63d0\u5347\u5355\u8fb9\u4ea4\u53c9\u6700\u5c0f\u5316\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4f46\u7ebf\u6027\u52a0\u901f\u6bd4\u5e76\u4e0d\u603b\u662f\u80fd\u591f\u5b9e\u73b0\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u539f\u56e0\u3002"}}
{"id": "2509.22944", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22944", "abs": "https://arxiv.org/abs/2509.22944", "authors": ["Lorenz K. M\u00fcller", "Philippe Bich", "Jiawei Zhuang", "Ahmet \u00c7elik", "Luca Benfenati", "Lukas Cavigelli"], "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights", "comment": null, "summary": "Post-training quantization has emerged as the most widely used strategy for\ndeploying large language models at low precision. Still, current methods show\nperplexity degradation at bit-widths less than or equal to 4, partly because\nrepresenting outliers causes precision issues in parameters that share the same\nscales as these outliers. This problem is especially pronounced for\ncalibration-free, uniform quantization methods. We introduce SINQ to augment\nexisting post-training quantizers with an additional second-axis scale factor\nand a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize\nper-row and per-column variances, thereby minimizing a novel per-matrix proxy\ntarget for quantization: the matrix imbalance. Our method has no interactions\nbetween layers and can be trivially applied to new architectures to quantize\nany linear layers. We evaluate our method on the Qwen3 model family and\nDeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against\nuncalibrated uniform quantization baselines and can be further enhanced by\ncombining it with calibration and non-uniform quantization levels. Code to\nreproduce the results of this work and to easily quantize models using SINQ is\navailable at https://github.com/huawei-csl/SINQ.", "AI": {"tldr": "SINQ\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7b2c\u4e8c\u8f74\u5c3a\u5ea6\u56e0\u5b50\u548c\u5feb\u901fSinkhorn-Knopp\u7b97\u6cd5\u6765\u5f52\u4e00\u5316\u884c\u5217\u65b9\u5dee\uff0c\u89e3\u51b3\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u7684\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u57284\u6bd4\u7279\u53ca\u4ee5\u4e0b\u7cbe\u5ea6\u65f6\u4f1a\u51fa\u73b0\u56f0\u60d1\u5ea6\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5f02\u5e38\u503c\u8868\u793a\u5bfc\u81f4\u5171\u4eab\u76f8\u540c\u5c3a\u5ea6\u7684\u53c2\u6570\u51fa\u73b0\u7cbe\u5ea6\u95ee\u9898\uff0c\u8fd9\u5728\u65e0\u6821\u51c6\u7684\u5747\u5300\u91cf\u5316\u65b9\u6cd5\u4e2d\u5c24\u4e3a\u660e\u663e\u3002", "method": "\u5728\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u5668\u57fa\u7840\u4e0a\u589e\u52a0\u7b2c\u4e8c\u8f74\u5c3a\u5ea6\u56e0\u5b50\uff0c\u4f7f\u7528\u5feb\u901fSinkhorn-Knopp\u7b97\u6cd5\u5bfb\u627e\u5c3a\u5ea6\u6765\u5f52\u4e00\u5316\u6bcf\u884c\u548c\u6bcf\u5217\u7684\u65b9\u5dee\uff0c\u6700\u5c0f\u5316\u77e9\u9635\u4e0d\u5e73\u8861\u8fd9\u4e00\u65b0\u7684\u91cf\u5316\u4ee3\u7406\u76ee\u6807\u3002", "result": "\u5728Qwen3\u6a21\u578b\u5bb6\u65cf\u548cDeepSeek-V2.5\u4e0a\u8bc4\u4f30\uff0cSINQ\u663e\u8457\u6539\u5584\u4e86WikiText2\u548cC4\u7684\u56f0\u60d1\u5ea6\u8868\u73b0\uff0c\u4f18\u4e8e\u65e0\u6821\u51c6\u5747\u5300\u91cf\u5316\u57fa\u7ebf\uff0c\u5e76\u53ef\u8fdb\u4e00\u6b65\u4e0e\u6821\u51c6\u548c\u975e\u5747\u5300\u91cf\u5316\u7ea7\u522b\u7ed3\u5408\u589e\u5f3a\u6548\u679c\u3002", "conclusion": "SINQ\u65b9\u6cd5\u65e0\u9700\u5c42\u95f4\u4ea4\u4e92\uff0c\u53ef\u8f7b\u677e\u5e94\u7528\u4e8e\u65b0\u67b6\u6784\u4ee5\u91cf\u5316\u4efb\u4f55\u7ebf\u6027\u5c42\uff0c\u4e3a\u89e3\u51b3\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u7684\u7cbe\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.23722", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23722", "abs": "https://arxiv.org/abs/2509.23722", "authors": ["Jihu Guo", "Tenghui Ma", "Wei Gao", "Peng Sun", "Jiaxing Li", "Xun Chen", "Yuyang Jin", "Dahua Lin"], "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models", "comment": "13 pages, 15 Figures; Under Review;", "summary": "Pipeline parallelism is widely used to train large language models (LLMs).\nHowever, increasing heterogeneity in model architectures exacerbates pipeline\nbubbles, thereby reducing training efficiency. Existing approaches overlook the\nco-optimization of model partition, model placement, and workload scheduling,\nresulting in limited efficiency improvement or even performance degradation. To\nrespond, we propose AdaPtis, an LLM training system that supports adaptive\npipeline parallelism. First, we develop a pipeline performance model to\naccurately estimate training throughput. Second, AdaPtis jointly optimizes\nmodel partition, model placement, and workload scheduling policies guided by\nthis performance model. Third, we design a unified pipeline executor that\nefficiently supports the execution of diverse pipeline strategies. Extensive\nexperiments show that AdaPtis achieves an average speedup of 1.42x (up to\n2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.", "AI": {"tldr": "AdaPtis\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6d41\u6c34\u7ebf\u5e76\u884c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6a21\u578b\u5206\u533a\u3001\u6a21\u578b\u653e\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6a21\u578b\u5206\u533a\u3001\u6a21\u578b\u653e\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u7684\u534f\u540c\u4f18\u5316\uff0c\u5bfc\u81f4\u6548\u7387\u63d0\u5347\u6709\u9650\u751a\u81f3\u6027\u80fd\u4e0b\u964d\u3002\u6a21\u578b\u67b6\u6784\u7684\u5f02\u6784\u6027\u52a0\u5267\u4e86\u6d41\u6c34\u7ebf\u6c14\u6ce1\u95ee\u9898\u3002", "method": "1) \u5f00\u53d1\u6d41\u6c34\u7ebf\u6027\u80fd\u6a21\u578b\u51c6\u786e\u4f30\u8ba1\u8bad\u7ec3\u541e\u5410\u91cf\uff1b2) \u57fa\u4e8e\u6027\u80fd\u6a21\u578b\u8054\u5408\u4f18\u5316\u6a21\u578b\u5206\u533a\u3001\u6a21\u578b\u653e\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u7b56\u7565\uff1b3) \u8bbe\u8ba1\u7edf\u4e00\u6d41\u6c34\u7ebf\u6267\u884c\u5668\u652f\u6301\u591a\u6837\u5316\u6d41\u6c34\u7ebf\u7b56\u7565\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAdaPtis\u76f8\u6bd4Megatron-LM I-1F1B\u5728\u5404\u79cdLLM\u67b6\u6784\u548c\u89c4\u6a21\u4e0a\u5e73\u5747\u52a0\u901f1.42\u500d\uff0c\u6700\u9ad8\u53ef\u8fbe2.14\u500d\u3002", "conclusion": "AdaPtis\u901a\u8fc7\u81ea\u9002\u5e94\u6d41\u6c34\u7ebf\u5e76\u884c\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2509.22949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22949", "abs": "https://arxiv.org/abs/2509.22949", "authors": ["Hamidreza Moazzami", "Asma Jamali", "Nicholas Kevlahan", "Rodrigo A. Vargas-Hern\u00e1ndez"], "title": "Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation", "comment": "6 pages, 2 figures, Machine Learning and the Physical Sciences\n  Workshop, (NeurIPS 2025)", "summary": "Data assimilation (DA) is crucial for enhancing solutions to partial\ndifferential equations (PDEs), such as those in numerical weather prediction,\nby optimizing initial conditions using observational data. Variational DA\nmethods are widely used in oceanic and atmospheric forecasting, but become\ncomputationally expensive, especially when Hessian information is involved. To\naddress this challenge, we propose a meta-learning framework that employs the\nFourier Neural Operator (FNO) to approximate the inverse Hessian operator\nacross a family of DA problems, thereby providing an effective initialization\nfor the conjugate gradient (CG) method. Numerical experiments on a linear\nadvection equation demonstrate that the resulting FNO-CG approach reduces the\naverage relative error by $62\\%$ and the number of iterations by $17\\%$\ncompared to the standard CG. These improvements are most pronounced in\nill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG\nfor challenging DA problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u6765\u8fd1\u4f3c\u6570\u636e\u540c\u5316\u95ee\u9898\u4e2d\u7684\u9006Hessian\u7b97\u5b50\uff0c\u4e3a\u5171\u8f6d\u68af\u5ea6\u6cd5\u63d0\u4f9b\u6709\u6548\u521d\u59cb\u5316\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u5dee\u548c\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "\u53d8\u5206\u6570\u636e\u540c\u5316\u65b9\u6cd5\u5728\u6d77\u6d0b\u548c\u5927\u6c14\u9884\u62a5\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6d89\u53caHessian\u4fe1\u606f\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u6765\u8fd1\u4f3c\u9006Hessian\u7b97\u5b50\uff0c\u6784\u5efaFNO-CG\u65b9\u6cd5\uff0c\u4e3a\u5171\u8f6d\u68af\u5ea6\u6cd5\u63d0\u4f9b\u521d\u59cb\u5316\u3002", "result": "\u5728\u7ebf\u6027\u5e73\u6d41\u65b9\u7a0b\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0cFNO-CG\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6CG\u65b9\u6cd5\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u51cf\u5c1162%\uff0c\u8fed\u4ee3\u6b21\u6570\u51cf\u5c1117%\uff0c\u5728\u75c5\u6001\u6761\u4ef6\u4e0b\u6539\u8fdb\u6700\u4e3a\u660e\u663e\u3002", "conclusion": "FNO-CG\u65b9\u6cd5\u5bf9\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u540c\u5316\u95ee\u9898\u5177\u6709\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u75c5\u6001\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.24030", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24030", "abs": "https://arxiv.org/abs/2509.24030", "authors": ["Anjus George", "Michael Brim", "Christopher Zimmer", "David Rogers", "Sarp Oral", "Zach Mayes"], "title": "From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures", "comment": null, "summary": "In this paper, we investigate three cross-facility data streaming\narchitectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed\nService Streaming (MSS). We examine their architectural variations in data flow\npaths and deployment feasibility, and detail their implementation using the\nData Streaming to HPC (DS2HPC) architectural framework and the SciStream\nmemory-to-memory streaming toolkit on the production-grade Advanced Computing\nEcosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility\n(OLCF). We present a workflow-specific evaluation of these architectures using\nthree synthetic workloads derived from the streaming characteristics of\nscientific workflows. Through simulated experiments, we measure streaming\nthroughput, round-trip time, and overhead under work sharing, work sharing with\nfeedback, and broadcast and gather messaging patterns commonly found in AI-HPC\ncommunication motifs. Our study shows that DTS offers a minimal-hop path,\nresulting in higher throughput and lower latency, whereas MSS provides greater\ndeployment feasibility and scalability across multiple users but incurs\nsignificant overhead. PRS lies in between, offering a scalable architecture\nwhose performance matches DTS in most cases.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u8de8\u8bbe\u65bd\u6570\u636e\u6d41\u67b6\u6784\uff1a\u76f4\u63a5\u6d41\u5f0f\u4f20\u8f93(DTS)\u3001\u4ee3\u7406\u6d41\u5f0f\u4f20\u8f93(PRS)\u548c\u7ba1\u7406\u670d\u52a1\u6d41\u5f0f\u4f20\u8f93(MSS)\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u8bc4\u4f30\u5b83\u4eec\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u8de8\u8bbe\u65bd\u6570\u636e\u6d41\u67b6\u6784\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u6027\u80fd\u7279\u5f81\uff0c\u4e3aAI-HPC\u901a\u4fe1\u6a21\u5f0f\u63d0\u4f9b\u67b6\u6784\u9009\u62e9\u6307\u5bfc\u3002", "method": "\u4f7f\u7528DS2HPC\u67b6\u6784\u6846\u67b6\u548cSciStream\u5185\u5b58\u5230\u5185\u5b58\u6d41\u5f0f\u5de5\u5177\u5305\uff0c\u5728OLCF\u7684ACE\u57fa\u7840\u8bbe\u65bd\u4e0a\u5b9e\u73b0\u4e09\u79cd\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u4f7f\u7528\u4e09\u4e2a\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u6027\u80fd\u3002", "result": "DTS\u63d0\u4f9b\u6700\u77ed\u8def\u5f84\uff0c\u5177\u6709\u66f4\u9ad8\u541e\u5410\u91cf\u548c\u66f4\u4f4e\u5ef6\u8fdf\uff1bMSS\u90e8\u7f72\u53ef\u884c\u6027\u66f4\u597d\u4e14\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\uff0c\u4f46\u5f00\u9500\u663e\u8457\uff1bPRS\u6027\u80fd\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4e0eDTS\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u6269\u5c55\u67b6\u6784\u3002", "conclusion": "\u4e0d\u540c\u6570\u636e\u6d41\u67b6\u6784\u5728\u6027\u80fd\u3001\u90e8\u7f72\u53ef\u884c\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u67b6\u6784\u3002"}}
{"id": "2509.22953", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22953", "abs": "https://arxiv.org/abs/2509.22953", "authors": ["Valentyn Melnychuk", "Stefan Feuerriegel"], "title": "GDR-learners: Orthogonal Learning of Generative Models for Potential Outcomes", "comment": null, "summary": "Various deep generative models have been proposed to estimate potential\noutcomes distributions from observational data. However, none of them have the\nfavorable theoretical property of general Neyman-orthogonality and, associated\nwith it, quasi-oracle efficiency and double robustness. In this paper, we\nintroduce a general suite of generative Neyman-orthogonal (doubly-robust)\nlearners that estimate the conditional distributions of potential outcomes. Our\nproposed GDR-learners are flexible and can be instantiated with many\nstate-of-the-art deep generative models. In particular, we develop GDR-learners\nbased on (a) conditional normalizing flows (which we call GDR-CNFs), (b)\nconditional generative adversarial networks (GDR-CGANs), (c) conditional\nvariational autoencoders (GDR-CVAEs), and (d) conditional diffusion models\n(GDR-CDMs). Unlike the existing methods, our GDR-learners possess the\nproperties of quasi-oracle efficiency and rate double robustness, and are thus\nasymptotically optimal. In a series of (semi-)synthetic experiments, we\ndemonstrate that our GDR-learners are very effective and outperform the\nexisting methods in estimating the conditional distributions of potential\noutcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u5957\u901a\u7528\u7684\u751f\u6210\u5f0fNeyman\u6b63\u4ea4\uff08\u53cc\u91cd\u7a33\u5065\uff09\u5b66\u4e60\u5668\uff08GDR-learners\uff09\uff0c\u7528\u4e8e\u4f30\u8ba1\u6f5c\u5728\u7ed3\u679c\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u5177\u6709\u51c6\u9884\u8a00\u673a\u6548\u7387\u548c\u53cc\u91cd\u7a33\u5065\u6027\u7b49\u7406\u8bba\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u901a\u7528\u7684Neyman\u6b63\u4ea4\u6027\u53ca\u5176\u76f8\u5173\u7684\u51c6\u9884\u8a00\u673a\u6548\u7387\u548c\u53cc\u91cd\u7a33\u5065\u6027\u7b49\u7406\u8bba\u4f18\u52bf\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u8fd9\u4e9b\u7406\u8bba\u4fdd\u8bc1\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\uff08GDR-CNFs\uff09\u3001\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GDR-CGANs\uff09\u3001\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08GDR-CVAEs\uff09\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08GDR-CDMs\uff09\u7684GDR-learners\u6846\u67b6\u3002", "result": "\u5728\uff08\u534a\uff09\u5408\u6210\u5b9e\u9a8c\u4e2d\uff0cGDR-learners\u5728\u4f30\u8ba1\u6f5c\u5728\u7ed3\u679c\u6761\u4ef6\u5206\u5e03\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GDR-learners\u5177\u6709\u51c6\u9884\u8a00\u673a\u6548\u7387\u548c\u901f\u7387\u53cc\u91cd\u7a33\u5065\u6027\uff0c\u56e0\u6b64\u662f\u6e10\u8fd1\u6700\u4f18\u7684\uff0c\u4e3a\u6f5c\u5728\u7ed3\u679c\u5206\u5e03\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2509.22957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22957", "abs": "https://arxiv.org/abs/2509.22957", "authors": ["Luke Guerdan", "Justin Whitehouse", "Kimberly Truong", "Kenneth Holstein", "Zhiwei Steven Wu"], "title": "Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas", "comment": null, "summary": "As Generative AI (GenAI) systems see growing adoption, a key concern involves\nthe external validity of evaluations, or the extent to which they generalize\nfrom lab-based to real-world deployment conditions. Threats to the external\nvalidity of GenAI evaluations arise when the source sample of human raters and\nsystem outputs used to obtain a system quality estimate differs from the target\ndistribution at deployment time. In this work, we propose a doubly-robust\nestimation framework designed to address this evaluation sampling bias. Key to\nour approach is the use of \"persona\" ratings produced by prompting an LLM\nevaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific\nsociodemographic characteristics. Our doubly-robust framework combines these\ninformative yet imperfect persona ratings with human ratings obtained under\nevaluation sampling bias to produce statistically valid system quality\nestimates. In particular, we show that our approach yields valid system quality\nestimates when either (i) a model trained to predict human ratings using\npersona ratings and source data observed under sampling bias, or (ii) a\nreweighting model that corrects for sampling bias is of sufficient quality. We\nvalidate our framework theoretically and via a novel Persona Simulation\nFramework (PSF) designed to systematically manipulate persona quality and the\ndegree of evaluation sampling bias present in source data. Our work provides a\nprincipled foundation for combining imperfect persona ratings with human\nratings observed under sampling bias to obtain valid system quality estimates.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.24381", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24381", "abs": "https://arxiv.org/abs/2509.24381", "authors": ["Tianyu Guo", "Tianming Xu", "Xianjie Chen", "Junru Chen", "Nong Xiao", "Xianwei Zhang"], "title": "RServe: Overlapping Encoding and Prefill for Efficient LMM Inference", "comment": null, "summary": "Large multimodal models (LMMs) typically employ an encoding module to\ntransform multimodal data inputs into embeddings, which are then fed to\nlanguage models for further processing. However, efficiently serving LMMs\nremains highly challenging due to the inherent complexity of their inference\npipelines. Traditional serving engines co-locate the encoding module and the\nlanguage model, leading to significant resource interference and tight data\ndependency. Recent studies have alleviated this issue by disaggregating the\nencoding module from the model, following a design style of prefill-decode\ndisaggregation. Nevertheless, these approaches fail to fully exploit\nparallelism both within individual requests (intra-request) and across multiple\nrequests (inter-request).\n  To overcome the limitation, we propose REDServe, an LMM inference system that\nefficiently orchestrates intra- and inter-request pipelines. REDServe is\ndesigned to reduce low latency and maximize parallelism at both intra- and\ninter-request granularities. Built on the disaggregated architecture of the\nencoding module and language model, REDServe adopts a fine-grained scheduling\nmethod that overlaps multimodal encoding with the forward computation of the\nlanguage model within a single request. For inter-request pipeline, REDServe\nleverages schedulable tokens and token budgets to balance computational loads\nacross micro-batches. Combined with chunked prefill, this enables a novel\nscheduling strategy that coordinates the execution of intra- and inter-request\npipelines. Experimental evaluations on representative LMMs show that REDServe\nachieves substantial latency reduction of up to 66% while improving throughput\nby up to 109%, significantly outperforming existing serving approaches.", "AI": {"tldr": "REDServe\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u7f16\u7801\u6a21\u5757\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6cd5\u5728\u5355\u8bf7\u6c42\u5185\u91cd\u53e0\u591a\u6a21\u6001\u7f16\u7801\u4e0e\u8bed\u8a00\u6a21\u578b\u524d\u5411\u8ba1\u7b97\uff0c\u5e76\u5728\u591a\u8bf7\u6c42\u95f4\u4f7f\u7528\u53ef\u8c03\u5ea6token\u548ctoken\u9884\u7b97\u6765\u5e73\u8861\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u670d\u52a1\u5f15\u64ce\u5c06\u7f16\u7801\u6a21\u5757\u548c\u8bed\u8a00\u6a21\u578b\u5171\u7f6e\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u8d44\u6e90\u5e72\u6270\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u73b0\u6709\u7684\u89e3\u8026\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8bf7\u6c42\u5185\u548c\u8bf7\u6c42\u95f4\u7684\u5e76\u884c\u6027\u3002", "method": "\u57fa\u4e8e\u89e3\u8026\u67b6\u6784\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6cd5\uff1a\u5728\u5355\u8bf7\u6c42\u5185\u91cd\u53e0\u591a\u6a21\u6001\u7f16\u7801\u4e0e\u8bed\u8a00\u6a21\u578b\u524d\u5411\u8ba1\u7b97\uff1b\u5728\u591a\u8bf7\u6c42\u95f4\u4f7f\u7528\u53ef\u8c03\u5ea6token\u548ctoken\u9884\u7b97\u5e73\u8861\u8ba1\u7b97\u8d1f\u8f7d\uff1b\u7ed3\u5408\u5206\u5757\u9884\u586b\u5145\u5b9e\u73b0\u65b0\u9896\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cREDServe\u5b9e\u73b0\u4e86\u9ad8\u8fbe66%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c109%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u670d\u52a1\u65b9\u6cd5\u3002", "conclusion": "REDServe\u901a\u8fc7\u6709\u6548\u534f\u8c03\u8bf7\u6c42\u5185\u548c\u8bf7\u6c42\u95f4\u6d41\u6c34\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u670d\u52a1\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.22963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22963", "abs": "https://arxiv.org/abs/2509.22963", "authors": ["Haitong Ma", "Ofir Nabati", "Aviv Rosenberg", "Bo Dai", "Oran Lang", "Idan Szpektor", "Craig Boutilier", "Na Li", "Shie Mannor", "Lior Shani", "Guy Tenneholtz"], "title": "Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces", "comment": "22 pages, 10 figures. Haitong Ma and Ofir Nabati contributed equally\n  to this paper", "summary": "Reinforcement learning (RL) struggles to scale to large, combinatorial action\nspaces common in many real-world problems. This paper introduces a novel\nframework for training discrete diffusion models as highly effective policies\nin these complex settings. Our key innovation is an efficient online training\nprocess that ensures stable and effective policy improvement. By leveraging\npolicy mirror descent (PMD) to define an ideal, regularized target policy\ndistribution, we frame the policy update as a distributional matching problem,\ntraining the expressive diffusion model to replicate this stable target. This\ndecoupled approach stabilizes learning and significantly enhances training\nperformance. Our method achieves state-of-the-art results and superior sample\nefficiency across a diverse set of challenging combinatorial benchmarks,\nincluding DNA sequence generation, RL with macro-actions, and multi-agent\nsystems. Experiments demonstrate that our diffusion policies attain superior\nperformance compared to other baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u79bb\u6563\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u5b9a\u4e49\u6b63\u5219\u5316\u76ee\u6807\u7b56\u7565\u5206\u5e03\uff0c\u5c06\u7b56\u7565\u66f4\u65b0\u8f6c\u5316\u4e3a\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u5728\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u5927\u89c4\u6a21\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u8fd9\u7c7b\u590d\u6742\u8bbe\u7f6e\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\uff0c\u901a\u8fc7\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u5b9a\u4e49\u6b63\u5219\u5316\u76ee\u6807\u7b56\u7565\u5206\u5e03\uff0c\u5c06\u7b56\u7565\u66f4\u65b0\u8f6c\u5316\u4e3a\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u91c7\u7528\u89e3\u8026\u65b9\u6cd5\u7a33\u5b9a\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728DNA\u5e8f\u5217\u751f\u6210\u3001\u5b8f\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u548c\u4f18\u8d8a\u7684\u6837\u672c\u6548\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u76f8\u7ed3\u5408\uff0c\u4e3a\u5927\u89c4\u6a21\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2509.24626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24626", "abs": "https://arxiv.org/abs/2509.24626", "authors": ["Qihui Zhou", "Peiqi Yin", "Pengfei Zuo", "James Cheng"], "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving", "comment": "14 pages, 16 figures", "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.", "AI": {"tldr": "SparseServe\u662f\u4e00\u4e2aLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u6548\u7684HBM-DRAM\u5206\u5c42\u7ba1\u7406\u89e3\u51b3\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u4e2d\u7684HBM\u5bb9\u91cf\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587LLM\u7684\u670d\u52a1\u6027\u80fd\u3002", "motivation": "\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5c06\u6027\u80fd\u74f6\u9888\u4eceHBM\u5e26\u5bbd\u8f6c\u79fb\u5230\u4e86HBM\u5bb9\u91cf\uff0c\u56e0\u4e3a\u672a\u9009\u4e2d\u7684KV\u7f13\u5b58\u4ecd\u9700\u4fdd\u7559\u5728HBM\u4e2d\uff0c\u9650\u5236\u4e86\u5e76\u884c\u6279\u5904\u7406\u5927\u5c0f\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u788e\u7247\u611f\u77e5KV\u7f13\u5b58\u4f20\u8f93\uff0c\u901a\u8fc7GPU\u76f4\u63a5\u52a0\u8f7d\u548cCPU\u8f85\u52a9\u4fdd\u5b58\u52a0\u901fHBM-DRAM\u6570\u636e\u4f20\u8f93\uff1b2) \u5de5\u4f5c\u96c6\u611f\u77e5\u6279\u5904\u7406\u5927\u5c0f\u63a7\u5236\uff0c\u57fa\u4e8e\u5b9e\u65f6\u5de5\u4f5c\u96c6\u4f30\u8ba1\u8c03\u6574\u6279\u5927\u5c0f\uff1b3) \u5206\u5c42\u5206\u6bb5\u9884\u586b\u5145\uff0c\u5c06\u9884\u586b\u5145\u9636\u6bb5\u7684HBM\u4f7f\u7528\u9650\u5236\u5728\u5355\u4e2a\u5c42\u5185\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSparseServe\u76f8\u6bd4\u6700\u5148\u8fdb\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe9.26\u500d\u66f4\u4f4e\u7684\u9996\u4ee4\u724c\u65f6\u95f4\u5ef6\u8fdf\u548c\u9ad8\u8fbe3.14\u500d\u66f4\u9ad8\u7684\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\u3002", "conclusion": "SparseServe\u901a\u8fc7\u9ad8\u6548\u7684HBM-DRAM\u5206\u5c42\u7ba1\u7406\u6210\u529f\u89e3\u9501\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u7684\u5e76\u884c\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u7684\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2509.22964", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22964", "abs": "https://arxiv.org/abs/2509.22964", "authors": ["Qinxun Bai", "Yuxuan Han", "Wei Xu", "Zhengyuan Zhou"], "title": "Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic", "comment": null, "summary": "Off-policy reinforcement learning (RL) with function approximation offers an\neffective way to improve sample efficiency by reusing past experience. Within\nthis setting, the actor-critic (AC) framework has achieved strong empirical\nsuccess. However, both the critic and actor learning is challenging for the\noff-policy AC methods: first of all, in addition to the classic \"deadly triad\"\ninstability of off-policy evaluation, it also suffers from a \"moving target\"\nproblem, where the policy being evaluated changes continually; secondly, actor\nlearning becomes less efficient due to the difficulty of estimating the exact\noff-policy policy gradient. The first challenge essentially reduces the problem\nto repeatedly performing off-policy evaluation for changing policies. For the\nsecond challenge, the off-policy policy gradient theorem requires a complex and\noften impractical algorithm to estimate an additional emphasis critic, which is\ntypically neglected in practice, thereby reducing to the on-policy policy\ngradient as an approximation. In this work, we introduce a novel concept of\nfunctional critic modeling, which leads to a new AC framework that addresses\nboth challenges for actor-critic learning under the deadly triad setting. We\nprovide a theoretical analysis in the linear function setting, establishing the\nprovable convergence of our framework, which, to the best of our knowledge, is\nthe first convergent off-policy target-based AC algorithm. From a practical\nperspective, we further propose a carefully designed neural network\narchitecture for the functional critic modeling and demonstrate its\neffectiveness through preliminary experiments on widely used RL tasks from the\nDeepMind Control Benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u529f\u80fd\u8bc4\u8bba\u5bb6\u5efa\u6a21\u6982\u5ff5\uff0c\u89e3\u51b3\u4e86\u79bb\u7b56\u7565\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8bc4\u8bba\u5bb6\u5b66\u4e60\u4e2d\u7684\"\u79fb\u52a8\u76ee\u6807\"\u95ee\u9898\u548c\u6f14\u5458\u5b66\u4e60\u4e2d\u7684\u79bb\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u56f0\u96be\u3002", "motivation": "\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8bc4\u8bba\u5bb6\u5b66\u4e60\u4e2d\u7684\"\u81f4\u547d\u4e09\u89d2\"\u4e0d\u7a33\u5b9a\u6027\u548c\"\u79fb\u52a8\u76ee\u6807\"\u95ee\u9898\uff0c\u4ee5\u53ca\u6f14\u5458\u5b66\u4e60\u4e2d\u7684\u79bb\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7406\u8bba\u4e0d\u6536\u655b\uff0c\u8981\u4e48\u5728\u5b9e\u8df5\u4e2d\u7b80\u5316\u5904\u7406\u3002", "method": "\u5f15\u5165\u529f\u80fd\u8bc4\u8bba\u5bb6\u5efa\u6a21\u6982\u5ff5\uff0c\u63d0\u51fa\u65b0\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u3002\u5728\u7406\u8bba\u5206\u6790\u4e2d\u91c7\u7528\u7ebf\u6027\u51fd\u6570\u8bbe\u7f6e\u8bc1\u660e\u6536\u655b\u6027\uff0c\u5728\u5b9e\u8df5\u4e2d\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u5b9e\u73b0\u529f\u80fd\u8bc4\u8bba\u5bb6\u5efa\u6a21\u3002", "result": "\u5728\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u53ef\u8bc1\u660e\u7684\u6536\u655b\u6027\uff0c\u636e\u6211\u4eec\u6240\u77e5\u8fd9\u662f\u7b2c\u4e00\u4e2a\u6536\u655b\u7684\u79bb\u7b56\u7565\u76ee\u6807\u578b\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u5728DeepMind Control Benchmark\u7684\u5e7f\u6cdbRL\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u529f\u80fd\u8bc4\u8bba\u5bb6\u5efa\u6a21\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7b56\u7565\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6709\u6548\u6027\uff0c\u4e3a\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24859", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24859", "abs": "https://arxiv.org/abs/2509.24859", "authors": ["Antian Liang", "Zhigang Zhao", "Kai Zhang", "Xuri Shi", "Chuantao Li", "Chunxiao Wang", "Zhenying He", "Yinan Jing", "X. Sean Wang"], "title": "HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters", "comment": null, "summary": "With the rapid evolution of GPU architectures, the heterogeneity of model\ntraining infrastructures is steadily increasing. In such environments,\neffectively utilizing all available heterogeneous accelerators becomes critical\nfor distributed model training. However, existing frameworks, which are\nprimarily designed for homogeneous clusters, often exhibit significant resource\nunderutilization when deployed on heterogeneous accelerators and networks. In\nthis paper, we present Hapt, an automated parallel training framework designed\nspecifically for heterogeneous clusters. Hapt introduces a fine-grained planner\nthat efficiently searches a wide space for the inter-operator parallel\nstrategy, enabling Hapt to alleviate communication overheads while maintaining\nbalanced loads across heterogeneous accelerators. In addition, Hapt implements\na heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution\ntiming and ordering of microbatches based on network characteristics,\nmaximizing computation-communication overlap under cross-cluster interconnects\nwhile incurring only minimal memory overhead. Our evaluation results show that\nHapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than\nstate-of-the-art training frameworks.", "AI": {"tldr": "Hapt\u662f\u4e00\u4e2a\u4e13\u4e3a\u5f02\u6784\u96c6\u7fa4\u8bbe\u8ba1\u7684\u81ea\u52a8\u5e76\u884c\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u89c4\u5212\u5668\u548c\u5f02\u6784\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u5728\u5f02\u6784\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u6bd4\u73b0\u6709\u6846\u67b6\u9ad81.3-1.6\u500d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740GPU\u67b6\u6784\u7684\u5feb\u901f\u6f14\u8fdb\uff0c\u6a21\u578b\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u7684\u5f02\u6784\u6027\u4e0d\u65ad\u589e\u52a0\u3002\u73b0\u6709\u4e3b\u8981\u4e3a\u540c\u6784\u96c6\u7fa4\u8bbe\u8ba1\u7684\u6846\u67b6\u5728\u5f02\u6784\u52a0\u901f\u5668\u548c\u7f51\u7edc\u4e0a\u90e8\u7f72\u65f6\u5f80\u5f80\u51fa\u73b0\u663e\u8457\u7684\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u95ee\u9898\u3002", "method": "Hapt\u5f15\u5165\u4e86\u7ec6\u7c92\u5ea6\u89c4\u5212\u5668\uff0c\u5728\u5e7f\u6cdb\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u9ad8\u6548\u641c\u7d22\u7b97\u5b50\u95f4\u5e76\u884c\u7b56\u7565\uff1b\u540c\u65f6\u5b9e\u73b0\u4e86\u5f02\u6784\u611f\u77e5\u76841F1B\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u7f51\u7edc\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u5fae\u6279\u5904\u7406\u7684\u6267\u884c\u65f6\u95f4\u548c\u987a\u5e8f\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cHapt\u5728\u5f02\u6784\u96c6\u7fa4\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u6846\u67b6\u63d0\u4f9b1.3-1.6\u500d\u7684\u66f4\u9ad8\u6027\u80fd\u3002", "conclusion": "Hapt\u6846\u67b6\u80fd\u591f\u6709\u6548\u7f13\u89e3\u901a\u4fe1\u5f00\u9500\uff0c\u5728\u5f02\u6784\u52a0\u901f\u5668\u95f4\u4fdd\u6301\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u5728\u8de8\u96c6\u7fa4\u4e92\u8fde\u4e0b\u6700\u5927\u5316\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\uff0c\u540c\u65f6\u4ec5\u4ea7\u751f\u6700\u5c0f\u7684\u5185\u5b58\u5f00\u9500\u3002"}}
{"id": "2509.22969", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22969", "abs": "https://arxiv.org/abs/2509.22969", "authors": ["Samuel V. Singh", "Shirley Coyle", "Mimi Zhang"], "title": "Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders", "comment": null, "summary": "We introduce FAEclust, a novel functional autoencoder framework for cluster\nanalysis of multi-dimensional functional data, data that are random\nrealizations of vector-valued random functions. Our framework features a\nuniversal-approximator encoder that captures complex nonlinear\ninterdependencies among component functions, and a universal-approximator\ndecoder capable of accurately reconstructing both Euclidean and manifold-valued\nfunctional data. Stability and robustness are enhanced through innovative\nregularization strategies applied to functional weights and biases.\nAdditionally, we incorporate a clustering loss into the network's training\nobjective, promoting the learning of latent representations that are conducive\nto effective clustering. A key innovation is our shape-informed clustering\nobjective, ensuring that the clustering results are resistant to phase\nvariations in the functions. We establish the universal approximation property\nof our non-linear decoder and validate the effectiveness of our model through\nextensive experiments.", "AI": {"tldr": "FAEclust\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u7ef4\u51fd\u6570\u6570\u636e\u805a\u7c7b\u5206\u6790\u7684\u65b0\u578b\u529f\u80fd\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5411\u91cf\u503c\u968f\u673a\u51fd\u6570\u6570\u636e\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u7f16\u7801\u5668\u6355\u83b7\u51fd\u6570\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u5f62\u72b6\u611f\u77e5\u805a\u7c7b\u76ee\u6807\u6765\u62b5\u6297\u51fd\u6570\u76f8\u4f4d\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u51fd\u6570\u6570\u636e\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u7ef4\u51fd\u6570\u6570\u636e\u4e2d\u590d\u6742\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u5bf9\u51fd\u6570\u76f8\u4f4d\u53d8\u5316\u654f\u611f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6355\u83b7\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5e76\u62b5\u6297\u76f8\u4f4d\u53d8\u5316\u7684\u9c81\u68d2\u805a\u7c7b\u6846\u67b6\u3002", "method": "\u63d0\u51faFAEclust\u6846\u67b6\uff0c\u5305\u542b\u901a\u7528\u903c\u8fd1\u5668\u7f16\u7801\u5668\u6355\u83b7\u51fd\u6570\u95f4\u975e\u7ebf\u6027\u4f9d\u8d56\uff0c\u901a\u7528\u903c\u8fd1\u5668\u89e3\u7801\u5668\u91cd\u5efa\u6b27\u51e0\u91cc\u5f97\u548c\u6d41\u5f62\u503c\u51fd\u6570\u6570\u636e\uff0c\u91c7\u7528\u529f\u80fd\u6743\u91cd\u548c\u504f\u7f6e\u7684\u6b63\u5219\u5316\u7b56\u7565\u589e\u5f3a\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u8bad\u7ec3\u76ee\u6807\u4e2d\u52a0\u5165\u805a\u7c7b\u635f\u5931\u548c\u5f62\u72b6\u611f\u77e5\u805a\u7c7b\u76ee\u6807\u3002", "result": "\u5efa\u7acb\u4e86\u975e\u7ebf\u6027\u89e3\u7801\u5668\u7684\u901a\u7528\u903c\u8fd1\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u7ef4\u51fd\u6570\u6570\u636e\u7684\u805a\u7c7b\u95ee\u9898\u3002", "conclusion": "FAEclust\u4e3a\u591a\u7ef4\u51fd\u6570\u6570\u636e\u7684\u805a\u7c7b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6355\u83b7\u590d\u6742\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\u5e76\u62b5\u6297\u76f8\u4f4d\u53d8\u5316\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.24932", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24932", "abs": "https://arxiv.org/abs/2509.24932", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Jacob Chakareski", "Nicholas Mastronarde", "Seyyedali Hosseinalipour"], "title": "Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization", "comment": "8 Figures, 6 Appendix", "summary": "We introduce Fed-Span, a novel federated/distributed learning framework\ndesigned for low Earth orbit satellite constellations. By leveraging\ngraph-theoretic principles, Fed-Span addresses critical challenges inherent to\ndistributed learning in dynamic satellite networks, including intermittent\nsatellite connectivity, heterogeneous computational capabilities of satellites,\nand time-varying satellites' datasets. At its core, Fed-Span builds upon\nminimum spanning tree (MST) and minimum spanning forest (MSF) topologies,\nenabling spanning model aggregation and dispatching processes for distributed\nlearning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF\ntopologies by formulating them through a set of continuous constraint\nrepresentations (CCRs), thereby devising graph-theoretical abstractions into an\noptimizable framework for satellite networks. Using these CCRs, we obtain the\nenergy consumption and latency of operations in Fed-Span. Moreover, we derive\nnovel convergence bounds for non-convex machine learning loss functions,\naccommodating the key system characteristics and degrees of freedom of\nFed-Span. Finally, we propose a comprehensive optimization problem that jointly\nminimizes model prediction loss, energy consumption, and latency of Fed-Span.\nWe unveil that this problem is NP-hard and develop a systematic approach to\ntransform it into a geometric programming formulation, solved via successive\nconvex optimization with performance guarantees. Through evaluations on\nreal-world datasets, we demonstrate that Fed-Span outperforms existing methods,\nwith faster model convergence, greater energy efficiency, and reduced latency.\nThese results highlight Fed-Span as a novel solution for efficient distributed\nlearning in satellite networks.", "AI": {"tldr": "Fed-Span\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u8bba\u539f\u7406\u7684\u8054\u90a6/\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u661f\u5ea7\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6700\u5c0f\u751f\u6210\u6811\u548c\u6700\u5c0f\u751f\u6210\u68ee\u6797\u62d3\u6251\u89e3\u51b3\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u8fde\u63a5\u6027\u3001\u8ba1\u7b97\u80fd\u529b\u548c\u6570\u636e\u96c6\u52a8\u6001\u53d8\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u536b\u661f\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u5b66\u4e60\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u95f4\u6b47\u6027\u536b\u661f\u8fde\u63a5\u3001\u536b\u661f\u8ba1\u7b97\u80fd\u529b\u5f02\u6784\u4ee5\u53ca\u968f\u65f6\u95f4\u53d8\u5316\u7684\u536b\u661f\u6570\u636e\u96c6\u3002", "method": "\u57fa\u4e8e\u6700\u5c0f\u751f\u6210\u6811\u548c\u6700\u5c0f\u751f\u6210\u68ee\u6797\u62d3\u6251\uff0c\u901a\u8fc7\u8fde\u7eed\u7ea6\u675f\u8868\u793a\u6784\u5efa\u56fe\u8bba\u62bd\u8c61\uff0c\u5efa\u7acb\u53ef\u4f18\u5316\u7684\u536b\u661f\u7f51\u7edc\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u80fd\u91cf\u6d88\u8017\u548c\u5ef6\u8fdf\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFed-Span\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u6a21\u578b\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u80fd\u91cf\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "Fed-Span\u662f\u536b\u661f\u7f51\u7edc\u4e2d\u9ad8\u6548\u5206\u5e03\u5f0f\u5b66\u4e60\u7684\u65b0\u9896\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6a21\u578b\u9884\u6d4b\u635f\u5931\u3001\u80fd\u91cf\u6d88\u8017\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22979", "abs": "https://arxiv.org/abs/2509.22979", "authors": ["Zeyi Chen", "Xinzhi Zhang", "Humishka Zope", "Hugo Barbalho", "Konstantina Mellou", "Marco Molinaro", "Janardhan Kulkarni", "Ishai Menache", "Sirui Li"], "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "comment": null, "summary": "Mathematical programming -- the task of expressing operations and\ndecision-making problems in precise mathematical language -- is fundamental\nacross domains, yet remains a skill-intensive process requiring operations\nresearch expertise. Recent advances in large language models for complex\nreasoning have spurred interest in automating this task, translating natural\nlanguage into executable optimization models. Current approaches, however,\nachieve limited accuracy, hindered by scarce and noisy training data without\nleveraging domain knowledge. In this work, we systematically integrate\noptimization expertise to improve formulation accuracy for mixed-integer linear\nprogramming, a key family of mathematical programs. Our approach first cleans\ntraining data through class-based error analysis to explicitly prevent common\nmistakes within each optimization class. We then develop multi-turn inference\nstrategies that guide LLMs with class-specific error summaries and solver\nfeedback, enabling iterative refinement. Experiments across multiple base LLMs\ndemonstrate that combining cleaned data with domain-informed prompting and\nfeedback improves formulation accuracy by 14 percentage points on average,\nenabling further progress toward robust LLM-assisted optimization formulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f18\u5316\u9886\u57df\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u548c\u591a\u8f6e\u63a8\u7406\u7b56\u7565\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u4e0a\u7684\u516c\u5f0f\u5316\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u6570\u5b66\u7f16\u7a0b\u7684\u65b9\u6cd5\u51c6\u786e\u6027\u6709\u9650\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u548c\u566a\u58f0\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u4f18\u5316\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u9996\u5148\u901a\u8fc7\u57fa\u4e8e\u7c7b\u522b\u7684\u9519\u8bef\u5206\u6790\u6e05\u6d17\u8bad\u7ec3\u6570\u636e\uff0c\u9632\u6b62\u5e38\u89c1\u9519\u8bef\uff1b\u7136\u540e\u5f00\u53d1\u591a\u8f6e\u63a8\u7406\u7b56\u7565\uff0c\u4f7f\u7528\u7279\u5b9a\u7c7b\u522b\u9519\u8bef\u6458\u8981\u548c\u6c42\u89e3\u5668\u53cd\u9988\u6765\u5f15\u5bfcLLM\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6e05\u6d17\u6570\u636e\u548c\u9886\u57df\u77e5\u8bc6\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u53ef\u5c06\u516c\u5f0f\u5316\u51c6\u786e\u6027\u63d0\u9ad814\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u7a33\u5065\u7684LLM\u8f85\u52a9\u4f18\u5316\u516c\u5f0f\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u4e00\u6b65\u8fdb\u5c55\u3002"}}
{"id": "2509.25041", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25041", "abs": "https://arxiv.org/abs/2509.25041", "authors": ["Yu Han", "Lehan Pan", "Jie Peng", "Ziyang Tao", "Wuyang Zhang", "Yanyong Zhang"], "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference", "comment": null, "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance.", "AI": {"tldr": "GRACE-MoE\u662f\u4e00\u4e2a\u9488\u5bf9\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u63a8\u7406\u7684\u534f\u540c\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7ec4\u590d\u5236\u548c\u5c40\u90e8\u611f\u77e5\u8def\u7531\u7b56\u7565\uff0c\u540c\u65f6\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u7f13\u89e3\u8ba1\u7b97\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8fbe3.79\u500d\u7684\u7aef\u5230\u7aef\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u5728\u5206\u5e03\u5f0f\u90e8\u7f72\u4e2d\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u901a\u4fe1\u95ee\u9898-\u7279\u5f81\u4f20\u8f93\u5230\u6fc0\u6d3b\u4e13\u5bb6\u8bbe\u5907\u4ea7\u751f\u663e\u8457\u901a\u4fe1\u5f00\u9500\uff1b2\uff09\u8ba1\u7b97\u8d1f\u8f7d\u95ee\u9898-\u4e13\u5bb6\u6fc0\u6d3b\u4e0d\u5747\u8861\u5bfc\u81f4GPU\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3002\u5176\u4e2d\u901a\u4fe1\u5f00\u9500\u88ab\u786e\u5b9a\u4e3aSMoE\u63a8\u7406\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "GRACE-MoE\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff1a1\uff09\u5206\u7ec4\u4e0e\u590d\u5236\uff1a\u57fa\u4e8e\u4e13\u5bb6\u4eb2\u548c\u6027\u5206\u7ec4\u4ee5\u51cf\u5c11\u8de8\u8bbe\u5907\u901a\u4fe1\uff0c\u5e94\u7528\u52a8\u6001\u590d\u5236\u89e3\u51b3\u8d1f\u8f7d\u503e\u659c\uff1b2\uff09\u8def\u7531\uff1a\u91c7\u7528\u5177\u6709\u8d1f\u8f7d\u9884\u6d4b\u7684\u5c40\u90e8\u611f\u77e5\u8def\u7531\u7b56\u7565\uff0c\u4f18\u5148\u4f7f\u7528\u672c\u5730\u526f\u672c\u6700\u5c0f\u5316\u901a\u4fe1\uff0c\u5fc5\u8981\u65f6\u5728\u8fdc\u7a0b\u526f\u672c\u95f4\u5e73\u8861\u8bf7\u6c42\u3002", "result": "\u5728\u591a\u6837\u5316\u6a21\u578b\u548c\u591a\u8282\u70b9\u591aGPU\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRACE-MoE\u80fd\u6709\u6548\u51cf\u5c11\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u8fbe3.79\u500d\u7684\u52a0\u901f\u3002", "conclusion": "GRACE-MoE\u901a\u8fc7\u534f\u540c\u4f18\u5316\u901a\u4fe1\u548c\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u6210\u529f\u89e3\u51b3\u4e86SMoE\u5206\u5e03\u5f0f\u63a8\u7406\u4e2d\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2509.22981", "categories": ["cs.LG", "math.OC", "90C15, 90C40"], "pdf": "https://arxiv.org/pdf/2509.22981", "abs": "https://arxiv.org/abs/2509.22981", "authors": ["David P. Morton", "Oscar Dowson", "Bernardo K. Pagnoncelli"], "title": "MDP modeling for multi-stage stochastic programs", "comment": null, "summary": "We study a class of multi-stage stochastic programs, which incorporate\nmodeling features from Markov decision processes (MDPs). This class includes\nstructured MDPs with continuous state and action spaces. We extend policy\ngraphs to include decision-dependent uncertainty for one-step transition\nprobabilities as well as a limited form of statistical learning. We focus on\nthe expressiveness of our modeling approach, illustrating ideas with a series\nof examples of increasing complexity. As a solution method, we develop new\nvariants of stochastic dual dynamic programming, including approximations to\nhandle non-convexities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e00\u7c7b\u7ed3\u5408MDP\u7279\u5f81\u7684\u591a\u9636\u6bb5\u968f\u673a\u89c4\u5212\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u7b56\u7565\u56fe\u4ee5\u5305\u542b\u51b3\u7b56\u4f9d\u8d56\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u7edf\u8ba1\u5b66\u4e60\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u968f\u673a\u5bf9\u5076\u52a8\u6001\u89c4\u5212\u53d8\u4f53\u6765\u5904\u7406\u975e\u51f8\u6027\u3002", "motivation": "\u7814\u7a76\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7279\u5f81\u7684\u591a\u9636\u6bb5\u968f\u673a\u89c4\u5212\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ed3\u6784\u5316MDP\uff0c\u4ee5\u589e\u5f3a\u5efa\u6a21\u80fd\u529b\u548c\u89e3\u51b3\u590d\u6742\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u6269\u5c55\u7b56\u7565\u56fe\u4ee5\u5305\u542b\u51b3\u7b56\u4f9d\u8d56\u7684\u8f6c\u79fb\u6982\u7387\u4e0d\u786e\u5b9a\u6027\u548c\u6709\u9650\u5f62\u5f0f\u7684\u7edf\u8ba1\u5b66\u4e60\uff0c\u5f00\u53d1\u65b0\u7684\u968f\u673a\u5bf9\u5076\u52a8\u6001\u89c4\u5212\u53d8\u4f53\uff0c\u5305\u62ec\u5904\u7406\u975e\u51f8\u6027\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u590d\u6742\u5ea6\u9012\u589e\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u5efa\u6a21\u65b9\u6cd5\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6c42\u89e3\u7b97\u6cd5\u6765\u5904\u7406\u8fd9\u7c7b\u590d\u6742\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u5efa\u6a21\u6846\u67b6\u80fd\u591f\u6709\u6548\u8868\u8fbe\u590d\u6742\u7684\u591a\u9636\u6bb5\u968f\u673a\u51b3\u7b56\u95ee\u9898\uff0c\u65b0\u7684\u968f\u673a\u5bf9\u5076\u52a8\u6001\u89c4\u5212\u53d8\u4f53\u4e3a\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.25121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25121", "abs": "https://arxiv.org/abs/2509.25121", "authors": ["Anvitha Ramachandran", "Dhruv Parikh", "Viktor Prasanna"], "title": "Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs", "comment": "IEEE HPEC 2025", "summary": "Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as\nunstructured graphs, achieving state of the art performance in computer vision\ntasks such as image classification, object detection, and instance\nsegmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by\nconnecting patches (nodes) based on feature similarity, and is dynamically\nrepeated in each ViG layer following GNN based patch (node) feature updates.\nHowever, DIGC constitutes over 50% of end to end ViG inference latency, rising\nto 95% at high image resolutions, making it the dominant computational\nbottleneck. While hardware acceleration holds promise, prior works primarily\noptimize graph construction algorithmically, often compromising DIGC\nflexibility, accuracy, or generality. To address these limitations, we propose\na streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip\nbuffers that process input features in small, uniform blocks. Our design\nminimizes external memory traffic via localized computation and performs\nefficient parallel sorting with local merge sort and global k way merging\ndirectly on streaming input blocks via heap insertion. This modular\narchitecture scales seamlessly across image resolutions, ViG layer types, and\nmodel sizes and variants, and supports DIGC across diverse ViG based vision\nbackbones. The design achieves high clock frequencies post place and route due\nto the statically configured parallelism minimizing critical path delay and\ndelivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d41\u5f0f\u3001\u6df1\u5ea6\u6d41\u6c34\u7ebf\u7684FPGA\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u89e3\u51b3Vision GNN\u4e2d\u52a8\u6001\u56fe\u50cf\u56fe\u6784\u5efa(DIGC)\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u8be5\u52a0\u901f\u5668\u80fd\u663e\u8457\u63d0\u5347\u5904\u7406\u901f\u5ea6\u3002", "motivation": "DIGC\u5728Vision GNN\u4e2d\u5360\u7528\u4e86\u8d85\u8fc750%\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0b\u751a\u81f3\u8fbe\u523095%\uff0c\u6210\u4e3a\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\uff0c\u800c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u727a\u7272\u4e86DIGC\u7684\u7075\u6d3b\u6027\u3001\u51c6\u786e\u6027\u6216\u901a\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6d41\u5f0f\u3001\u6df1\u5ea6\u6d41\u6c34\u7ebf\u7684FPGA\u52a0\u901f\u5668\uff0c\u91c7\u7528\u7247\u4e0a\u7f13\u51b2\u5668\u4ee5\u5c0f\u5757\u5747\u5300\u5757\u5904\u7406\u8f93\u5165\u7279\u5f81\uff0c\u901a\u8fc7\u5c40\u90e8\u8ba1\u7b97\u6700\u5c0f\u5316\u5916\u90e8\u5185\u5b58\u6d41\u91cf\uff0c\u5e76\u4f7f\u7528\u5c40\u90e8\u5f52\u5e76\u6392\u5e8f\u548c\u5168\u5c40k\u8def\u5408\u5e76\u7684\u5e76\u884c\u6392\u5e8f\u65b9\u6cd5\u3002", "result": "\u8be5\u8bbe\u8ba1\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u5230\u4e0d\u540c\u56fe\u50cf\u5206\u8fa8\u7387\u3001ViG\u5c42\u7c7b\u578b\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u5728\u5e03\u5c40\u5e03\u7ebf\u540e\u5b9e\u73b0\u9ad8\u65f6\u949f\u9891\u7387\uff0c\u76f8\u6bd4\u4f18\u5316\u7684CPU\u548cGPU DIGC\u57fa\u7ebf\u5206\u522b\u5b9e\u73b0\u4e8616.6\u500d\u548c6.8\u500d\u7684\u52a0\u901f\u6bd4\u3002", "conclusion": "\u63d0\u51fa\u7684FPGA\u52a0\u901f\u5668\u6709\u6548\u89e3\u51b3\u4e86DIGC\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7075\u6d3b\u6027\u3001\u51c6\u786e\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3aVision GNN\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22992", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.22992", "abs": "https://arxiv.org/abs/2509.22992", "authors": ["Yuanyuan Yang", "Ruimin Zhang", "Jamie Morgenstern", "Haifeng Xu"], "title": "T-TAMER: Provably Taming Trade-offs in ML Serving", "comment": "Correspondence should be directed to yyangh@cs.washington.edu or\n  haifengxu@uchicago.edu. This manuscript extends our earlier workshop version\n  accepted at NeurIPS SPIGM 2025", "summary": "As machine learning models continue to grow in size and complexity, efficient\nserving faces increasingly broad trade-offs spanning accuracy, latency,\nresource usage, and other objectives. Multi-model serving further complicates\nthese trade-offs; for example, in cascaded models, each early-exit decision\nbalances latency reduction against potential accuracy loss. Despite the\npervasiveness and importance of such trade-offs, current strategies remain\nlargely heuristic and case-specific, limiting both their theoretical guarantees\nand general applicability.\n  We present a general framework, T-Tamer, which formalizes this setting as a\nmulti-stage decision process, where the objective is to determine both when to\nexit and which model to consult. Our main result shows that recall (i.e., the\nability to revisit earlier models) is both necessary and sufficient for\nachieving provable performance guarantees. In particular, we prove that\nstrategies without recall cannot obtain any constant-factor approximation to\nthe optimal trade-off, whereas recall-based strategies provably attain the\noptimal trade-off in polynomial time.\n  We validate our analysis through experiments on synthetic datasets and\nearly-exit workloads for vision and NLP benchmarks. The results show that\nrecall-based strategies consistently yield efficient accuracy-latency\ntrade-offs. We hope this work provides a principled foundation for bridging\nheuristic practice with theoretical guarantees in the design of early-exit and\ncascaded models.", "AI": {"tldr": "T-Tamer\u6846\u67b6\u5c06\u591a\u6a21\u578b\u670d\u52a1\u5f62\u5f0f\u5316\u4e3a\u591a\u9636\u6bb5\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc1\u660e\u53ec\u56de\u673a\u5236\u5bf9\u4e8e\u5b9e\u73b0\u53ef\u8bc1\u660e\u6027\u80fd\u4fdd\u8bc1\u662f\u5fc5\u8981\u4e14\u5145\u5206\u7684\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u589e\u957f\uff0c\u9ad8\u6548\u670d\u52a1\u9762\u4e34\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u3001\u8d44\u6e90\u4f7f\u7528\u7b49\u591a\u76ee\u6807\u6743\u8861\u3002\u5f53\u524d\u7b56\u7565\u4e3b\u8981\u57fa\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u548c\u901a\u7528\u6027\u3002", "method": "\u5c06\u591a\u6a21\u578b\u670d\u52a1\u5f62\u5f0f\u5316\u4e3a\u591a\u9636\u6bb5\u51b3\u7b56\u8fc7\u7a0b\uff0c\u786e\u5b9a\u4f55\u65f6\u9000\u51fa\u548c\u54a8\u8be2\u54ea\u4e2a\u6a21\u578b\u3002\u8bc1\u660e\u53ec\u56de\u673a\u5236\u7684\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u53ec\u56de\u7684\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u89c6\u89c9\u3001NLP\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u53ec\u56de\u7684\u7b56\u7565\u80fd\u6301\u7eed\u4ea7\u751f\u9ad8\u6548\u7684\u7cbe\u5ea6-\u5ef6\u8fdf\u6743\u8861\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u65e9\u671f\u9000\u51fa\u548c\u7ea7\u8054\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c06\u542f\u53d1\u5f0f\u5b9e\u8df5\u4e0e\u7406\u8bba\u4fdd\u8bc1\u76f8\u8fde\u63a5\u3002"}}
{"id": "2509.25155", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25155", "abs": "https://arxiv.org/abs/2509.25155", "authors": ["Neelesh Gupta", "Rakshith Jayanth", "Dhruv Parikh", "Viktor Prasanna"], "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units", "comment": "IEEE HiPC 2025", "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u8fb9\u7f18NPU\u4e0a\u90e8\u7f72\u957f\u4e0a\u4e0b\u6587LLM\u7684\u6027\u80fd\u6311\u6218\uff0c\u6bd4\u8f83\u4e86\u6807\u51c6\u4e8c\u6b21\u6ce8\u610f\u529b\u4e0e\u591a\u79cd\u6b21\u4e8c\u6b21\u66ff\u4ee3\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u5728NPU\u4e0a\u5b58\u5728\u4e0d\u540c\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7531\u4e8e\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u4e0e\u8fb9\u7f18\u52a0\u901f\u5668\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6a21\u5f0f\u4e0d\u5339\u914d\uff0c\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002", "method": "\u5728\u73b0\u4ee3NPU\u4e0a\u5bf9\u5404\u79cd\u56e0\u679c\u63a8\u7406\u7b97\u5b50\u8fdb\u884c\u5168\u9762\u7684\u6027\u80fd\u5206\u6790\uff0c\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u4e8c\u6b21\u6ce8\u610f\u529b\u4e0e\u591a\u79cd\u6b21\u4e8c\u6b21\u66ff\u4ee3\u65b9\u6cd5\uff08\u5305\u62ec\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\uff09\u3002", "result": "\u5206\u6790\u663e\u793a\u6b21\u4e8c\u6b21\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5728NPU\u4e13\u7528\u6267\u884c\u5355\u5143\u4e0a\u5f15\u5165\u4e0d\u540c\u7684\u8ba1\u7b97\u74f6\u9888\u3002\u4e8c\u6b21\u6ce8\u610f\u529b\u5728\u957f\u4e0a\u4e0b\u6587\u4e0b\u4e25\u91cd\u53d7\u5185\u5b58\u9650\u5236\uff0c\u7f13\u5b58\u6548\u7387\u4f4e\u4e0b\uff0c\u6d41\u6c34\u7ebf\u505c\u987f\u8d85\u8fc795%\uff1b\u800c\u6b21\u4e8c\u6b21\u6a21\u578b\u53ef\u80fd\u5728\u53ef\u7f16\u7a0b\u5411\u91cf\u6838\u5fc3\u4e0a\u53d7\u8ba1\u7b97\u9650\u5236\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u548c\u4f18\u5316\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u4ee5\u5b9e\u73b0\u8bbe\u5907\u4e0a\u7684\u957f\u4e0a\u4e0b\u6587AI\u63a8\u7406\u3002"}}
{"id": "2509.22994", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22994", "abs": "https://arxiv.org/abs/2509.22994", "authors": ["Zachary Baker", "Yuxiao Li"], "title": "Analysis of Variational Autoencoders", "comment": "15 pages, 11 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach for\ninterpreting neural network representations by learning sparse,\nhuman-interpretable features from dense activations. We investigate whether\nincorporating variational methods into SAE architectures can improve feature\norganization and interpretability. We introduce the variational Sparse\nAutoencoder (vSAE), which replaces deterministic ReLU gating with stochastic\nsampling from learned Gaussian posteriors and incorporates KL divergence\nregularization toward a standard normal prior. Our hypothesis is that this\nprobabilistic sampling creates dispersive pressure, causing features to\norganize more coherently in the latent space while avoiding overlap. We\nevaluate a Topk vSAE against a standard TopK SAE on Pythia-70M transformer\nresidual steam activations using comprehensive benchmarks including SAE Bench,\nindividual feature interpretability analysis, and global latent space\nvisualization through t-SNE. The vSAE underperforms standard SAE across core\nevaluation metrics, though excels at feature independence and ablation metrics.\nThe KL divergence term creates excessive regularization pressure that\nsubstantially reduces the fraction of living features, leading to observed\nperformance degradation. While vSAE features demonstrate improved robustness,\nthey exhibit many more dead features than baseline. Our findings suggest that\nnaive application of variational methods to SAEs does not improve feature\norganization or interpretability.", "AI": {"tldr": "\u5c06\u53d8\u5206\u65b9\u6cd5\u5f15\u5165\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u5e76\u4e0d\u80fd\u6539\u5584\u7279\u5f81\u7ec4\u7ec7\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u53cd\u800c\u7531\u4e8eKL\u6563\u5ea6\u6b63\u5219\u5316\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u5927\u91cf\u6b7b\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u53d8\u5206\u65b9\u6cd5\u662f\u5426\u80fd\u6539\u5584\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u7279\u5f81\u7ec4\u7ec7\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u6982\u7387\u91c7\u6837\u521b\u5efa\u5206\u6563\u538b\u529b\u4f7f\u7279\u5f81\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u8fde\u8d2f\u5730\u7ec4\u7ec7\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u7a00\u758f\u81ea\u7f16\u7801\u5668(vSAE)\uff0c\u7528\u4ece\u5b66\u4e60\u7684\u9ad8\u65af\u540e\u9a8c\u4e2d\u968f\u673a\u91c7\u6837\u66ff\u4ee3\u786e\u5b9a\u6027ReLU\u95e8\u63a7\uff0c\u5e76\u52a0\u5165\u671d\u5411\u6807\u51c6\u6b63\u6001\u5148\u9a8c\u7684KL\u6563\u5ea6\u6b63\u5219\u5316\u3002", "result": "vSAE\u5728\u6838\u5fc3\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u5982\u6807\u51c6SAE\uff0c\u867d\u7136\u5728\u7279\u5f81\u72ec\u7acb\u6027\u548c\u6d88\u878d\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46KL\u6563\u5ea6\u9879\u5bfc\u81f4\u8fc7\u5ea6\u6b63\u5219\u5316\uff0c\u663e\u8457\u51cf\u5c11\u6d3b\u8dc3\u7279\u5f81\u6bd4\u4f8b\uff0c\u4ea7\u751f\u5927\u91cf\u6b7b\u7279\u5f81\u3002", "conclusion": "\u5c06\u53d8\u5206\u65b9\u6cd5\u7b80\u5355\u5e94\u7528\u4e8eSAE\u5e76\u4e0d\u80fd\u6539\u5584\u7279\u5f81\u7ec4\u7ec7\u6216\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u6b63\u5219\u5316\u538b\u529b\u3002"}}
{"id": "2509.23049", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23049", "abs": "https://arxiv.org/abs/2509.23049", "authors": ["Zijian Wang", "Xiaofei Zhang", "Xin Zhang", "Yukun Liu", "Qiong Zhang"], "title": "Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning", "comment": null, "summary": "Federated learning (FL) is increasingly adopted in domains like healthcare,\nwhere data privacy is paramount. A fundamental challenge in these systems is\nstatistical heterogeneity-the fact that data distributions vary significantly\nacross clients (e.g., different hospitals may treat distinct patient\ndemographics). While current FL algorithms focus on aggregating model updates\nfrom these heterogeneous clients, the potential of the central server remains\nunder-explored. This paper is motivated by a healthcare scenario: could a\ncentral server not only build a model but also guide a new patient to the\nhospital best equipped for their specific condition? We generalize this idea to\npropose a novel paradigm for FL systems where the server actively guides the\nallocation of new tasks or queries to the most appropriate client in the\nnetwork. To enable this, we introduce an empirical likelihood-based framework\nthat simultaneously addresses two goals: (1) learning effective local models on\neach client, and (2) finding the best matching client for a new query.\nEmpirical results demonstrate the framework's effectiveness on benchmark\ndatasets, showing improvements in both model accuracy and the precision of\nclient guidance compared to standard FL approaches. This work opens a new\ndirection for building more intelligent and resource-efficient federated\nsystems that leverage heterogeneity as a feature, not just a bug. Code is\navailable at https://github.com/zijianwang0510/FedDRM.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\uff0c\u5176\u4e2d\u670d\u52a1\u5668\u4e0d\u4ec5\u805a\u5408\u6a21\u578b\u66f4\u65b0\uff0c\u8fd8\u4e3b\u52a8\u6307\u5bfc\u65b0\u4efb\u52a1\u5206\u914d\u7ed9\u6700\u5408\u9002\u7684\u5ba2\u6237\u7aef\uff0c\u5229\u7528\u7edf\u8ba1\u5f02\u8d28\u6027\u4f5c\u4e3a\u7279\u5f81\u800c\u975e\u7f3a\u9677\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u6570\u636e\u9690\u79c1\u91cd\u8981\u7684\u9886\u57df\uff0c\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u7edf\u8ba1\u5f02\u8d28\u6027\u6311\u6218\u3002\u672c\u6587\u53d7\u533b\u7597\u573a\u666f\u542f\u53d1\uff1a\u4e2d\u592e\u670d\u52a1\u5668\u80fd\u5426\u4e0d\u4ec5\u6784\u5efa\u6a21\u578b\uff0c\u8fd8\u80fd\u6839\u636e\u65b0\u60a3\u8005\u7684\u5177\u4f53\u60c5\u51b5\u6307\u5bfc\u5176\u5230\u6700\u9002\u5408\u7684\u533b\u9662\uff1f", "method": "\u5f15\u5165\u57fa\u4e8e\u7ecf\u9a8c\u4f3c\u7136\u7684\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u4e24\u4e2a\u76ee\u6807\uff1a(1) \u5728\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4e0a\u5b66\u4e60\u6709\u6548\u7684\u672c\u5730\u6a21\u578b\uff0c(2) \u4e3a\u65b0\u67e5\u8be2\u627e\u5230\u6700\u4f73\u5339\u914d\u7684\u5ba2\u6237\u7aef\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5728\u6a21\u578b\u51c6\u786e\u6027\u548c\u5ba2\u6237\u7aef\u6307\u5bfc\u7cbe\u5ea6\u65b9\u9762\u90fd\u6709\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u66f4\u667a\u80fd\u3001\u8d44\u6e90\u6548\u7387\u66f4\u9ad8\u7684\u8054\u90a6\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c06\u5f02\u8d28\u6027\u4f5c\u4e3a\u7279\u5f81\u800c\u975e\u7f3a\u9677\u52a0\u4ee5\u5229\u7528\u3002"}}
{"id": "2509.23000", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.23000", "abs": "https://arxiv.org/abs/2509.23000", "authors": ["Konstantina Bairaktari", "Huy L. Nguyen"], "title": "Sample-efficient Multiclass Calibration under $\\ell_{p}$ Error", "comment": null, "summary": "Calibrating a multiclass predictor, that outputs a distribution over labels,\nis particularly challenging due to the exponential number of possible\nprediction values. In this work, we propose a new definition of calibration\nerror that interpolates between two established calibration error notions, one\nwith known exponential sample complexity and one with polynomial sample\ncomplexity for calibrating a given predictor. Our algorithm can calibrate any\ngiven predictor for the entire range of interpolation, except for one endpoint,\nusing only a polynomial number of samples. At the other endpoint, we achieve\nnearly optimal dependence on the error parameter, improving upon previous work.\nA key technical contribution is a novel application of adaptive data analysis\nwith high adaptivity but only logarithmic overhead in the sample complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6821\u51c6\u8bef\u5dee\u5b9a\u4e49\uff0c\u5728\u4e24\u79cd\u73b0\u6709\u6821\u51c6\u8bef\u5dee\u6982\u5ff5\u4e4b\u95f4\u63d2\u503c\uff0c\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u7684\u6821\u51c6\u7b97\u6cd5\u3002", "motivation": "\u591a\u7c7b\u9884\u6d4b\u5668\u7684\u6821\u51c6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u53ef\u80fd\u7684\u9884\u6d4b\u503c\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002\u73b0\u6709\u6821\u51c6\u8bef\u5dee\u6982\u5ff5\u8981\u4e48\u9700\u8981\u6307\u6570\u7ea7\u6837\u672c\uff0c\u8981\u4e48\u9700\u8981\u591a\u9879\u5f0f\u6837\u672c\uff0c\u4f46\u5404\u6709\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u7684\u65b0\u5e94\u7528\uff0c\u5177\u6709\u9ad8\u81ea\u9002\u5e94\u6027\u4f46\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u5bf9\u6570\u5f00\u9500\u3002\u7b97\u6cd5\u53ef\u4ee5\u6821\u51c6\u4efb\u4f55\u7ed9\u5b9a\u9884\u6d4b\u5668\uff0c\u8986\u76d6\u6574\u4e2a\u63d2\u503c\u8303\u56f4\uff08\u9664\u4e00\u4e2a\u7aef\u70b9\u5916\uff09\u3002", "result": "\u5728\u9664\u4e00\u4e2a\u7aef\u70b9\u5916\u7684\u6574\u4e2a\u63d2\u503c\u8303\u56f4\u5185\uff0c\u4ec5\u4f7f\u7528\u591a\u9879\u5f0f\u6570\u91cf\u7684\u6837\u672c\u5373\u53ef\u6821\u51c6\u9884\u6d4b\u5668\u3002\u5728\u53e6\u4e00\u4e2a\u7aef\u70b9\u5904\uff0c\u5bf9\u8bef\u5dee\u53c2\u6570\u7684\u4f9d\u8d56\u63a5\u8fd1\u6700\u4f18\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u6821\u51c6\u8bef\u5dee\u5b9a\u4e49\u548c\u7b97\u6cd5\u5728\u591a\u7c7b\u9884\u6d4b\u5668\u6821\u51c6\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5e73\u8861\u4e86\u6837\u672c\u590d\u6742\u5ea6\u548c\u6821\u51c6\u7cbe\u5ea6\u3002"}}
{"id": "2509.23101", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23101", "abs": "https://arxiv.org/abs/2509.23101", "authors": ["M. Z. Haider", "Tayyaba Noreen", "M. Salman"], "title": "Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks", "comment": null, "summary": "Blockchain Business applications and cryptocurrencies such as enable secure,\ndecentralized value transfer, yet their pseudonymous nature creates\nopportunities for illicit activity, challenging regulators and exchanges in\nanti money laundering (AML) enforcement. Detecting fraudulent transactions in\nblockchain networks requires models that can capture both structural and\ntemporal dependencies while remaining resilient to noise, imbalance, and\nadversarial behavior. In this work, we propose an ensemble framework that\nintegrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT),\nand Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection.\nUsing the real-world Elliptic dataset, our tuned soft voting ensemble achieves\nhigh recall of illicit transactions while maintaining a false positive rate\nbelow 1%, beating individual GNN models and baseline methods. The modular\narchitecture incorporates quantum-ready design hooks, allowing seamless future\nintegration of quantum feature mappings and hybrid quantum classical graph\nneural networks. This ensures scalability, robustness, and long-term\nadaptability as quantum computing technologies mature. Our findings highlight\nensemble GNNs as a practical and forward-looking solution for real-time\ncryptocurrency monitoring, providing both immediate AML utility and a pathway\ntoward quantum-enhanced financial security analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408GCN\u3001GAT\u548cGIN\u6a21\u578b\uff0c\u7528\u4e8e\u533a\u5757\u94fe\u6b3a\u8bc8\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u4f4e\u8bef\u62a5\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u975e\u6cd5\u4ea4\u6613\u53ec\u56de\u7387\uff0c\u5e76\u8bbe\u8ba1\u4e86\u91cf\u5b50\u5c31\u7eea\u67b6\u6784\u3002", "motivation": "\u533a\u5757\u94fe\u548c\u52a0\u5bc6\u8d27\u5e01\u7684\u533f\u540d\u6027\u4e3a\u975e\u6cd5\u6d3b\u52a8\u521b\u9020\u4e86\u673a\u4f1a\uff0c\u4f20\u7edf\u53cd\u6d17\u94b1\u76d1\u7ba1\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u540c\u65f6\u6355\u6349\u7ed3\u6784\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GAT)\u548c\u56fe\u540c\u6784\u7f51\u7edc(GIN)\u6784\u5efa\u96c6\u6210\u6846\u67b6\uff0c\u91c7\u7528\u8f6f\u6295\u7968\u96c6\u6210\u65b9\u6cd5\uff0c\u5728\u771f\u5b9eElliptic\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u4f18\u5316\u7684\u8f6f\u6295\u7968\u96c6\u6210\u5728\u4fdd\u6301\u8bef\u62a5\u7387\u4f4e\u4e8e1%\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u975e\u6cd5\u4ea4\u6613\u53ec\u56de\u7387\uff0c\u4f18\u4e8e\u5355\u4e2aGNN\u6a21\u578b\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u96c6\u6210GNN\u4e3a\u5b9e\u65f6\u52a0\u5bc6\u8d27\u5e01\u76d1\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u524d\u77bb\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u5177\u6709\u5373\u65f6\u53cd\u6d17\u94b1\u6548\u7528\uff0c\u53c8\u4e3a\u91cf\u5b50\u589e\u5f3a\u91d1\u878d\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2509.23003", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23003", "abs": "https://arxiv.org/abs/2509.23003", "authors": ["Jiayin Liu", "Yulong Yang", "Vineet Bansal", "Christine Allen-Blanchette"], "title": "Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery", "comment": null, "summary": "From metronomes to celestial bodies, mechanics underpins how the world\nevolves in time and space. With consideration of this, a number of recent\nneural network models leverage inductive biases from classical mechanics to\nencourage model interpretability and ensure forecasted states are physical.\nHowever, in general, these models are designed to capture the dynamics of a\nsingle system with fixed physical parameters, from state-space measurements of\na known configuration space. In this paper we introduce Symplectic Phase Space\nGAN (SPS-GAN) which can capture the dynamics of multiple systems, and\ngeneralize to unseen physical parameters from. Moreover, SPS-GAN does not\nrequire prior knowledge of the system configuration space. In fact, SPS-GAN can\ndiscover the configuration space structure of the system from arbitrary\nmeasurement types (e.g., state-space measurements, video frames). To achieve\nphysically plausible generation, we introduce a novel architecture which embeds\na Hamiltonian neural network recurrent module in a conditional GAN backbone. To\ndiscover the structure of the configuration space, we optimize the conditional\ntime-series GAN objective with an additional physically motivated term to\nencourages a sparse representation of the configuration space. We demonstrate\nthe utility of SPS-GAN for trajectory prediction, video generation and symmetry\ndiscovery. Our approach captures multiple systems and achieves performance on\npar with supervised models designed for single systems.", "AI": {"tldr": "SPS-GAN\u662f\u4e00\u79cd\u57fa\u4e8e\u54c8\u5bc6\u987f\u529b\u5b66\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u80fd\u591f\u4ece\u591a\u79cd\u6d4b\u91cf\u7c7b\u578b\u4e2d\u6355\u83b7\u591a\u4e2a\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u7406\u53c2\u6570\uff0c\u540c\u65f6\u81ea\u52a8\u53d1\u73b0\u7cfb\u7edf\u7684\u6784\u578b\u7a7a\u95f4\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u7cfb\u7edf\u4e14\u9700\u8981\u5df2\u77e5\u6784\u578b\u7a7a\u95f4\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u5904\u7406\u591a\u4e2a\u7cfb\u7edf\u4e14\u6784\u578b\u7a7a\u95f4\u672a\u77e5\u7684\u60c5\u51b5\u3002", "method": "\u5728\u6761\u4ef6GAN\u4e3b\u5e72\u4e2d\u5d4c\u5165\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc\u5faa\u73af\u6a21\u5757\uff0c\u901a\u8fc7\u4f18\u5316\u6761\u4ef6\u65f6\u95f4\u5e8f\u5217GAN\u76ee\u6807\u5e76\u6dfb\u52a0\u7269\u7406\u6fc0\u52b1\u9879\u6765\u9f13\u52b1\u6784\u578b\u7a7a\u95f4\u7684\u7a00\u758f\u8868\u793a\u3002", "result": "SPS-GAN\u5728\u8f68\u8ff9\u9884\u6d4b\u3001\u89c6\u9891\u751f\u6210\u548c\u5bf9\u79f0\u6027\u53d1\u73b0\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4e0e\u4e13\u4e3a\u5355\u4e00\u7cfb\u7edf\u8bbe\u8ba1\u7684\u76d1\u7763\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "SPS-GAN\u80fd\u591f\u6709\u6548\u6355\u83b7\u591a\u4e2a\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u65e0\u9700\u5148\u9a8c\u6784\u578b\u7a7a\u95f4\u77e5\u8bc6\uff0c\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23803", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.23803", "abs": "https://arxiv.org/abs/2509.23803", "authors": ["Pramit Saha", "Joshua Strong", "Divyanshu Mishra", "Cheng Ouyang", "J. Alison Noble"], "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "comment": null, "summary": "Federated learning (FL) allows collaborative model training across healthcare\nsites without sharing sensitive patient data. However, real-world FL deployment\nis often hindered by complex operational challenges that demand substantial\nhuman efforts. This includes: (a) selecting appropriate clients (hospitals),\n(b) coordinating between the central server and clients, (c) client-level data\npre-processing, (d) harmonizing non-standardized data and labels across\nclients, and (e) selecting FL algorithms based on user instructions and\ncross-client data characteristics. However, the existing FL works overlook\nthese practical orchestration challenges. These operational bottlenecks\nmotivate the need for autonomous, agent-driven FL systems, where intelligent\nagents at each hospital client and the central server agent collaboratively\nmanage FL setup and model training with minimal human intervention. To this\nend, we first introduce an agent-driven FL framework that captures key phases\nof real-world FL workflows from client selection to training completion and a\nbenchmark dubbed FedAgentBench that evaluates the ability of LLM agents to\nautonomously coordinate healthcare FL. Our framework incorporates 40 FL\nalgorithms, each tailored to address diverse task-specific requirements and\ncross-client characteristics. Furthermore, we introduce a diverse set of\ncomplex tasks across 201 carefully curated datasets, simulating 6\nmodality-specific real-world healthcare environments, viz., Dermatoscopy,\nUltrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic\nperformance of 14 open-source and 10 proprietary LLMs spanning small, medium,\nand large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3\ncan automate various stages of the FL pipeline, our results reveal that more\ncomplex, interdependent tasks based on implicit goals remain challenging for\neven the strongest models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedAgent\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u533b\u7597\u8054\u90a6\u5b66\u4e60\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u9009\u62e9\u3001\u534f\u8c03\u3001\u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6807\u51c6\u5316\u548c\u7b97\u6cd5\u9009\u62e9\u7b49\u590d\u6742\u64cd\u4f5c\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u90e8\u7f72\u53d7\u5230\u590d\u6742\u64cd\u4f5c\u6311\u6218\u7684\u963b\u788d\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u9009\u62e9\u3001\u534f\u8c03\u3001\u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6807\u51c6\u5316\u548c\u7b97\u6cd5\u9009\u62e9\u7b49\u3002\u8fd9\u4e9b\u64cd\u4f5c\u74f6\u9888\u4fc3\u4f7f\u9700\u8981\u81ea\u4e3b\u3001\u4ee3\u7406\u9a71\u52a8\u7684\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u4ee3\u7406\u9a71\u52a8\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u6355\u6349\u4ece\u5ba2\u6237\u7aef\u9009\u62e9\u5230\u8bad\u7ec3\u5b8c\u6210\u7684\u771f\u5b9e\u4e16\u754c\u8054\u90a6\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u7684\u5173\u952e\u9636\u6bb5\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3aFedAgentBench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u81ea\u4e3b\u534f\u8c03\u533b\u7597\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5305\u542b40\u79cd\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u9488\u5bf9\u4e0d\u540c\u7684\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u548c\u8de8\u5ba2\u6237\u7aef\u7279\u5f81\u8fdb\u884c\u5b9a\u5236\u3002", "result": "\u8bc4\u4f30\u4e8614\u4e2a\u5f00\u6e90\u548c10\u4e2a\u4e13\u6709LLM\u7684\u4ee3\u7406\u6027\u80fd\uff0c\u6db5\u76d6\u5c0f\u3001\u4e2d\u3001\u5927\u6a21\u578b\u89c4\u6a21\u3002\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u4e00\u4e9b\u4ee3\u7406\u6838\u5fc3\u5982GPT-4.1\u548cDeepSeek V3\u53ef\u4ee5\u81ea\u52a8\u5316\u8054\u90a6\u5b66\u4e60\u7ba1\u9053\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u4f46\u57fa\u4e8e\u9690\u542b\u76ee\u6807\u7684\u66f4\u590d\u6742\u3001\u76f8\u4e92\u4f9d\u8d56\u7684\u4efb\u52a1\u5bf9\u5373\u4f7f\u662f\u6700\u5f3a\u7684\u6a21\u578b\u4e5f\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "\u4ee3\u7406\u9a71\u52a8\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u533b\u7597\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u4e2d\u7684\u4eba\u5de5\u5e72\u9884\uff0c\u4f46\u590d\u6742\u3001\u76f8\u4e92\u4f9d\u8d56\u7684\u4efb\u52a1\u4ecd\u7136\u662f\u5f53\u524dLLM\u4ee3\u7406\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2509.23012", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23012", "abs": "https://arxiv.org/abs/2509.23012", "authors": ["Lauren. A Hannah", "Soheil Zibakhsh", "Kumari Nishu", "Arnav Kundu", "Mohammad Samragh Razlighi", "Mehrdad Farajtabar", "Minsik Cho"], "title": "MoE-PHDS: One MoE checkpoint for flexible runtime sparsity", "comment": null, "summary": "Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed\nsparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity\nlevel determines an operating point on the accuracy/latency curve; currently,\nmeeting multiple efficiency targets means training and maintaining multiple\nmodels. This practice complicates serving, increases training and maintenance\ncosts, and limits flexibility in meeting diverse latency, efficiency, and\nenergy requirements. We show that pretrained MoEs are more robust to runtime\nsparsity shifts than commonly assumed, and introduce MoE-PHDS ({\\bf P}ost {\\bf\nH}oc {\\bf D}eclared {\\bf S}parsity), a lightweight SFT method that turns a\nsingle checkpoint into a global sparsity control surface. PHDS mixes training\nacross sparsity levels and anchors with a short curriculum at high sparsity,\nrequiring no architectural changes. The result is predictable accuracy/latency\ntradeoffs from one model: practitioners can ``dial $k$'' at inference time\nwithout swapping checkpoints, changing architecture, or relying on token-level\nheuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary\nmodels fit on multiple operating points show that PHDS matches or exceeds\nwell-specified oracle models, improves cross-sparsity agreement by up to 22\\%\nvs. well-specified oracle models, and enables simplified, flexible runtime MoE\ndeployment by making global sparsity a first-class serving primitive.", "AI": {"tldr": "MoE-PHDS\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7SFT\u8bad\u7ec3\uff0c\u5c06\u5355\u4e2aMoE\u6a21\u578b\u8f6c\u6362\u4e3a\u53ef\u8c03\u8282\u7a00\u758f\u5ea6\u7684\u63a7\u5236\u8868\u9762\uff0c\u5b9e\u73b0\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574k\u503c\u800c\u65e0\u9700\u5207\u6362\u6a21\u578b\u3002", "motivation": "\u4f20\u7edfMoE\u6a21\u578b\u9700\u8981\u4e3a\u4e0d\u540c\u7a00\u758f\u5ea6\u8bad\u7ec3\u548c\u7ef4\u62a4\u591a\u4e2a\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u90e8\u7f72\u590d\u6742\u6027\u548c\u6210\u672c\uff0c\u9650\u5236\u4e86\u6ee1\u8db3\u591a\u6837\u5316\u5ef6\u8fdf\u548c\u6548\u7387\u9700\u6c42\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faMoE-PHDS\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u7a00\u758f\u5ea6\u6df7\u5408\u8bad\u7ec3\u548c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7684\u77ed\u8bfe\u7a0b\u5b66\u4e60\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u5c06\u5355\u4e2a\u68c0\u67e5\u70b9\u8f6c\u6362\u4e3a\u5168\u5c40\u7a00\u758f\u5ea6\u63a7\u5236\u8868\u9762\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPHDS\u5339\u914d\u6216\u8d85\u8fc7\u4e13\u7528oracle\u6a21\u578b\uff0c\u8de8\u7a00\u758f\u5ea6\u4e00\u81f4\u6027\u63d0\u5347\u8fbe22%\uff0c\u5b9e\u73b0\u4e86\u7b80\u5316\u7684\u8fd0\u884c\u65f6MoE\u90e8\u7f72\u3002", "conclusion": "MoE-PHDS\u4f7f\u5168\u5c40\u7a00\u758f\u5ea6\u6210\u4e3a\u4e00\u7ea7\u670d\u52a1\u539f\u8bed\uff0c\u5b9e\u73b0\u4e86\u5355\u4e00\u6a21\u578b\u7684\u53ef\u9884\u6d4b\u7cbe\u5ea6/\u5ef6\u8fdf\u6743\u8861\uff0c\u7b80\u5316\u4e86MoE\u7684\u7075\u6d3b\u90e8\u7f72\u3002"}}
{"id": "2509.24305", "categories": ["cs.LG", "cs.DC", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24305", "abs": "https://arxiv.org/abs/2509.24305", "authors": ["Alexander Tyurin", "Andrei Spiridonov", "Varvara Rudenko"], "title": "Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning", "comment": null, "summary": "We study distributed reinforcement learning (RL) with policy gradient methods\nunder asynchronous and parallel computations and communications. While\nnon-distributed methods are well understood theoretically and have achieved\nremarkable empirical success, their distributed counterparts remain less\nexplored, particularly in the presence of heterogeneous asynchronous\ncomputations and communication bottlenecks. We introduce two new algorithms,\nRennala NIGT and Malenia NIGT, which implement asynchronous policy gradient\naggregation and achieve state-of-the-art efficiency. In the homogeneous\nsetting, Rennala NIGT provably improves the total computational and\ncommunication complexity while supporting the AllReduce operation. In the\nheterogeneous setting, Malenia NIGT simultaneously handles asynchronous\ncomputations and heterogeneous environments with strictly better theoretical\nguarantees. Our results are further corroborated by experiments, showing that\nour methods significantly outperform prior approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5Rennala NIGT\u548cMalenia NIGT\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6b65\u5e76\u884c\u8ba1\u7b97\u548c\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u805a\u5408\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u5728\u5f02\u6784\u5f02\u6b65\u8ba1\u7b97\u548c\u901a\u4fe1\u74f6\u9888\u4e0b\u7684\u7406\u8bba\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7406\u8bba\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u7b97\u6cd5\uff1aRennala NIGT\u652f\u6301AllReduce\u64cd\u4f5c\uff0c\u5728\u5747\u5300\u8bbe\u7f6e\u4e0b\u4f18\u5316\u8ba1\u7b97\u548c\u901a\u4fe1\u590d\u6742\u5ea6\uff1bMalenia NIGT\u4e13\u95e8\u5904\u7406\u5f02\u6b65\u8ba1\u7b97\u548c\u5f02\u6784\u73af\u5883\u3002", "result": "\u5728\u5747\u5300\u8bbe\u7f6e\u4e0b\uff0cRennala NIGT\u663e\u8457\u6539\u5584\u4e86\u603b\u8ba1\u7b97\u548c\u901a\u4fe1\u590d\u6742\u5ea6\uff1b\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\uff0cMalenia NIGT\u5177\u6709\u66f4\u4f18\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u7b97\u6cd5\u5728\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u7684\u5f02\u6b65\u548c\u5f02\u6784\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3a\u5904\u7406\u590d\u6742\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23020", "categories": ["cs.LG", "math.AT"], "pdf": "https://arxiv.org/pdf/2509.23020", "abs": "https://arxiv.org/abs/2509.23020", "authors": ["Jacob Hume", "Pietro Li\u00f2"], "title": "On the Sheafification of Higher-Order Message Passing", "comment": "45 pages, 24 figures", "summary": "Recent work in Topological Deep Learning (TDL) seeks to generalize graph\nlearning's preeminent $message \\ passing$ paradigm to more complex relational\nstructures: simplicial complexes, cell complexes, hypergraphs, and combinations\nthereof. Many approaches to such ${higher\\text{-}order \\ message \\ passing}$\n(HOMP) admit formulation in terms of nonlinear diffusion with the Hodge\n(combinatorial) Laplacian, a graded operator which carries an inductive bias\nthat dimension-$k$ data features correlate with dimension-$k$ topological\nfeatures encoded in the (singular) cohomology of the underlying domain. For\n$k=0$ this recovers the graph Laplacian and its well-studied homophily bias. In\nhigher gradings, however, the Hodge Laplacian's bias is more opaque and\npotentially even degenerate. In this essay, we position sheaf theory as a\nnatural and principled formalism for modifying the Hodge Laplacian's\ndiffusion-mediated interface between local and global descriptors toward more\nexpressive message passing. The sheaf Laplacian's inductive bias correlates\ndimension-$k$ data features with dimension-$k$ $sheaf$ cohomology, a data-aware\ngeneralization of singular cohomology. We will contextualize and novelly extend\nprior theory on sheaf diffusion in graph learning ($k=0$) in such a light --\nand explore how it fails to generalize to $k>0$ -- before developing novel\ntheory and practice for the higher-order setting. Our exposition is accompanied\nby a self-contained introduction shepherding sheaves from the abstract to the\napplied.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5c42\u7406\u8bba\u6765\u6539\u8fdb\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9ad8\u9636\u6d88\u606f\u4f20\u9012\uff0c\u901a\u8fc7\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u66ff\u4ee3\u970d\u5947\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u611f\u77e5\u7684\u62d3\u6251\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u9636\u6d88\u606f\u4f20\u9012\u65b9\u6cd5\u57fa\u4e8e\u970d\u5947\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u4f46\u5176\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u5f52\u7eb3\u504f\u7f6e\u4e0d\u591f\u660e\u786e\u751a\u81f3\u53ef\u80fd\u9000\u5316\u3002\u9700\u8981\u66f4\u5bcc\u8868\u8fbe\u529b\u7684\u6d88\u606f\u4f20\u9012\u6846\u67b6\u6765\u6539\u8fdb\u5c40\u90e8\u548c\u5168\u5c40\u63cf\u8ff0\u7b26\u4e4b\u95f4\u7684\u63a5\u53e3\u3002", "method": "\u91c7\u7528\u5c42\u7406\u8bba\u4f5c\u4e3a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u66ff\u4ee3\u970d\u5947\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u8fdb\u884c\u975e\u7ebf\u6027\u6269\u6563\u3002\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5c06k\u7ef4\u6570\u636e\u7279\u5f81\u4e0ek\u7ef4\u5c42\u4e0a\u540c\u8c03\u76f8\u5173\u8054\uff0c\u8fd9\u662f\u5947\u5f02\u4e0a\u540c\u8c03\u7684\u6570\u636e\u611f\u77e5\u6cdb\u5316\u3002", "result": "\u8bba\u6587\u6269\u5c55\u4e86\u56fe\u5b66\u4e60\u4e2d\u5c42\u6269\u6563\u7684\u73b0\u6709\u7406\u8bba\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u9ad8\u9636\u8bbe\u7f6e\u7684\u65b0\u7406\u8bba\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002", "conclusion": "\u5c42\u7406\u8bba\u4e3a\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u81ea\u7136\u4e14\u539f\u5219\u6027\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u521b\u5efa\u66f4\u5bcc\u8868\u8fbe\u529b\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u7279\u522b\u662f\u5728\u9ad8\u9636\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684\u970d\u5947\u62c9\u666e\u62c9\u65af\u65b9\u6cd5\u3002"}}
{"id": "2509.23024", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23024", "abs": "https://arxiv.org/abs/2509.23024", "authors": ["Melody Zixuan Li", "Kumar Krishna Agrawal", "Arna Ghosh", "Komal Kumar Teru", "Adam Santoro", "Guillaume Lajoie", "Blake A. Richards"], "title": "Tracing the Representation Geometry of Language Models from Pretraining to Post-training", "comment": "33 pages, 14 figures, 9 tables", "summary": "Standard training metrics like loss fail to explain the emergence of complex\ncapabilities in large language models. We take a spectral approach to\ninvestigate the geometry of learned representations across pretraining and\npost-training, measuring effective rank (RankMe) and eigenspectrum decay\n($\\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a\nconsistent non-monotonic sequence of three geometric phases during\nautoregressive pretraining. The initial \"warmup\" phase exhibits rapid\nrepresentational collapse. This is followed by an \"entropy-seeking\" phase,\nwhere the manifold's dimensionality expands substantially, coinciding with peak\nn-gram memorization. Subsequently, a \"compression-seeking\" phase imposes\nanisotropic consolidation, selectively preserving variance along dominant\neigendirections while contracting others, a transition marked with significant\nimprovement in downstream task performance. We show these phases can emerge\nfrom a fundamental interplay of cross-entropy optimization under skewed token\nfrequencies and representational bottlenecks ($d \\ll |V|$). Post-training\nfurther transforms geometry: SFT and DPO drive \"entropy-seeking\" dynamics to\nintegrate specific instructional or preferential data, improving\nin-distribution performance while degrading out-of-distribution robustness.\nConversely, RLVR induces \"compression-seeking\", enhancing reward alignment but\nreducing generation diversity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8c31\u5206\u6790\u65b9\u6cd5\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u793a\u51e0\u4f55\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u4e86\u4e09\u4e2a\u4e00\u81f4\u7684\u51e0\u4f55\u9636\u6bb5\uff1a\u521d\u59cb\u7684\u8868\u793a\u574d\u7f29\u3001\u71b5\u5bfb\u6c42\u9636\u6bb5\u548c\u538b\u7f29\u5bfb\u6c42\u9636\u6bb5\uff0c\u8fd9\u4e9b\u9636\u6bb5\u4e0e\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u6807\u51c6\u8bad\u7ec3\u6307\u6807\u5982\u635f\u5931\u65e0\u6cd5\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u590d\u6742\u80fd\u529b\u7684\u6d8c\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u8868\u793a\u51e0\u4f55\u7684\u89d2\u5ea6\u6765\u7406\u89e3\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u8c31\u5206\u6790\u65b9\u6cd5\uff0c\u6d4b\u91cf\u6709\u6548\u79e9\u548c\u7279\u5f81\u8c31\u8870\u51cf\uff0c\u5206\u6790OLMo\u548cPythia\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8868\u793a\u51e0\u4f55\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e00\u81f4\u7684\u51e0\u4f55\u9636\u6bb5\uff1a\u521d\u59cb\u8868\u793a\u574d\u7f29\u3001\u71b5\u5bfb\u6c42\u9636\u6bb5\uff08\u7ef4\u5ea6\u6269\u5f20\uff09\u548c\u538b\u7f29\u5bfb\u6c42\u9636\u6bb5\uff08\u5404\u5411\u5f02\u6027\u538b\u7f29\uff09\uff0c\u8fd9\u4e9b\u9636\u6bb5\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u76f8\u5173\u3002\u540e\u8bad\u7ec3\u8fdb\u4e00\u6b65\u6539\u53d8\u51e0\u4f55\uff1aSFT\u548cDPO\u9a71\u52a8\u71b5\u5bfb\u6c42\u52a8\u6001\uff0cRLVR\u8bf1\u5bfc\u538b\u7f29\u5bfb\u6c42\u3002", "conclusion": "\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u51e0\u4f55\u53d8\u5316\u63ed\u793a\u4e86\u8868\u793a\u5b66\u4e60\u7684\u57fa\u672c\u673a\u5236\uff0c\u8fd9\u4e9b\u51e0\u4f55\u9636\u6bb5\u6e90\u4e8e\u4ea4\u53c9\u71b5\u4f18\u5316\u4e0e\u8868\u793a\u74f6\u9888\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u80fd\u529b\u6d8c\u73b0\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.23027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23027", "abs": "https://arxiv.org/abs/2509.23027", "authors": ["Yuke Li", "Yujia Zheng", "Tianyi Xiong", "Zhenyi Wang", "Heng Huang"], "title": "Understanding Catastrophic Interference On the Identifibility of Latent Representations", "comment": null, "summary": "Catastrophic interference, also known as catastrophic forgetting, is a\nfundamental challenge in machine learning, where a trained learning model\nprogressively loses performance on previously learned tasks when adapting to\nnew ones. In this paper, we aim to better understand and model the catastrophic\ninterference problem from a latent representation learning point of view, and\npropose a novel theoretical framework that formulates catastrophic interference\nas an identification problem. Our analysis demonstrates that the forgetting\nphenomenon can be quantified by the distance between partial-task aware (PTA)\nand all-task aware (ATA) setups. Building upon recent advances in\nidentifiability theory, we prove that this distance can be minimized through\nidentification of shared latent variables between these setups. When learning,\nwe propose our method \\ourmeos with two-stage training strategy: First, we\nemploy maximum likelihood estimation to learn the latent representations from\nboth PTA and ATA configurations. Subsequently, we optimize the KL divergence to\nidentify and learn the shared latent variables. Through theoretical guarantee\nand empirical validations, we establish that identifying and learning these\nshared representations can effectively mitigate catastrophic interference in\nmachine learning systems. Our approach provides both theoretical guarantees and\npractical performance improvements across both synthetic and benchmark\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u89d2\u5ea6\u7406\u89e3\u707e\u96be\u6027\u5e72\u6270\u7684\u65b0\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8bc6\u522b\u5171\u4eab\u6f5c\u5728\u53d8\u91cf\u6765\u6700\u5c0f\u5316\u9057\u5fd8\u73b0\u8c61\u3002", "motivation": "\u707e\u96be\u6027\u5e72\u6270\uff08\u707e\u96be\u6027\u9057\u5fd8\uff09\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u5f53\u6a21\u578b\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u4f1a\u9010\u6e10\u4e27\u5931\u5bf9\u5148\u524d\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u4ece\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u7684\u89d2\u5ea6\u66f4\u597d\u5730\u7406\u89e3\u548c\u5efa\u6a21\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4ece\u90e8\u5206\u4efb\u52a1\u611f\u77e5\uff08PTA\uff09\u548c\u5168\u4efb\u52a1\u611f\u77e5\uff08ATA\uff09\u914d\u7f6e\u4e2d\u5b66\u4e60\u6f5c\u5728\u8868\u793a\uff0c\u7136\u540e\u4f18\u5316KL\u6563\u5ea6\u6765\u8bc6\u522b\u548c\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8bc6\u522b\u548c\u5b66\u4e60\u8fd9\u4e9b\u5171\u4eab\u8868\u793a\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u707e\u96be\u6027\u5e72\u6270\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u6539\u8fdb\uff0c\u4e3a\u7f13\u89e3\u707e\u96be\u6027\u5e72\u6270\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23030", "abs": "https://arxiv.org/abs/2509.23030", "authors": ["Yang Lv", "Jin Cao", "Ben Niu", "Zhe Sun", "Fengwei Wang", "Fenghua Li", "Hui Li"], "title": "DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture Search for 6G Edge Intelligence", "comment": null, "summary": "The Sixth-Generation (6G) network envisions pervasive artificial intelligence\n(AI) as a core goal, enabled by edge intelligence through on-device data\nutilization. To realize this vision, federated learning (FL) has emerged as a\nkey paradigm for collaborative training across edge devices. However, the\nsensitivity and heterogeneity of edge data pose key challenges to FL: parameter\nsharing risks data reconstruction, and a unified global model struggles to\nadapt to diverse local distributions. In this paper, we propose a novel\nfederated learning framework that integrates personalized differential privacy\n(DP) and adaptive model design. To protect training data, we leverage\nsample-level representations for knowledge sharing and apply a personalized DP\nstrategy to resist reconstruction attacks. To ensure distribution-aware\nadaptation under privacy constraints, we develop a privacy-aware neural\narchitecture search (NAS) algorithm that generates locally customized\narchitectures and hyperparameters. To the best of our knowledge, this is the\nfirst personalized DP solution tailored for representation-based FL with\ntheoretical convergence guarantees. Our scheme achieves strong privacy\nguarantees for training data while significantly outperforming state-of-the-art\nmethods in model performance. Experiments on benchmark datasets such as\nCIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\\%\nover the federated NAS method PerFedRLNAS, while reducing model size to 1/10\nand communication cost to 1/20.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e2a\u6027\u5316\u5dee\u5206\u9690\u79c1\u548c\u81ea\u9002\u5e94\u6a21\u578b\u8bbe\u8ba1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "6G\u7f51\u7edc\u9700\u8981\u8fb9\u7f18\u667a\u80fd\uff0c\u4f46\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u6570\u5171\u4eab\u5b58\u5728\u6570\u636e\u91cd\u6784\u98ce\u9669\uff0c\u7edf\u4e00\u5168\u5c40\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u672c\u5730\u6570\u636e\u5206\u5e03", "method": "\u4f7f\u7528\u6837\u672c\u7ea7\u8868\u793a\u8fdb\u884c\u77e5\u8bc6\u5171\u4eab\uff0c\u5e94\u7528\u4e2a\u6027\u5316\u5dee\u5206\u9690\u79c1\u7b56\u7565\u62b5\u6297\u91cd\u6784\u653b\u51fb\uff1b\u5f00\u53d1\u9690\u79c1\u611f\u77e5\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7b97\u6cd5\u751f\u6210\u672c\u5730\u5b9a\u5236\u5316\u67b6\u6784\u548c\u8d85\u53c2\u6570", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4PerFedRLNAS\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53476.82%\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u52301/10\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u52301/20", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4e3a\u57fa\u4e8e\u8868\u793a\u7684\u8054\u90a6\u5b66\u4e60\u91cf\u8eab\u5b9a\u5236\u7684\u4e2a\u6027\u5316\u5dee\u5206\u9690\u79c1\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\uff0c\u5728\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2509.23037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23037", "abs": "https://arxiv.org/abs/2509.23037", "authors": ["Javad Forough", "Mohammad Maheri", "Hamed Haddadi"], "title": "GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly susceptible to jailbreak\nattacks, which are adversarial prompts that bypass alignment constraints and\ninduce unauthorized or harmful behaviors. These vulnerabilities undermine the\nsafety, reliability, and trustworthiness of LLM outputs, posing critical risks\nin domains such as healthcare, finance, and legal compliance. In this paper, we\npropose GuardNet, a hierarchical filtering framework that detects and filters\njailbreak prompts prior to inference. GuardNet constructs structured graphs\nthat combine sequential links, syntactic dependencies, and attention-derived\ntoken relations to capture both linguistic structure and contextual patterns\nindicative of jailbreak behavior. It then applies graph neural networks at two\nlevels: (i) a prompt-level filter that detects global adversarial prompts, and\n(ii) a token-level filter that pinpoints fine-grained adversarial spans.\nExtensive experiments across three datasets and multiple attack settings show\nthat GuardNet substantially outperforms prior defenses. It raises prompt-level\nF$_1$ scores from 66.4\\% to 99.8\\% on LLM-Fuzzer, and from 67-79\\% to over 94\\%\non PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\\% to\n74-91\\%, with IoU gains up to +28\\%. Despite its structural complexity,\nGuardNet maintains acceptable latency and generalizes well in cross-domain\nevaluations, making it a practical and robust defense against jailbreak threats\nin real-world LLM deployments.", "AI": {"tldr": "GuardNet\u662f\u4e00\u4e2a\u5206\u5c42\u8fc7\u6ee4\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u56fe\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u68c0\u6d4b\u548c\u8fc7\u6ee4\u8d8a\u72f1\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9632\u5fa1\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd9\u4e9b\u5bf9\u6297\u6027\u63d0\u793a\u4f1a\u7ed5\u8fc7\u5bf9\u9f50\u7ea6\u675f\uff0c\u5f15\u53d1\u672a\u7ecf\u6388\u6743\u6216\u6709\u5bb3\u884c\u4e3a\uff0c\u5a01\u80c1LLM\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u5173\u952e\u9886\u57df\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u7ed3\u6784\u5316\u56fe\u7ed3\u5408\u5e8f\u5217\u94fe\u63a5\u3001\u53e5\u6cd5\u4f9d\u8d56\u548c\u6ce8\u610f\u529b\u6d3e\u751f\u7684token\u5173\u7cfb\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e24\u7ea7\u8fc7\u6ee4\uff1a\u63d0\u793a\u7ea7\u8fc7\u6ee4\u68c0\u6d4b\u5168\u5c40\u5bf9\u6297\u63d0\u793a\uff0ctoken\u7ea7\u8fc7\u6ee4\u7cbe\u786e\u5b9a\u4f4d\u7ec6\u7c92\u5ea6\u5bf9\u6297\u7247\u6bb5\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cd\u653b\u51fb\u8bbe\u7f6e\u4e0b\uff0cGuardNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff1a\u5728LLM-Fuzzer\u4e0a\u5c06\u63d0\u793a\u7ea7F1\u5206\u6570\u4ece66.4%\u63d0\u5347\u523099.8%\uff0c\u5728PLeak\u6570\u636e\u96c6\u4e0a\u4ece67-79%\u63d0\u5347\u523094%\u4ee5\u4e0a\uff1btoken\u7ea7F1\u4ece48-75%\u63d0\u5347\u523074-91%\uff0cIoU\u589e\u76ca\u9ad8\u8fbe+28%\u3002", "conclusion": "GuardNet\u867d\u7136\u7ed3\u6784\u590d\u6742\uff0c\u4f46\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u5ef6\u8fdf\uff0c\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\u6cdb\u5316\u826f\u597d\uff0c\u662f\u5b9e\u9645LLM\u90e8\u7f72\u4e2d\u5bf9\u6297\u8d8a\u72f1\u5a01\u80c1\u7684\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2509.23043", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.23043", "abs": "https://arxiv.org/abs/2509.23043", "authors": ["Saleh Bunaiyan", "Corentin Delacour", "Shuvro Chowdhury", "Kyle Lee", "Kerem Y. Camsari"], "title": "IsingFormer: Augmenting Parallel Tempering With Learned Proposals", "comment": "SB, CD, SC, KL are equally contributing authors", "summary": "Markov Chain Monte Carlo (MCMC) underlies both statistical physics and\ncombinatorial optimization, but mixes slowly near critical points and in rough\nlandscapes. Parallel Tempering (PT) improves mixing by swapping replicas across\ntemperatures, yet each replica still relies on slow local updates to change its\nconfiguration. We introduce IsingFormer, a Transformer trained on equilibrium\nsamples that can generate entire spin configurations resembling those from the\ntarget distribution. These uncorrelated samples are used as proposals for\nglobal moves within a Metropolis step in PT, complementing the usual\nsingle-spin flips. On 2D Ising models (sampling), IsingFormer reproduces\nmagnetization and free-energy curves and generalizes to unseen temperatures,\nincluding the critical region. Injecting even a single proposal sharply reduces\nequilibration time, replacing thousands of local updates. On 3D spin glasses\n(optimization), PT enhanced with IsingFormer finds substantially lower-energy\nstates, demonstrating how global moves accelerate search in rugged landscapes.\nFinally, applied to integer factorization encoded as Ising problems,\nIsingFormer trained on a limited set of semiprimes transfers successfully to\nunseen semiprimes, boosting success rates beyond the training distribution.\nSince factorization is a canonical hard benchmark, this ability to generalize\nacross instances highlights the potential of learning proposals that move\nbeyond single problems to entire families of instances. The IsingFormer\ndemonstrates that Monte Carlo methods can be systematically accelerated by\nneural proposals that capture global structure, yielding faster sampling and\nstronger performance in combinatorial optimization.", "AI": {"tldr": "IsingFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u751f\u6210\u5168\u5c40\u81ea\u65cb\u914d\u7f6e\u6765\u52a0\u901f\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u5728Ising\u6a21\u578b\u91c7\u6837\u548c\u81ea\u65cb\u73bb\u7483\u4f18\u5316\u4e2d\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u4f20\u7edfMCMC\u65b9\u6cd5\u5728\u4e34\u754c\u70b9\u548c\u7c97\u7cd9\u80fd\u91cf\u666f\u89c2\u4e2d\u6df7\u5408\u7f13\u6162\uff0c\u5373\u4f7f\u4f7f\u7528\u5e76\u884c\u56de\u706b\u6280\u672f\uff0c\u6bcf\u4e2a\u526f\u672c\u4ecd\u4f9d\u8d56\u7f13\u6162\u7684\u5c40\u90e8\u66f4\u65b0\u6765\u6539\u53d8\u914d\u7f6e\u3002", "method": "\u8bad\u7ec3Transformer\u5728\u5e73\u8861\u6837\u672c\u4e0a\u751f\u6210\u5b8c\u6574\u7684\u81ea\u65cb\u914d\u7f6e\uff0c\u4f5c\u4e3aMetropolis\u6b65\u9aa4\u4e2d\u7684\u5168\u5c40\u79fb\u52a8\u63d0\u8bae\uff0c\u4e0e\u4f20\u7edf\u7684\u5355\u81ea\u65cb\u7ffb\u8f6c\u4e92\u8865\u3002", "result": "\u57282D Ising\u6a21\u578b\u91c7\u6837\u4e2d\uff0cIsingFormer\u91cd\u73b0\u4e86\u78c1\u5316\u548c\u81ea\u7531\u80fd\u66f2\u7ebf\uff0c\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u6e29\u5ea6\uff1b\u57283D\u81ea\u65cb\u73bb\u7483\u4f18\u5316\u4e2d\uff0c\u627e\u5230\u66f4\u4f4e\u80fd\u91cf\u72b6\u6001\uff1b\u5728\u6574\u6570\u5206\u89e3\u95ee\u9898\u4e2d\uff0c\u6210\u529f\u6cdb\u5316\u5230\u672a\u89c1\u534a\u7d20\u6570\u3002", "conclusion": "IsingFormer\u8bc1\u660e\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u6355\u6349\u5168\u5c40\u7ed3\u6784\u7684\u795e\u7ecf\u63d0\u8bae\u7cfb\u7edf\u6027\u5730\u52a0\u901f\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u91c7\u6837\u548c\u66f4\u5f3a\u7684\u7ec4\u5408\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2509.23050", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23050", "abs": "https://arxiv.org/abs/2509.23050", "authors": ["Lin Long", "Changdae Oh", "Seongheon Park", "Yixuan Li"], "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding", "comment": null, "summary": "Large vision-language models (LVLMs) achieve strong performance on multimodal\ntasks, yet they often default to their language prior (LP) -- memorized textual\npatterns from pre-training while under-utilizing visual evidence. Prior\nanalyses of LP mostly rely on input-output probing, which fails to reveal the\ninternal mechanisms governing when and how vision influences model behavior. To\naddress this gap, we present the first systematic analysis of language prior\nthrough the lens of chain-of-embedding, which examines the layer-wise\nrepresentation dynamics within LVLMs. Our analysis reveals a universal\nphenomenon: each model exhibits a Visual Integration Point (VIP), a critical\nlayer at which visual information begins to meaningfully reshape hidden\nrepresentations and influence decoding. Building on this observation, we\nintroduce the Total Visual Integration (TVI) estimator, which aggregates\nrepresentation distance beyond the VIP to quantify how strongly visual query\ninfluences response generation. Across 54 model-dataset combinations spanning 9\ncontemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently\nemerges, and that TVI reliably predicts the strength of language prior. This\noffers a principled toolkit for diagnosing and understanding language prior in\nLVLMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5d4c\u5165\u94fe\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u6574\u5408\u70b9\u7684\u5b58\u5728\uff0c\u5e76\u63d0\u51fa\u4e86\u91cf\u5316\u89c6\u89c9\u4fe1\u606f\u5f71\u54cd\u7684\u603b\u89c6\u89c9\u6574\u5408\u4f30\u8ba1\u5668\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f80\u5f80\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u5ffd\u89c6\u89c6\u89c9\u8bc1\u636e\uff0c\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u65e0\u6cd5\u63ed\u793a\u89c6\u89c9\u4fe1\u606f\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u7684\u5185\u90e8\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5d4c\u5165\u94fe\u5206\u6790\u5c42\u95f4\u8868\u793a\u52a8\u6001\uff0c\u8bc6\u522b\u89c6\u89c9\u6574\u5408\u70b9\uff0c\u5e76\u5f00\u53d1\u603b\u89c6\u89c9\u6574\u5408\u4f30\u8ba1\u5668\u6765\u91cf\u5316\u89c6\u89c9\u67e5\u8be2\u5bf9\u54cd\u5e94\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u572854\u4e2a\u6a21\u578b-\u6570\u636e\u96c6\u7ec4\u5408\u4e2d\uff0c\u89c6\u89c9\u6574\u5408\u70b9\u666e\u904d\u5b58\u5728\uff0c\u603b\u89c6\u89c9\u6574\u5408\u4f30\u8ba1\u5668\u80fd\u53ef\u9760\u9884\u6d4b\u8bed\u8a00\u5148\u9a8c\u7684\u5f3a\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bca\u65ad\u548c\u7406\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u5148\u9a8c\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5de5\u5177\u5305\u3002"}}
{"id": "2509.23052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23052", "abs": "https://arxiv.org/abs/2509.23052", "authors": ["Matt L. Sampson", "Peter Melchior"], "title": "Dynamics of Learning: Generative Schedules from Latent ODEs", "comment": "9 pages, 5 figures, comments welcome", "summary": "The learning rate schedule is one of the most impactful aspects of neural\nnetwork optimization, yet most schedules either follow simple parametric\nfunctions or react only to short-term training signals. None of them are\nsupported by a comprehensive temporal view of how well neural networks actually\ntrain. We present a new learning rate scheduler that models the training\nperformance of neural networks as a dynamical system. It leverages training\nruns from a hyperparameter search to learn a latent representation of the\ntraining process. Given current training metrics, it predicts the future\nlearning rate schedule with the best long-term validation performance. Our\nscheduler generalizes beyond previously observed training dynamics and creates\nspecialized schedules that deviate noticeably from common parametric functions.\nIt achieves SOTA results for image classification with CNN and ResNet models as\nwell as for next-token prediction with a transformer model. The trained models\nare located in flatter regions of the loss landscape and thus provide better\ngeneralization than those trained with other schedules. Our method is\ncomputationally efficient, optimizer-agnostic, and can easily be layered on top\nof ML experiment-tracking platforms. An implementation of our scheduler will be\nmade available after acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u641c\u7d22\u7684\u8bad\u7ec3\u8fd0\u884c\u6765\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6f5c\u5728\u8868\u793a\uff0c\u9884\u6d4b\u5177\u6709\u6700\u4f73\u957f\u671f\u9a8c\u8bc1\u6027\u80fd\u7684\u672a\u6765\u5b66\u4e60\u7387\u8ba1\u5212\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u8981\u4e48\u9075\u5faa\u7b80\u5355\u7684\u53c2\u6570\u51fd\u6570\uff0c\u8981\u4e48\u4ec5\u5bf9\u77ed\u671f\u8bad\u7ec3\u4fe1\u53f7\u505a\u51fa\u53cd\u5e94\uff0c\u7f3a\u4e4f\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b9e\u9645\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5168\u9762\u65f6\u95f4\u89c6\u89d2\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6027\u80fd\u5efa\u6a21\u4e3a\u52a8\u6001\u7cfb\u7edf\uff0c\u5229\u7528\u8d85\u53c2\u6570\u641c\u7d22\u7684\u8bad\u7ec3\u8fd0\u884c\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6f5c\u5728\u8868\u793a\uff0c\u6839\u636e\u5f53\u524d\u8bad\u7ec3\u6307\u6807\u9884\u6d4b\u6700\u4f73\u957f\u671f\u9a8c\u8bc1\u6027\u80fd\u7684\u5b66\u4e60\u7387\u8ba1\u5212\u3002", "result": "\u5728CNN\u3001ResNet\u56fe\u50cf\u5206\u7c7b\u548cTransformer\u4e0b\u4e00\u8bcd\u9884\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u7ed3\u679c\uff0c\u8bad\u7ec3\u6a21\u578b\u4f4d\u4e8e\u635f\u5931\u666f\u89c2\u7684\u66f4\u5e73\u5766\u533a\u57df\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u3001\u4f18\u5316\u5668\u65e0\u5173\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230ML\u5b9e\u9a8c\u8ddf\u8e2a\u5e73\u53f0\u4e0a\uff0c\u63d0\u4f9b\u4f18\u4e8e\u4f20\u7edf\u53c2\u6570\u5316\u51fd\u6570\u7684\u4e13\u95e8\u5316\u5b66\u4e60\u7387\u8ba1\u5212\u3002"}}
{"id": "2509.23074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23074", "abs": "https://arxiv.org/abs/2509.23074", "authors": ["Wanjin Feng", "Yuan Yuan", "Jingtao Ding", "Yong Li"], "title": "Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting", "comment": null, "summary": "In the era of increasingly complex AI models for time series forecasting,\nprogress is often measured by marginal improvements on benchmark leaderboards.\nHowever, this approach suffers from a fundamental flaw: standard evaluation\nmetrics conflate a model's performance with the data's intrinsic\nunpredictability. To address this pressing challenge, we introduce a novel,\npredictability-aligned diagnostic framework grounded in spectral coherence. Our\nframework makes two primary contributions: the Spectral Coherence\nPredictability (SCP), a computationally efficient ($O(N\\log N)$) and\ntask-aligned score that quantifies the inherent difficulty of a given\nforecasting instance, and the Linear Utilization Ratio (LUR), a\nfrequency-resolved diagnostic tool that precisely measures how effectively a\nmodel exploits the linearly predictable information within the data. We\nvalidate our framework's effectiveness and leverage it to reveal two core\ninsights. First, we provide the first systematic evidence of \"predictability\ndrift\", demonstrating that a task's forecasting difficulty varies sharply over\ntime. Second, our evaluation reveals a key architectural trade-off: complex\nmodels are superior for low-predictability data, whereas linear models are\nhighly effective on more predictable tasks. We advocate for a paradigm shift,\nmoving beyond simplistic aggregate scores toward a more insightful,\npredictability-aware evaluation that fosters fairer model comparisons and a\ndeeper understanding of model behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9891\u8c31\u76f8\u5e72\u6027\u7684\u53ef\u9884\u6d4b\u6027\u5bf9\u9f50\u8bca\u65ad\u6846\u67b6\uff0c\u5305\u542bSCP\u8bc4\u5206\u548cLUR\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u53ef\u9884\u6d4b\u6027\u6f02\u79fb\u73b0\u8c61\u548c\u6a21\u578b\u67b6\u6784\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5c06\u6a21\u578b\u6027\u80fd\u4e0e\u6570\u636e\u5185\u5728\u4e0d\u53ef\u9884\u6d4b\u6027\u6df7\u4e3a\u4e00\u8c08\u7684\u6839\u672c\u7f3a\u9677\uff0c\u63a8\u52a8\u4ece\u7b80\u5355\u805a\u5408\u8bc4\u5206\u8f6c\u5411\u66f4\u6df1\u5165\u7684\u53ef\u9884\u6d4b\u6027\u611f\u77e5\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u9891\u8c31\u76f8\u5e72\u6027\u53ef\u9884\u6d4b\u6027(SCP)\u8bc4\u5206\u6765\u91cf\u5316\u9884\u6d4b\u4efb\u52a1\u7684\u56fa\u6709\u96be\u5ea6\uff0c\u4ee5\u53ca\u7ebf\u6027\u5229\u7528\u7387\u6bd4(LUR)\u6765\u8bca\u65ad\u6a21\u578b\u5229\u7528\u7ebf\u6027\u53ef\u9884\u6d4b\u4fe1\u606f\u7684\u6548\u7387\u3002", "result": "\u9996\u6b21\u7cfb\u7edf\u8bc1\u660e\u4e86\"\u53ef\u9884\u6d4b\u6027\u6f02\u79fb\"\u73b0\u8c61\uff0c\u5373\u4efb\u52a1\u7684\u9884\u6d4b\u96be\u5ea6\u968f\u65f6\u95f4\u5267\u70c8\u53d8\u5316\uff1b\u63ed\u793a\u4e86\u590d\u6742\u6a21\u578b\u5728\u4f4e\u53ef\u9884\u6d4b\u6027\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u7ebf\u6027\u6a21\u578b\u5728\u66f4\u53ef\u9884\u6d4b\u4efb\u52a1\u4e2d\u9ad8\u6548\u7684\u67b6\u6784\u6743\u8861\u3002", "conclusion": "\u5021\u5bfc\u8bc4\u4f30\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u4ece\u7b80\u5355\u805a\u5408\u8bc4\u5206\u8f6c\u5411\u66f4\u5177\u6d1e\u5bdf\u529b\u7684\u53ef\u9884\u6d4b\u6027\u611f\u77e5\u8bc4\u4f30\uff0c\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u6a21\u578b\u6bd4\u8f83\u548c\u66f4\u6df1\u5165\u7684\u884c\u4e3a\u7406\u89e3\u3002"}}
{"id": "2509.23077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23077", "abs": "https://arxiv.org/abs/2509.23077", "authors": ["Reza Rahimi Azghan", "Gautham Krishna Gudur", "Mohit Malu", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "CLAD-Net: Continual Activity Recognition in Multi-Sensor Wearable Systems", "comment": null, "summary": "The rise of deep learning has greatly advanced human behavior monitoring\nusing wearable sensors, particularly human activity recognition (HAR). While\ndeep models have been widely studied, most assume stationary data distributions\n- an assumption often violated in real-world scenarios. For example, sensor\ndata from one subject may differ significantly from another, leading to\ndistribution shifts. In continual learning, this shift is framed as a sequence\nof tasks, each corresponding to a new subject. Such settings suffer from\ncatastrophic forgetting, where prior knowledge deteriorates as new tasks are\nlearned. This challenge is compounded by the scarcity and inconsistency of\nlabeled data in human studies. To address these issues, we propose CLAD-Net\n(Continual Learning with Attention and Distillation), a framework enabling\nwearable-sensor models to be updated continuously without sacrificing\nperformance on past tasks. CLAD-Net integrates a self-supervised transformer,\nacting as long-term memory, with a supervised Convolutional Neural Network\n(CNN) trained via knowledge distillation for activity classification. The\ntransformer captures global activity patterns through cross-attention across\nbody-mounted sensors, learning generalizable representations without labels.\nMeanwhile, the CNN leverages knowledge distillation to retain prior knowledge\nduring subject-wise fine-tuning. On PAMAP2, CLAD-Net achieves 91.36 percent\nfinal accuracy with only 8.78 percent forgetting, surpassing memory-based and\nregularization-based baselines such as Experience Replay and Elastic Weight\nConsolidation. In semi-supervised settings with only 10-20 percent labeled\ndata, CLAD-Net still delivers strong performance, demonstrating robustness to\nlabel scarcity. Ablation studies further validate each module's contribution.", "AI": {"tldr": "CLAD-Net\u662f\u4e00\u4e2a\u7528\u4e8e\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6301\u7eed\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763Transformer\u548c\u77e5\u8bc6\u84b8\u998fCNN\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5206\u5e03\u504f\u79fb\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728PAMAP2\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8691.36%\u7684\u6700\u7ec8\u51c6\u786e\u7387\u548c\u4ec58.78%\u7684\u9057\u5fd8\u7387\u3002", "motivation": "\u89e3\u51b3\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u4e2d\u5b58\u5728\u7684\u975e\u5e73\u7a33\u5206\u5e03\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u5b66\u4e60\u573a\u666f\u4e0b\uff0c\u5f53\u6a21\u578b\u5b66\u4e60\u65b0\u7528\u6237\u6570\u636e\u65f6\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4ee5\u53ca\u4eba\u7c7b\u7814\u7a76\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "CLAD-Net\u6574\u5408\u4e86\u81ea\u76d1\u7763Transformer\uff08\u4f5c\u4e3a\u957f\u671f\u8bb0\u5fc6\uff09\u548c\u76d1\u7763CNN\uff08\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8fdb\u884c\u6d3b\u52a8\u5206\u7c7b\uff09\u3002Transformer\u901a\u8fc7\u8de8\u8eab\u4f53\u4f20\u611f\u5668\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6355\u83b7\u5168\u5c40\u6d3b\u52a8\u6a21\u5f0f\uff0c\u5b66\u4e60\u65e0\u9700\u6807\u7b7e\u7684\u53ef\u6cdb\u5316\u8868\u793a\uff1bCNN\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u5728\u9010\u7528\u6237\u5fae\u8c03\u65f6\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002", "result": "\u5728PAMAP2\u6570\u636e\u96c6\u4e0a\uff0cCLAD-Net\u8fbe\u523091.36%\u7684\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u4ec5\u67098.78%\u7684\u9057\u5fd8\u7387\uff0c\u4f18\u4e8e\u57fa\u4e8e\u8bb0\u5fc6\u548c\u6b63\u5219\u5316\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u4ec5\u670910-20%\u6807\u8bb0\u6570\u636e\u7684\u534a\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u4ecd\u80fd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u8d21\u732e\u3002", "conclusion": "CLAD-Net\u6709\u6548\u5730\u89e3\u51b3\u4e86\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23085", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23085", "abs": "https://arxiv.org/abs/2509.23085", "authors": ["Hyunwoo Lee", "Hayoung Choi", "Hyunju Kim"], "title": "Signal Preserving Weight Initialization for Odd-Sigmoid Activations", "comment": null, "summary": "Activation functions critically influence trainability and expressivity, and\nrecent work has therefore explored a broad range of nonlinearities. However,\nactivations and weight initialization are interdependent: without an\nappropriate initialization method, nonlinearities can cause saturation,\nvariance collapse, and increased learning rate sensitivity. We address this by\ndefining an odd sigmoid function class and, given any activation f in this\nclass, proposing an initialization method tailored to f. The method selects a\nnoise scale in closed form so that forward activations remain well dispersed up\nto a target layer, thereby avoiding collapse to zero or saturation.\nEmpirically, the approach trains reliably without normalization layers,\nexhibits strong data efficiency, and enables learning for activations under\nwhich standard initialization methods (Xavier, He, Orthogonal) often do not\nconverge reliably.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5947Sigmoid\u6fc0\u6d3b\u51fd\u6570\u7c7b\u7684\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u9002\u5f53\u7684\u566a\u58f0\u5c3a\u5ea6\u6765\u907f\u514d\u6fc0\u6d3b\u503c\u5d29\u6e83\u6216\u9971\u548c\uff0c\u4f7f\u7f51\u7edc\u65e0\u9700\u5f52\u4e00\u5316\u5c42\u4e5f\u80fd\u53ef\u9760\u8bad\u7ec3\u3002", "motivation": "\u6fc0\u6d3b\u51fd\u6570\u548c\u6743\u91cd\u521d\u59cb\u5316\u76f8\u4e92\u4f9d\u8d56\uff0c\u4e0d\u5408\u9002\u7684\u521d\u59cb\u5316\u4f1a\u5bfc\u81f4\u9971\u548c\u3001\u65b9\u5dee\u5d29\u6e83\u548c\u5b66\u4e60\u7387\u654f\u611f\u6027\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u5947Sigmoid\u51fd\u6570\u7c7b\uff0c\u5e76\u4e3a\u8be5\u7c7b\u4e2d\u7684\u4efb\u4f55\u6fc0\u6d3b\u51fd\u6570f\u63d0\u51fa\u4e13\u95e8\u7684\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u5f0f\u89e3\u9009\u62e9\u566a\u58f0\u5c3a\u5ea6\u6765\u4fdd\u6301\u524d\u5411\u6fc0\u6d3b\u7684\u826f\u597d\u5206\u6563\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5f52\u4e00\u5316\u5c42\u5373\u53ef\u53ef\u9760\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6570\u636e\u6548\u7387\uff0c\u5e76\u80fd\u652f\u6301\u6807\u51c6\u521d\u59cb\u5316\u65b9\u6cd5\uff08Xavier\u3001He\u3001Orthogonal\uff09\u901a\u5e38\u65e0\u6cd5\u53ef\u9760\u6536\u655b\u7684\u6fc0\u6d3b\u51fd\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u6fc0\u6d3b\u51fd\u6570\u548c\u6743\u91cd\u521d\u59cb\u5316\u7684\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u907f\u514d\u9971\u548c\u548c\u65b9\u5dee\u5d29\u6e83\uff0c\u63d0\u5347\u8bad\u7ec3\u53ef\u9760\u6027\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2509.23087", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23087", "abs": "https://arxiv.org/abs/2509.23087", "authors": ["Deshu Chen", "Yuchen Liu", "Zhijian Zhou", "Chao Qu", "Yuan Qi"], "title": "Unleashing Flow Policies with Distributional Critics", "comment": null, "summary": "Flow-based policies have recently emerged as a powerful tool in offline and\noffline-to-online reinforcement learning, capable of modeling the complex,\nmultimodal behaviors found in pre-collected datasets. However, the full\npotential of these expressive actors is often bottlenecked by their critics,\nwhich typically learn a single, scalar estimate of the expected return. To\naddress this limitation, we introduce the Distributional Flow Critic (DFC), a\nnovel critic architecture that learns the complete state-action return\ndistribution. Instead of regressing to a single value, DFC employs flow\nmatching to model the distribution of return as a continuous, flexible\ntransformation from a simple base distribution to the complex target\ndistribution of returns. By doing so, DFC provides the expressive flow-based\npolicy with a rich, distributional Bellman target, which offers a more stable\nand informative learning signal. Extensive experiments across D4RL and OGBench\nbenchmarks demonstrate that our approach achieves strong performance,\nespecially on tasks requiring multimodal action distributions, and excels in\nboth offline and offline-to-online fine-tuning compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86Distributional Flow Critic (DFC)\uff0c\u4e00\u79cd\u65b0\u7684\u8bc4\u8bba\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u5b66\u4e60\u5b8c\u6574\u7684\u72b6\u6001-\u52a8\u4f5c\u56de\u62a5\u5206\u5e03\u6765\u89e3\u51b3\u6d41\u7b56\u7565\u4e2d\u8bc4\u8bba\u5bb6\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u6d41\u7b56\u7565\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5176\u6f5c\u529b\u5f80\u5f80\u53d7\u5230\u8bc4\u8bba\u5bb6\u67b6\u6784\u7684\u9650\u5236\u2014\u2014\u4f20\u7edf\u8bc4\u8bba\u5bb6\u53ea\u5b66\u4e60\u5355\u4e00\u7684\u6807\u91cf\u671f\u671b\u56de\u62a5\u4f30\u8ba1\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6d41\u7b56\u7565\u7684\u591a\u6a21\u6001\u5efa\u6a21\u80fd\u529b\u3002", "method": "DFC\u91c7\u7528\u6d41\u5339\u914d\u6280\u672f\uff0c\u5c06\u56de\u62a5\u5206\u5e03\u5efa\u6a21\u4e3a\u4ece\u7b80\u5355\u57fa\u5206\u5e03\u5230\u590d\u6742\u76ee\u6807\u56de\u62a5\u5206\u5e03\u7684\u8fde\u7eed\u7075\u6d3b\u53d8\u6362\uff0c\u800c\u975e\u56de\u5f52\u5230\u5355\u4e00\u503c\u3002", "result": "\u5728D4RL\u548cOGBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebf\u5fae\u8c03\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DFC\u901a\u8fc7\u4e3a\u8868\u8fbe\u6027\u6d41\u7b56\u7565\u63d0\u4f9b\u4e30\u5bcc\u7684\u5206\u5e03\u5f0fBellman\u76ee\u6807\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23089", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23089", "abs": "https://arxiv.org/abs/2509.23089", "authors": ["Sylee", "Beltiukov", "Satyandra Guthula", "Wenbo Guo", "Walter Willinger", "Arpit Gupta"], "title": "Demystifying Network Foundation Models", "comment": null, "summary": "This work presents a systematic investigation into the latent knowledge\nencoded within Network Foundation Models (NFMs) that focuses on hidden\nrepresentations analysis rather than pure downstream task performance.\nDifferent from existing efforts, we analyze the models through a three-part\nevaluation: Embedding Geometry Analysis to assess representation space\nutilization, Metric Alignment Assessment to measure correspondence with\ndomain-expert features, and Causal Sensitivity Testing to evaluate robustness\nto protocol perturbations. Using five diverse network datasets spanning\ncontrolled and real-world environments, we evaluate four state-of-the-art NFMs,\nrevealing that they all exhibit significant anisotropy, inconsistent feature\nsensitivity patterns, an inability to separate the high-level context, payload\ndependency, and other properties. Our work identifies numerous limitations\nacross all models and demonstrates that addressing them can significantly\nimprove model performance (by up to +0.35 $F_1$ score without architectural\nchanges).", "AI": {"tldr": "\u5bf9\u7f51\u7edc\u57fa\u7840\u6a21\u578b(NFMs)\u6f5c\u5728\u77e5\u8bc6\u7684\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u901a\u8fc7\u5d4c\u5165\u51e0\u4f55\u5206\u6790\u3001\u5ea6\u91cf\u5bf9\u9f50\u8bc4\u4f30\u548c\u56e0\u679c\u654f\u611f\u6027\u6d4b\u8bd5\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u5404\u5411\u5f02\u6027\u3001\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u654f\u611f\u6027\u6a21\u5f0f\u7b49\u95ee\u9898\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u800c\u672c\u6587\u65e8\u5728\u6df1\u5165\u5206\u6790\u7f51\u7edc\u57fa\u7840\u6a21\u578b\u4e2d\u7f16\u7801\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u5173\u6ce8\u9690\u85cf\u8868\u793a\u800c\u975e\u7eaf\u7cb9\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e09\u90e8\u5206\u8bc4\u4f30\u65b9\u6cd5\uff1a\u5d4c\u5165\u51e0\u4f55\u5206\u6790\u8bc4\u4f30\u8868\u793a\u7a7a\u95f4\u5229\u7528\uff0c\u5ea6\u91cf\u5bf9\u9f50\u8bc4\u4f30\u6d4b\u91cf\u4e0e\u9886\u57df\u4e13\u5bb6\u7279\u5f81\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u56e0\u679c\u654f\u611f\u6027\u6d4b\u8bd5\u8bc4\u4f30\u5bf9\u534f\u8bae\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002\u4f7f\u7528\u4e94\u4e2a\u4e0d\u540c\u7684\u7f51\u7edc\u6570\u636e\u96c6\u8bc4\u4f30\u56db\u4e2a\u6700\u5148\u8fdb\u7684NFMs\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u5404\u5411\u5f02\u6027\u3001\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u654f\u611f\u6027\u6a21\u5f0f\u3001\u65e0\u6cd5\u5206\u79bb\u9ad8\u5c42\u4e0a\u4e0b\u6587\u3001\u8d1f\u8f7d\u4f9d\u8d56\u6027\u7b49\u95ee\u9898\u3002\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff08F1\u5206\u6570\u6700\u9ad8\u63d0\u53470.35\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u7f51\u7edc\u57fa\u7840\u6a21\u578b\u5728\u6f5c\u5728\u77e5\u8bc6\u7f16\u7801\u65b9\u9762\u7684\u591a\u79cd\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u901a\u8fc7\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u672a\u6765NFMs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.23092", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23092", "abs": "https://arxiv.org/abs/2509.23092", "authors": ["Christopher Scarvelis", "Justin Solomon"], "title": "Sensitivity Analysis for Diffusion Models", "comment": null, "summary": "Training a diffusion model approximates a map from a data distribution $\\rho$\nto the optimal score function $s_t$ for that distribution. Can we differentiate\nthis map? If we could, then we could predict how the score, and ultimately the\nmodel's samples, would change under small perturbations to the training set\nbefore committing to costly retraining. We give a closed-form procedure for\ncomputing this map's directional derivatives, relying only on black-box access\nto a pre-trained score model and its derivatives with respect to its inputs. We\nextend this result to estimate the sensitivity of a diffusion model's samples\nto additive perturbations of its target measure, with runtime comparable to\nsampling from a diffusion model and computing log-likelihoods along the sample\npath. Our method is robust to numerical and approximation error, and the\nresulting sensitivities correlate with changes in an image diffusion model's\nsamples after retraining and fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6269\u6563\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u6270\u52a8\u7684\u654f\u611f\u6027\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9884\u6d4b\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u5c06\u6570\u636e\u5206\u5e03\u6620\u5c04\u5230\u6700\u4f18\u5f97\u5206\u51fd\u6570\uff0c\u4f46\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u5982\u679c\u80fd\u8ba1\u7b97\u8fd9\u79cd\u6620\u5c04\u7684\u5bfc\u6570\uff0c\u5c31\u80fd\u5728\u91cd\u65b0\u8bad\u7ec3\u524d\u9884\u6d4b\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u6270\u52a8\u7684\u54cd\u5e94\u3002", "method": "\u5f00\u53d1\u4e86\u95ed\u5f0f\u7a0b\u5e8f\u8ba1\u7b97\u6620\u5c04\u7684\u65b9\u5411\u5bfc\u6570\uff0c\u4ec5\u9700\u9884\u8bad\u7ec3\u5f97\u5206\u6a21\u578b\u53ca\u5176\u5bf9\u8f93\u5165\u7684\u5bfc\u6570\u3002\u6269\u5c55\u8be5\u65b9\u6cd5\u4f30\u8ba1\u6269\u6563\u6a21\u578b\u6837\u672c\u5bf9\u76ee\u6807\u6d4b\u5ea6\u52a0\u6027\u6270\u52a8\u7684\u654f\u611f\u6027\u3002", "result": "\u65b9\u6cd5\u5bf9\u6570\u503c\u548c\u8fd1\u4f3c\u8bef\u5dee\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e0e\u4ece\u6269\u6563\u6a21\u578b\u91c7\u6837\u548c\u8ba1\u7b97\u6837\u672c\u8def\u5f84\u5bf9\u6570\u4f3c\u7136\u76f8\u5f53\u3002\u654f\u611f\u6027\u7ed3\u679c\u4e0e\u91cd\u65b0\u8bad\u7ec3\u548c\u5fae\u8c03\u540e\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6837\u672c\u7684\u53d8\u5316\u76f8\u5173\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u6269\u6563\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u6270\u52a8\u654f\u611f\u6027\u7684\u5de5\u5177\uff0c\u53ef\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\u53d8\u5316\u3002"}}
{"id": "2509.23095", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23095", "abs": "https://arxiv.org/abs/2509.23095", "authors": ["Xiangqi Wang", "Yue Huang", "Yujun Zhou", "Xiaonan Luo", "Kehan Guo", "Xiangliang Zhang"], "title": "Causally-Enhanced Reinforcement Policy Optimization", "comment": "Reinforcement learning publication of 24 pages", "summary": "Large language models (LLMs) trained with reinforcement objectives often\nachieve superficially correct answers via shortcut strategies, pairing correct\noutputs with spurious or unfaithful reasoning and degrading under small causal\nperturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a\ndrop-in reward-shaping framework that augments policy optimization with a\ndifferentiable proxy for causal coherence along the generation pathway from\nprompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal\ninfluence with Jacobian-based sensitivities, counterfactually hardens these\nsignals to suppress nuisance cues, and fuses the resulting coherence score with\ntask-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single\ntunable between accuracy and coherence trade-off. The unified reward integrates\nwith PPO/GRPO without architectural changes. Across reasoning benchmarks and\ncausal stress tests, CE-PO reduces reward hacking and unfaithful\nchain-of-thought while improving robustness to correlation-causation flips and\nlight counterfactual edits, all at near-parity accuracy. Experimental results\nacross 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on\naverage (up to 9.58%), while improving robustness to correlation-causation\nflips and light counterfactual edits.", "AI": {"tldr": "CE-PO\u662f\u4e00\u4e2a\u589e\u5f3a\u7b56\u7565\u4f18\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u4e00\u81f4\u6027\u5956\u52b1\u6765\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u653b\u51fb\u548c\u4e0d\u5fe0\u5b9e\u63a8\u7406\uff0c\u540c\u65f6\u63d0\u9ad8\u5bf9\u56e0\u679c\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7ecf\u5e38\u901a\u8fc7\u6377\u5f84\u7b56\u7565\u83b7\u5f97\u8868\u9762\u6b63\u786e\u7684\u7b54\u6848\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u4e0d\u5fe0\u5b9e\uff0c\u4e14\u5728\u5c0f\u56e0\u679c\u6270\u52a8\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528Jacobian\u654f\u611f\u6027\u4f30\u8ba1\u6a21\u578b\u5185\u90e8\u5f71\u54cd\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u786c\u5316\u6291\u5236\u566a\u58f0\u7ebf\u7d22\uff0c\u5c06\u56e0\u679c\u4e00\u81f4\u6027\u5206\u6570\u4e0e\u4efb\u52a1\u51c6\u786e\u6027\u53cd\u9988\u901a\u8fc7Minkowski\u7ec4\u5408\u5668\u878d\u5408\uff0c\u96c6\u6210\u5230PPO/GRPO\u4e2d\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCE-PO\u5e73\u5747\u6bd4\u57fa\u7ebf\u63d0\u9ad85.49%\u7684\u51c6\u786e\u7387\uff08\u6700\u9ad89.58%\uff09\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5bf9\u76f8\u5173-\u56e0\u679c\u7ffb\u8f6c\u548c\u8f7b\u91cf\u53cd\u4e8b\u5b9e\u7f16\u8f91\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CE-PO\u80fd\u6709\u6548\u51cf\u5c11\u5956\u52b1\u653b\u51fb\u548c\u4e0d\u5fe0\u5b9e\u63a8\u7406\u94fe\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u56e0\u679c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.23106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23106", "abs": "https://arxiv.org/abs/2509.23106", "authors": ["Aman Gupta", "Rafael Celente", "Abhishek Shivanna", "D. T. Braithwaite", "Gregory Dexter", "Shao Tang", "Hiroto Udagawa", "Daniel Silva", "Rohan Ramanath", "S. Sathiya Keerthi"], "title": "Effective Quantization of Muon Optimizer States", "comment": "17 pages", "summary": "The Muon optimizer, based on matrix orthogonalization, has recently shown\nfaster convergence and up to 2x computational efficiency over AdamW in LLM\npretraining. Like AdamW, Muon is stateful, requiring storage of both model\nweights and accumulated gradients. While 8-bit AdamW variants mitigate this\noverhead using blockwise quantization, they are typically stable only under\ndynamic quantization - which improves stability on linear quantization for\nextreme values. In this paper, we introduce the 8-bit Muon optimizer using\nblockwise quantization, supporting both linear and dynamic schemes. We\ndemonstrate that 8-bit Muon maintains stability under both, while delivering\n$\\sim$74\\% reduction in memory footprint compared to full-precision Muon. In\nextensive experiments, 8-bit Muon closely matches the performance of Muon while\noutperforming AdamW and 8-bit AdamW in pre-training a 1.6B model on 4B FineWeb\ntokens. It also shows competitive results when fine-tuning the Llama 3.2 3B\nmodel on post-training data. We also provide a theoretical perspective to help\nexplain this robustness under quantization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e868\u4f4dMuon\u4f18\u5316\u5668\uff0c\u4f7f\u7528\u5206\u5757\u91cf\u5316\u6280\u672f\uff0c\u5728\u4fdd\u6301Muon\u4f18\u5316\u5668\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u6bd4AdamW\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u50cfAdamW\u4e00\u6837\u9700\u8981\u5b58\u50a8\u6a21\u578b\u6743\u91cd\u548c\u7d2f\u79ef\u68af\u5ea6\uff0c\u5b58\u5728\u5185\u5b58\u5f00\u9500\u95ee\u9898\u3002\u73b0\u6709\u76848\u4f4dAdamW\u53d8\u4f53\u901a\u5e38\u53ea\u5728\u52a8\u6001\u91cf\u5316\u4e0b\u7a33\u5b9a\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u652f\u6301\u7ebf\u6027\u548c\u52a8\u6001\u91cf\u5316\u76848\u4f4dMuon\u4f18\u5316\u5668\u3002", "method": "\u91c7\u7528\u5206\u5757\u91cf\u5316\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u652f\u6301\u7ebf\u6027\u548c\u52a8\u6001\u91cf\u5316\u65b9\u6848\u76848\u4f4dMuon\u4f18\u5316\u5668\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6765\u89e3\u91ca\u5176\u5728\u91cf\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "8\u4f4dMuon\u76f8\u6bd4\u5168\u7cbe\u5ea6Muon\u51cf\u5c11\u4e86\u7ea674%\u7684\u5185\u5b58\u5360\u7528\uff0c\u57281.6B\u6a21\u578b\u9884\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\u4e0eMuon\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4f18\u4e8eAdamW\u548c8\u4f4dAdamW\u3002\u5728Llama 3.2 3B\u6a21\u578b\u5fae\u8c03\u4e2d\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "8\u4f4dMuon\u4f18\u5316\u5668\u5728\u4fdd\u6301Muon\u4f18\u5316\u5668\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23115", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23115", "abs": "https://arxiv.org/abs/2509.23115", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility", "comment": "Advances in Neural Information Processing Systems 39 (NeurIPS) 2025", "summary": "Predicting human mobility is inherently challenging due to complex long-range\ndependencies and multi-scale periodic behaviors. To address this, we introduce\nRHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),\na unified framework that leverages large language models (LLMs) as\ngeneral-purpose spatio-temporal predictors and trajectory reasoners.\nMethodologically, RHYTHM employs temporal tokenization to partition each\ntrajectory into daily segments and encode them as discrete tokens with\nhierarchical attention that captures both daily and weekly dependencies,\nthereby significantly reducing the sequence length while preserving cyclical\ninformation. Additionally, we enrich token representations by adding\npre-computed prompt embeddings for trajectory segments and prediction targets\nvia a frozen LLM, and feeding these combined embeddings back into the LLM\nbackbone to capture complex interdependencies. Computationally, RHYTHM freezes\nthe pretrained LLM's backbone to reduce attention complexity and memory cost.\nWe evaluate our model against state-of-the-art methods using three real-world\ndatasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a\n5.0% increase on weekends, and a 24.6% reduction in training time. Code is\npublicly available at https://github.com/he-h/rhythm.", "AI": {"tldr": "RHYTHM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6807\u8bb0\u5316\u548c\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u9762\u4e34\u590d\u6742\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u591a\u5c3a\u5ea6\u5468\u671f\u6027\u884c\u4e3a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u6807\u8bb0\u5316\u5c06\u8f68\u8ff9\u5206\u5272\u4e3a\u65e5\u6bb5\u5e76\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u4f7f\u7528\u5206\u5c42\u6ce8\u610f\u529b\u6355\u6349\u65e5\u548c\u5468\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u63d0\u793a\u5d4c\u5165\u589e\u5f3a\u6807\u8bb0\u8868\u793a\uff0c\u5e76\u51bb\u7ed3\u9884\u8bad\u7ec3LLM\u9aa8\u5e72\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cRHYTHM\u5b9e\u73b0\u4e86\u6574\u4f53\u51c6\u786e\u7387\u63d0\u53472.4%\uff0c\u5468\u672b\u51c6\u786e\u7387\u63d0\u53475.0%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1124.6%\u3002", "conclusion": "RHYTHM\u6846\u67b6\u6210\u529f\u5229\u7528LLM\u4f5c\u4e3a\u901a\u7528\u65f6\u7a7a\u9884\u6d4b\u5668\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.23126", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23126", "abs": "https://arxiv.org/abs/2509.23126", "authors": ["Dengyi Liu", "Honggang Wang", "Hua Fang"], "title": "Impute-MACFM: Imputation based on Mask-Aware Flow Matching", "comment": "Preprint, 2025. 9 pages (main) + appendix", "summary": "Tabular data are central to many applications, especially longitudinal data\nin healthcare, where missing values are common, undermining model fidelity and\nreliability. Prior imputation methods either impose restrictive assumptions or\nstruggle with complex cross-feature structure, while recent generative\napproaches suffer from instability and costly inference. We propose\nImpute-MACFM, a mask-aware conditional flow matching framework for tabular\nimputation that addresses missingness mechanisms, missing completely at random,\nmissing at random, and missing not at random. Its mask-aware objective builds\ntrajectories only on missing entries while constraining predicted velocity to\nremain near zero on observed entries, using flexible nonlinear schedules.\nImpute-MACFM combines: (i) stability penalties on observed positions, (ii)\nconsistency regularization enforcing local invariance, and (iii) time-decayed\nnoise injection for numeric features. Inference uses constraint-preserving\nordinary differential equation integration with per-step projection to fix\nobserved values, optionally aggregating multiple trajectories for robustness.\nAcross diverse benchmarks, Impute-MACFM achieves state-of-the-art results while\ndelivering more robust, efficient, and higher-quality imputation than competing\napproaches, establishing flow matching as a promising direction for tabular\nmissing-data problems, including longitudinal data.", "AI": {"tldr": "\u63d0\u51faImpute-MACFM\uff0c\u4e00\u79cd\u7528\u4e8e\u8868\u683c\u6570\u636e\u586b\u8865\u7684\u63a9\u7801\u611f\u77e5\u6761\u4ef6\u6d41\u5339\u914d\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u968f\u673a\u7f3a\u5931\u3001\u968f\u673a\u7f3a\u5931\u548c\u975e\u968f\u673a\u7f3a\u5931\u673a\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u8868\u683c\u6570\u636e\u5728\u533b\u7597\u7b49\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u7f3a\u5931\u503c\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4f1a\u964d\u4f4e\u6a21\u578b\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u73b0\u6709\u586b\u8865\u65b9\u6cd5\u8981\u4e48\u5047\u8bbe\u8fc7\u4e8e\u4e25\u683c\uff0c\u8981\u4e48\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u8de8\u7279\u5f81\u7ed3\u6784\uff0c\u800c\u6700\u8fd1\u7684\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u63a9\u7801\u611f\u77e5\u76ee\u6807\u51fd\u6570\uff0c\u4ec5\u5728\u7f3a\u5931\u6761\u76ee\u4e0a\u6784\u5efa\u8f68\u8ff9\uff0c\u540c\u65f6\u7ea6\u675f\u89c2\u6d4b\u6761\u76ee\u7684\u9884\u6d4b\u901f\u5ea6\u63a5\u8fd1\u96f6\uff1b\u7ed3\u5408\u7a33\u5b9a\u6027\u60e9\u7f5a\u3001\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u65f6\u95f4\u8870\u51cf\u566a\u58f0\u6ce8\u5165\uff1b\u63a8\u7406\u65f6\u4f7f\u7528\u7ea6\u675f\u4fdd\u6301\u7684ODE\u79ef\u5206\u548c\u6bcf\u6b65\u6295\u5f71\u6765\u56fa\u5b9a\u89c2\u6d4b\u503c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cImpute-MACFM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u6bd4\u7ade\u4e89\u65b9\u6cd5\u66f4\u7a33\u5065\u3001\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u586b\u8865\u7ed3\u679c\u3002", "conclusion": "\u6d41\u5339\u914d\u662f\u89e3\u51b3\u8868\u683c\u7f3a\u5931\u6570\u636e\u95ee\u9898\uff08\u5305\u62ec\u7eb5\u5411\u6570\u636e\uff09\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.23129", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23129", "abs": "https://arxiv.org/abs/2509.23129", "authors": ["Haotian Liu", "Shuo Wang", "Hongteng Xu"], "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy\nOptimization (GRPO) and its variants, play a central role in developing\nreasoning models. However, these methods often suffer from a critical\noverconfidence issue, which prevents them from achieving self-aware reasoning\nmodels. In this study, we propose a simple yet effective confidence-calibration\ngroup sequence policy gradient method, called C$^2$GSPG, which simultaneously\nenhances reasoning performance while suppressing overconfidence. In principle,\nwe propose a Group Sequence Policy Gradient (GSPG) framework for learning\nreasoning models, which eliminates the token-level bias commonly appearing in\nGRPO and its variants. In this framework, we define the model confidence for\neach reasoning problem using the normalized sequence-level probability, and\nthen apply a cross-entropy regularizer to calibrate the model confidence to the\nsequence's reward. We demonstrate that the confidence calibration regularizer\nand GSPG are collaborative for binary rewards, as their objectives always share\nthe same gradient direction. For non-binary rewards, we apply nonlinear reward\nnormalization and adaptive regularizer clipping, mitigating the potential\nconflict between the two objectives. Applying C$^2$GSPG to post-train large\nlanguage models in logical and mathematical reasoning tasks, we show its\nsuperiority over state-of-the-art methods in both reasoning accuracy and\nconfidence calibration. The code of C$^2$GSPG is available at\nhttps://github.com/HaotianLiu123/CCGSPG.", "AI": {"tldr": "\u63d0\u51faC\u00b2GSPG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u7ec4\u5e8f\u5217\u7b56\u7565\u68af\u5ea6\u6846\u67b6\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u7406\u6027\u80fd\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\u53ca\u5176\u53d8\u4f53\uff09\u5728\u5f00\u53d1\u63a8\u7406\u6a21\u578b\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u963b\u788d\u4e86\u81ea\u611f\u77e5\u63a8\u7406\u6a21\u578b\u7684\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u7ec4\u5e8f\u5217\u7b56\u7565\u68af\u5ea6\uff08GSPG\uff09\u6846\u67b6\u6d88\u9664token\u7ea7\u504f\u5dee\uff0c\u4f7f\u7528\u5f52\u4e00\u5316\u5e8f\u5217\u7ea7\u6982\u7387\u5b9a\u4e49\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u5e94\u7528\u4ea4\u53c9\u71b5\u6b63\u5219\u5668\u5c06\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5230\u5e8f\u5217\u5956\u52b1\u3002\u5bf9\u4e8e\u975e\u4e8c\u5143\u5956\u52b1\uff0c\u91c7\u7528\u975e\u7ebf\u6027\u5956\u52b1\u5f52\u4e00\u5316\u548c\u81ea\u9002\u5e94\u6b63\u5219\u5668\u88c1\u526a\u3002", "result": "\u5728\u903b\u8f91\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\uff0cC\u00b2GSPG\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "C\u00b2GSPG\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548cGSPG\u6846\u67b6\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6548\u679c\u3002"}}
{"id": "2509.23135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23135", "abs": "https://arxiv.org/abs/2509.23135", "authors": ["Yang Chen", "Menglin Zou", "Jiaqi Zhang", "Yitan Zhang", "Junyi Yang", "Gael Gendron", "Libo Zhang", "Jiamou Liu", "Michael J. Witbrock"], "title": "Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm", "comment": "Accepted to NeurIPS 2025", "summary": "Inverse Reinforcement Learning (IRL) learns a reward function to explain\nexpert demonstrations. Modern IRL methods often use the adversarial (minimax)\nformulation that alternates between reward and policy optimization, which often\nlead to unstable training. Recent non-adversarial IRL approaches improve\nstability by jointly learning reward and policy via energy-based formulations\nbut lack formal guarantees. This work bridges this gap. We first present a\nunified view showing canonical non-adversarial methods explicitly or implicitly\nmaximize the likelihood of expert behavior, which is equivalent to minimizing\nthe expected return gap. This insight leads to our main contribution: Trust\nRegion Reward Optimization (TRRO), a framework that guarantees monotonic\nimprovement in this likelihood via a Minorization-Maximization process. We\ninstantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical\nand stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to\nthe stability guarantees of Trust Region Policy Optimization (TRPO) in forward\nRL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward\nrecovery, policy imitation with high sample efficiency on MuJoCo and\nGym-Robotics benchmarks and a real-world animal behavior modeling task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TRRO\u6846\u67b6\u548cPIRO\u7b97\u6cd5\uff0c\u901a\u8fc7Minorization-Maximization\u8fc7\u7a0b\u4fdd\u8bc1\u4e13\u5bb6\u884c\u4e3a\u4f3c\u7136\u5355\u8c03\u63d0\u5347\uff0c\u89e3\u51b3\u4e86IRL\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3IRL\u65b9\u6cd5\u4f7f\u7528\u5bf9\u6297\u5f0f\u8bad\u7ec3\u5e38\u5bfc\u81f4\u4e0d\u7a33\u5b9a\uff0c\u800c\u6700\u8fd1\u7684\u975e\u5bf9\u6297\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faTRRO\u6846\u67b6\uff0c\u901a\u8fc7Minorization-Maximization\u8fc7\u7a0b\u4fdd\u8bc1\u5355\u8c03\u63d0\u5347\uff1b\u5b9e\u4f8b\u5316\u4e3aPIRO\u7b97\u6cd5\uff0c\u7ed3\u5408\u4fe1\u4efb\u533a\u57df\u4f18\u5316\u3002", "result": "\u7406\u8bba\u4e0aTRRO\u63d0\u4f9bIRL\u7684\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff1b\u5b9e\u9a8c\u663e\u793aPIRO\u5728\u5956\u52b1\u6062\u590d\u3001\u7b56\u7565\u6a21\u4eff\u548c\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TRRO\u4e3aIRL\u63d0\u4f9b\u4e86\u7c7b\u4f3cTRPO\u7684\u7a33\u5b9a\u6027\u7406\u8bba\u4fdd\u8bc1\uff0cPIRO\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3aIRL\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23139", "abs": "https://arxiv.org/abs/2509.23139", "authors": ["Sipeng Chen", "Yan Zhang", "Shibo Li"], "title": "Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations", "comment": null, "summary": "Implicit Neural Representations (INRs) have emerged as a transformative\nparadigm in signal processing and computer vision, excelling in tasks from\nimage reconstruction to 3D shape modeling. Yet their effectiveness is\nfundamentally limited by the absence of principled strategies for optimal\nconfiguration - spanning activation selection, initialization scales,\nlayer-wise adaptation, and their intricate interdependencies. These choices\ndictate performance, stability, and generalization, but current practice relies\non ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often\nleading to inconsistent results across modalities. This work introduces\nOptiINR, the first unified framework that formulates INR configuration as a\nrigorous optimization problem. Leveraging Bayesian optimization, OptiINR\nefficiently explores the joint space of discrete activation families - such as\nsinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER) - and\ntheir associated continuous initialization parameters. This systematic approach\nreplaces fragmented manual tuning with a coherent, data-driven optimization\nprocess. By delivering globally optimal configurations, OptiINR establishes a\nprincipled foundation for INR design, consistently maximizing performance\nacross diverse signal processing applications.", "AI": {"tldr": "OptiINR\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5c06\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u914d\u7f6e\u5236\u5b9a\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u81ea\u52a8\u5bfb\u627e\u6700\u4f73\u6fc0\u6d3b\u51fd\u6570\u548c\u521d\u59cb\u5316\u53c2\u6570\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u624b\u52a8\u8c03\u4f18\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dINR\u7684\u6709\u6548\u6027\u53d7\u5230\u7f3a\u4e4f\u539f\u5219\u6027\u914d\u7f6e\u7b56\u7565\u7684\u9650\u5236\uff0c\u5305\u62ec\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u3001\u521d\u59cb\u5316\u5c3a\u5ea6\u3001\u5c42\u95f4\u9002\u5e94\u7b49\uff0c\u8fd9\u4e9b\u9009\u62e9\u5f71\u54cd\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e34\u65f6\u542f\u53d1\u5f0f\u6216\u66b4\u529b\u7f51\u683c\u641c\u7d22\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u9ad8\u6548\u63a2\u7d22\u79bb\u6563\u6fc0\u6d3b\u51fd\u6570\u65cf\uff08\u5982SIREN\u3001WIRE\u3001FINER\uff09\u53ca\u5176\u76f8\u5173\u8fde\u7eed\u521d\u59cb\u5316\u53c2\u6570\u7684\u8054\u5408\u7a7a\u95f4\uff0c\u7528\u7cfb\u7edf\u5316\u7684\u6570\u636e\u9a71\u52a8\u4f18\u5316\u8fc7\u7a0b\u66ff\u4ee3\u788e\u7247\u5316\u624b\u52a8\u8c03\u4f18\u3002", "result": "OptiINR\u901a\u8fc7\u63d0\u4f9b\u5168\u5c40\u6700\u4f18\u914d\u7f6e\uff0c\u4e3aINR\u8bbe\u8ba1\u5efa\u7acb\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5728\u591a\u6837\u4fe1\u53f7\u5904\u7406\u5e94\u7528\u4e2d\u6301\u7eed\u6700\u5927\u5316\u6027\u80fd\u3002", "conclusion": "OptiINR\u662f\u7b2c\u4e00\u4e2a\u5c06INR\u914d\u7f6e\u5236\u5b9a\u4e3a\u4e25\u683c\u4f18\u5316\u95ee\u9898\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86INR\u914d\u7f6e\u7684\u6311\u6218\uff0c\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u539f\u5219\u6027\u8bbe\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2509.23145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23145", "abs": "https://arxiv.org/abs/2509.23145", "authors": ["Xiaowen Ma", "Shuning Ge", "Fan Yang", "Xiangyu Li", "Yun Chen", "Mengting Ma", "Wei Zhang", "Zhipeng Liu"], "title": "TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts", "comment": "Under Review", "summary": "Transformer-based architectures dominate time series modeling by enabling\nglobal attention over all timestamps, yet their rigid 'one-size-fits-all'\ncontext aggregation fails to address two critical challenges in real-world\ndata: (1) inherent lag effects, where the relevance of historical timestamps to\na query varies dynamically; (2) anomalous segments, which introduce noisy\nsignals that degrade forecasting accuracy. To resolve these problems, we\npropose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism\nthat reimagines key-value (K-V) pairs as local experts (each specialized in a\ndistinct temporal context) and performs adaptive expert selection for each\nquery via localized filtering of irrelevant timestamps. Complementing this\nlocal adaptation, a shared global expert preserves the Transformer's strength\nin capturing long-range dependencies. We then replace the vanilla attention\nmechanism in popular time-series Transformer frameworks (i.e., PatchTST and\nTimer) with TMOE, without extra structural modifications, yielding our specific\nversion TimeExpert and general version TimeExpert-G. Extensive experiments on\nseven real-world long-term forecasting benchmarks demonstrate that TimeExpert\nand TimeExpert-G outperform state-of-the-art methods. Code is available at\nhttps://github.com/xwmaxwma/TimeExpert.", "AI": {"tldr": "\u63d0\u51faTemporal Mix of Experts (TMOE)\u673a\u5236\uff0c\u5c06\u952e\u503c\u5bf9\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5c40\u90e8\u4e13\u5bb6\uff0c\u901a\u8fc7\u5c40\u90e8\u8fc7\u6ee4\u65e0\u5173\u65f6\u95f4\u6233\u5b9e\u73b0\u81ea\u9002\u5e94\u4e13\u5bb6\u9009\u62e9\uff0c\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u4e13\u5bb6\u4ee5\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1) \u56fa\u6709\u7684\u6ede\u540e\u6548\u5e94\uff0c\u5386\u53f2\u65f6\u95f4\u6233\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u52a8\u6001\u53d8\u5316\uff1b(2) \u5f02\u5e38\u6bb5\u5f15\u5165\u566a\u58f0\u4fe1\u53f7\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5c06\u952e\u503c\u5bf9\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5c40\u90e8\u4e13\u5bb6\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u5c40\u90e8\u8fc7\u6ee4\u673a\u5236\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u81ea\u9002\u5e94\u9009\u62e9\u4e13\u5bb6\uff0c\u540c\u65f6\u4fdd\u7559\u5171\u4eab\u5168\u5c40\u4e13\u5bb6\u3002\u5728PatchTST\u548cTimer\u6846\u67b6\u4e2d\u66ff\u6362\u539f\u59cb\u6ce8\u610f\u529b\u673a\u5236\u4e3aTMOE\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u957f\u671f\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTimeExpert\u548cTimeExpert-G\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "TMOE\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u6ede\u540e\u6548\u5e94\u548c\u5f02\u5e38\u6bb5\u95ee\u9898\uff0c\u5728\u4fdd\u6301Transformer\u957f\u671f\u4f9d\u8d56\u6355\u6349\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.23152", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23152", "abs": "https://arxiv.org/abs/2509.23152", "authors": ["Zhicheng Yang", "Zhijiang Guo", "Yinya Huang", "Yongxin Wang", "Yiwei Wang", "Xiaodan Liang", "Jing Tang"], "title": "Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers", "comment": "15 pages, 7 figures", "summary": "Test-time scaling via solution sampling and aggregation has become a key\nparadigm for improving the reasoning performance of Large Language Models\n(LLMs). While reward model selection is commonly employed in this approach, it\noften fails to identify minority-yet-correct answers, which limits its\neffectiveness beyond that of simple majority voting. We argue that this\nlimitation stems from a lack of informative critique signals during verifier\ntraining. To bridge this gap, we introduce Mirror-Critique, a framework that\ntrains a verifier with informative critiques. Our key insight is to leverage\nthe rich critique signal by contrasting model-generated solutions with\nground-truth solutions. We deploy a small instruction-tuned model to synthesize\nhigh-quality critique data with rejection sampling that teaches the verifier\nnot only what is wrong, but also why. The synthetic data is used to cold-start\nthe LLMs in the RLVR process to further improve the verification ability. The\nresulting Mirror-Verifier is deployed to evaluate candidate solutions by\ngenerating multiple critiques per solution, aggregating them into a verify\nscore used for weighted voting or selective abstention. The experimental\nresults show that our Mirror-Verifier significantly outperforms majority voting\nin terms of solution accuracy and also improves the solver's honesty to\nrecognize and abstain from answering beyond its capability boundaries.", "AI": {"tldr": "\u63d0\u51faMirror-Critique\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u751f\u6210\u89e3\u4e0e\u771f\u5b9e\u89e3\u6765\u8bad\u7ec3\u5177\u6709\u4fe1\u606f\u6027\u6279\u5224\u80fd\u529b\u7684\u9a8c\u8bc1\u5668\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u9009\u62e9\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u5e38\u5e38\u65e0\u6cd5\u8bc6\u522b\u5c11\u6570\u4f46\u6b63\u786e\u7684\u7b54\u6848\uff0c\u9650\u5236\u4e86\u5176\u8d85\u8d8a\u7b80\u5355\u591a\u6570\u6295\u7968\u7684\u6548\u679c\uff0c\u8fd9\u6e90\u4e8e\u9a8c\u8bc1\u5668\u8bad\u7ec3\u4e2d\u7f3a\u4e4f\u4fe1\u606f\u6027\u6279\u5224\u4fe1\u53f7\u3002", "method": "\u5229\u7528\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u5408\u6210\u9ad8\u8d28\u91cf\u6279\u5224\u6570\u636e\uff0c\u6559\u5bfc\u9a8c\u8bc1\u5668\u4e0d\u4ec5\u77e5\u9053\u4ec0\u4e48\u9519\u8bef\uff0c\u8fd8\u77e5\u9053\u4e3a\u4ec0\u4e48\u9519\u8bef\uff1b\u4f7f\u7528\u5408\u6210\u6570\u636e\u51b7\u542f\u52a8RLVR\u8fc7\u7a0b\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u6279\u5224\u5e76\u805a\u5408\u6210\u9a8c\u8bc1\u5206\u6570\u7528\u4e8e\u52a0\u6743\u6295\u7968\u6216\u9009\u62e9\u6027\u5f03\u6743\u3002", "result": "Mirror-Verifier\u5728\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u591a\u6570\u6295\u7968\uff0c\u5e76\u63d0\u9ad8\u4e86\u6c42\u89e3\u5668\u8bc6\u522b\u548c\u5f03\u6743\u8d85\u51fa\u5176\u80fd\u529b\u8fb9\u754c\u95ee\u9898\u7684\u8bda\u5b9e\u6027\u3002", "conclusion": "Mirror-Critique\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u4fe1\u606f\u6027\u6279\u5224\u4fe1\u53f7\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u6027\u80fd\u9a8c\u8bc1\u80fd\u529b\u3002"}}
{"id": "2509.23156", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23156", "abs": "https://arxiv.org/abs/2509.23156", "authors": ["Prashant Govindarajan", "Mathieu Reymond", "Antoine Clavaud", "Mariano Phielipp", "Santiago Miret", "Sarath Chandar"], "title": "CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning", "comment": null, "summary": "In silico design and optimization of new materials primarily relies on\nhigh-accuracy atomic simulators that perform density functional theory (DFT)\ncalculations. While recent works showcase the strong potential of machine\nlearning to accelerate the material design process, they mostly consist of\ngenerative approaches that do not use direct DFT signals as feedback to improve\ntraining and generation mainly due to DFT's high computational cost. To aid the\nadoption of direct DFT signals in the materials design loop through online\nreinforcement learning (RL), we propose CrystalGym, an open-source RL\nenvironment for crystalline material discovery. Using CrystalGym, we benchmark\ncommon value- and policy-based reinforcement learning algorithms for designing\nvarious crystals conditioned on target properties. Concretely, we optimize for\nchallenging properties like the band gap, bulk modulus, and density, which are\ndirectly calculated from DFT in the environment. While none of the algorithms\nwe benchmark solve all CrystalGym tasks, our extensive experiments and\nablations show different sample efficiencies and ease of convergence to\noptimality for different algorithms and environment settings. Additionally, we\ninclude a case study on the scope of fine-tuning large language models with\nreinforcement learning for improving DFT-based rewards. Our goal is for\nCrystalGym to serve as a test bed for reinforcement learning researchers and\nmaterial scientists to address these real-world design problems with practical\napplications. We therefore introduce a novel class of challenges for\nreinforcement learning methods dealing with time-consuming reward signals,\npaving the way for future interdisciplinary research for machine learning\nmotivated by real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86CrystalGym\uff0c\u4e00\u4e2a\u7528\u4e8e\u6676\u4f53\u6750\u6599\u53d1\u73b0\u7684\u5f00\u6e90\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u65e8\u5728\u5c06DFT\u8ba1\u7b97\u76f4\u63a5\u6574\u5408\u5230\u6750\u6599\u8bbe\u8ba1\u5faa\u73af\u4e2d\u3002", "motivation": "\u5f53\u524d\u6750\u6599\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u9ad8\u7cbe\u5ea6DFT\u8ba1\u7b97\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5f88\u5c11\u76f4\u63a5\u4f7f\u7528DFT\u4fe1\u53f7\u4f5c\u4e3a\u53cd\u9988\uff0c\u4e3b\u8981\u662f\u56e0\u4e3aDFT\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5f00\u53d1\u4e86CrystalGym\u73af\u5883\uff0c\u5728\u73af\u5883\u4e2d\u4f7f\u7528DFT\u76f4\u63a5\u8ba1\u7b97\u80fd\u5e26\u9699\u3001\u4f53\u79ef\u6a21\u91cf\u548c\u5bc6\u5ea6\u7b49\u5c5e\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5e38\u89c1\u7684\u57fa\u4e8e\u503c\u548c\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u867d\u7136\u57fa\u51c6\u7b97\u6cd5\u672a\u80fd\u89e3\u51b3\u6240\u6709CrystalGym\u4efb\u52a1\uff0c\u4f46\u5b9e\u9a8c\u663e\u793a\u4e86\u4e0d\u540c\u7b97\u6cd5\u548c\u73af\u5883\u8bbe\u7f6e\u4e0b\u7684\u6837\u672c\u6548\u7387\u548c\u6536\u655b\u5230\u6700\u4f18\u6027\u7684\u5dee\u5f02\u3002", "conclusion": "CrystalGym\u65e8\u5728\u6210\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8005\u548c\u6750\u6599\u79d1\u5b66\u5bb6\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e3a\u5904\u7406\u8017\u65f6\u5956\u52b1\u4fe1\u53f7\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f15\u5165\u65b0\u7684\u6311\u6218\u7c7b\u522b\u3002"}}
{"id": "2509.23158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23158", "abs": "https://arxiv.org/abs/2509.23158", "authors": ["Yufei Shen", "Ji Hwan Park", "Minchao Huang", "Jared F. Benge", "Justin F. Rousseau", "Rosemary A. Lester-Smith", "Edison Thomaz"], "title": "Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization", "comment": "Accepted at 2025 IEEE EMBS International Conference on Biomedical and\n  Health Informatics (IEEE BHI 2025)", "summary": "Early detection of cognitive impairment is critical for timely diagnosis and\nintervention, yet infrequent clinical assessments often lack the sensitivity\nand temporal resolution to capture subtle cognitive declines in older adults.\nPassive smartphone sensing has emerged as a promising approach for naturalistic\nand continuous cognitive monitoring. Building on this potential, we implemented\na Long Short-Term Memory (LSTM) model to detect cognitive impairment from\nsequences of daily behavioral features, derived from multimodal sensing data\ncollected in an ongoing one-year study of older adults. Our key contributions\nare two techniques to enhance model generalizability across participants: (1)\nroutine-aware augmentation, which generates synthetic sequences by replacing\neach day with behaviorally similar alternatives, and (2) demographic\npersonalization, which reweights training samples to emphasize those from\nindividuals demographically similar to the test participant. Evaluated on\n6-month data from 36 older adults, these techniques jointly improved the Area\nUnder the Precision-Recall Curve (AUPRC) of the model trained on sensing and\ndemographic features from 0.637 to 0.766, highlighting the potential of\nscalable monitoring of cognitive impairment in aging populations with passive\nsensing.", "AI": {"tldr": "\u4f7f\u7528LSTM\u6a21\u578b\u7ed3\u5408\u65e5\u5e38\u884c\u4e3a\u7279\u5f81\u5e8f\u5217\u68c0\u6d4b\u8ba4\u77e5\u969c\u788d\uff0c\u901a\u8fc7\u5e38\u89c4\u611f\u77e5\u589e\u5f3a\u548c\u4eba\u53e3\u7edf\u8ba1\u4e2a\u6027\u5316\u4e24\u79cd\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u572836\u540d\u8001\u5e74\u4eba6\u4e2a\u6708\u6570\u636e\u4e0aAUPRC\u4ece0.637\u63d0\u5347\u52300.766\u3002", "motivation": "\u4e34\u5e8a\u8bc4\u4f30\u9891\u7387\u4f4e\u4e14\u7f3a\u4e4f\u654f\u611f\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u8001\u5e74\u4eba\u7ec6\u5fae\u7684\u8ba4\u77e5\u8870\u9000\u3002\u88ab\u52a8\u667a\u80fd\u624b\u673a\u4f20\u611f\u4e3a\u81ea\u7136\u8fde\u7eed\u8ba4\u77e5\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u591a\u6a21\u6001\u4f20\u611f\u6570\u636e\u63d0\u53d6\u65e5\u5e38\u884c\u4e3a\u7279\u5f81\u5e8f\u5217\uff0c\u4f7f\u7528LSTM\u6a21\u578b\u68c0\u6d4b\u8ba4\u77e5\u969c\u788d\u3002\u91c7\u7528\u4e24\u79cd\u6280\u672f\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u6027\uff1a\u5e38\u89c4\u611f\u77e5\u589e\u5f3a\uff08\u7528\u884c\u4e3a\u76f8\u4f3c\u66ff\u4ee3\u65e5\u751f\u6210\u5408\u6210\u5e8f\u5217\uff09\u548c\u4eba\u53e3\u7edf\u8ba1\u4e2a\u6027\u5316\uff08\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6837\u672c\u4ee5\u5f3a\u8c03\u4e0e\u6d4b\u8bd5\u53c2\u4e0e\u8005\u4eba\u53e3\u7edf\u8ba1\u76f8\u4f3c\u7684\u4e2a\u4f53\uff09\u3002", "result": "\u572836\u540d\u8001\u5e74\u4eba6\u4e2a\u6708\u6570\u636e\u4e0a\uff0c\u7ed3\u5408\u4f20\u611f\u548c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u6a21\u578bAUPRC\u4ece0.637\u63d0\u5347\u52300.766\u3002", "conclusion": "\u8fd9\u4e9b\u6280\u672f\u7a81\u663e\u4e86\u4f7f\u7528\u88ab\u52a8\u4f20\u611f\u5bf9\u8001\u9f84\u5316\u4eba\u7fa4\u8fdb\u884c\u53ef\u6269\u5c55\u8ba4\u77e5\u969c\u788d\u76d1\u6d4b\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.23159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23159", "abs": "https://arxiv.org/abs/2509.23159", "authors": ["Ziheng Peng", "Shijie Ren", "Xinyue Gu", "Linxiao Yang", "Xiting Wang", "Liang Sun"], "title": "ProtoTS: Learning Hierarchical Prototypes for Explainable Time Series Forecasting", "comment": "Under submission", "summary": "While deep learning has achieved impressive performance in time series\nforecasting, it becomes increasingly crucial to understand its decision-making\nprocess for building trust in high-stakes scenarios. Existing interpretable\nmodels often provide only local and partial explanations, lacking the\ncapability to reveal how heterogeneous and interacting input variables jointly\nshape the overall temporal patterns in the forecast curve. We propose ProtoTS,\na novel interpretable forecasting framework that achieves both high accuracy\nand transparent decision-making through modeling prototypical temporal\npatterns. ProtoTS computes instance-prototype similarity based on a denoised\nrepresentation that preserves abundant heterogeneous information. The\nprototypes are organized hierarchically to capture global temporal patterns\nwith coarse prototypes while capturing finer-grained local variations with\ndetailed prototypes, enabling expert steering and multi-level interpretability.\nExperiments on multiple realistic benchmarks, including a newly released LOF\ndataset, show that ProtoTS not only exceeds existing methods in forecast\naccuracy but also delivers expert-steerable interpretations for better model\nunderstanding and decision support.", "AI": {"tldr": "ProtoTS\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u539f\u578b\u65f6\u95f4\u6a21\u5f0f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u900f\u660e\u51b3\u7b56\uff0c\u63d0\u4f9b\u591a\u5c42\u6b21\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6a21\u578b\u53ea\u80fd\u63d0\u4f9b\u5c40\u90e8\u548c\u90e8\u5206\u89e3\u91ca\uff0c\u65e0\u6cd5\u63ed\u793a\u5f02\u8d28\u8f93\u5165\u53d8\u91cf\u5982\u4f55\u5171\u540c\u5851\u9020\u9884\u6d4b\u66f2\u7ebf\u4e2d\u7684\u6574\u4f53\u65f6\u95f4\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8e\u53bb\u566a\u8868\u793a\u8ba1\u7b97\u5b9e\u4f8b-\u539f\u578b\u76f8\u4f3c\u5ea6\uff0c\u539f\u578b\u5206\u5c42\u7ec4\u7ec7\u4ee5\u6355\u83b7\u5168\u5c40\u65f6\u95f4\u6a21\u5f0f\u548c\u5c40\u90e8\u7ec6\u7c92\u5ea6\u53d8\u5316\uff0c\u652f\u6301\u4e13\u5bb6\u5f15\u5bfc\u3002", "result": "\u5728\u591a\u4e2a\u73b0\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProtoTS\u4e0d\u4ec5\u9884\u6d4b\u7cbe\u5ea6\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u63d0\u4f9b\u4e13\u5bb6\u53ef\u5f15\u5bfc\u7684\u89e3\u91ca\u4ee5\u652f\u6301\u6a21\u578b\u7406\u89e3\u548c\u51b3\u7b56\u3002", "conclusion": "ProtoTS\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u900f\u660e\u51b3\u7b56\u7684\u5e73\u8861\uff0c\u4e3a\u9ad8\u98ce\u9669\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u4fe1\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.23162", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.23162", "abs": "https://arxiv.org/abs/2509.23162", "authors": ["Chandan Tankala", "Krishnakumar Balasubramanian"], "title": "Dense associative memory on the Bures-Wasserstein space", "comment": null, "summary": "Dense associative memories (DAMs) store and retrieve patterns via\nenergy-functional fixed points, but existing models are limited to vector\nrepresentations. We extend DAMs to probability distributions equipped with the\n2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of\nGaussian densities. Our framework defines a log-sum-exp energy over stored\ndistributions and a retrieval dynamics aggregating optimal transport maps in a\nGibbs-weighted manner. Stationary points correspond to self-consistent\nWasserstein barycenters, generalizing classical DAM fixed points. We prove\nexponential storage capacity, provide quantitative retrieval guarantees under\nWasserstein perturbations, and validate the model on synthetic and real-world\ndistributional tasks. This work elevates associative memory from vectors to\nfull distributions, bridging classical DAMs with modern generative modeling and\nenabling distributional storage and retrieval in memory-augmented learning.", "AI": {"tldr": "\u5c06\u5bc6\u96c6\u5173\u8054\u8bb0\u5fc6\u4ece\u5411\u91cf\u6269\u5c55\u5230\u6982\u7387\u5206\u5e03\uff0c\u7279\u522b\u662f\u9ad8\u65af\u5206\u5e03\uff0c\u4f7f\u75282-Wasserstein\u8ddd\u79bb\u5b9a\u4e49\u80fd\u91cf\u51fd\u6570\u548c\u68c0\u7d22\u52a8\u6001\uff0c\u5b9e\u73b0\u4e86\u5206\u5e03\u5b58\u50a8\u548c\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u5173\u8054\u8bb0\u5fc6\u6a21\u578b\u4ec5\u9650\u4e8e\u5411\u91cf\u8868\u793a\uff0c\u65e0\u6cd5\u5904\u7406\u6982\u7387\u5206\u5e03\u3002\u672c\u6587\u65e8\u5728\u5c06\u5173\u8054\u8bb0\u5fc6\u6269\u5c55\u5230\u5206\u5e03\u7a7a\u95f4\uff0c\u8fde\u63a5\u7ecf\u5178DAM\u4e0e\u73b0\u4ee3\u751f\u6210\u5efa\u6a21\u3002", "method": "\u57282-Wasserstein\u8ddd\u79bb\u4e0b\u5b9a\u4e49\u5bf9\u6570-\u6c42\u548c-\u6307\u6570\u80fd\u91cf\u51fd\u6570\uff0c\u68c0\u7d22\u52a8\u6001\u901a\u8fc7\u5409\u5e03\u65af\u52a0\u6743\u65b9\u5f0f\u805a\u5408\u6700\u4f18\u4f20\u8f93\u6620\u5c04\uff0c\u56fa\u5b9a\u70b9\u5bf9\u5e94\u81ea\u6d3d\u7684Wasserstein\u91cd\u5fc3\u3002", "result": "\u8bc1\u660e\u4e86\u6307\u6570\u5b58\u50a8\u5bb9\u91cf\uff0c\u63d0\u4f9b\u4e86Wasserstein\u6270\u52a8\u4e0b\u7684\u5b9a\u91cf\u68c0\u7d22\u4fdd\u8bc1\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u5173\u8054\u8bb0\u5fc6\u4ece\u5411\u91cf\u63d0\u5347\u5230\u5b8c\u6574\u5206\u5e03\uff0c\u4e3a\u8bb0\u5fc6\u589e\u5f3a\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5b58\u50a8\u548c\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.23173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23173", "abs": "https://arxiv.org/abs/2509.23173", "authors": ["Hangwei Zhang", "Chun Kang", "Yan Wang", "Difan Zou"], "title": "F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning", "comment": "NeurIPS 2025 Main Track", "summary": "Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for\ncomplex downstream tasks has proven effective in vision and language\nprocessing, yet this paradigm remains unexplored in scientific machine\nlearning, where the objective is to model complex physical systems. We conduct\nthe first systematic study of PEFT for pre-trained Large Operator Models (LOMs)\nobtained by scaling variants of Fourier Neural Operator. First, we observe that\nthe widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance\non LOMs than Adapter tuning. Then, we further theoretically establish that\nstacked LoRA incurs a depth-amplified lower bound on approximation error within\nFourier layers, whereas adapters retain universal approximation capacity and,\nby concentrating parameters on energy-dominant low-frequency modes, attain\nexponentially decaying error with bottleneck width in the Fourier domain.\nMotivated by the robust empirical gains of adapters and by our theoretical\ncharacterization of PDE solutions as spectrally sparse, we introduce\nFrequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity\nbased on spectral complexity, assigning higher-dimension modules to\nlow-frequency components and lower-dimension modules to high-frequency\ncomponents. Our F-Adapters establish state-of-the-art (SOTA) results on\nmultiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both\ngeneralization and spectral fidelity over LoRA and other PEFT techniques\ncommonly used in LLMs. To the best of our knowledge, this work is the first to\nexplore PEFT for scientific machine-learning and establishes F-Adapter as an\neffective paradigm for this domain.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0Adapter\u8c03\u4f18\u5728\u5927\u578b\u7b97\u5b50\u6a21\u578b(LOMs)\u4e0a\u663e\u8457\u4f18\u4e8eLoRA\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u9891\u8c31\u590d\u6742\u5ea6\u7684\u9891\u7387\u81ea\u9002\u5e94\u9002\u914d\u5668(F-Adapter)\uff0c\u57283D Navier-Stokes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u4e2d\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u9886\u57df\u5c1a\u672a\u63a2\u7d22\uff0c\u8be5\u9886\u57df\u7684\u76ee\u6807\u662f\u5efa\u6a21\u590d\u6742\u7269\u7406\u7cfb\u7edf\u3002", "method": "1. \u7cfb\u7edf\u6bd4\u8f83LoRA\u548cAdapter\u5728\u5927\u578b\u7b97\u5b50\u6a21\u578b\u4e0a\u7684\u6027\u80fd\uff1b2. \u7406\u8bba\u5206\u6790LoRA\u5728\u5085\u91cc\u53f6\u5c42\u4e2d\u7684\u8fd1\u4f3c\u8bef\u5dee\u95ee\u9898\uff1b3. \u63d0\u51fa\u9891\u7387\u81ea\u9002\u5e94\u9002\u914d\u5668(F-Adapter)\uff0c\u6839\u636e\u9891\u8c31\u590d\u6742\u5ea6\u5206\u914d\u9002\u914d\u5668\u5bb9\u91cf\u3002", "result": "F-Adapter\u5728\u591a\u4e2a\u6311\u6218\u6027\u76843D Navier-Stokes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9891\u8c31\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8eLoRA\u548c\u5176\u4ed6PEFT\u6280\u672f\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u63a2\u7d22\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2dPEFT\u7684\u5de5\u4f5c\uff0c\u786e\u7acb\u4e86F-Adapter\u4f5c\u4e3a\u8be5\u9886\u57df\u7684\u6709\u6548\u8303\u5f0f\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7684\u9891\u8c31\u7a00\u758f\u7279\u6027\u3002"}}
{"id": "2509.23183", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23183", "abs": "https://arxiv.org/abs/2509.23183", "authors": ["Guohao Chen", "Shuaicheng Niu", "Deyu Chen", "Jiahao Yang", "Zitian Zhang", "Mingkui Tan", "Pengcheng Wu", "Zhiqi Shen"], "title": "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse", "comment": null, "summary": "Test-time entropy minimization helps adapt a model to novel environments and\nincentivize its reasoning capability, unleashing the model's potential during\ninference by allowing it to evolve and improve in real-time using its own\npredictions, achieving promising performance. However, pure entropy\nminimization can favor non-generalizable shortcuts, such as inflating the logit\nnorm and driving all predictions to a dominant class to reduce entropy, risking\ncollapsed solutions (e.g., constant one-hot outputs) that trivially minimize\nthe objective without meaningful learning. In this paper, we introduce\nZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time\nentropy minimization. ZeroSiam prevents collapse through asymmetric divergence\nalignment, which is efficiently achieved by a learnable predictor and a\nstop-gradient operator before the classifier. We provide empirical and\ntheoretical evidence that ZeroSiam not only prevents collapse solutions, but\nalso absorbs and regularizes biased learning signals, enhancing performance\neven when no collapse occurs. Despite its simplicity, extensive results show\nthat ZeroSiam performs more stably over prior methods using negligible\noverhead, demonstrating efficacy on both vision adaptation and large language\nmodel reasoning tasks across challenging test scenarios and diverse models,\nincluding tiny models that are particularly collapse-prone.", "AI": {"tldr": "\u63d0\u51fa\u4e86ZeroSiam\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u5b6a\u751f\u67b6\u6784\u89e3\u51b3\u6d4b\u8bd5\u65f6\u71b5\u6700\u5c0f\u5316\u4e2d\u7684\u5d29\u6e83\u95ee\u9898\uff0c\u9632\u6b62\u6a21\u578b\u9677\u5165\u5e73\u51e1\u89e3\uff0c\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u71b5\u6700\u5c0f\u5316\u867d\u7136\u80fd\u63d0\u5347\u6a21\u578b\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7eaf\u71b5\u6700\u5c0f\u5316\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u9009\u62e9\u975e\u6cdb\u5316\u7684\u6377\u5f84\uff08\u5982\u653e\u5927logit\u8303\u6570\u6216\u5c06\u6240\u6709\u9884\u6d4b\u63a8\u5411\u4e3b\u5bfc\u7c7b\u522b\uff09\uff0c\u4ea7\u751f\u5d29\u6e83\u89e3\u3002", "method": "\u8bbe\u8ba1\u4e86ZeroSiam\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u975e\u5bf9\u79f0\u5b6a\u751f\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9884\u6d4b\u5668\u548c\u5206\u7c7b\u5668\u524d\u7684\u505c\u6b62\u68af\u5ea6\u64cd\u4f5c\u5b9e\u73b0\u975e\u5bf9\u79f0\u6563\u5ea6\u5bf9\u9f50\uff0c\u9632\u6b62\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eZeroSiam\u4e0d\u4ec5\u80fd\u9632\u6b62\u5d29\u6e83\u89e3\uff0c\u8fd8\u80fd\u5438\u6536\u548c\u6b63\u5219\u5316\u6709\u504f\u5b66\u4e60\u4fe1\u53f7\uff0c\u5728\u5404\u79cd\u6d4b\u8bd5\u573a\u666f\u548c\u4e0d\u540c\u6a21\u578b\uff08\u5305\u62ec\u5bb9\u6613\u5d29\u6e83\u7684\u5c0f\u6a21\u578b\uff09\u4e0a\u8868\u73b0\u7a33\u5b9a\u4e14\u9ad8\u6548\u3002", "conclusion": "ZeroSiam\u5728\u89c6\u89c9\u9002\u5e94\u548c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4ee5\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u5b9e\u73b0\u4e86\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23190", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23190", "abs": "https://arxiv.org/abs/2509.23190", "authors": ["Zhanhong Xie", "Meifan Zhang", "Lihua Yin"], "title": "CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy", "comment": null, "summary": "Federated learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data locality. However, it still faces\nchallenges from malicious or compromised clients, as well as difficulties in\nincentivizing participants to contribute high-quality data under strict privacy\nrequirements. Motivated by these considerations, we propose CoSIFL, a novel\nframework that integrates proactive alarming for robust security and local\ndifferential privacy (LDP) for inference attacks, together with a\nStackelberg-based incentive scheme to encourage client participation and data\nsharing. Specifically, CoSIFL uses an active alarming mechanism and robust\naggregation to defend against Byzantine and inference attacks, while a Tullock\ncontest-inspired incentive module rewards honest clients for both data\ncontributions and reliable alarm triggers. We formulate the interplay between\nthe server and clients as a two-stage game: in the first stage, the server\ndetermines total rewards, selects participants, and fixes global iteration\nsettings, whereas in the second stage, each client decides its mini-batch size,\nprivacy noise scale, and alerting strategy. We prove that the server-client\ngame admits a unique equilibrium, and analyze how clients' multi-dimensional\nattributes - such as non-IID degrees and privacy budgets - jointly affect\nsystem efficiency. Experimental results on standard benchmarks demonstrate that\nCoSIFL outperforms state-of-the-art solutions in improving model robustness and\nreducing total server costs, highlighting the effectiveness of our integrated\ndesign.", "AI": {"tldr": "CoSIFL\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u4e3b\u52a8\u8b66\u62a5\u673a\u5236\u3001\u672c\u5730\u5dee\u5206\u9690\u79c1\u548c\u57fa\u4e8eStackelberg\u535a\u5f08\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u4ee5\u9632\u5fa1\u6076\u610f\u653b\u51fb\u5e76\u6fc0\u52b1\u5ba2\u6237\u7aef\u53c2\u4e0e\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u6076\u610f\u5ba2\u6237\u7aef\u653b\u51fb\u548c\u6fc0\u52b1\u53c2\u4e0e\u8005\u8d21\u732e\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u6311\u6218\uff0c\u9700\u8981\u5728\u4e25\u683c\u9690\u79c1\u8981\u6c42\u4e0b\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u53c2\u4e0e\u79ef\u6781\u6027\u3002", "method": "\u4f7f\u7528\u4e3b\u52a8\u8b66\u62a5\u673a\u5236\u548c\u9c81\u68d2\u805a\u5408\u9632\u5fa1\u62dc\u5360\u5ead\u548c\u63a8\u7406\u653b\u51fb\uff0c\u91c7\u7528Tullock\u7ade\u8d5b\u542f\u53d1\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u5c06\u670d\u52a1\u5668\u4e0e\u5ba2\u6237\u7aef\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eCoSIFL\u5728\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u548c\u964d\u4f4e\u670d\u52a1\u5668\u603b\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "CoSIFL\u7684\u96c6\u6210\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u548c\u6fc0\u52b1\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u591a\u7ef4\u5ea6\u5c5e\u6027\u5bf9\u7cfb\u7edf\u6548\u7387\u7684\u8054\u5408\u5f71\u54cd\u3002"}}
{"id": "2509.23202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23202", "abs": "https://arxiv.org/abs/2509.23202", "authors": ["Vage Egiazarian", "Roberto L. Castro", "Denis Kuznedelev", "Andrei Panferov", "Eldar Kurtic", "Shubhra Pandit", "Alexandre Marques", "Mark Kurtz", "Saleh Ashkboos", "Torsten Hoefler", "Dan Alistarh"], "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization", "comment": null, "summary": "The recent hardware-accelerated microscaling 4-bit floating-point formats\nsuch as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to\nrevolutionize large language model (LLM) inference. Yet, their practical\nbenefits remain unproven. We present the first comprehensive study of MXFP4 and\nNVFP4 for post-training quantization, revealing gaps between their promise and\nreal-world performance. Our analysis shows that state-of-the-art methods\nstruggle with FP4, due to two key issues: (1) NVFP4's small group size provably\nneutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two\nscale quantization severely degrades accuracy due to high induced error. To\nbridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the\nclassic GPTQ quantization algorithm that tailors the quantization process to\nFP4's unique properties, by using block-wise Hadamard transforms and\nformat-specific optimizations. We support our proposal with a set of\nhigh-performance GPU kernels that enable the MR-GPTQ format with negligible\noverhead, by rotation fusion into the weights, and fast online computation of\nthe activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and\n2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on\nRTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches\nor outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the\npoint where it nears that of NVFP4. We conclude that, while FP4 is not an\nautomatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock\na new frontier of accuracy-performance trade-offs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9MXFP4\u548cNVFP4\u4e24\u79cd4\u4f4d\u6d6e\u70b9\u683c\u5f0f\u8fdb\u884c\u5168\u9762\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u4e0e\u627f\u8bfa\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51faMR-GPTQ\u65b9\u6cd5\u6765\u89e3\u51b3FP4\u91cf\u5316\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u786c\u4ef6\u52a0\u901f\u76844\u4f4d\u6d6e\u70b9\u683c\u5f0f\uff08MXFP4\u548cNVFP4\uff09\u6709\u671b\u9769\u65b0\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u76ca\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u683c\u5f0f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728FP4\u4e0a\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMicro-Rotated-GPTQ (MR-GPTQ)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u7ea7Hadamard\u53d8\u6362\u548c\u683c\u5f0f\u7279\u5b9a\u4f18\u5316\u6765\u9002\u5e94FP4\u7684\u72ec\u7279\u5c5e\u6027\u3002\u5f00\u53d1\u9ad8\u6027\u80fdGPU\u5185\u6838\uff0c\u901a\u8fc7\u6743\u91cd\u65cb\u8f6c\u878d\u5408\u548c\u5feb\u901f\u5728\u7ebf\u6fc0\u6d3b\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "result": "MR-GPTQ\u5728NVIDIA B200\u4e0a\u5b9e\u73b0\u5c42\u95f43.6\u500d\u548c\u7aef\u5230\u7aef2.2\u500d\u52a0\u901f\uff0c\u5728RTX5090\u4e0a\u5b9e\u73b0\u5c42\u95f46\u500d\u548c\u7aef\u5230\u7aef4\u500d\u52a0\u901f\u3002\u663e\u8457\u63d0\u5347MXFP4\u7cbe\u5ea6\uff0c\u4f7f\u5176\u63a5\u8fd1NVFP4\u6c34\u5e73\u3002", "conclusion": "\u867d\u7136FP4\u4e0d\u662fINT4\u7684\u81ea\u52a8\u5347\u7ea7\uff0c\u4f46\u50cfMR-GPTQ\u8fd9\u6837\u7684\u683c\u5f0f\u4e13\u7528\u65b9\u6cd5\u53ef\u4ee5\u5f00\u542f\u7cbe\u5ea6-\u6027\u80fd\u6743\u8861\u7684\u65b0\u524d\u6cbf\u3002"}}
{"id": "2509.23209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23209", "abs": "https://arxiv.org/abs/2509.23209", "authors": ["Wenhao Zhang", "Shao Zhang", "Xihuai Wang", "Yang Li", "Ying Wen"], "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning", "comment": null, "summary": "In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm\nfor developing agents that can rapidly adapt to new tasks by leveraging past\nexperiences as context, without updating their parameters. Recent approaches\ntrain large sequence models on monotonic policy improvement data from online\nRL, aiming to a continue improved testing time performance. However, our\nexperimental analysis reveals a critical flaw: these models cannot show a\ncontinue improvement like the training data during testing time. Theoretically,\nwe identify this phenomenon as Contextual Ambiguity, where the model's own\nstochastic actions can generate an interaction history that misleadingly\nresembles that of a sub-optimal policy from the training data, initiating a\nvicious cycle of poor action selection. To resolve the Contextual Ambiguity, we\nintroduce Context Value into training phase and propose Context Value Informed\nICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing\nthe ideal performance theoretically achievable by a policy given the current\ncontext. As the context expands, Context Value could include more task-relevant\ninformation, and therefore the ideal performance should be non-decreasing. We\nprove that the Context Value tightens the lower bound on the performance gap\nrelative to an ideal, monotonically improving policy. We fruther propose two\nmethods for estimating Context Value at both training and testing time.\nExperiments conducted on the Dark Room and Minigrid testbeds demonstrate that\nCV-ICRL effectively mitigates performance degradation and improves overall ICRL\nabilities across various tasks and environments. The source code and data of\nthis paper are available at\nhttps://github.com/Bluixe/towards_monotonic_improvement .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u4ef7\u503c\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(CV-ICRL)\uff0c\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u4ef7\u503c\u6765\u89e3\u51b3\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u73b0\u6709ICRL\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709ICRL\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u5355\u8c03\u7b56\u7565\u6539\u8fdb\u6570\u636e\uff0c\u4f46\u5728\u6d4b\u8bd5\u65f6\u65e0\u6cd5\u6301\u7eed\u6539\u8fdb\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u5bfc\u81f4\u7684\uff0c\u6a21\u578b\u81ea\u8eab\u7684\u968f\u673a\u52a8\u4f5c\u53ef\u80fd\u751f\u6210\u4e0e\u8bad\u7ec3\u6570\u636e\u4e2d\u6b21\u4f18\u7b56\u7565\u76f8\u4f3c\u7684\u4ea4\u4e92\u5386\u53f2\uff0c\u4ece\u800c\u5f15\u53d1\u6076\u6027\u5faa\u73af\u3002", "method": "\u63d0\u51faCV-ICRL\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4e0a\u4e0b\u6587\u4ef7\u503c\u4f5c\u4e3a\u663e\u5f0f\u4fe1\u53f7\uff0c\u8868\u793a\u7ed9\u5b9a\u5f53\u524d\u4e0a\u4e0b\u6587\u65f6\u7406\u8bba\u4e0a\u53ef\u8fbe\u5230\u7684\u7406\u60f3\u6027\u80fd\u3002\u968f\u7740\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u4e0a\u4e0b\u6587\u4ef7\u503c\u5e94\u975e\u9012\u51cf\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u4f30\u8ba1\u4e0a\u4e0b\u6587\u4ef7\u503c\u7684\u65b9\u6cd5\u3002", "result": "\u5728Dark Room\u548cMinigrid\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCV-ICRL\u6709\u6548\u7f13\u89e3\u4e86\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u63d0\u5347\u4e86ICRL\u7684\u6574\u4f53\u80fd\u529b\u3002", "conclusion": "CV-ICRL\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u4ef7\u503c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3aICRL\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.23213", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23213", "abs": "https://arxiv.org/abs/2509.23213", "authors": ["Hugo Math", "Robin Sch\u00f6n", "Rainer Lienhart"], "title": "One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences", "comment": "Accepted at NeuRIPS2025 Workshop CauScien: Discovering Causality in\n  Science. arXiv admin note: substantial text overlap with arXiv:2509.19112", "summary": "Understanding causality in event sequences with thousands of sparse event\ntypes is critical in domains such as healthcare, cybersecurity, or vehicle\ndiagnostics, yet current methods fail to scale. We present OSCAR, a one-shot\ncausal autoregressive method that infers per-sequence Markov Boundaries using\ntwo pretrained Transformers as density estimators. This enables efficient,\nparallel causal discovery without costly global CI testing. On a real-world\nautomotive dataset with 29,100 events and 474 labels, OSCAR recovers\ninterpretable causal structures in minutes, while classical methods fail to\nscale, enabling practical scientific diagnostics at production scale.", "AI": {"tldr": "OSCAR\u662f\u4e00\u79cd\u4e00\u6b21\u6027\u56e0\u679c\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3Transformer\u4f5c\u4e3a\u5bc6\u5ea6\u4f30\u8ba1\u5668\u6765\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u5e76\u884c\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5168\u5c40\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u3002", "motivation": "\u5728\u533b\u7597\u4fdd\u5065\u3001\u7f51\u7edc\u5b89\u5168\u6216\u8f66\u8f86\u8bca\u65ad\u7b49\u9886\u57df\uff0c\u7406\u89e3\u5177\u6709\u6570\u5343\u4e2a\u7a00\u758f\u4e8b\u4ef6\u7c7b\u578b\u7684\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u5982\u6b64\u89c4\u6a21\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3Transformer\u4f5c\u4e3a\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u4e00\u6b21\u6027\u56e0\u679c\u81ea\u56de\u5f52\u65b9\u6cd5\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\uff0c\u5b9e\u73b0\u5e76\u884c\u56e0\u679c\u53d1\u73b0\u3002", "result": "\u5728\u5305\u542b29,100\u4e2a\u4e8b\u4ef6\u548c474\u4e2a\u6807\u7b7e\u7684\u771f\u5b9e\u4e16\u754c\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\uff0cOSCAR\u5728\u51e0\u5206\u949f\u5185\u6062\u590d\u4e86\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u800c\u7ecf\u5178\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u3002", "conclusion": "OSCAR\u80fd\u591f\u5728\u751f\u4ea7\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u7528\u7684\u79d1\u5b66\u8bca\u65ad\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7a00\u758f\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u56e0\u679c\u53d1\u73b0\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002"}}
{"id": "2509.23219", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23219", "abs": "https://arxiv.org/abs/2509.23219", "authors": ["Xin Li", "Mengbing Liu", "Yiyang Zhu", "Wenhe Zhang", "Li Wei", "Jiancheng An", "Chau Yuen"], "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning", "comment": "Project Homepage: https://lixin.ai/WirelessMathLM", "summary": "Large language models (LLMs) excel at general mathematical reasoning but fail\ncatastrophically on specialized technical mathematics. In wireless\ncommunications, where problems require precise manipulation of\ninformation-theoretic bounds, optimization constraints, and signal processing\nformulations, even state-of-the-art models struggle to achieve competent\nperformance. We present WirelessMathLM, demonstrating that compact models\n(0.5B-7B parameters) can match or exceed much larger models through\ndomain-specific reinforcement learning with verifiable rewards. Our key insight\nis that wireless mathematics problems possess a unique property--verifiable\ncorrectness--that enables effective reinforcement learning without human\nfeedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027\nproblems from 970 papers. Using Group Relative Policy Optimization (GRPO) with\nbinary verification rewards, we train models directly from base checkpoints\nwithout supervised warm-start. Our 7B model achieves 39.5% accuracy on\nWirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times\nfewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training\nnearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B\n+81%), with positive transfer to general mathematics benchmarks--our models\ngain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and\nAIME without any training on these tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86WirelessMathLM\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7d27\u51d1\u6a21\u578b(0.5B-7B)\uff0c\u5728\u65e0\u7ebf\u901a\u4fe1\u6570\u5b66\u95ee\u9898\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u5927\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u80fd\u63d0\u5347\u901a\u7528\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u6570\u5b66\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u64cd\u4f5c\u4fe1\u606f\u8bba\u8fb9\u754c\u3001\u4f18\u5316\u7ea6\u675f\u548c\u4fe1\u53f7\u5904\u7406\u516c\u5f0f\u7684\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u5177\u6709\u53ef\u9a8c\u8bc1\u6b63\u786e\u6027\u7684\u65e0\u7ebf\u6570\u5b66\u95ee\u9898\uff0c\u901a\u8fc7Group Relative Policy Optimization (GRPO)\u548c\u4e8c\u5143\u9a8c\u8bc1\u5956\u52b1\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u76d1\u7763\u9884\u70ed\u3002", "result": "7B\u6a21\u578b\u5728WirelessMathBench-XL\u4e0a\u8fbe\u523039.5%\u51c6\u786e\u7387\uff0c\u63a5\u8fd1GPT-4o(40.4%)\uff0c\u800c\u53c2\u6570\u91cf\u4ec5\u4e3aDeepSeek-R1(671B, 57.4%)\u7684\u7ea61/100\u3002GRPO\u8bad\u7ec3\u4f7f\u6240\u6709\u6a21\u578b\u89c4\u6a21\u6027\u80fd\u51e0\u4e4e\u7ffb\u500d\uff0c\u5e76\u5728\u672a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u901a\u7528\u6570\u5b66\u57fa\u51c6\u8868\u73b0\u3002", "conclusion": "\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5728\u4e13\u4e1a\u6570\u5b66\u4efb\u52a1\u4e0a\u8fbe\u5230\u5927\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5177\u6709\u6b63\u5411\u8fc1\u79fb\u5230\u901a\u7528\u6570\u5b66\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2509.23232", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23232", "abs": "https://arxiv.org/abs/2509.23232", "authors": ["Bingshuai Liu", "Ante Wang", "Zijun Min", "Liang Yao", "Haibo Zhang", "Yang Liu", "Anxiang Zeng", "Jinsong Su"], "title": "SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on reinforcement learning with\nverifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.\nHowever, the training process remains bottlenecked by the computationally\nexpensive rollout stage. Existing acceleration methods-such as parallelization,\nobjective- and data-driven modifications, and replay buffers-either incur\ndiminishing returns, introduce bias, or overlook redundancy across iterations.\nWe identify that rollouts from consecutive training epochs frequently share a\nlarge portion of overlapping segments, wasting computation. To address this, we\npropose SPEC-RL, a novel framework that integrates SPECulative decoding with\nthe RL rollout process. SPEC-RL reuses prior trajectory segments as speculative\nprefixes and extends them via a draft-and-verify mechanism, avoiding redundant\ngeneration while ensuring policy consistency. Experiments on diverse math\nreasoning and generalization benchmarks, including GSM8K, MATH-500,\nOlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout\ntime by 2-3x without compromising policy quality. As a purely rollout-stage\nenhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,\nPPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large\nreasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL", "AI": {"tldr": "SPEC-RL\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u6d4b\u6027\u89e3\u7801\u4e0eRL rollout\u8fc7\u7a0b\u96c6\u6210\uff0c\u5229\u7528\u5148\u524d\u8f68\u8ff9\u6bb5\u4f5c\u4e3a\u63a8\u6d4b\u524d\u7f00\u8fdb\u884c\u6269\u5c55\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5c06rollout\u65f6\u95f4\u51cf\u5c112-3\u500d\u800c\u4e0d\u5f71\u54cd\u7b56\u7565\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff08\u5982\u5e76\u884c\u5316\u3001\u76ee\u6807\u9a71\u52a8\u4fee\u6539\u3001\u91cd\u653e\u7f13\u51b2\u533a\uff09\u5b58\u5728\u6536\u76ca\u9012\u51cf\u3001\u5f15\u5165\u504f\u5dee\u6216\u5ffd\u7565\u8fed\u4ee3\u95f4\u5197\u4f59\u7684\u95ee\u9898\u3002\u8fde\u7eed\u8bad\u7ec3\u65f6\u671f\u7684rollout\u7ecf\u5e38\u5305\u542b\u5927\u91cf\u91cd\u53e0\u6bb5\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "SPEC-RL\u91cd\u7528\u5148\u524d\u7684\u8f68\u8ff9\u6bb5\u4f5c\u4e3a\u63a8\u6d4b\u524d\u7f00\uff0c\u901a\u8fc7\u8349\u7a3f-\u9a8c\u8bc1\u673a\u5236\u8fdb\u884c\u6269\u5c55\uff0c\u907f\u514d\u5197\u4f59\u751f\u6210\u540c\u65f6\u786e\u4fdd\u7b56\u7565\u4e00\u81f4\u6027\u3002", "result": "\u5728GSM8K\u3001MATH-500\u3001OlympiadBench\u3001MMLU-STEM\u7b49\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u548c\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPEC-RL\u5c06rollout\u65f6\u95f4\u51cf\u5c112-3\u500d\uff0c\u4e14\u4e0d\u635f\u5bb3\u7b56\u7565\u8d28\u91cf\u3002", "conclusion": "SPEC-RL\u4f5c\u4e3a\u7eafrollout\u9636\u6bb5\u589e\u5f3a\uff0c\u53ef\u4e0e\u4e3b\u6d41\u7b97\u6cd5\uff08PPO\u3001GRPO\u3001DAPO\uff09\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684RLVR\u6269\u5c55\u63d0\u4f9b\u4e86\u901a\u7528\u5b9e\u7528\u7684\u8def\u5f84\u3002"}}
{"id": "2509.23240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23240", "abs": "https://arxiv.org/abs/2509.23240", "authors": ["Shayan Alahyari"], "title": "More Data or Better Algorithms: Latent Diffusion Augmentation for Deep Imbalanced Regression", "comment": null, "summary": "In many real-world regression tasks, the data distribution is heavily skewed,\nand models learn predominantly from abundant majority samples while failing to\npredict minority labels accurately. While imbalanced classification has been\nextensively studied, imbalanced regression remains relatively unexplored. Deep\nimbalanced regression (DIR) represents cases where the input data are\nhigh-dimensional and unstructured. Although several data-level approaches for\ntabular imbalanced regression exist, deep imbalanced regression currently lacks\ndedicated data-level solutions suitable for high-dimensional data and relies\nprimarily on algorithmic modifications. To fill this gap, we propose\nLatentDiff, a novel framework that uses conditional diffusion models with\npriority-based generation to synthesize high-quality features in the latent\nrepresentation space. LatentDiff is computationally efficient and applicable\nacross diverse data modalities, including images, text, and other\nhigh-dimensional inputs. Experiments on three DIR benchmarks demonstrate\nsubstantial improvements in minority regions while maintaining overall\naccuracy.", "AI": {"tldr": "\u63d0\u51faLatentDiff\u6846\u67b6\uff0c\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5408\u6210\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u89e3\u51b3\u6df1\u5ea6\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6574\u4f53\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5c11\u6570\u533a\u57df\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56de\u5f52\u4efb\u52a1\u4e2d\u6570\u636e\u5206\u5e03\u4e25\u91cd\u504f\u659c\uff0c\u6a21\u578b\u4e3b\u8981\u4ece\u4e30\u5bcc\u7684\u591a\u6570\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u4f46\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u5c11\u6570\u6807\u7b7e\u3002\u867d\u7136\u4e0d\u5e73\u8861\u5206\u7c7b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u4e0d\u5e73\u8861\u56de\u5f52\u76f8\u5bf9\u8f83\u5c11\u63a2\u7d22\u3002\u6df1\u5ea6\u4e0d\u5e73\u8861\u56de\u5f52\u7f3a\u4e4f\u9002\u5408\u9ad8\u7ef4\u6570\u636e\u7684\u4e13\u7528\u6570\u636e\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLatentDiff\u6846\u67b6\uff0c\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5408\u6210\u9ad8\u8d28\u91cf\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u3001\u6587\u672c\u7b49\u591a\u79cd\u9ad8\u7ef4\u6570\u636e\u6a21\u6001\u3002", "result": "\u5728\u4e09\u4e2a\u6df1\u5ea6\u4e0d\u5e73\u8861\u56de\u5f52\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u6570\u533a\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "LatentDiff\u586b\u8865\u4e86\u6df1\u5ea6\u4e0d\u5e73\u8861\u56de\u5f52\u9886\u57df\u6570\u636e\u7ea7\u89e3\u51b3\u65b9\u6848\u7684\u7a7a\u767d\uff0c\u4e3a\u5904\u7406\u9ad8\u7ef4\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2509.23246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23246", "abs": "https://arxiv.org/abs/2509.23246", "authors": ["Manjiang Yu", "Priyanka Singh", "Xue Li", "Yang Cao"], "title": "Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection", "comment": "18 pages", "summary": "Large language models (LLMs) frequently memorize sensitive or personal\ninformation, raising significant privacy concerns. Existing variants of\ndifferential privacy stochastic gradient descent (DPSGD) inject uniform noise\ninto every gradient step, significantly extending training time and reducing\nmodel accuracy. We propose that concentrating noise primarily on gradients\nassociated with sensitive tokens can substantially decrease DP training time,\nstrengthen the protection of sensitive information, and simultaneously preserve\nthe model's performance on non-sensitive data. We operationalize this insight\nthrough Adaptive Token-Weighted Differential Privacy (ATDP), a modification of\nvanilla DP-SGD that adaptively assigns different gradient weights to sensitive\nand non-sensitive tokens. By employing a larger noise scale at the early stage\nof training, ATDP rapidly disrupts memorization of sensitive content. As a\nresult, ATDP only requires a few additional epochs of lightweight\npost-processing following standard fine-tuning, injecting targeted noise\nprimarily on parameters corresponding to sensitive tokens, thus minimally\naffecting the model's general capabilities. ATDP can be seamlessly integrated\ninto any existing DP-based fine-tuning pipeline or directly applied to\nnon-private models as a fast privacy-enhancing measure. Additionally, combined\nwith an initial redacted fine-tuning phase, ATDP forms a streamlined DP\npipeline that achieves comparable canary protection to state-of-the-art DP-SGD\nmethods, significantly reduces the computational overhead of DP fine-tuning,\nshortening training time by approximately 90 percent, while achieving\ncomparable or superior privacy protection and minimal accuracy degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e86ATDP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u654f\u611f\u548c\u975e\u654f\u611ftoken\u5206\u914d\u4e0d\u540c\u68af\u5ea6\u6743\u91cd\uff0c\u5728\u8bad\u7ec3\u65e9\u671f\u4f7f\u7528\u66f4\u5927\u566a\u58f0\u5c3a\u5ea6\u5feb\u901f\u7834\u574f\u654f\u611f\u5185\u5bb9\u8bb0\u5fc6\uff0c\u663e\u8457\u51cf\u5c11DP\u8bad\u7ec3\u65f6\u95f4\u7ea690%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "LLM\u7ecf\u5e38\u8bb0\u5fc6\u654f\u611f\u4fe1\u606f\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\uff0c\u73b0\u6709DPSGD\u65b9\u6cd5\u5728\u6240\u6709\u68af\u5ea6\u6b65\u9aa4\u6ce8\u5165\u5747\u5300\u566a\u58f0\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u5ef6\u957f\u548c\u6a21\u578b\u51c6\u786e\u7387\u4e0b\u964d\u3002", "method": "ATDP\u65b9\u6cd5\u4fee\u6539\u6807\u51c6DP-SGD\uff0c\u81ea\u9002\u5e94\u5730\u4e3a\u654f\u611f\u548c\u975e\u654f\u611ftoken\u5206\u914d\u4e0d\u540c\u68af\u5ea6\u6743\u91cd\uff0c\u5728\u8bad\u7ec3\u65e9\u671f\u4f7f\u7528\u66f4\u5927\u566a\u58f0\u5c3a\u5ea6\uff0c\u4ec5\u9700\u5c11\u91cf\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u9636\u6bb5\u3002", "result": "ATDP\u663e\u8457\u51cf\u5c11DP\u5fae\u8c03\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\u7ea690%\uff0c\u5b9e\u73b0\u53ef\u6bd4\u6216\u66f4\u4f18\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u51c6\u786e\u7387\u4e0b\u964d\u3002", "conclusion": "ATDP\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709DP\u5fae\u8c03\u6d41\u7a0b\u4e2d\uff0c\u6216\u76f4\u63a5\u5e94\u7528\u4e8e\u975e\u79c1\u6709\u6a21\u578b\u4f5c\u4e3a\u5feb\u901f\u9690\u79c1\u589e\u5f3a\u63aa\u65bd\uff0c\u5f62\u6210\u9ad8\u6548\u7684DP\u6d41\u6c34\u7ebf\u3002"}}
{"id": "2509.23249", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.23249", "abs": "https://arxiv.org/abs/2509.23249", "authors": ["Vladimir Fanaskov", "Vladislav Trifonov", "Alexander Rudikov", "Ekaterina Muravleva", "Ivan Oseledets"], "title": "Deep Learning for Subspace Regression", "comment": null, "summary": "It is often possible to perform reduced order modelling by specifying linear\nsubspace which accurately captures the dynamics of the system. This approach\nbecomes especially appealing when linear subspace explicitly depends on\nparameters of the problem. A practical way to apply such a scheme is to compute\nsubspaces for a selected set of parameters in the computationally demanding\noffline stage and in the online stage approximate subspace for unknown\nparameters by interpolation. For realistic problems the space of parameters is\nhigh dimensional, which renders classical interpolation strategies infeasible\nor unreliable. We propose to relax the interpolation problem to regression,\nintroduce several loss functions suitable for subspace data, and use a neural\nnetwork as an approximation to high-dimensional target function. To further\nsimplify a learning problem we introduce redundancy: in place of predicting\nsubspace of a given dimension we predict larger subspace. We show theoretically\nthat this strategy decreases the complexity of the mapping for elliptic\neigenproblems with constant coefficients and makes the mapping smoother for\ngeneral smooth function on the Grassmann manifold. Empirical results also show\nthat accuracy significantly improves when larger-than-needed subspaces are\npredicted. With the set of numerical illustrations we demonstrate that subspace\nregression can be useful for a range of tasks including parametric\neigenproblems, deflation techniques, relaxation methods, optimal control and\nsolution of parametric partial differential equations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u5b50\u7a7a\u95f4\u56de\u5f52\u65b9\u6cd5\uff0c\u7528\u4e8e\u964d\u9636\u5efa\u6a21\u3002\u901a\u8fc7\u9884\u6d4b\u6bd4\u6240\u9700\u7ef4\u5ea6\u66f4\u5927\u7684\u5b50\u7a7a\u95f4\u6765\u7b80\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u53c2\u6570\u5316\u964d\u9636\u5efa\u6a21\u4e2d\uff0c\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4f7f\u5f97\u7ecf\u5178\u63d2\u503c\u7b56\u7565\u4e0d\u53ef\u884c\u6216\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5b50\u7a7a\u95f4\u6570\u636e\u7684\u8fd1\u4f3c\u95ee\u9898\u3002", "method": "\u5c06\u63d2\u503c\u95ee\u9898\u653e\u677e\u4e3a\u56de\u5f52\u95ee\u9898\uff0c\u5f15\u5165\u9002\u5408\u5b50\u7a7a\u95f4\u6570\u636e\u7684\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u9ad8\u7ef4\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u66f4\u5927\u7ef4\u5ea6\u7684\u5b50\u7a7a\u95f4\u6765\u7b80\u5316\u5b66\u4e60\u95ee\u9898\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u692d\u5706\u7279\u5f81\u95ee\u9898\u7684\u6620\u5c04\u590d\u6742\u5ea6\uff0c\u5e76\u4f7fGrassmann\u6d41\u5f62\u4e0a\u7684\u5149\u6ed1\u6620\u5c04\u66f4\u5e73\u6ed1\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u9884\u6d4b\u66f4\u5927\u5b50\u7a7a\u95f4\u80fd\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u5b50\u7a7a\u95f4\u56de\u5f52\u65b9\u6cd5\u5728\u53c2\u6570\u7279\u5f81\u95ee\u9898\u3001\u7d27\u7f29\u6280\u672f\u3001\u677e\u5f1b\u65b9\u6cd5\u3001\u6700\u4f18\u63a7\u5236\u548c\u53c2\u6570\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.23252", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23252", "abs": "https://arxiv.org/abs/2509.23252", "authors": ["Raviteja Anantha", "Soheil Hor", "Teodor Nicola Antoniu", "Layne C. Price"], "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning", "comment": "preprint version", "summary": "We present NanoFlux, a novel adversarial framework for generating targeted\ntraining data to improve LLM reasoning, where adversarially-generated datasets\ncontaining fewer than 200 examples outperform conventional fine-tuning\napproaches. The framework employs a competitive dynamic between models\nalternating as Attacker and Defender, supervised by a tool-augmented Judge,\nsynthesizing multi-step questions with explanatory annotations that target\nspecific reasoning capabilities. Fine-tuning a 4B-parameter model on\nNanoFlux-generated data yields performance gains across diverse domains\ncompared to full-benchmark fine-tuning: +5.9% on mathematical reasoning\n(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical\nreasoning (MultiMedQA), while reducing computational requirements by 3-14x.\nAblation studies reveal a non-monotonic relationship between dataset\ncharacteristics and model performance, uncovering domain-specific optimal\npoints for question complexity and reasoning quality. NanoFlux automates\ntraining data generation through embedding-based novelty filtering,\ntool-augmented evaluation, and multi-hop reasoning, suggesting that future\nmodel improvements may lie in the intelligent synthesis of small, precisely\ntargeted training datasets.", "AI": {"tldr": "NanoFlux\u662f\u4e00\u4e2a\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5c11\u4e8e200\u4e2a\u4f8b\u5b50\u7684\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e\uff0c\u5728LLM\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u5408\u6210\u5c0f\u578b\u4f46\u7cbe\u51c6\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u6765\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u6a21\u578b\u4ea4\u66ff\u626e\u6f14\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u89d2\u8272\uff0c\u7531\u5de5\u5177\u589e\u5f3a\u7684\u6cd5\u5b98\u76d1\u7763\uff0c\u751f\u6210\u5e26\u89e3\u91ca\u6027\u6ce8\u91ca\u7684\u591a\u6b65\u9aa4\u95ee\u9898\uff0c\u9488\u5bf9\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u57284B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cNanoFlux\u751f\u6210\u7684\u6570\u636e\u76f8\u6bd4\u5b8c\u6574\u57fa\u51c6\u5fae\u8c03\uff1a\u6570\u5b66\u63a8\u7406\u63d0\u53475.9%\uff0c\u79d1\u5b66\u63a8\u7406\u63d0\u53473.6%\uff0c\u533b\u5b66\u63a8\u7406\u63d0\u534716.6%\uff0c\u8ba1\u7b97\u9700\u6c42\u51cf\u5c113-14\u500d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6a21\u578b\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u6765\u81ea\u667a\u80fd\u5408\u6210\u7684\u5c0f\u578b\u7cbe\u51c6\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5927\u89c4\u6a21\u6570\u636e\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.23254", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.23254", "abs": "https://arxiv.org/abs/2509.23254", "authors": ["Zhang-Yu You", "Jiahao Ma", "Hongzong Li", "Ye-Fan Hu", "Jian-Dong Huang"], "title": "ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction", "comment": null, "summary": "Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for\nvaccine design, immunodiagnostics, and therapeutic antibody development.\nHowever, achieving reliable predictions from sequences alone remains a\nchallenge. In this paper, we present ABCONFORMER, a model based on the\nConformer backbone that captures both local and global features of a\nbiosequence. To accurately capture Ab-Ag interactions, we introduced the\nphysics-inspired sliding attention, enabling residue-level contact recovery\nwithout relying on three-dimensional structural data. ABConformer can\naccurately predict paratopes and epitopes given the antibody and antigen\nsequence, and predict pan-epitopes on the antigen without antibody information.\nIn comparison experiments, ABCONFORMER achieves state-of-the-art performance on\na recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based\nmethods for antibody-agnostic epitope prediction. Ablation studies further\nquantify the contribution of each component, demonstrating that, compared to\nconventional cross-attention, sliding attention significantly enhances the\nprecision of epitope prediction. To facilitate reproducibility, we will release\nthe code under an open-source license upon acceptance.", "AI": {"tldr": "ABCONFORMER\u662f\u57fa\u4e8eConformer\u67b6\u6784\u7684\u6a21\u578b\uff0c\u80fd\u591f\u4ec5\u4ece\u5e8f\u5217\u51c6\u786e\u9884\u6d4b\u6297\u4f53-\u6297\u539f\u754c\u9762\uff0c\u5305\u62ec\u4e92\u8865\u4f4d\u3001\u8868\u4f4d\u548c\u6cdb\u8868\u4f4d\uff0c\u5728SARS-CoV-2\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u6297\u4f53-\u6297\u539f\u754c\u9762\u5bf9\u4e8e\u75ab\u82d7\u8bbe\u8ba1\u3001\u514d\u75ab\u8bca\u65ad\u548c\u6cbb\u7597\u6027\u6297\u4f53\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ec5\u4ece\u5e8f\u5217\u8fdb\u884c\u53ef\u9760\u9884\u6d4b\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528Conformer\u9aa8\u5e72\u7f51\u7edc\u6355\u83b7\u751f\u7269\u5e8f\u5217\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5f15\u5165\u7269\u7406\u542f\u53d1\u7684\u6ed1\u52a8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e0\u9700\u4e09\u7ef4\u7ed3\u6784\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u6b8b\u57fa\u7ea7\u63a5\u89e6\u6062\u590d\u3002", "result": "\u5728SARS-CoV-2\u6297\u4f53-\u6297\u539f\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u6297\u4f53\u65e0\u5173\u8868\u4f4d\u9884\u6d4b\u65b9\u9762\u8d85\u8d8a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5e8f\u5217\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6ed1\u52a8\u6ce8\u610f\u529b\u663e\u8457\u63d0\u5347\u8868\u4f4d\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "ABCONFORMER\u80fd\u591f\u4ec5\u4ece\u5e8f\u5217\u51c6\u786e\u9884\u6d4b\u6297\u4f53-\u6297\u539f\u754c\u9762\uff0c\u4e3a\u75ab\u82d7\u548c\u6cbb\u7597\u6027\u6297\u4f53\u5f00\u53d1\u63d0\u4f9b\u6709\u529b\u5de5\u5177\uff0c\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u5f00\u6e90\u3002"}}
{"id": "2509.23265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23265", "abs": "https://arxiv.org/abs/2509.23265", "authors": ["Jiajun He", "Paul Jeha", "Peter Potaptchik", "Leo Zhang", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Yuanqi Du", "Saifuddin Syed", "Francisco Vargas"], "title": "CREPE: Controlling Diffusion with Replica Exchange", "comment": "29 pages, 14 figures, 3 tables", "summary": "Inference-time control of diffusion models aims to steer model outputs to\nsatisfy new constraints without retraining. Previous approaches have mostly\nrelied on heuristic guidance or have been coupled with Sequential Monte Carlo\n(SMC) for bias correction. In this paper, we propose a flexible alternative\nbased on replica exchange, an algorithm designed initially for sampling\nproblems. We refer to this method as the CREPE (Controlling with REPlica\nExchange). Unlike SMC, CREPE: (1) generates particles sequentially, (2)\nmaintains high diversity in the generated samples after a burn-in period, and\n(3) enables online refinement or early termination. We demonstrate its\nversatility across various tasks, including temperature annealing,\nreward-tilting, model composition and classifier-free guidance debiasing, with\ncompetitive performance compared to prior SMC methods.", "AI": {"tldr": "\u63d0\u51faCREPE\u65b9\u6cd5\uff0c\u57fa\u4e8e\u526f\u672c\u4ea4\u6362\u7b97\u6cd5\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u65f6\u63a7\u5236\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6ee1\u8db3\u65b0\u7ea6\u675f", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u63a8\u7406\u65f6\u63a7\u5236\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u5f15\u5bfc\u6216\u4e0eSMC\u7ed3\u5408\u8fdb\u884c\u504f\u5dee\u6821\u6b63\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4f7f\u7528\u526f\u672c\u4ea4\u6362\u7b97\u6cd5\uff0c\u987a\u5e8f\u751f\u6210\u7c92\u5b50\uff0c\u5728\u9884\u70ed\u671f\u540e\u4fdd\u6301\u6837\u672c\u591a\u6837\u6027\uff0c\u652f\u6301\u5728\u7ebf\u4f18\u5316\u548c\u63d0\u524d\u7ec8\u6b62", "result": "\u5728\u6e29\u5ea6\u9000\u706b\u3001\u5956\u52b1\u503e\u659c\u3001\u6a21\u578b\u7ec4\u5408\u548c\u5206\u7c7b\u5668\u65e0\u5f15\u5bfc\u53bb\u504f\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4e0e\u73b0\u6709SMC\u65b9\u6cd5\u76f8\u5f53", "conclusion": "CREPE\u4e3a\u6269\u6563\u6a21\u578b\u63a8\u7406\u65f6\u63a7\u5236\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u987a\u5e8f\u751f\u6210\u3001\u6837\u672c\u591a\u6837\u6027\u548c\u5728\u7ebf\u4f18\u5316\u7b49\u4f18\u52bf"}}
{"id": "2509.23268", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.23268", "abs": "https://arxiv.org/abs/2509.23268", "authors": ["Lisa Pilgram", "Kai Yang", "Ana-Alicia Beltran-Bless", "Gregory R. Pond", "Lisa Vandermeer", "John Hilton", "Marie-France Savard", "Andr\u00e9anne Leblanc", "Lois Sheperd", "Bingshu E. Chen", "John M. S. Bartlett", "Karen J. Taylor", "Jane Bayani", "Sarah L. Barker", "Melanie Spears", "Cornelis J. H. van der Velde", "Elma Meershoek-Klein Kranenbarg", "Luc Dirix", "Elizabeth Mallon", "Annette Hasenburg", "Christos Markopoulos", "Lamin Juwara", "Fida K. Dankar", "Mark Clemons", "Khaled El Emam"], "title": "Transfer Learning and Machine Learning for Training Five Year Survival Prognostic Models in Early Breast Cancer", "comment": null, "summary": "Prognostic information is essential for decision-making in breast cancer\nmanagement. Recently trials have predominantly focused on genomic\nprognostication tools, even though clinicopathological prognostication is less\ncostly and more widely accessible. Machine learning (ML), transfer learning and\nensemble integration offer opportunities to build robust prognostication\nframeworks. We evaluate this potential to improve survival prognostication in\nbreast cancer by comparing de-novo ML, transfer learning from a pre-trained\nprognostic tool and ensemble integration. Data from the MA.27 trial was used\nfor model training, with external validation on the TEAM trial and a SEER\ncohort. Transfer learning was applied by fine-tuning the pre-trained prognostic\ntool PREDICT v3, de-novo ML included Random Survival Forests and Extreme\nGradient Boosting, and ensemble integration was realized through a weighted sum\nof model predictions. Transfer learning, de-novo RSF, and ensemble integration\nimproved calibration in MA.27 over the pre-trained model (ICI reduced from\n0.042 in PREDICT v3 to <=0.007) while discrimination remained comparable (AUC\nincreased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3\npredictions were observed in 23.8-25.8% of MA.27 individuals due to missing\ninformation. In contrast, ML models and ensemble integration could predict\nsurvival regardless of missing information. Across all models, patient age,\nnodal status, pathological grading and tumor size had the highest SHAP values,\nindicating their importance for survival prognostication. External validation\nin SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and\nensemble integration. This study demonstrates that transfer learning, de-novo\nRSF, and ensemble integration can improve prognostication in situations where\nrelevant information for PREDICT v3 is lacking or where a dataset shift is\nlikely.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u8fc1\u79fb\u5b66\u4e60\u3001\u4ece\u5934\u673a\u5668\u5b66\u4e60\u3001\u96c6\u6210\u6574\u5408\uff09\u5728\u4e73\u817a\u764c\u751f\u5b58\u9884\u540e\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5728PREDICT v3\u5de5\u5177\u56e0\u4fe1\u606f\u7f3a\u5931\u65e0\u6cd5\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u5e76\u6539\u5584\u4e86\u9884\u6d4b\u6821\u51c6\u5ea6\u3002", "motivation": "\u57fa\u56e0\u7ec4\u9884\u540e\u5de5\u5177\u6210\u672c\u9ad8\u4e14\u53ef\u53ca\u6027\u6709\u9650\uff0c\u800c\u4e34\u5e8a\u75c5\u7406\u9884\u540e\u66f4\u7ecf\u6d4e\u5b9e\u7528\u3002\u673a\u5668\u5b66\u4e60\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u96c6\u6210\u6574\u5408\u4e3a\u6784\u5efa\u7a33\u5065\u7684\u9884\u540e\u6846\u67b6\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u4f7f\u7528MA.27\u8bd5\u9a8c\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u5728TEAM\u8bd5\u9a8c\u548cSEER\u961f\u5217\u4e2d\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u5fae\u8c03PREDICT v3\u3001\u4ece\u5934\u673a\u5668\u5b66\u4e60\uff08\u968f\u673a\u751f\u5b58\u68ee\u6797\u548c\u6781\u7aef\u68af\u5ea6\u63d0\u5347\uff09\u4ee5\u53ca\u901a\u8fc7\u52a0\u6743\u6c42\u548c\u5b9e\u73b0\u96c6\u6210\u6574\u5408\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u3001\u4ece\u5934RSF\u548c\u96c6\u6210\u6574\u5408\u5728MA.27\u4e2d\u6539\u5584\u4e86\u6821\u51c6\u5ea6\uff08ICI\u4ece0.042\u964d\u81f3\u22640.007\uff09\uff0c\u533a\u5206\u5ea6\u4fdd\u6301\u76f8\u5f53\uff08AUC\u4ece0.738\u63d0\u5347\u81f30.744-0.799\uff09\u3002ML\u6a21\u578b\u80fd\u5904\u7406PREDICT v3\u56e0\u4fe1\u606f\u7f3a\u5931\u65e0\u6cd5\u9884\u6d4b\u7684\u75c5\u4f8b\uff0823.8-25.8%\uff09\u3002", "conclusion": "\u5f53PREDICT v3\u6240\u9700\u76f8\u5173\u4fe1\u606f\u7f3a\u5931\u6216\u53ef\u80fd\u5b58\u5728\u6570\u636e\u96c6\u504f\u79fb\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u3001\u4ece\u5934RSF\u548c\u96c6\u6210\u6574\u5408\u53ef\u4ee5\u6539\u5584\u4e73\u817a\u764c\u9884\u540e\u9884\u6d4b\u3002"}}
{"id": "2509.23280", "categories": ["cs.LG", "cs.AI", "math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2509.23280", "abs": "https://arxiv.org/abs/2509.23280", "authors": ["Yilie Huang"], "title": "Continuous-Time Reinforcement Learning for Asset-Liability Management", "comment": "Accepted at the 6th ACM International Conference on AI in Finance\n  (ICAIF 2025), 8 pages, 2 figures", "summary": "This paper proposes a novel approach for Asset-Liability Management (ALM) by\nemploying continuous-time Reinforcement Learning (RL) with a linear-quadratic\n(LQ) formulation that incorporates both interim and terminal objectives. We\ndevelop a model-free, policy gradient-based soft actor-critic algorithm\ntailored to ALM for dynamically synchronizing assets and liabilities. To ensure\nan effective balance between exploration and exploitation with minimal tuning,\nwe introduce adaptive exploration for the actor and scheduled exploration for\nthe critic. Our empirical study evaluates this approach against two enhanced\ntraditional financial strategies, a model-based continuous-time RL method, and\nthree state-of-the-art RL algorithms. Evaluated across 200 randomized market\nscenarios, our method achieves higher average rewards than all alternative\nstrategies, with rapid initial gains and sustained superior performance. The\noutperformance stems not from complex neural networks or improved parameter\nestimation, but from directly learning the optimal ALM strategy without\nlearning the environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u7684\u8d44\u4ea7-\u8d1f\u503a\u7ba1\u7406\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u7ebf\u6027\u4e8c\u6b21\u578b\u516c\u5f0f\u7ed3\u5408\u4e2d\u671f\u548c\u6700\u7ec8\u76ee\u6807\uff0c\u901a\u8fc7\u6a21\u578b\u65e0\u5173\u7684\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u8d44\u4ea7\u4e0e\u8d1f\u503a\u7684\u52a8\u6001\u540c\u6b65\u3002", "motivation": "\u4f20\u7edf\u8d44\u4ea7-\u8d1f\u503a\u7ba1\u7406\u65b9\u6cd5\u5728\u52a8\u6001\u540c\u6b65\u8d44\u4ea7\u548c\u8d1f\u503a\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3001\u65e0\u9700\u590d\u6742\u8c03\u53c2\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u578b\u65e0\u5173\u7684\u7b56\u7565\u68af\u5ea6\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u5f15\u5165\u6f14\u5458\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u548c\u8bc4\u8bba\u5bb6\u7684\u8ba1\u5212\u6027\u63a2\u7d22\u673a\u5236\u3002", "result": "\u5728200\u4e2a\u968f\u673a\u5e02\u573a\u60c5\u666f\u4e2d\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u66ff\u4ee3\u7b56\u7565\u4e2d\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u5956\u52b1\uff0c\u5177\u6709\u5feb\u901f\u521d\u59cb\u6536\u76ca\u548c\u6301\u7eed\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u5e76\u975e\u6765\u81ea\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u6216\u6539\u8fdb\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u800c\u662f\u6e90\u4e8e\u65e0\u9700\u5b66\u4e60\u73af\u5883\u5c31\u80fd\u76f4\u63a5\u5b66\u4e60\u6700\u4f18ALM\u7b56\u7565\u7684\u80fd\u529b\u3002"}}
{"id": "2509.23307", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23307", "abs": "https://arxiv.org/abs/2509.23307", "authors": ["Gabriel Jarry", "Ramon Dalmau", "Xavier Olive", "Philippe Very"], "title": "A Neural ODE Approach to Aircraft Flight Dynamics Modelling", "comment": null, "summary": "Accurate aircraft trajectory prediction is critical for air traffic\nmanagement, airline operations, and environmental assessment. This paper\nintroduces NODE-FDM, a Neural Ordinary Differential Equations-based Flight\nDynamics Model trained on Quick Access Recorder (QAR) data. By combining\nanalytical kinematic relations with data-driven components, NODE-FDM achieves a\nmore accurate reproduction of recorded trajectories than state-of-the-art\nmodels such as a BADA-based trajectory generation methodology (BADA4\nperformance model combined with trajectory control routines), particularly in\nthe descent phase of the flight. The analysis demonstrates marked improvements\nacross altitude, speed, and mass dynamics. Despite current limitations,\nincluding limited physical constraints and the limited availability of QAR\ndata, the results demonstrate the potential of physics-informed neural ordinary\ndifferential equations as a high-fidelity, data-driven approach to aircraft\nperformance modelling. Future work will extend the framework to incorporate a\nfull modelling of the lateral dynamics of the aircraft.", "AI": {"tldr": "NODE-FDM\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u98de\u884c\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f7f\u7528QAR\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u98de\u673a\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u6bd4BADA4\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u66f4\u51c6\u786e\uff0c\u7279\u522b\u662f\u5728\u4e0b\u964d\u9636\u6bb5\u3002", "motivation": "\u51c6\u786e\u7684\u98de\u673a\u8f68\u8ff9\u9884\u6d4b\u5bf9\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u3001\u822a\u7a7a\u516c\u53f8\u8fd0\u8425\u548c\u73af\u5883\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u89e3\u6790\u8fd0\u52a8\u5b66\u5173\u7cfb\u4e0e\u6570\u636e\u9a71\u52a8\u7ec4\u4ef6\uff0c\u4f7f\u7528\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u6784\u5efa\u98de\u884c\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u57fa\u4e8eQAR\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u9ad8\u5ea6\u3001\u901f\u5ea6\u548c\u8d28\u91cf\u52a8\u529b\u5b66\u65b9\u9762\u663e\u8457\u6539\u8fdb\uff0c\u6bd4BADA4\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u66f4\u51c6\u786e\u5730\u518d\u73b0\u8bb0\u5f55\u8f68\u8ff9\uff0c\u7279\u522b\u662f\u5728\u4e0b\u964d\u9636\u6bb5\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u6570\u636e\u9a71\u52a8\u7684\u98de\u673a\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u6846\u67b6\u4ee5\u5305\u542b\u5b8c\u6574\u7684\u98de\u673a\u6a2a\u5411\u52a8\u529b\u5b66\u5efa\u6a21\u3002"}}
{"id": "2509.23313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23313", "abs": "https://arxiv.org/abs/2509.23313", "authors": ["Xvyuan Liu", "Xiangfei Qiu", "Hanyin Cheng", "Xingjian Wu", "Chenjuan Guo", "Bin Yang", "Jilin Hu"], "title": "ASTGI: Adaptive Spatio-Temporal Graph Interactions for Irregular Multivariate Time Series Forecasting", "comment": null, "summary": "Irregular multivariate time series (IMTS) are prevalent in critical domains\nlike healthcare and finance, where accurate forecasting is vital for proactive\ndecision-making. However, the asynchronous sampling and irregular intervals\ninherent to IMTS pose two core challenges for existing methods: (1) how to\naccurately represent the raw information of irregular time series without\nintroducing data distortion, and (2) how to effectively capture the complex\ndynamic dependencies between observation points. To address these challenges,\nwe propose the Adaptive Spatio-Temporal Graph Interaction (ASTGI) framework.\nSpecifically, the framework first employs a Spatio-Temporal Point\nRepresentation module to encode each discrete observation as a point within a\nlearnable spatio-temporal embedding space. Second, a Neighborhood-Adaptive\nGraph Construction module adaptively builds a causal graph for each point in\nthe embedding space via nearest neighbor search. Subsequently, a\nSpatio-Temporal Dynamic Propagation module iteratively updates information on\nthese adaptive causal graphs by generating messages and computing interaction\nweights based on the relative spatio-temporal positions between points.\nFinally, a Query Point-based Prediction module generates the final forecast by\naggregating neighborhood information for a new query point and performing\nregression. Extensive experiments on multiple benchmark datasets demonstrate\nthat ASTGI outperforms various state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u65f6\u7a7a\u56fe\u4ea4\u4e92\uff08ASTGI\uff09\u6846\u67b6\u6765\u89e3\u51b3\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u7a7a\u70b9\u8868\u793a\u3001\u90bb\u57df\u81ea\u9002\u5e94\u56fe\u6784\u5efa\u3001\u65f6\u7a7a\u52a8\u6001\u4f20\u64ad\u548c\u67e5\u8be2\u70b9\u9884\u6d4b\u7b49\u6a21\u5757\uff0c\u6709\u6548\u5904\u7406\u5f02\u6b65\u91c7\u6837\u548c\u4e0d\u89c4\u5219\u95f4\u9694\u7684\u6311\u6218\u3002", "motivation": "\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5728\u533b\u7597\u548c\u91d1\u878d\u7b49\u5173\u952e\u9886\u57df\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5f02\u6b65\u91c7\u6837\u548c\u4e0d\u89c4\u5219\u95f4\u9694\u7ed9\u73b0\u6709\u65b9\u6cd5\u5e26\u6765\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5982\u4f55\u51c6\u786e\u8868\u793a\u539f\u59cb\u4fe1\u606f\u800c\u4e0d\u5f15\u5165\u6570\u636e\u5931\u771f\uff0c\u4ee5\u53ca\u5982\u4f55\u6709\u6548\u6355\u6349\u89c2\u6d4b\u70b9\u4e4b\u95f4\u7684\u590d\u6742\u52a8\u6001\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "ASTGI\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6a21\u5757\uff1a1\uff09\u65f6\u7a7a\u70b9\u8868\u793a\u6a21\u5757\u5c06\u79bb\u6563\u89c2\u6d4b\u7f16\u7801\u4e3a\u53ef\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u70b9\uff1b2\uff09\u90bb\u57df\u81ea\u9002\u5e94\u56fe\u6784\u5efa\u6a21\u5757\u901a\u8fc7\u6700\u8fd1\u90bb\u641c\u7d22\u4e3a\u6bcf\u4e2a\u70b9\u81ea\u9002\u5e94\u6784\u5efa\u56e0\u679c\u56fe\uff1b3\uff09\u65f6\u7a7a\u52a8\u6001\u4f20\u64ad\u6a21\u5757\u57fa\u4e8e\u70b9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u7a7a\u4f4d\u7f6e\u751f\u6210\u6d88\u606f\u5e76\u8ba1\u7b97\u4ea4\u4e92\u6743\u91cd\uff0c\u8fed\u4ee3\u66f4\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u56fe\u4e0a\u7684\u4fe1\u606f\uff1b4\uff09\u67e5\u8be2\u70b9\u9884\u6d4b\u6a21\u5757\u901a\u8fc7\u805a\u5408\u90bb\u57df\u4fe1\u606f\u5e76\u8fdb\u884c\u56de\u5f52\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cASTGI\u4f18\u4e8e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "ASTGI\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u7a7a\u56fe\u4ea4\u4e92\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23314", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23314", "abs": "https://arxiv.org/abs/2509.23314", "authors": ["Francesco Pappone", "Donato Crisostomi", "Emanuele Rodol\u00e0"], "title": "Two-Scale Latent Dynamics for Recurrent-Depth Transformers", "comment": null, "summary": "Recurrent-depth transformers scale test-time compute by iterating latent\ncomputations before emitting tokens. We study the geometry of these iterates\nand argue for a simple, \\emph{two-scale} operational picture: (i) within a\nlooped block, updates act as \\emph{small-scale refinements}; (ii) across\nconsecutive blocks, states undergo a \\emph{larger-scale drift}. Across\ncheckpoints, our measurements show that loop steps become \\emph{smaller} and\nincreasingly \\emph{orthogonal} to one another, indicating better local modeling\nof fine structure rather than merely pushing in a single direction. These\ndynamics motivate an early-exit mechanism based on the model's second-order\ndifference in step-size, which we show is superior in terms of performance,\nstability and time-efficiency, when compared to the KL-divergence exit strategy\nof Geiping et al. and its naive first-order counterpart.", "AI": {"tldr": "\u5faa\u73af\u6df1\u5ea6transformer\u901a\u8fc7\u8fed\u4ee3\u6f5c\u5728\u8ba1\u7b97\u6765\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3002\u7814\u7a76\u53d1\u73b0\u8fed\u4ee3\u8fc7\u7a0b\u5448\u73b0\u4e24\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\uff1a\u5757\u5185\u4e3a\u5c0f\u5c3a\u5ea6\u7cbe\u70bc\uff0c\u5757\u95f4\u4e3a\u5927\u5c3a\u5ea6\u6f02\u79fb\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e8c\u9636\u6b65\u957f\u5dee\u7684\u65e9\u9000\u673a\u5236\uff0c\u4f18\u4e8eKL\u6563\u5ea6\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u5faa\u73af\u6df1\u5ea6transformer\u4e2d\u8fed\u4ee3\u8fc7\u7a0b\u7684\u51e0\u4f55\u7279\u6027\uff0c\u7406\u89e3\u5176\u8ba1\u7b97\u52a8\u6001\uff0c\u4ee5\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65e9\u9000\u673a\u5236\u3002", "method": "\u5206\u6790\u8fed\u4ee3\u8fc7\u7a0b\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u6d4b\u91cf\u5faa\u73af\u6b65\u957f\u53d8\u5316\u548c\u6b63\u4ea4\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e8c\u9636\u6b65\u957f\u5dee\u7684\u65e9\u9000\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u5faa\u73af\u6b65\u957f\u53d8\u5c0f\u4e14\u66f4\u6b63\u4ea4\uff0c\u8868\u660e\u5c40\u90e8\u7cbe\u7ec6\u7ed3\u6784\u5efa\u6a21\u6539\u5584\u3002\u63d0\u51fa\u7684\u4e8c\u9636\u65e9\u9000\u673a\u5236\u5728\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u6548\u7387\u4e0a\u4f18\u4e8eKL\u6563\u5ea6\u7b56\u7565\u3002", "conclusion": "\u5faa\u73af\u6df1\u5ea6transformer\u7684\u8fed\u4ee3\u8fc7\u7a0b\u5177\u6709\u4e24\u5c3a\u5ea6\u51e0\u4f55\u7279\u6027\uff0c\u57fa\u4e8e\u4e8c\u9636\u6b65\u957f\u5dee\u7684\u65e9\u9000\u673a\u5236\u662f\u66f4\u4f18\u7684\u9000\u51fa\u7b56\u7565\u3002"}}
{"id": "2509.23315", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23315", "abs": "https://arxiv.org/abs/2509.23315", "authors": ["Khang Tran", "Hieu Cao", "Thinh Pham", "Nghiem Diep", "Tri Cao", "Binh Nguyen"], "title": "MELCOT: A Hybrid Learning Architecture with Marginal Preservation for Matrix-Valued Regression", "comment": null, "summary": "Regression is essential across many domains but remains challenging in\nhigh-dimensional settings, where existing methods often lose spatial structure\nor demand heavy storage. In this work, we address the problem of matrix-valued\nregression, where each sample is naturally represented as a matrix. We propose\nMELCOT, a hybrid model that integrates a classical machine learning-based\nMarginal Estimation (ME) block with a deep learning-based Learnable-Cost\nOptimal Transport (LCOT) block. The ME block estimates data marginals to\npreserve spatial information, while the LCOT block learns complex global\nfeatures. This design enables MELCOT to inherit the strengths of both classical\nand deep learning methods. Extensive experiments across diverse datasets and\ndomains demonstrate that MELCOT consistently outperforms all baselines while\nremaining highly efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86MELCOT\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u8fb9\u9645\u4f30\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u53ef\u5b66\u4e60\u6210\u672c\u6700\u4f18\u4f20\u8f93\uff0c\u7528\u4e8e\u77e9\u9635\u503c\u56de\u5f52\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u56de\u5f52\u95ee\u9898\u4e2d\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4e22\u5931\u7a7a\u95f4\u7ed3\u6784\u6216\u9700\u8981\u5927\u91cf\u5b58\u50a8\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u7136\u8868\u793a\u4e3a\u77e9\u9635\u7684\u6837\u672c\u7684\u77e9\u9635\u503c\u56de\u5f52\u95ee\u9898\u3002", "method": "\u63d0\u51faMELCOT\u6df7\u5408\u6a21\u578b\uff0c\u5305\u542b\u8fb9\u9645\u4f30\u8ba1(ME)\u5757\u548c\u53ef\u5b66\u4e60\u6210\u672c\u6700\u4f18\u4f20\u8f93(LCOT)\u5757\u3002ME\u5757\u4f30\u8ba1\u6570\u636e\u8fb9\u9645\u4ee5\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\uff0cLCOT\u5757\u5b66\u4e60\u590d\u6742\u5168\u5c40\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cMELCOT\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7387\u3002", "conclusion": "MELCOT\u6210\u529f\u7ed3\u5408\u4e86\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u4e3a\u77e9\u9635\u503c\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23323", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23323", "abs": "https://arxiv.org/abs/2509.23323", "authors": ["Xiangchen Song", "Jiaqi Sun", "Zijian Li", "Yujia Zheng", "Kun Zhang"], "title": "LLM Interpretability with Identifiable Temporal-Instantaneous Representation", "comment": "NeurIPS 2025", "summary": "Despite Large Language Models' remarkable capabilities, understanding their\ninternal representations remains challenging. Mechanistic interpretability\ntools such as sparse autoencoders (SAEs) were developed to extract\ninterpretable features from LLMs but lack temporal dependency modeling,\ninstantaneous relation representation, and more importantly theoretical\nguarantees, undermining both the theoretical foundations and the practical\nconfidence necessary for subsequent analyses. While causal representation\nlearning (CRL) offers theoretically grounded approaches for uncovering latent\nconcepts, existing methods cannot scale to LLMs' rich conceptual space due to\ninefficient computation. To bridge the gap, we introduce an identifiable\ntemporal causal representation learning framework specifically designed for\nLLMs' high-dimensional concept space, capturing both time-delayed and\ninstantaneous causal relations. Our approach provides theoretical guarantees\nand demonstrates efficacy on synthetic datasets scaled to match real-world\ncomplexity. By extending SAE techniques with our temporal causal framework, we\nsuccessfully discover meaningful concept relationships in LLM activations. Our\nfindings show that modeling both temporal and instantaneous conceptual\nrelationships advances the interpretability of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u8bc6\u522b\u7684\u65f6\u95f4\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7f3a\u4e4f\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u548c\u7406\u8bba\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\u7f3a\u4e4f\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u3001\u77ac\u65f6\u5173\u7cfb\u8868\u793a\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u9650\u5236\u4e86LLM\u5185\u90e8\u8868\u793a\u7684\u7406\u89e3\u3002\u56e0\u679c\u8868\u793a\u5b66\u4e60\u867d\u7136\u6709\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u65e0\u6cd5\u6269\u5c55\u5230LLM\u7684\u9ad8\u7ef4\u6982\u5ff5\u7a7a\u95f4\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u8bc6\u522b\u7684\u65f6\u95f4\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9LLM\u7684\u9ad8\u7ef4\u6982\u5ff5\u7a7a\u95f4\u8bbe\u8ba1\uff0c\u80fd\u591f\u6355\u6349\u65f6\u95f4\u5ef6\u8fdf\u548c\u77ac\u65f6\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5c06\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6280\u672f\u4e0e\u65f6\u95f4\u56e0\u679c\u6846\u67b6\u7ed3\u5408\uff0c\u6210\u529f\u53d1\u73b0\u4e86LLM\u6fc0\u6d3b\u4e2d\u6709\u610f\u4e49\u7684\u6982\u5ff5\u5173\u7cfb\u3002", "conclusion": "\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u548c\u77ac\u65f6\u6982\u5ff5\u5173\u7cfb\u80fd\u591f\u663e\u8457\u63a8\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.23325", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23325", "abs": "https://arxiv.org/abs/2509.23325", "authors": ["Jonas Ngnaw\u00e9", "Maxime Heuillet", "Sabyasachi Sahoo", "Yann Pequignot", "Ola Ahmad", "Audrey Durand", "Fr\u00e9d\u00e9ric Precioso", "Christian Gagn\u00e9"], "title": "Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling", "comment": null, "summary": "Fine-tuning pretrained models is a standard and effective workflow in modern\nmachine learning. However, robust fine-tuning (RFT), which aims to\nsimultaneously achieve adaptation to a downstream task and robustness to\nadversarial examples, remains challenging. Despite the abundance of non-robust\npretrained models in open-source repositories, their potential for RFT is less\nunderstood. We address this knowledge gap by systematically examining RFT from\nsuch non-robust models. Our experiments reveal that fine-tuning non-robust\nmodels with a robust objective, even under small perturbations, can lead to\npoor performance, a phenomenon that we dub \\emph{suboptimal transfer}. In\nchallenging scenarios (eg, difficult tasks, high perturbation), the resulting\nperformance can be so low that it may be considered a transfer failure. We find\nthat fine-tuning using a robust objective impedes task adaptation at the\nbeginning of training and eventually prevents optimal transfer. However, we\npropose a novel heuristic, \\emph{Epsilon-Scheduling}, a schedule over\nperturbation strength used during training that promotes optimal transfer.\nAdditionally, we introduce \\emph{expected robustness}, a metric that captures\nperformance across a range of perturbations, providing a more comprehensive\nevaluation of the accuracy-robustness trade-off for diverse models at test\ntime. Extensive experiments on a wide range of configurations (six pretrained\nmodels and five datasets) show that \\emph{Epsilon-Scheduling} successfully\nprevents \\emph{suboptimal transfer} and consistently improves expected\nrobustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u975e\u9c81\u68d2\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9c81\u68d2\u5fae\u8c03(RFT)\u7684\u6311\u6218\uff0c\u53d1\u73b0\u4e86\u6b21\u4f18\u8fc1\u79fb\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86Epsilon-Scheduling\u65b9\u6cd5\u548c\u9884\u671f\u9c81\u68d2\u6027\u6307\u6807\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5f00\u6e90\u5e93\u4e2d\u6709\u5927\u91cf\u975e\u9c81\u68d2\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u5728\u9c81\u68d2\u5fae\u8c03\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4f7f\u7528\u9c81\u68d2\u76ee\u6807\u5fae\u8c03\u975e\u9c81\u68d2\u6a21\u578b\u65f6\uff0c\u5373\u4f7f\u5728\u5c0f\u7684\u6270\u52a8\u4e0b\u4e5f\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86Epsilon-Scheduling\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u6270\u52a8\u5f3a\u5ea6\u8fdb\u884c\u8c03\u5ea6\u6765\u4fc3\u8fdb\u6700\u4f18\u8fc1\u79fb\uff1b\u540c\u65f6\u5f15\u5165\u4e86\u9884\u671f\u9c81\u68d2\u6027\u6307\u6807\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5728\u591a\u79cd\u6270\u52a8\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u914d\u7f6e\u5b9e\u9a8c\uff086\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c5\u4e2a\u6570\u636e\u96c6\uff09\u4e2d\uff0cEpsilon-Scheduling\u65b9\u6cd5\u6210\u529f\u9632\u6b62\u4e86\u6b21\u4f18\u8fc1\u79fb\uff0c\u5e76\u6301\u7eed\u63d0\u9ad8\u4e86\u9884\u671f\u9c81\u68d2\u6027\u3002", "conclusion": "\u4f7f\u7528\u9c81\u68d2\u76ee\u6807\u5fae\u8c03\u975e\u9c81\u68d2\u6a21\u578b\u4f1a\u963b\u788d\u4efb\u52a1\u9002\u5e94\u5e76\u963b\u6b62\u6700\u4f18\u8fc1\u79fb\uff0c\u4f46\u901a\u8fc7Epsilon-Scheduling\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002"}}
{"id": "2509.23348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23348", "abs": "https://arxiv.org/abs/2509.23348", "authors": ["Xavier Aramayo Carrasco", "Grigoriy Ksenofontov", "Aleksei Leonov", "Iaroslav Sergeevich Koshelev", "Alexander Korotin"], "title": "Entering the Era of Discrete Diffusion Models: A Benchmark for Schr\u00f6dinger Bridges and Entropic Optimal Transport", "comment": null, "summary": "The Entropic Optimal Transport (EOT) problem and its dynamic counterpart, the\nSchr\\\"odinger bridge (SB) problem, play an important role in modern machine\nlearning, linking generative modeling with optimal transport theory. While\nrecent advances in discrete diffusion and flow models have sparked growing\ninterest in applying SB methods to discrete domains, there is still no reliable\nway to evaluate how well these methods actually solve the underlying problem.\nWe address this challenge by introducing a benchmark for SB on discrete spaces.\nOur construction yields pairs of probability distributions with analytically\nknown SB solutions, enabling rigorous evaluation. As a byproduct of building\nthis benchmark, we obtain two new SB algorithms, DLightSB and DLightSB-M, and\nadditionally extend prior related work to construct the $\\alpha$-CSBM\nalgorithm. We demonstrate the utility of our benchmark by evaluating both\nexisting and new solvers in high-dimensional discrete settings. This work\nprovides the first step toward proper evaluation of SB methods on discrete\nspaces, paving the way for more reproducible future studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u79bb\u6563\u7a7a\u95f4\u4e0a\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u89e3\u6790\u5df2\u77e5\u89e3\u7684\u5206\u5e03\u5bf9\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u4e2a\u65b0\u7b97\u6cd5DLightSB\u548cDLightSB-M\u3002", "motivation": "\u867d\u7136\u79bb\u6563\u6269\u6563\u548c\u6d41\u6a21\u578b\u6fc0\u53d1\u4e86\u5728\u79bb\u6563\u57df\u5e94\u7528\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u7684\u5174\u8da3\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\u7684\u6548\u679c\u3002", "method": "\u6784\u5efa\u4e86\u5177\u6709\u89e3\u6790\u5df2\u77e5\u859b\u5b9a\u8c14\u6865\u89e3\u7684\u5206\u5e03\u5bf9\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86DLightSB\u3001DLightSB-M\u7b97\u6cd5\uff0c\u8fd8\u6269\u5c55\u4e86\u03b1-CSBM\u7b97\u6cd5\u3002", "result": "\u5728\u9ad8\u7ef4\u79bb\u6563\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u4e86\u73b0\u6709\u548c\u65b0\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8fd9\u662f\u671d\u7740\u5728\u79bb\u6563\u7a7a\u95f4\u4e0a\u6b63\u786e\u8bc4\u4f30\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u8fc8\u51fa\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u672a\u6765\u66f4\u53ef\u91cd\u590d\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.23357", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23357", "abs": "https://arxiv.org/abs/2509.23357", "authors": ["Andrey Kharitenko", "Zebang Shen", "Riccardo de Santi", "Niao He", "Florian Doerfler"], "title": "Landing with the Score: Riemannian Optimization through Denoising", "comment": "37 pages, 9 figures", "summary": "Under the data manifold hypothesis, high-dimensional data are concentrated\nnear a low-dimensional manifold. We study the problem of Riemannian\noptimization over such manifolds when they are given only implicitly through\nthe data distribution, and the standard manifold operations required by\nclassical algorithms are unavailable. This formulation captures a broad class\nof data-driven design problems that are central to modern generative AI. Our\nkey idea is to introduce a link function that connects the data distribution to\nthe geometric operations needed for optimization. We show that this function\nenables the recovery of essential manifold operations, such as retraction and\nRiemannian gradient computation. Moreover, we establish a direct connection\nbetween our construction and the score function in diffusion models of the data\ndistribution. This connection allows us to leverage well-studied\nparameterizations, efficient training procedures, and even pretrained score\nnetworks from the diffusion model literature to perform optimization. Building\non this foundation, we propose two efficient inference-time algorithms --\nDenoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD)\n-- and provide theoretical guarantees for both feasibility (approximate\nmanifold adherence) and optimality (small Riemannian gradient norm). Finally,\nwe demonstrate the effectiveness of our approach on finite-horizon reference\ntracking tasks in data-driven control, highlighting its potential for practical\ngenerative and design applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u6570\u636e\u6d41\u5f62\u4e0a\u8fdb\u884c\u9ece\u66fc\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u94fe\u63a5\u51fd\u6570\u8fde\u63a5\u6570\u636e\u5206\u5e03\u4e0e\u51e0\u4f55\u64cd\u4f5c\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f97\u5206\u51fd\u6570\u5b9e\u73b0\u4f18\u5316\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u9ad8\u6548\u63a8\u7406\u7b97\u6cd5\u3002", "motivation": "\u5728\u9ad8\u7ef4\u6570\u636e\u96c6\u4e2d\u5728\u4f4e\u7ef4\u6d41\u5f62\u7684\u5047\u8bbe\u4e0b\uff0c\u89e3\u51b3\u5f53\u6d41\u5f62\u4ec5\u901a\u8fc7\u6570\u636e\u5206\u5e03\u9690\u5f0f\u7ed9\u51fa\u4e14\u7f3a\u4e4f\u7ecf\u5178\u7b97\u6cd5\u6240\u9700\u6d41\u5f62\u64cd\u4f5c\u65f6\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8fd9\u5bf9\u73b0\u4ee3\u751f\u6210AI\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u8bbe\u8ba1\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u94fe\u63a5\u51fd\u6570\u8fde\u63a5\u6570\u636e\u5206\u5e03\u4e0e\u51e0\u4f55\u64cd\u4f5c\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f97\u5206\u51fd\u6570\u6062\u590d\u6d41\u5f62\u64cd\u4f5c\uff0c\u63d0\u51faDLF\u548cDRGD\u4e24\u79cd\u63a8\u7406\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5efa\u7acb\u4e86\u4e0e\u6269\u6563\u6a21\u578b\u5f97\u5206\u51fd\u6570\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u80fd\u591f\u5229\u7528\u5df2\u6709\u53c2\u6570\u5316\u3001\u8bad\u7ec3\u8fc7\u7a0b\u548c\u9884\u8bad\u7ec3\u5f97\u5206\u7f51\u7edc\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u6570\u636e\u9a71\u52a8\u63a7\u5236\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u548c\u8bbe\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u63a5\u51fd\u6570\u548c\u6269\u6563\u6a21\u578b\u6280\u672f\u6210\u529f\u89e3\u51b3\u4e86\u9690\u5f0f\u6d41\u5f62\u4e0a\u7684\u9ece\u66fc\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2509.23365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23365", "abs": "https://arxiv.org/abs/2509.23365", "authors": ["Hanlin Zhu", "Shibo Hao", "Zhiting Hu", "Jiantao Jiao", "Stuart Russell", "Yuandong Tian"], "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought", "comment": "29 pages, 5 figures", "summary": "Previous work shows that the chain of continuous thought (continuous CoT)\nimproves the reasoning capability of large language models (LLMs) by enabling\nimplicit parallel thinking, and a subsequent work provided theoretical insight\nby showing that a two-layer transformer equipped with continuous CoT can\nefficiently solve directed graph reachability by maintaining a superposition of\nmultiple reasoning traces in the continuous thought. However, it remains\nunclear how the superposition mechanism is naturally learned from\ngradient-based training methods. To fill this gap, we theoretically analyze the\ntraining dynamics of a simplified two-layer transformer on the directed graph\nreachability problem to unveil how the superposition mechanism emerges during\ntraining in two training stages -- (i) a thought-generation stage that\nautoregressively expands the continuous thought, and (ii) a prediction stage\nthat converts the thought into the final answer. Our analysis reveals that\nduring training using continuous thought, the index-matching logit, an\nimportant quantity which reflects the strength of the model's local search\nability, will first increase and then remain bounded under mild assumptions.\nThe bounded index-matching logit effectively balances exploration and\nexploitation during the reasoning process: the model will exploit local problem\nstructures to identify plausible search traces, and assign comparable weights\nto multiple such traces to explore when it is uncertain about which solution is\ncorrect, which results in superposition. Our experimental results tracking the\ngrowth of logits further validate our theory.", "AI": {"tldr": "\u672c\u6587\u7406\u8bba\u5206\u6790\u4e86\u8fde\u7eed\u601d\u7ef4\u94fe\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u7279\u6027\uff0c\u63ed\u793a\u4e86\u53e0\u52a0\u673a\u5236\u5982\u4f55\u901a\u8fc7\u68af\u5ea6\u8bad\u7ec3\u81ea\u7136\u4ea7\u751f\uff0c\u5305\u62ec\u601d\u7ef4\u751f\u6210\u548c\u9884\u6d4b\u4e24\u4e2a\u9636\u6bb5\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660e\u8fde\u7eed\u601d\u7ef4\u94fe\u80fd\u901a\u8fc7\u9690\u5f0f\u5e76\u884c\u601d\u7ef4\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53e0\u52a0\u673a\u5236\u5982\u4f55\u4ece\u68af\u5ea6\u8bad\u7ec3\u4e2d\u81ea\u7136\u5b66\u4e60\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u7406\u8bba\u5206\u6790\u7b80\u5316\u53cc\u5c42transformer\u5728\u6709\u5411\u56fe\u53ef\u8fbe\u6027\u95ee\u9898\u4e0a\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u8ffd\u8e2a\u7d22\u5f15\u5339\u914dlogit\u7684\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7d22\u5f15\u5339\u914dlogit\u5728\u8bad\u7ec3\u4e2d\u5148\u589e\u540e\u4fdd\u6301\u6709\u754c\uff0c\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4f7f\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u65f6\u5bf9\u591a\u4e2a\u641c\u7d22\u8f68\u8ff9\u8d4b\u4e88\u76f8\u8fd1\u6743\u91cd\uff0c\u5f62\u6210\u53e0\u52a0\u3002", "conclusion": "\u8fde\u7eed\u601d\u7ef4\u94fe\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6709\u754c\u7684\u7d22\u5f15\u5339\u914dlogit\u4fc3\u4f7f\u6a21\u578b\u81ea\u7136\u5b66\u4e60\u53e0\u52a0\u673a\u5236\uff0c\u5728\u5229\u7528\u5c40\u90e8\u7ed3\u6784\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u591a\u4e2a\u63a8\u7406\u8def\u5f84\u7684\u63a2\u7d22\u80fd\u529b\u3002"}}
{"id": "2509.23366", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23366", "abs": "https://arxiv.org/abs/2509.23366", "authors": ["Ange-Cl\u00e9ment Akazan", "Verlon Roel Mbingui"], "title": "Splines-Based Feature Importance in Kolmogorov-Arnold Networks: A Framework for Supervised Tabular Data Dimensionality Reduction", "comment": null, "summary": "High-dimensional datasets require effective feature selection to improve\npredictive performance, interpretability, and robustness. We propose and\nevaluate feature selection methods for tabular datasets based on\nKolmogorov-Arnold networks (KANs), which parameterize feature transformations\nthrough splines, enabling direct access to interpretable importance measures.\nWe introduce four KAN-based selectors ($\\textit{KAN-L1}$, $\\textit{KAN-L2}$,\n$\\textit{KAN-SI}$, $\\textit{KAN-KO}$) and compare them against classical\nbaselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple\nclassification and regression tabular dataset benchmarks. Average (over three\nretention levels: 20\\%, 40\\%, and 60\\%) F1 scores and $R^2$ score results\nreveal that KAN-based selectors, particularly $\\textit{KAN-L2}$,\n$\\textit{KAN-L1}$, $\\textit{KAN-SI}$, and $\\textit{KAN-KO}$, are competitive\nwith and sometimes superior to classical baselines in structured and synthetic\ndatasets. However, $\\textit{KAN-L1}$ is often too aggressive in regression,\nremoving useful features, while $\\textit{KAN-L2}$ underperforms in\nclassification, where simple coefficient shrinkage misses complex feature\ninteractions. $\\textit{KAN-L2}$ and $\\textit{KAN-SI}$ provide robust\nperformance on noisy regression datasets and heterogeneous datasets, aligning\nclosely with ensemble predictors. In classification tasks, KAN selectors such\nas $\\textit{KAN-L1}$, $\\textit{KAN-KO}$, and $\\textit{KAN-SI}$ sometimes\nsurpass the other selectors by eliminating redundancy, particularly in\nhigh-dimensional multi-class data. Overall, our findings demonstrate that\nKAN-based feature selection provides a powerful and interpretable alternative\nto traditional methods, capable of uncovering nonlinear and multivariate\nfeature relevance beyond sparsity or impurity-based measures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eKolmogorov-Arnold\u7f51\u7edc(KANs)\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u79cdKAN\u9009\u62e9\u5668\u4e0e\u7ecf\u5178\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u5728\u8868\u683c\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u7ade\u4e89\u6027\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u96c6\u9700\u8981\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u6765\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u7279\u5f81\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cdKAN-based\u7279\u5f81\u9009\u62e9\u5668(KAN-L1, KAN-L2, KAN-SI, KAN-KO)\uff0c\u4f7f\u7528\u6837\u6761\u53c2\u6570\u5316\u7279\u5f81\u53d8\u6362\uff0c\u5e76\u4e0eLASSO\u3001\u968f\u673a\u68ee\u6797\u3001\u4e92\u4fe1\u606f\u3001SVM-RFE\u7b49\u7ecf\u5178\u65b9\u6cd5\u5728\u591a\u4e2a\u5206\u7c7b\u548c\u56de\u5f52\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "KAN\u9009\u62e9\u5668\u5728\u7ed3\u6784\u5316\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u4e0e\u7ecf\u5178\u57fa\u7ebf\u65b9\u6cd5\u7ade\u4e89\u751a\u81f3\u66f4\u4f18\u3002KAN-L2\u548cKAN-SI\u5728\u566a\u58f0\u56de\u5f52\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5065\uff0cKAN-L1\u3001KAN-KO\u548cKAN-SI\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u6709\u65f6\u4f18\u4e8e\u5176\u4ed6\u9009\u62e9\u5668\u3002", "conclusion": "\u57fa\u4e8eKAN\u7684\u7279\u5f81\u9009\u62e9\u4e3a\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u53d1\u73b0\u8d85\u8d8a\u7a00\u758f\u6027\u6216\u57fa\u4e8e\u4e0d\u7eaf\u5ea6\u6d4b\u91cf\u7684\u975e\u7ebf\u6027\u548c\u591a\u53d8\u91cf\u7279\u5f81\u76f8\u5173\u6027\u3002"}}
{"id": "2509.23373", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23373", "abs": "https://arxiv.org/abs/2509.23373", "authors": ["Xi Ding", "Lei Wang", "Piotr Koniusz", "Yongsheng Gao"], "title": "Graph Your Own Prompt", "comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "We propose Graph Consistency Regularization (GCR), a novel framework that\ninjects relational graph structures, derived from model predictions, into the\nlearning process to promote class-aware, semantically meaningful feature\nrepresentations. Functioning as a form of self-prompting, GCR enables the model\nto refine its internal structure using its own outputs. While deep networks\nlearn rich representations, these often capture noisy inter-class similarities\nthat contradict the model's predicted semantics. GCR addresses this issue by\nintroducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths.\nEach GCL builds a batch-level feature similarity graph and aligns it with a\nglobal, class-aware masked prediction graph, derived by modulating softmax\nprediction similarities with intra-class indicators. This alignment enforces\nthat feature-level relationships reflect class-consistent prediction behavior,\nacting as a semantic regularizer throughout the network. Unlike prior work, GCR\nintroduces a multi-layer, cross-space graph alignment mechanism with adaptive\nweighting, where layer importance is learned from graph discrepancy magnitudes.\nThis allows the model to prioritize semantically reliable layers and suppress\nnoisy ones, enhancing feature quality without modifying the architecture or\ntraining procedure. GCR is model-agnostic, lightweight, and improves semantic\nstructure across various networks and datasets. Experiments show that GCR\npromotes cleaner feature structure, stronger intra-class cohesion, and improved\ngeneralization, offering a new perspective on learning from prediction\nstructure. [Project website](https://darcyddx.github.io/gcr/)\n[Code](https://github.com/Darcyddx/graph-prompt)", "AI": {"tldr": "\u63d0\u51fa\u56fe\u4e00\u81f4\u6027\u6b63\u5219\u5316(GCR)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u9884\u6d4b\u751f\u6210\u7684\u56fe\u7ed3\u6784\u6ce8\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u7c7b\u611f\u77e5\u7684\u8bed\u4e49\u7279\u5f81\u8868\u793a\uff0c\u5b9e\u73b0\u7279\u5f81\u7ea7\u5173\u7cfb\u4e0e\u7c7b\u4e00\u81f4\u9884\u6d4b\u884c\u4e3a\u7684\u5bf9\u9f50\u3002", "motivation": "\u6df1\u5ea6\u7f51\u7edc\u5b66\u4e60\u5230\u7684\u4e30\u5bcc\u8868\u793a\u5e38\u5305\u542b\u4e0e\u6a21\u578b\u9884\u6d4b\u8bed\u4e49\u77db\u76fe\u7684\u566a\u58f0\u7c7b\u95f4\u76f8\u4f3c\u6027\uff0c\u9700\u8981\u63d0\u5347\u7279\u5f81\u8868\u793a\u7684\u8bed\u4e49\u8d28\u91cf\u3002", "method": "\u5728\u4efb\u610f\u6df1\u5ea6\u5f15\u5165\u53c2\u6570\u81ea\u7531\u7684\u56fe\u4e00\u81f4\u6027\u5c42(GCL)\uff0c\u6784\u5efa\u6279\u6b21\u7ea7\u7279\u5f81\u76f8\u4f3c\u56fe\u5e76\u4e0e\u5168\u5c40\u7c7b\u611f\u77e5\u63a9\u7801\u9884\u6d4b\u56fe\u5bf9\u9f50\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u5b66\u4e60\u5c42\u91cd\u8981\u6027\u3002", "result": "GCR\u4fc3\u8fdb\u4e86\u66f4\u6e05\u6670\u7684\u7279\u5f81\u7ed3\u6784\u3001\u66f4\u5f3a\u7684\u7c7b\u5185\u51dd\u805a\u529b\u548c\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5404\u79cd\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "GCR\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u9884\u6d4b\u7ed3\u6784\u5b66\u4e60\u7684\u65b0\u89c6\u89d2\uff0c\u662f\u6a21\u578b\u65e0\u5173\u3001\u8f7b\u91cf\u7ea7\u7684\u65b9\u6cd5\uff0c\u80fd\u589e\u5f3a\u8bed\u4e49\u7ed3\u6784\u800c\u4e0d\u6539\u53d8\u67b6\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2509.23405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23405", "abs": "https://arxiv.org/abs/2509.23405", "authors": ["Fred Zhangzhi Peng", "Zachary Bezemek", "Jarrid Rector-Brooks", "Shuibai Zhang", "Anru R. Zhang", "Michael Bronstein", "Avishek Joey Bose", "Alexander Tong"], "title": "Planner Aware Path Learning in Diffusion Language Models Training", "comment": null, "summary": "Diffusion language models have emerged as a powerful alternative to\nautoregressive models, enabling fast inference through flexible and parallel\ngeneration paths. This flexibility is enabled by new sampling strategies, or\nplanners, that iteratively choose where to denoise along the sequence rather\nthan sampling uniformly at random. However, by modifying reverse paths,\nplanners introduce a mismatch between the uniformly random denoising paths used\nduring training and the planning-based paths used at inference. In this work,\nwe systematically investigate this mismatch and theoretically show that the\nstandard discrete diffusion training evidence lower bound (ELBO) does not\naccurately describe a denoiser under non-uniform planning. To bridge this gap,\nwe derive a new Planned Evidence Lower Bound (P-ELBO) that directly\nincorporates planner-based reverse dynamics into the training objective.\nBuilding on this, we propose Planner Aware Path Learning (PAPL), a simple and\neffective modification of the standard masked discrete diffusion loss that\naligns training and inference under planned denoisers. Empirically, PAPL\ndelivers consistent improvements across domains, including a 40% relative gain\nin protein sequence modeling, up to a 4x improvement in MAUVE for text\ngeneration, and a 23% relative gain in HumanEval pass@10 for code generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPAPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u6765\u89e3\u51b3\u8bad\u7ec3\u65f6\u5747\u5300\u968f\u673a\u53bb\u566a\u4e0e\u63a8\u7406\u65f6\u89c4\u5212\u53bb\u566a\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7075\u6d3b\u7684\u5e76\u884c\u751f\u6210\u8def\u5f84\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u4f46\u89c4\u5212\u5668\u4fee\u6539\u4e86\u53cd\u5411\u8def\u5f84\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u7684\u5747\u5300\u968f\u673a\u53bb\u566a\u8def\u5f84\u4e0e\u63a8\u7406\u65f6\u7684\u89c4\u5212\u8def\u5f84\u4e0d\u5339\u914d\u3002", "method": "\u63a8\u5bfc\u4e86\u65b0\u7684\u89c4\u5212\u8bc1\u636e\u4e0b\u754c(P-ELBO)\uff0c\u5e76\u63d0\u51faPlanner Aware Path Learning (PAPL)\u65b9\u6cd5\uff0c\u4fee\u6539\u6807\u51c6\u63a9\u7801\u79bb\u6563\u6269\u6563\u635f\u5931\u4ee5\u5728\u89c4\u5212\u53bb\u566a\u5668\u4e0b\u5bf9\u9f50\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "PAPL\u5728\u4e0d\u540c\u9886\u57df\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\uff1a\u86cb\u767d\u8d28\u5e8f\u5217\u5efa\u6a21\u76f8\u5bf9\u63d0\u534740%\uff0c\u6587\u672c\u751f\u6210MAUVE\u6307\u6807\u63d0\u5347\u9ad8\u8fbe4\u500d\uff0c\u4ee3\u7801\u751f\u6210HumanEval pass@10\u76f8\u5bf9\u63d0\u534723%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c4\u5212\u5668\u7eb3\u5165\u8bad\u7ec3\u76ee\u6807\uff0cPAPL\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u9886\u57df\u7684\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2509.23409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23409", "abs": "https://arxiv.org/abs/2509.23409", "authors": ["Devesh Sharma", "Aditya Kishore", "Ayush Garg", "Debajyoti Mazumder", "Debasis Mohapatra", "Jasabanta Patro"], "title": "Mind the Links: Cross-Layer Attention for Link Prediction in Multiplex Networks", "comment": null, "summary": "Multiplex graphs capture diverse relations among shared nodes. Most\npredictors either collapse layers or treat them independently. This loses\ncrucial inter-layer dependencies and struggles with scalability. To overcome\nthis, we frame multiplex link prediction as multi-view edge classification. For\neach node pair, we construct a sequence of per-layer edge views and apply\ncross-layer self-attention to fuse evidence for the target layer. We present\ntwo models as instances of this framework: Trans-SLE, a lightweight transformer\nover static embeddings, and Trans-GAT, which combines layer-specific GAT\nencoders with transformer fusion. To ensure scalability and fairness, we\nintroduce a Union--Set candidate pool and two leakage-free protocols:\ncross-layer and inductive subgraph generalization. Experiments on six public\nmultiplex datasets show consistent macro-F_1 gains over strong baselines (MELL,\nHOPLP-MUL, RMNE). Our approach is simple, scalable, and compatible with both\nprecomputed embeddings and GNN encoders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u56fe\u8fb9\u7f18\u5206\u7c7b\u7684\u591a\u8def\u56fe\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u8de8\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u5404\u5c42\u4fe1\u606f\uff0c\u5305\u542bTrans-SLE\u548cTrans-GAT\u4e24\u4e2a\u6a21\u578b\u5b9e\u4f8b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u6269\u5c55\u4e14\u65e0\u6cc4\u6f0f\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5408\u5e76\u56fe\u5c42\u8981\u4e48\u72ec\u7acb\u5904\u7406\u56fe\u5c42\uff0c\u8fd9\u4f1a\u4e22\u5931\u5173\u952e\u7684\u5c42\u95f4\u4f9d\u8d56\u5173\u7cfb\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5c06\u591a\u8def\u94fe\u63a5\u9884\u6d4b\u6784\u5efa\u4e3a\u591a\u89c6\u56fe\u8fb9\u7f18\u5206\u7c7b\u95ee\u9898\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u5bf9\u6784\u5efa\u6bcf\u5c42\u8fb9\u7f18\u89c6\u56fe\u5e8f\u5217\uff0c\u5e94\u7528\u8de8\u5c42\u81ea\u6ce8\u610f\u529b\u878d\u5408\u76ee\u6807\u5c42\u8bc1\u636e\u3002\u63d0\u51fa\u4e86Trans-SLE\uff08\u57fa\u4e8e\u9759\u6001\u5d4c\u5165\u7684\u8f7b\u91cftransformer\uff09\u548cTrans-GAT\uff08\u7ed3\u5408\u5c42\u7279\u5b9aGAT\u7f16\u7801\u5668\u548ctransformer\u878d\u5408\uff09\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u591a\u8def\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff08MELL\u3001HOPLP-MUL\u3001RMNE\uff09\u83b7\u5f97\u4e86\u6301\u7eed\u7684macro-F1\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u53ef\u6269\u5c55\uff0c\u4e14\u517c\u5bb9\u9884\u8ba1\u7b97\u5d4c\u5165\u548cGNN\u7f16\u7801\u5668\u3002"}}
{"id": "2509.23413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23413", "abs": "https://arxiv.org/abs/2509.23413", "authors": ["Changliang Zhou", "Canhong Yu", "Shunyu Yao", "Xi Lin", "Zhenkun Wang", "Yu Zhou", "Qingfu Zhang"], "title": "URS: A Unified Neural Routing Solver for Cross-Problem Zero-Shot Generalization", "comment": "31 pages,3 figures", "summary": "Multi-task neural routing solvers have emerged as a promising paradigm for\ntheir ability to solve multiple vehicle routing problems (VRPs) using a single\nmodel. However, existing neural solvers typically rely on predefined problem\nconstraints or require per-problem fine-tuning, which substantially limits\ntheir zero-shot generalization ability to unseen VRP variants. To address this\ncritical bottleneck, we propose URS, a unified neural routing solver capable of\nzero-shot generalization across a wide range of unseen VRPs using a single\nmodel without any fine-tuning. The key component of URS is the unified data\nrepresentation (UDR), which replaces problem enumeration with data unification,\nthereby broadening the problem coverage and reducing reliance on domain\nexpertise. In addition, we propose a Mixed Bias Module (MBM) to efficiently\nlearn the geometric and relational biases inherent in various problems. On top\nof the proposed UDR, we further develop a parameter generator that adaptively\nadjusts the decoder and bias weights of MBM to enhance zero-shot\ngeneralization. Moreover, we propose an LLM-driven constraint satisfaction\nmechanism, which translates raw problem descriptions into executable stepwise\nmasking functions to ensure solution feasibility. Extensive experiments\ndemonstrate that URS can consistently produce high-quality solutions for more\nthan 100 distinct VRP variants without any fine-tuning, which includes more\nthan 90 unseen variants. To the best of our knowledge, URS is the first neural\nsolver capable of handling over 100 VRP variants with a single model.", "AI": {"tldr": "URS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u795e\u7ecf\u8def\u7531\u6c42\u89e3\u5668\uff0c\u80fd\u591f\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5728\u8d85\u8fc7100\u79cd\u4e0d\u540c\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u53d8\u4f53\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u6c42\u89e3\u5668\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u95ee\u9898\u7ea6\u675f\u6216\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u95ee\u9898\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u672a\u89c1VRP\u53d8\u4f53\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6570\u636e\u8868\u793a(UDR)\u66ff\u4ee3\u95ee\u9898\u679a\u4e3e\uff0c\u6df7\u5408\u504f\u7f6e\u6a21\u5757(MBM)\u5b66\u4e60\u51e0\u4f55\u548c\u5173\u7cfb\u504f\u7f6e\uff0c\u53c2\u6570\u751f\u6210\u5668\u81ea\u9002\u5e94\u8c03\u6574\u89e3\u7801\u5668\u548c\u504f\u7f6e\u6743\u91cd\uff0c\u4ee5\u53caLLM\u9a71\u52a8\u7684\u7ea6\u675f\u6ee1\u8db3\u673a\u5236\u3002", "result": "URS\u80fd\u591f\u5728\u8d85\u8fc7100\u79cd\u4e0d\u540c\u7684VRP\u53d8\u4f53\u4e0a\u4e00\u81f4\u5730\u4ea7\u751f\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2d\u5305\u62ec\u8d85\u8fc790\u79cd\u672a\u89c1\u53d8\u4f53\uff0c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u3002", "conclusion": "URS\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5904\u7406\u8d85\u8fc7100\u79cdVRP\u53d8\u4f53\u7684\u795e\u7ecf\u6c42\u89e3\u5668\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.23436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23436", "abs": "https://arxiv.org/abs/2509.23436", "authors": ["Ashkan Shahbazi", "Chayne Thrash", "Yikun Bai", "Keaton Hamm", "Navid NaderiAlizadeh", "Soheil Kolouri"], "title": "LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport", "comment": null, "summary": "Transformers have proven highly effective across a wide range of modalities.\nHowever, the quadratic complexity of the standard softmax attention mechanism\nposes a fundamental barrier to scaling them to long context windows. A large\nbody of work addresses this with linear attention, which reformulates attention\nas a kernel function and approximates it with finite feature maps to achieve\nlinear-time computation. Orthogonal to computational scaling, most attention\nmechanisms -- both quadratic and linear -- produce row-normalized maps that can\nover-focus on a few tokens, degrading robustness and information flow.\nEnforcing doubly-stochastic attention alleviates this by balancing token\nparticipation across rows and columns, but existing doubly-stochastic attention\nmechanisms typically introduce substantial overhead, undermining scalability.\nWe propose LOTFormer, a principled attention mechanism that is simultaneously\nlinear-time and doubly-stochastic. Our approach exploits the connection between\nattention maps and transportation plans between query and key measures. The\ncentral idea is to constrain the transport plan to be low-rank by conditioning\nit on a learnable pivot measure with small support. Concretely, we solve two\nentropic optimal transport problems (queries $\\to$ pivot and pivot $\\to$ keys)\nand compose them into a conditional (glued) coupling. This yields an attention\nmatrix that is provably doubly-stochastic, has rank at most $r \\ll n$, and\napplies to values in $O(nr)$ time without forming the full $n \\times n$ map.\nThe pivot locations and masses are learned end-to-end. Empirically, LOTFormer\nachieves state-of-the-art results on the Long Range Arena benchmark, surpassing\nprior linear and transport-based attention methods in both accuracy and\nefficiency.", "AI": {"tldr": "LOTFormer\u662f\u4e00\u79cd\u540c\u65f6\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u53cc\u91cd\u968f\u673a\u6027\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u6620\u5c04\u5efa\u6a21\u4e3a\u67e5\u8be2\u548c\u952e\u4e4b\u95f4\u7684\u4f20\u8f93\u8ba1\u5212\uff0c\u5e76\u4f7f\u7528\u4f4e\u79e9\u6761\u4ef6\u4f20\u8f93\u6765\u5e73\u8861\u4ee4\u724c\u53c2\u4e0e\u5ea6\uff0c\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "motivation": "\u6807\u51c6softmax\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86Transformer\u5728\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u7684\u6269\u5c55\uff0c\u800c\u73b0\u6709\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u867d\u7136\u89e3\u51b3\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4f46\u6ce8\u610f\u529b\u6620\u5c04\u7684\u884c\u5f52\u4e00\u5316\u7279\u6027\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u5173\u6ce8\u5c11\u6570\u4ee4\u724c\uff0c\u964d\u4f4e\u9c81\u68d2\u6027\u548c\u4fe1\u606f\u6d41\u52a8\u3002\u53cc\u91cd\u968f\u673a\u6ce8\u610f\u529b\u53ef\u4ee5\u5e73\u8861\u4ee4\u724c\u53c2\u4e0e\u5ea6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u663e\u8457\u5f00\u9500\uff0c\u5f71\u54cd\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5c06\u6ce8\u610f\u529b\u6620\u5c04\u5efa\u6a21\u4e3a\u67e5\u8be2\u548c\u952e\u4e4b\u95f4\u7684\u4f20\u8f93\u8ba1\u5212\uff0c\u901a\u8fc7\u7ea6\u675f\u4f20\u8f93\u8ba1\u5212\u4e3a\u4f4e\u79e9\u6765\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u8ba1\u7b97\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5b66\u4e60\u4e00\u4e2a\u5177\u6709\u5c0f\u652f\u6491\u7684\u53ef\u5b66\u4e60\u67a2\u8f74\u5ea6\u91cf\uff1b2\uff09\u6c42\u89e3\u4e24\u4e2a\u71b5\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff08\u67e5\u8be2\u2192\u67a2\u8f74\u548c\u67a2\u8f74\u2192\u952e\uff09\uff1b3\uff09\u5c06\u5b83\u4eec\u7ec4\u5408\u6210\u6761\u4ef6\u8026\u5408\uff0c\u751f\u6210\u79e9\u6700\u591a\u4e3ar\u226an\u7684\u6ce8\u610f\u529b\u77e9\u9635\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(nr)\u3002", "result": "\u5728Long Range Arena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLOTFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u7ebf\u6027\u548c\u57fa\u4e8e\u4f20\u8f93\u7684\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "LOTFormer\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u53cc\u91cd\u968f\u673a\u6027\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6ce8\u610f\u529b\u8d28\u91cf\uff0c\u4e3a\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23437", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23437", "abs": "https://arxiv.org/abs/2509.23437", "authors": ["Steve Hong", "Runa Eschenhagen", "Bruno Mlodozeniec", "Richard Turner"], "title": "Better Hessians Matter: Studying the Impact of Curvature Approximations in Influence Functions", "comment": null, "summary": "Influence functions offer a principled way to trace model predictions back to\ntraining data, but their use in deep learning is hampered by the need to invert\na large, ill-conditioned Hessian matrix. Approximations such as Generalised\nGauss-Newton (GGN) and Kronecker-Factored Approximate Curvature (K-FAC) have\nbeen proposed to make influence computation tractable, yet it remains unclear\nhow the departure from exactness impacts data attribution performance.\nCritically, given the restricted regime in which influence functions are\nderived, it is not necessarily clear better Hessian approximations should even\nlead to better data attribution performance. In this paper, we investigate the\neffect of Hessian approximation quality on influence-function attributions in a\ncontrolled classification setting. Our experiments show that better Hessian\napproximations consistently yield better influence score quality, offering\njustification for recent research efforts towards that end. We further\ndecompose the approximation steps for recent Hessian approximation methods and\nevaluate each step's influence on attribution accuracy. Notably, the mismatch\nbetween K-FAC eigenvalues and GGN/EK-FAC eigenvalues accounts for the majority\nof the error and influence loss. These findings highlight which approximations\nare most critical, guiding future efforts to balance computational tractability\nand attribution accuracy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Hessian\u77e9\u9635\u8fd1\u4f3c\u8d28\u91cf\u5bf9\u5f71\u54cd\u51fd\u6570\u6570\u636e\u5f52\u56e0\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u66f4\u597d\u7684Hessian\u8fd1\u4f3c\u786e\u5b9e\u80fd\u5e26\u6765\u66f4\u597d\u7684\u5f71\u54cd\u5206\u6570\u8d28\u91cf\u3002", "motivation": "\u5f71\u54cd\u51fd\u6570\u867d\u7136\u80fd\u8ffd\u6eaf\u6a21\u578b\u9884\u6d4b\u5230\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u5927\u5c3a\u5ea6\u3001\u75c5\u6001Hessian\u77e9\u9635\u6c42\u9006\u7684\u963b\u788d\u3002\u73b0\u6709\u8fd1\u4f3c\u65b9\u6cd5\u5982GGN\u548cK-FAC\u7684\u7cbe\u786e\u5ea6\u5bf9\u6570\u636e\u5f52\u56e0\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728\u53d7\u63a7\u5206\u7c7b\u8bbe\u7f6e\u4e0b\uff0c\u7814\u7a76Hessian\u8fd1\u4f3c\u8d28\u91cf\u5bf9\u5f71\u54cd\u51fd\u6570\u5f52\u56e0\u7684\u5f71\u54cd\uff0c\u5206\u89e3\u6700\u8fd1Hessian\u8fd1\u4f3c\u65b9\u6cd5\u7684\u5404\u4e2a\u6b65\u9aa4\u5e76\u8bc4\u4f30\u6bcf\u4e2a\u6b65\u9aa4\u5bf9\u5f52\u56e0\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u66f4\u597d\u7684Hessian\u8fd1\u4f3c\u4e00\u81f4\u5730\u4ea7\u751f\u66f4\u597d\u7684\u5f71\u54cd\u5206\u6570\u8d28\u91cf\u3002K-FAC\u7279\u5f81\u503c\u4e0eGGN/EK-FAC\u7279\u5f81\u503c\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u662f\u4e3b\u8981\u8bef\u5dee\u6765\u6e90\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6539\u8fdbHessian\u8fd1\u4f3c\u7684\u7814\u7a76\u52aa\u529b\u63d0\u4f9b\u4e86\u5408\u7406\u6027\uff0c\u5e76\u6307\u660e\u4e86\u54ea\u4e9b\u8fd1\u4f3c\u6b65\u9aa4\u5bf9\u5f52\u56e0\u51c6\u786e\u6027\u6700\u4e3a\u5173\u952e\uff0c\u6709\u52a9\u4e8e\u5728\u8ba1\u7b97\u53ef\u884c\u6027\u548c\u5f52\u56e0\u51c6\u786e\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002"}}
{"id": "2509.23443", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23443", "abs": "https://arxiv.org/abs/2509.23443", "authors": ["Wenhao Yang", "Lin Li", "Xiaohui Tao", "Kaize Shi"], "title": "Factor Decorrelation Enhanced Data Removal from Deep Predictive Models", "comment": "accepted by NeurIPS 2025", "summary": "The imperative of user privacy protection and regulatory compliance\nnecessitates sensitive data removal in model training, yet this process often\ninduces distributional shifts that undermine model performance-particularly in\nout-of-distribution (OOD) scenarios. We propose a novel data removal approach\nthat enhances deep predictive models through factor decorrelation and loss\nperturbation. Our approach introduces: (1) a discriminative-preserving factor\ndecorrelation module employing dynamic adaptive weight adjustment and iterative\nrepresentation updating to reduce feature redundancy and minimize inter-feature\ncorrelations. (2) a smoothed data removal mechanism with loss perturbation that\ncreates information-theoretic safeguards against data leakage during removal\noperations. Extensive experiments on five benchmark datasets show that our\napproach outperforms other baselines and consistently achieves high predictive\naccuracy and robustness even under significant distribution shifts. The results\nhighlight its superior efficiency and adaptability in both in-distribution and\nout-of-distribution scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u79fb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u5b50\u53bb\u76f8\u5173\u548c\u635f\u5931\u6270\u52a8\u6765\u589e\u5f3a\u6df1\u5ea6\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u548c\u6cd5\u89c4\u9075\u4ece\u8981\u6c42\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u79fb\u9664\u654f\u611f\u6570\u636e\uff0c\u4f46\u8fd9\u4e00\u8fc7\u7a0b\u5f80\u5f80\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5224\u522b\u4fdd\u6301\u56e0\u5b50\u53bb\u76f8\u5173\u6a21\u5757\uff08\u52a8\u6001\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u548c\u8fed\u4ee3\u8868\u793a\u66f4\u65b0\uff09\u548c\u5e73\u6ed1\u6570\u636e\u79fb\u9664\u673a\u5236\uff08\u635f\u5931\u6270\u52a8\uff09\uff0c\u51cf\u5c11\u7279\u5f81\u5197\u4f59\u5e76\u9632\u6b62\u6570\u636e\u6cc4\u9732\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u663e\u8457\u5206\u5e03\u504f\u79fb\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23453", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.23453", "abs": "https://arxiv.org/abs/2509.23453", "authors": ["Dawei Gao", "Dali Wang", "Zhuowei Gu", "Qinglei Cao", "Xiao Wang", "Peter Thornton", "Dan Ricciuto", "Yunhe Feng"], "title": "PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations", "comment": "19 pages, 13 figures", "summary": "Large-scale numerical simulations underpin modern scientific discovery but\nremain constrained by prohibitive computational costs. AI surrogates offer\nacceleration, yet adoption in mission-critical settings is limited by concerns\nover physical plausibility, trustworthiness, and the fusion of heterogeneous\ndata. We introduce PHASE, a modular deep-learning framework for\nphysics-integrated, heterogeneity-aware surrogates in scientific simulations.\nPHASE combines data-type-aware encoders for heterogeneous inputs with\nmulti-level physics-based constraints that promote consistency from local\ndynamics to global system behavior. We validate PHASE on the biogeochemical\n(BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth\nSystem Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first\nscientifically validated AI-accelerated solution for this task. Using only the\nfirst 20 simulation years, PHASE infers a near-equilibrium state that otherwise\nrequires more than 1,200 years of integration, yielding an effective reduction\nin required integration length by at least 60x. The framework is enabled by a\npipeline for fusing heterogeneous scientific data and demonstrates strong\ngeneralization to higher spatial resolutions with minimal fine-tuning. These\nresults indicate that PHASE captures governing physical regularities rather\nthan surface correlations, enabling practical, physically consistent\nacceleration of land-surface modeling and other complex scientific workflows.", "AI": {"tldr": "PHASE\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u96c6\u6210\u548c\u5f02\u6784\u6570\u636e\u878d\u5408\uff0c\u5c06\u5730\u7403\u7cfb\u7edf\u6a21\u578b\u4e2d\u7684\u751f\u7269\u5730\u7403\u5316\u5b66\u81ea\u65cb\u8fc7\u7a0b\u4ece1200\u5e74\u52a0\u901f\u523020\u5e74\uff0c\u5b9e\u73b060\u500d\u52a0\u901f\u3002", "motivation": "\u5927\u89c4\u6a21\u6570\u503c\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709AI\u66ff\u4ee3\u6a21\u578b\u5728\u5173\u952e\u4efb\u52a1\u5e94\u7528\u4e2d\u5b58\u5728\u7269\u7406\u5408\u7406\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u5f02\u6784\u6570\u636e\u878d\u5408\u7684\u5c40\u9650\u6027\u3002", "method": "PHASE\u6846\u67b6\u7ed3\u5408\u4e86\u5f02\u6784\u8f93\u5165\u7684\u6570\u636e\u7c7b\u578b\u611f\u77e5\u7f16\u7801\u5668\u548c\u591a\u7ea7\u7269\u7406\u7ea6\u675f\uff0c\u4ece\u5c40\u90e8\u52a8\u529b\u5b66\u5230\u5168\u5c40\u7cfb\u7edf\u884c\u4e3a\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u5728E3SM\u9646\u5730\u6a21\u578b\u4e2d\uff0c\u4ec5\u4f7f\u7528\u524d20\u5e74\u6a21\u62df\u6570\u636e\u5c31\u80fd\u63a8\u65ad\u51fa\u539f\u672c\u9700\u89811200\u5e74\u624d\u80fd\u8fbe\u5230\u7684\u8fd1\u5e73\u8861\u72b6\u6001\uff0c\u6709\u6548\u51cf\u5c11\u79ef\u5206\u957f\u5ea6\u81f3\u5c1160\u500d\u3002", "conclusion": "PHASE\u80fd\u591f\u6355\u6349\u63a7\u5236\u7269\u7406\u89c4\u5f8b\u800c\u975e\u8868\u9762\u76f8\u5173\u6027\uff0c\u4e3a\u9646\u5730\u8868\u9762\u5efa\u6a21\u548c\u5176\u4ed6\u590d\u6742\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u5b9e\u7528\u3001\u7269\u7406\u4e00\u81f4\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23461", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23461", "abs": "https://arxiv.org/abs/2509.23461", "authors": ["Ziheng Cheng", "Zhong Li", "Jiang Bian"], "title": "Data-Efficient Training by Evolved Sampling", "comment": null, "summary": "Data selection is designed to accelerate learning with preserved performance.\nTo achieve this, a fundamental thought is to identify informative data samples\nwith significant contributions to the training. In this work, we propose\n\\textbf{Evolved Sampling} (\\textbf{ES}), a simple yet effective framework for\n\\emph{dynamic} sampling along the training process. This method conducts \\em\nbatch \\em level data selection based on the dynamics of losses and augmented\n\\emph{loss differences}, which enables flexible \\emph{frequency tuning}, and\nhence significantly reduces the back propagation time with maintained model\nperformance. Due to its conciseness, ES is also readily extensible to\nincorporate \\em set \\em level data selection (to form ES with pruning,\n\\textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP)\nconsistently achieves lossless training accelerations across various\npre-training and post-training tasks, saving up to nearly 45\\% wall-clock time.\nOur results motivate further investigations on the data efficiency aspect of\nmodern large-scale machine learning.", "AI": {"tldr": "\u63d0\u51faEvolved Sampling (ES)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u91c7\u6837\u548c\u635f\u5931\u5dee\u5f02\u5206\u6790\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u6570\u636e\u9009\u62e9\u65e8\u5728\u52a0\u901f\u5b66\u4e60\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u8bc6\u522b\u5bf9\u8bad\u7ec3\u6709\u663e\u8457\u8d21\u732e\u7684\u4fe1\u606f\u6570\u636e\u6837\u672c\u3002", "method": "\u57fa\u4e8e\u635f\u5931\u52a8\u6001\u548c\u589e\u5f3a\u635f\u5931\u5dee\u5f02\u8fdb\u884c\u6279\u7ea7\u6570\u636e\u9009\u62e9\uff0c\u652f\u6301\u7075\u6d3b\u7684\u9891\u7387\u8c03\u6574\uff0c\u5e76\u53ef\u6269\u5c55\u4e3a\u7ed3\u5408\u96c6\u5408\u7ea7\u6570\u636e\u9009\u62e9\u7684ESWP\u65b9\u6cd5\u3002", "result": "\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0cES(WP)\u5728\u5404\u79cd\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u65e0\u635f\u8bad\u7ec3\u52a0\u901f\uff0c\u8282\u7701\u9ad8\u8fbe\u8fd145%\u7684\u6302\u949f\u65f6\u95f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6fc0\u52b1\u5bf9\u73b0\u4ee3\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u6570\u636e\u6548\u7387\u65b9\u9762\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2509.23462", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23462", "abs": "https://arxiv.org/abs/2509.23462", "authors": ["Alakh Sharma", "Gaurish Trivedi", "Kartikey Bhandari", "Yash Sinha", "Dhruv Kumar", "Pratik Narang", "Jagat Sesh Challa"], "title": "Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning", "comment": "Under review", "summary": "Scalable multi-agent reinforcement learning (MARL) remains a central\nchallenge for AI. Existing population-based methods, like Policy-Space Response\nOracles, PSRO, require storing explicit policy populations and constructing\nfull payoff matrices, incurring quadratic computation and linear memory costs.\nWe present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free\nframework that replaces explicit populations with a compact set of latent\nanchors and a single amortized generator. Instead of exhaustively constructing\nthe payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,\nmultiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB\noracle to adaptively expand the policy set. Best responses are trained within\nthe generator using an advantage-based trust-region objective, eliminating the\nneed to store and train separate actors. We evaluated GEMS in a variety of\nTwo-player and Multi-Player games such as the Deceptive Messages Game, Kuhn\nPoker and Multi-Particle environment. We find that GEMS is up to ~6x faster,\nhas 1.3x less memory usage than PSRO, while also reaps higher rewards\nsimultaneously. These results demonstrate that GEMS retains the game theoretic\nguarantees of PSRO, while overcoming its fundamental inefficiencies, hence\nenabling scalable multi-agent learning in multiple domains.", "AI": {"tldr": "GEMS\u662f\u4e00\u4e2a\u66ff\u4ee3PSRO\u7684\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6f5c\u5728\u951a\u70b9\u548c\u5355\u4e2a\u751f\u6210\u5668\u53d6\u4ee3\u663e\u5f0f\u7b56\u7565\u79cd\u7fa4\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u63a8\u6f14\u548c\u5143\u52a8\u6001\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u79cd\u7fa4\u7684MARL\u65b9\u6cd5\uff08\u5982PSRO\uff09\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u4e0a\u7684\u4e8c\u6b21\u65b9\u548c\u7ebf\u6027\u6210\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u7684\u6f5c\u5728\u951a\u70b9\u96c6\u548c\u5355\u4e2a\u644a\u9500\u751f\u6210\u5668\u66ff\u4ee3\u663e\u5f0f\u7b56\u7565\u79cd\u7fa4\uff1b\u91c7\u7528\u65e0\u504f\u8499\u7279\u5361\u6d1b\u63a8\u6f14\u3001\u4e58\u6cd5\u6743\u91cd\u5143\u52a8\u6001\u548c\u65e0\u6a21\u578b\u7ecf\u9a8c\u4f2f\u6069\u65af\u5766UCB\u9884\u8a00\u673a\u81ea\u9002\u5e94\u6269\u5c55\u7b56\u7565\u96c6\uff1b\u5728\u751f\u6210\u5668\u5185\u4f7f\u7528\u57fa\u4e8e\u4f18\u52bf\u7684\u4fe1\u4efb\u57df\u76ee\u6807\u8bad\u7ec3\u6700\u4f73\u54cd\u5e94\u3002", "result": "\u5728\u53cc\u4eba\u548c\u591a\u4eba\u6e38\u620f\u4e2d\uff0cGEMS\u6bd4PSRO\u5feb\u7ea66\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c111.3\u500d\uff0c\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u3002", "conclusion": "GEMS\u5728\u4fdd\u6301PSRO\u535a\u5f08\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u514b\u670d\u4e86\u5176\u57fa\u672c\u4f4e\u6548\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u4e2a\u9886\u57df\u7684\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u3002"}}
{"id": "2509.23470", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23470", "abs": "https://arxiv.org/abs/2509.23470", "authors": ["Rui Ai", "Hugo De Oliveira Barbalho", "Sirui Li", "Alexei Robsky", "David Simchi-Levi", "Ishai Menache"], "title": "Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving", "comment": null, "summary": "A common challenge in real-time operations is deciding whether to re-solve an\noptimization problem or continue using an existing solution. While modern data\nplatforms may collect information at high frequencies, many real-time\noperations require repeatedly solving computationally intensive optimization\nproblems formulated as Mixed-Integer Linear Programs (MILPs). Determining when\nto re-solve is, therefore, an economically important question. This problem\nposes several challenges: 1) How to characterize solution optimality and\nsolving cost; 2) How to detect environmental changes and select beneficial\nsamples for solving the MILP; 3) Given the large time horizon and non-MDP\nstructure, vanilla reinforcement learning (RL) methods are not directly\napplicable and tend to suffer from value function explosion. Existing\nliterature largely focuses on heuristics, low-data settings, and smooth\nobjectives, with little focus on common NP-hard MILPs. We propose a framework\ncalled Proximal Policy Optimization with Change Point Detection (POC), which\nsystematically offers a solution for balancing performance and cost when\ndeciding appropriate re-solving times. Theoretically, we establish the\nrelationship between the number of re-solves and the re-solving cost. To test\nour framework, we assemble eight synthetic and real-world datasets, and show\nthat POC consistently outperforms existing baselines by 2%-17%. As a side\nbenefit, our work fills the gap in the literature by introducing real-time MILP\nbenchmarks and evaluation criteria.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPOC\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b9e\u65f6\u64cd\u4f5c\u4e2d\u51b3\u5b9a\u4f55\u65f6\u91cd\u65b0\u6c42\u89e3MILP\u4f18\u5316\u95ee\u9898\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5b9e\u65f6\u64cd\u4f5c\u4e2d\u9700\u8981\u53cd\u590d\u6c42\u89e3\u8ba1\u7b97\u5bc6\u96c6\u7684MILP\u95ee\u9898\uff0c\u4f46\u4f55\u65f6\u91cd\u65b0\u6c42\u89e3\u662f\u4e00\u4e2a\u7ecf\u6d4e\u4e0a\u91cd\u8981\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u542f\u53d1\u5f0f\u3001\u4f4e\u6570\u636e\u8bbe\u7f6e\u548c\u5e73\u6ed1\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86POC\u6846\u67b6\uff0c\u7ed3\u5408\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u548c\u53d8\u70b9\u68c0\u6d4b\uff0c\u7cfb\u7edf\u6027\u5730\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\uff0c\u5e76\u5efa\u7acb\u4e86\u91cd\u65b0\u6c42\u89e3\u6b21\u6570\u4e0e\u6210\u672c\u7684\u7406\u8bba\u5173\u7cfb\u3002", "result": "\u57288\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cPOC\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd52%-17%\u3002", "conclusion": "POC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MILP\u91cd\u65b0\u6c42\u89e3\u65f6\u673a\u95ee\u9898\uff0c\u540c\u65f6\u586b\u8865\u4e86\u5b9e\u65f6MILP\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6807\u51c6\u7684\u6587\u732e\u7a7a\u767d\u3002"}}
{"id": "2509.23471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23471", "abs": "https://arxiv.org/abs/2509.23471", "authors": ["Harshil Vejendla"], "title": "Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases", "comment": "EMNLP 2025 Main 12 pages, 6 figures", "summary": "Upgrading embedding models in production vector databases typically requires\nre-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor\n(ANN) index, leading to significant operational disruption and computational\ncost. This paper presents Drift-Adapter, a lightweight, learnable\ntransformation layer designed to bridge embedding spaces between model\nversions. By mapping new queries into the legacy embedding space, Drift-Adapter\nenables the continued use of the existing ANN index, effectively deferring full\nre-computation. We systematically evaluate three adapter parameterizations:\nOrthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on\na small sample of paired old and new embeddings. Experiments on MTEB text\ncorpora and a CLIP image model upgrade (1M items) show that Drift-Adapter\nrecovers 95-99% of the retrieval recall (Recall@10, MRR) of a full\nre-embedding, adding less than 10 microseconds of query latency. Compared to\noperational strategies like full re-indexing or dual-index serving,\nDrift-Adapter reduces recompute costs by over 100 times and facilitates\nupgrades with near-zero operational interruption. We analyze robustness to\nvaried model drift, training data size, scalability to billion-item systems,\nand the impact of design choices like diagonal scaling, demonstrating\nDrift-Adapter's viability as a pragmatic solution for agile model deployment.", "AI": {"tldr": "Drift-Adapter\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u8f6c\u6362\u5c42\uff0c\u901a\u8fc7\u5728\u65e7\u7248\u548c\u65b0\u7248\u5d4c\u5165\u7a7a\u95f4\u4e4b\u95f4\u5efa\u7acb\u6620\u5c04\uff0c\u907f\u514d\u5347\u7ea7\u5d4c\u5165\u6a21\u578b\u65f6\u91cd\u65b0\u7f16\u7801\u6574\u4e2a\u8bed\u6599\u5e93\u548c\u91cd\u5efaANN\u7d22\u5f15\uff0c\u663e\u8457\u964d\u4f4e\u64cd\u4f5c\u4e2d\u65ad\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u751f\u4ea7\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u5347\u7ea7\u5d4c\u5165\u6a21\u578b\u901a\u5e38\u9700\u8981\u91cd\u65b0\u7f16\u7801\u6574\u4e2a\u8bed\u6599\u5e93\u5e76\u91cd\u5efa\u8fd1\u4f3c\u6700\u8fd1\u90bb\u7d22\u5f15\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u64cd\u4f5c\u4e2d\u65ad\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u9002\u914d\u5668\u53c2\u6570\u5316\u65b9\u6cd5\uff1a\u6b63\u4ea4Procrustes\u3001\u4f4e\u79e9\u4eff\u5c04\u548c\u7d27\u51d1\u6b8b\u5deeMLP\uff0c\u901a\u8fc7\u5728\u5c11\u91cf\u6210\u5bf9\u65b0\u65e7\u5d4c\u5165\u6837\u672c\u4e0a\u8bad\u7ec3\uff0c\u5c06\u65b0\u67e5\u8be2\u6620\u5c04\u5230\u65e7\u7248\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728MTEB\u6587\u672c\u8bed\u6599\u548cCLIP\u56fe\u50cf\u6a21\u578b\u5347\u7ea7\u5b9e\u9a8c\u4e2d\uff0cDrift-Adapter\u6062\u590d\u4e8695-99%\u7684\u68c0\u7d22\u53ec\u56de\u7387\uff0c\u67e5\u8be2\u5ef6\u8fdf\u4ec5\u589e\u52a0\u4e0d\u523010\u5fae\u79d2\uff0c\u76f8\u6bd4\u5b8c\u5168\u91cd\u65b0\u7d22\u5f15\u51cf\u5c11100\u500d\u4ee5\u4e0a\u7684\u91cd\u65b0\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "Drift-Adapter\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4ee5\u63a5\u8fd1\u96f6\u64cd\u4f5c\u4e2d\u65ad\u7684\u65b9\u5f0f\u4fc3\u8fdb\u654f\u6377\u6a21\u578b\u90e8\u7f72\uff0c\u5728\u6a21\u578b\u6f02\u79fb\u3001\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.23472", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23472", "abs": "https://arxiv.org/abs/2509.23472", "authors": ["Jiang-Xin Shi", "Wen-Da Wei", "Jin-Fei Qi", "Xuanyu Chen", "Tong Wei", "Yu-Feng Li"], "title": "Memory-Efficient Fine-Tuning via Low-Rank Activation Compression", "comment": null, "summary": "The parameter-efficient fine-tuning paradigm has garnered significant\nattention with the advancement of foundation models. Although numerous methods\nhave been proposed to reduce the number of trainable parameters, their\nsubstantial memory overhead remains a critical bottleneck that hinders\npractical deployment. In this paper, we observe that model activations\nconstitute a major source of memory consumption, especially under large batch\nsizes and long context lengths; however, the rank of the activations remains\nconsistently low. Motivated by this insight, we propose a memory-efficient\nfine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior\nwork, LoRAct provides a more flexible and versatile compressing strategy that\ncan be applied online during the forward pass without the need for any\ncalibration data. Moreover, LoRAct incorporates a novel sampling-based\northogonal decomposition algorithm specifically designed for low-rank matrices,\noffering improved computational efficiency and a tighter error bound compared\nto the widely used RSVD. Experiments on both vision and language tasks\ndemonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces\nactivation memory by approximately 80% in comparison with the widely adopted\nLoRA method, while maintaining competitive performance. The source code is\navailable at https://github.com/shijxcs/meft.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoRAct\u7684\u5185\u5b58\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u6fc0\u6d3b\u538b\u7f29\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u6bd4LoRA\u65b9\u6cd5\u51cf\u5c11\u7ea680%\u7684\u6fc0\u6d3b\u5185\u5b58\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u8303\u5f0f\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u867d\u7136\u5df2\u6709\u8bb8\u591a\u65b9\u6cd5\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5185\u5b58\u5f00\u9500\u4ecd\u7136\u662f\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u74f6\u9888\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u6a21\u578b\u6fc0\u6d3b\u662f\u5185\u5b58\u6d88\u8017\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u7279\u522b\u662f\u5728\u5927\u6279\u91cf\u548c\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0c\u4f46\u6fc0\u6d3b\u7684\u79e9\u59cb\u7ec8\u4fdd\u6301\u8f83\u4f4e\u3002", "method": "\u63d0\u51faLoRAct\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u901a\u7528\u7684\u538b\u7f29\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u5728\u7ebf\u5e94\u7528\uff0c\u65e0\u9700\u4efb\u4f55\u6821\u51c6\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u79cd\u4e13\u95e8\u4e3a\u4f4e\u79e9\u77e9\u9635\u8bbe\u8ba1\u7684\u65b0\u578b\u57fa\u4e8e\u91c7\u6837\u7684\u6b63\u4ea4\u5206\u89e3\u7b97\u6cd5\uff0c\u76f8\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684RSVD\u5177\u6709\u66f4\u597d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u66f4\u7d27\u7684\u8bef\u5dee\u754c\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86LoRAct\u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e0e\u5e7f\u6cdb\u91c7\u7528\u7684LoRA\u65b9\u6cd5\u76f8\u6bd4\uff0cLoRAct\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u7ea680%\u7684\u6fc0\u6d3b\u5185\u5b58\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "LoRAct\u662f\u4e00\u79cd\u6709\u6548\u7684\u5185\u5b58\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u6fc0\u6d3b\u538b\u7f29\u89e3\u51b3\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23474", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.23474", "abs": "https://arxiv.org/abs/2509.23474", "authors": ["Yahong Yang", "Wei Zhu"], "title": "Statistical Learning Guarantees for Group-Invariant Barron Functions", "comment": null, "summary": "We investigate the generalization error of group-invariant neural networks\nwithin the Barron framework. Our analysis shows that incorporating\ngroup-invariant structures introduces a group-dependent factor\n$\\delta_{G,\\Gamma,\\sigma} \\le 1$ into the approximation rate. When this factor\nis small, group invariance yields substantial improvements in approximation\naccuracy. On the estimation side, we establish that the Rademacher complexity\nof the group-invariant class is no larger than that of the non-invariant\ncounterpart, implying that the estimation error remains unaffected by the\nincorporation of symmetry. Consequently, the generalization error can improve\nsignificantly when learning functions with inherent group symmetries. We\nfurther provide illustrative examples demonstrating both favorable cases, where\n$\\delta_{G,\\Gamma,\\sigma}\\approx |G|^{-1}$, and unfavorable ones, where\n$\\delta_{G,\\Gamma,\\sigma}\\approx 1$. Overall, our results offer a rigorous\ntheoretical foundation showing that encoding group-invariant structures in\nneural networks leads to clear statistical advantages for symmetric target\nfunctions.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7fa4\u4e0d\u53d8\u795e\u7ecf\u7f51\u7edc\u5728Barron\u6846\u67b6\u4e0b\u7684\u6cdb\u5316\u8bef\u5dee\uff0c\u8bc1\u660e\u4e86\u7fa4\u4e0d\u53d8\u7ed3\u6784\u80fd\u663e\u8457\u63d0\u5347\u5bf9\u79f0\u76ee\u6807\u51fd\u6570\u7684\u903c\u8fd1\u7cbe\u5ea6\uff0c\u4e14\u4e0d\u5f71\u54cd\u4f30\u8ba1\u8bef\u5dee\uff0c\u4ece\u800c\u5e26\u6765\u7edf\u8ba1\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u7fa4\u4e0d\u53d8\u795e\u7ecf\u7f51\u7edc\u5728\u5bf9\u79f0\u51fd\u6570\u5b66\u4e60\u4e2d\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u4e3a\u7f16\u7801\u5bf9\u79f0\u7ed3\u6784\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5728Barron\u6846\u67b6\u4e0b\u5206\u6790\u7fa4\u4e0d\u53d8\u795e\u7ecf\u7f51\u7edc\u7684\u903c\u8fd1\u8bef\u5dee\u548cRademacher\u590d\u6742\u5ea6\uff0c\u5f15\u5165\u7fa4\u4f9d\u8d56\u56e0\u5b50\u03b4\u6765\u91cf\u5316\u5bf9\u79f0\u6027\u5e26\u6765\u7684\u6539\u8fdb\u3002", "result": "\u7fa4\u4e0d\u53d8\u7ed3\u6784\u5f15\u5165\u03b4\u22641\u56e0\u5b50\uff0c\u5f53\u03b4\u8f83\u5c0f\u65f6\u80fd\u663e\u8457\u63d0\u5347\u903c\u8fd1\u7cbe\u5ea6\uff1bRademacher\u590d\u6742\u5ea6\u4e0d\u9ad8\u4e8e\u975e\u4e0d\u53d8\u7f51\u7edc\uff0c\u4f30\u8ba1\u8bef\u5dee\u4e0d\u53d7\u5f71\u54cd\uff1b\u6cdb\u5316\u8bef\u5dee\u5728\u5bf9\u79f0\u51fd\u6570\u5b66\u4e60\u4e2d\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u7f16\u7801\u7fa4\u4e0d\u53d8\u7ed3\u6784\u80fd\u4e3a\u5bf9\u79f0\u76ee\u6807\u51fd\u6570\u5e26\u6765\u660e\u786e\u7684\u7edf\u8ba1\u4f18\u52bf\uff0c\u7406\u8bba\u5206\u6790\u4e3a\u5bf9\u79f0\u6027\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2509.23487", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23487", "abs": "https://arxiv.org/abs/2509.23487", "authors": ["Divyam Madaan", "Sumit Chopra", "Kyunghyun Cho"], "title": "Temporal Generalization: A Reality Check", "comment": null, "summary": "Machine learning (ML) models often struggle to maintain performance under\ndistribution shifts, leading to inaccurate predictions on unseen future data.\nIn this work, we investigate whether and under what conditions models can\nachieve such a generalization when relying solely on past data. We explore two\nprimary approaches: convex combinations of past model parameters\n(\\emph{parameter interpolation}) and explicit extrapolation beyond the convex\nhull of past parameters (\\emph{parameter extrapolation}). We benchmark several\nmethods within these categories on a diverse set of temporal tasks, including\nlanguage modeling, news summarization, news tag prediction, academic paper\ncategorization, satellite image-based land use classification over time, and\nhistorical yearbook photo gender prediction. Our empirical findings show that\nnone of the evaluated methods consistently outperforms the simple baseline of\nusing the latest available model parameters in all scenarios. In the absence of\naccess to future data or robust assumptions about the underlying\ndata-generating process, these results underscore the inherent difficulties of\ngeneralizing and extrapolating to future data and warrant caution when\nevaluating claims of such generalization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4ec5\u4f7f\u7528\u8fc7\u53bb\u6570\u636e\u5b9e\u73b0\u6a21\u578b\u6cdb\u5316\u7684\u4e24\u79cd\u65b9\u6cd5\uff1a\u53c2\u6570\u63d2\u503c\u548c\u53c2\u6570\u5916\u63a8\u3002\u5728\u591a\u4e2a\u65f6\u5e8f\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6ca1\u6709\u65b9\u6cd5\u80fd\u59cb\u7ec8\u4f18\u4e8e\u7b80\u5355\u4f7f\u7528\u6700\u65b0\u6a21\u578b\u53c2\u6570\u7684\u57fa\u7ebf\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u7814\u7a76\u662f\u5426\u4ec5\u4f9d\u9760\u8fc7\u53bb\u6570\u636e\u5c31\u80fd\u5b9e\u73b0\u5bf9\u672a\u6765\u6570\u636e\u7684\u6cdb\u5316\u3002", "method": "\u63a2\u7d22\u4e24\u79cd\u65b9\u6cd5\uff1a\u53c2\u6570\u63d2\u503c\uff08\u8fc7\u53bb\u6a21\u578b\u53c2\u6570\u7684\u51f8\u7ec4\u5408\uff09\u548c\u53c2\u6570\u5916\u63a8\uff08\u8d85\u51fa\u8fc7\u53bb\u53c2\u6570\u51f8\u5305\u7684\u5916\u63a8\uff09\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u65b0\u95fb\u6458\u8981\u3001\u56fe\u50cf\u5206\u7c7b\u7b49\u65f6\u5e8f\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u65b9\u6cd5\u90fd\u65e0\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e2d\u4e00\u81f4\u4f18\u4e8e\u7b80\u5355\u4f7f\u7528\u6700\u65b0\u6a21\u578b\u53c2\u6570\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5728\u6ca1\u6709\u672a\u6765\u6570\u636e\u6216\u5bf9\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u5f3a\u5047\u8bbe\u65f6\uff0c\u6cdb\u5316\u548c\u5916\u63a8\u5230\u672a\u6765\u6570\u636e\u5b58\u5728\u56fa\u6709\u56f0\u96be\uff0c\u9700\u8981\u8c28\u614e\u8bc4\u4f30\u6b64\u7c7b\u6cdb\u5316\u58f0\u660e\u3002"}}
{"id": "2509.23494", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23494", "abs": "https://arxiv.org/abs/2509.23494", "authors": ["Jie Yang", "Yifan Hu", "Kexin Zhang", "Luyang Niu", "Yushun Dong", "Philip S. Yu", "Kaize Ding"], "title": "Revisiting Multivariate Time Series Forecasting with Missing Values", "comment": null, "summary": "Missing values are common in real-world time series, and multivariate time\nseries forecasting with missing values (MTSF-M) has become a crucial area of\nresearch for ensuring reliable predictions. To address the challenge of missing\ndata, current approaches have developed an imputation-then-prediction framework\nthat uses imputation modules to fill in missing values, followed by forecasting\non the imputed data. However, this framework overlooks a critical issue: there\nis no ground truth for the missing values, making the imputation process\nsusceptible to errors that can degrade prediction accuracy. In this paper, we\nconduct a systematic empirical study and reveal that imputation without direct\nsupervision can corrupt the underlying data distribution and actively degrade\nprediction accuracy. To address this, we propose a paradigm shift that moves\naway from imputation and directly predicts from the partially observed time\nseries. We introduce Consistency-Regularized Information Bottleneck (CRIB), a\nnovel framework built on the Information Bottleneck principle. CRIB combines a\nunified-variate attention mechanism with a consistency regularization scheme to\nlearn robust representations that filter out noise introduced by missing values\nwhile preserving essential predictive signals. Comprehensive experiments on\nfour real-world datasets demonstrate the effectiveness of CRIB, which predicts\naccurately even under high missing rates. Our code is available in\nhttps://github.com/Muyiiiii/CRIB.", "AI": {"tldr": "\u63d0\u51fa\u4e86CRIB\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u7406\u76f4\u63a5\u4ece\u4e0d\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u9884\u6d4b\uff0c\u907f\u514d\u4f20\u7edf\u63d2\u503c-\u9884\u6d4b\u6846\u67b6\u4e2d\u63d2\u503c\u8bef\u5dee\u5bf9\u9884\u6d4b\u7cbe\u5ea6\u7684\u5f71\u54cd", "motivation": "\u4f20\u7edf\u63d2\u503c-\u9884\u6d4b\u6846\u67b6\u5b58\u5728\u6839\u672c\u95ee\u9898\uff1a\u7f3a\u5931\u503c\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e\uff0c\u63d2\u503c\u8fc7\u7a0b\u5bb9\u6613\u4ea7\u751f\u8bef\u5dee\u5e76\u964d\u4f4e\u9884\u6d4b\u7cbe\u5ea6", "method": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u7406\u7684CRIB\u6846\u67b6\uff0c\u7ed3\u5408\u7edf\u4e00\u53d8\u91cf\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u65b9\u6848\uff0c\u5b66\u4e60\u8fc7\u6ee4\u7f3a\u5931\u503c\u566a\u58f0\u7684\u9c81\u68d2\u8868\u793a", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660eCRIB\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u4e5f\u80fd\u51c6\u786e\u9884\u6d4b", "conclusion": "CRIB\u901a\u8fc7\u76f4\u63a5\u4ece\u4e0d\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u907f\u514d\u4e86\u63d2\u503c\u8bef\u5dee\uff0c\u4e3aMTSF-M\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.23500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23500", "abs": "https://arxiv.org/abs/2509.23500", "authors": ["Georgios Vlassis", "Saleh Ashkboos", "Alexandra Volkova", "Torsten Hoefler", "Dan Alistarh"], "title": "Beyond Outliers: A Study of Optimizers Under Quantization", "comment": "20 pages", "summary": "As new optimizers gain traction and model quantization becomes standard for\nefficient deployment, a key question arises: how does the choice of optimizer\naffect model performance in the presence of quantization? Despite progress in\nboth areas, systematic evidence on optimizer-quantization interactions remains\nlimited. To fill this gap, we study the impact of optimizer choice on model\nrobustness under quantization, considering both post-training quantization\n(PTQ), and quantization-aware training (QAT). We first train full-precision\nmodels, ranging from 50M to 1.5B parameters, with six optimizers, to explore\nthe hyperparameter landscape, and establish well-tuned baselines. We then apply\nPTQ to evaluate how model performance degrades when trained with different\noptimizers. We find that outlier-related metrics, such as the max-to-mean ratio\n(MMR) and Kurtosis, fail to predict the PTQ performance across different\noptimizers. We show analytically that this is due to the MMR capturing only\nisolated layer errors, while ignoring how quantization errors accumulate and\npropagate through the network. To study the QAT degradation, we train quantized\nmodels from scratch and compare them to our original-precision baselines. We\nfind that optimizers performing well in the original pretraining setup may not\nremain optimal under QAT, and that models trained with Shampoo show the lowest\naccuracy degradation. Finally, we derive scaling laws for quantization-aware\ntraining under different optimizers, showing that Shampoo achieves the highest\nparameter efficiency of all tested optimizers.", "AI": {"tldr": "\u7814\u7a76\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u91cf\u5316\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0Shampoo\u4f18\u5316\u5668\u5728\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u5177\u6709\u6700\u9ad8\u7684\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u968f\u7740\u65b0\u4f18\u5316\u5668\u7684\u666e\u53ca\u548c\u6a21\u578b\u91cf\u5316\u6210\u4e3a\u9ad8\u6548\u90e8\u7f72\u7684\u6807\u51c6\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4f18\u5316\u5668\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u91cf\u5316\u540e\u7684\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bad\u7ec350M\u52301.5B\u53c2\u6570\u7684\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u4f7f\u7528\u516d\u79cd\u4f18\u5316\u5668\u5efa\u7acb\u57fa\u51c6\uff0c\u7136\u540e\u5e94\u7528\u540e\u8bad\u7ec3\u91cf\u5316\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u8bc4\u4f30\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u53d1\u73b0\u4e0e\u5f02\u5e38\u503c\u76f8\u5173\u7684\u6307\u6807\u65e0\u6cd5\u9884\u6d4b\u4e0d\u540c\u4f18\u5316\u5668\u7684PTQ\u6027\u80fd\uff1b\u5728QAT\u4e2d\uff0cShampoo\u4f18\u5316\u5668\u663e\u793a\u51fa\u6700\u4f4e\u7684\u7cbe\u5ea6\u4e0b\u964d\u548c\u6700\u9ad8\u7684\u53c2\u6570\u6548\u7387\u3002", "conclusion": "\u4f18\u5316\u5668\u9009\u62e9\u663e\u8457\u5f71\u54cd\u91cf\u5316\u6a21\u578b\u6027\u80fd\uff0cShampoo\u5728\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4e3a\u9ad8\u6548\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.23548", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23548", "abs": "https://arxiv.org/abs/2509.23548", "authors": ["Yijie Zhang", "Yiyang Shen", "Weiran Wang"], "title": "Disentanglement of Variations with Multimodal Generative Modeling", "comment": "22 pages, 14 figures, 7 tables", "summary": "Multimodal data are prevalent across various domains, and learning robust\nrepresentations of such data is paramount to enhancing generation quality and\ndownstream task performance. To handle heterogeneity and interconnections among\ndifferent modalities, recent multimodal generative models extract shared and\nprivate (modality-specific) information with two separate variables. Despite\nattempts to enforce disentanglement between these two variables, these methods\nstruggle with challenging datasets where the likelihood model is insufficient.\nIn this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to\nexplicitly address this issue, with rigorous mutual information-based\nregularizations, including cross-view mutual information maximization for\nextracting shared variables, and a cycle-consistency style loss for redundancy\nremoval using generative augmentations. We further introduce diffusion models\nto improve the capacity of latent priors. These newly proposed components are\ncomplementary to each other. Compared to existing approaches, IDMVAE shows a\nclean separation between shared and private information, demonstrating superior\ngeneration quality and semantic coherence on challenging datasets.", "AI": {"tldr": "\u63d0\u51faIDMVAE\u6a21\u578b\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u6b63\u5219\u5316\u548c\u6269\u6563\u6a21\u578b\u6539\u8fdb\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u5b9e\u73b0\u5171\u4eab\u4e0e\u79c1\u6709\u4fe1\u606f\u7684\u6e05\u6670\u5206\u79bb\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u548c\u6a21\u6001\u95f4\u5173\u8054\u65f6\uff0c\u96be\u4ee5\u6709\u6548\u5206\u79bb\u5171\u4eab\u548c\u79c1\u6709\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u3002", "method": "\u4f7f\u7528\u4e92\u4fe1\u606f\u6b63\u5219\u5316\uff08\u8de8\u89c6\u56fe\u4e92\u4fe1\u606f\u6700\u5927\u5316\u548c\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\uff09\u63d0\u53d6\u5171\u4eab\u53d8\u91cf\u5e76\u53bb\u9664\u5197\u4f59\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u589e\u5f3a\u5148\u9a8c\u5206\u5e03\u80fd\u529b\u3002", "result": "IDMVAE\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5171\u4eab\u4e0e\u79c1\u6709\u4fe1\u606f\u7684\u6e05\u6670\u5206\u79bb\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u751f\u6210\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ec4\u4ef6\u76f8\u4e92\u8865\u5145\uff0cIDMVAE\u5728\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4fe1\u606f\u89e3\u8026\u3002"}}
{"id": "2509.23552", "categories": ["cs.LG", "cs.AI", "q-bio.GN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.23552", "abs": "https://arxiv.org/abs/2509.23552", "authors": ["Md. Saiful Bari Siddiqui", "Nowshin Tarannum"], "title": "Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble", "comment": "Submitted to SCA/HPCAsia 2026. This preprint version has been\n  prepared for open-access distribution and may differ in formatting from the\n  official proceedings. Also available on bioRxiv for visibility to the life\n  sciences community", "summary": "Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis.\nWhile genomic sequencing enables rapid prediction of resistance phenotypes,\ncurrent computational methods have limitations. Standard machine learning\nmodels treat the genome as an unordered collection of features, ignoring the\nsequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art\nsequence models like Transformers are often too data-hungry and computationally\nexpensive for the moderately-sized datasets that are typical in this domain. To\naddress these challenges, we propose AMR-EnsembleNet, an ensemble framework\nthat synergistically combines sequence-based and feature-based learning. We\ndeveloped a lightweight, custom 1D Convolutional Neural Network (CNN) to\nefficiently learn predictive sequence motifs from high-dimensional SNP data.\nThis sequence-aware model was ensembled with an XGBoost model, a powerful\ngradient boosting system adept at capturing complex, non-local feature\ninteractions. We trained and evaluated our framework on a benchmark dataset of\n809 E. coli strains, predicting resistance across four antibiotics with varying\nclass imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier\nperformance across all the antibiotics, reaching a Matthews Correlation\nCoefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro\nF1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also\nshow that our model consistently focuses on SNPs within well-known AMR genes\nlike fusA and parC, confirming it learns the correct genetic signals for\nresistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a\nfeature-based XGBoost model creates a powerful ensemble, overcoming the\nlimitations of using either an order-agnostic or a standalone sequence model.", "AI": {"tldr": "\u63d0\u51fa\u4e86AMR-EnsembleNet\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5e8f\u5217\u5b66\u4e60\u548c\u7279\u5f81\u5b66\u4e60\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea71D CNN\u548cXGBoost\u6a21\u578b\u9884\u6d4b\u6297\u751f\u7d20\u8010\u836f\u6027\uff0c\u5728E. coli\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u57fa\u56e0\u7ec4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\u5ffd\u89c6SNP\u987a\u5e8f\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53caTransformer\u7b49\u5e8f\u5217\u6a21\u578b\u5728\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u8f7b\u91cf\u7ea71D CNN\u4ece\u9ad8\u7ef4SNP\u6570\u636e\u4e2d\u5b66\u4e60\u9884\u6d4b\u5e8f\u5217\u6a21\u5f0f\uff0c\u4e0eXGBoost\u6a21\u578b\u96c6\u6210\uff0c\u7ed3\u5408\u5e8f\u5217\u611f\u77e5\u548c\u7279\u5f81\u4ea4\u4e92\u80fd\u529b\u3002", "result": "\u5728809\u682aE. coli\u83cc\u682a\u6570\u636e\u96c6\u4e0a\uff0c\u5bf94\u79cd\u6297\u751f\u7d20\u9884\u6d4b\u5747\u8868\u73b0\u4f18\u5f02\uff0cCIP\u7684MCC\u8fbe0.926\uff0cGEN\u7684Macro F1-score\u8fbe0.691\uff0c\u6a21\u578b\u80fd\u6b63\u786e\u805a\u7126\u4e8e\u5df2\u77e5\u8010\u836f\u57fa\u56e0\u3002", "conclusion": "\u878d\u5408\u5e8f\u5217\u611f\u77e51D CNN\u548c\u7279\u5f81\u57faXGBoost\u7684\u96c6\u6210\u65b9\u6cd5\u514b\u670d\u4e86\u987a\u5e8f\u65e0\u5173\u6216\u5355\u72ec\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u521b\u5efa\u4e86\u5f3a\u5927\u7684\u6297\u751f\u7d20\u8010\u836f\u6027\u9884\u6d4b\u6846\u67b6\u3002"}}
{"id": "2509.23570", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23570", "abs": "https://arxiv.org/abs/2509.23570", "authors": ["Ruiqi Lyu", "Alistair Turcan", "Martin Jinye Zhang", "Bryan Wilder"], "title": "Improving constraint-based discovery with robust propagation and reliable LLM priors", "comment": null, "summary": "Learning causal structure from observational data is central to scientific\nmodeling and decision-making. Constraint-based methods aim to recover\nconditional independence (CI) relations in a causal directed acyclic graph\n(DAG). Classical approaches such as PC and subsequent methods orient\nv-structures first and then propagate edge directions from these seeds,\nassuming perfect CI tests and exhaustive search of separating subsets --\nassumptions often violated in practice, leading to cascading errors in the\nfinal graph. Recent work has explored using large language models (LLMs) as\nexperts, prompting sets of nodes for edge directions, and could augment edge\norientation when assumptions are not met. However, such methods implicitly\nassume perfect experts, which is unrealistic for hallucination-prone LLMs. We\npropose MosaCD, a causal discovery method that propagates edges from a\nhigh-confidence set of seeds derived from both CI tests and LLM annotations. To\nfilter hallucinations, we introduce shuffled queries that exploit LLMs'\npositional bias, retaining only high-confidence seeds. We then apply a novel\nconfidence-down propagation strategy that orients the most reliable edges\nfirst, and can be integrated with any skeleton-based discovery method. Across\nmultiple real-world graphs, MosaCD achieves higher accuracy in final graph\nconstruction than existing constraint-based methods, largely due to the\nimproved reliability of initial seeds and robust propagation strategies.", "AI": {"tldr": "MosaCD\u662f\u4e00\u79cd\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u7ed3\u5408CI\u6d4b\u8bd5\u548cLLM\u6807\u6ce8\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u79cd\u5b50\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u4e0b\u964d\u4f20\u64ad\u7b56\u7565\u63d0\u9ad8\u56e0\u679c\u56fe\u6784\u5efa\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u7ea6\u675f\u65b9\u6cd5\u5982PC\u5728CI\u6d4b\u8bd5\u4e0d\u5b8c\u7f8e\u65f6\u4f1a\u4ea7\u751f\u7ea7\u8054\u9519\u8bef\uff0c\u800c\u73b0\u6709LLM\u65b9\u6cd5\u5047\u8bbe\u5b8c\u7f8e\u4e13\u5bb6\uff0c\u4e0d\u9002\u7528\u4e8e\u6613\u4ea7\u751f\u5e7b\u89c9\u7684LLM", "method": "\u4f7f\u7528\u6df7\u6d17\u67e5\u8be2\u8fc7\u6ee4LLM\u5e7b\u89c9\uff0c\u4ece\u9ad8\u7f6e\u4fe1\u5ea6\u79cd\u5b50\u51fa\u53d1\uff0c\u91c7\u7528\u7f6e\u4fe1\u5ea6\u4e0b\u964d\u4f20\u64ad\u7b56\u7565\u5b9a\u5411\u8fb9\uff0c\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8e\u9aa8\u67b6\u7684\u53d1\u73b0\u65b9\u6cd5\u96c6\u6210", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u4e0a\uff0cMosaCD\u5728\u6700\u7ec8\u56fe\u6784\u5efa\u4e2d\u6bd4\u73b0\u6709\u7ea6\u675f\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u521d\u59cb\u79cd\u5b50\u53ef\u9760\u6027\u548c\u9c81\u68d2\u4f20\u64ad\u7b56\u7565\uff0cMosaCD\u663e\u8457\u63d0\u5347\u4e86\u56e0\u679c\u53d1\u73b0\u6027\u80fd"}}
{"id": "2509.23585", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23585", "abs": "https://arxiv.org/abs/2509.23585", "authors": ["Emerald Zhang", "Julian Weaver", "Edward Castillo"], "title": "EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations", "comment": "15 pages", "summary": "Explainable AI (XAI) methods help identify which image regions influence a\nmodel's prediction, but often face a trade-off between detail and\ninterpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware\nalternative. However, LRP implementations commonly rely on heuristic rule sets\nthat are not optimized for clarity or alignment with model behavior. We\nintroduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES) to tune LRP hyperparameters based on quantitative\ninterpretability metrics, such as faithfulness or sparseness. EVO-LRP\noutperforms traditional XAI approaches in both interpretability metric\nperformance and visual coherence, with strong sensitivity to class-specific\nfeatures. These findings demonstrate that attribution quality can be\nsystematically improved through principled, task-specific optimization.", "AI": {"tldr": "EVO-LRP\u4f7f\u7528CMA-ES\u8fdb\u5316\u7b56\u7565\u4f18\u5316LRP\u8d85\u53c2\u6570\uff0c\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u6307\u6807\u63d0\u5347\u5f52\u56e0\u8d28\u91cf\uff0c\u5728\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u5bf9\u7c7b\u522b\u7279\u5b9a\u7279\u5f81\u7684\u654f\u611f\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfXAI\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfXAI\u65b9\u6cd5\u5728\u7ec6\u8282\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cLRP\u867d\u7136\u63d0\u4f9b\u6a21\u578b\u611f\u77e5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5b9e\u73b0\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u96c6\uff0c\u672a\u9488\u5bf9\u6e05\u6670\u5ea6\u6216\u4e0e\u6a21\u578b\u884c\u4e3a\u5bf9\u9f50\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u5f15\u5165EVO-LRP\u65b9\u6cd5\uff0c\u5e94\u7528\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565(CMA-ES)\u6765\u57fa\u4e8e\u5b9a\u91cf\u53ef\u89e3\u91ca\u6027\u6307\u6807\uff08\u5982\u5fe0\u5b9e\u5ea6\u6216\u7a00\u758f\u6027\uff09\u8c03\u6574LRP\u8d85\u53c2\u6570\u3002", "result": "EVO-LRP\u5728\u53ef\u89e3\u91ca\u6027\u6307\u6807\u6027\u80fd\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u5bf9\u7c7b\u522b\u7279\u5b9a\u7279\u5f81\u8868\u73b0\u51fa\u5f3a\u654f\u611f\u6027\u3002", "conclusion": "\u901a\u8fc7\u539f\u5219\u6027\u7684\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\uff0c\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u63d0\u9ad8\u5f52\u56e0\u8d28\u91cf\u3002"}}
{"id": "2509.23587", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.23587", "abs": "https://arxiv.org/abs/2509.23587", "authors": ["Andres Fernandez", "Felix Dangel", "Philipp Hennig", "Frank Schneider"], "title": "Sketching Low-Rank Plus Diagonal Matrices", "comment": null, "summary": "Many relevant machine learning and scientific computing tasks involve\nhigh-dimensional linear operators accessible only via costly matrix-vector\nproducts. In this context, recent advances in sketched methods have enabled the\nconstruction of *either* low-rank *or* diagonal approximations from few\nmatrix-vector products. This provides great speedup and scalability, but\napproximation errors arise due to the assumed simpler structure. This work\nintroduces SKETCHLORD, a method that simultaneously estimates both low-rank\n*and* diagonal components, targeting the broader class of Low-Rank *plus*\nDiagonal (LoRD) linear operators. We demonstrate theoretically and empirically\nthat this joint estimation is superior also to any sequential variant\n(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as\na convex optimization problem, leading to a scalable algorithm. Comprehensive\nexperiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's\nperformance in accurately recovering these structures. This positions it as a\nvaluable addition to the structured approximation toolkit, particularly when\nhigh-fidelity approximations are desired for large-scale operators, such as the\ndeep learning Hessian.", "AI": {"tldr": "SKETCHLORD\u65b9\u6cd5\u540c\u65f6\u4f30\u8ba1\u4f4e\u79e9\u548c\u5bf9\u89d2\u5206\u91cf\uff0c\u9488\u5bf9\u4f4e\u79e9\u52a0\u5bf9\u89d2\u7ebf\u6027\u7b97\u5b50\uff0c\u6bd4\u987a\u5e8f\u4f30\u8ba1\u65b9\u6cd5\u66f4\u4f18\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u6d89\u53ca\u9ad8\u7ef4\u7ebf\u6027\u7b97\u5b50\uff0c\u53ea\u80fd\u901a\u8fc7\u6602\u8d35\u7684\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u8bbf\u95ee\u3002\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u6784\u9020\u4f4e\u79e9\u6216\u5bf9\u89d2\u8fd1\u4f3c\uff0c\u4f46\u5b9e\u9645\u7b97\u5b50\u53ef\u80fd\u540c\u65f6\u5305\u542b\u8fd9\u4e24\u79cd\u7ed3\u6784\u3002", "method": "\u5c06SKETCHLORD\u5efa\u6a21\u4e3a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7b97\u6cd5\uff0c\u540c\u65f6\u4f30\u8ba1\u4f4e\u79e9\u548c\u5bf9\u89d2\u5206\u91cf\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u8054\u5408\u4f30\u8ba1\u4f18\u4e8e\u4efb\u4f55\u987a\u5e8f\u53d8\u4f53\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u51c6\u786e\u6062\u590d\u7ed3\u6784\u3002", "conclusion": "SKETCHLORD\u662f\u9ad8\u4fdd\u771f\u8fd1\u4f3c\u5927\u89c4\u6a21\u7b97\u5b50\uff08\u5982\u6df1\u5ea6\u5b66\u4e60Hessian\uff09\u7684\u6709\u4ef7\u503c\u5de5\u5177\u3002"}}
{"id": "2509.23592", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23592", "abs": "https://arxiv.org/abs/2509.23592", "authors": ["Hoang Phan", "Sungmin Cha", "Tung Lam Tran", "Qi Lei"], "title": "Toward a Holistic Approach to Continual Model Merging", "comment": "Accepted to Workshop on Continual Learning in Computer Vision, ICCV\n  2025", "summary": "We present a holistic framework for continual model merging that intervenes\nat three critical stages: pre-merging, during merging, and post-merging-to\naddress two fundamental challenges in continual learning. In particular,\nconventional approaches either maintain a growing list of per-domain task\nvectors, leading to scalability issues or rely solely on weight-space merging\nwhen old data is inaccessible, thereby losing crucial functional information.\nOur method overcomes these limitations by first fine-tuning the main model\nwithin its tangent space on domain-specific data; this linearization amplifies\nper-task weight disentanglement, effectively mitigating across-task\ninterference. During merging, we leverage functional information from available\noptimizer states beyond mere parameter averages to avoid the need to revisit\nold data. Finally, a post-merging correction aligns the representation\ndiscrepancy between pre- and post-merged models, reducing bias and enhancing\noverall performance-all while operating under constant memory constraints\nwithout accessing historical data. Extensive experiments on standard\nclass-incremental and domain-incremental benchmarks demonstrate that our\napproach not only achieves competitive performance but also provides a scalable\nand efficient solution to the catastrophic forgetting problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6301\u7eed\u6a21\u578b\u878d\u5408\u7684\u6574\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u878d\u5408\u524d\u3001\u878d\u5408\u4e2d\u548c\u878d\u5408\u540e\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u5e72\u9884\uff0c\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u65e0\u9700\u8bbf\u95ee\u5386\u53f2\u6570\u636e\u4e14\u4fdd\u6301\u6052\u5b9a\u5185\u5b58\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u7ef4\u62a4\u4e0d\u65ad\u589e\u957f\u7684\u6bcf\u9886\u57df\u4efb\u52a1\u5411\u91cf\u5217\u8868\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u8981\u4e48\u4ec5\u4f9d\u8d56\u6743\u91cd\u7a7a\u95f4\u878d\u5408\u800c\u4e22\u5931\u5173\u952e\u529f\u80fd\u4fe1\u606f\u3002", "method": "\u5728\u5207\u7ebf\u7a7a\u95f4\u4e2d\u5bf9\u4e3b\u6a21\u578b\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u6570\u636e\u7684\u5fae\u8c03\u4ee5\u589e\u5f3a\u4efb\u52a1\u95f4\u6743\u91cd\u89e3\u8026\uff1b\u5728\u878d\u5408\u65f6\u5229\u7528\u4f18\u5316\u5668\u72b6\u6001\u7684\u529f\u80fd\u4fe1\u606f\uff1b\u878d\u5408\u540e\u8fdb\u884c\u8868\u793a\u5dee\u5f02\u6821\u6b63\u3002", "result": "\u5728\u6807\u51c6\u7c7b\u589e\u91cf\u548c\u9886\u57df\u589e\u91cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u8fd8\u4e3a\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23593", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23593", "abs": "https://arxiv.org/abs/2509.23593", "authors": ["Zekun Wang", "Anant Gupta", "Zihan Dong", "Christopher J. MacLellan"], "title": "Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models", "comment": "18 pages, 14 figures", "summary": "Catastrophic forgetting remains a central obstacle for continual learning in\nneural models. Popular approaches -- replay and elastic weight consolidation\n(EWC) -- have limitations: replay requires a strong generator and is prone to\ndistributional drift, while EWC implicitly assumes a shared optimum across\ntasks and typically uses a diagonal Fisher approximation. In this work, we\nstudy the gradient geometry of diffusion models, which can already produce\nhigh-quality replay data. We provide theoretical and empirical evidence that,\nin the low signal-to-noise ratio (SNR) regime, per-sample gradients become\nstrongly collinear, yielding an empirical Fisher that is effectively rank-1 and\naligned with the mean gradient. Leveraging this structure, we propose a rank-1\nvariant of EWC that is as cheap as the diagonal approximation yet captures the\ndominant curvature direction. We pair this penalty with a replay-based approach\nto encourage parameter sharing across tasks while mitigating drift. On\nclass-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10,\nImageNet-1k), our method consistently improves average FID and reduces\nforgetting relative to replay-only and diagonal-EWC baselines. In particular,\nforgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved\non ImageNet-1k. These results suggest that diffusion models admit an\napproximately rank-1 Fisher. With a better Fisher estimate, EWC becomes a\nstrong complement to replay: replay encourages parameter sharing across tasks,\nwhile EWC effectively constrains replay-induced drift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cd\u653e\u548c\u79e9-1 EWC\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u68af\u5ea6\u5171\u7ebf\u6027\u7684\u7279\u6027\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u91cd\u653e\u9700\u8981\u5f3a\u751f\u6210\u5668\u4e14\u6613\u5206\u5e03\u6f02\u79fb\uff0cEWC\u5047\u8bbe\u4efb\u52a1\u95f4\u5171\u4eab\u6700\u4f18\u89e3\u4e14\u4f7f\u7528\u5bf9\u89d2Fisher\u8fd1\u4f3c\u5b58\u5728\u5c40\u9650\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u68af\u5ea6\u5171\u7ebf\u6027\u7684\u7279\u6027\uff0c\u63d0\u51fa\u79e9-1 EWC\u53d8\u4f53\uff0c\u7ed3\u5408\u91cd\u653e\u65b9\u6cd5\u9f13\u52b1\u53c2\u6570\u5171\u4eab\u5e76\u7ea6\u675f\u6f02\u79fb\u3002", "result": "\u5728\u7c7b\u522b\u589e\u91cf\u56fe\u50cf\u751f\u6210\u6570\u636e\u96c6\u4e0a\u663e\u8457\u6539\u5584\u5e73\u5747FID\uff0c\u51cf\u5c11\u9057\u5fd8\uff0c\u5728MNIST\u548cFashionMNIST\u4e0a\u51e0\u4e4e\u6d88\u9664\u9057\u5fd8\uff0c\u5728ImageNet-1k\u4e0a\u51cf\u5c11\u7ea6\u4e00\u534a\u9057\u5fd8\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5177\u6709\u8fd1\u4f3c\u79e9-1\u7684Fisher\u77e9\u9635\uff0c\u7ed3\u5408\u91cd\u653e\u548cEWC\u80fd\u6709\u6548\u7ea6\u675f\u6f02\u79fb\uff0c\u662f\u6301\u7eed\u5b66\u4e60\u7684\u5f3a\u6709\u529b\u65b9\u6cd5\u3002"}}
{"id": "2509.23597", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23597", "abs": "https://arxiv.org/abs/2509.23597", "authors": ["Zheng Wang", "Kaixuan Zhang", "Wanfang Chen", "Xiaonan Lu", "Longyuan Li", "Tobias Schlagenhauf"], "title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting", "comment": null, "summary": "Time series forecasting remains a critical challenge across numerous domains,\nyet the effectiveness of complex models often varies unpredictably across\ndatasets. Recent studies highlight the surprising competitiveness of simple\nlinear models, suggesting that their robustness and interpretability warrant\ndeeper theoretical investigation. This paper presents a systematic study of\nlinear models for time series forecasting, with a focus on the role of\ncharacteristic roots in temporal dynamics. We begin by analyzing the noise-free\nsetting, where we show that characteristic roots govern long-term behavior and\nexplain how design choices such as instance normalization and channel\nindependence affect model capabilities. We then extend our analysis to the\nnoisy regime, revealing that models tend to produce spurious roots. This leads\nto the identification of a key data-scaling property: mitigating the influence\nof noise requires disproportionately large training data, highlighting the need\nfor structural regularization. To address these challenges, we propose two\ncomplementary strategies for robust root restructuring. The first uses rank\nreduction techniques, including Reduced-Rank Regression and Direct Weight Rank\nReduction, to recover the low-dimensional latent dynamics. The second, a novel\nadaptive method called Root Purge, encourages the model to learn a\nnoise-suppressing null space during training. Extensive experiments on standard\nbenchmarks demonstrate the effectiveness of both approaches, validating our\ntheoretical insights and achieving state-of-the-art results in several\nsettings. Our findings underscore the potential of integrating classical\ntheories for linear systems with modern learning techniques to build robust,\ninterpretable, and data-efficient forecasting models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u7ebf\u6027\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u7279\u5f81\u6839\u5728\u65f6\u95f4\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9c81\u68d2\u6839\u91cd\u6784\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u590d\u6742\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u800c\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u5c55\u73b0\u51fa\u4ee4\u4eba\u60ca\u8bb6\u7684\u7ade\u4e89\u529b\uff0c\u5176\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u503c\u5f97\u6df1\u5165\u7406\u8bba\u7814\u7a76\u3002", "method": "\u5206\u6790\u7279\u5f81\u6839\u5728\u65e0\u566a\u58f0\u548c\u542b\u566a\u58f0\u73af\u5883\u4e0b\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u4f7f\u7528\u964d\u79e9\u6280\u672f\u6062\u590d\u4f4e\u7ef4\u6f5c\u5728\u52a8\u6001\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u81ea\u9002\u5e94\u65b9\u6cd5Root Purge\u6765\u5b66\u4e60\u566a\u58f0\u6291\u5236\u96f6\u7a7a\u95f4\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u89c1\u89e3\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u5c06\u7ebf\u6027\u7cfb\u7edf\u7684\u7ecf\u5178\u7406\u8bba\u4e0e\u73b0\u4ee3\u5b66\u4e60\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u6709\u671b\u6784\u5efa\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u3002"}}
{"id": "2509.23616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23616", "abs": "https://arxiv.org/abs/2509.23616", "authors": ["Fanlong Zeng", "Wensheng Gan", "Philip S. Yu"], "title": "GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning", "comment": "PrePrint, 16 pages, 7 tables, 6 figures", "summary": "The class imbalance problem refers to the disproportionate distribution of\nsamples across different classes within a dataset, where the minority classes\nare significantly underrepresented. This issue is also prevalent in\ngraph-structured data. Most graph neural networks (GNNs) implicitly assume a\nbalanced class distribution and therefore often fail to account for the\nchallenges introduced by class imbalance, which can lead to biased learning and\ndegraded performance on minority classes. We identify a quality inconsistency\nproblem in synthesized nodes, which leads to suboptimal performance under graph\nimbalance conditions. To mitigate this issue, we propose GraphIFE (Graph\nInvariant Feature Extraction), a novel framework designed to mitigate quality\ninconsistency in synthesized nodes. Our approach incorporates two key concepts\nfrom graph invariant learning and introduces strategies to strengthen the\nembedding space representation, thereby enhancing the model's ability to\nidentify invariant features. Extensive experiments demonstrate the framework's\nefficiency and robust generalization, as GraphIFE consistently outperforms\nvarious baselines across multiple datasets. The code is publicly available at\nhttps://github.com/flzeng1/GraphIFE.", "AI": {"tldr": "\u63d0\u51fa\u4e86GraphIFE\u6846\u67b6\u6765\u89e3\u51b3\u56fe\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u4e0d\u53d8\u7279\u5f81\u63d0\u53d6\u6765\u7f13\u89e3\u5408\u6210\u8282\u70b9\u7684\u8d28\u91cf\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u5c11\u6570\u7c7b\u522b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u5047\u8bbe\u7c7b\u522b\u5206\u5e03\u5e73\u8861\uff0c\u4f46\u5728\u5b9e\u9645\u56fe\u6570\u636e\u4e2d\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u5b66\u4e60\u504f\u5dee\u548c\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u5408\u6210\u8282\u70b9\u5b58\u5728\u8d28\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "GraphIFE\u6846\u67b6\u7ed3\u5408\u56fe\u4e0d\u53d8\u5b66\u4e60\u7684\u4e24\u4e2a\u5173\u952e\u6982\u5ff5\uff0c\u91c7\u7528\u7b56\u7565\u6765\u589e\u5f3a\u5d4c\u5165\u7a7a\u95f4\u8868\u793a\uff0c\u63d0\u9ad8\u6a21\u578b\u8bc6\u522b\u4e0d\u53d8\u7279\u5f81\u7684\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eGraphIFE\u6846\u67b6\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GraphIFE\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u56fe\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u7684\u8d28\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2509.23631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23631", "abs": "https://arxiv.org/abs/2509.23631", "authors": ["Chen Yang", "Changhao Zhao", "Chen Wang", "Jiansheng Fan"], "title": "DRIK: Distribution-Robust Inductive Kriging without Information Leakage", "comment": null, "summary": "Inductive kriging supports high-resolution spatio-temporal estimation with\nsparse sensor networks, but conventional training-evaluation setups often\nsuffer from information leakage and poor out-of-distribution (OOD)\ngeneralization. We find that the common 2x2 spatio-temporal split allows test\ndata to influence model selection through early stopping, obscuring the true\nOOD characteristics of inductive kriging. To address this issue, we propose a\n3x3 partition that cleanly separates training, validation, and test sets,\neliminating leakage and better reflecting real-world applications. Building on\nthis redefined setting, we introduce DRIK, a Distribution-Robust Inductive\nKriging approach designed with the intrinsic properties of inductive kriging in\nmind to explicitly enhance OOD generalization, employing a three-tier strategy\nat the node, edge, and subgraph levels. DRIK perturbs node coordinates to\ncapture continuous spatial relationships, drops edges to reduce ambiguity in\ninformation flow and increase topological diversity, and adds pseudo-labeled\nsubgraphs to strengthen domain generalization. Experiments on six diverse\nspatio-temporal datasets show that DRIK consistently outperforms existing\nmethods, achieving up to 12.48% lower MAE while maintaining strong scalability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDRIK\u65b9\u6cd5\uff0c\u901a\u8fc73x3\u6570\u636e\u5206\u5272\u89e3\u51b3\u4f20\u7edf2x2\u5206\u5272\u4e2d\u7684\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\uff0c\u5e76\u5728\u8282\u70b9\u3001\u8fb9\u548c\u5b50\u56fe\u4e09\u4e2a\u5c42\u9762\u589e\u5f3a\u65f6\u7a7a\u514b\u91cc\u91d1\u6cd5\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65f6\u7a7a\u514b\u91cc\u91d1\u6cd5\u5728\u8bad\u7ec3\u8bc4\u4f30\u4e2d\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\uff0c\u6d4b\u8bd5\u6570\u636e\u901a\u8fc7\u65e9\u505c\u5f71\u54cd\u6a21\u578b\u9009\u62e9\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5206\u5e03\u5916\u6cdb\u5316\u7279\u6027\u3002", "method": "\u91c7\u75283x3\u6570\u636e\u5206\u5272\u6d88\u9664\u4fe1\u606f\u6cc4\u9732\uff0c\u63d0\u51faDRIK\u65b9\u6cd5\uff1a\u6270\u52a8\u8282\u70b9\u5750\u6807\u6355\u6349\u8fde\u7eed\u7a7a\u95f4\u5173\u7cfb\u3001\u4e22\u5f03\u8fb9\u51cf\u5c11\u4fe1\u606f\u6d41\u6a21\u7cca\u6027\u3001\u6dfb\u52a0\u4f2a\u6807\u8bb0\u5b50\u56fe\u589e\u5f3a\u9886\u57df\u6cdb\u5316\u3002", "result": "\u5728\u516d\u4e2a\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRIK\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cMAE\u964d\u4f4e\u8fbe12.48%\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DRIK\u901a\u8fc7\u6539\u8fdb\u7684\u6570\u636e\u5206\u5272\u548c\u591a\u5c42\u9762\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u514b\u91cc\u91d1\u6cd5\u7684\u5206\u5e03\u5916\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.23638", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23638", "abs": "https://arxiv.org/abs/2509.23638", "authors": ["Enda Yu", "Zhaoning Zhang", "Dezun Dong", "Yongwei Wu", "Xiangke Liao"], "title": "PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when\ndeployed on commodity hardware. Offloading expert weights to CPU memory results\nin PCIe transfer latency that exceeds GPU computation by several folds. We\npresent PreScope, a prediction-driven expert scheduling system that addresses\nthree key challenges: inaccurate activation prediction, PCIe bandwidth\ncompetition, and cross-device scheduling complexity. Our solution includes: 1)\nLearnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert\nactivation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that\ngenerates globally optimal plans balancing prefetching costs and loading\noverhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from\ncomputation, eliminating waiting bubbles. PreScope achieves 141% higher\nthroughput and 74.6% lower latency than state-of-the-art solutions.", "AI": {"tldr": "PreScope\u662f\u4e00\u4e2a\u9884\u6d4b\u9a71\u52a8\u7684\u4e13\u5bb6\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5c42\u611f\u77e5\u9884\u6d4b\u5668\u3001\u9884\u53d6\u611f\u77e5\u7684\u8de8\u5c42\u8c03\u5ea6\u548c\u5f02\u6b65I/O\u4f18\u5316\uff0c\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u7684\u5185\u5b58\u548cPCIe\u5ef6\u8fdf\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u5185\u5b58\u548cPCIe\u5ef6\u8fdf\u74f6\u9888\uff0c\u5c06\u4e13\u5bb6\u6743\u91cd\u5378\u8f7d\u5230CPU\u5185\u5b58\u4f1a\u5bfc\u81f4PCIe\u4f20\u8f93\u5ef6\u8fdf\u8d85\u8fc7GPU\u8ba1\u7b97\u6570\u500d\u3002", "method": "1) \u53ef\u5b66\u4e60\u7684\u5c42\u611f\u77e5\u9884\u6d4b\u5668\u6355\u6349\u5c42\u7279\u5b9a\u7684\u4e13\u5bb6\u6fc0\u6d3b\u6a21\u5f0f\uff1b2) \u9884\u53d6\u611f\u77e5\u7684\u8de8\u5c42\u8c03\u5ea6\u751f\u6210\u5e73\u8861\u9884\u53d6\u6210\u672c\u548c\u52a0\u8f7d\u5f00\u9500\u7684\u5168\u5c40\u6700\u4f18\u8ba1\u5212\uff1b3) \u5f02\u6b65I/O\u4f18\u5316\u5668\u5c06I/O\u4e0e\u8ba1\u7b97\u89e3\u8026\uff0c\u6d88\u9664\u7b49\u5f85\u6c14\u6ce1\u3002", "result": "PreScope\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86141%\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c74.6%\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "PreScope\u901a\u8fc7\u9884\u6d4b\u9a71\u52a8\u7684\u4e13\u5bb6\u8c03\u5ea6\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.23660", "categories": ["cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.23660", "abs": "https://arxiv.org/abs/2509.23660", "authors": ["Ranhui Yan", "Jia cai"], "title": "Virtual Nodes based Heterogeneous Graph Convolutional Neural Network for Efficient Long-Range Information Aggregation", "comment": null, "summary": "Heterogeneous Graph Neural Networks (HGNNs) have exhibited powerful\nperformance in heterogeneous graph learning by aggregating information from\nvarious types of nodes and edges. However, existing heterogeneous graph models\noften struggle to capture long-range information or necessitate stacking\nnumerous layers to learn such dependencies, resulting in high computational\ncomplexity and encountering over-smoothing issues. In this paper, we propose a\nVirtual Nodes based Heterogeneous Graph Convolutional Network (VN-HGCN), which\nleverages virtual nodes to facilitate enhanced information flow within the\ngraph. Virtual nodes are auxiliary nodes interconnected with all nodes of a\nspecific type in the graph, facilitating efficient aggregation of long-range\ninformation across different types of nodes and edges. By incorporating virtual\nnodes into the graph structure, VN-HGCN achieves effective information\naggregation with only $4$ layers. Additionally, we demonstrate that VN-HGCN can\nserve as a versatile framework that can be seamlessly applied to other HGNN\nmodels, showcasing its generalizability. Empirical evaluations validate the\neffectiveness of VN-HGCN, and extensive experiments conducted on three\nreal-world heterogeneous graph datasets demonstrate the superiority of our\nmodel over several state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u865a\u62df\u8282\u70b9\u7684\u5f02\u8d28\u56fe\u5377\u79ef\u7f51\u7edc(VN-HGCN)\uff0c\u901a\u8fc7\u5f15\u5165\u865a\u62df\u8282\u70b9\u6765\u589e\u5f3a\u56fe\u5185\u4fe1\u606f\u6d41\u52a8\uff0c\u4ec5\u97004\u5c42\u5373\u53ef\u6709\u6548\u805a\u5408\u957f\u8ddd\u79bb\u4fe1\u606f\uff0c\u907f\u514d\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f02\u8d28\u56fe\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u6355\u83b7\u957f\u8ddd\u79bb\u4fe1\u606f\uff0c\u9700\u8981\u5806\u53e0\u591a\u5c42\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u5e76\u51fa\u73b0\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "method": "\u5f15\u5165\u865a\u62df\u8282\u70b9\u4f5c\u4e3a\u8f85\u52a9\u8282\u70b9\uff0c\u4e0e\u7279\u5b9a\u7c7b\u578b\u7684\u6240\u6709\u8282\u70b9\u76f8\u8fde\uff0c\u4fc3\u8fdb\u4e0d\u540c\u7c7b\u578b\u8282\u70b9\u548c\u8fb9\u4e4b\u95f4\u7684\u957f\u8ddd\u79bb\u4fe1\u606f\u9ad8\u6548\u805a\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u5f02\u8d28\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVN-HGCN\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VN-HGCN\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u5176\u4ed6HGNN\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.23662", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23662", "abs": "https://arxiv.org/abs/2509.23662", "authors": ["Fanlong Zeng", "Wensheng Gan", "Jiayang Wu", "Philip S. Yu"], "title": "Pure Node Selection for Imbalanced Graph Node Classification", "comment": "Preprint, 8 tables, 9 figures", "summary": "The problem of class imbalance refers to an uneven distribution of quantity\namong classes in a dataset, where some classes are significantly\nunderrepresented compared to others. Class imbalance is also prevalent in\ngraph-structured data. Graph neural networks (GNNs) are typically based on the\nassumption of class balance, often overlooking the issue of class imbalance. In\nour investigation, we identified a problem, which we term the Randomness\nAnomalous Connectivity Problem (RACP), where certain off-the-shelf models are\naffected by random seeds, leading to a significant performance degradation. To\neliminate the influence of random factors in algorithms, we proposed PNS (Pure\nNode Sampling) to address the RACP in the node synthesis stage. Unlike existing\napproaches that design specialized algorithms to handle either quantity\nimbalance or topological imbalance, PNS is a novel plug-and-play module that\noperates directly during node synthesis to mitigate RACP. Moreover, PNS also\nalleviates performance degradation caused by abnormal distribution of node\nneighbors. We conduct a series of experiments to identify what factors are\ninfluenced by random seeds. Experimental results demonstrate the effectiveness\nand stability of our method, which not only eliminates the effect of\nunfavorable random seeds but also outperforms the baseline across various\nbenchmark datasets with different GNN backbones. Data and code are available at\nhttps://github.com/flzeng1/PNS.", "AI": {"tldr": "\u63d0\u51faPNS\uff08\u7eaf\u8282\u70b9\u91c7\u6837\uff09\u65b9\u6cd5\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u968f\u673a\u6027\u5f02\u5e38\u8fde\u63a5\u95ee\u9898\uff08RACP\uff09\uff0c\u8be5\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u968f\u673a\u79cd\u5b50\u5f71\u54cd\u800c\u663e\u8457\u4e0b\u964d\u3002PNS\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u8282\u70b9\u5408\u6210\u6a21\u5757\uff0c\u80fd\u591f\u6d88\u9664\u968f\u673a\u56e0\u7d20\u5f71\u54cd\u5e76\u7f13\u89e3\u90bb\u5c45\u8282\u70b9\u5f02\u5e38\u5206\u5e03\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u56fe\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u800c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u57fa\u4e8e\u7c7b\u522b\u5e73\u8861\u5047\u8bbe\uff0c\u5ffd\u7565\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u73b0\u6210\u6a21\u578b\u53d7\u968f\u673a\u79cd\u5b50\u5f71\u54cd\u4f1a\u51fa\u73b0\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u968f\u673a\u6027\u5f02\u5e38\u8fde\u63a5\u95ee\u9898\uff08RACP\uff09\u3002", "method": "\u63d0\u51faPNS\uff08\u7eaf\u8282\u70b9\u91c7\u6837\uff09\u65b9\u6cd5\uff0c\u5728\u8282\u70b9\u5408\u6210\u9636\u6bb5\u76f4\u63a5\u64cd\u4f5c\u4ee5\u7f13\u89e3RACP\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cPNS\u4e0d\u8bbe\u8ba1\u4e13\u95e8\u7b97\u6cd5\u5904\u7406\u6570\u91cf\u4e0d\u5e73\u8861\u6216\u62d3\u6251\u4e0d\u5e73\u8861\uff0c\u800c\u662f\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePNS\u65b9\u6cd5\u6709\u6548\u4e14\u7a33\u5b9a\uff0c\u4e0d\u4ec5\u6d88\u9664\u4e86\u4e0d\u5229\u968f\u673a\u79cd\u5b50\u7684\u5f71\u54cd\uff0c\u8fd8\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e0d\u540cGNN\u9aa8\u5e72\u7f51\u7edc\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PNS\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u968f\u673a\u6027\u5f02\u5e38\u8fde\u63a5\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6d88\u9664\u968f\u673a\u56e0\u7d20\u5f71\u54cd\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23665", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.23665", "abs": "https://arxiv.org/abs/2509.23665", "authors": ["Kristina P. Sinaga", "Arjun S. Nair"], "title": "Calibration Meets Reality: Making Machine Learning Predictions Trustworthy", "comment": "30 pages, 7 figures, 5 tables", "summary": "Post-hoc calibration methods are widely used to improve the reliability of\nprobabilistic predictions from machine learning models. Despite their\nprevalence, a comprehensive theoretical understanding of these methods remains\nelusive, particularly regarding their performance across different datasets and\nmodel architectures. Input features play a crucial role in shaping model\npredictions and, consequently, their calibration. However, the interplay\nbetween feature quality and calibration performance has not been thoroughly\ninvestigated. In this work, we present a rigorous theoretical analysis of\npost-hoc calibration methods, focusing on Platt scaling and isotonic\nregression. We derive convergence guarantees, computational complexity bounds,\nand finite-sample performance metrics for these methods. Furthermore, we\nexplore the impact of feature informativeness on calibration performance\nthrough controlled synthetic experiments. Our empirical evaluation spans a\ndiverse set of real-world datasets and model architectures, demonstrating\nconsistent improvements in calibration metrics across various scenarios. By\nexamining calibration performance under varying feature conditions utilizing\nonly informative features versus complete feature spaces including noise\ndimensions, we provide fundamental insights into the robustness and reliability\nof different calibration approaches. Our findings offer practical guidelines\nfor selecting appropriate calibration methods based on dataset characteristics\nand computational constraints, bridging the gap between theoretical\nunderstanding and practical implementation in uncertainty quantification. Code\nand experimental data are available at:\nhttps://github.com/Ajwebdevs/calibration-analysis-experiments.", "AI": {"tldr": "\u672c\u6587\u5bf9\u540e\u9a8c\u6821\u51c6\u65b9\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8Platt\u7f29\u653e\u548c\u7b49\u6e17\u56de\u5f52\uff0c\u7814\u7a76\u4e86\u7279\u5f81\u4fe1\u606f\u91cf\u5bf9\u6821\u51c6\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1\u540e\u9a8c\u6821\u51c6\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0b\u7684\u6027\u80fd\u7f3a\u4e4f\u5168\u9762\u7684\u7406\u8bba\u7406\u89e3\uff0c\u7279\u522b\u662f\u7279\u5f81\u8d28\u91cf\u4e0e\u6821\u51c6\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5bf9Platt\u7f29\u653e\u548c\u7b49\u6e17\u56de\u5f52\u8fdb\u884c\u4e25\u683c\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u6536\u655b\u4fdd\u8bc1\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u754c\u9650\u548c\u6709\u9650\u6837\u672c\u6027\u80fd\u6307\u6807\uff0c\u901a\u8fc7\u53d7\u63a7\u5408\u6210\u5b9e\u9a8c\u63a2\u7d22\u7279\u5f81\u4fe1\u606f\u91cf\u5bf9\u6821\u51c6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u5728\u5404\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0b\uff0c\u6821\u51c6\u6307\u6807\u5747\u5f97\u5230\u4e00\u81f4\u6539\u8fdb\u3002\u7814\u7a76\u53d1\u73b0\u4ec5\u4f7f\u7528\u4fe1\u606f\u7279\u5f81\u4e0e\u5305\u542b\u566a\u58f0\u7ef4\u5ea6\u7684\u5b8c\u6574\u7279\u5f81\u7a7a\u95f4\u76f8\u6bd4\uff0c\u5bf9\u6821\u51c6\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8e\u6570\u636e\u96c6\u7279\u5f81\u548c\u8ba1\u7b97\u7ea6\u675f\u9009\u62e9\u9002\u5f53\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5f25\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7406\u8bba\u7406\u89e3\u4e0e\u5b9e\u9645\u5b9e\u65bd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.23666", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23666", "abs": "https://arxiv.org/abs/2509.23666", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability", "comment": "Accepted as poster in NeurIPS 2025", "summary": "Early-Exit Deep Neural Networks enable adaptive inference by allowing\nprediction at intermediary layers, significantly reducing computational costs\nand latency. Most of the early exit strategies greedily exit a sample at an\nintermediary layer if the confidence in class prediction exceeds a predefined\nthreshold that is set using a static validation set. This is problematic as the\nmodel might be overconfident in a wrong class. Also, they are not robust to\ndistribution shifts encountered in deployment, which can undermine model\ntrustworthiness and accuracy. To address these challenges, we propose UAT that\nadapts the threshold for exit decisions using a Multi-Armed Bandit framework,\nenabling online, unsupervised adjustment of exit decisions. UAT makes decisions\nbased on a new reward function that assesses predictive certainty and its\nreliability to balance computational efficiency and prediction quality while\npenalizing unnecessary late exits. We provide guarantees on risk achieved by\nUAT and validate its performance on diverse tasks spanning vision-language\nunderstanding, text generation, and classification. Our framework demonstrates\nconsistent improvements in speedup (1.70-2.10x) with a minimal performance drop\n(<2%) as compared to full model performance. Our source code is available at\nhttps://github.com/Div290/UAT.", "AI": {"tldr": "\u63d0\u51faUAT\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\u5728\u7ebf\u81ea\u9002\u5e94\u8c03\u6574\u65e9\u9000\u51b3\u7b56\u9608\u503c\uff0c\u89e3\u51b3\u4f20\u7edf\u65e9\u9000\u7b56\u7565\u4e2d\u6a21\u578b\u53ef\u80fd\u5bf9\u9519\u8bef\u7c7b\u522b\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u65e9\u9000\u7b56\u7565\u4f7f\u7528\u9759\u6001\u9a8c\u8bc1\u96c6\u8bbe\u7f6e\u56fa\u5b9a\u9608\u503c\uff0c\u5b58\u5728\u6a21\u578b\u5bf9\u9519\u8bef\u7c7b\u522b\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\uff0c\u4e14\u5bf9\u90e8\u7f72\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u4e0d\u9c81\u68d2\uff0c\u5f71\u54cd\u6a21\u578b\u53ef\u4fe1\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7684\u5956\u52b1\u51fd\u6570\u5728\u7ebf\u65e0\u76d1\u7763\u8c03\u6574\u65e9\u9000\u51b3\u7b56\u9608\u503c\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u8d28\u91cf\uff0c\u60e9\u7f5a\u4e0d\u5fc5\u8981\u7684\u665a\u671f\u9000\u51fa\u3002", "result": "\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u3001\u6587\u672c\u751f\u6210\u548c\u5206\u7c7b\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5b8c\u6574\u6a21\u578b\u6027\u80fd\uff0c\u901f\u5ea6\u63d0\u53471.70-2.10\u500d\uff0c\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e2%\u3002", "conclusion": "UAT\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u65e9\u9000\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.23667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23667", "abs": "https://arxiv.org/abs/2509.23667", "authors": ["Sungmin Cha", "Kyunghyun Cho"], "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation", "comment": "Preprint", "summary": "For efficiency, preference alignment is often performed on compact,\nknowledge-distilled (KD) models. We argue this common practice introduces a\nsignificant limitation by overlooking a key property of the alignment's\nreference model: its distributional recall. We show that the standard KD ->\nAlign workflow diminishes the model's capacity to align rare yet desirable\nbehaviors, even under strong preference signals. We instead demonstrate that\nreversing the pipeline (i.e., Align -> KD) is essential: alignment must first\nbe performed on a high-recall reference before distillation. Our contributions\nare threefold. First, we provide a minimal working explanation of how the\nreference model constrains preference alignment objectives at a fundamental\nlevel. Second, we validate this theory in a controllable Mixture-of-Gaussians\nexperiment, where low-recall anchoring consistently results in suboptimal model\nperformance. Finally, we demonstrate that the same phenomenon holds in LLM\nalignment with the SmolLM2 family: models aligned after KD fail to effectively\nalign target behaviors, resulting in substantially lower reward and target\nprecision. In contrast, our proposed Align -> KD pipeline robustly aligns these\nbehaviors, yielding models with superior target-oriented metrics and lower\nvariance. Together, these results establish reference-model recall as a\nfirst-order design choice in alignment, offering a clear principle: alignment\nmust precede distillation.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5e38\u89c1\u7684\u77e5\u8bc6\u84b8\u998f\u540e\u5bf9\u9f50\uff08KD -> Align\uff09\u6d41\u7a0b\u4f1a\u524a\u5f31\u6a21\u578b\u5bf9\u9f50\u7f55\u89c1\u4f46\u671f\u671b\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u5e94\u8be5\u5148\u5bf9\u9f50\u540e\u84b8\u998f\uff08Align -> KD\uff09\u624d\u80fd\u6709\u6548\u4fdd\u7559\u53c2\u8003\u6a21\u578b\u7684\u9ad8\u53ec\u56de\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u6548\u7387\u5bfc\u5411\u7684\u504f\u597d\u5bf9\u9f50\u901a\u5e38\u5148\u5728\u7d27\u51d1\u7684\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\u4e0a\u8fdb\u884c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u89c6\u4e86\u53c2\u8003\u6a21\u578b\u7684\u5206\u5e03\u53ec\u56de\u7279\u6027\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\u7f55\u89c1\u4f46\u671f\u671b\u7684\u884c\u4e3a\u3002", "method": "1. \u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u53c2\u8003\u6a21\u578b\u5982\u4f55\u7ea6\u675f\u504f\u597d\u5bf9\u9f50\u76ee\u6807\uff1b2. \u5728\u53ef\u63a7\u7684\u9ad8\u65af\u6df7\u5408\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4f4e\u53ec\u56de\u951a\u5b9a\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\uff1b3. \u5728SmolLM2\u7cfb\u5217LLM\u4e0a\u9a8c\u8bc1Align -> KD\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3002", "result": "KD -> Align\u6d41\u7a0b\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\u76ee\u6807\u884c\u4e3a\uff0c\u5956\u52b1\u548c\u76ee\u6807\u7cbe\u5ea6\u663e\u8457\u964d\u4f4e\uff1b\u800cAlign -> KD\u6d41\u7a0b\u80fd\u7a33\u5065\u5bf9\u9f50\u8fd9\u4e9b\u884c\u4e3a\uff0c\u83b7\u5f97\u66f4\u4f18\u7684\u76ee\u6807\u5bfc\u5411\u6307\u6807\u548c\u66f4\u4f4e\u65b9\u5dee\u3002", "conclusion": "\u53c2\u8003\u6a21\u578b\u7684\u53ec\u56de\u7387\u662f\u5bf9\u9f50\u4e2d\u7684\u9996\u8981\u8bbe\u8ba1\u9009\u62e9\uff0c\u660e\u786e\u539f\u5219\uff1a\u5bf9\u9f50\u5fc5\u987b\u5148\u4e8e\u84b8\u998f\u3002"}}
{"id": "2509.23668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23668", "abs": "https://arxiv.org/abs/2509.23668", "authors": ["Xiangfei Qiu", "Liu Yang", "Hanyin Cheng", "Xingjian Wu", "Rongjia Wu", "Zhigang Zhang", "Ding Tu", "Chenjuan Guo", "Bin Yang", "Christian S. Jensen", "Jilin Hu"], "title": "Multi-Scale Spatial-Temporal Hypergraph Network with Lead-Lag Structures for Stock Time Series Forecasting", "comment": null, "summary": "Time series forecasting occurs in a range of financial applications providing\nessential decision-making support to investors, regulatory institutions, and\nanalysts. Unlike multivariate time series from other domains, stock time series\nexhibit industry correlation. Exploiting this kind of correlation can improve\nforecasting accuracy. However, existing methods based on hypergraphs can only\ncapture industry correlation relatively superficially. These methods face two\nkey limitations: they do not fully consider inter-industry lead-lag\ninteractions, and they do not model multi-scale information within and among\nindustries. This study proposes the Hermes framework for stock time series\nforecasting that aims to improve the exploitation of industry correlation by\neliminating these limitations. The framework integrates moving aggregation and\nmulti-scale fusion modules in a hypergraph network. Specifically, to more\nflexibly capture the lead-lag relationships among industries, Hermes proposes a\nhyperedge-based moving aggregation module. This module incorporates a sliding\nwindow and utilizes dynamic temporal aggregation operations to consider\nlead-lag dependencies among industries. Additionally, to effectively model\nmulti-scale information, Hermes employs cross-scale, edge-to-edge message\npassing to integrate information from different scales while maintaining the\nconsistency of each scale. Experimental results on multiple real-world stock\ndatasets show that Hermes outperforms existing state-of-the-art methods in both\nefficiency and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86Hermes\u6846\u67b6\u7528\u4e8e\u80a1\u7968\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u8d85\u56fe\u7f51\u7edc\u4e2d\u7684\u79fb\u52a8\u805a\u5408\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u6765\u66f4\u597d\u5730\u5229\u7528\u884c\u4e1a\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u884c\u4e1a\u95f4\u9886\u5148-\u6ede\u540e\u5173\u7cfb\u548c\u591a\u5c3a\u5ea6\u4fe1\u606f\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u80a1\u7968\u65f6\u95f4\u5e8f\u5217\u5177\u6709\u884c\u4e1a\u76f8\u5173\u6027\uff0c\u5229\u7528\u8fd9\u79cd\u76f8\u5173\u6027\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002\u4f46\u73b0\u6709\u57fa\u4e8e\u8d85\u56fe\u7684\u65b9\u6cd5\u53ea\u80fd\u76f8\u5bf9\u8868\u9762\u5730\u6355\u6349\u884c\u4e1a\u76f8\u5173\u6027\uff0c\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u672a\u5145\u5206\u8003\u8651\u884c\u4e1a\u95f4\u7684\u9886\u5148-\u6ede\u540e\u4ea4\u4e92\uff0c\u4ee5\u53ca\u672a\u5efa\u6a21\u884c\u4e1a\u5185\u90e8\u548c\u884c\u4e1a\u95f4\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u3002", "method": "Hermes\u6846\u67b6\u5728\u8d85\u56fe\u7f51\u7edc\u4e2d\u96c6\u6210\u4e86\u79fb\u52a8\u805a\u5408\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u8d85\u8fb9\u7684\u79fb\u52a8\u805a\u5408\u6a21\u5757\uff0c\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u548c\u52a8\u6001\u65f6\u95f4\u805a\u5408\u64cd\u4f5c\u6765\u7075\u6d3b\u6355\u6349\u884c\u4e1a\u95f4\u7684\u9886\u5148-\u6ede\u540e\u4f9d\u8d56\u5173\u7cfb\uff1b2\uff09\u8de8\u5c3a\u5ea6\u3001\u8fb9\u5230\u8fb9\u7684\u6d88\u606f\u4f20\u9012\uff0c\u4ee5\u6574\u5408\u4e0d\u540c\u5c3a\u5ea6\u7684\u4fe1\u606f\u540c\u65f6\u4fdd\u6301\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u80a1\u7968\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHermes\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Hermes\u6846\u67b6\u901a\u8fc7\u66f4\u6709\u6548\u5730\u5229\u7528\u884c\u4e1a\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6355\u6349\u884c\u4e1a\u95f4\u7684\u9886\u5148-\u6ede\u540e\u5173\u7cfb\u548c\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80a1\u7968\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23671", "abs": "https://arxiv.org/abs/2509.23671", "authors": ["Jingqi Xu", "Guibin Chen", "Jingxi Lu", "Yuzhang Lin"], "title": "Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting", "comment": null, "summary": "Recently, numerous deep models have been proposed to enhance the performance\nof multivariate time series (MTS) forecasting. Among them, Graph Neural\nNetworks (GNNs)-based methods have shown great potential due to their\ncapability to explicitly model inter-variable dependencies. However, these\nmethods often overlook the diversity of information among neighbors, which may\nlead to redundant information aggregation. In addition, their final prediction\ntypically relies solely on the representation from a single temporal scale. To\ntackle these issues, we propose a Graph Neural Networks (GNNs) with\nDiversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN).\nDIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to\nensure that each variable shares high informational similarity with its\nneighbors while maintaining diversity among neighbors themselves. Furthermore,\na Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust\nthe contributions of prediction results from different temporal scales to the\nfinal forecasting result. Extensive experiments on real-world datasets\ndemonstrate that DIMIGNN consistently outperforms prior methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86DIMIGNN\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6837\u6027\u611f\u77e5\u90bb\u5c45\u9009\u62e9\u548c\u52a8\u6001\u591a\u5c3a\u5ea6\u878d\u5408\u6765\u6539\u8fdb\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGNN\u65b9\u6cd5\u4e2d\u90bb\u5c45\u4fe1\u606f\u5197\u4f59\u548c\u5355\u65f6\u95f4\u5c3a\u5ea6\u4f9d\u8d56\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5ffd\u7565\u4e86\u90bb\u5c45\u4e4b\u95f4\u7684\u4fe1\u606f\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\u4fe1\u606f\u805a\u5408\uff1b2\uff09\u6700\u7ec8\u9884\u6d4b\u4ec5\u4f9d\u8d56\u5355\u4e00\u65f6\u95f4\u5c3a\u5ea6\u7684\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86DIMIGNN\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u591a\u6837\u6027\u611f\u77e5\u90bb\u5c45\u9009\u62e9\u673a\u5236\uff08DNSM\uff09\u786e\u4fdd\u53d8\u91cf\u4e0e\u90bb\u5c45\u5177\u6709\u9ad8\u4fe1\u606f\u76f8\u4f3c\u6027\u540c\u65f6\u4fdd\u6301\u90bb\u5c45\u95f4\u591a\u6837\u6027\uff1b\u52a8\u6001\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\uff08DMFM\uff09\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u9884\u6d4b\u7ed3\u679c\u5bf9\u6700\u7ec8\u9884\u6d4b\u7684\u8d21\u732e\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDIMIGNN\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "DIMIGNN\u901a\u8fc7\u5f15\u5165\u591a\u6837\u6027\u611f\u77e5\u90bb\u5c45\u9009\u62e9\u548c\u52a8\u6001\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2509.23678", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23678", "abs": "https://arxiv.org/abs/2509.23678", "authors": ["Guoliang Zhao", "Yuhan Fu", "Shuaipeng Li", "Xingwu Sun", "Ruobing Xie", "An Wang", "Weidong Han", "Zhen Yang", "Weixuan Sun", "Yudong Zhang", "Cheng-zhong Xu", "Di Wang", "Jie Jiang"], "title": "Towards a Comprehensive Scaling Law of Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models have become the consensus approach for\nenabling parameter-efficient scaling and cost-effective deployment in large\nlanguage models. However, existing scaling laws for dense models are\ninapplicable to MoE models, which stems from three critical challenges: the\nmultiplicity of influencing factors, their intricate coupling relationships and\nthe non-monotonic nature of their performance impacts. They collectively\nnecessitate a fine-grained investigation into MoE-specific scaling laws. In\nthis work, we perform a systematic decomposition of MoE settings, identifying\nfive key factors that influence model performance from both size and structural\nperspectives (data size ($D$), total model size ($N$), activated model size\n($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).\nSpecifically, we design $446$ controlled experiments to characterize their\nmarginal effects, ultimately constructing a comprehensive and precise joint MoE\nscaling law that considers all essential factors. Furthermore, we derive the\ntheoretically optimal and practically efficiency-aware optimal configurations\nfor $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that\nthe optimal settings for $G$ and $S$ are independent of both the model\narchitecture and data size. With the scaling of $N$, the optimal activation\nparameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could\nfunction as an accurate and insightful guidance to facilitate future MoE model\ndesign and training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u6df7\u5408\u4e13\u5bb6(MoE)\u6a21\u578b\u7684\u4e13\u7528\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u89e3MoE\u8bbe\u7f6e\uff0c\u8bc6\u522b\u4e865\u4e2a\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u6784\u5efa\u4e86\u8003\u8651\u6240\u6709\u5fc5\u8981\u56e0\u5b50\u7684\u8054\u5408MoE\u7f29\u653e\u5b9a\u5f8b\u3002", "motivation": "\u73b0\u6709\u7684\u7a20\u5bc6\u6a21\u578b\u7f29\u653e\u5b9a\u5f8b\u4e0d\u9002\u7528\u4e8eMoE\u6a21\u578b\uff0c\u56e0\u4e3aMoE\u6a21\u578b\u5b58\u5728\u5f71\u54cd\u56e0\u7d20\u591a\u6837\u6027\u3001\u590d\u6742\u8026\u5408\u5173\u7cfb\u548c\u975e\u5355\u8c03\u6027\u80fd\u5f71\u54cd\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76MoE\u7279\u5b9a\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "method": "\u7cfb\u7edf\u5206\u89e3MoE\u8bbe\u7f6e\uff0c\u8bc6\u522b5\u4e2a\u5173\u952e\u56e0\u7d20(\u6570\u636e\u5927\u5c0fD\u3001\u603b\u6a21\u578b\u5927\u5c0fN\u3001\u6fc0\u6d3b\u6a21\u578b\u5927\u5c0fNa\u3001\u6fc0\u6d3b\u4e13\u5bb6\u6570G\u3001\u5171\u4eab\u4e13\u5bb6\u6bd4\u4f8bS)\uff0c\u8bbe\u8ba1446\u4e2a\u5bf9\u7167\u5b9e\u9a8c\u6765\u8868\u5f81\u5176\u8fb9\u9645\u6548\u5e94\uff0c\u6784\u5efa\u8054\u5408MoE\u7f29\u653e\u5b9a\u5f8b\u3002", "result": "\u53d1\u73b0G\u548cS\u7684\u6700\u4f18\u8bbe\u7f6e\u4e0e\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u5927\u5c0f\u65e0\u5173\uff1b\u968f\u7740N\u7684\u7f29\u653e\uff0c\u6700\u4f18\u6fc0\u6d3b\u53c2\u6570\u6bd4\u4f8bNa/N\u53d8\u5f97\u66f4\u7a00\u758f\uff1b\u63d0\u51fa\u4e86\u51c6\u786e\u4e14\u5177\u6709\u6d1e\u5bdf\u529b\u7684MoE\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "\u63d0\u51fa\u7684MoE\u7f29\u653e\u5b9a\u5f8b\u53ef\u4f5c\u4e3a\u672a\u6765MoE\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7684\u51c6\u786e\u6307\u5bfc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u4e0d\u9002\u7528\u4e8eMoE\u6a21\u578b\u7684\u95ee\u9898\u3002"}}
{"id": "2509.23683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23683", "abs": "https://arxiv.org/abs/2509.23683", "authors": ["Danni Yang", "Zhikang Chen", "Sen Cui", "Mengyue Yang", "Ding Li", "Abudukelimu Wuerkaixi", "Haoxuan Li", "Jinke Ren", "Mingming Gong"], "title": "Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning", "comment": null, "summary": "Federated continual learning (FCL) has garnered increasing attention for its\nability to support distributed computation in environments with evolving data\ndistributions. However, the emergence of new tasks introduces both temporal and\ncross-client shifts, making catastrophic forgetting a critical challenge. Most\nexisting works aggregate knowledge from clients into a global model, which may\nnot enhance client performance since irrelevant knowledge could introduce\ninterference, especially in heterogeneous scenarios. Additionally, directly\napplying decentralized approaches to FCL suffers from ineffective group\nformation caused by task changes. To address these challenges, we propose a\ndecentralized dynamic cooperation framework for FCL, where clients establish\ndynamic cooperative learning coalitions to balance the acquisition of new\nknowledge and the retention of prior learning, thereby obtaining personalized\nmodels. To maximize model performance, each client engages in selective\ncooperation, dynamically allying with others who offer meaningful performance\ngains. This results in non-overlapping, variable coalitions at each stage of\nthe task. Moreover, we use coalitional affinity game to simulate coalition\nrelationships between clients. By assessing both client gradient coherence and\nmodel similarity, we quantify the client benefits derived from cooperation. We\nalso propose a merge-blocking algorithm and a dynamic cooperative evolution\nalgorithm to achieve cooperative and dynamic equilibrium. Comprehensive\nexperiments demonstrate the superiority of our method compared to various\nbaselines. Code is available at: https://github.com/ydn3229/DCFCL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u52a8\u6001\u5408\u4f5c\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u5efa\u7acb\u52a8\u6001\u5408\u4f5c\u5b66\u4e60\u8054\u76df\u6765\u5e73\u8861\u65b0\u77e5\u8bc6\u83b7\u53d6\u548c\u65e7\u77e5\u8bc6\u4fdd\u7559\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u7531\u4e8e\u65b0\u4efb\u52a1\u5f15\u5165\u7684\u65f6\u95f4\u548c\u8de8\u5ba2\u6237\u7aef\u504f\u79fb\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5f02\u6784\u573a\u666f\u4e0b\u53ef\u80fd\u5f15\u5165\u65e0\u5173\u77e5\u8bc6\u5e72\u6270\uff0c\u800c\u76f4\u63a5\u5e94\u7528\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u53c8\u9762\u4e34\u4efb\u52a1\u53d8\u5316\u5bfc\u81f4\u7684\u7ec4\u5f62\u6210\u65e0\u6548\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8054\u76df\u4eb2\u548c\u535a\u5f08\u6a21\u62df\u5ba2\u6237\u7aef\u95f4\u5408\u4f5c\u5173\u7cfb\uff0c\u901a\u8fc7\u8bc4\u4f30\u5ba2\u6237\u7aef\u68af\u5ea6\u4e00\u81f4\u6027\u548c\u6a21\u578b\u76f8\u4f3c\u6027\u6765\u91cf\u5316\u5408\u4f5c\u6536\u76ca\uff0c\u63d0\u51fa\u5408\u5e76\u963b\u585e\u7b97\u6cd5\u548c\u52a8\u6001\u5408\u4f5c\u8fdb\u5316\u7b97\u6cd5\u5b9e\u73b0\u5408\u4f5c\u52a8\u6001\u5747\u8861\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u52a8\u6001\u5408\u4f5c\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u5f02\u6784\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u4e2a\u6027\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.23684", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23684", "abs": "https://arxiv.org/abs/2509.23684", "authors": ["Tanya Chowdhury", "Atharva Nijasure", "Yair Zick", "James Allan"], "title": "Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs", "comment": "Preprint", "summary": "Fine-tuned Large Language Models (LLMs) encode rich task-specific features,\nbut the form of these representations, especially within MLP layers, remains\nunclear. Empirical inspection of LoRA updates shows that new features\nconcentrate in mid-layer MLPs, yet the scale of these layers obscures\nmeaningful structure. Prior probing suggests that statistical priors may\nstrengthen, split, or vanish across depth, motivating the need to study how\nneurons work together rather than in isolation.\n  We introduce a mechanistic interpretability framework based on coalitional\ngame theory, where neurons mimic agents in a hedonic game whose preferences\ncapture their synergistic contributions to layer-local computations. Using\ntop-responsive utilities and the PAC-Top-Cover algorithm, we extract stable\ncoalitions of neurons: groups whose joint ablation has non-additive effects. We\nthen track their transitions across layers as persistence, splitting, merging,\nor disappearance.\n  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR\ntasks, our method finds coalitions with consistently higher synergy than\nclustering baselines. By revealing how neurons cooperate to encode features,\nhedonic coalitions uncover higher-order structure beyond disentanglement and\nyield computational units that are functionally important, interpretable, and\npredictive across domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8054\u76df\u535a\u5f08\u8bba\u7684\u673a\u5236\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u8054\u76df\u5206\u6790LLM\u4e2dMLP\u5c42\u7684\u534f\u540c\u5de5\u4f5c\u673a\u5236\uff0c\u63ed\u793a\u7279\u5f81\u7f16\u7801\u7684\u9ad8\u9636\u7ed3\u6784", "motivation": "\u5fae\u8c03\u540eLLM\u7684MLP\u5c42\u4e2d\u7279\u5f81\u8868\u793a\u5f62\u5f0f\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7814\u7a76\u795e\u7ecf\u5143\u5982\u4f55\u534f\u540c\u5de5\u4f5c\u800c\u975e\u5b64\u7acb\u5206\u6790\uff0c\u4ee5\u7406\u89e3\u7279\u5f81\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u53d8\u5316", "method": "\u4f7f\u7528\u8054\u76df\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u5143\u89c6\u4e3a\u4eab\u4e50\u535a\u5f08\u4e2d\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7PAC-Top-Cover\u7b97\u6cd5\u63d0\u53d6\u7a33\u5b9a\u7684\u795e\u7ecf\u5143\u8054\u76df\uff0c\u8ffd\u8e2a\u8de8\u5c42\u53d8\u5316", "result": "\u5728LLaMA\u3001Mistral\u548cPythia\u91cd\u6392\u5e8f\u5668\u4e0a\u5e94\u7528\uff0c\u53d1\u73b0\u6bd4\u805a\u7c7b\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u534f\u540c\u6027\u7684\u8054\u76df\uff0c\u63ed\u793a\u4e86\u8d85\u8d8a\u89e3\u7f20\u7ed3\u7684\u9ad8\u9636\u7ed3\u6784", "conclusion": "\u4eab\u4e50\u8054\u76df\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u529f\u80fd\u91cd\u8981\u3001\u53ef\u89e3\u91ca\u4e14\u8de8\u9886\u57df\u9884\u6d4b\u7684\u8ba1\u7b97\u5355\u5143\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u5143\u534f\u4f5c\u7f16\u7801\u7279\u5f81\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2509.23688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23688", "abs": "https://arxiv.org/abs/2509.23688", "authors": ["Soroosh Safari Loaliyan", "Jose-Luis Ambite", "Paul M. Thompson", "Neda Jahanshad", "Greg Ver Steeg"], "title": "FedDAPL: Toward Client-Private Generalization in Federated Learning", "comment": "4 Pages", "summary": "Federated Learning (FL) trains models locally at each research center or\nclinic and aggregates only model updates, making it a natural fit for medical\nimaging, where strict privacy laws forbid raw data sharing. A major obstacle is\nscanner-induced domain shift: non-biological variations in hardware or\nacquisition protocols can cause models to fail on external sites. Most\nharmonization methods correct this shift by directly comparing data across\nsites, conflicting with FL's privacy constraints. Domain Generalization (DG)\noffers a privacy-friendly alternative - learning site-invariant representations\nwithout sharing raw data - but standard DG pipelines still assume centralized\naccess to multi-site data, again violating FL's guarantees. This paper meets\nthese difficulties with a straightforward integration of a Domain-Adversarial\nNeural Network (DANN) within the FL process. After demonstrating that a naive\nfederated DANN fails to converge, we propose a proximal regularization method\nthat stabilizes adversarial training among clients. Experiments on T1-weighted\n3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on\nparticipants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79\ny (mean 19+/-13 y; 55 percent male) in validation, show that training on 15\nsites and testing on 19 unseen sites yields superior cross-site generalization\nover FedAvg and ERM while preserving data privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u6574\u5408\u9886\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u7aef\u6b63\u5219\u5316\u7a33\u5b9a\u5ba2\u6237\u7aef\u95f4\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u626b\u63cf\u4eea\u5f15\u8d77\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u7981\u6b62\u539f\u59cb\u6570\u636e\u5171\u4eab\uff0c\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u81ea\u7136\u9009\u62e9\u3002\u4f46\u626b\u63cf\u4eea\u5f15\u8d77\u7684\u9886\u57df\u504f\u79fb\u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u5916\u90e8\u7ad9\u70b9\u5931\u6548\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u9700\u8981\u8de8\u7ad9\u70b9\u76f4\u63a5\u6bd4\u8f83\u6570\u636e\uff0c\u4e0e\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u7ea6\u675f\u51b2\u7a81\u3002", "method": "\u5c06\u9886\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\u6574\u5408\u5230\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u8fd1\u7aef\u6b63\u5219\u5316\u65b9\u6cd5\u7a33\u5b9a\u5ba2\u6237\u7aef\u95f4\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u89e3\u51b3\u6734\u7d20\u8054\u90a6DANN\u4e0d\u6536\u655b\u7684\u95ee\u9898\u3002", "result": "\u5728OpenBHB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u572815\u4e2a\u7ad9\u70b9\u8bad\u7ec3\u5e76\u572819\u4e2a\u672a\u89c1\u7ad9\u70b9\u6d4b\u8bd5\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u7ad9\u70b9\u6cdb\u5316\u65b9\u9762\u4f18\u4e8eFedAvg\u548cERM\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u7ad9\u70b9\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4e25\u683c\u9075\u5b88\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\u3002"}}
{"id": "2509.23689", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23689", "abs": "https://arxiv.org/abs/2509.23689", "authors": ["Ankit Gangwal", "Aaryan Ajay Sharma"], "title": "Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability", "comment": null, "summary": "Model Merging (MM) has emerged as a promising alternative to multi-task\nlearning, where multiple fine-tuned models are combined, without access to\ntasks' training data, into a single model that maintains performance across\ntasks. Recent works have explored the impact of MM on adversarial attacks,\nparticularly backdoor attacks. However, none of them have sufficiently explored\nits impact on transfer attacks using adversarial examples, i.e., a black-box\nadversarial attack where examples generated for a surrogate model successfully\nmislead a target model.\n  In this work, we study the effect of MM on the transferability of adversarial\nexamples. We perform comprehensive evaluations and statistical analysis\nconsisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336\ndistinct attack settings. Through it, we first challenge the prevailing notion\nof MM conferring free adversarial robustness, and show MM cannot reliably\ndefend against transfer attacks, with over 95% relative transfer attack success\nrate. Moreover, we reveal 3 key insights for machine-learning practitioners\nregarding MM and transferability for a robust system design: (1) stronger MM\nmethods increase vulnerability to transfer attacks; (2) mitigating\nrepresentation bias increases vulnerability to transfer attacks; and (3) weight\naveraging, despite being the weakest MM method, is the most vulnerable MM\nmethod to transfer attacks. Finally, we analyze the underlying reasons for this\nincreased vulnerability, and provide potential solutions to the problem. Our\nfindings offer critical insights for designing more secure systems employing\nMM.", "AI": {"tldr": "\u6a21\u578b\u878d\u5408\u4e0d\u80fd\u53ef\u9760\u9632\u5fa1\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u653b\u51fb\uff0c\u53cd\u800c\u53ef\u80fd\u589e\u52a0\u7cfb\u7edf\u8106\u5f31\u6027\uff0c95%\u4ee5\u4e0a\u7684\u76f8\u5bf9\u8fc1\u79fb\u653b\u51fb\u6210\u529f\u7387\u6311\u6218\u4e86\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u514d\u8d39\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u666e\u904d\u89c2\u5ff5\u3002", "motivation": "\u7814\u7a76\u6a21\u578b\u878d\u5408\u5bf9\u5bf9\u6297\u6837\u672c\u8fc1\u79fb\u6027\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u8fc1\u79fb\u653b\u51fb\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u6311\u6218\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u514d\u8d39\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u666e\u904d\u8ba4\u77e5\u3002", "method": "\u4f7f\u75288\u79cd\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u30017\u4e2a\u6570\u636e\u96c6\u548c6\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u5728336\u4e2a\u4e0d\u540c\u7684\u653b\u51fb\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u548c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u6a21\u578b\u878d\u5408\u65e0\u6cd5\u53ef\u9760\u9632\u5fa1\u8fc1\u79fb\u653b\u51fb\uff0c\u76f8\u5bf9\u8fc1\u79fb\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc795%\uff1b\u66f4\u5f3a\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u53cd\u800c\u589e\u52a0\u5bf9\u8fc1\u79fb\u653b\u51fb\u7684\u8106\u5f31\u6027\uff1b\u7f13\u89e3\u8868\u793a\u504f\u5dee\u4f1a\u589e\u52a0\u8106\u5f31\u6027\uff1b\u6743\u91cd\u5e73\u5747\u662f\u6700\u8106\u5f31\u7684\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u878d\u5408\u4f1a\u589e\u52a0\u7cfb\u7edf\u5bf9\u8fc1\u79fb\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u5b89\u5168\u7684\u6a21\u578b\u878d\u5408\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u5206\u6790\u4e86\u8106\u5f31\u6027\u539f\u56e0\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23695", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23695", "abs": "https://arxiv.org/abs/2509.23695", "authors": ["Qingren Yao", "Ming Jin", "Chengqi Zhang", "Chao-Han Huck Yang", "Jun Qi", "Shirui Pan"], "title": "Estimating Time Series Foundation Model Transferability via In-Context Learning", "comment": null, "summary": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via\nlarge-scale pre-training, yet fine-tuning remains critical for boosting\nperformance in domains with limited public data. With the growing number of\nTSFMs, efficiently identifying the best model for downstream fine-tuning\nbecomes increasingly challenging. In this work, we introduce TimeTic, a\ntransferability estimation framework that recasts model selection as an\nin-context-learning problem: given observations on known (source) datasets, it\npredicts how a TSFM will perform after fine-tuning on a downstream (target)\ndataset. TimeTic flexibly organizes the observed model-data relationships as\ncontextual information, allowing it to adapt seamlessly to various test-time\nscenarios. Leveraging the natural tabular structure formed by dataset\nmeta-features, model characteristics, and fine-tuned performance, we employ\ntabular foundation models to serve as in-context learners. We further introduce\na novel model characterization based on entropy evolution across model layers,\ncapturing embedding-space distinctions and enabling TimeTic to generalize\nacross arbitrary model sets. We establish a comprehensive benchmark for\ntransferability estimation including 10 datasets, 10 foundation models, and 3\nforecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong\nalignment with actual fine-tuned performance for previously unseen datasets,\nachieving a mean rank correlation of approximately 0.6 and a 30% improvement\ncompared to using zero-shot performance as the transferability score.", "AI": {"tldr": "TimeTic\u662f\u4e00\u4e2a\u53ef\u8fc1\u79fb\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u9009\u62e9\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u5143\u7279\u5f81\u3001\u6a21\u578b\u7279\u6027\u548c\u5fae\u8c03\u6027\u80fd\u7684\u8868\u683c\u7ed3\u6784\uff0c\u4f7f\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u5668\u6765\u9884\u6d4bTSFM\u5728\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TSFMs)\u6570\u91cf\u7684\u589e\u957f\uff0c\u5728\u6709\u9650\u516c\u5171\u6570\u636e\u7684\u9886\u57df\u4e2d\uff0c\u9ad8\u6548\u8bc6\u522b\u6700\u9002\u5408\u4e0b\u6e38\u5fae\u8c03\u7684\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u3002", "method": "TimeTic\u5c06\u89c2\u5bdf\u5230\u7684\u6a21\u578b-\u6570\u636e\u5173\u7cfb\u7ec4\u7ec7\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u5668\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6a21\u578b\u5c42\u95f4\u71b5\u6f14\u5316\u7684\u65b0\u578b\u6a21\u578b\u7279\u5f81\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u5305\u542b10\u4e2a\u6570\u636e\u96c6\u300110\u4e2a\u57fa\u7840\u6a21\u578b\u548c3\u4e2a\u9884\u6d4b\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTimeTic\u7684\u4f30\u8ba1\u4e0e\u771f\u5b9e\u5fae\u8c03\u6027\u80fd\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e73\u5747\u79e9\u76f8\u5173\u7ea6\u4e3a0.6\uff0c\u76f8\u6bd4\u4f7f\u7528\u96f6\u6837\u672c\u6027\u80fd\u4f5c\u4e3a\u53ef\u8fc1\u79fb\u6027\u5f97\u5206\u63d0\u9ad8\u4e8630%\u3002", "conclusion": "TimeTic\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23711", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23711", "abs": "https://arxiv.org/abs/2509.23711", "authors": ["Ziheng Cheng", "Xin Guo", "Yufei Zhang"], "title": "Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization", "comment": null, "summary": "The theory of discrete-time reinforcement learning (RL) has advanced rapidly\nover the past decades. Although primarily designed for discrete environments,\nmany real-world RL applications are inherently continuous and complex. A major\nchallenge in extending discrete-time algorithms to continuous-time settings is\ntheir sensitivity to time discretization, often leading to poor stability and\nslow convergence. In this paper, we investigate deterministic policy gradient\nmethods for continuous-time RL. We derive a continuous-time policy gradient\nformula based on an analogue of the advantage function and establish its\nmartingale characterization. This theoretical foundation leads to our proposed\nalgorithm, CT-DDPG, which enables stable learning with deterministic policies\nin continuous-time environments. Numerical experiments show that the proposed\nCT-DDPG algorithm offers improved stability and faster convergence compared to\nexisting discrete-time and continuous-time methods, across a wide range of\ncontrol tasks with varying time discretizations and noise levels.", "AI": {"tldr": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5CT-DDPG\uff0c\u89e3\u51b3\u79bb\u6563\u65f6\u95f4RL\u7b97\u6cd5\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u7684\u65f6\u95f4\u79bb\u6563\u5316\u654f\u611f\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u548c\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754cRL\u5e94\u7528\u591a\u4e3a\u8fde\u7eed\u590d\u6742\u73af\u5883\uff0c\u4f46\u73b0\u6709\u79bb\u6563\u65f6\u95f4\u7b97\u6cd5\u5bf9\u65f6\u95f4\u79bb\u6563\u5316\u654f\u611f\uff0c\u5bfc\u81f4\u7a33\u5b9a\u6027\u5dee\u548c\u6536\u655b\u6162\u3002\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u73af\u5883\u7684\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4f18\u52bf\u51fd\u6570\u7684\u7c7b\u6bd4\u63a8\u5bfc\u8fde\u7eed\u65f6\u95f4\u7b56\u7565\u68af\u5ea6\u516c\u5f0f\uff0c\u5efa\u7acb\u5176\u9785\u7279\u6027\u8868\u5f81\uff0c\u63d0\u51faCT-DDPG\u7b97\u6cd5\u5b9e\u73b0\u8fde\u7eed\u65f6\u95f4\u73af\u5883\u4e2d\u786e\u5b9a\u6027\u7b56\u7565\u7684\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cCT-DDPG\u76f8\u6bd4\u73b0\u6709\u79bb\u6563\u65f6\u95f4\u548c\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u79bb\u6563\u5316\u548c\u566a\u58f0\u6c34\u5e73\u3002", "conclusion": "\u8fde\u7eed\u65f6\u95f4\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8fde\u7eed\u73af\u5883RL\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2509.23712", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23712", "abs": "https://arxiv.org/abs/2509.23712", "authors": ["Gholamali Aminian", "Andrew Elliott", "Tiger Li", "Timothy Cheuk Hin Wong", "Victor Claude Dehon", "Lukasz Szpruch", "Carsten Maple", "Christopher Read", "Martin Brown", "Gesine Reinert", "Mo Mamouei"], "title": "FraudTransformer: Time-Aware GPT for Transaction Fraud Detection", "comment": "Pre-print", "summary": "Detecting payment fraud in real-world banking streams requires models that\ncan exploit both the order of events and the irregular time gaps between them.\nWe introduce FraudTransformer, a sequence model that augments a vanilla\nGPT-style architecture with (i) a dedicated time encoder that embeds either\nabsolute timestamps or inter-event values, and (ii) a learned positional\nencoder that preserves relative order. Experiments on a large industrial\ndataset -- tens of millions of transactions and auxiliary events -- show that\nFraudTransformer surpasses four strong classical baselines (Logistic\nRegression, XGBoost and LightGBM) as well as transformer ablations that omit\neither the time or positional component. On the held-out test set it delivers\nthe highest AUROC and PRAUC.", "AI": {"tldr": "FraudTransformer\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u94f6\u884c\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u7684\u5e8f\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u7f16\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u6765\u5229\u7528\u4e8b\u4ef6\u987a\u5e8f\u548c\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u6d88\u878d\u6a21\u578b\u3002", "motivation": "\u5b9e\u65f6\u94f6\u884c\u652f\u4ed8\u6d41\u4e2d\u7684\u6b3a\u8bc8\u68c0\u6d4b\u9700\u8981\u80fd\u591f\u540c\u65f6\u5229\u7528\u4e8b\u4ef6\u987a\u5e8f\u548c\u4e8b\u4ef6\u95f4\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u7684\u6a21\u578b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u5728\u6807\u51c6GPT\u67b6\u6784\u57fa\u7840\u4e0a\u589e\u52a0\u4e13\u7528\u65f6\u95f4\u7f16\u7801\u5668\uff08\u5d4c\u5165\u7edd\u5bf9\u65f6\u95f4\u6233\u6216\u4e8b\u4ef6\u95f4\u503c\uff09\u548c\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\u5668\uff08\u4fdd\u6301\u76f8\u5bf9\u987a\u5e8f\uff09\u3002", "result": "\u5728\u5305\u542b\u6570\u5343\u4e07\u4ea4\u6613\u548c\u8f85\u52a9\u4e8b\u4ef6\u7684\u5927\u578b\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cFraudTransformer\u5728AUROC\u548cPRAUC\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u56db\u79cd\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff08\u903b\u8f91\u56de\u5f52\u3001XGBoost\u3001LightGBM\uff09\u4ee5\u53ca\u7701\u7565\u65f6\u95f4\u6216\u4f4d\u7f6e\u7ec4\u4ef6\u7684\u6d88\u878d\u6a21\u578b\u3002", "conclusion": "FraudTransformer\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u548c\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5728\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u7f16\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.23720", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23720", "abs": "https://arxiv.org/abs/2509.23720", "authors": ["Xian Zeng", "Tianze Xu", "Kai Yang", "Jie Sun", "Youran Wang", "Jun Xu", "Mucheng Ren"], "title": "A Self-Adaptive Frequency Domain Network for Continuous Intraoperative Hypotension Prediction", "comment": "Accepted at ECAI 2025 main conference", "summary": "Intraoperative hypotension (IOH) is strongly associated with postoperative\ncomplications, including postoperative delirium and increased mortality, making\nits early prediction crucial in perioperative care. While several artificial\nintelligence-based models have been developed to provide IOH warnings, existing\nmethods face limitations in incorporating both time and frequency domain\ninformation, capturing short- and long-term dependencies, and handling noise\nsensitivity in biosignal data. To address these challenges, we propose a novel\nSelf-Adaptive Frequency Domain Network (SAFDNet). Specifically, SAFDNet\nintegrates an adaptive spectral block, which leverages Fourier analysis to\nextract frequency-domain features and employs self-adaptive thresholding to\nmitigate noise. Additionally, an interactive attention block is introduced to\ncapture both long-term and short-term dependencies in the data. Extensive\ninternal and external validations on two large-scale real-world datasets\ndemonstrate that SAFDNet achieves up to 97.3\\% AUROC in IOH early warning,\noutperforming state-of-the-art models. Furthermore, SAFDNet exhibits robust\npredictive performance and low sensitivity to noise, making it well-suited for\npractical clinical applications.", "AI": {"tldr": "\u63d0\u51faSAFDNet\u6a21\u578b\u7528\u4e8e\u672f\u4e2d\u4f4e\u8840\u538b\u65e9\u671f\u9884\u8b66\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u57df\u5206\u6790\u548c\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.3% AUROC\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u672f\u4e2d\u4f4e\u8840\u538b\u4e0e\u672f\u540e\u5e76\u53d1\u75c7\u5bc6\u5207\u76f8\u5173\uff0c\u73b0\u6709AI\u9884\u8b66\u6a21\u578b\u5728\u65f6\u9891\u57df\u4fe1\u606f\u878d\u5408\u3001\u957f\u77ed\u65f6\u4f9d\u8d56\u6355\u6349\u548c\u566a\u58f0\u5904\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faSAFDNet\u6a21\u578b\uff0c\u5305\u542b\u81ea\u9002\u5e94\u9891\u8c31\u5757\uff08\u5229\u7528\u5085\u91cc\u53f6\u5206\u6790\u63d0\u53d6\u9891\u57df\u7279\u5f81\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u9608\u503c\u964d\u566a\uff09\u548c\u4ea4\u4e92\u6ce8\u610f\u529b\u5757\uff08\u6355\u6349\u957f\u77ed\u65f6\u4f9d\u8d56\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5185\u5916\u9a8c\u8bc1\u663e\u793a\uff0cSAFDNet\u8fbe\u523097.3% AUROC\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u4e14\u5177\u6709\u9c81\u68d2\u9884\u6d4b\u6027\u80fd\u548c\u4f4e\u566a\u58f0\u654f\u611f\u6027\u3002", "conclusion": "SAFDNet\u9002\u5408\u4e34\u5e8a\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u672f\u4e2d\u4f4e\u8840\u538b\u65e9\u671f\u9884\u8b66\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23742", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23742", "abs": "https://arxiv.org/abs/2509.23742", "authors": ["Yewang Chen", "Junfeng Li", "Shuyin Xia", "Qinghong Lai", "Xinbo Gao", "Guoyin Wang", "Dongdong Cheng", "Yi Liu", "Yi Wang"], "title": "GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data", "comment": null, "summary": "To effectively handle clustering task for large-scale datasets, we propose a\nnovel scalable skeleton clustering algorithm, namely GBSK, which leverages the\ngranular-ball technique to capture the underlying structure of data. By\nmulti-sampling the dataset and constructing multi-grained granular-balls, GBSK\nprogressively uncovers a statistical \"skeleton\" -- a spatial abstraction that\napproximates the essential structure and distribution of the original data.\nThis strategy enables GBSK to dramatically reduce computational overhead while\nmaintaining high clustering accuracy. In addition, we introduce an adaptive\nversion, AGBSK, with simplified parameter settings to enhance usability and\nfacilitate deployment in real-world scenarios. Extensive experiments conducted\non standard computing hardware demonstrate that GBSK achieves high efficiency\nand strong clustering performance on large-scale datasets, including one with\nup to 100 million instances across 256 dimensions. Our implementation and\nexperimental results are available at: https://github.com/XFastDataLab/GBSK/.", "AI": {"tldr": "\u63d0\u51faGBSK\u53ef\u6269\u5c55\u9aa8\u67b6\u805a\u7c7b\u7b97\u6cd5\uff0c\u5229\u7528\u7c92\u5ea6\u7403\u6280\u672f\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u91c7\u6837\u6784\u5efa\u591a\u7c92\u5ea6\u7c92\u5ea6\u7403\u6765\u8fd1\u4f3c\u6570\u636e\u672c\u8d28\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u9ad8\u805a\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u805a\u7c7b\u4efb\u52a1\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6355\u6349\u6570\u636e\u5e95\u5c42\u7ed3\u6784\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u7c92\u5ea6\u7403\u6280\u672f\uff0c\u901a\u8fc7\u591a\u91c7\u6837\u6570\u636e\u96c6\u6784\u5efa\u591a\u7c92\u5ea6\u7c92\u5ea6\u7403\uff0c\u9010\u6b65\u63ed\u793a\u7edf\u8ba1\"\u9aa8\u67b6\"\u4f5c\u4e3a\u539f\u59cb\u6570\u636e\u7ed3\u6784\u548c\u5206\u5e03\u7684\u7a7a\u95f4\u62bd\u8c61\u3002", "result": "\u5728\u6807\u51c6\u8ba1\u7b97\u786c\u4ef6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGBSK\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7387\u548c\u9ad8\u805a\u7c7b\u6027\u80fd\uff0c\u5305\u62ec\u5904\u74061\u4ebf\u5b9e\u4f8b256\u7ef4\u7684\u6570\u636e\u96c6\u3002", "conclusion": "GBSK\u7b97\u6cd5\u901a\u8fc7\u7c92\u5ea6\u7403\u6280\u672f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u805a\u7c7b\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u4f9b\u81ea\u9002\u5e94\u7248\u672cAGBSK\u7b80\u5316\u53c2\u6570\u8bbe\u7f6e\uff0c\u4fbf\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.23749", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23749", "abs": "https://arxiv.org/abs/2509.23749", "authors": ["Ting-Kang Wang", "Chih-Pin Tan", "Yi-Hsuan Yang"], "title": "Time-Shifted Token Scheduling for Symbolic Music Generation", "comment": null, "summary": "Symbolic music generation faces a fundamental trade-off between efficiency\nand quality. Fine-grained tokenizations achieve strong coherence but incur long\nsequences and high complexity, while compact tokenizations improve efficiency\nat the expense of intra-token dependencies. To address this, we adapt a\ndelay-based scheduling mechanism (DP) that expands compound-like tokens across\ndecoding steps, enabling autoregressive modeling of intra-token dependencies\nwhile preserving efficiency. Notably, DP is a lightweight strategy that\nintroduces no additional parameters and can be seamlessly integrated into\nexisting representations. Experiments on symbolic orchestral MIDI datasets show\nthat our method improves all metrics over standard compound tokenizations and\nnarrows the gap to fine-grained tokenizations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ef6\u8fdf\u8c03\u5ea6\u7684\u673a\u5236\uff08DP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u4e2d\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u5efa\u6a21token\u5185\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u9762\u4e34\u6548\u7387\u4e0e\u8d28\u91cf\u7684\u57fa\u672c\u6743\u8861\uff1a\u7ec6\u7c92\u5ea6token\u5316\u80fd\u83b7\u5f97\u5f3a\u8fde\u8d2f\u6027\u4f46\u5e8f\u5217\u957f\u590d\u6742\u5ea6\u9ad8\uff0c\u7d27\u51d1token\u5316\u63d0\u9ad8\u6548\u7387\u4f46\u727a\u7272\u4e86token\u5185\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5ef6\u8fdf\u7684\u8c03\u5ea6\u673a\u5236\uff08DP\uff09\uff0c\u5c06\u7c7b\u590d\u5408token\u6269\u5c55\u5230\u591a\u4e2a\u89e3\u7801\u6b65\u9aa4\uff0c\u5b9e\u73b0token\u5185\u4f9d\u8d56\u5173\u7cfb\u7684\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002DP\u662f\u8f7b\u91cf\u7ea7\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u8868\u793a\u4e2d\u3002", "result": "\u5728\u7b26\u53f7\u7ba1\u5f26\u4e50MIDI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u6807\u51c6\u590d\u5408token\u5316\uff0c\u5e76\u7f29\u5c0f\u4e86\u4e0e\u7ec6\u7c92\u5ea6token\u5316\u7684\u5dee\u8ddd\u3002", "conclusion": "DP\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.23750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23750", "abs": "https://arxiv.org/abs/2509.23750", "authors": ["Li Wang", "Sudun", "Xingjian Zhang", "Wenjun Wu", "Lei Huang"], "title": "An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms", "comment": null, "summary": "Batch Normalization (BN) has played a pivotal role in the success of deep\nlearning by improving training stability, mitigating overfitting, and enabling\nmore effective optimization. However, its adoption in deep reinforcement\nlearning (DRL) has been limited due to the inherent non-i.i.d. nature of data\nand the dynamically shifting distributions induced by the agent's learning\nprocess. In this paper, we argue that, despite these challenges, BN retains\nunique advantages in DRL settings, particularly through its stochasticity and\nits ability to ease training. When applied appropriately, BN can adapt to\nevolving data distributions and enhance both convergence speed and final\nperformance. To this end, we conduct a comprehensive empirical study on the use\nof BN in off-policy actor-critic algorithms, systematically analyzing how\ndifferent training and evaluation modes impact performance. We further identify\nfailure modes that lead to instability or divergence, analyze their underlying\ncauses, and propose the Mode-Aware Batch Normalization (MA-BN) method with\npractical actionable recommendations for robust BN integration in DRL\npipelines. We also empirically validate that, in RL settings, MA-BN accelerates\nand stabilizes training, broadens the effective learning rate range, enhances\nexploration, and reduces overall optimization difficulty. Our code is available\nat: https://github.com/monster476/ma-bn.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u6a21\u5f0f\u611f\u77e5\u6279\u5f52\u4e00\u5316(MA-BN)\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6279\u5f52\u4e00\u5316\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\u5bfc\u81f4\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u6279\u5f52\u4e00\u5316(BN)\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u4e2d\u6570\u636e\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\uff0cBN\u5728DRL\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u3002\u4f5c\u8005\u8ba4\u4e3aBN\u5728DRL\u4e2d\u4ecd\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u9700\u8981\u627e\u5230\u9002\u5f53\u7684\u5e94\u7528\u65b9\u6cd5\u3002", "method": "\u5bf9\u79bb\u7ebf\u7b56\u7565actor-critic\u7b97\u6cd5\u4e2dBN\u7684\u4f7f\u7528\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u5f0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u5931\u6548\u6a21\u5f0f\uff0c\u63d0\u51fa\u6a21\u5f0f\u611f\u77e5\u6279\u5f52\u4e00\u5316(MA-BN)\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u7a33\u5065\u96c6\u6210BN\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0cMA-BN\u80fd\u591f\u52a0\u901f\u548c\u7a33\u5b9a\u8bad\u7ec3\uff0c\u6269\u5927\u6709\u6548\u5b66\u4e60\u7387\u8303\u56f4\uff0c\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u6574\u4f53\u4f18\u5316\u96be\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u5e94\u7528\u65b9\u6cd5\uff0cBN\u80fd\u591f\u9002\u5e94DRL\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\uff0c\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\uff0cMA-BN\u4e3aBN\u5728DRL\u4e2d\u7684\u7a33\u5065\u96c6\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23753", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23753", "abs": "https://arxiv.org/abs/2509.23753", "authors": ["He Zhu", "Junyou Su", "Peng Lai", "Ren Ma", "Wenjia Zhang", "Linyi Yang", "Guanhua Chen"], "title": "Anchored Supervised Fine-Tuning", "comment": null, "summary": "Post-training of large language models involves a fundamental trade-off\nbetween supervised fine-tuning (SFT), which efficiently mimics demonstrations\nbut tends to memorize, and reinforcement learning (RL), which achieves better\ngeneralization at higher computational cost. Dynamic Fine-Tuning (DFT) recently\nemerged as a promising middle ground, reweighting SFT objectives with token\nprobabilities and achieving improvements in certain reasoning domains, though\nit exhibits instability in other tasks. We provide a analysis of DFT through\nthe reward-weighted regression (RWR) framework, revealing that it corresponds\nto a specific auxiliary distribution choice that yields provably tighter RL\nbounds than standard SFT. However, our analysis also uncovers a critical\nlimitation: this construction lacks distributional anchoring, leading to\nprogressive drift that undermines training stability. To address this, we\npropose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's\nreweighting with lightweight KL regularization to preserve tightness while\nensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT\nacross mathematical reasoning, medical knowledge grounding, and code\ngeneration, achieving substantial improvements with minimal computational\noverhead. Our RWR framework provides a systematic lens for understanding\npost-training methods and demonstrates that principled theoretical analysis\nleads to both stronger guarantees and practical gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Anchored Supervised Fine-Tuning (ASFT)\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6b63\u5219\u5316\u89e3\u51b3Dynamic Fine-Tuning (DFT)\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u533b\u5b66\u77e5\u8bc6\u57fa\u7840\u548c\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8eSFT\u548cDFT\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u5bb9\u6613\u8bb0\u5fc6\u5316\uff0c\u800c\u5f3a\u5316\u5b66\u4e60(RL)\u8ba1\u7b97\u6210\u672c\u9ad8\u3002DFT\u4f5c\u4e3a\u6298\u4e2d\u65b9\u6848\u5728\u67d0\u4e9b\u63a8\u7406\u9886\u57df\u8868\u73b0\u826f\u597d\u4f46\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65e2\u4fdd\u6301DFT\u4f18\u52bf\u53c8\u89e3\u51b3\u5176\u4e0d\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5956\u52b1\u52a0\u6743\u56de\u5f52(RWR)\u6846\u67b6\u5206\u6790DFT\uff0c\u53d1\u73b0\u5176\u7f3a\u4e4f\u5206\u5e03\u951a\u5b9a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u63d0\u51faASFT\u65b9\u6cd5\uff0c\u5728DFT\u91cd\u52a0\u6743\u57fa\u7840\u4e0a\u589e\u52a0\u8f7b\u91cf\u7ea7KL\u6b63\u5219\u5316\u6765\u4fdd\u6301\u7d27\u81f4\u6027\u540c\u65f6\u786e\u4fdd\u7a33\u5b9a\u6027\u3002", "result": "ASFT\u5728\u6570\u5b66\u63a8\u7406\u3001\u533b\u5b66\u77e5\u8bc6\u57fa\u7840\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u4e00\u81f4\u4f18\u4e8eSFT\u548cDFT\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "RWR\u6846\u67b6\u4e3a\u7406\u89e3\u540e\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89c6\u89d2\uff0c\u8868\u660e\u539f\u5219\u6027\u7406\u8bba\u5206\u6790\u65e2\u80fd\u5e26\u6765\u66f4\u5f3a\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e5f\u80fd\u83b7\u5f97\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.23756", "categories": ["cs.LG", "cs.AI", "I.2.6; J.3; H.4.2"], "pdf": "https://arxiv.org/pdf/2509.23756", "abs": "https://arxiv.org/abs/2509.23756", "authors": ["Tomer D. Meirman", "Bracha Shapira", "Noa Dagan", "Lior S. Rokach"], "title": "SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values", "comment": "29 pages inc. references for main article. 6 Figures and 7 Tables.\n  Including Data and Code availability statements", "summary": "Interpretable risk scores play a vital role in clinical decision support, yet\ntraditional methods for deriving such scores often rely on manual\npreprocessing, task-specific modeling, and simplified assumptions that limit\ntheir flexibility and predictive power. We present SHAPoint, a novel,\ntask-agnostic framework that integrates the predictive accuracy of gradient\nboosted trees with the interpretability of point-based risk scores. SHAPoint\nsupports classification, regression, and survival tasks, while also inheriting\nvaluable properties from tree-based models, such as native handling of missing\ndata and support for monotonic constraints. Compared to existing frameworks,\nSHAPoint offers superior flexibility, reduced reliance on manual preprocessing,\nand faster runtime performance. Empirical results show that SHAPoint produces\ncompact and interpretable scores with predictive performance comparable to\nstate-of-the-art methods, but at a fraction of the runtime, making it a\npowerful tool for transparent and scalable risk stratification.", "AI": {"tldr": "SHAPoint\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4efb\u52a1\u65e0\u5173\u6846\u67b6\uff0c\u5c06\u68af\u5ea6\u63d0\u5347\u6811\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u57fa\u4e8e\u70b9\u7684\u98ce\u9669\u8bc4\u5206\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u5206\u7c7b\u3001\u56de\u5f52\u548c\u751f\u5b58\u4efb\u52a1\uff0c\u5177\u6709\u7075\u6d3b\u6027\u9ad8\u3001\u9884\u5904\u7406\u9700\u6c42\u5c11\u3001\u8fd0\u884c\u901f\u5ea6\u5feb\u7b49\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u751f\u6210\u53ef\u89e3\u91ca\u98ce\u9669\u8bc4\u5206\u4f9d\u8d56\u4e8e\u624b\u52a8\u9884\u5904\u7406\u3001\u4efb\u52a1\u7279\u5b9a\u5efa\u6a21\u548c\u7b80\u5316\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002", "method": "SHAPoint\u6846\u67b6\u6574\u5408\u68af\u5ea6\u63d0\u5347\u6811\u7684\u9884\u6d4b\u80fd\u529b\u548c\u57fa\u4e8e\u70b9\u7684\u98ce\u9669\u8bc4\u5206\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u7ee7\u627f\u6811\u6a21\u578b\u7684\u4f18\u52bf\u5982\u7f3a\u5931\u6570\u636e\u5904\u7406\u548c\u5355\u8c03\u7ea6\u675f\u652f\u6301\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aSHAPoint\u4ea7\u751f\u7d27\u51d1\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\uff0c\u9884\u6d4b\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8fd0\u884c\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "SHAPoint\u662f\u900f\u660e\u4e14\u53ef\u6269\u5c55\u98ce\u9669\u5206\u5c42\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u63d0\u4f9b\u7075\u6d3b\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.23773", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.23773", "abs": "https://arxiv.org/abs/2509.23773", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Mahantesh Halappanavar", "Nedim Lipka", "Ryan A. Rossi", "Franck Dernoncourt", "Yu Zhang", "Yao Ma", "Yu Wang"], "title": "Knowledge Homophily in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u4e2d\u7684\u77e5\u8bc6\u540c\u8d28\u6027\u6a21\u5f0f\uff0c\u63d0\u51fa\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5b9e\u4f53\u77e5\u8bc6\u6027\u5f97\u5206\uff0c\u4ee5\u4f18\u5316\u77e5\u8bc6\u8986\u76d6\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u53d7\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4e2d\u8bed\u4e49\u805a\u7c7b\u548c\u542f\u52a8\u6548\u5e94\u7684\u542f\u53d1\uff0c\u63a2\u7d22LLMs\u4e2d\u7c7b\u4f3c\u7684\u77e5\u8bc6\u540c\u8d28\u6027\u6a21\u5f0f\uff0c\u5373\u76f8\u5173\u5b9e\u4f53\u5728\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u4e0a\u5177\u6709\u76f8\u4f3c\u6027\u3002", "method": "\u5c06LLM\u77e5\u8bc6\u6620\u5c04\u4e3a\u56fe\u8868\u793a\uff0c\u5206\u6790\u5b9e\u4f53\u4e0e\u90bb\u5c45\u7684\u77e5\u8bc6\u6027\u5173\u7cfb\uff0c\u63d0\u51faGNN\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u5b9e\u4f53\u7ea7\u77e5\u8bc6\u6027\u5f97\u5206\uff0c\u4f18\u5148\u68c0\u67e5\u77e5\u8bc6\u6027\u8f83\u4f4e\u7684triplet\u3002", "result": "\u53d1\u73b0LLMs\u503e\u5411\u4e8e\u5bf9\u56fe\u4e2d\u4f4d\u7f6e\u76f8\u8fd1\u7684\u5b9e\u4f53\u5177\u6709\u76f8\u4f3c\u7684\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\uff0c\u57fa\u4e8e\u540c\u8d28\u6027\u539f\u5219\u7684GNN\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u77e5\u8bc6\u6027\u5f97\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u77e5\u8bc6\u6ce8\u5165LLMs\u7684\u4e3b\u52a8\u6807\u6ce8\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u63a8\u7406\u5bc6\u96c6\u578b\u95ee\u7b54\u4e2d\u7684\u591a\u8df3\u8def\u5f84\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2509.23779", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23779", "abs": "https://arxiv.org/abs/2509.23779", "authors": ["Jiarui Jiang", "Wei Huang", "Miao Zhang", "Taiji Suzuki", "Liqiang Nie"], "title": "Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression", "comment": null, "summary": "State-space models (SSMs), particularly Mamba, emerge as an efficient\nTransformer alternative with linear complexity for long-sequence modeling.\nRecent empirical works demonstrate Mamba's in-context learning (ICL)\ncapabilities competitive with Transformers, a critical capacity for large\nfoundation models. However, theoretical understanding of Mamba's ICL remains\nlimited, restricting deeper insights into its underlying mechanisms. Even\nfundamental tasks such as linear regression ICL, widely studied as a standard\ntheoretical benchmark for Transformers, have not been thoroughly analyzed in\nthe context of Mamba. To address this gap, we study the training dynamics of\nMamba on the linear regression ICL task. By developing novel techniques\ntackling non-convex optimization with gradient descent related to Mamba's\nstructure, we establish an exponential convergence rate to ICL solution, and\nderive a loss bound that is comparable to Transformer's. Importantly, our\nresults reveal that Mamba can perform a variant of \\textit{online gradient\ndescent} to learn the latent function in context. This mechanism is different\nfrom that of Transformer, which is typically understood to achieve ICL through\ngradient descent emulation. The theoretical results are verified by\nexperimental simulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86Mamba\u6a21\u578b\u5728\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5176\u901a\u8fc7\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u673a\u5236\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4e0eTransformer\u7684\u68af\u5ea6\u4e0b\u964d\u6a21\u62df\u673a\u5236\u4e0d\u540c\u3002", "motivation": "Mamba\u4f5c\u4e3aTransformer\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u5e95\u5c42\u673a\u5236\u7684\u6df1\u5165\u6d1e\u5bdf\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u9488\u5bf9Mamba\u7ed3\u6784\u975e\u51f8\u4f18\u5316\u7684\u65b0\u6280\u672f\uff0c\u5206\u6790\u4e86Mamba\u5728\u7ebf\u6027\u56de\u5f52\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5efa\u7acb\u4e86\u6307\u6570\u6536\u655b\u901f\u7387\u548c\u635f\u5931\u8fb9\u754c\u3002", "result": "\u8bc1\u660e\u4e86Mamba\u80fd\u591f\u4ee5\u6307\u6570\u901f\u7387\u6536\u655b\u5230\u4e0a\u4e0b\u6587\u5b66\u4e60\u89e3\uff0c\u5176\u635f\u5931\u8fb9\u754c\u4e0eTransformer\u76f8\u5f53\uff0c\u5e76\u63ed\u793a\u4e86Mamba\u901a\u8fc7\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u673a\u5236\u5b66\u4e60\u6f5c\u5728\u51fd\u6570\u3002", "conclusion": "Mamba\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u4e0eTransformer\u4e0d\u540c\uff0c\u524d\u8005\u901a\u8fc7\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\uff0c\u540e\u8005\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u6a21\u62df\u5b9e\u73b0\uff0c\u8fd9\u4e00\u7406\u8bba\u7ed3\u679c\u5f97\u5230\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2509.23789", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23789", "abs": "https://arxiv.org/abs/2509.23789", "authors": ["Chunxue Xu", "Yiwei Wang", "Yujun Cai", "Bryan Hooi", "Songze Li"], "title": "Visual CoT Makes VLMs Smarter but More Fragile", "comment": null, "summary": "Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in\nVision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates\nexplicit visual edits, such as cropping or annotating regions of interest, into\nthe reasoning process, achieving superior multimodal performance. However, the\nrobustness of Visual CoT-based VLMs against image-level noise remains\nunexplored. In this paper, we present the first systematic evaluation of Visual\nCoT robustness under visual perturbations. Our benchmark spans 12 image\ncorruption types across 4 Visual Question Answering (VQA) datasets, enabling a\ncomprehensive comparison between VLMs that use Visual CoT, and VLMs that do\nnot. The results reveal that integrating Visual CoT consistently improves\nabsolute accuracy regardless of whether the input images are clean or corrupted\nby noise; however, it also increases sensitivity to input perturbations,\nresulting in sharper performance degradation compared to standard VLMs. Through\nextensive analysis, we identify the intermediate reasoning components of Visual\nCoT, i.e., the edited image patches , as the primary source of fragility.\nBuilding on this analysis, we propose a plug-and-play robustness enhancement\nmethod that integrates Grounding DINO model into the Visual CoT pipeline,\nproviding high-confidence local visual cues to stabilize reasoning. Our work\nreveals clear fragility patterns in Visual CoT and offers an effective,\narchitecture-agnostic solution for enhancing visual robustness.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86Visual CoT\u5728\u89c6\u89c9\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u867d\u7136Visual CoT\u80fd\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGrounding DINO\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "Visual CoT\u6280\u672f\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u7f16\u8f91\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5728\u56fe\u50cf\u7ea7\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b12\u79cd\u56fe\u50cf\u635f\u574f\u7c7b\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57284\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4f7f\u7528\u548c\u4e0d\u4f7f\u7528Visual CoT\u7684VLM\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u96c6\u6210Grounding DINO\u6a21\u578b\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "Visual CoT\u5728\u5e72\u51c0\u548c\u566a\u58f0\u56fe\u50cf\u4e0a\u90fd\u80fd\u63d0\u9ad8\u7edd\u5bf9\u51c6\u786e\u7387\uff0c\u4f46\u5bf9\u8f93\u5165\u6270\u52a8\u66f4\u654f\u611f\uff0c\u6027\u80fd\u4e0b\u964d\u66f4\u663e\u8457\uff1b\u7f16\u8f91\u540e\u7684\u56fe\u50cf\u5757\u662f\u8106\u5f31\u6027\u7684\u4e3b\u8981\u6765\u6e90\u3002", "conclusion": "\u63ed\u793a\u4e86Visual CoT\u7684\u8106\u5f31\u6027\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u67b6\u6784\u65e0\u5173\u89e3\u51b3\u65b9\u6848\u6765\u589e\u5f3a\u89c6\u89c9\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.23799", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23799", "abs": "https://arxiv.org/abs/2509.23799", "authors": ["Anyi Wang", "Xuansheng Wu", "Dong Shu", "Yunpu Ma", "Ninghao Liu"], "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement", "comment": "19 pages, 11 figures, 7 tables", "summary": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs.", "AI": {"tldr": "\u63d0\u51faSAE-RSV\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4ece\u6709\u9650\u6570\u636e\u4e2d\u7cbe\u70bc\u63a7\u5236\u5411\u91cf\uff0c\u901a\u8fc7\u8bed\u4e49\u53bb\u566a\u548c\u7279\u5f81\u589e\u5f3a\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6765\u5b66\u4e60\u884c\u4e3a\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4ece\u5c0f\u6570\u636e\u96c6\u63d0\u53d6\u7684\u63a7\u5236\u5411\u91cf\u5e38\u5305\u542b\u4efb\u52a1\u65e0\u5173\u7684\u566a\u58f0\u7279\u5f81\uff0c\u5f71\u54cd\u63a7\u5236\u6548\u679c\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5bf9\u63a7\u5236\u5411\u91cf\u8fdb\u884c\u8bed\u4e49\u53bb\u566a\u548c\u589e\u5f3a\uff1a\u9996\u5148\u6839\u636eSAE\u63d0\u4f9b\u7684\u8bed\u4e49\u53bb\u9664\u4efb\u52a1\u65e0\u5173\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u4e30\u5bcc\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSAE-RSV\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "\u901a\u8fc7SAE\u7cbe\u70bc\u539f\u59cb\u63a7\u5236\u5411\u91cf\uff0c\u53ef\u4ee5\u4ece\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e2d\u6784\u5efa\u6709\u6548\u7684\u63a7\u5236\u5411\u91cf\u3002"}}
{"id": "2509.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23802", "abs": "https://arxiv.org/abs/2509.23802", "authors": ["Yao Luan", "Ni Mu", "Yiqin Yang", "Bo Xu", "Qing-Shan Jia"], "title": "STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Preference-based reinforcement learning (PbRL) bypasses complex reward\nengineering by learning rewards directly from human preferences, enabling\nbetter alignment with human intentions. However, its effectiveness in\nmulti-stage tasks, where agents sequentially perform sub-tasks (e.g.,\nnavigation, grasping), is limited by stage misalignment: Comparing segments\nfrom mismatched stages, such as movement versus manipulation, results in\nuninformative feedback, thus hindering policy learning. In this paper, we\nvalidate the stage misalignment issue through theoretical analysis and\nempirical experiments. To address this issue, we propose STage-AlIgned Reward\nlearning (STAIR), which first learns a stage approximation based on temporal\ndistance, then prioritizes comparisons within the same stage. Temporal distance\nis learned via contrastive learning, which groups temporally close states into\ncoherent stages, without predefined task knowledge, and adapts dynamically to\npolicy changes. Extensive experiments demonstrate STAIR's superiority in\nmulti-stage tasks and competitive performance in single-stage tasks.\nFurthermore, human studies show that stages approximated by STAIR are\nconsistent with human cognition, confirming its effectiveness in mitigating\nstage misalignment.", "AI": {"tldr": "\u63d0\u51faSTAIR\u65b9\u6cd5\u89e3\u51b3\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u504f\u597d\u5b66\u4e60\u9636\u6bb5\u9519\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u8ddd\u79bb\u5b66\u4e60\u9636\u6bb5\u8fd1\u4f3c\u5e76\u4f18\u5148\u540c\u9636\u6bb5\u6bd4\u8f83\uff0c\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u6548\u679c", "motivation": "\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\uff0c\u6bd4\u8f83\u4e0d\u540c\u9636\u6bb5\uff08\u5982\u79fb\u52a8vs\u64cd\u4f5c\uff09\u7684\u7247\u6bb5\u4f1a\u4ea7\u751f\u65e0\u610f\u4e49\u7684\u53cd\u9988\uff0c\u963b\u788d\u7b56\u7565\u5b66\u4e60\uff0c\u9700\u8981\u89e3\u51b3\u9636\u6bb5\u9519\u4f4d\u95ee\u9898", "method": "STAIR\u65b9\u6cd5\uff1a\u57fa\u4e8e\u65f6\u95f4\u8ddd\u79bb\u5b66\u4e60\u9636\u6bb5\u8fd1\u4f3c\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u65f6\u95f4\u76f8\u8fd1\u72b6\u6001\u5206\u7ec4\u4e3a\u8fde\u8d2f\u9636\u6bb5\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u4efb\u52a1\u77e5\u8bc6\uff0c\u52a8\u6001\u9002\u5e94\u7b56\u7565\u53d8\u5316\uff0c\u4f18\u5148\u540c\u9636\u6bb5\u6bd4\u8f83", "result": "\u5b9e\u9a8c\u8bc1\u660eSTAIR\u5728\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5728\u5355\u9636\u6bb5\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4eba\u7c7b\u7814\u7a76\u8868\u660eSTAIR\u8fd1\u4f3c\u9636\u6bb5\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e00\u81f4", "conclusion": "STAIR\u6709\u6548\u7f13\u89e3\u9636\u6bb5\u9519\u4f4d\u95ee\u9898\uff0c\u63d0\u5347\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c"}}
{"id": "2509.23808", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23808", "abs": "https://arxiv.org/abs/2509.23808", "authors": ["Fanding Huang", "Guanbo Huang", "Xiao Fan", "Yi He", "Xiao Liang", "Xiao Chen", "Qinting Jiang", "Faisal Nadeem Khan", "Jingyan Jiang", "Zhi Wang"], "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR", "comment": null, "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22-\u5229\u7528\u6743\u8861\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u63d0\u51fa\u8fd9\u79cd\u6743\u8861\u53ef\u80fd\u662f\u6d4b\u91cf\u5c42\u9762\u7684\u5047\u8c61\u3002\u901a\u8fc7\u8f6c\u5411\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5206\u6790\uff0c\u53d1\u73b0\u63a2\u7d22\u548c\u5229\u7528\u53ef\u4ee5\u89e3\u8026\uff0c\u4ece\u800c\u5f00\u53d1\u4e86VERL\u65b9\u6cd5\u540c\u65f6\u589e\u5f3a\u4e24\u8005\u80fd\u529b\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6RLVR\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u8ba4\u4e3a\u8fd9\u79cd\u6743\u8861\u53ef\u80fd\u4e0d\u662f\u57fa\u672c\u7ea6\u675f\u800c\u662f\u6d4b\u91cf\u5c42\u9762\u7684\u4ea7\u7269\uff0c\u5e0c\u671b\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u63d0\u51faVERL\u65b9\u6cd5\uff0c\u5229\u7528\u6709\u6548\u79e9\u53ca\u5176\u5bfc\u6570\uff08ERV\u548cERA\uff09\u91cf\u5316\u63a2\u7d22\u548c\u5229\u7528\u52a8\u6001\uff0c\u901a\u8fc7\u76f4\u63a5\u5851\u9020RL\u4f18\u52bf\u51fd\u6570\u5b9e\u73b0\u534f\u540c\u589e\u5f3a\uff0c\u4f7f\u7528\u7406\u8bba\u7a33\u5b9a\u7684ERA\u4f5c\u4e3a\u9884\u6d4b\u5143\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u6837\u5316LLM\u548c\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e00\u81f4\u589e\u76ca\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684Gaokao 2024\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe21.4%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u63a2\u7d22\u548c\u5229\u7528\u53ef\u4ee5\u5728\u9690\u85cf\u72b6\u6001\u5c42\u9762\u89e3\u8026\uff0cVERL\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4e24\u8005\u534f\u540c\u589e\u5f3a\uff0c\u4e3aRLVR\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2509.23809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23809", "abs": "https://arxiv.org/abs/2509.23809", "authors": ["Hong Huang", "Decheng Wu", "Rui Cen", "Guanghua Yu", "Zonghang Li", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Xue Liu", "Dapeng Wu"], "title": "Tequila: Trapping-free Ternary Quantization for Large Language Models", "comment": null, "summary": "Quantization techniques are essential for the deployment of Large Language\nModels (LLMs) on edge devices. However, prevailing methods often rely on\nmixed-precision multiplication that lacks efficient hardware support, making it\nnot feasible. Ternary weight quantization addresses this by constraining\nweights to {-1, 0, 1}, replacing expensive multiplications with\nhardware-efficient additions. However, such aggressive compression leads to\nsignificant accuracy degradation, even after costly quantization-aware training\nwith massive data. We identify the core issue as deadzone trapping: a large\nnumber of weights are trapped at the deadzone boundary. This occurs because\nthese weights receive only noisy, uninformative gradients, preventing stable\nescape from the deadzone and severely impeding model capacity and optimization.\nTo address this issue, we propose Tequila, a trapping-free quantization\noptimization method that reactivates deadzone-trapped weights by repurposing\nthem as dynamic biases. This allows the repurposed weights to provide a\ncontinuous signal in the forward pass and, critically, receive direct,\nmeaningful gradient signals during backpropagation, thereby enhancing model\ncapacity and optimization with nearly zero inference overhead. Extensive\nevaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)\nternary quantization methods across five benchmarks. Specifically, on the ARC\nbenchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly\nmatching full-precision performance (within <1% gap) with a 3.0x inference\nspeedup. Consequently, Tequila offers a highly practical and efficient\nimplementation for the deployment of advanced LLMs in resource-constrained\nenvironments. The code is available at https://github.com/Tencent/AngelSlim.", "AI": {"tldr": "Tequila\u662f\u4e00\u79cd\u65e0\u9677\u9631\u7684\u4e09\u5143\u91cf\u5316\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6b7b\u533a\u9677\u9631\u6743\u91cd\u91cd\u65b0\u7528\u4f5c\u52a8\u6001\u504f\u7f6e\u6765\u91cd\u65b0\u6fc0\u6d3b\u5b83\u4eec\uff0c\u5728\u51e0\u4e4e\u96f6\u63a8\u7406\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u5bb9\u91cf\u548c\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4e09\u5143\u6743\u91cd\u91cf\u5316\u4e2d\u7531\u4e8e\u6b7b\u533a\u9677\u9631\u5bfc\u81f4\u7684\u663e\u8457\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7f3a\u4e4f\u9ad8\u6548\u786c\u4ef6\u652f\u6301\u7684\u6df7\u5408\u7cbe\u5ea6\u4e58\u6cd5\uff0c\u800c\u6fc0\u8fdb\u538b\u7f29\u4f1a\u5bfc\u81f4\u6743\u91cd\u88ab\u56f0\u5728\u6b7b\u533a\u8fb9\u754c\u3002", "method": "\u63d0\u51faTequila\u65b9\u6cd5\uff0c\u5c06\u6b7b\u533a\u9677\u9631\u6743\u91cd\u91cd\u65b0\u7528\u4f5c\u52a8\u6001\u504f\u7f6e\uff0c\u4f7f\u8fd9\u4e9b\u6743\u91cd\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u63d0\u4f9b\u8fde\u7eed\u4fe1\u53f7\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u63a5\u6536\u76f4\u63a5\u6709\u610f\u4e49\u7684\u68af\u5ea6\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e09\u5143\u91cf\u5316\u65b9\u6cd5\uff0c\u5728ARC\u57fa\u51c6\u4e0a\u6bd4SOTA\u57fa\u7ebf\u83b7\u5f97>4%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6027\u80fd\uff08\u5dee\u8ddd<1%\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473.0\u500d\u3002", "conclusion": "Tequila\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5148\u8fdbLLM\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u5b9e\u7528\u548c\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2509.23813", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23813", "abs": "https://arxiv.org/abs/2509.23813", "authors": ["Beiliang Wu", "Peiyuan Liu", "Yifan Hu", "Luyan Zhang", "Ao Hu", "Zenglin Xu"], "title": "IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasting (MTSF) plays a vital role in a wide\nrange of real-world applications, such as weather prediction and traffic flow\nforecasting. Although recent advances have significantly improved the modeling\nof temporal dynamics and inter-variable dependencies, most existing methods\noverlook index-related descriptive information, such as timestamps and variable\nindices, which carry rich contextual semantics. To unlock the potential of such\ninformation and take advantage of the lightweight and powerful periodic capture\nability of MLP-based architectures, we propose IndexNet, an MLP-based framework\naugmented with an Index Embedding (IE) module. The IE module consists of two\nkey components: Timestamp Embedding (TE) and Channel Embedding (CE).\nSpecifically, TE transforms timestamps into embedding vectors and injects them\ninto the input sequence, thereby improving the model's ability to capture\nlong-term complex periodic patterns. In parallel, CE assigns each variable a\nunique and trainable identity embedding based on its index, allowing the model\nto explicitly distinguish between heterogeneous variables and avoid homogenized\npredictions when input sequences seem close. Extensive experiments on 12\ndiverse real-world datasets demonstrate that IndexNet achieves comparable\nperformance across mainstream baselines, validating the effectiveness of our\ntemporally and variably aware design. Moreover, plug-and-play experiments and\nvisualization analyses further reveal that IndexNet exhibits strong generality\nand interpretability, two aspects that remain underexplored in current MTSF\nresearch.", "AI": {"tldr": "IndexNet\u662f\u4e00\u4e2a\u57fa\u4e8eMLP\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7d22\u5f15\u5d4c\u5165\u6a21\u5757\uff08\u5305\u542b\u65f6\u95f4\u6233\u5d4c\u5165\u548c\u901a\u9053\u5d4c\u5165\uff09\u6765\u5229\u7528\u63cf\u8ff0\u6027\u4fe1\u606f\uff0c\u63d0\u5347\u957f\u671f\u5468\u671f\u6a21\u5f0f\u6355\u6349\u80fd\u529b\u548c\u53d8\u91cf\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u65f6\u95f4\u6233\u548c\u53d8\u91cf\u7d22\u5f15\u7b49\u63cf\u8ff0\u6027\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5305\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u5bf9\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u5177\u6709\u6f5c\u529b\u3002", "method": "\u63d0\u51faIndexNet\u6846\u67b6\uff0c\u5305\u542b\u7d22\u5f15\u5d4c\u5165\u6a21\u5757\uff1a\u65f6\u95f4\u6233\u5d4c\u5165\u5c06\u65f6\u95f4\u6233\u8f6c\u6362\u4e3a\u5d4c\u5165\u5411\u91cf\u6ce8\u5165\u8f93\u5165\u5e8f\u5217\uff1b\u901a\u9053\u5d4c\u5165\u4e3a\u6bcf\u4e2a\u53d8\u91cf\u5206\u914d\u53ef\u8bad\u7ec3\u7684\u8eab\u4efd\u5d4c\u5165\uff0c\u533a\u5206\u5f02\u8d28\u53d8\u91cf\u3002", "result": "\u572812\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIndexNet\u4e0e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u65f6\u95f4\u611f\u77e5\u548c\u53d8\u91cf\u611f\u77e5\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "IndexNet\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u4e24\u4e2a\u65b9\u9762\u5728\u5f53\u524d\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7814\u7a76\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002"}}
{"id": "2509.23816", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23816", "abs": "https://arxiv.org/abs/2509.23816", "authors": ["Bo Li", "Xin Zheng", "Ming Jin", "Can Wang", "Shirui Pan"], "title": "Test-time GNN Model Evaluation on Dynamic Graphs", "comment": "Accepted by ICDM 2025", "summary": "Dynamic graph neural networks (DGNNs) have emerged as a leading paradigm for\nlearning from dynamic graphs, which are commonly used to model real-world\nsystems and applications. However, due to the evolving nature of dynamic graph\ndata distributions over time, well-trained DGNNs often face significant\nperformance uncertainty when inferring on unseen and unlabeled test graphs in\npractical deployment. In this case, evaluating the performance of deployed\nDGNNs at test time is crucial to determine whether a well-trained DGNN is\nsuited for inference on an unseen dynamic test graph. In this work, we\nintroduce a new research problem: DGNN model evaluation, which aims to assess\nthe performance of a specific DGNN model trained on observed dynamic graphs by\nestimating its performance on unseen dynamic graphs during test time.\nSpecifically, we propose a Dynamic Graph neural network Evaluator, dubbed\nDyGEval, to address this new problem. The proposed DyGEval involves a two-stage\nframework: (1) test-time dynamic graph simulation, which captures the\ntraining-test distributional differences as supervision signals and trains an\nevaluator; and (2) DyGEval development and training, which accurately estimates\nthe performance of the well-trained DGNN model on the test-time dynamic graphs.\nExtensive experiments demonstrate that the proposed DyGEval serves as an\neffective evaluator for assessing various DGNN backbones across different\ndynamic graphs under distribution shifts.", "AI": {"tldr": "\u63d0\u51fa\u4e86DyGEval\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u672a\u89c1\u6d4b\u8bd5\u56fe\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u89e3\u51b3DGNN\u6a21\u578b\u8bc4\u4f30\u95ee\u9898", "motivation": "\u7531\u4e8e\u52a8\u6001\u56fe\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u6f14\u53d8\uff0c\u8bad\u7ec3\u597d\u7684DGNN\u5728\u90e8\u7f72\u5230\u672a\u89c1\u6d4b\u8bd5\u56fe\u65f6\u9762\u4e34\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u9002\u5408\u5728\u7279\u5b9a\u6d4b\u8bd5\u56fe\u4e0a\u8fdb\u884c\u63a8\u7406", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u6d4b\u8bd5\u65f6\u52a8\u6001\u56fe\u6a21\u62df\uff0c\u6355\u6349\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5e03\u5dee\u5f02\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff1b2) DyGEval\u5f00\u53d1\u4e0e\u8bad\u7ec3\uff0c\u51c6\u786e\u4f30\u8ba1\u8bad\u7ec3\u597d\u7684DGNN\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u56fe\u4e0a\u7684\u6027\u80fd", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eDyGEval\u4f5c\u4e3a\u6709\u6548\u8bc4\u4f30\u5668\uff0c\u80fd\u591f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u51c6\u786e\u8bc4\u4f30\u5404\u79cdDGNN\u9aa8\u5e72\u7f51\u7edc\u5728\u4e0d\u540c\u52a8\u6001\u56fe\u4e0a\u7684\u6027\u80fd", "conclusion": "DyGEval\u6210\u529f\u89e3\u51b3\u4e86DGNN\u6a21\u578b\u8bc4\u4f30\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6027\u80fd\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2509.23822", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23822", "abs": "https://arxiv.org/abs/2509.23822", "authors": ["Omri Puny", "Yaron Lipman", "Benjamin Kurt Miller"], "title": "Space Group Conditional Flow Matching", "comment": null, "summary": "Inorganic crystals are periodic, highly-symmetric arrangements of atoms in\nthree-dimensional space. Their structures are constrained by the symmetry\noperations of a crystallographic \\emph{space group} and restricted to lie in\nspecific affine subspaces known as \\emph{Wyckoff positions}. The frequency an\natom appears in the crystal and its rough positioning are determined by its\nWyckoff position. Most generative models that predict atomic coordinates\noverlook these symmetry constraints, leading to unrealistically high\npopulations of proposed crystals exhibiting limited symmetry. We introduce\nSpace Group Conditional Flow Matching, a novel generative framework that\nsamples significantly closer to the target population of highly-symmetric,\nstable crystals. We achieve this by conditioning the entire generation process\non a given space group and set of Wyckoff positions; specifically, we define a\nconditionally symmetric noise base distribution and a group-conditioned,\nequivariant, parametric vector field that restricts the motion of atoms to\ntheir initial Wyckoff position. Our form of group-conditioned equivariance is\nachieved using an efficient reformulation of \\emph{group averaging} tailored\nfor symmetric crystals. Importantly, it reduces the computational overhead of\nsymmetrization to a negligible level. We achieve state of the art results on\ncrystal structure prediction and de novo generation benchmarks. We also perform\nrelevant ablations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u7fa4\u6761\u4ef6\u6d41\u5339\u914d\u7684\u6676\u4f53\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u539f\u5b50\u5728Wyckoff\u4f4d\u7f6e\u4e0a\u7684\u8fd0\u52a8\u6765\u751f\u6210\u66f4\u5bf9\u79f0\u3001\u66f4\u7a33\u5b9a\u7684\u6676\u4f53\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5ffd\u7565\u4e86\u6676\u4f53\u5bf9\u79f0\u6027\u7ea6\u675f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u6676\u4f53\u5bf9\u79f0\u6027\u4e0d\u8db3\u4e14\u4e0d\u73b0\u5b9e\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e25\u683c\u9075\u5faa\u7a7a\u95f4\u7fa4\u548cWyckoff\u4f4d\u7f6e\u7ea6\u675f\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u5bf9\u79f0\u566a\u58f0\u57fa\u5206\u5e03\u548c\u7fa4\u6761\u4ef6\u7b49\u53d8\u5411\u91cf\u573a\uff0c\u5c06\u539f\u5b50\u8fd0\u52a8\u9650\u5236\u5728\u5176\u521d\u59cbWyckoff\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u7fa4\u5e73\u5747\u65b9\u6cd5\u5b9e\u73b0\u5bf9\u79f0\u5316\u3002", "result": "\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u548c\u4ece\u5934\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6676\u4f53\u7684\u5bf9\u79f0\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u5ea6\u5bf9\u79f0\u7684\u6676\u4f53\u7ed3\u6784\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u4e3a\u6676\u4f53\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.23825", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23825", "abs": "https://arxiv.org/abs/2509.23825", "authors": ["Alexander Kolesov", "Stepan Manukhov", "Vladimir V. Palyulin", "Alexander Korotin"], "title": "Electric Currents for Discrete Data Generation", "comment": "generative models, electrodynamics", "summary": "We propose $\\textbf{E}$lectric $\\textbf{C}$urrent $\\textbf{D}$iscrete\n$\\textbf{D}$ata $\\textbf{G}$eneration (ECD$^{2}$G), a pioneering method for\ndata generation in discrete settings that is grounded in electrical engineering\ntheory. Our approach draws an analogy between electric current flow in a\ncircuit and the transfer of probability mass between data distributions. We\ninterpret samples from the source distribution as current input nodes of a\ncircuit and samples from the target distribution as current output nodes. A\nneural network is then used to learn the electric currents to represent the\nprobability flow in the circuit. To map the source distribution to the target,\nwe sample from the source and transport these samples along the circuit\npathways according to the learned currents. This process provably guarantees\ntransfer between data distributions. We present proof-of-concept experiments to\nillustrate our ECD$^{2}$G method.", "AI": {"tldr": "\u63d0\u51faECD\u00b2G\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7535\u8def\u7406\u8bba\u8fdb\u884c\u79bb\u6563\u6570\u636e\u751f\u6210\uff0c\u5c06\u6982\u7387\u8d28\u91cf\u4f20\u8f93\u7c7b\u6bd4\u4e3a\u7535\u6d41\u6d41\u52a8\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7535\u6d41\u8868\u793a\u6982\u7387\u6d41\uff0c\u5b9e\u73b0\u5206\u5e03\u95f4\u7684\u53ef\u8bc1\u660e\u4f20\u8f93\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u53ef\u9760\u65b9\u6cd5\u6765\u5b9e\u73b0\u5206\u5e03\u95f4\u7684\u53ef\u8bc1\u660e\u4f20\u8f93\u3002", "method": "\u5c06\u6e90\u5206\u5e03\u6837\u672c\u89c6\u4e3a\u7535\u8def\u8f93\u5165\u8282\u70b9\uff0c\u76ee\u6807\u5206\u5e03\u6837\u672c\u89c6\u4e3a\u8f93\u51fa\u8282\u70b9\uff0c\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8868\u793a\u6982\u7387\u6d41\u7684\u7535\u6d41\uff0c\u6cbf\u7535\u8def\u8def\u5f84\u4f20\u8f93\u6837\u672c\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u5c55\u793a\u4e86ECD\u00b2G\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5206\u5e03\u95f4\u7684\u53ef\u9760\u4f20\u8f93\u3002", "conclusion": "ECD\u00b2G\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7535\u8def\u7406\u8bba\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u53ef\u8bc1\u660e\u7684\u4f20\u8f93\u4fdd\u8bc1\uff0c\u4e3a\u7269\u7406\u542f\u53d1\u7684\u751f\u6210\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.23830", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.23830", "abs": "https://arxiv.org/abs/2509.23830", "authors": ["Albus Yizhuo Li"], "title": "Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has enabled the creation of massive\nyet efficient Large Language Models (LLMs). However, the standard deterministic\nrouting mechanism presents a significant limitation: its inherent brittleness\nis a key contributor to model miscalibration and overconfidence, resulting in\nsystems that often do not know what they don't know.\n  This thesis confronts this challenge by proposing a structured\n\\textbf{Bayesian MoE routing framework}. Instead of forcing a single,\ndeterministic expert selection, our approach models a probability distribution\nover the routing decision itself. We systematically investigate three families\nof methods that introduce this principled uncertainty at different stages of\nthe routing pipeline: in the \\textbf{weight-space}, the \\textbf{logit-space},\nand the final \\textbf{selection-space}.\n  Through a series of controlled experiments on a 3-billion parameter MoE\nmodel, we demonstrate that this framework significantly improves routing\nstability, in-distribution calibration, and out-of-distribution (OoD)\ndetection. The results show that by targeting this core architectural\ncomponent, we can create a more reliable internal uncertainty signal. This work\nprovides a practical and computationally tractable pathway towards building\nmore robust and self-aware LLMs, taking a crucial step towards making them know\nwhat they don't know.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65afMoE\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u8def\u7531\u51b3\u7b56\u7684\u6982\u7387\u5206\u5e03\u6765\u66ff\u4ee3\u786e\u5b9a\u6027\u8def\u7531\uff0c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u6807\u51c6MoE\u67b6\u6784\u7684\u786e\u5b9a\u6027\u8def\u7531\u673a\u5236\u5b58\u5728\u8106\u5f31\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u6821\u51c6\u4e0d\u826f\u548c\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f7f\u7cfb\u7edf\u65e0\u6cd5\u8bc6\u522b\u81ea\u8eab\u77e5\u8bc6\u7684\u5c40\u9650\u6027\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u4e09\u79cd\u5728\u8def\u7531\u7ba1\u9053\u4e0d\u540c\u9636\u6bb5\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff1a\u6743\u91cd\u7a7a\u95f4\u3001\u5bf9\u6570\u7a7a\u95f4\u548c\u9009\u62e9\u7a7a\u95f4\uff0c\u6784\u5efa\u8d1d\u53f6\u65afMoE\u8def\u7531\u6846\u67b6\u3002", "result": "\u572830\u4ebf\u53c2\u6570MoE\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u7a33\u5b9a\u6027\u3001\u5206\u5e03\u5185\u6821\u51c6\u548c\u5206\u5e03\u5916\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6838\u5fc3\u67b6\u6784\u7ec4\u4ef6\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u4ee5\u521b\u5efa\u66f4\u53ef\u9760\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u81ea\u77e5\u7684LLMs\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.23846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23846", "abs": "https://arxiv.org/abs/2509.23846", "authors": ["Daniele Foffano", "Alessio Russo", "Alexandre Proutiere"], "title": "Adversarial Diffusion for Robust Reinforcement Learning", "comment": null, "summary": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods.", "AI": {"tldr": "\u63d0\u51faAD-RRL\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6761\u4ef6\u91c7\u6837\u751f\u6210\u6700\u574f\u60c5\u51b5\u8f68\u8ff9\u6765\u4f18\u5316\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\uff0c\u63d0\u9ad8\u5bf9\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u5efa\u6a21\u8bef\u5dee\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u4ecd\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u73af\u5883\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u91c7\u6837\u80fd\u529b\uff0c\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\u5728\u8bad\u7ec3\u671f\u95f4\u751f\u6210\u6700\u574f\u60c5\u51b5\u8f68\u8ff9\uff0c\u4ece\u800c\u4f18\u5316\u7d2f\u79ef\u56de\u62a5\u7684\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAD-RRL\u76f8\u6bd4\u73b0\u6709\u9c81\u68d2RL\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6761\u4ef6\u91c7\u6837\u751f\u6210\u6700\u574f\u60c5\u51b5\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u4e3a\u8bad\u7ec3\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5bf9\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2509.23866", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23866", "abs": "https://arxiv.org/abs/2509.23866", "authors": ["Pengxiang Li", "Zechen Hu", "Zirui Shang", "Jingrong Wu", "Yang Liu", "Hui Liu", "Zhi Gao", "Chenrui Shi", "Bofei Zhang", "Zihao Zhang", "Xiaochuan Shi", "Zedong YU", "Yuwei Wu", "Xinxiao Wu", "Yunde Jia", "Liuyu Xiang", "Zhaofeng He", "Qing Li"], "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation", "comment": null, "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.", "AI": {"tldr": "DART\u662f\u4e00\u4e2a\u89e3\u8026\u7684GUI\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u6a21\u5757\u8bbe\u8ba1\u89e3\u51b3VLM GUI\u4ee3\u7406\u5728RL\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e8642.13%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684GUI\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u684c\u9762\u548c\u79fb\u52a8\u4efb\u52a1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u7684\u4e24\u5927\u6311\u6218\uff1aGUI\u73af\u5883\u591a\u8f6e\u4ea4\u4e92\u901f\u5ea6\u6162\uff0c\u4ee5\u53ca\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u8d28\u91cf\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDART\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u7cfb\u7edf\u5206\u4e3a\u56db\u4e2a\u5f02\u6b65\u6a21\u5757\uff1a\u73af\u5883\u96c6\u7fa4\u3001\u6eda\u52a8\u670d\u52a1\u3001\u6570\u636e\u7ba1\u7406\u5668\u548c\u8bad\u7ec3\u5668\uff0c\u5b9e\u73b0\u975e\u963b\u585e\u901a\u4fe1\u548c\u5f02\u6b65\u8bad\u7ec3\u3002\u5f15\u5165\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u7406\u65b9\u6848\uff0c\u5305\u62ec\u9884\u6536\u96c6\u6210\u529f\u8f68\u8ff9\u3001\u52a8\u6001\u8c03\u6574\u6eda\u52a8\u53c2\u6570\u3001\u9009\u62e9\u6027\u8bad\u7ec3\u9ad8\u71b5\u6b65\u9aa4\u548c\u4f7f\u7528\u622a\u65ad\u91cd\u8981\u6027\u91c7\u6837\u3002", "result": "DART\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6548\u7387\uff1a1.6\u500dGPU\u5229\u7528\u7387\u30011.9\u500d\u8bad\u7ec3\u541e\u5410\u91cf\u548c5.5\u500d\u73af\u5883\u5229\u7528\u7387\u3002\u5728OSWorld\u57fa\u51c6\u4e0a\uff0cDART-GUI-7B\u8fbe\u523042.13%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534714.61%\uff0c\u6bd4\u5f00\u6e90SOTA\u9ad87.34%\u3002", "conclusion": "DART\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u7387\u74f6\u9888\uff0c\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u7406\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d21\u732e\u3002"}}
{"id": "2509.23886", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23886", "abs": "https://arxiv.org/abs/2509.23886", "authors": ["Simon Schrodi", "Elias Kempf", "Fazl Barez", "Thomas Brox"], "title": "Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer", "comment": null, "summary": "Language models can transfer hidden biases during distillation. For example,\na teacher that \"likes owls\" can make its student \"like owls\" too, even when the\ntraining data consists only of lists of numbers. This surprising phenomenon is\ncalled subliminal learning. Subliminal learning can be expected under soft\ndistillation, where the student is trained on the teacher's full next-token\ndistribution. But the fact that this also occurs under hard distillation-where\nthe student only sees sampled tokens-raises a deeper question: when and how\ndoes subliminal learning actually occur? We answer this question through\ncontrolled experiments and mechanistic analysis. Our results show that\nsubliminal learning does not need (global) token entanglement or logit leakage.\nInstead, it comes down to a small set of divergence tokens-rare cases where\nteachers with different biases would predict different tokens. Masking out\nthese tokens mostly removes the hidden bias transfer. Mechanistically,\ndivergence tokens reveal that early layers are critical. Surprisingly,\nfinetuning even a single such early layer is sufficient for subliminal\nlearning. Finally, we find that subliminal learning is fragile. Even small\nchanges, like paraphrasing prompts, are usually sufficient to suppress it.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u4f20\u9012\u9690\u85cf\u504f\u89c1\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u6f5c\u610f\u8bc6\u5b66\u4e60\u3002\u7814\u7a76\u53d1\u73b0\u6f5c\u610f\u8bc6\u5b66\u4e60\u4e0d\u9700\u8981\u5168\u5c40\u6807\u8bb0\u7ea0\u7f20\u6216logit\u6cc4\u6f0f\uff0c\u800c\u662f\u7531\u4e00\u5c0f\u90e8\u5206\u5206\u6b67\u6807\u8bb0\u9a71\u52a8\uff0c\u8fd9\u4e9b\u6807\u8bb0\u5728\u5177\u6709\u4e0d\u540c\u504f\u89c1\u7684\u6559\u5e08\u6a21\u578b\u4f1a\u9884\u6d4b\u4e0d\u540c\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u8fc7\u7a0b\u4e2d\u9690\u85cf\u504f\u89c1\u4f20\u9012\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u4e3a\u4ec0\u4e48\u5728\u786c\u84b8\u998f\uff08\u5b66\u751f\u53ea\u770b\u5230\u91c7\u6837\u6807\u8bb0\uff09\u4e2d\u4e5f\u4f1a\u51fa\u73b0\u6f5c\u610f\u8bc6\u5b66\u4e60\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u548c\u673a\u5236\u5206\u6790\uff0c\u8bc6\u522b\u5206\u6b67\u6807\u8bb0\u7684\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u8fd9\u4e9b\u6807\u8bb0\u6765\u9a8c\u8bc1\u5176\u5173\u952e\u6027\u3002\u8fd8\u6d4b\u8bd5\u4e86\u5fae\u8c03\u65e9\u671f\u5c42\u5bf9\u6f5c\u610f\u8bc6\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u5206\u6b67\u6807\u8bb0\u662f\u6f5c\u610f\u8bc6\u5b66\u4e60\u7684\u5173\u952e\uff0c\u63a9\u7801\u8fd9\u4e9b\u6807\u8bb0\u80fd\u57fa\u672c\u6d88\u9664\u9690\u85cf\u504f\u89c1\u4f20\u9012\u3002\u65e9\u671f\u5c42\u5728\u6f5c\u610f\u8bc6\u5b66\u4e60\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5fae\u8c03\u5355\u4e2a\u65e9\u671f\u5c42\u5c31\u8db3\u4ee5\u5b9e\u73b0\u6f5c\u610f\u8bc6\u5b66\u4e60\u3002\u6f5c\u610f\u8bc6\u5b66\u4e60\u5f88\u8106\u5f31\uff0c\u5373\u4f7f\u5c0f\u7684\u63d0\u793a\u6539\u5199\u4e5f\u80fd\u6291\u5236\u5b83\u3002", "conclusion": "\u6f5c\u610f\u8bc6\u5b66\u4e60\u4e3b\u8981\u7531\u5206\u6b67\u6807\u8bb0\u9a71\u52a8\uff0c\u901a\u8fc7\u65e9\u671f\u5c42\u5b9e\u73b0\uff0c\u4f46\u8fd9\u79cd\u73b0\u8c61\u5f88\u8106\u5f31\uff0c\u5bb9\u6613\u88ab\u5e72\u6270\u56e0\u7d20\u7834\u574f\u3002"}}
{"id": "2509.23887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23887", "abs": "https://arxiv.org/abs/2509.23887", "authors": ["Yash Jakhmola"], "title": "Gradient Flow Convergence Guarantee for General Neural Network Architectures", "comment": "12 pages, 3 figures, 1 table", "summary": "A key challenge in modern deep learning theory is to explain the remarkable\nsuccess of gradient-based optimization methods when training large-scale,\ncomplex deep neural networks. Though linear convergence of such methods has\nbeen proved for a handful of specific architectures, a united theory still\nevades researchers. This article presents a unified proof for linear\nconvergence of continuous gradient descent, also called gradient flow, while\ntraining any neural network with piecewise non-zero polynomial activations or\nReLU, sigmoid activations. Our primary contribution is a single, general\ntheorem that not only covers architectures for which this result was previously\nunknown but also consolidates existing results under weaker assumptions. While\nour focus is theoretical and our results are only exact in the infinitesimal\nstep size limit, we nevertheless find excellent empirical agreement between the\npredictions of our result and those of the practical step-size gradient descent\nmethod.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\uff0c\u8bc1\u660e\u4f7f\u7528\u5206\u6bb5\u975e\u96f6\u591a\u9879\u5f0f\u6fc0\u6d3b\u51fd\u6570\u6216ReLU\u3001sigmoid\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u8fde\u7eed\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u4e2d\u5177\u6709\u7ebf\u6027\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u662f\u89e3\u91ca\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u8bad\u7ec3\u5927\u89c4\u6a21\u590d\u6742\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\u7684\u663e\u8457\u6210\u529f\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8bc1\u660e\u4e86\u7279\u5b9a\u67b6\u6784\u7684\u7ebf\u6027\u6536\u655b\u6027\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u8bc1\u660e\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u4f7f\u7528\u5206\u6bb5\u975e\u96f6\u591a\u9879\u5f0f\u6fc0\u6d3b\u51fd\u6570\u6216ReLU\u3001sigmoid\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u8fde\u7eed\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u4e2d\u7684\u7ebf\u6027\u6536\u655b\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u5b9a\u7406\uff0c\u4e0d\u4ec5\u8986\u76d6\u4e86\u5148\u524d\u672a\u77e5\u7684\u67b6\u6784\uff0c\u8fd8\u5728\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\u6574\u5408\u4e86\u73b0\u6709\u7ed3\u679c\u3002\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9645\u6b65\u957f\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u7684\u7ecf\u9a8c\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u68af\u5ea6\u4e0b\u964d\u7684\u7ebf\u6027\u6536\u655b\u6027\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2509.23893", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.23893", "abs": "https://arxiv.org/abs/2509.23893", "authors": ["Zhixin Zhang", "Zeming Wei", "Meng Sun"], "title": "Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings", "comment": null, "summary": "Catastrophic forgetting remains a critical challenge in continual learning\nfor large language models (LLMs), where models struggle to retain performance\non historical tasks when fine-tuning on new sequential data without access to\npast datasets. In this paper, we first reveal that the drift of functional\ndirections during the fine-tuning process is a key reason why existing\nregularization-based methods fail in long-term LLM continual learning. To\naddress this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a\nnovel approach that tracks the drift of these functional directions and\ndynamically updates them during the fine-tuning process. Furthermore, by\nadjusting the gradients of new task parameters to be orthogonal to the tracked\nhistorical function directions, our method mitigates interference between new\nand old tasks. Extensive experiments on various LLM continual learning\nbenchmarks demonstrate that this approach outperforms prior methods,\neffectively reducing catastrophic forgetting and providing a robust tool for\ncontinuous LLM fine-tuning. Our code is available at\nhttps://github.com/meloxxxxxx/DOC.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6b63\u4ea4\u6301\u7eed\u5fae\u8c03\uff08DOC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u529f\u80fd\u65b9\u5411\u6f02\u79fb\u5e76\u52a8\u6001\u66f4\u65b0\uff0c\u8c03\u6574\u65b0\u4efb\u52a1\u68af\u5ea6\u4e0e\u5386\u53f2\u529f\u80fd\u65b9\u5411\u6b63\u4ea4\uff0c\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u5728\u957f\u671fLLM\u6301\u7eed\u5b66\u4e60\u4e2d\u5931\u8d25\uff0c\u5173\u952e\u539f\u56e0\u662f\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u529f\u80fd\u65b9\u5411\u7684\u6f02\u79fb\u3002", "method": "DOC\u65b9\u6cd5\u8ffd\u8e2a\u529f\u80fd\u65b9\u5411\u6f02\u79fb\u5e76\u52a8\u6001\u66f4\u65b0\uff0c\u901a\u8fc7\u8c03\u6574\u65b0\u4efb\u52a1\u53c2\u6570\u68af\u5ea6\u4e0e\u5386\u53f2\u529f\u80fd\u65b9\u5411\u6b63\u4ea4\u6765\u51cf\u5c11\u65b0\u65e7\u4efb\u52a1\u5e72\u6270\u3002", "result": "\u5728\u591a\u4e2aLLM\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "DOC\u65b9\u6cd5\u4e3a\u6301\u7eedLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u9c81\u68d2\u5de5\u5177\uff0c\u901a\u8fc7\u52a8\u6001\u6b63\u4ea4\u5316\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2509.23898", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23898", "abs": "https://arxiv.org/abs/2509.23898", "authors": ["Chris Kolb", "Laetitia Frost", "Bernd Bischl", "David R\u00fcgamer"], "title": "Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization", "comment": null, "summary": "Structured sparsity regularization offers a principled way to compact neural\nnetworks, but its non-differentiability breaks compatibility with conventional\nstochastic gradient descent and requires either specialized optimizers or\nadditional post-hoc pruning without formal guarantees. In this work, we propose\n$D$-Gating, a fully differentiable structured overparameterization that splits\neach group of weights into a primary weight vector and multiple scalar gating\nfactors. We prove that any local minimum under $D$-Gating is also a local\nminimum using non-smooth structured $L_{2,2/D}$ penalization, and further show\nthat the $D$-Gating objective converges at least exponentially fast to the\n$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results\nshow that $D$-Gating is theoretically equivalent to solving the original group\nsparsity problem, yet induces distinct learning dynamics that evolve from a\nnon-sparse regime into sparse optimization. We validate our theory across\nvision, language, and tabular tasks, where $D$-Gating consistently delivers\nstrong performance-sparsity tradeoffs and outperforms both direct optimization\nof structured penalties and conventional pruning baselines.", "AI": {"tldr": "\u63d0\u51faD-Gating\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u7684\u7ed3\u6784\u5316\u8fc7\u53c2\u6570\u5316\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7406\u8bba\u8bc1\u660e\u4e0eL2,2/D\u6b63\u5219\u5316\u7b49\u4ef7\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u6784\u5316\u7a00\u758f\u6b63\u5219\u5316\u80fd\u6709\u6548\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u5176\u4e0d\u53ef\u5fae\u6027\u5bfc\u81f4\u4e0e\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u4e13\u95e8\u4f18\u5316\u5668\u6216\u540e\u5904\u7406\u526a\u679d\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "D-Gating\u65b9\u6cd5\u5c06\u6bcf\u7ec4\u6743\u91cd\u5206\u89e3\u4e3a\u4e3b\u6743\u91cd\u5411\u91cf\u548c\u591a\u4e2a\u6807\u91cf\u95e8\u63a7\u56e0\u5b50\uff0c\u5b9e\u73b0\u5b8c\u5168\u53ef\u5fae\u7684\u7ed3\u6784\u5316\u8fc7\u53c2\u6570\u5316\u3002", "result": "\u7406\u8bba\u8bc1\u660eD-Gating\u7684\u5c40\u90e8\u6700\u4f18\u89e3\u4e5f\u662f\u975e\u5149\u6ed1\u7ed3\u6784\u5316L2,2/D\u60e9\u7f5a\u7684\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u68af\u5ea6\u6d41\u6781\u9650\u4e0b\u4ee5\u6307\u6570\u901f\u5ea6\u6536\u655b\u5230L2,2/D\u6b63\u5219\u5316\u635f\u5931\u3002", "conclusion": "D-Gating\u5728\u7406\u8bba\u4e0a\u7b49\u4ef7\u4e8e\u89e3\u51b3\u539f\u59cb\u7ec4\u7a00\u758f\u95ee\u9898\uff0c\u4f46\u5177\u6709\u4ece\u975e\u7a00\u758f\u5230\u7a00\u758f\u4f18\u5316\u7684\u72ec\u7279\u5b66\u4e60\u52a8\u6001\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u7a00\u758f\u5ea6\u6743\u8861\u3002"}}
{"id": "2509.23905", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23905", "abs": "https://arxiv.org/abs/2509.23905", "authors": ["Tianjiao Sun", "Ningyan Guo", "Haozhe Gu", "Yanyan Peng", "Zhiyong Feng"], "title": "Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach", "comment": null, "summary": "The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication\nnetworks has become an increasingly vital approach for remediating coverage\nlimitations in infrastructure-deficient environments, with especially pressing\napplications in temporary scenarios, such as emergency rescue, military and\nsecurity operations, and remote area coverage. However, complex geographic\nenvironments lead to unpredictable and highly dynamic wireless channel\nconditions, resulting in frequent interruptions of air-to-ground (A2G) links\nthat severely constrain the reliability and quality of service in UAV\nswarm-assisted mobile communications. To improve the quality of UAV\nswarm-assisted communications in complex geographic environments, we propose an\nintegrated communication and control co-design mechanism. Given the stringent\nenergy constraints inherent in UAV swarms, our proposed mechanism is designed\nto optimize energy efficiency while maintaining an equilibrium between\nequitable communication rates for mobile ground users (GUs) and UAV energy\nexpenditure. We formulate the joint resource allocation and 3D trajectory\ncontrol problem as a Markov decision process (MDP), and develop a multi-agent\nreinforcement learning (MARL) framework to enable real-time coordinated actions\nacross the UAV swarm. To optimize the action policy of UAV swarms, we propose a\nnovel multi-agent hybrid proximal policy optimization with action masking\n(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action\nspaces. The algorithm incorporates action masking to enforce hard constraints\nin high-dimensional action spaces. Experimental results demonstrate that our\napproach achieves a fairness index of 0.99 while reducing energy consumption by\nup to 25% compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u7fa4\u901a\u4fe1\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u673a\u5236\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c3D\u8f68\u8ff9\u63a7\u5236\uff0c\u5728\u590d\u6742\u5730\u7406\u73af\u5883\u4e2d\u63d0\u9ad8\u901a\u4fe1\u8d28\u91cf\u548c\u80fd\u6548\u3002", "motivation": "\u590d\u6742\u5730\u7406\u73af\u5883\u5bfc\u81f4\u65e0\u4eba\u673a\u5bf9\u5730\u94fe\u8def\u9891\u7e41\u4e2d\u65ad\uff0c\u4e25\u91cd\u5236\u7ea6\u65e0\u4eba\u673a\u7fa4\u8f85\u52a9\u79fb\u52a8\u901a\u4fe1\u7684\u53ef\u9760\u6027\u548c\u670d\u52a1\u8d28\u91cf\uff0c\u9700\u8981\u89e3\u51b3\u80fd\u6548\u4e0e\u901a\u4fe1\u516c\u5e73\u6027\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5c06\u8054\u5408\u8d44\u6e90\u5206\u914d\u548c3D\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51faMAHPPO-AM\u7b97\u6cd5\u5904\u7406\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e860.99\u7684\u516c\u5e73\u6027\u6307\u6570\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe25%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u901a\u4fe1\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u7fa4\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u8d28\u91cf\u548c\u80fd\u6548\u8868\u73b0\u3002"}}
{"id": "2509.23923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23923", "abs": "https://arxiv.org/abs/2509.23923", "authors": ["Maya Bechler-Speicher", "Andrea Zerio", "Maor Huri", "Marie Vibeke Vestergaard", "Ran Gilad-Bachrach", "Tine Jess", "Samir Bhatt", "Aleksejs Sazonovs"], "title": "Graph Mixing Additive Networks", "comment": "arXiv admin note: substantial text overlap with arXiv:2505.19193", "summary": "We introduce GMAN, a flexible, interpretable, and expressive framework that\nextends Graph Neural Additive Networks (GNANs) to learn from sets of sparse\ntime-series data. GMAN represents each time-dependent trajectory as a directed\ngraph and applies an enriched, more expressive GNAN to each graph. It allows\nusers to control the interpretability-expressivity trade-off by grouping\nfeatures and graphs to encode priors, and it provides feature, node, and\ngraph-level interpretability. On real-world datasets, including mortality\nprediction from blood tests and fake-news detection, GMAN outperforms strong\nnon-interpretable black-box baselines while delivering actionable,\ndomain-aligned explanations.", "AI": {"tldr": "GMAN\u662f\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u56fe\u795e\u7ecf\u52a0\u6027\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u3002\u5b83\u5c06\u6bcf\u4e2a\u65f6\u95f4\u76f8\u5173\u8f68\u8ff9\u8868\u793a\u4e3a\u6709\u5411\u56fe\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u56fe\u5e94\u7528\u66f4\u4e30\u5bcc\u7684GNAN\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u7279\u5f81\u548c\u56fe\u7684\u7ec4\u5408\u6765\u63a7\u5236\u53ef\u89e3\u91ca\u6027\u4e0e\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u9ed1\u76d2\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u63d0\u4f9b\u9886\u57df\u5bf9\u9f50\u7684\u53ef\u64cd\u4f5c\u89e3\u91ca\u3002GMAN\u65e8\u5728\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u591a\u5c42\u6b21\u7684\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u5c06\u6bcf\u4e2a\u65f6\u95f4\u76f8\u5173\u8f68\u8ff9\u8868\u793a\u4e3a\u6709\u5411\u56fe\uff0c\u5e94\u7528\u589e\u5f3a\u7684\u56fe\u795e\u7ecf\u52a0\u6027\u7f51\u7edc\uff08GNAN\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u5206\u7ec4\u548c\u56fe\u5206\u7ec4\u6765\u7f16\u7801\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u7279\u5f81\u7ea7\u3001\u8282\u70b9\u7ea7\u548c\u56fe\u7ea7\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u5305\u62ec\u8840\u6db2\u68c0\u6d4b\u6b7b\u4ea1\u7387\u9884\u6d4b\u548c\u5047\u65b0\u95fb\u68c0\u6d4b\u5728\u5185\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cGMAN\u4f18\u4e8e\u5f3a\u5927\u7684\u975e\u53ef\u89e3\u91ca\u9ed1\u76d2\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u3001\u9886\u57df\u5bf9\u9f50\u7684\u89e3\u91ca\u3002", "conclusion": "GMAN\u6846\u67b6\u6210\u529f\u5730\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u591a\u5c42\u6b21\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u53ef\u89e3\u91ca\u6a21\u578b\u53ef\u4ee5\u8d85\u8d8a\u9ed1\u76d2\u65b9\u6cd5\u3002"}}
{"id": "2509.23928", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23928", "abs": "https://arxiv.org/abs/2509.23928", "authors": ["Zhinan Xie", "Peisong Wang", "Jian Cheng"], "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models", "comment": null, "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.", "AI": {"tldr": "HiViS\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u6d4b\u89e3\u7801\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u4ee4\u724c\u4ece\u8349\u7a3f\u6a21\u578b\u4e2d\u9690\u85cf\uff0c\u4ec5\u4fdd\u7559\u6587\u672c\u4ee4\u724c\u4f5c\u4e3a\u663e\u5f0f\u8f93\u5165\uff0c\u540c\u65f6\u91cd\u7528\u76ee\u6807VLM\u7684\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u9690\u5f0f\u89c6\u89c9\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e862.65\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u63a8\u6d4b\u89e3\u7801\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6709\u6548\uff0c\u4f46\u5e94\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65f6\u9762\u4e34\u6311\u6218\uff1a\u89c6\u89c9\u4ee4\u724c\u5bfc\u81f4\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807VLM\u8bed\u4e49\u8868\u793a\u4e0d\u5339\u914d\uff0c\u4ee5\u53ca\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u51cf\u6162\u8349\u7a3f\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u663e\u5f0f-\u9690\u5f0f\u8f93\u5165\u5206\u89e3\u6846\u67b6\uff0c\u4ece\u8349\u7a3f\u6a21\u578b\u4e2d\u79fb\u9664\u6240\u6709\u89c6\u89c9\u4ee4\u724c\uff0c\u4ec5\u4fdd\u7559\u6587\u672c\u4ee4\u724c\u4f5c\u4e3a\u663e\u5f0f\u8f93\u5165\uff0c\u76f4\u63a5\u91cd\u7528\u76ee\u6807VLM\u7684\u5bf9\u5e94\u6700\u540e\u4e00\u5c42\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u9690\u5f0f\u89c6\u89c9\u4fe1\u606f\u3002\u91c7\u7528\u591a\u6b65\u81ea\u53cd\u9988\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5c06\u8349\u7a3f\u6a21\u578b\u7684\u9884\u586b\u5145\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u5230\u76ee\u6807VLM\u8f93\u5165\u76840.7%-1.3%\uff0c\u540c\u65f6\u4fdd\u6301\u65e0\u635f\u751f\u6210\u8d28\u91cf\u3002\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u6700\u9ad8\u53ef\u8fbe2.65\u500d\u7684\u52a0\u901f\u6548\u679c\u3002", "conclusion": "HiViS\u901a\u8fc7\u9690\u85cf\u89c6\u89c9\u4ee4\u724c\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86VLM\u4e2d\u63a8\u6d4b\u89e3\u7801\u7684\u6311\u6218\uff0c\u663e\u8457\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.23933", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23933", "abs": "https://arxiv.org/abs/2509.23933", "authors": ["Jiahao Ying", "Mingbao Lin", "Qianru Sun", "Yixin Cao"], "title": "Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising direction,\noffering efficiency and scalability by activating only a subset of parameters\nduring inference. However, current research remains largely\nperformance-centric, with limited understanding of its internal mechanisms,\nthereby constraining broader progress. In this work, we use an internal metric\nto investigate the mechanisms of MoE architecture by explicitly incorporating\nrouting mechanisms and analyzing expert-level behaviors. Through systematic\nanalyses of a wide range of publicly available MoE models, we uncover several\nfindings: (1) neuron utilization decreases as models evolve, reflecting\nstronger generalization; (2) training exhibits a dynamic trajectory, where\nbenchmark performance alone provides limited signal while MUI reveals deeper\ninsights; (3) task completion emerges from collaborative contributions of\nmultiple experts, with shared experts driving concentration; and (4) activation\npatterns at the neuron level provide a fine-grained proxy for data diversity.\nTogether, these results demonstrate the potential of MUI as a complementary\nindicator to benchmark performance, offering new insights into the capacity,\ndynamics, and specialization of MoE models. Our project can be found at\nhttps://yingjiahao14.github.io/MoE-MUI/.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165MUI\u6307\u6807\u7cfb\u7edf\u5206\u6790MoE\u67b6\u6784\u7684\u5185\u90e8\u673a\u5236\uff0c\u53d1\u73b0\u795e\u7ecf\u5143\u5229\u7528\u7387\u968f\u6a21\u578b\u8fdb\u5316\u800c\u4e0b\u964d\u53cd\u6620\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u5448\u73b0\u52a8\u6001\u8f68\u8ff9\uff0c\u4efb\u52a1\u5b8c\u6210\u4f9d\u8d56\u591a\u4e13\u5bb6\u534f\u4f5c\uff0c\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u53ef\u4f5c\u4e3a\u6570\u636e\u591a\u6837\u6027\u4ee3\u7406\u6307\u6807\u3002", "motivation": "\u5f53\u524dMoE\u67b6\u6784\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\u8868\u73b0\uff0c\u5bf9\u5176\u5185\u90e8\u673a\u5236\u7406\u89e3\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5185\u90e8\u6307\u6807\u6df1\u5165\u63a2\u7a76MoE\u67b6\u6784\u7684\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5185\u90e8\u6307\u6807MUI\uff0c\u660e\u786e\u7ed3\u5408\u8def\u7531\u673a\u5236\u5e76\u5206\u6790\u4e13\u5bb6\u7ea7\u884c\u4e3a\uff0c\u7cfb\u7edf\u5206\u6790\u591a\u79cd\u516c\u5f00\u53ef\u7528\u7684MoE\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u795e\u7ecf\u5143\u5229\u7528\u7387\u968f\u6a21\u578b\u8fdb\u5316\u4e0b\u964d\u53cd\u6620\u66f4\u5f3a\u6cdb\u5316\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u5448\u73b0\u52a8\u6001\u8f68\u8ff9\uff1b\u4efb\u52a1\u5b8c\u6210\u4f9d\u8d56\u591a\u4e13\u5bb6\u534f\u4f5c\uff0c\u5171\u4eab\u4e13\u5bb6\u9a71\u52a8\u96c6\u4e2d\uff1b\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u53ef\u4f5c\u4e3a\u6570\u636e\u591a\u6837\u6027\u7ec6\u7c92\u5ea6\u4ee3\u7406\u6307\u6807\u3002", "conclusion": "MUI\u53ef\u4f5c\u4e3a\u57fa\u51c6\u6027\u80fd\u7684\u8865\u5145\u6307\u6807\uff0c\u4e3a\u7406\u89e3MoE\u6a21\u578b\u7684\u5bb9\u91cf\u3001\u52a8\u6001\u548c\u4e13\u4e1a\u5316\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.23937", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.23937", "abs": "https://arxiv.org/abs/2509.23937", "authors": ["Akhil Premkumar"], "title": "Diffusion Models are Kelly Gamblers", "comment": "26 pages + references, 13 figures", "summary": "We draw a connection between diffusion models and the Kelly criterion for\nmaximizing returns in betting games. We find that conditional diffusion models\nstore additional information to bind the signal $X$ with the conditioning\ninformation $Y$, equal to the mutual information between them. Classifier-free\nguidance effectively boosts the mutual information between $X$ and $Y$ at\nsampling time. This is especially helpful in image models, since the mutual\ninformation between images and their labels is low, a fact which is intimately\nconnected to the manifold hypothesis. Finally, we point out some nuances in the\npopular perspective that diffusion models are infinitely deep autoencoders. In\ndoing so, we relate the denoising loss to the Fermi Golden Rule from quantum\nmechanics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u6269\u6563\u6a21\u578b\u4e0e\u51ef\u5229\u51c6\u5219\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u53d1\u73b0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b58\u50a8\u4e86\u4fe1\u53f7X\u4e0e\u6761\u4ef6\u4fe1\u606fY\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u5728\u91c7\u6837\u65f6\u80fd\u6709\u6548\u63d0\u5347\u8fd9\u79cd\u4e92\u4fe1\u606f\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u4e0e\u51ef\u5229\u51c6\u5219\u5728\u8d4c\u535a\u6e38\u620f\u4e2d\u7684\u8054\u7cfb\uff0c\u7406\u89e3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u4fe1\u606f\u5b58\u50a8\u673a\u5236\uff0c\u4ee5\u53ca\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u5bf9\u4e92\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8fde\u63a5\u6269\u6563\u6a21\u578b\u4e0e\u51ef\u5229\u51c6\u5219\uff0c\u7814\u7a76\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u4fe1\u606f\u5b58\u50a8\u673a\u5236\uff0c\u5206\u6790\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u5bf9\u4e92\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b58\u50a8\u7684\u989d\u5916\u4fe1\u606f\u7b49\u4e8eX\u4e0eY\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u80fd\u6709\u6548\u63d0\u5347\u91c7\u6837\u65f6\u7684\u4e92\u4fe1\u606f\uff0c\u8fd9\u5728\u56fe\u50cf\u6a21\u578b\u4e2d\u7279\u522b\u6709\u7528\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e0e\u51ef\u5229\u51c6\u5219\u5b58\u5728\u6df1\u523b\u8054\u7cfb\uff0c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u901a\u8fc7\u63d0\u5347\u4e92\u4fe1\u606f\u6539\u5584\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5c06\u6269\u6563\u6a21\u578b\u89c6\u4e3a\u65e0\u9650\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u89c2\u70b9\u5b58\u5728\u7ec6\u5fae\u5dee\u522b\u3002"}}
{"id": "2509.23941", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.23941", "abs": "https://arxiv.org/abs/2509.23941", "authors": ["Victoria Bosch", "Daniel Anthes", "Adrien Doerig", "Sushrut Thorat", "Peter K\u00f6nig", "Tim Christian Kietzmann"], "title": "Brain-language fusion enables interactive neural readout and in-silico experimentation", "comment": null, "summary": "Large language models (LLMs) have revolutionized human-machine interaction,\nand have been extended by embedding diverse modalities such as images into a\nshared language space. Yet, neural decoding has remained constrained by static,\nnon-interactive methods. We introduce CorText, a framework that integrates\nneural activity directly into the latent space of an LLM, enabling open-ended,\nnatural language interaction with brain data. Trained on fMRI data recorded\nduring viewing of natural scenes, CorText generates accurate image captions and\ncan answer more detailed questions better than controls, while having access to\nneural data only. We showcase that CorText achieves zero-shot generalization\nbeyond semantic categories seen during training. Furthermore, we present a\ncounterfactual analysis that emulates in-silico cortical microstimulation.\nThese advances mark a shift from passive decoding toward generative, flexible\ninterfaces between brain activity and language.", "AI": {"tldr": "CorText\u6846\u67b6\u5c06\u795e\u7ecf\u6d3b\u52a8\u76f4\u63a5\u96c6\u6210\u5230LLM\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e0e\u8111\u6570\u636e\u7684\u5f00\u653e\u5f0f\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u56fe\u50cf\u63cf\u8ff0\u5e76\u56de\u7b54\u8be6\u7ec6\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u89e3\u7801\u65b9\u6cd5\u53d7\u9650\u4e8e\u9759\u6001\u3001\u975e\u4ea4\u4e92\u7684\u65b9\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u5927\u8111\u6d3b\u52a8\u4e0e\u8bed\u8a00\u4e4b\u95f4\u751f\u6210\u6027\u3001\u7075\u6d3b\u63a5\u53e3\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8efMRI\u6570\u636e\u8bad\u7ec3\uff0c\u5c06\u795e\u7ecf\u6d3b\u52a8\u76f4\u63a5\u5d4c\u5165\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4e0e\u8111\u6570\u636e\u7684\u4ea4\u4e92\u3002", "result": "CorText\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u6bd4\u5bf9\u7167\u7ec4\u66f4\u597d\u5730\u56de\u7b54\u8be6\u7ec6\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u8bad\u7ec3\u65f6\u89c1\u8fc7\u7684\u8bed\u4e49\u7c7b\u522b\u3002", "conclusion": "\u8fd9\u4e9b\u8fdb\u5c55\u6807\u5fd7\u7740\u4ece\u88ab\u52a8\u89e3\u7801\u5411\u5927\u8111\u6d3b\u52a8\u4e0e\u8bed\u8a00\u4e4b\u95f4\u751f\u6210\u6027\u3001\u7075\u6d3b\u63a5\u53e3\u7684\u8f6c\u53d8\u3002"}}
{"id": "2509.23942", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.23942", "abs": "https://arxiv.org/abs/2509.23942", "authors": ["John N. Daras"], "title": "Efficient Identification of High Similarity Clusters in Polygon Datasets", "comment": "11 pages, 3 figures", "summary": "Advancements in tools like Shapely 2.0 and Triton can significantly improve\nthe efficiency of spatial similarity computations by enabling faster and more\nscalable geometric operations. However, for extremely large datasets, these\noptimizations may face challenges due to the sheer volume of computations\nrequired. To address this, we propose a framework that reduces the number of\nclusters requiring verification, thereby decreasing the computational load on\nthese systems. The framework integrates dynamic similarity index thresholding,\nsupervised scheduling, and recall-constrained optimization to efficiently\nidentify clusters with the highest spatial similarity while meeting\nuser-defined precision and recall requirements. By leveraging Kernel Density\nEstimation (KDE) to dynamically determine similarity thresholds and machine\nlearning models to prioritize clusters, our approach achieves substantial\nreductions in computational cost without sacrificing accuracy. Experimental\nresults demonstrate the scalability and effectiveness of the method, offering a\npractical solution for large-scale geospatial analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u9700\u8981\u9a8c\u8bc1\u7684\u805a\u7c7b\u6570\u91cf\u6765\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u7ed3\u5408\u52a8\u6001\u76f8\u4f3c\u6027\u7d22\u5f15\u9608\u503c\u3001\u76d1\u7763\u8c03\u5ea6\u548c\u53ec\u56de\u7ea6\u675f\u4f18\u5316\uff0c\u5728\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002", "motivation": "\u867d\u7136Shapely 2.0\u548cTriton\u7b49\u5de5\u5177\u80fd\u63d0\u9ad8\u7a7a\u95f4\u76f8\u4f3c\u6027\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u5728\u5904\u7406\u6781\u5927\u6570\u636e\u96c6\u65f6\u4ecd\u9762\u4e34\u8ba1\u7b97\u91cf\u8fc7\u5927\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u52a8\u6001\u786e\u5b9a\u76f8\u4f3c\u6027\u9608\u503c\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f18\u5148\u5904\u7406\u805a\u7c7b\uff0c\u96c6\u6210\u52a8\u6001\u76f8\u4f3c\u6027\u7d22\u5f15\u9608\u503c\u3001\u76d1\u7763\u8c03\u5ea6\u548c\u53ec\u56de\u7ea6\u675f\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u5177\u6709\u6700\u9ad8\u7a7a\u95f4\u76f8\u4f3c\u6027\u7684\u805a\u7c7b\uff0c\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u8981\u6c42\u3002"}}
{"id": "2509.23946", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23946", "abs": "https://arxiv.org/abs/2509.23946", "authors": ["Kaisen Yang", "Lixuan He", "Rushi Shah", "Kaicheng Yang", "Qinwei Ma", "Dianbo Liu", "Alex Lamb"], "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "comment": "Under review ICLR 2026", "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning\nabilities of Large Language Models (LLMs), yet their monolithic and\nauto-regressive architecture inherently conflates high-level strategic planning\nwith low-level step-by-step execution, leading to computational inefficiency,\nlimited exploration of reasoning paths, and reduced interpretability. To\novercome these issues, we propose the Explore-Execute Chain ($E^2C$), a\nstructured reasoning framework that decouples reasoning into two distinct\nphases: an exploratory phase that stochastically generates succinct high-level\nplans, followed by an execution phase that deterministically carries out the\nchosen plan. Our approach incorporates a two-stage training methodology, which\ncombines Supervised Fine-Tuning (SFT) - augmented by a novel data generation\nalgorithm enforcing strict plan adherence - with a subsequent Reinforcement\nLearning (RL) stage that capitalizes on the informativeness of exploration and\nreinforces the determinism of execution.This decomposition enables an efficient\ntest-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches\n58.1% accuracy using <10% of the decoding tokens required by comparable methods\n(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For\ncross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with\nonly 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher\naccuracy than standard SFT on medical benchmarks, delivering state-of-the-art\nperformance, strong generalization, and greater interpretability by separating\nplanning from execution. The code and pre-trained models for the project are\navailable at: https://github.com/yks23/Explore-Execute-Chain.git", "AI": {"tldr": "\u63d0\u51fa\u4e86Explore-Execute Chain (E\u00b2C)\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u63a2\u7d22\u9636\u6bb5\uff08\u751f\u6210\u9ad8\u5c42\u8ba1\u5212\uff09\u548c\u6267\u884c\u9636\u6bb5\uff08\u6267\u884c\u9009\u5b9a\u8ba1\u5212\uff09\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfChain-of-Thought\u65b9\u6cd5\u5c06\u9ad8\u5c42\u7b56\u7565\u89c4\u5212\u4e0e\u4f4e\u5c42\u9010\u6b65\u6267\u884c\u6df7\u4e3a\u4e00\u8c08\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3001\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u6709\u9650\u548c\u53ef\u89e3\u91ca\u6027\u964d\u4f4e\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u9ad8\u6548\u6269\u5c55\u7b56\u7565\uff0c\u63a2\u7d22\u9636\u6bb5\u968f\u673a\u751f\u6210\u7b80\u6d01\u9ad8\u5c42\u8ba1\u5212\uff0c\u6267\u884c\u9636\u6bb5\u786e\u5b9a\u6027\u6267\u884c\u9009\u5b9a\u8ba1\u5212\u3002", "result": "\u5728AIME'2024\u4e0a\u8fbe\u523058.1%\u51c6\u786e\u7387\uff0c\u4ec5\u4f7f\u7528\u4e0d\u523010%\u7684\u89e3\u7801token\uff1b\u5728\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u6807\u51c6SFT\u51c6\u786e\u7387\u9ad814.5%\uff0c\u4ec5\u4f7f\u75283.5%\u7684token\u3002", "conclusion": "E\u00b2C\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3001\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u8457\u964d\u4f4e\u81ea\u4e00\u81f4\u6027\u5f00\u9500\u3002"}}
{"id": "2509.23948", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23948", "abs": "https://arxiv.org/abs/2509.23948", "authors": ["Surya Murthy", "Kushagra Gupta", "Mustafa O. Karabag", "David Fridovich-Keil", "Ufuk Topcu"], "title": "DiBS-MTL: Transformation-Invariant Multitask Learning with Direction Oracles", "comment": null, "summary": "Multitask learning (MTL) algorithms typically rely on schemes that combine\ndifferent task losses or their gradients through weighted averaging. These\nmethods aim to find Pareto stationary points by using heuristics that require\naccess to task loss values, gradients, or both. In doing so, a central\nchallenge arises because task losses can be arbitrarily, nonaffinely scaled\nrelative to one another, causing certain tasks to dominate training and degrade\noverall performance. A recent advance in cooperative bargaining theory, the\nDirection-based Bargaining Solution (DiBS), yields Pareto stationary solutions\nimmune to task domination because of its invariance to monotonic nonaffine task\nloss transformations. However, the convergence behavior of DiBS in nonconvex\nMTL settings is currently not understood. To this end, we prove that under\nstandard assumptions, a subsequence of DiBS iterates converges to a Pareto\nstationary point when task losses are possibly nonconvex, and propose DiBS-MTL,\na computationally efficient adaptation of DiBS to the MTL setting. Finally, we\nvalidate DiBS-MTL empirically on standard MTL benchmarks, showing that it\nachieves competitive performance with state-of-the-art methods while\nmaintaining robustness to nonaffine monotonic transformations that\nsignificantly degrade the performance of existing approaches, including prior\nbargaining-inspired MTL methods. Code available at\nhttps://github.com/suryakmurthy/dibs-mtl.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiBS-MTL\u65b9\u6cd5\uff0c\u57fa\u4e8e\u65b9\u5411\u6027\u8c08\u5224\u89e3\u51b3\u65b9\u6848(DiBS)\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u4efb\u52a1\u635f\u5931\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u5bfc\u81f4\u67d0\u4e9b\u4efb\u52a1\u4e3b\u5bfc\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5355\u8c03\u975e\u7ebf\u6027\u53d8\u6362\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u4efb\u52a1\u5b66\u4e60\u7b97\u6cd5\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u7ec4\u5408\u4efb\u52a1\u635f\u5931\u6216\u68af\u5ea6\uff0c\u4f46\u4efb\u52a1\u635f\u5931\u53ef\u80fd\u56e0\u975e\u7ebf\u6027\u5c3a\u5ea6\u53d8\u6362\u800c\u4efb\u610f\u7f29\u653e\uff0c\u5bfc\u81f4\u67d0\u4e9b\u4efb\u52a1\u4e3b\u5bfc\u8bad\u7ec3\u5e76\u964d\u4f4e\u6574\u4f53\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u5408\u4f5c\u535a\u5f08\u7406\u8bba\u4e2d\u7684\u65b9\u5411\u6027\u8c08\u5224\u89e3\u51b3\u65b9\u6848(DiBS)\uff0c\u5f00\u53d1\u4e86DiBS-MTL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5355\u8c03\u975e\u7ebf\u6027\u4efb\u52a1\u635f\u5931\u53d8\u6362\u5177\u6709\u4e0d\u53d8\u6027\uff0c\u786e\u4fdd\u6536\u655b\u5230\u5e15\u7d2f\u6258\u7a33\u5b9a\u70b9\u3002", "result": "\u5728\u6807\u51c6\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiBS-MTL\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u5bf9\u663e\u8457\u964d\u4f4e\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u7684\u975e\u7ebf\u6027\u5355\u8c03\u53d8\u6362\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "DiBS-MTL\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u4efb\u52a1\u635f\u5931\u5c3a\u5ea6\u53d8\u6362\u9c81\u68d2\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u4e3b\u5bfc\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.23963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23963", "abs": "https://arxiv.org/abs/2509.23963", "authors": ["Rylan Schaeffer", "Noam Levi", "Andreas Kirsch", "Theo Guenais", "Brando Miranda", "Elyas Obbad", "Sanmi Koyejo"], "title": "Evaluating the Robustness of Chinchilla Compute-Optimal Scaling", "comment": null, "summary": "Hoffman et al (2022)'s Chinchilla paper introduced the principle of\ncompute-optimal scaling, laying a foundation for future scaling of language\nmodels. In the years since, however, valid concerns about Chinchilla have been\nraised: wide confidence intervals, discrepancies between its three approaches,\nand incongruities with other scaling laws. This raises a critical question for\nthe field: Can practitioners still rely on Chinchilla's prescriptions? Our work\ndemonstrates the answer is yes. We begin by uncovering that the model\nparameters central to Chinchilla's analyses were ambiguous: three\ninterpretations are possible, with relative differences between different\ninterpretations of model parameters as high as 15.2%. We find that, perhaps\nsurprisingly, which model parameters are used for the analyses do not\nmeaningfully affect key results: the scaling law estimates and the\ncompute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,\nthe tokens-to-parameter ratio becomes more constant with the target compute\nbudget. We then ask how distorted the Chinchilla model parameters could have\nbeen without meaningfully affecting the key results. By deliberately perturbing\nmodel parameters in four structured ways, we find that key Chinchilla results\nare most sensitive to additive or systematic errors, which can alter the\notherwise flat trend of the optimal tokens-to-parameter ratio, but overall,\nChinchilla's key results withstand sizable perturbations. Altogether, our\nfindings offer the field renewed confidence in Chinchilla as a durable guide\nfor scaling language models.", "AI": {"tldr": "\u672c\u6587\u9a8c\u8bc1\u4e86Chinchilla\u7f29\u653e\u5b9a\u5f8b\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5b58\u5728\u53c2\u6570\u5b9a\u4e49\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u4f46\u5176\u6838\u5fc3\u7ed3\u679c\uff08\u8ba1\u7b97\u6700\u4f18\u7684token-\u53c2\u6570\u6bd4\u4f8b\uff09\u5bf9\u53c2\u6570\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "Chinchilla\u8bba\u6587\u63d0\u51fa\u4e86\u8ba1\u7b97\u6700\u4f18\u7f29\u653e\u539f\u5219\uff0c\u4f46\u540e\u7eed\u7814\u7a76\u5bf9\u5176\u6709\u6548\u6027\u63d0\u51fa\u8d28\u7591\uff0c\u5305\u62ec\u7f6e\u4fe1\u533a\u95f4\u5bbd\u3001\u4e0d\u540c\u65b9\u6cd5\u95f4\u5b58\u5728\u5dee\u5f02\u7b49\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1Chinchilla\u7684\u6307\u5bfc\u539f\u5219\u662f\u5426\u4ecd\u7136\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u5206\u6790Chinchilla\u4e2d\u6a21\u578b\u53c2\u6570\u7684\u4e09\u79cd\u53ef\u80fd\u89e3\u91ca\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u5bf9\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u56db\u79cd\u7ed3\u6784\u5316\u6270\u52a8\uff0c\u8bc4\u4f30\u5173\u952e\u7ed3\u679c\u5bf9\u53c2\u6570\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u53c2\u6570\u7684\u4e0d\u540c\u89e3\u91ca\u5bf9\u5173\u952e\u7ed3\u679c\u5f71\u54cd\u4e0d\u5927\uff0c\u8ba1\u7b97\u6700\u4f18\u7684token-\u53c2\u6570\u6bd4\u4f8b\u5728\u53c2\u6570\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0cChinchilla\u7684\u6838\u5fc3\u7ed3\u8bba\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "Chinchilla\u7684\u7f29\u653e\u5b9a\u5f8b\u4ecd\u7136\u662f\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u7684\u53ef\u9760\u6307\u5bfc\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u65b0\u4fe1\u5fc3\u3002"}}
{"id": "2509.23964", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23964", "abs": "https://arxiv.org/abs/2509.23964", "authors": ["Dang Huu-Tien", "Naoya Inoue"], "title": "Detecting and Rectifying Noisy Labels: A Similarity-based Approach", "comment": null, "summary": "Label noise in datasets could damage the performance of neural net training.\nAs the size of modern deep networks grows, there is a growing demand for\nautomated tools for detecting such errors. In this paper, we propose post-hoc,\nmodel-agnostic error detection and rectification methods utilizing the\npenultimate feature from a neural network. Our idea is based on the observation\nthat the similarity between the penultimate feature of a mislabeled data point\nand its true class data points is higher than that for data points from other\nclasses, making the probability of label occurrence within a tight, similar\ncluster informative for detecting and rectifying errors. Extensive experiments\nshow our method not only demonstrates high performance across various noises\nbut also automatically rectifies these errors to improve the quality of\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u5012\u6570\u7b2c\u4e8c\u5c42\u7279\u5f81\u7684\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u4e0e\u4fee\u6b63\u65b9\u6cd5\uff0c\u5229\u7528\u9519\u8bef\u6807\u7b7e\u6837\u672c\u4e0e\u5176\u771f\u5b9e\u7c7b\u522b\u6837\u672c\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u6765\u8bc6\u522b\u548c\u7ea0\u6b63\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u7f51\u7edc\u89c4\u6a21\u589e\u5927\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7684\u9519\u8bef\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5012\u6570\u7b2c\u4e8c\u5c42\u7279\u5f81\uff0c\u57fa\u4e8e\u9519\u8bef\u6807\u7b7e\u6837\u672c\u4e0e\u5176\u771f\u5b9e\u7c7b\u522b\u6837\u672c\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u9ad8\u4e8e\u5176\u4ed6\u7c7b\u522b\u6837\u672c\u7684\u89c2\u5bdf\uff0c\u901a\u8fc7\u7d27\u5bc6\u76f8\u4f3c\u7c07\u4e2d\u7684\u6807\u7b7e\u51fa\u73b0\u6982\u7387\u6765\u68c0\u6d4b\u548c\u4fee\u6b63\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u566a\u58f0\u7c7b\u578b\u4e0b\u90fd\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u5e76\u80fd\u81ea\u52a8\u4fee\u6b63\u9519\u8bef\u4ee5\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u4fee\u6b63\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5904\u7406\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002"}}
{"id": "2509.23976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23976", "abs": "https://arxiv.org/abs/2509.23976", "authors": ["Maruf Ahmed Mridul", "Oshani Seneviratne"], "title": "Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts", "comment": "8 pages, 3 figures, 2 tables", "summary": "Smart contract-based automation of financial derivatives offers substantial\nefficiency gains, but its real-world adoption is constrained by the complexity\nof translating financial specifications into gas-efficient executable code. In\nparticular, generating code that is both functionally correct and economically\nviable from high-level specifications, such as the Common Domain Model (CDM),\nremains a significant challenge. This paper introduces a Reinforcement Learning\n(RL) framework to generate functional and gas-optimized Solidity smart\ncontracts directly from CDM specifications. We employ a Proximal Policy\nOptimization (PPO) agent that learns to select optimal code snippets from a\npre-defined library. To manage the complex search space, a two-phase curriculum\nfirst trains the agent for functional correctness before shifting its focus to\ngas optimization. Our empirical results show the RL agent learns to generate\ncontracts with significant gas savings, achieving cost reductions of up to\n35.59% on unseen test data compared to unoptimized baselines. This work\npresents a viable methodology for the automated synthesis of reliable and\neconomically sustainable smart contracts, bridging the gap between high-level\nfinancial agreements and efficient on-chain execution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4eceCDM\u89c4\u8303\u751f\u6210\u529f\u80fd\u6b63\u786e\u4e14\u71c3\u6c14\u4f18\u5316\u7684Solidity\u667a\u80fd\u5408\u7ea6\uff0c\u5b9e\u73b0\u9ad8\u8fbe35.59%\u7684\u71c3\u6c14\u8282\u7701\u3002", "motivation": "\u91d1\u878d\u884d\u751f\u54c1\u667a\u80fd\u5408\u7ea6\u81ea\u52a8\u5316\u80fd\u5e26\u6765\u6548\u7387\u63d0\u5347\uff0c\u4f46\u5c06\u91d1\u878d\u89c4\u8303\u8f6c\u5316\u4e3a\u71c3\u6c14\u9ad8\u6548\u7684\u53ef\u6267\u884c\u4ee3\u7801\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u4ece\u9ad8\u7ea7\u89c4\u8303\u751f\u6210\u529f\u80fd\u6b63\u786e\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u4ee3\u7801\u3002", "method": "\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u667a\u80fd\u4f53\u4ece\u9884\u5b9a\u4e49\u5e93\u4e2d\u9009\u62e9\u6700\u4f18\u4ee3\u7801\u7247\u6bb5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u5148\u8bad\u7ec3\u529f\u80fd\u6b63\u786e\u6027\uff0c\u518d\u5173\u6ce8\u71c3\u6c14\u4f18\u5316\u3002", "result": "RL\u667a\u80fd\u4f53\u5b66\u4f1a\u751f\u6210\u663e\u8457\u8282\u7701\u71c3\u6c14\u7684\u5408\u7ea6\uff0c\u5728\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u4e0a\u76f8\u6bd4\u672a\u4f18\u5316\u57fa\u7ebf\u5b9e\u73b0\u9ad8\u8fbe35.59%\u7684\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u81ea\u52a8\u5408\u6210\u53ef\u9760\u4e14\u7ecf\u6d4e\u53ef\u6301\u7eed\u7684\u667a\u80fd\u5408\u7ea6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u9ad8\u7ea7\u91d1\u878d\u534f\u8bae\u4e0e\u9ad8\u6548\u94fe\u4e0a\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.23992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23992", "abs": "https://arxiv.org/abs/2509.23992", "authors": ["Amartya Roy", "Devharish N", "Shreya Ganguly", "Kripabandhu Ghosh"], "title": "Guide: Generalized-Prior and Data Encoders for DAG Estimation", "comment": null, "summary": "Modern causal discovery methods face critical limitations in scalability,\ncomputational efficiency, and adaptability to mixed data types, as evidenced by\nbenchmarks on node scalability (30, $\\le 50$, $\\ge 70$ nodes), computational\nenergy demands, and continuous/non-continuous data handling. While traditional\nalgorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,\nexhibiting prohibitive energy costs for higher-order nodes and poor scalability\nbeyond 70 nodes, we propose \\textbf{GUIDE}, a framework that integrates Large\nLanguage Model (LLM)-generated adjacency matrices with observational data\nthrough a dual-encoder architecture. GUIDE uniquely optimizes computational\nefficiency, reducing runtime on average by $\\approx 42%$ compared to RL-BIC and\nKCRL methods, while achieving an average $\\approx 117%$ improvement in accuracy\nover both NOTEARS and GraN-DAG individually. During training, GUIDE's\nreinforcement learning agent dynamically balances reward maximization\n(accuracy) and penalty avoidance (DAG constraints), enabling robust performance\nacross mixed data types and scalability to $\\ge 70$ nodes -- a setting where\nbaseline methods fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86GUIDE\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210LLM\u751f\u6210\u7684\u90bb\u63a5\u77e9\u9635\u548c\u89c2\u6d4b\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6df7\u5408\u6570\u636e\u7c7b\u578b\u9002\u5e94\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff08\u5982PC\u3001GES\u3001ICA-LiNGAM\uff09\u9762\u4e34\u53ef\u6269\u5c55\u6027\u5dee\u3001\u8ba1\u7b97\u80fd\u8017\u9ad8\u3001\u96be\u4ee5\u5904\u7406\u6df7\u5408\u6570\u636e\u7c7b\u578b\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8282\u70b9\u6570\u8d85\u8fc770\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "GUIDE\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5c06LLM\u751f\u6210\u7684\u90bb\u63a5\u77e9\u9635\u4e0e\u89c2\u6d4b\u6570\u636e\u96c6\u6210\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u52a8\u6001\u5e73\u8861\u5956\u52b1\u6700\u5927\u5316\uff08\u51c6\u786e\u6027\uff09\u548c\u60e9\u7f5a\u907f\u514d\uff08DAG\u7ea6\u675f\uff09\u3002", "result": "\u76f8\u6bd4RL-BIC\u548cKCRL\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u5e73\u5747\u51cf\u5c11\u7ea642%\uff1b\u76f8\u6bd4NOTEARS\u548cGraN-DAG\uff0c\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8\u7ea6117%\uff1b\u80fd\u591f\u6269\u5c55\u523070\u4e2a\u4ee5\u4e0a\u8282\u70b9\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u5728\u6b64\u89c4\u6a21\u4e0b\u5931\u6548\u3002", "conclusion": "GUIDE\u6846\u67b6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6df7\u5408\u6570\u636e\u7c7b\u578b\u5e76\u5b9e\u73b0\u5927\u89c4\u6a21\u8282\u70b9\u7684\u56e0\u679c\u53d1\u73b0\u3002"}}
{"id": "2509.24005", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24005", "abs": "https://arxiv.org/abs/2509.24005", "authors": ["Chenruo Liu", "Yijun Dong", "Qi Lei"], "title": "Does Weak-to-strong Generalization Happen under Spurious Correlations?", "comment": null, "summary": "We initiate a unified theoretical and algorithmic study of a key problem in\nweak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained\nstudent with pseudolabels from a weaker teacher on a downstream task with\nspurious correlations, does W2S happen, and how to improve it upon failures? We\nconsider two sources of spurious correlations caused by group imbalance: (i) a\nweak teacher fine-tuned on group-imbalanced labeled data with a minority group\nof fraction $\\eta_\\ell$, and (ii) a group-imbalanced unlabeled set\npseudolabeled by the teacher with a minority group of fraction $\\eta_u$.\nTheoretically, a precise characterization of W2S gain at the proportional\nasymptotic limit shows that W2S always happens with sufficient pseudolabels\nwhen $\\eta_u = \\eta_\\ell$ but may fail when $\\eta_u \\ne \\eta_\\ell$, where W2S\ngain diminishes as $(\\eta_u - \\eta_\\ell)^2$ increases. Our theory is\ncorroborated by extensive experiments on various spurious correlation\nbenchmarks and teacher-student pairs. To boost W2S performance upon failures,\nwe further propose a simple, effective algorithmic remedy that retrains the\nstrong student on its high-confidence data subset after W2S fine-tuning. Our\nalgorithm is group-label-free and achieves consistent, substantial improvements\nover vanilla W2S fine-tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f31\u5230\u5f3a\u6cdb\u5316\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff1a\u5f53\u4f7f\u7528\u5e26\u6709\u865a\u5047\u76f8\u5173\u6027\u7684\u5f31\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u6765\u5fae\u8c03\u5f3a\u9884\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u65f6\uff0cW2S\u662f\u5426\u4f1a\u53d1\u751f\u4ee5\u53ca\u5982\u4f55\u6539\u8fdb\u5931\u8d25\u60c5\u51b5\u3002\u8bba\u6587\u8003\u8651\u4e86\u7531\u7fa4\u4f53\u4e0d\u5e73\u8861\u5f15\u8d77\u7684\u4e24\u79cd\u865a\u5047\u76f8\u5173\u6027\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u7406\u8bba\u5206\u6790\u548c\u6539\u8fdb\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5728\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5f31\u5230\u5f3a\u6cdb\u5316\u662f\u5426\u4f1a\u53d1\u751f\u4ee5\u53ca\u5982\u4f55\u6539\u8fdb\u5931\u8d25\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5728\u7fa4\u4f53\u4e0d\u5e73\u8861\u7684\u6807\u6ce8\u6570\u636e\u548c\u672a\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\u3002", "method": "\u7406\u8bba\u5206\u6790\u5728\u6bd4\u4f8b\u6e10\u8fd1\u6781\u9650\u4e0b\u7cbe\u786e\u8868\u5f81W2S\u589e\u76ca\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7b97\u6cd5\u8865\u6551\u63aa\u65bd\uff1a\u5728W2S\u5fae\u8c03\u540e\uff0c\u5728\u5f3a\u5b66\u751f\u6a21\u578b\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u6570\u636e\u5b50\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u7406\u8bba\u8868\u660e\u5f53\u672a\u6807\u6ce8\u6570\u636e\u4e0e\u6807\u6ce8\u6570\u636e\u7684\u5c11\u6570\u7fa4\u4f53\u6bd4\u4f8b\u76f8\u540c\u65f6\uff0cW2S\u603b\u662f\u53d1\u751f\uff1b\u4f46\u5f53\u6bd4\u4f8b\u4e0d\u540c\u65f6\uff0cW2S\u53ef\u80fd\u5931\u8d25\uff0c\u4e14\u589e\u76ca\u968f\u6bd4\u4f8b\u5dee\u5f02\u7684\u5e73\u65b9\u800c\u51cf\u5c0f\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "W2S\u5728\u7fa4\u4f53\u4e0d\u5e73\u8861\u573a\u666f\u4e0b\u53ef\u80fd\u5931\u8d25\uff0c\u4f46\u901a\u8fc7\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u6570\u636e\u5b50\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347W2S\u6027\u80fd\uff0c\u4e14\u8be5\u65b9\u6cd5\u65e0\u9700\u7fa4\u4f53\u6807\u7b7e\u4fe1\u606f\u3002"}}
{"id": "2509.24006", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24006", "abs": "https://arxiv.org/abs/2509.24006", "authors": ["Jintao Zhang", "Haoxu Wang", "Kai Jiang", "Shuo Yang", "Kaiwen Zheng", "Haocheng Xi", "Ziteng Wang", "Hongzhou Zhu", "Min Zhao", "Ion Stoica", "Joseph E. Gonzalez", "Jun Zhu", "Jianfei Chen"], "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention", "comment": null, "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.", "AI": {"tldr": "\u63d0\u51faSLA\uff08\u7a00\u758f-\u7ebf\u6027\u6ce8\u610f\u529b\uff09\u65b9\u6cd5\uff0c\u5c06\u6ce8\u610f\u529b\u6743\u91cd\u5206\u4e3a\u5173\u952e\u3001\u8fb9\u7f18\u548c\u53ef\u5ffd\u7565\u4e09\u7c7b\uff0c\u5206\u522b\u5e94\u7528O(N\u00b2)\u3001O(N)\u8ba1\u7b97\u548c\u8df3\u8fc7\u5904\u7406\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u6a21\u578b\u4e2d\uff0c\u7531\u4e8e\u957f\u5e8f\u5217\u957f\u5ea6\u548c\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u6ce8\u610f\u529b\u5ef6\u8fdf\u662f\u4e3b\u8981\u74f6\u9888\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u6743\u91cd\u53ef\u5206\u4e3a\u9ad8\u79e9\u5927\u6743\u91cd\u548c\u4f4e\u79e9\u5c0f\u6743\u91cd\u4e24\u90e8\u5206\u3002", "method": "SLA\u5c06\u6ce8\u610f\u529b\u6743\u91cd\u5206\u7c7b\u4e3a\u5173\u952e\u3001\u8fb9\u7f18\u548c\u53ef\u5ffd\u7565\u4e09\u7c7b\uff1a\u5bf9\u5173\u952e\u6743\u91cd\u5e94\u7528O(N\u00b2)\u6ce8\u610f\u529b\uff0c\u5bf9\u8fb9\u7f18\u6743\u91cd\u5e94\u7528O(N)\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u8df3\u8fc7\u53ef\u5ffd\u7565\u6743\u91cd\u3002\u6240\u6709\u8ba1\u7b97\u96c6\u6210\u5728\u5355\u4e2aGPU\u5185\u6838\u4e2d\uff0c\u652f\u6301\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u3002", "result": "\u4ec5\u9700\u5c11\u91cf\u5fae\u8c03\u6b65\u9aa4\uff0cSLA\u4f7fDiT\u6a21\u578b\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u51cf\u5c1120\u500d\uff0c\u6ce8\u610f\u529b\u8ba1\u7b97\u51cf\u5c1195%\u4e14\u4e0d\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u3002\u9ad8\u6548GPU\u5185\u6838\u5b9e\u73b0\u6ce8\u610f\u529b\u8ba1\u7b9713.7\u500d\u52a0\u901f\u548c\u7aef\u5230\u7aef\u89c6\u9891\u751f\u62102.2\u500d\u52a0\u901f\u3002", "conclusion": "SLA\u6210\u529f\u878d\u5408\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u5927\u5e45\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.24012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24012", "abs": "https://arxiv.org/abs/2509.24012", "authors": ["Rylan Schaeffer", "Noam Levi", "Brando Miranda", "Sanmi Koyejo"], "title": "Pretraining Scaling Laws for Generative Evaluations of Language Models", "comment": null, "summary": "Neural scaling laws have played a central role in modern machine learning,\ndriving the field's ever-expanding scaling of parameters, data and compute.\nWhile much research has gone into fitting scaling laws and predicting\nperformance on pretraining losses and on discriminative evaluations such as\nmultiple-choice question-answering, comparatively little research has been done\non fitting scaling laws and predicting performance on generative evaluations\nsuch as mathematical problem-solving or software engineering. We propose and\nevaluate three different pretraining scaling laws for fitting pass-at-$k$ on\ngenerative evaluations and for predicting pass-at-$k$ of the most expensive\nmodel using the performance of cheaper models. Our three scaling laws differ in\nthe covariates used: (1) compute, (2) model parameters and tokens, (3) log\nlikelihoods of gold reference solutions. We make four main contributions: (1)\nWe show how generative evaluations offer new hyperparameters (in our setting,\n$k$) that researchers can use to control the scaling laws parameters and the\npredictability of performance. (2) In terms of scaling law parameters, we find\nthat the compute scaling law and parameters\\,+\\,tokens scaling law stabilize\nfor the last ~$1.5{-}2.5$ orders of magnitude, whereas the gold reference\nlikelihood scaling law stabilizes for the last ~$5$ orders of magnitude. (3) In\nterms of predictive performance, we find all three scaling laws perform\ncomparably, although the compute scaling law predicts slightly worse for small\n$k$ and the log likelihoods of gold reference solutions predicts slightly worse\nfor large $k$. (4) We establish a theoretical connection that the compute\nscaling law emerges as the compute-optimal envelope of the\nparameters-and-tokens scaling law. Our framework provides researchers and\npractitioners with insights and methodologies to forecast generative\nperformance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u7f29\u653e\u5b9a\u5f8b\uff0c\u7528\u4e8e\u62df\u5408\u751f\u6210\u5f0f\u8bc4\u4f30\u4e2d\u7684pass-at-k\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u8f83\u4fbf\u5b9c\u6a21\u578b\u7684\u6027\u80fd\u9884\u6d4b\u6700\u6602\u8d35\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9884\u8bad\u7ec3\u635f\u5931\u548c\u5224\u522b\u5f0f\u8bc4\u4f30\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u800c\u751f\u6210\u5f0f\u8bc4\u4f30\uff08\u5982\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u3001\u8f6f\u4ef6\u5de5\u7a0b\uff09\u7684\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u7f29\u653e\u5b9a\u5f8b\uff1a(1) \u57fa\u4e8e\u8ba1\u7b97\u91cf\u7684\u7f29\u653e\u5b9a\u5f8b\uff1b(2) \u57fa\u4e8e\u6a21\u578b\u53c2\u6570\u548c\u8bad\u7ec3token\u6570\u91cf\u7684\u7f29\u653e\u5b9a\u5f8b\uff1b(3) \u57fa\u4e8e\u9ec4\u91d1\u53c2\u8003\u89e3\u5bf9\u6570\u4f3c\u7136\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) \u8ba1\u7b97\u91cf\u548c\u53c2\u6570+token\u7f29\u653e\u5b9a\u5f8b\u5728\u6700\u540e1.5-2.5\u4e2a\u6570\u91cf\u7ea7\u7a33\u5b9a\uff0c\u800c\u9ec4\u91d1\u53c2\u8003\u4f3c\u7136\u7f29\u653e\u5b9a\u5f8b\u5728\u6700\u540e5\u4e2a\u6570\u91cf\u7ea7\u7a33\u5b9a\uff1b(2) \u4e09\u79cd\u7f29\u653e\u5b9a\u5f8b\u9884\u6d4b\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u91cf\u7f29\u653e\u5b9a\u5f8b\u5728\u5c0fk\u65f6\u9884\u6d4b\u7a0d\u5dee\uff0c\u9ec4\u91d1\u53c2\u8003\u4f3c\u7136\u5728\u5927k\u65f6\u9884\u6d4b\u7a0d\u5dee\u3002", "conclusion": "\u5efa\u7acb\u4e86\u8ba1\u7b97\u91cf\u7f29\u653e\u5b9a\u5f8b\u4f5c\u4e3a\u53c2\u6570\u548ctoken\u7f29\u653e\u5b9a\u5f8b\u7684\u8ba1\u7b97\u6700\u4f18\u5305\u7edc\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u9884\u6d4b\u751f\u6210\u5f0f\u6027\u80fd\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u89c1\u89e3\u3002"}}
{"id": "2509.24031", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.24031", "abs": "https://arxiv.org/abs/2509.24031", "authors": ["Umang Garg", "Bowen Zhang", "Anantanjit Subrahmanya", "Chandrakanth Gudavalli", "BS Manjunath"], "title": "GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning", "comment": "4 pages, 2 figures", "summary": "Foundation models have driven remarkable progress in text, vision, and video\nunderstanding, and are now poised to unlock similar breakthroughs in trajectory\nmodeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a\nfoundation model for large-scale mobility data that captures patterns of\nnormalcy in human movement. Unlike prior approaches that flatten trajectories\ninto coordinate streams, GPS-MTM decomposes mobility into two complementary\nmodalities: states (point-of-interest categories) and actions (agent\ntransitions). Leveraging a bi-directional Transformer with a self-supervised\nmasked modeling objective, the model reconstructs missing segments across\nmodalities, enabling it to learn rich semantic correlations without manual\nlabels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and\nGeolife, GPS-MTM consistently outperforms on downstream tasks such as\ntrajectory infilling and next-stop prediction. Its advantages are most\npronounced in dynamic tasks (inverse and forward dynamics), where contextual\nreasoning is critical. These results establish GPS-MTM as a robust foundation\nmodel for trajectory analytics, positioning mobility data as a first-class\nmodality for large-scale representation learning. Code is released for further\nreference.", "AI": {"tldr": "GPS-MTM\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u79fb\u52a8\u6570\u636e\u7684\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u79fb\u52a8\u5206\u89e3\u4e3a\u72b6\u6001\u548c\u52a8\u4f5c\u4e24\u4e2a\u6a21\u6001\uff0c\u4f7f\u7528\u53cc\u5411Transformer\u548c\u63a9\u7801\u5efa\u6a21\u76ee\u6807\u6765\u5b66\u4e60\u4e30\u5bcc\u7684\u8bed\u4e49\u5173\u8054\uff0c\u5728\u8f68\u8ff9\u586b\u5145\u548c\u4e0b\u4e00\u7ad9\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u6587\u672c\u3001\u89c6\u89c9\u548c\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u73b0\u5728\u6709\u671b\u5728\u8f68\u8ff9\u5efa\u6a21\u9886\u57df\u5b9e\u73b0\u7c7b\u4f3c\u7a81\u7834\u3002\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6355\u6349\u4eba\u7c7b\u79fb\u52a8\u6b63\u5e38\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u79fb\u52a8\u6570\u636e\u57fa\u7840\u6a21\u578b\u3002", "method": "GPS-MTM\u5c06\u79fb\u52a8\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e92\u8865\u6a21\u6001\uff1a\u72b6\u6001\uff08\u5174\u8da3\u70b9\u7c7b\u522b\uff09\u548c\u52a8\u4f5c\uff08\u667a\u80fd\u4f53\u8f6c\u79fb\uff09\u3002\u91c7\u7528\u53cc\u5411Transformer\u67b6\u6784\u548c\u81ea\u76d1\u7763\u63a9\u7801\u5efa\u6a21\u76ee\u6807\uff0c\u901a\u8fc7\u91cd\u5efa\u8de8\u6a21\u6001\u7684\u7f3a\u5931\u7247\u6bb5\u6765\u5b66\u4e60\u8bed\u4e49\u5173\u8054\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u5728Numosim-LA\u3001Urban Anomalies\u548cGeolife\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGPS-MTM\u5728\u8f68\u8ff9\u586b\u5145\u548c\u4e0b\u4e00\u7ad9\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u5728\u9700\u8981\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u52a8\u6001\u4efb\u52a1\uff08\u9006\u52a8\u6001\u548c\u524d\u5411\u52a8\u6001\uff09\u4e2d\u4f18\u52bf\u6700\u4e3a\u660e\u663e\u3002", "conclusion": "GPS-MTM\u88ab\u786e\u7acb\u4e3a\u4e00\u4e2a\u7a33\u5065\u7684\u8f68\u8ff9\u5206\u6790\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u79fb\u52a8\u6570\u636e\u5b9a\u4f4d\u4e3a\u5927\u89c4\u6a21\u8868\u793a\u5b66\u4e60\u7684\u4e00\u7b49\u6a21\u6001\u3002\u4ee3\u7801\u5df2\u53d1\u5e03\u4f9b\u8fdb\u4e00\u6b65\u53c2\u8003\u3002"}}
{"id": "2509.24047", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24047", "abs": "https://arxiv.org/abs/2509.24047", "authors": ["Runyu Zhang", "Na Li", "Asuman Ozdaglar", "Jeff Shamma", "Gioele Zardini"], "title": "Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning", "comment": null, "summary": "Risk sensitivity has become a central theme in reinforcement learning (RL),\nwhere convex risk measures and robust formulations provide principled ways to\nmodel preferences beyond expected return. Recent extensions to multi-agent RL\n(MARL) have largely emphasized the risk-averse setting, prioritizing robustness\nto uncertainty. In cooperative MARL, however, such conservatism often leads to\nsuboptimal equilibria, and a parallel line of work has shown that optimism can\npromote cooperation. Existing optimistic methods, though effective in practice,\nare typically heuristic and lack theoretical grounding. Building on the dual\nrepresentation for convex risk measures, we propose a principled framework that\ninterprets risk-seeking objectives as optimism. We introduce optimistic value\nfunctions, which formalize optimism as divergence-penalized risk-seeking\nevaluations. Building on this foundation, we derive a policy-gradient theorem\nfor optimistic value functions, including explicit formulas for the entropic\nrisk/KL-penalty setting, and develop decentralized optimistic actor-critic\nalgorithms that implement these updates. Empirical results on cooperative\nbenchmarks demonstrate that risk-seeking optimism consistently improves\ncoordination over both risk-neutral baselines and heuristic optimistic methods.\nOur framework thus unifies risk-sensitive learning and optimism, offering a\ntheoretically grounded and practically effective approach to cooperation in\nMARL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51f8\u98ce\u9669\u5ea6\u91cf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u98ce\u9669\u5bfb\u6c42\u76ee\u6807\u89e3\u91ca\u4e3a\u4e50\u89c2\u4e3b\u4e49\uff0c\u5e76\u5f00\u53d1\u4e86\u5206\u6563\u5f0f\u4e50\u89c2\u884c\u52a8\u8005-\u6279\u8bc4\u8005\u7b97\u6cd5\u6765\u6539\u5584\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5408\u4f5c\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u6b21\u4f18\u5747\u8861\uff0c\u800c\u73b0\u6709\u7684\u4e50\u89c2\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u98ce\u9669\u654f\u611f\u5b66\u4e60\u548c\u4e50\u89c2\u4e3b\u4e49\uff0c\u4e3a\u5408\u4f5c\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u57fa\u4e8e\u51f8\u98ce\u9669\u5ea6\u91cf\u7684\u5bf9\u5076\u8868\u793a\uff0c\u63d0\u51fa\u4e86\u4e50\u89c2\u4ef7\u503c\u51fd\u6570\u6846\u67b6\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u53d1\u6563\u60e9\u7f5a\u7684\u98ce\u9669\u5bfb\u6c42\u8bc4\u4f30\u3002\u63a8\u5bfc\u4e86\u4e50\u89c2\u4ef7\u503c\u51fd\u6570\u7684\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u5e76\u5f00\u53d1\u4e86\u5206\u6563\u5f0f\u4e50\u89c2\u884c\u52a8\u8005-\u6279\u8bc4\u8005\u7b97\u6cd5\u3002", "result": "\u5728\u5408\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u98ce\u9669\u5bfb\u6c42\u7684\u4e50\u89c2\u4e3b\u4e49\u76f8\u6bd4\u98ce\u9669\u4e2d\u6027\u57fa\u7ebf\u548c\u542f\u53d1\u5f0f\u4e50\u89c2\u65b9\u6cd5\uff0c\u6301\u7eed\u6539\u5584\u4e86\u534f\u8c03\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u98ce\u9669\u654f\u611f\u5b66\u4e60\u548c\u4e50\u89c2\u4e3b\u4e49\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5408\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.24050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24050", "abs": "https://arxiv.org/abs/2509.24050", "authors": ["Wenzhi Fang", "Dong-Jun Han", "Liangqi Yuan", "Christopher Brinton"], "title": "Collaborative Device-Cloud LLM Inference through Reinforcement Learning", "comment": "We propose a unified post-training framework that integrates routing\n  optimization, enabling the on-device LLM to improve its problem-solving\n  ability while learning routing strategies", "summary": "Device-cloud collaboration has emerged as a promising paradigm for deploying\nlarge language models (LLMs), combining the efficiency of lightweight on-device\ninference with the superior performance of powerful cloud LLMs. An essential\nproblem in this scenario lies in deciding whether a given query is best handled\nlocally or delegated to the cloud. Existing approaches typically rely on\nexternal routers, implemented as binary classifiers, which often struggle to\ndetermine task difficulty from the prompt's surface pattern. To address these\nlimitations, we propose a framework where the on-device LLM makes routing\ndecisions at the end of its solving process, with this capability instilled\nthrough post-training. In particular, we formulate a reward maximization\nproblem with carefully designed rewards that encourage effective problem\nsolving and judicious offloading to the cloud. To solve this problem, we\ndevelop a group-adaptive policy gradient algorithm, featuring a group-level\npolicy gradient, designed to yield an unbiased gradient estimator of the\nreward, and adaptive prompt filtering, developed to enforce the constraint on\ncloud LLM usage. Extensive experiments across models and benchmarks show that\nthe proposed methodology consistently outperforms existing baselines and\nsignificantly narrows the gap to full cloud LLM performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u5907-\u4e91\u534f\u4f5c\u6846\u67b6\uff0c\u8ba9\u8bbe\u5907\u7aefLLM\u5728\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u5c06\u67e5\u8be2\u5378\u8f7d\u5230\u4e91\u7aef\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u5b9e\u73b0\u8def\u7531\u51b3\u7b56\u80fd\u529b", "motivation": "\u73b0\u6709\u8bbe\u5907-\u4e91\u534f\u4f5c\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8def\u7531\u5668\u4f5c\u4e3a\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u96be\u4ee5\u4ece\u63d0\u793a\u8868\u9762\u6a21\u5f0f\u51c6\u786e\u5224\u65ad\u4efb\u52a1\u96be\u5ea6\uff0c\u5bfc\u81f4\u8def\u7531\u51b3\u7b56\u6548\u679c\u4e0d\u4f73", "method": "\u8bbe\u8ba1\u5956\u52b1\u6700\u5927\u5316\u95ee\u9898\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u5956\u52b1\u673a\u5236\u9f13\u52b1\u6709\u6548\u95ee\u9898\u89e3\u51b3\u548c\u660e\u667a\u7684\u4e91\u7aef\u5378\u8f7d\uff1b\u5f00\u53d1\u7ec4\u81ea\u9002\u5e94\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u5305\u542b\u7ec4\u7ea7\u7b56\u7565\u68af\u5ea6\u548c\u81ea\u9002\u5e94\u63d0\u793a\u8fc7\u6ee4", "result": "\u8de8\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u663e\u8457\u7f29\u5c0f\u4e0e\u5168\u4e91LLM\u6027\u80fd\u7684\u5dee\u8ddd", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u8ba9\u8bbe\u5907\u7aefLLM\u81ea\u4e3b\u51b3\u7b56\u8def\u7531\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bbe\u5907-\u4e91\u534f\u4f5c\u4e2d\u7684\u8def\u7531\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u5e73\u8861"}}
{"id": "2509.24058", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24058", "abs": "https://arxiv.org/abs/2509.24058", "authors": ["Julia Wenkmann", "Damien Garreau"], "title": "On The Variability of Concept Activation Vectors", "comment": "26 pages (including appendix), 24 figures (44 panels). Submitted to\n  AAAI-26", "summary": "One of the most pressing challenges in artificial intelligence is to make\nmodels more transparent to their users. Recently, explainable artificial\nintelligence has come up with numerous method to tackle this challenge. A\npromising avenue is to use concept-based explanations, that is, high-level\nconcepts instead of plain feature importance score. Among this class of\nmethods, Concept Activation vectors (CAVs), Kim et al. (2018) stands out as one\nof the main protagonists. One interesting aspect of CAVs is that their\ncomputation requires sampling random examples in the train set. Therefore, the\nactual vectors obtained may vary from user to user depending on the randomness\nof this sampling. In this paper, we propose a fine-grained theoretical analysis\nof CAVs construction in order to quantify their variability. Our results,\nconfirmed by experiments on several real-life datasets, point out towards an\nuniversal result: the variance of CAVs decreases as $1/N$, where $N$ is the\nnumber of random examples. Based on this we give practical recommendations for\na resource-efficient application of the method.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf(CAVs)\u7684\u53d8\u5f02\u6027\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u53d1\u73b0\u5176\u65b9\u5dee\u968f\u968f\u673a\u6837\u672c\u6570\u91cfN\u7684\u589e\u52a0\u800c\u4ee51/N\u7684\u901f\u5ea6\u51cf\u5c0f\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u4e86\u8d44\u6e90\u9ad8\u6548\u5e94\u7528\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf(CAVs)\u5728\u8ba1\u7b97\u65f6\u9700\u8981\u4ece\u8bad\u7ec3\u96c6\u4e2d\u968f\u673a\u91c7\u6837\uff0c\u8fd9\u5bfc\u81f4\u4e0d\u540c\u7528\u6237\u53ef\u80fd\u5f97\u5230\u4e0d\u540c\u7684\u5411\u91cf\u7ed3\u679c\u3002\u672c\u6587\u65e8\u5728\u91cf\u5316CAVs\u7684\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u5176\u53ef\u9760\u6027\u3002", "method": "\u5bf9CAVs\u6784\u5efa\u8fc7\u7a0b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u7406\u8bba\u5206\u6790\uff0c\u91cf\u5316\u5176\u53d8\u5f02\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCAVs\u7684\u65b9\u5dee\u968f\u968f\u673a\u6837\u672c\u6570\u91cfN\u7684\u589e\u52a0\u800c\u4ee51/N\u7684\u901f\u5ea6\u51cf\u5c0f\uff0c\u8fd9\u662f\u4e00\u4e2a\u666e\u9002\u6027\u89c4\u5f8b\u3002", "conclusion": "\u57fa\u4e8e\u65b9\u5dee\u968f\u6837\u672c\u6570\u91cf\u9012\u51cf\u7684\u89c4\u5f8b\uff0c\u63d0\u51fa\u4e86\u8d44\u6e90\u9ad8\u6548\u5e94\u7528CAVs\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6982\u5ff5\u89e3\u91ca\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.24067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24067", "abs": "https://arxiv.org/abs/2509.24067", "authors": ["Qiushui Xu", "Yuhao Huang", "Yushu Jiang", "Lei Song", "Jinyu Wang", "Wenliang Zheng", "Jiang Bian"], "title": "In-Context Compositional Q-Learning for Offline Reinforcement Learning", "comment": null, "summary": "Accurately estimating the Q-function is a central challenge in offline\nreinforcement learning. However, existing approaches often rely on a single\nglobal Q-function, which struggles to capture the compositional nature of tasks\ninvolving diverse subtasks. We propose In-context Compositional Q-Learning\n(\\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a\ncontextual inference problem, using linear Transformers to adaptively infer\nlocal Q-functions from retrieved transitions without explicit subtask labels.\nTheoretically, we show that under two assumptions--linear approximability of\nthe local Q-function and accurate weight inference from retrieved\ncontext--\\texttt{ICQL} achieves bounded Q-function approximation error, and\nsupports near-optimal policy extraction. Empirically, \\texttt{ICQL}\nsubstantially improves performance in offline settings: improving performance\nin kitchen tasks by up to 16.4\\%, and in Gym and Adroit tasks by up to 8.6\\%\nand 6.3\\%. These results highlight the underexplored potential of in-context\nlearning for robust and compositional value estimation, positioning\n\\texttt{ICQL} as a principled and effective framework for offline RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86ICQL\u6846\u67b6\uff0c\u5c06Q\u5b66\u4e60\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u63a8\u7406\u95ee\u9898\uff0c\u4f7f\u7528\u7ebf\u6027Transformer\u4ece\u68c0\u7d22\u7684\u8f6c\u79fb\u4e2d\u81ea\u9002\u5e94\u63a8\u65ad\u5c40\u90e8Q\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u5b50\u4efb\u52a1\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u5168\u5c40Q\u51fd\u6570\uff0c\u96be\u4ee5\u6355\u6349\u5305\u542b\u591a\u6837\u5b50\u4efb\u52a1\u7684\u4efb\u52a1\u7684\u7ec4\u5408\u6027\u8d28\u3002", "method": "\u4f7f\u7528\u7ebf\u6027Transformer\u5c06Q\u5b66\u4e60\u6784\u5efa\u4e3a\u4e0a\u4e0b\u6587\u63a8\u7406\u95ee\u9898\uff0c\u4ece\u68c0\u7d22\u7684\u8f6c\u79fb\u4e2d\u81ea\u9002\u5e94\u63a8\u65ad\u5c40\u90e8Q\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u5b50\u4efb\u52a1\u6807\u7b7e\u3002", "result": "\u5728\u53a8\u623f\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe16.4%\uff0c\u5728Gym\u548cAdroit\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u53478.6%\u548c6.3%\u3002", "conclusion": "ICQL\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u9c81\u68d2\u548c\u7ec4\u5408\u4ef7\u503c\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2509.24068", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24068", "abs": "https://arxiv.org/abs/2509.24068", "authors": ["Roussel Rahman", "Jeff Shrager"], "title": "A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture", "comment": null, "summary": "Strategy Choice Theory (SCT)\\footnote{``Strategy Choice Theory'',\n``Distributions of Associations'', and ``Overlapping Wave Theory'' have been\nused to refer to this line of work, emphasizing different\naspects.}\\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}\nexplains important aspects of children's arithmetic learning based upon\nprinciples including learning from developmentally naturalistic data,\nprobabilistic representation, confidence-based retrieval, and the phase-like\nimportance of scaffolding strategies, such as finger-counting. Here we recast\nSCT as a ``Small Math Model'' (SMM), employing a neural-network-based\narchitecture analogous to LLMs. The SMM extends SCT to include counting\npractice\\footnote{The original SCT model was pre-biased in accordance with the\nsupposed experience of counting.}, symbol (number) embedding, and gated\nattention. Similar to earlier work, the SMM demonstrates constructive and\ndestructive interference between counting and addition, and the ``wave-like''\nuse of finger-counting as sum recall improves. We plan to extend the SMM to\nlater aspects of the decades-long SCT program, including adaptive strategy\nchoice and eventually strategy discovery, providing a unified platform to\ninvestigate the understanding of numerical characteristics and relationships\nessential for mathematical reasoning -- as it can emerge in LLM-based agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u5c0f\u6570\u5b66\u6a21\u578b\"\uff08SMM\uff09\uff0c\u5c06\u7b56\u7565\u9009\u62e9\u7406\u8bba\uff08SCT\uff09\u91cd\u65b0\u6784\u5efa\u4e3a\u7c7b\u4f3c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u7814\u7a76\u513f\u7ae5\u7b97\u672f\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u5c06\u7ecf\u5178\u7684\u7b56\u7565\u9009\u62e9\u7406\u8bba\u73b0\u4ee3\u5316\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u6a21\u62df\u513f\u7ae5\u7b97\u672f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4e3a\u7814\u7a76\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5728\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u4e2d\u7684\u6d8c\u73b0\u63d0\u4f9b\u7edf\u4e00\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u6269\u5c55SCT\u7406\u8bba\uff0c\u5305\u542b\u8ba1\u6570\u7ec3\u4e60\u3001\u6570\u5b57\u7b26\u53f7\u5d4c\u5165\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u62df\u513f\u7ae5\u4ece\u624b\u6307\u8ba1\u6570\u5230\u52a0\u6cd5\u8bb0\u5fc6\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "SMM\u6a21\u578b\u6210\u529f\u5c55\u793a\u4e86\u8ba1\u6570\u4e0e\u52a0\u6cd5\u4e4b\u95f4\u7684\u5efa\u8bbe\u6027\u548c\u7834\u574f\u6027\u5e72\u6270\uff0c\u4ee5\u53ca\u968f\u7740\u52a0\u6cd5\u8bb0\u5fc6\u80fd\u529b\u63d0\u9ad8\u800c\u51fa\u73b0\u7684\"\u6ce2\u6d6a\u5f0f\"\u624b\u6307\u8ba1\u6570\u4f7f\u7528\u6a21\u5f0f\u3002", "conclusion": "SMM\u4e3a\u7814\u7a76\u7b56\u7565\u9009\u62e9\u7406\u8bba\u7684\u540e\u7eed\u65b9\u9762\uff08\u5982\u9002\u5e94\u6027\u7b56\u7565\u9009\u62e9\u548c\u7b56\u7565\u53d1\u73b0\uff09\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6570\u5b66\u63a8\u7406\u4e2d\u6570\u503c\u7279\u5f81\u548c\u5173\u7cfb\u7406\u89e3\u7684\u6d8c\u73b0\u673a\u5236\u3002"}}
{"id": "2509.24069", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "62M10, 68T45, 62P35, 92C40, 65C20, 60G35, 92C42, 92C35, 93E10", "I.2.6; C.2.4; H.3.4; I.2.4; H.3.5; C.2.4; C.3; I.4.8; I.5.1; J.3;\n  K.6.1; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.24069", "abs": "https://arxiv.org/abs/2509.24069", "authors": ["Youssef Sabiri", "Walid Houmaidi", "Ouail El Maadi", "Yousra Chtouki"], "title": "AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring", "comment": "6 pages, 6 figures, 3 tables. Accepted at the 9th IEEE Global\n  Conference on Artificial Intelligence & Internet of Things (IEEE GCAIoT)\n  2025. Final camera-ready manuscript. Math expressions in this field are\n  rendered via MathJax", "summary": "Smart aquaculture systems depend on rich environmental data streams to\nprotect fish welfare, optimize feeding, and reduce energy use. Yet public\ndatasets that describe the air surrounding indoor tanks remain scarce, limiting\nthe development of forecasting and anomaly-detection tools that couple\nhead-space conditions with water-quality dynamics. We therefore introduce\nAQUAIR, an open-access public dataset that logs six Indoor Environmental\nQuality (IEQ) variables--air temperature, relative humidity, carbon dioxide,\ntotal volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture\nfacility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every\nfive minutes from 14 October 2024 to 9 January 2025, producing more than 23,000\ntime-stamped observations that are fully quality-controlled and publicly\narchived on Figshare. We describe the sensor placement, ISO-compliant mounting\nheight, calibration checks against reference instruments, and an open-source\nprocessing pipeline that normalizes timestamps, interpolates short gaps, and\nexports analysis-ready tables. Exploratory statistics show stable conditions\n(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time\npeaks, offering rich structure for short-horizon forecasting, event detection,\nand sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture\ninformatics and provides a reproducible benchmark for data-centric machine\nlearning curricula and environmental sensing research focused on head-space\ndynamics in recirculating aquaculture systems.", "AI": {"tldr": "AQUAIR\u662f\u4e00\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u5ba4\u5185\u6c34\u4ea7\u517b\u6b96\u73af\u5883\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e866\u4e2a\u73af\u5883\u53d8\u91cf\uff0c\u5305\u542b\u8d85\u8fc723,000\u4e2a\u65f6\u95f4\u6233\u89c2\u6d4b\u6570\u636e\uff0c\u586b\u8865\u4e86\u667a\u80fd\u6c34\u4ea7\u517b\u6b96\u4fe1\u606f\u5b66\u7684\u5173\u952e\u7a7a\u767d\u3002", "motivation": "\u7531\u4e8e\u63cf\u8ff0\u5ba4\u5185\u6c34\u4ea7\u517b\u6b96\u8bbe\u65bd\u7a7a\u6c14\u73af\u5883\u7684\u516c\u5171\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u7ed3\u5408\u7a7a\u6c14\u6761\u4ef6\u548c\u6c34\u8d28\u52a8\u6001\u7684\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u5de5\u5177\u7684\u5f00\u53d1\u3002", "method": "\u4f7f\u7528\u5355\u4e2aAwair HOME\u76d1\u6d4b\u5668\u6bcf5\u5206\u949f\u91c7\u6837\u4e00\u6b21\uff0c\u4ece2024\u5e7410\u670814\u65e5\u81f32025\u5e741\u67089\u65e5\uff0c\u91c7\u7528ISO\u6807\u51c6\u5b89\u88c5\u9ad8\u5ea6\uff0c\u7ecf\u8fc7\u53c2\u8003\u4eea\u5668\u6821\u51c6\u68c0\u67e5\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5904\u7406\u7ba1\u9053\u8fdb\u884c\u6570\u636e\u6807\u51c6\u5316\u3002", "result": "\u6570\u636e\u96c6\u663e\u793a\u7a33\u5b9a\u7684\u73af\u5883\u6761\u4ef6\uff08\u4e2d\u4f4d\u6570CO2=758 ppm\uff1bPM2.5=12\u5fae\u514b/\u7acb\u65b9\u7c73\uff09\uff0c\u6709\u660e\u663e\u7684\u5582\u98df\u65f6\u95f4\u5cf0\u503c\uff0c\u4e3a\u77ed\u671f\u9884\u6d4b\u3001\u4e8b\u4ef6\u68c0\u6d4b\u548c\u4f20\u611f\u5668\u6f02\u79fb\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7ed3\u6784\u3002", "conclusion": "AQUAIR\u586b\u8865\u4e86\u667a\u80fd\u6c34\u4ea7\u517b\u6b96\u4fe1\u606f\u5b66\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7684\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u548c\u73af\u5883\u4f20\u611f\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.24076", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24076", "abs": "https://arxiv.org/abs/2509.24076", "authors": ["Bo Hu", "Jos\u00e9 C. Pr\u00edncipe"], "title": "A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks", "comment": null, "summary": "Pairwise distance-based costs are crucial for self-supervised and contrastive\nfeature learning. Mixture Density Networks (MDNs) are a widely used approach\nfor generative models and density approximation, using neural networks to\nproduce multiple centers that define a Gaussian mixture. By combining MDNs with\ncontrastive costs, this paper proposes data density approximation using four\ntypes of kernelized matrix costs: the scalar cost, the vector-matrix cost, the\nmatrix-matrix cost (the trace of Schur complement), and the SVD cost (the\nnuclear norm), for learning multiple centers required to define a mixture\ndensity.", "AI": {"tldr": "\u7ed3\u5408\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u6210\u672c\uff0c\u63d0\u51fa\u56db\u79cd\u6838\u5316\u77e9\u9635\u6210\u672c\u7528\u4e8e\u6570\u636e\u5bc6\u5ea6\u8fd1\u4f3c\u548c\u591a\u4e2d\u5fc3\u5b66\u4e60", "motivation": "\u57fa\u4e8e\u8ddd\u79bb\u7684\u6210\u5bf9\u6210\u672c\u5728\u81ea\u76d1\u7763\u548c\u5bf9\u6bd4\u7279\u5f81\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u662f\u751f\u6210\u6a21\u578b\u548c\u5bc6\u5ea6\u8fd1\u4f3c\u7684\u5e38\u7528\u65b9\u6cd5", "method": "\u5c06\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u4e0e\u5bf9\u6bd4\u6210\u672c\u7ed3\u5408\uff0c\u4f7f\u7528\u56db\u79cd\u6838\u5316\u77e9\u9635\u6210\u672c\uff1a\u6807\u91cf\u6210\u672c\u3001\u5411\u91cf-\u77e9\u9635\u6210\u672c\u3001\u77e9\u9635-\u77e9\u9635\u6210\u672c\uff08Schur\u8865\u7684\u8ff9\uff09\u548cSVD\u6210\u672c\uff08\u6838\u8303\u6570\uff09", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u6570\u636e\u5bc6\u5ea6\u8fd1\u4f3c\uff0c\u5b66\u4e60\u4e86\u5b9a\u4e49\u6df7\u5408\u5bc6\u5ea6\u6240\u9700\u7684\u591a\u4e2a\u4e2d\u5fc3", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u79cd\u6838\u5316\u77e9\u9635\u6210\u672c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bc6\u5ea6\u8fd1\u4f3c"}}
{"id": "2509.24077", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24077", "abs": "https://arxiv.org/abs/2509.24077", "authors": ["Zhongteng Cai", "Mohammad Mahdi Khalili", "Xueru Zhang"], "title": "Demographic-Agnostic Fairness without Harm", "comment": null, "summary": "As machine learning (ML) algorithms are increasingly used in social domains\nto make predictions about humans, there is a growing concern that these\nalgorithms may exhibit biases against certain social groups. Numerous notions\nof fairness have been proposed in the literature to measure the unfairness of\nML. Among them, one class that receives the most attention is\n\\textit{parity-based}, i.e., achieving fairness by equalizing treatment or\noutcomes for different social groups. However, achieving parity-based fairness\noften comes at the cost of lowering model accuracy and is undesirable for many\nhigh-stakes domains like healthcare. To avoid inferior accuracy, a line of\nresearch focuses on \\textit{preference-based} fairness, under which any group\nof individuals would experience the highest accuracy and collectively prefer\nthe ML outcomes assigned to them if they were given the choice between various\nsets of outcomes. However, these works assume individual demographic\ninformation is known and fully accessible during training. In this paper, we\nrelax this requirement and propose a novel \\textit{demographic-agnostic\nfairness without harm (DAFH)} optimization algorithm, which jointly learns a\ngroup classifier that partitions the population into multiple groups and a set\nof decoupled classifiers associated with these groups. Theoretically, we\nconduct sample complexity analysis and show that our method can outperform the\nbaselines when demographic information is known and used to train decoupled\nclassifiers. Experiments on both synthetic and real data validate the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7684\u516c\u5e73\u6027\u4f18\u5316\u7b97\u6cd5DAFH\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u7fa4\u4f53\u5206\u7c7b\u5668\u548c\u89e3\u8026\u5206\u7c7b\u5668\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u504f\u597d\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u8981\u4e48\u4ee5\u964d\u4f4e\u6a21\u578b\u51c6\u786e\u7387\u4e3a\u4ee3\u4ef7\u3002\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e14\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u516c\u5e73\u6027\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDAFH\u7b97\u6cd5\uff0c\u8054\u5408\u5b66\u4e60\u7fa4\u4f53\u5206\u7c7b\u5668\uff08\u5c06\u4eba\u53e3\u5212\u5206\u4e3a\u591a\u4e2a\u7fa4\u4f53\uff09\u548c\u4e0e\u8fd9\u4e9b\u7fa4\u4f53\u5173\u8054\u7684\u89e3\u8026\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u53ef\u77e5\u60c5\u51b5\u4e0b\u7684\u504f\u597d\u516c\u5e73\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u5f53\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u5df2\u77e5\u65f6\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DAFH\u7b97\u6cd5\u80fd\u591f\u5728\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u504f\u597d\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6a21\u578b\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.24085", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24085", "abs": "https://arxiv.org/abs/2509.24085", "authors": ["Ju-Hyung Lee", "Yanqing Lu", "Klaus Doppler"], "title": "PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM", "comment": null, "summary": "We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a\nframework for cooperative cross-layer optimization in device-to-device (D2D)\ncommunication. Building on our previous work on single-device on-device LLMs,\nPEARL extends the paradigm by leveraging both publisher and subscriber states\nto guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which\nnormalizes latency by application tolerances and modulates energy by device\nbattery states, provides richer supervision for KL-based finetuning. We study\ntwo lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves\nthe best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms\ninference at near-identical objective scores. Across synthetic scenarios\ngrounded in real measurements, PEARL improves objective scores over heuristic\nand compact model baselines and reduces energy by up to 16% in cooperative\nlow-battery cases. These results demonstrate that peer-aware context,\nreward-aligned training, and head-based efficiency make LLMs practical for\nalways-on, on-device cross-layer control.", "AI": {"tldr": "PEARL\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bbe\u5907\u7aefLLM\u7684D2D\u901a\u4fe1\u534f\u4f5c\u8de8\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u53d1\u5e03\u8005\u548c\u8ba2\u9605\u8005\u72b6\u6001\u6765\u6307\u5bfcWi-Fi Aware\u53c2\u6570\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3002", "motivation": "\u6269\u5c55\u5355\u8bbe\u5907\u8bbe\u5907\u7aefLLM\u8303\u5f0f\uff0c\u5229\u7528\u5bf9\u7b49\u8bbe\u5907\u72b6\u6001\u4fe1\u606f\u6765\u4f18\u5316D2D\u901a\u4fe1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7535\u91cf\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u3002", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u5956\u52b1\uff08\u57fa\u4e8e\u5ef6\u8fdf\u5bb9\u5fcd\u5ea6\u548c\u8bbe\u5907\u7535\u91cf\u72b6\u6001\uff09\u8fdb\u884cKL\u5fae\u8c03\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u8f7b\u91cf\u53d8\u4f53\uff1aPEARL\uff08Head+LoRA\uff09\u548cPEARL-Lite\uff08\u4ec5Head\uff09\u3002", "result": "PEARL\u5728\u5408\u6210\u573a\u666f\u4e2d\u4f18\u4e8e\u542f\u53d1\u5f0f\u548c\u7d27\u51d1\u6a21\u578b\u57fa\u7ebf\uff0c\u5728\u534f\u4f5c\u4f4e\u7535\u91cf\u60c5\u51b5\u4e0b\u8282\u80fd\u9ad8\u8fbe16%\uff0cPEARL-Lite\u5b9e\u73b0\u4e9a20ms\u63a8\u7406\u4e14\u6027\u80fd\u63a5\u8fd1\u3002", "conclusion": "\u5bf9\u7b49\u611f\u77e5\u4e0a\u4e0b\u6587\u3001\u5956\u52b1\u5bf9\u9f50\u8bad\u7ec3\u548c\u57fa\u4e8e\u5934\u90e8\u7684\u6548\u7387\u4f7fLLM\u9002\u7528\u4e8e\u59cb\u7ec8\u5728\u7ebf\u7684\u8bbe\u5907\u7aef\u8de8\u5c42\u63a7\u5236\u3002"}}
{"id": "2509.24093", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24093", "abs": "https://arxiv.org/abs/2509.24093", "authors": ["Owen Lewis Howell", "Linfeng Zhao", "Xupeng Zhu", "Yaoyao Qian", "Haojie Huang", "Lingfeng Sun", "Wil Thomason", "Robert Platt", "Robin Walters"], "title": "Clebsch-Gordan Transformer: Fast and Global Equivariant Attention", "comment": null, "summary": "The global attention mechanism is one of the keys to the success of\ntransformer architecture, but it incurs quadratic computational costs in\nrelation to the number of tokens. On the other hand, equivariant models, which\nleverage the underlying geometric structures of problem instance, often achieve\nsuperior accuracy in physical, biochemical, computer vision, and robotic tasks,\nat the cost of additional compute requirements. As a result, existing\nequivariant transformers only support low-order equivariant features and local\ncontext windows, limiting their expressiveness and performance. This work\nproposes Clebsch-Gordan Transformer, achieving efficient global attention by a\nnovel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations. Our\nmethod enables equivariant modeling of features at all orders while achieving\n${O}(N \\log N)$ input token complexity. Additionally, the proposed method\nscales well with high-order irreducible features, by exploiting the sparsity of\nthe Clebsch-Gordon matrix. Lastly, we also incorporate optional token\npermutation equivariance through either weight sharing or data augmentation. We\nbenchmark our method on a diverse set of benchmarks including n-body\nsimulation, QM9, ModelNet point cloud classification and a robotic grasping\ndataset, showing clear gains over existing equivariant transformers in GPU\nmemory size, speed, and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86Clebsch-Gordan Transformer\uff0c\u901a\u8fc7\u65b0\u9896\u7684Clebsch-Gordon\u5377\u79ef\u5728SO(3)\u4e0d\u53ef\u7ea6\u8868\u793a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5168\u5c40\u6ce8\u610f\u529b\uff0c\u89e3\u51b3\u4e86\u7b49\u53d8transformer\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8868\u8fbe\u80fd\u529b\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4f20\u7edftransformer\u7684\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u7b49\u53d8transformer\u4ec5\u652f\u6301\u4f4e\u9636\u7b49\u53d8\u7279\u5f81\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528Clebsch-Gordon\u5377\u79ef\u5728SO(3)\u4e0d\u53ef\u7ea6\u8868\u793a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5168\u5c40\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u5229\u7528Clebsch-Gordon\u77e9\u9635\u7684\u7a00\u758f\u6027\u6765\u6269\u5c55\u9ad8\u9636\u4e0d\u53ef\u7ea6\u7279\u5f81\u3002", "result": "\u5728n\u4f53\u6a21\u62df\u3001QM9\u3001ModelNet\u70b9\u4e91\u5206\u7c7b\u548c\u673a\u5668\u4eba\u6293\u53d6\u6570\u636e\u96c6\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728GPU\u5185\u5b58\u5927\u5c0f\u3001\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7b49\u53d8transformer\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86O(N log N)\u7684\u8f93\u5165\u4ee4\u724c\u590d\u6742\u5ea6\uff0c\u80fd\u591f\u5bf9\u6240\u6709\u9636\u6b21\u7684\u7b49\u53d8\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.24115", "categories": ["cs.LG", "cond-mat.mtrl-sci", "math.OC", "68Q32 (Primary), 68T07 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.24115", "abs": "https://arxiv.org/abs/2509.24115", "authors": ["Evan Dramko", "Yihuang Xiong", "Yizhi Zhu", "Geoffroy Hautier", "Thomas Reps", "Christopher Jermaine", "Anastasios Kyrillidis"], "title": "ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs", "comment": "14 total pages of main content, 4 of references, 3 in Appendix", "summary": "Point defects play a central role in driving the properties of materials.\nFirst-principles methods are widely used to compute defect energetics and\nstructures, including at scale for high-throughput defect databases. However,\nthese methods are computationally expensive, making machine-learning force\nfields (MLFFs) an attractive alternative for accelerating structural\nrelaxations. Most existing MLFFs are based on graph neural networks (GNNs),\nwhich can suffer from oversmoothing and poor representation of long-range\ninteractions. Both of these issues are especially of concern when modeling\npoint defects. To address these challenges, we introduce the Accelerated Deep\nAtomic Potential Transformer (ADAPT), an MLFF that replaces graph\nrepresentations with a direct coordinates-in-space formulation and explicitly\nconsiders all pairwise atomic interactions. Atoms are treated as tokens, with a\nTransformer encoder modeling their interactions. Applied to a dataset of\nsilicon point defects, ADAPT achieves a roughly 33 percent reduction in both\nforce and energy prediction errors relative to a state-of-the-art GNN-based\nmodel, while requiring only a fraction of the computational cost.", "AI": {"tldr": "ADAPT\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u529b\u573a\uff0c\u901a\u8fc7\u76f4\u63a5\u5750\u6807\u7a7a\u95f4\u8868\u793a\u548c\u663e\u5f0f\u8003\u8651\u6240\u6709\u539f\u5b50\u5bf9\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u7845\u70b9\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u51cf\u5c11\u4e86\u7ea633%\u7684\u529b\u548c\u80fd\u91cf\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u8ba1\u7b97\u70b9\u7f3a\u9677\u80fd\u7ea7\u548c\u7ed3\u6784\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u5b66\u4e60\u529b\u573a\u5b58\u5728\u8fc7\u5e73\u6ed1\u548c\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u8868\u793a\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5efa\u6a21\u70b9\u7f3a\u9677\u65f6\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51faADAPT\u6a21\u578b\uff0c\u7528\u76f4\u63a5\u5750\u6807\u7a7a\u95f4\u8868\u793a\u66ff\u4ee3\u56fe\u8868\u793a\uff0c\u663e\u5f0f\u8003\u8651\u6240\u6709\u539f\u5b50\u5bf9\u76f8\u4e92\u4f5c\u7528\uff0c\u5c06\u539f\u5b50\u89c6\u4e3atoken\uff0c\u4f7f\u7528Transformer\u7f16\u7801\u5668\u5efa\u6a21\u539f\u5b50\u95f4\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u7845\u70b9\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\uff0cADAPT\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u529b\u548c\u80fd\u91cf\u9884\u6d4b\u8bef\u5dee\u5747\u51cf\u5c11\u7ea633%\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u524d\u8005\u7684\u4e00\u5c0f\u90e8\u5206\u3002", "conclusion": "ADAPT\u901a\u8fc7\u521b\u65b0\u7684\u5750\u6807\u7a7a\u95f4\u8868\u793a\u548cTransformer\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u70b9\u7f3a\u9677\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u52a0\u901f\u6750\u6599\u7f3a\u9677\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2509.24117", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24117", "abs": "https://arxiv.org/abs/2509.24117", "authors": ["Sifan Wang", "Zhikai Wu", "David van Dijk", "Lu Lu"], "title": "GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries", "comment": "26 pages, 13 figures, 9 tables", "summary": "Inverse problems governed by partial differential equations (PDEs) are\ncrucial in science and engineering. They are particularly challenging due to\nill-posedness, data sparsity, and the added complexity of irregular geometries.\nClassical PDE-constrained optimization methods are computationally expensive,\nespecially when repeated posterior sampling is required. Learning-based\napproaches improve efficiency and scalability, yet most are designed for\nregular domains or focus on forward modeling. Here, we introduce {\\em\nGeoFunFlow}, a geometric diffusion model framework for inverse problems on\ncomplex geometries. GeoFunFlow combines a novel geometric function autoencoder\n(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE\nemploys a Perceiver module to process unstructured meshes of varying sizes and\nproduces continuous reconstructions of physical fields, while the diffusion\nmodel enables posterior sampling from sparse and noisy data. Across five\nbenchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over\ncomplex geometries, provides calibrated uncertainty quantification, and\ndelivers efficient inference compared to operator-learning and diffusion model\nbaselines.", "AI": {"tldr": "GeoFunFlow\u662f\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u51e0\u4f55\u4e0a\u9006\u95ee\u9898\u7684\u51e0\u4f55\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u51fd\u6570\u81ea\u7f16\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5728\u7a00\u758f\u566a\u58f0\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u540e\u9a8c\u91c7\u6837\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "PDE\u63a7\u5236\u7684\u9006\u95ee\u9898\u5728\u79d1\u5b66\u5de5\u7a0b\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e0d\u9002\u5b9a\u6027\u3001\u6570\u636e\u7a00\u758f\u6027\u548c\u4e0d\u89c4\u5219\u51e0\u4f55\u590d\u6742\u6027\u7b49\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\uff0c\u800c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u89c4\u5219\u57df\u6216\u6b63\u5411\u5efa\u6a21\u3002", "method": "\u63d0\u51faGeoFunFlow\u6846\u67b6\uff0c\u5305\u542b\u51e0\u4f55\u51fd\u6570\u81ea\u7f16\u7801\u5668(GeoFAE)\u548c\u57fa\u4e8e\u6574\u6d41\u6d41\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002GeoFAE\u4f7f\u7528Perceiver\u6a21\u5757\u5904\u7406\u4e0d\u540c\u5c3a\u5bf8\u7684\u975e\u7ed3\u6784\u5316\u7f51\u683c\uff0c\u4ea7\u751f\u7269\u7406\u573a\u7684\u8fde\u7eed\u91cd\u5efa\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoFunFlow\u5728\u590d\u6742\u51e0\u4f55\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u76f8\u6bd4\u7b97\u5b50\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u63a8\u7406\u6548\u7387\u3002", "conclusion": "GeoFunFlow\u4e3a\u590d\u6742\u51e0\u4f55\u4e0a\u7684\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u611f\u77e5\u7f16\u7801\u548c\u6269\u6563\u5efa\u6a21\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.24118", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24118", "abs": "https://arxiv.org/abs/2509.24118", "authors": ["Md Mozaharul Mottalib", "Thao-Ly T. Phan", "Rahmatollah Beheshti"], "title": "HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning", "comment": null, "summary": "Electronic health Records (EHRs) have become a cornerstone in modern-day\nhealthcare. They are a crucial part for analyzing the progression of patient\nhealth; however, their complexity, characterized by long, multivariate\nsequences, sparsity, and missing values poses significant challenges in\ntraditional deep learning modeling. While Transformer-based models have\ndemonstrated success in modeling EHR data and predicting clinical outcomes,\ntheir quadratic computational complexity and limited context length hinder\ntheir efficiency and practical applications. On the other hand, State Space\nModels (SSMs) like Mamba present a promising alternative offering linear-time\nsequence modeling and improved efficiency for handling long sequences, but\nfocus mostly on mixing sequence-level information rather than channel-level\ndata. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and\nTransformer Model for EHR Representation Learning), a novel hybrid model\ntailored for representing longitudinal data, combining the strengths of SSMs\nwith advanced attention mechanisms. By testing the model on predictive tasks on\nmultiple clinical datasets, we demonstrate HyMaTE's ability to capture an\neffective, richer, and more nuanced unified representation of EHR data.\nAdditionally, the interpretability of the outcomes achieved by self-attention\nillustrates the effectiveness of our model as a scalable and generalizable\nsolution for real-world healthcare applications. Codes are available at:\nhttps://github.com/healthylaife/HyMaTE.", "AI": {"tldr": "\u63d0\u51fa\u4e86HyMaTE\u6a21\u578b\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cTransformer\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u957f\u5e8f\u5217\u3001\u7a00\u758f\u6570\u636e\u548c\u7f3a\u5931\u503c\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5177\u6709\u957f\u5e8f\u5217\u3001\u591a\u53d8\u91cf\u3001\u7a00\u758f\u548c\u7f3a\u5931\u503c\u7b49\u590d\u6742\u7279\u6027\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002Transformer\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u4e0a\u4e0b\u6587\u957f\u5ea6\u6709\u9650\uff0c\u800c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u5e8f\u5217\u7ea7\u4fe1\u606f\u800c\u975e\u901a\u9053\u7ea7\u6570\u636e\u3002", "method": "\u63d0\u51faHyMaTE\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u7684\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u52bf\uff0c\u4e13\u95e8\u4e3a\u7eb5\u5411\u6570\u636e\u8868\u793a\u5b66\u4e60\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\u4efb\u52a1\u6d4b\u8bd5\u8868\u660e\uff0cHyMaTE\u80fd\u591f\u6355\u6349\u66f4\u6709\u6548\u3001\u66f4\u4e30\u5bcc\u3001\u66f4\u7ec6\u81f4\u7684\u7edf\u4e00EHR\u6570\u636e\u8868\u793a\uff0c\u4e14\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4e86\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "HyMaTE\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u7597\u4fdd\u5065\u5e94\u7528\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.24122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24122", "abs": "https://arxiv.org/abs/2509.24122", "authors": ["Hongbo Liu", "Jia Xu"], "title": "Echo Flow Networks", "comment": "Under Review", "summary": "At the heart of time-series forecasting (TSF) lies a fundamental challenge:\nhow can models efficiently and effectively capture long-range temporal\ndependencies across ever-growing sequences? While deep learning has brought\nnotable progress, conventional architectures often face a trade-off between\ncomputational complexity and their ability to retain accumulative information\nover extended horizons.\n  Echo State Networks (ESNs), a class of reservoir computing models, have\nrecently regained attention for their exceptional efficiency, offering constant\nmemory usage and per-step training complexity regardless of input length. This\nmakes them particularly attractive for modeling extremely long-term event\nhistory in TSF. However, traditional ESNs fall short of state-of-the-art\nperformance due to their limited nonlinear capacity, which constrains both\ntheir expressiveness and stability.\n  We introduce Echo Flow Networks (EFNs), a framework composed of a group of\nextended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel\nMatrix-Gated Composite Random Activation (MCRA), which enables complex,\nneuron-specific temporal dynamics, significantly expanding the network's\nrepresentational capacity without compromising computational efficiency. In\naddition, we propose a dual-stream architecture in which recent input history\ndynamically selects signature reservoir features from an infinite-horizon\nmemory, leading to improved prediction accuracy and long-term stability.\n  Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to\n4x faster training and 3x smaller model size compared to leading methods like\nPatchTST, reducing forecasting error from 43% to 35%, a 20% relative\nimprovement. One instantiation of our framework, EchoFormer, consistently\nachieves new state-of-the-art performance across five benchmark datasets: ETTh,\nETTm, DMV, Weather, and Air Quality.", "AI": {"tldr": "Echo Flow Networks (EFNs) \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u77e9\u9635\u95e8\u63a7\u590d\u5408\u968f\u673a\u6fc0\u6d3b\u548c\u53cc\u6d41\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u4fe1\u606f\u4fdd\u6301\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u800c\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u867d\u7136\u9ad8\u6548\u4f46\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faEFNs\u6846\u67b6\uff0c\u5305\u542b\u6269\u5c55\u56de\u58f0\u72b6\u6001\u7f51\u7edc(X-ESNs)\u548cMLP\u8bfb\u51fa\u5c42\uff0c\u91c7\u7528\u65b0\u9896\u7684\u77e9\u9635\u95e8\u63a7\u590d\u5408\u968f\u673a\u6fc0\u6d3b(MCRA)\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4f7f\u7528\u53cc\u6d41\u67b6\u6784\u52a8\u6001\u9009\u62e9\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cEFNs\u76f8\u6bd4PatchTST\u7b49\u65b9\u6cd5\u8bad\u7ec3\u901f\u5ea6\u5feb4\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u5c0f3\u500d\uff0c\u9884\u6d4b\u8bef\u5dee\u4ece43%\u964d\u81f335%\uff0c\u76f8\u5bf9\u6539\u8fdb20%\u3002EchoFormer\u5b9e\u4f8b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "EFNs\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.24125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24125", "abs": "https://arxiv.org/abs/2509.24125", "authors": ["Rohan Alur", "Chris Hays", "Manish Raghavan", "Devavrat Shah"], "title": "The Impossibility of Inverse Permutation Learning in Transformer Models", "comment": null, "summary": "In this technical note, we study the problem of inverse permutation learning\nin decoder-only transformers. Given a permutation and a string to which that\npermutation has been applied, the model is tasked with producing the original\n(``canonical'') string. We argue that this task models a natural robustness\nproperty across a variety of reasoning tasks, including long-context retrieval,\nmultiple choice QA and in-context learning. Our primary contribution is an\nimpossibility result: we show that an arbitrary depth, decoder-only transformer\ncannot learn this task. This result concerns the expressive capacity of\ndecoder-only transformer models and is agnostic to training dynamics or sample\ncomplexity. We give a pair of alternative constructions under which inverse\npermutation learning is feasible. The first of these highlights the fundamental\nrole of the causal attention mask, and reveals a gap between the expressivity\nof encoder-decoder transformers and the more popular decoder-only architecture.\nThe latter result is more surprising: we show that simply padding the input\nwith ``scratch tokens\" yields a construction under which inverse permutation\nlearning is possible. We conjecture that this may suggest an alternative\nmechanism by which chain-of-thought prompting or, more generally, intermediate\n``thinking'' tokens can enable reasoning in large language models, even when\nthese tokens encode no meaningful semantic information (e.g., the results of\nintermediate computations).", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u4ec5\u89e3\u7801\u5668Transformer\u65e0\u6cd5\u5b66\u4e60\u9006\u6392\u5217\u4efb\u52a1\uff0c\u4f46\u901a\u8fc7\u6dfb\u52a0\u8349\u7a3f\u4ee4\u724c\u53ef\u4ee5\u5b9e\u73b0\u8be5\u4efb\u52a1\uff0c\u8fd9\u4e3a\u601d\u7ef4\u94fe\u63d0\u793a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u91ca\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u4ec5\u89e3\u7801\u5668Transformer\u5728\u9006\u6392\u5217\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u8be5\u4efb\u52a1\u6a21\u62df\u4e86\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u5c5e\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4ec5\u89e3\u7801\u5668Transformer\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u53ef\u884c\u6784\u9020\uff1a\u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\u548c\u6dfb\u52a0\u8349\u7a3f\u4ee4\u724c\u3002", "result": "\u53d1\u73b0\u4ec5\u89e3\u7801\u5668Transformer\u5728\u6807\u51c6\u8bbe\u7f6e\u4e0b\u65e0\u6cd5\u5b66\u4e60\u9006\u6392\u5217\u4efb\u52a1\uff0c\u4f46\u901a\u8fc7\u6dfb\u52a0\u65e0\u610f\u4e49\u7684\u4e2d\u95f4\u8ba1\u7b97\u4ee4\u724c\u53ef\u4ee5\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "conclusion": "\u8349\u7a3f\u4ee4\u724c\u53ef\u80fd\u4e3a\u601d\u7ef4\u94fe\u63d0\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u673a\u5236\uff0c\u5373\u4f7f\u8fd9\u4e9b\u4ee4\u724c\u4e0d\u5305\u542b\u8bed\u4e49\u4fe1\u606f\uff0c\u4e5f\u80fd\u901a\u8fc7\u4e2d\u95f4\u8ba1\u7b97\u652f\u6301\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2509.24140", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24140", "abs": "https://arxiv.org/abs/2509.24140", "authors": ["H. N. Mhaskar", "Ryan O'Dowd"], "title": "A signal separation view of classification", "comment": null, "summary": "The problem of classification in machine learning has often been approached\nin terms of function approximation. In this paper, we propose an alternative\napproach for classification in arbitrary compact metric spaces which, in\ntheory, yields both the number of classes, and a perfect classification using a\nminimal number of queried labels. Our approach uses localized trigonometric\npolynomial kernels initially developed for the point source signal separation\nproblem in signal processing. Rather than point sources, we argue that the\nvarious classes come from different probability distributions. The localized\nkernel technique developed for separating point sources is then shown to\nseparate the supports of these distributions. This is done in a hierarchical\nmanner in our MASC algorithm to accommodate touching/overlapping class\nboundaries. We illustrate our theory on several simulated and real life\ndatasets, including the Salinas and Indian Pines hyperspectral datasets and a\ndocument dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u4e09\u89d2\u591a\u9879\u5f0f\u6838\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4efb\u610f\u7d27\u81f4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u786e\u5b9a\u7c7b\u522b\u6570\u91cf\u5e76\u4f7f\u7528\u6700\u5c11\u6807\u7b7e\u67e5\u8be2\u5b9e\u73b0\u5b8c\u7f8e\u5206\u7c7b", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u95ee\u9898\u5e38\u91c7\u7528\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u51fa\u66ff\u4ee3\u65b9\u6848\uff0c\u65e8\u5728\u7406\u8bba\u4e0a\u540c\u65f6\u786e\u5b9a\u7c7b\u522b\u6570\u91cf\u5e76\u4f7f\u7528\u6700\u5c11\u6807\u7b7e\u5b9e\u73b0\u5b8c\u7f8e\u5206\u7c7b", "method": "\u4f7f\u7528\u6700\u521d\u4e3a\u4fe1\u53f7\u5904\u7406\u4e2d\u70b9\u6e90\u4fe1\u53f7\u5206\u79bb\u95ee\u9898\u5f00\u53d1\u7684\u5c40\u90e8\u4e09\u89d2\u591a\u9879\u5f0f\u6838\u6280\u672f\uff0c\u5c06\u4e0d\u540c\u7c7b\u522b\u89c6\u4e3a\u6765\u81ea\u4e0d\u540c\u6982\u7387\u5206\u5e03\uff0c\u901a\u8fc7\u5206\u5c42MASC\u7b97\u6cd5\u5904\u7406\u91cd\u53e0/\u63a5\u89e6\u7684\u7c7b\u522b\u8fb9\u754c", "result": "\u5728\u591a\u4e2a\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u5305\u62ecSalinas\u548cIndian Pines\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4ee5\u53ca\u6587\u6863\u6570\u636e\u96c6", "conclusion": "\u5c40\u90e8\u6838\u6280\u672f\u80fd\u591f\u6709\u6548\u5206\u79bb\u4e0d\u540c\u6982\u7387\u5206\u5e03\u7684\u652f\u6301\u96c6\uff0c\u4e3a\u5206\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5"}}
{"id": "2509.24146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24146", "abs": "https://arxiv.org/abs/2509.24146", "authors": ["Ethan Zachary Lo", "Dan Chie-Tien Lo"], "title": "Evaluation of Machine and Deep Learning Techniques for Cyclone Trajectory Regression and Status Classification by Time Series Data", "comment": null, "summary": "Accurate cyclone forecasting is essential for minimizing loss of life,\ninfrastructure damage, and economic disruption. Traditional numerical weather\nprediction models, though effective, are computationally intensive and prone to\nerror due to the chaotic nature of atmospheric systems. This study proposes a\nmachine learning (ML) approach to forecasting tropical cyclone trajectory and\nstatus using time series data from the National Hurricane Center, including\nrecently added best track wind radii. A two-stage ML pipeline is developed: a\nregression model first predicts cyclone features maximum wind speed, minimum\npressure, trajectory length, and directional change using a sliding window of\nhistorical data. These outputs are then input into classification models to\npredict the cyclone's categorical status. Gradient boosting regression and\nthree classifiers random forest (RF), support vector machine (SVM), and\nmultilayer perceptron (MLP) are evaluated. After hyperparameter tuning and\nsynthetic minority oversampling (SMOTE), the RF classifier achieves the highest\nperformance with 93% accuracy, outperforming SVM and MLP across precision,\nrecall, and F1 score. The RF model is particularly robust in identifying\nminority cyclone statuses and minimizing false negatives. Regression results\nyield low mean absolute errors, with pressure and wind predictions within about\n2.2 mb and 2.4 kt, respectively. These findings demonstrate that ML models,\nespecially ensemble-based classifiers, offer an effective, scalable alternative\nto traditional forecasting methods, with potential for real-time cyclone\nprediction and integration into decision support systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53f0\u98ce\u8f68\u8ff9\u548c\u72b6\u6001\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5728\u53f0\u98ce\u72b6\u6001\u5206\u7c7b\u4e0a\u8fbe\u523093%\u7684\u51c6\u786e\u7387\uff0c\u4e3a\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5bb9\u6613\u56e0\u5927\u6c14\u7cfb\u7edf\u6df7\u6c8c\u7279\u6027\u4ea7\u751f\u8bef\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u53f0\u98ce\u9884\u6d4b\u65b9\u6cd5\u6765\u51cf\u5c11\u751f\u547d\u635f\u5931\u3001\u57fa\u7840\u8bbe\u65bd\u7834\u574f\u548c\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u5f00\u53d1\u4e24\u9636\u6bb5\u673a\u5668\u5b66\u4e60\u7ba1\u9053\uff1a\u9996\u5148\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5386\u53f2\u6570\u636e\u901a\u8fc7\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u53f0\u98ce\u7279\u5f81\uff08\u6700\u5927\u98ce\u901f\u3001\u6700\u4f4e\u6c14\u538b\u3001\u8f68\u8ff9\u957f\u5ea6\u548c\u65b9\u5411\u53d8\u5316\uff09\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8f93\u51fa\u8f93\u5165\u5230\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\u4e2d\u9884\u6d4b\u53f0\u98ce\u72b6\u6001\u7c7b\u522b\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5728\u8d85\u53c2\u6570\u8c03\u6574\u548cSMOTE\u5904\u7406\u540e\u8fbe\u523093%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSVM\u548cMLP\uff0c\u5728\u5c11\u6570\u7c7b\u522b\u8bc6\u522b\u548c\u51cf\u5c11\u5047\u9634\u6027\u65b9\u9762\u8868\u73b0\u7a33\u5065\uff1b\u56de\u5f52\u6a21\u578b\u7684\u6c14\u538b\u548c\u98ce\u901f\u9884\u6d4b\u8bef\u5dee\u5206\u522b\u4e3a2.2mb\u548c2.4kt\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u96c6\u6210\u7684\u5206\u7c7b\u5668\uff0c\u4e3a\u4f20\u7edf\u9884\u62a5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u65f6\u53f0\u98ce\u9884\u6d4b\u548c\u96c6\u6210\u5230\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.24166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24166", "abs": "https://arxiv.org/abs/2509.24166", "authors": ["Arpit Garg", "Hemanth Saratchandran", "Ravi Garg", "Simon Lucey"], "title": "Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs", "comment": "In Submission", "summary": "Machine unlearning in large language models (LLMs) is essential for privacy\nand safety; however, existing approaches remain unstable and unreliable. A\nwidely used strategy, the gradient difference method, applies gradient descent\non retained data while performing gradient ascent on forget data, the data\nwhose influence should be removed. However, when combined with cross-entropy\nloss, this procedure causes unbounded growth of weights and gradients, leading\nto training instability and degrading both forgetting and retention. We provide\na theoretical framework that explains this failure, explicitly showing how\nascent on the forget set destabilizes optimization in the feedforward MLP\nlayers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient\nUnlearning, a parameter-efficient approach that stabilizes LoRA-based\nfine-tuning by applying bounded functions to MLP adapters. This simple\nmodification controls the weight dynamics during ascent, enabling the gradient\ndifference method to converge reliably. Across the TOFU, TDEC, and MUSE\nbenchmarks, and across architectures and scales from 125M to 8B parameters, our\nmethod achieves substantial improvements in forgetting while preserving\nretention, establishing a novel theoretically grounded and practically scalable\nframework for unlearning in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u754c\u53c2\u6570\u9ad8\u6548\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6709\u754c\u51fd\u6570\u4e2d\u5e94\u7528MLP\u9002\u914d\u5668\u6765\u7a33\u5b9aLoRA\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u5dee\u5f02\u65b9\u6cd5\u5728LLM\u9057\u5fd8\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u4e0d\u7a33\u5b9a\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u68af\u5ea6\u5dee\u5f02\u65b9\u6cd5\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u4f1a\u5bfc\u81f4\u6743\u91cd\u548c\u68af\u5ea6\u65e0\u754c\u589e\u957f\uff0c\u9020\u6210\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u5e76\u5f71\u54cd\u9057\u5fd8\u548c\u4fdd\u7559\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u6709\u754c\u53c2\u6570\u9ad8\u6548\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6709\u754c\u51fd\u6570\u4e2d\u5e94\u7528MLP\u9002\u914d\u5668\u6765\u7a33\u5b9aLoRA\u5fae\u8c03\uff0c\u63a7\u5236\u6743\u91cd\u52a8\u6001\u53d8\u5316\uff0c\u4f7f\u68af\u5ea6\u5dee\u5f02\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u6536\u655b\u3002", "result": "\u5728TOFU\u3001TDEC\u548cMUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u53ca\u4ece125M\u52308B\u53c2\u6570\u7684\u4e0d\u540c\u67b6\u6784\u548c\u89c4\u6a21\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u9057\u5fd8\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4fdd\u7559\u6548\u679c\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u53ef\u6269\u5c55\u7684LLM\u9057\u5fd8\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2509.24168", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24168", "abs": "https://arxiv.org/abs/2509.24168", "authors": ["Qipeng Zhan", "Zhuoping Zhou", "Zexuan Wang", "Li Shen"], "title": "Multi-Scale Geometric Autoencoder", "comment": null, "summary": "Autoencoders have emerged as powerful models for visualization and\ndimensionality reduction based on the fundamental assumption that\nhigh-dimensional data is generated from a low-dimensional manifold. A critical\nchallenge in autoencoder design is to preserve the geometric structure of data\nin the latent space, with existing approaches typically focusing on either\nglobal or local geometric properties separately. Global approaches often\nencounter errors in distance approximation that accumulate, while local methods\nfrequently converge to suboptimal solutions that distort large-scale\nrelationships. We propose Multi-Scale Geometric Autoencoder (MAE), which\nintroduces an asymmetric architecture that simultaneously preserves both scales\nof the geometric structure by applying global distance constraints to the\nencoder and local geometric constraints to the decoder. Through theoretical\nanalysis, we establish that this asymmetric design aligns naturally with the\ndistinct roles of the encoder and decoder components. Our comprehensive\nexperiments on both synthetic manifolds and real-world datasets demonstrate\nthat MAE consistently outperforms existing methods across various evaluation\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u51e0\u4f55\u81ea\u7f16\u7801\u5668(MAE)\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u67b6\u6784\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7684\u5168\u5c40\u548c\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u7f16\u7801\u5668\u5e94\u7528\u5168\u5c40\u8ddd\u79bb\u7ea6\u675f\uff0c\u5728\u89e3\u7801\u5668\u5e94\u7528\u5c40\u90e8\u51e0\u4f55\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\u901a\u5e38\u5206\u522b\u5173\u6ce8\u5168\u5c40\u6216\u5c40\u90e8\u51e0\u4f55\u7279\u6027\uff0c\u5168\u5c40\u65b9\u6cd5\u5b58\u5728\u8ddd\u79bb\u8fd1\u4f3c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5c40\u90e8\u65b9\u6cd5\u5bb9\u6613\u6536\u655b\u5230\u626d\u66f2\u5927\u89c4\u6a21\u5173\u7cfb\u7684\u6b21\u4f18\u89e3\u3002", "method": "\u91c7\u7528\u975e\u5bf9\u79f0\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u7f16\u7801\u5668\u65bd\u52a0\u5168\u5c40\u8ddd\u79bb\u7ea6\u675f\uff0c\u5728\u89e3\u7801\u5668\u65bd\u52a0\u5c40\u90e8\u51e0\u4f55\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u7559\u591a\u5c3a\u5ea6\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u5408\u6210\u6d41\u5f62\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cMAE\u5728\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u975e\u5bf9\u79f0\u8bbe\u8ba1\u81ea\u7136\u5951\u5408\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4e0d\u540c\u89d2\u8272\uff0c\u80fd\u591f\u6709\u6548\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7684\u5168\u5c40\u548c\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2509.24171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24171", "abs": "https://arxiv.org/abs/2509.24171", "authors": ["Ruibo Chen", "Sheng Zhang", "Yihan Wu", "Tong Zheng", "Peihua Mai", "Heng Huang"], "title": "Model Correlation Detection via Random Selection Probing", "comment": null, "summary": "The growing prevalence of large language models (LLMs) and vision-language\nmodels (VLMs) has heightened the need for reliable techniques to determine\nwhether a model has been fine-tuned from or is even identical to another.\nExisting similarity-based methods often require access to model parameters or\nproduce heuristic scores without principled thresholds, limiting their\napplicability. We introduce Random Selection Probing (RSP), a\nhypothesis-testing framework that formulates model correlation detection as a\nstatistical test. RSP optimizes textual or visual prefixes on a reference model\nfor a random selection task and evaluates their transferability to a target\nmodel, producing rigorous p-values that quantify evidence of correlation. To\nmitigate false positives, RSP incorporates an unrelated baseline model to\nfilter out generic, transferable features. We evaluate RSP across both LLMs and\nVLMs under diverse access conditions for reference models and test models.\nExperiments on fine-tuned and open-source models show that RSP consistently\nyields small p-values for related models while maintaining high p-values for\nunrelated ones. Extensive ablation studies further demonstrate the robustness\nof RSP. These results establish RSP as the first principled and general\nstatistical framework for model correlation detection, enabling transparent and\ninterpretable decisions in modern machine learning ecosystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86RSP\uff08\u968f\u673a\u9009\u62e9\u63a2\u6d4b\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u6765\u68c0\u6d4b\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4e3aLLM\u548cVLM\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684p\u503c\u6765\u91cf\u5316\u6a21\u578b\u5173\u8054\u8bc1\u636e\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u76f8\u4f3c\u6027\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\u6216\u4ea7\u751f\u65e0\u539f\u5219\u9608\u503c\u7684\u542f\u53d1\u5f0f\u5206\u6570\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u9700\u8981\u4e00\u79cd\u539f\u5219\u6027\u7684\u7edf\u8ba1\u6846\u67b6\u6765\u53ef\u9760\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u7ecf\u8fc7\u5fae\u8c03\u6216\u5b8c\u5168\u76f8\u540c\u3002", "method": "RSP\u5728\u53c2\u8003\u6a21\u578b\u4e0a\u4f18\u5316\u6587\u672c\u6216\u89c6\u89c9\u524d\u7f00\u7528\u4e8e\u968f\u673a\u9009\u62e9\u4efb\u52a1\uff0c\u7136\u540e\u8bc4\u4f30\u8fd9\u4e9b\u524d\u7f00\u5728\u76ee\u6807\u6a21\u578b\u4e0a\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u4ea7\u751f\u4e25\u683c\u7684p\u503c\u6765\u91cf\u5316\u76f8\u5173\u6027\u8bc1\u636e\u3002\u901a\u8fc7\u5f15\u5165\u65e0\u5173\u57fa\u7ebf\u6a21\u578b\u8fc7\u6ee4\u901a\u7528\u53ef\u8f6c\u79fb\u7279\u5f81\u6765\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u5728LLM\u548cVLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRSP\u5bf9\u76f8\u5173\u6a21\u578b\u4ea7\u751f\u5c0fp\u503c\uff0c\u5bf9\u65e0\u5173\u6a21\u578b\u4fdd\u6301\u9ad8p\u503c\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86RSP\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RSP\u662f\u7b2c\u4e00\u4e2a\u539f\u5219\u6027\u7684\u901a\u7528\u7edf\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u578b\u76f8\u5173\u6027\u68c0\u6d4b\uff0c\u80fd\u591f\u5728\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002"}}
{"id": "2509.24176", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24176", "abs": "https://arxiv.org/abs/2509.24176", "authors": ["Chuntian Chi", "John Clapham", "Leslie Cloud", "Ingrid Pretzer-Aboff", "GinaMari Blackwell", "Huajie Shao", "Gang Zhou"], "title": "FM-FoG: A Real-Time Foundation Model-based Wearable System for Freezing-of-Gait Mitigation", "comment": "This is a preprint version, 12 pages, 7 figures, 8 tables", "summary": "Freezing-of-Gait (FoG) affects over 50% of mid-to-late stage Parkinson's\ndisease (PD) patients, significantly impairing patients' mobility independence\nand reducing quality of life. FoG is characterized by sudden episodes where\nwalking cannot start or is interrupted, occurring exclusively during standing\nor walking, and never while sitting or lying down. Current FoG detection\nsystems require extensive patient-specific training data and lack\ngeneralization, limiting clinical deployment. To address these issues, we\nintroduce FM-FoG, a real-time foundation model-based wearable system achieving\nFoG detection in unseen patients without patient-specific training. Our\napproach combines self-supervised pretraining on diverse Inertial Measurement\nUnit (IMU) datasets with sensor context integration. Since FoG occurs only\nduring ambulatory activities, a lightweight CNN-LSTM activity classifier\nselectively activates the foundation model only during walking or standing,\navoiding unnecessary computation. Evaluated on the VCU FoG-IMU dataset with 23\nPD patients, FM-FoG achieves a 98.5% F1-score when tested on previously unseen\npatients, substantially outperforming competitive baseline methods. Deployed on\na Google Pixel 8a smartphone, the system extends battery life by up to 72%\nwhile maintaining sub-20ms intervention latency. The results indicate that our\nFM-FoG can enable practical, energy-efficient healthcare applications that\ngeneralize across patients without individual training requirements.", "AI": {"tldr": "FM-FoG\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u5b9e\u65f6\u53ef\u7a7f\u6234\u7cfb\u7edf\uff0c\u53ef\u5728\u65e0\u9700\u60a3\u8005\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u7684\u51bb\u7ed3\u6b65\u6001\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u60a3\u8005\u4e2d\u8fbe\u523098.5%\u7684F1\u5206\u6570\uff0c\u5e76\u5728\u667a\u80fd\u624b\u673a\u4e0a\u5b9e\u73b072%\u7684\u7535\u6c60\u7eed\u822a\u63d0\u5347\u3002", "motivation": "\u51bb\u7ed3\u6b65\u6001\u5f71\u54cd\u8d85\u8fc750%\u7684\u4e2d\u665a\u671f\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\uff0c\u663e\u8457\u635f\u5bb3\u60a3\u8005\u884c\u52a8\u72ec\u7acb\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u3002\u73b0\u6709\u68c0\u6d4b\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u60a3\u8005\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u4e14\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u90e8\u7f72\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u4f20\u611f\u5668\u4e0a\u4e0b\u6587\u96c6\u6210\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7CNN-LSTM\u6d3b\u52a8\u5206\u7c7b\u5668\u4ec5\u5728\u884c\u8d70\u6216\u7ad9\u7acb\u65f6\u6fc0\u6d3b\u57fa\u7840\u6a21\u578b\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u8ba1\u7b97\u3002", "result": "\u572823\u540dPD\u60a3\u8005\u7684VCU FoG-IMU\u6570\u636e\u96c6\u4e0a\uff0cFM-FoG\u5728\u672a\u89c1\u8fc7\u7684\u60a3\u8005\u4e2d\u8fbe\u523098.5%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728Google Pixel 8a\u667a\u80fd\u624b\u673a\u4e0a\u90e8\u7f72\uff0c\u7cfb\u7edf\u7535\u6c60\u7eed\u822a\u63d0\u5347\u8fbe72%\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u4e8e20ms\u7684\u5e72\u9884\u5ef6\u8fdf\u3002", "conclusion": "FM-FoG\u80fd\u591f\u5b9e\u73b0\u5b9e\u7528\u3001\u8282\u80fd\u7684\u533b\u7597\u5e94\u7528\uff0c\u65e0\u9700\u4e2a\u4f53\u8bad\u7ec3\u8981\u6c42\u5373\u53ef\u8de8\u60a3\u8005\u6cdb\u5316\u3002"}}
{"id": "2509.24198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24198", "abs": "https://arxiv.org/abs/2509.24198", "authors": ["Linghao Kong", "Angelina Ning", "Micah Adler", "Nir Shavit"], "title": "Negative Pre-activations Differentiate Syntax", "comment": "10 pages, 7 figures", "summary": "A recently discovered class of entangled neurons, known as Wasserstein\nneurons, is disproportionately critical in large language models despite\nconstituting only a very small fraction of the network: their targeted removal\ncollapses the model, consistent with their unique role in differentiating\nsimilar inputs. Interestingly, in Wasserstein neurons immediately preceding\nsmooth activation functions, such differentiation manifests in the negative\npre-activation space, especially in early layers. Pairs of similar inputs are\ndriven to highly distinct negative values, and these pairs involve syntactic\ntokens such as determiners and prepositions. We show that this negative region\nis functional rather than simply favorable for optimization. A minimal,\nsign-specific intervention that zeroes only the negative pre-activations of a\nsmall subset of entangled neurons significantly weakens overall model function\nand disrupts grammatical behavior, while both random and perplexity-matched\ncontrols leave grammatical performance largely unchanged. Part of speech\nanalysis localizes the excess surprisal to syntactic scaffolding tokens, and\nlayer-specific interventions reveal that small local degradations accumulate\nacross depth. Over training checkpoints, the same ablation impairs grammatical\nbehavior as Wasserstein neurons emerge and stabilize. Together, these results\nidentify negative differentiation in a sparse subset of entangled neurons as a\ncrucial mechanism that language models rely on for syntax.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0Wasserstein\u795e\u7ecf\u5143\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u901a\u8fc7\u8d1f\u9884\u6fc0\u6d3b\u7a7a\u95f4\u7684\u5dee\u5f02\u5316\u5904\u7406\u6765\u533a\u5206\u76f8\u4f3c\u8f93\u5165\uff0c\u7279\u522b\u662f\u5bf9\u53e5\u6cd5\u6807\u8bb0\u7684\u5904\u7406\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e00\u7c7b\u7279\u6b8a\u7684\u7ea0\u7f20\u795e\u7ecf\u5143\uff08Wasserstein\u795e\u7ecf\u5143\uff09\u7684\u529f\u80fd\u673a\u5236\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u867d\u7136\u6570\u91cf\u5f88\u5c11\u4f46\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u79fb\u9664\u5b9e\u9a8c\u3001\u7b26\u53f7\u7279\u5f02\u6027\u5e72\u9884\uff08\u5c06\u8d1f\u9884\u6fc0\u6d3b\u5f52\u96f6\uff09\u3001\u8bcd\u6027\u5206\u6790\u548c\u5206\u5c42\u5e72\u9884\u7b49\u65b9\u6cd5\uff0c\u7814\u7a76Wasserstein\u795e\u7ecf\u5143\u5728\u8bed\u6cd5\u5904\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u9488\u5bf9Wasserstein\u795e\u7ecf\u5143\u8d1f\u9884\u6fc0\u6d3b\u7684\u5e72\u9884\u663e\u8457\u524a\u5f31\u4e86\u6a21\u578b\u529f\u80fd\u5e76\u7834\u574f\u4e86\u8bed\u6cd5\u884c\u4e3a\uff0c\u800c\u968f\u673a\u548c\u56f0\u60d1\u5ea6\u5339\u914d\u7684\u63a7\u5236\u7ec4\u5bf9\u8bed\u6cd5\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\u3002\u8bcd\u6027\u5206\u6790\u663e\u793a\u5f02\u5e38\u4e3b\u8981\u51fa\u73b0\u5728\u53e5\u6cd5\u652f\u67b6\u6807\u8bb0\u4e0a\u3002", "conclusion": "\u8d1f\u5dee\u5f02\u5316\u5728\u7a00\u758f\u7684\u7ea0\u7f20\u795e\u7ecf\u5143\u5b50\u96c6\u4e2d\u662f\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u7684\u5173\u952e\u673a\u5236\uff0c\u7528\u4e8e\u5904\u7406\u8bed\u6cd5\u7ed3\u6784\u3002"}}
{"id": "2509.24203", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24203", "abs": "https://arxiv.org/abs/2509.24203", "authors": ["Chaorui Yao", "Yanxi Chen", "Yuchang Sun", "Yushuo Chen", "Wenhao Zhang", "Xuchen Pan", "Yaliang Li", "Bolin Ding"], "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends", "comment": null, "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.", "AI": {"tldr": "\u672c\u6587\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u4e86\u7ec4\u76f8\u5bf9REINFORCE\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u5929\u7136\u652f\u6301\u79bb\u7b56\u7565\u89e3\u91ca\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u901a\u7528\u539f\u5219\u6765\u9002\u914d\u79bb\u7b56\u7565\u8bbe\u7f6e\uff0c\u5e76\u7edf\u4e00\u89e3\u91ca\u4e86\u591a\u4e2a\u8fd1\u671f\u7b97\u6cd5\u3002", "motivation": "\u53d7\u5b9e\u9645\u5e94\u7528\u7ea6\u675f\u3001LLM-RL\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\u548cRL\u65b9\u6cd5\u521b\u65b0\u7684\u9a71\u52a8\uff0c\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u7ec4\u76f8\u5bf9REINFORCE\u7b97\u6cd5\uff0c\u63d0\u51fa\u6b63\u5219\u5316\u7b56\u7565\u66f4\u65b0\u548c\u4e3b\u52a8\u5851\u9020\u6570\u636e\u5206\u5e03\u4e24\u4e2a\u539f\u5219\uff0c\u7edf\u4e00\u89e3\u91caOPMD\u548cAsymRE\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u539f\u5219\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u7684\u79bb\u7b56\u7565RL\u63d0\u4f9b\u4e86\u65b0\u7684\u7b97\u6cd5\u8bbe\u8ba1\u673a\u4f1a\u3002", "conclusion": "REINFORCE\u7c7b\u7b97\u6cd5\u5929\u7136\u652f\u6301\u79bb\u7b56\u7565\u89e3\u91ca\uff0c\u63d0\u51fa\u7684\u539f\u5219\u4e3aLLM\u7684\u79bb\u7b56\u7565RL\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2509.24217", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.24217", "abs": "https://arxiv.org/abs/2509.24217", "authors": ["Yuyang Sha", "Hongxin Pan", "Gang Luo", "Caijuan Shi", "Jing Wang", "Kefeng Li"], "title": "MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis", "comment": null, "summary": "Background Major depressive disorder (MDD) is a leading cause of global\ndisability, yet current diagnostic approaches often rely on subjective\nassessments and lack the ability to integrate multimodal clinical information.\nLarge language models (LLMs) hold promise for enhancing diagnostic accuracy\nthrough advanced reasoning but face challenges in interpretability,\nhallucination, and reliance on synthetic data.\n  Methods We developed MDD-Thinker, an LLM-based diagnostic framework that\nintegrates supervised fine-tuning (SFT) with reinforcement learning (RL) to\nstrengthen reasoning ability and interpretability. Using the UK Biobank\ndataset, we generated 40,000 reasoning samples, supplemented with 10,000\nsamples from publicly available mental health datasets. The model was\nfine-tuned on these reasoning corpora, and its diagnostic and reasoning\nperformance was evaluated against machine learning, deep learning, and\nstate-of-the-art LLM baselines.\n  Findings MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081,\nsignificantly outperforming traditional baselines such as SVM and MLP, as well\nas general-purpose LLMs. Incorporating both SFT and RL yielded the greatest\nimprovements, with relative gains of 29.0% in accuracy, 38.1% in F1-score, and\n34.8% in AUC. Moreover, the model demonstrated comparable reasoning performance\ncompared to much larger LLMs, while maintaining computational efficiency.\n  Interpretation This study presents the first reasoning-enhanced LLM framework\nfor MDD diagnosis trained on large-scale real-world clinical data. By\nintegrating SFT and RL, MDD-Thinker balances accuracy, interpretability, and\nefficiency, offering a scalable approach for intelligent psychiatric\ndiagnostics. These findings suggest that reasoning-oriented LLMs can provide\nclinically reliable support for MDD detection and may inform broader\napplications in mental health care.", "AI": {"tldr": "MDD-Thinker\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6291\u90c1\u75c7\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5728UK Biobank\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u901a\u7528LLM\u3002", "motivation": "\u5f53\u524d\u6291\u90c1\u75c7\u8bca\u65ad\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u4e34\u5e8a\u4fe1\u606f\u6574\u5408\u80fd\u529b\u3002LLM\u6709\u6f5c\u529b\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f46\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u3001\u5e7b\u89c9\u548c\u4f9d\u8d56\u5408\u6210\u6570\u636e\u7b49\u6311\u6218\u3002", "method": "\u5f00\u53d1MDD-Thinker\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002\u4f7f\u7528UK Biobank\u6570\u636e\u96c6\u751f\u621040,000\u4e2a\u63a8\u7406\u6837\u672c\uff0c\u8865\u514510,000\u4e2a\u516c\u5f00\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\u6837\u672c\u3002\u4e0e\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u6700\u5148\u8fdbLLM\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "MDD-Thinker\u8fbe\u52300.8268\u51c6\u786e\u7387\u548c0.8081 F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8eSVM\u3001MLP\u548c\u901a\u7528LLM\u3002\u7ed3\u5408SFT\u548cRL\u5e26\u6765\u6700\u5927\u6539\u8fdb\uff1a\u51c6\u786e\u7387\u63d0\u534729.0%\uff0cF1\u5206\u6570\u63d0\u534738.1%\uff0cAUC\u63d0\u534734.8%\u3002\u63a8\u7406\u6027\u80fd\u4e0e\u66f4\u5927LLM\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u7684\u63a8\u7406\u589e\u5f3aLLM\u6291\u90c1\u75c7\u8bca\u65ad\u6846\u67b6\u3002MDD-Thinker\u5e73\u8861\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u7cbe\u795e\u79d1\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u8868\u660e\u63a8\u7406\u5bfc\u5411\u7684LLM\u53ef\u4e3a\u6291\u90c1\u75c7\u68c0\u6d4b\u63d0\u4f9b\u4e34\u5e8a\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2509.24218", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24218", "abs": "https://arxiv.org/abs/2509.24218", "authors": ["Junjie Wang", "Pan Zhou", "Yiming Dong", "Huan Li", "Jia Li", "Xun Zhou", "Qicheng Lao", "Cong Fang", "Zhouchen Lin"], "title": "Conda: Column-Normalized Adam for Training Large Language Models Faster", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive generalization and\nemergent capabilities, yet their pre-training remains computationally expensive\nand sensitive to optimization dynamics. While Adam-based optimizers offer fast\nconvergence by adapting learning rates coordinate-wise, recent studies reveal\nthat their updates often suffer from poor spectral conditioning and low-rank\nstructures, hindering efficiency. Muon addresses this issue via global spectral\nnormalization but lacks the per-coordinate adaptivity of Adam. In this work, we\npropose \\textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges\nthe strengths of both approaches. Conda projects updates into an orthogonal\nsubspace and applies column-wise second moment normalization based on the\nprojected gradients, thereby achieving both improved spectral conditioning and\nmaintaining coordinate-wise adaptivity. This design alleviates the spectral\npathologies of Adam while preserving its fast convergence behavior. Extensive\nexperiments on the LLaMA and GPT-2 series show that Conda consistently\noutperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on\nthe LLaMA series, \\textbf{Conda achieves $2{\\sim}2.5\\times$ the convergence\nspeed of AdamW, measured in both training steps and training time.} Further\nablations demonstrate its robustness under diverse training setups. These\nresults collectively highlight Conda as an effective and broadly applicable\noptimizer for large-scale LLM training. The code is released on\nhttps://github.com/jie040109/Conda", "AI": {"tldr": "\u63d0\u51fa\u4e86Conda\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86Adam\u7684\u5750\u6807\u81ea\u9002\u5e94\u6027\u548cMuon\u7684\u8c31\u5f52\u4e00\u5316\u4f18\u52bf\uff0c\u5728LLaMA\u548cGPT-2\u7cfb\u5217\u4e0a\u5b9e\u73b02-2.5\u500d\u4e8eAdamW\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "Adam\u4f18\u5316\u5668\u867d\u7136\u6536\u655b\u5feb\uff0c\u4f46\u5b58\u5728\u8c31\u6761\u4ef6\u5dee\u548c\u4f4e\u79e9\u7ed3\u6784\u95ee\u9898\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u7387\uff1bMuon\u901a\u8fc7\u5168\u5c40\u8c31\u5f52\u4e00\u5316\u89e3\u51b3\u4e86\u8c31\u95ee\u9898\u4f46\u7f3a\u4e4f\u5750\u6807\u81ea\u9002\u5e94\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "Conda\u5c06\u66f4\u65b0\u6295\u5f71\u5230\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u5e76\u57fa\u4e8e\u6295\u5f71\u68af\u5ea6\u5e94\u7528\u5217\u7ea7\u4e8c\u9636\u77e9\u5f52\u4e00\u5316\uff0c\u65e2\u6539\u5584\u8c31\u6761\u4ef6\u53c8\u4fdd\u6301\u5750\u6807\u81ea\u9002\u5e94\u3002", "result": "\u5728LLaMA\u548cGPT-2\u7cfb\u5217\u9884\u8bad\u7ec3\u4e2d\uff0cConda\u59cb\u7ec8\u4f18\u4e8eAdamW\u3001Muon\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728LLaMA\u4e0a\u8fbe\u5230AdamW 2-2.5\u500d\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "Conda\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u4f18\u5316\u5668\uff0c\u89e3\u51b3\u4e86Adam\u7684\u8c31\u75c5\u7406\u95ee\u9898\u540c\u65f6\u4fdd\u6301\u5feb\u901f\u6536\u655b\u7279\u6027\u3002"}}
{"id": "2509.24223", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24223", "abs": "https://arxiv.org/abs/2509.24223", "authors": ["Jianxin Zhang", "Clayton Scott"], "title": "Semantic Editing with Coupled Stochastic Differential Equations", "comment": null, "summary": "Editing the content of an image with a pretrained text-to-image model remains\nchallenging. Existing methods often distort fine details or introduce\nunintended artifacts. We propose using coupled stochastic differential\nequations (coupled SDEs) to guide the sampling process of any pre-trained\ngenerative model that can be sampled by solving an SDE, including diffusion and\nrectified flow models. By driving both the source image and the edited image\nwith the same correlated noise, our approach steers new samples toward the\ndesired semantics while preserving visual similarity to the source. The method\nworks out-of-the-box-without retraining or auxiliary networks-and achieves high\nprompt fidelity along with near-pixel-level consistency. These results position\ncoupled SDEs as a simple yet powerful tool for controlled generative AI.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u8026\u5408\u968f\u673a\u5fae\u5206\u65b9\u7a0b(coupled SDEs)\u6765\u5f15\u5bfc\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u65f6\u4fdd\u6301\u6e90\u56fe\u50cf\u7ec6\u8282\u7684\u540c\u65f6\u8fbe\u5230\u9ad8\u63d0\u793a\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5e38\u5e38\u626d\u66f2\u7ec6\u8282\u6216\u5f15\u5165\u4f2a\u5f71\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u89c6\u89c9\u76f8\u4f3c\u6027\u540c\u65f6\u5b9e\u73b0\u8bed\u4e49\u7f16\u8f91\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8026\u5408SDEs\uff0c\u901a\u8fc7\u9a71\u52a8\u6e90\u56fe\u50cf\u548c\u7f16\u8f91\u56fe\u50cf\u4f7f\u7528\u76f8\u540c\u7684\u76f8\u5173\u566a\u58f0\uff0c\u5f15\u5bfc\u91c7\u6837\u8fc7\u7a0b\u5411\u671f\u671b\u8bed\u4e49\u53d1\u5c55\u3002", "result": "\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u8f85\u52a9\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u63d0\u793a\u4fdd\u771f\u5ea6\u548c\u63a5\u8fd1\u50cf\u7d20\u7ea7\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "conclusion": "\u8026\u5408SDEs\u662f\u53d7\u63a7\u751f\u6210AI\u7684\u7b80\u5355\u800c\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2509.24224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24224", "abs": "https://arxiv.org/abs/2509.24224", "authors": ["Ashiqur Rahman", "Hamed Alhoori"], "title": "Proposing a Framework for Machine Learning Adoption on Legacy Systems", "comment": "Accepted at The First International Workshop on Resilient Artificial\n  Intelligence for Manufacturing (ICDM'25)", "summary": "The integration of machine learning (ML) is critical for industrial\ncompetitiveness, yet its adoption is frequently stalled by the prohibitive\ncosts and operational disruptions of upgrading legacy systems. The financial\nand logistical overhead required to support the full ML lifecycle presents a\nformidable barrier to widespread implementation, particularly for small and\nmedium-sized enterprises. This paper introduces a pragmatic, API-based\nframework designed to overcome these challenges by strategically decoupling the\nML model lifecycle from the production environment. Our solution delivers the\nanalytical power of ML to domain experts through a lightweight, browser-based\ninterface, eliminating the need for local hardware upgrades and ensuring model\nmaintenance can occur with zero production downtime. This human-in-the-loop\napproach empowers experts with interactive control over model parameters,\nfostering trust and facilitating seamless integration into existing workflows.\nBy mitigating the primary financial and operational risks, this framework\noffers a scalable and accessible pathway to enhance production quality and\nsafety, thereby strengthening the competitive advantage of the manufacturing\nsector.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eAPI\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5c06ML\u6a21\u578b\u751f\u547d\u5468\u671f\u4e0e\u751f\u4ea7\u73af\u5883\u89e3\u8026\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u754c\u9762\u8ba9\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\u6a21\u578b\u8c03\u4f18\uff0c\u907f\u514d\u786c\u4ef6\u5347\u7ea7\u9700\u6c42\u548c\u751f\u4ea7\u505c\u673a\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u9886\u57dfML\u5e94\u7528\u9762\u4e34\u7684\u9ad8\u6210\u672c\u548c\u7cfb\u7edf\u5347\u7ea7\u56f0\u96be\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e2d\u5c0f\u4f01\u4e1a\u5728ML\u5168\u751f\u547d\u5468\u671f\u4e2d\u7684\u8d22\u52a1\u548c\u8fd0\u8425\u969c\u788d\u3002", "method": "\u91c7\u7528API\u6846\u67b6\u5b9e\u73b0ML\u6a21\u578b\u4e0e\u751f\u4ea7\u73af\u5883\u7684\u6218\u7565\u89e3\u8026\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u754c\u9762\u63d0\u4f9b\u4eba\u673a\u4ea4\u4e92\u63a7\u5236\uff0c\u652f\u6301\u96f6\u505c\u673a\u6a21\u578b\u7ef4\u62a4\u3002", "result": "\u6846\u67b6\u80fd\u591f\u964d\u4f4eML\u5b9e\u65bd\u7684\u4e3b\u8981\u8d22\u52a1\u548c\u8fd0\u8425\u98ce\u9669\uff0c\u4e3a\u5236\u9020\u4e1a\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u8bbf\u95ee\u7684ML\u5e94\u7528\u8def\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u51cf\u8f7b\u6210\u672c\u548c\u8fd0\u8425\u969c\u788d\uff0c\u589e\u5f3a\u4e86\u5236\u9020\u4e1a\u7684\u7ade\u4e89\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u751f\u4ea7\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.24228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24228", "abs": "https://arxiv.org/abs/2509.24228", "authors": ["Wei Wang", "Dong-Dong Wu", "Ming Li", "Jingxiong Zhang", "Gang Niu", "Masashi Sugiyama"], "title": "Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms", "comment": null, "summary": "Positive-unlabeled (PU) learning is a weakly supervised binary classification\nproblem, in which the goal is to learn a binary classifier from only positive\nand unlabeled data, without access to negative data. In recent years, many PU\nlearning algorithms have been developed to improve model performance. However,\nexperimental settings are highly inconsistent, making it difficult to identify\nwhich algorithm performs better. In this paper, we propose the first PU\nlearning benchmark to systematically compare PU learning algorithms. During our\nimplementation, we identify subtle yet critical factors that affect the\nrealistic and fair evaluation of PU learning algorithms. On the one hand, many\nPU learning algorithms rely on a validation set that includes negative data for\nmodel selection. This is unrealistic in traditional PU learning settings, where\nno negative data are available. To handle this problem, we systematically\ninvestigate model selection criteria for PU learning. On the other hand, the\nproblem settings and solutions of PU learning have different families, i.e.,\nthe one-sample and two-sample settings. However, existing evaluation protocols\nare heavily biased towards the one-sample setting and neglect the significant\ndifference between them. We identify the internal label shift problem of\nunlabeled training data for the one-sample setting and propose a simple yet\neffective calibration approach to ensure fair comparisons within and across\nfamilies. We hope our framework will provide an accessible, realistic, and fair\nenvironment for evaluating PU learning algorithms in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2aPU\u5b66\u4e60\u57fa\u51c6\uff0c\u7cfb\u7edf\u6bd4\u8f83PU\u5b66\u4e60\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u3001\u6a21\u578b\u9009\u62e9\u4e0d\u73b0\u5b9e\u4ee5\u53ca\u4e0d\u540c\u8bbe\u7f6e\u95f4\u4e0d\u516c\u5e73\u6bd4\u8f83\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PU\u5b66\u4e60\u7b97\u6cd5\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u9ad8\u5ea6\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u786e\u5b9a\u54ea\u79cd\u7b97\u6cd5\u6027\u80fd\u66f4\u597d\uff0c\u4e14\u8bc4\u4f30\u4e2d\u5b58\u5728\u4e0d\u73b0\u5b9e\u548c\u4e0d\u516c\u5e73\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaPU\u5b66\u4e60\u57fa\u51c6\uff0c\u7cfb\u7edf\u7814\u7a76PU\u5b66\u4e60\u7684\u6a21\u578b\u9009\u62e9\u6807\u51c6\uff0c\u8bc6\u522b\u5355\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u5185\u90e8\u6807\u7b7e\u504f\u79fb\u95ee\u9898\u5e76\u63d0\u51fa\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u521b\u5efa\u4e86\u53ef\u8bbf\u95ee\u3001\u73b0\u5b9e\u548c\u516c\u5e73\u7684PU\u5b66\u4e60\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u8fdb\u884c\u7b97\u6cd5\u95f4\u7684\u516c\u5e73\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u672a\u6765PU\u5b66\u4e60\u7b97\u6cd5\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2509.24239", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24239", "abs": "https://arxiv.org/abs/2509.24239", "authors": ["Jincheng Liu", "Sijun He", "Jingjing Wu", "Xiangsen Wang", "Yang Chen", "Zhaoqi Kuang", "Siqi Bao", "Yuan Yao"], "title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models", "comment": null, "summary": "Recent large language models (LLMs) have shown strong reasoning capabilities.\nHowever, a critical question remains: do these models possess genuine reasoning\nskills particularly complex strategic reasoning or are they primarily excelling\nat sophisticated pattern recognition within their training data? To address\nthis question, this paper presents a chess testbed, ChessArena, to evaluate the\nstrategic reasoning capabilities of LLMs. Chess requires complex strategic\nreasoning capabilities including long-term planning, strict rule comprehension,\nand multi-turn conversation memorization. Specifically, ChessArena is a\ncompetitive framework where LLMs play against each other, under four different\nplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.\nThe testbed can also evaluate fine-grained capabilities including basic\nunderstanding, move selection, and puzzle solving. Over 13 LLMs with different\nmodes are evaluated in ChessArena, playing over 800 games. The results reveal\nsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chess\nengine at human amateur level), while some even failed to defeat a random\nplayer that selects moves arbitrarily. We also present a strong baseline to the\ntestbed: our fine-tuned Qwen3-8B substantially improved performance,\napproaching much larger state-of-the-art reasoning models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ChessArena\u6d4b\u8bd5\u5e73\u53f0\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u56fd\u9645\u8c61\u68cb\u5bf9\u5f08\u6d4b\u8bd5\u53d1\u73b0\u5f53\u524dLLMs\u5728\u590d\u6742\u6218\u7565\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5373\u4f7f\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e5f\u4ec5\u80fd\u8fbe\u5230\u4e1a\u4f59\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u590d\u6742\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u64c5\u957f\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u8bc6\u522b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9700\u8981\u957f\u671f\u89c4\u5212\u3001\u4e25\u683c\u89c4\u5219\u7406\u89e3\u548c\u591a\u8f6e\u5bf9\u8bdd\u8bb0\u5fc6\u7684\u6218\u7565\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u6784\u5efaChessArena\u7ade\u4e89\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u8ba9LLMs\u5728\u56db\u79cd\u4e0d\u540c\u6e38\u620f\u6a21\u5f0f\u4e0b\u76f8\u4e92\u5bf9\u5f08\uff0c\u914d\u5907\u6392\u540d\u7b97\u6cd5\u548c\u6392\u884c\u699c\uff0c\u8bc4\u4f30\u57fa\u672c\u7406\u89e3\u3001\u8d70\u5b50\u9009\u62e9\u548c\u8c1c\u9898\u89e3\u51b3\u7b49\u7ec6\u7c92\u5ea6\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u4e8613\u4e2aLLMs\u7684800\u591a\u573a\u5bf9\u5c40\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u90fd\u65e0\u6cd5\u51fb\u8d25\u4e1a\u4f59\u4eba\u7c7b\u6c34\u5e73\u7684Maia-1100\uff0c\u6709\u4e9b\u751a\u81f3\u8f93\u7ed9\u968f\u673a\u8d70\u5b50\u7684\u73a9\u5bb6\u3002\u5fae\u8c03\u7684Qwen3-8B\u6027\u80fd\u5927\u5e45\u63d0\u5347\uff0c\u63a5\u8fd1\u66f4\u5927\u89c4\u6a21\u7684\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6218\u7565\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u771f\u6b63\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.24256", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24256", "abs": "https://arxiv.org/abs/2509.24256", "authors": ["Yunhao Liang", "Pujun Zhang", "Yuan Qu", "Shaochong Lin", "Zuo-jun Max Shen"], "title": "Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization", "comment": null, "summary": "The pretrain-transfer paradigm, which underpins the success of large language\nmodels (LLMs), has demonstrated the immense power of creating foundation models\nthat learn generalizable representations from vast datasets. However, extending\nthis paradigm to Operations Research (OR) problems on graph structures remains\nchallenging due to the fundamental conflict between the statistical flexibility\nof language and the strict combinatorial constraints of graphs. To bridge this\ngap, we introduce the Graph Foundation Model (GFM), the first framework capable\nof solving all distance-based optimization problems on graph structures. By\nintroducing the LLM-like self-supervised pre-training paradigm on the paths\ngenerated from random walks in the graph, GFM is compelled to internalize the\ngraph's complex topological and combinatorial rules, where the connectivity of\nthe structure itself can be treated as the supervisory signal. Unlike existing\nneural methods that learn complex and task-specific solving policies, our\napproach leverages the pre-trained GFM as a foundational model of the graph's\nintrinsic structure, which in turn enables a simple generative heuristic to\ntackle a diverse range of optimization challenges effectively. Comprehensive\nexperiments on networks ranging from 20 to 893 nodes demonstrate that GFM\nachieves competitive performance against specialized solvers across a variety\nof distinct optimization task classes, while maintaining significantly faster\ninference times. Our work establishes a new paradigm of adapting the\npretrain-transfer framework to graph optimization, opening the door for\napplying foundation model innovations to OR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u56fe\u57fa\u7840\u6a21\u578b\uff08GFM\uff09\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u89e3\u51b3\u56fe\u4e0a\u6240\u6709\u57fa\u4e8e\u8ddd\u79bb\u7684\u4f18\u5316\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60\u56fe\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u79cd\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u7ade\u4e89\u6027\u8868\u73b0\u3002", "motivation": "\u5c06\u9884\u8bad\u7ec3-\u8fc1\u79fb\u8303\u5f0f\u6269\u5c55\u5230\u56fe\u7ed3\u6784\u4e0a\u7684\u8fd0\u7b79\u5b66\u95ee\u9898\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8bed\u8a00\u7684\u7edf\u8ba1\u7075\u6d3b\u6027\u4e0e\u56fe\u7684\u4e25\u683c\u7ec4\u5408\u7ea6\u675f\u5b58\u5728\u6839\u672c\u51b2\u7a81\u3002", "method": "\u5f15\u5165LLM\u5f0f\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u56fe\u4e0a\u7684\u968f\u673a\u6e38\u8d70\u8def\u5f84\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u5185\u5316\u56fe\u7684\u590d\u6742\u62d3\u6251\u548c\u7ec4\u5408\u89c4\u5219\uff0c\u5c06\u7ed3\u6784\u8fde\u901a\u6027\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u572820\u5230893\u4e2a\u8282\u70b9\u7684\u7f51\u7edc\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0cGFM\u5728\u591a\u79cd\u4e0d\u540c\u4f18\u5316\u4efb\u52a1\u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u4e13\u7528\u6c42\u89e3\u5668\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u663e\u8457\u66f4\u5feb\u7684\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u5c06\u9884\u8bad\u7ec3-\u8fc1\u79fb\u6846\u67b6\u9002\u5e94\u4e8e\u56fe\u4f18\u5316\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u5c06\u57fa\u7840\u6a21\u578b\u521b\u65b0\u5e94\u7528\u4e8e\u8fd0\u7b79\u5b66\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2509.24274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24274", "abs": "https://arxiv.org/abs/2509.24274", "authors": ["Inkyu Park", "Jeong-Gwan Lee", "Taehwan Kwon", "Juheon Choi", "Seungku Kim", "Junsu Kim", "Kimin Lee"], "title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation", "comment": null, "summary": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game\ninformation such as enemy locations, are difficult to detect because their\neffects are not directly observable in player behavior. The lack of observable\nevidence makes it difficult to collect reliably labeled data, which is\nessential for training effective anti-cheat systems. Furthermore, cheaters\noften adapt their behavior by limiting or disguising their cheat usage, which\nfurther complicates detection and detector development. To address these\nchallenges, we propose a simulation framework for controlled modeling of ESP\ncheaters, non-cheaters, and trajectory-based detectors. We model cheaters and\nnon-cheaters as reinforcement learning agents with different levels of\nobservability, while detectors classify their behavioral trajectories. Next, we\nformulate the interaction between the cheater and the detector as an\nadversarial game, allowing both players to co-adapt over time. To reflect\nrealistic cheater strategies, we introduce a structured cheater model that\ndynamically switches between cheating and non-cheating behaviors based on\ndetection risk. Experiments demonstrate that our framework successfully\nsimulates adaptive cheater behaviors that strategically balance reward\noptimization and detection evasion. This work provides a controllable and\nextensible platform for studying adaptive cheating behaviors and developing\neffective cheat detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u62df\u6846\u67b6\u6765\u5efa\u6a21ESP\u4f5c\u5f0a\u8005\u3001\u975e\u4f5c\u5f0a\u8005\u548c\u57fa\u4e8e\u8f68\u8ff9\u7684\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5bf9\u6297\u6e38\u620f\u7814\u7a76\u4f5c\u5f0a\u8005\u4e0e\u68c0\u6d4b\u5668\u7684\u5171\u540c\u6f14\u5316\u3002", "motivation": "ESP\u4f5c\u5f0a\u96be\u4ee5\u68c0\u6d4b\uff0c\u56e0\u4e3a\u5176\u6548\u679c\u5728\u73a9\u5bb6\u884c\u4e3a\u4e2d\u4e0d\u53ef\u76f4\u63a5\u89c2\u5bdf\uff0c\u4e14\u7f3a\u4e4f\u53ef\u9760\u6807\u8bb0\u6570\u636e\uff0c\u4f5c\u5f0a\u8005\u8fd8\u4f1a\u8c03\u6574\u884c\u4e3a\u6765\u9003\u907f\u68c0\u6d4b\u3002", "method": "\u5c06\u4f5c\u5f0a\u8005\u548c\u975e\u4f5c\u5f0a\u8005\u5efa\u6a21\u4e3a\u5177\u6709\u4e0d\u540c\u53ef\u89c2\u5bdf\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u68c0\u6d4b\u5668\u5bf9\u5176\u884c\u4e3a\u8f68\u8ff9\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5c06\u4f5c\u5f0a\u8005\u4e0e\u68c0\u6d4b\u5668\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5bf9\u6297\u6e38\u620f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u6210\u529f\u6a21\u62df\u4e86\u81ea\u9002\u5e94\u4f5c\u5f0a\u884c\u4e3a\uff0c\u80fd\u591f\u7b56\u7565\u6027\u5730\u5e73\u8861\u5956\u52b1\u4f18\u5316\u548c\u68c0\u6d4b\u89c4\u907f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u81ea\u9002\u5e94\u4f5c\u5f0a\u884c\u4e3a\u548c\u5f00\u53d1\u6709\u6548\u4f5c\u5f0a\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.24302", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24302", "abs": "https://arxiv.org/abs/2509.24302", "authors": ["Muyun Jiang", "Shuailei Zhang", "Zhenjie Yang", "Mengjun Wu", "Weibang Jiang", "Zhiwei Guo", "Wei Zhang", "Rui Liu", "Shangen Zhang", "Yong Li", "Yi Ding", "Cuntai Guan"], "title": "ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying", "comment": null, "summary": "Recent advances in electroencephalography (EEG) foundation models, which\ncapture transferable EEG representations, have greatly accelerated the\ndevelopment of brain-computer interfaces (BCI). However, existing approaches\nstill struggle to incorporate language instructions as prior constraints for\nEEG representation learning, limiting their ability to leverage the semantic\nknowledge inherent in language to unify different labels and tasks. To address\nthis challenge, we present ELASTIQ, a foundation model for EEG-Language\nAlignment with Semantic Task Instruction and Querying. ELASTIQ integrates\ntask-aware semantic guidance to produce structured and linguistically aligned\nEEG embeddings, thereby enhancing decoding robustness and transferability. In\nthe pretraining stage, we introduce a joint Spectral-Temporal Reconstruction\n(STR) module, which combines frequency masking as a global spectral\nperturbation with two complementary temporal objectives: random masking to\ncapture contextual dependencies and causal masking to model sequential\ndynamics. In the instruction tuning stage, we propose the\nInstruction-conditioned Q-Former (IQF), a query-based cross-attention\ntransformer that injects instruction embeddings into EEG tokens and aligns them\nwith textual label embeddings through learnable queries. We evaluate ELASTIQ on\n20 datasets spanning motor imagery, emotion recognition, steady-state visual\nevoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves\nstate-of-the-art performance on 14 of the 20 datasets and obtains the best\naverage results across all five task categories. Importantly, our analyses\nreveal for the first time that explicit task instructions serve as semantic\npriors guiding EEG embeddings into coherent and linguistically grounded spaces.\nThe code and pre-trained weights will be released.", "AI": {"tldr": "ELASTIQ\u662f\u4e00\u4e2a\u7528\u4e8e\u8111\u7535\u56fe-\u8bed\u8a00\u5bf9\u9f50\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u4efb\u52a1\u6307\u4ee4\u548c\u67e5\u8be2\u6765\u589e\u5f3aEEG\u8868\u793a\u5b66\u4e60\uff0c\u5728\u591a\u4e2aBCI\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709EEG\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u5c06\u8bed\u8a00\u6307\u4ee4\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\u7eb3\u5165EEG\u8868\u793a\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u5229\u7528\u8bed\u8a00\u8bed\u4e49\u77e5\u8bc6\u7edf\u4e00\u4e0d\u540c\u6807\u7b7e\u548c\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8054\u5408\u8c31\u65f6\u91cd\u6784\u6a21\u5757\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u9891\u7387\u63a9\u7801\u548c\u4e24\u79cd\u65f6\u5e8f\u76ee\u6807\uff1b\u5728\u6307\u4ee4\u8c03\u4f18\u9636\u6bb5\u4f7f\u7528\u6307\u4ee4\u6761\u4ef6Q-Former\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u67e5\u8be2\u5c06\u6307\u4ee4\u5d4c\u5165\u6ce8\u5165EEG\u6807\u8bb0\u5e76\u4e0e\u6587\u672c\u6807\u7b7e\u5d4c\u5165\u5bf9\u9f50\u3002", "result": "\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6\u8fd0\u52a8\u60f3\u8c61\u3001\u60c5\u7eea\u8bc6\u522b\u3001\u7a33\u6001\u89c6\u89c9\u8bf1\u53d1\u7535\u4f4d\u3001\u5185\u9690\u8a00\u8bed\u548c\u533b\u7597\u4efb\u52a1\uff0c\u572814\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\u4e2d\u5e73\u5747\u7ed3\u679c\u6700\u4f73\u3002", "conclusion": "\u5206\u6790\u9996\u6b21\u63ed\u793a\u663e\u5f0f\u4efb\u52a1\u6307\u4ee4\u4f5c\u4e3a\u8bed\u4e49\u5148\u9a8c\uff0c\u5f15\u5bfcEEG\u5d4c\u5165\u8fdb\u5165\u8fde\u8d2f\u4e14\u8bed\u8a00\u57fa\u7840\u7684\u7a7a\u95f4\uff0c\u589e\u5f3a\u4e86EEG\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8f6c\u79fb\u6027\u3002"}}
{"id": "2509.24306", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24306", "abs": "https://arxiv.org/abs/2509.24306", "authors": ["Satyanarayana Raju G. V. V", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "A study of Universal ODE approaches to predicting soil organic carbon", "comment": null, "summary": "Soil Organic Carbon (SOC) is a foundation of soil health and global climate\nresilience, yet its prediction remains difficult because of intricate physical,\nchemical, and biological processes. In this study, we explore a Scientific\nMachine Learning (SciML) framework built on Universal Differential Equations\n(UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend\nmechanistic physics, such as advection diffusion transport, with neural\nnetworks that learn nonlinear microbial production and respiration. Using\nsynthetic datasets, we systematically evaluated six experimental cases,\nprogressing from clean, noise free benchmarks to stress tests with high (35%)\nmultiplicative, spatially correlated noise. Our results highlight both the\npotential and limitations of the approach. In noise free and moderate noise\nsettings, the UDE accurately reconstructed SOC dynamics. In clean terminal\nprofile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5,\nand R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 =\n0.99998), capturing depth wise SOC trends while tolerating realistic\nmeasurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear\nevidence of overfitting: the model reproduced noisy inputs with high accuracy\nbut lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise\nat t = 50) collapsed toward overly smooth mean profiles, failing to capture\ndepth wise variability and yielding negative R2, underscoring the limits of\nstandard training under severe uncertainty. These findings suggest that UDEs\nare well suited for scalable, noise tolerant SOC forecasting, though advancing\ntoward field deployment will require noise aware loss functions, probabilistic\nmodelling, and tighter integration of microbial dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u901a\u7528\u5fae\u5206\u65b9\u7a0b\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6846\u67b6\u6765\u9884\u6d4b\u571f\u58e4\u6709\u673a\u78b3\u52a8\u6001\uff0c\u5728\u65e0\u566a\u58f0\u548c\u4e2d\u7b49\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u571f\u58e4\u6709\u673a\u78b3\u662f\u571f\u58e4\u5065\u5eb7\u548c\u5168\u7403\u6c14\u5019\u97e7\u6027\u7684\u57fa\u7840\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u7269\u7406\u3001\u5316\u5b66\u548c\u751f\u7269\u8fc7\u7a0b\uff0c\u5176\u9884\u6d4b\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u901a\u7528\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u5c06\u673a\u68b0\u7269\u7406\uff08\u5982\u5e73\u6d41\u6269\u6563\u4f20\u8f93\uff09\u4e0e\u5b66\u4e60\u975e\u7ebf\u6027\u5fae\u751f\u7269\u751f\u4ea7\u548c\u547c\u5438\u7684\u795e\u7ecf\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u4e2a\u5b9e\u9a8c\u6848\u4f8b\u3002", "result": "\u5728\u65e0\u566a\u58f0\u548c7%\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff08MSE=1.6e-5\uff0cR2=0.9999\uff1bMSE=3.4e-6\uff0cR2=0.99998\uff09\uff0c\u4f46\u572835%\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u51fa\u73b0\u8fc7\u62df\u5408\uff08R2=0.94\uff09\u6216\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff08\u8d1fR2\uff09\u3002", "conclusion": "\u901a\u7528\u5fae\u5206\u65b9\u7a0b\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u3001\u566a\u58f0\u5bb9\u5fcd\u7684\u571f\u58e4\u6709\u673a\u78b3\u9884\u6d4b\uff0c\u4f46\u9700\u8981\u566a\u58f0\u611f\u77e5\u635f\u5931\u51fd\u6570\u3001\u6982\u7387\u5efa\u6a21\u548c\u66f4\u7d27\u5bc6\u7684\u5fae\u751f\u7269\u52a8\u529b\u5b66\u96c6\u6210\u6765\u63a8\u8fdb\u5b9e\u5730\u90e8\u7f72\u3002"}}
{"id": "2509.24317", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24317", "abs": "https://arxiv.org/abs/2509.24317", "authors": ["Xianhang Li", "Chen Huang", "Chun-Liang Li", "Eran Malach", "Josh Susskind", "Vimal Thilak", "Etai Littwin"], "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers", "comment": "Technical Report", "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.", "AI": {"tldr": "SALT\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65e0\u6b63\u5219\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u56fa\u5b9a\u6559\u5e08\u6a21\u578b\u66ff\u4ee3EMA\u66f4\u65b0\uff0c\u5728\u89c6\u9891\u8868\u793a\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "V-JEPA\u67b6\u6784\u4e2d\u7684EMA\u66f4\u65b0\u867d\u7136\u9632\u6b62\u4e86\u8868\u793a\u5d29\u6e83\uff0c\u4f46\u589e\u52a0\u4e86\u6a21\u578b\u9009\u62e9\u7684\u590d\u6742\u6027\u5e76\u8026\u5408\u4e86\u5e08\u751f\u67b6\u6784\u3002\u7814\u7a76\u65e8\u5728\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u6548\u7387\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u50cf\u7d20\u91cd\u5efa\u76ee\u6807\u8bad\u7ec3\u6559\u5e08\u7f16\u7801\u5668\uff0c\u7b2c\u4e8c\u9636\u6bb5\u51bb\u7ed3\u6559\u5e08\uff0c\u8bad\u7ec3\u5b66\u751f\u9884\u6d4b\u6559\u5e08\u6f5c\u5728\u8868\u793a\u4e2d\u7684\u63a9\u7801\u533a\u57df\u3002", "result": "SALT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eV-JEPA 2\u7f16\u7801\u5668\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u7f29\u653e\u66f2\u7ebf\u4e3b\u5bfc\u4e86\u51c6\u786e\u7387-FLOPs\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e14\u5b66\u751f\u5bf9\u6559\u5e08\u8d28\u91cf\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "SALT\u4e3a\u57fa\u4e8eEMA\u7684\u81ea\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5efa\u8bae\u5c06\u8ba1\u7b97\u9884\u7b97\u4e3b\u8981\u5206\u914d\u7ed9\u5b66\u751f\u8bad\u7ec3\u3002"}}
{"id": "2509.24320", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24320", "abs": "https://arxiv.org/abs/2509.24320", "authors": ["Dipan Maity"], "title": "AuON: A Linear-time Alternative to Semi-Orthogonal Momentum Updates", "comment": null, "summary": "Orthogonal gradient updates have emerged as a promising direction in\noptimization for machine learning. However, traditional approaches such as\nSVD/QR decomposition incur prohibitive computational costs of O(n^3) and\nunderperform compared to well-tuned SGD with momentum, since momentum is\napplied only after strict orthogonalization. Recent advances, such as Muon,\nimprove efficiency by applying momentum before orthogonalization and producing\nsemi-orthogonal matrices via Newton-Schulz iterations, reducing complexity to\nO(n^2). Nevertheless, quadratic costs remain a bottleneck.\n  In this work, we study the semi-orthogonal properties of momentum-based\nupdates and develop a method to bound momentum updates under a spectral-norm\ntrust region, preserving directional information without requiring explicit\nsemi-orthogonalization.\n  We propose AuON (Alternative Unit-norm momentum updates by Normalized\nnonlinear scaling), a linear-time optimizer that achieves strong performance\nwithout constructing semi-orthogonal matrices, while preserving structural\nalignment and reconditioning ill-posed updates. Our approach combines\nhyperbolic-cosine RMS scaling transformations with normalization, demonstrating\nboth effectiveness and computational efficiency compared to Newton-Schulz\nmethods. We further introduce a hybrid variant (Hybrid-AuON) that applies a\nsingle Newton-Schulz iteration. Experiments across vision and language\nbenchmarks show that AuON and its hybrid variant achieve performance comparable\nto strong baselines such as AdamW and Muon.\n  Code is available at: https://github.com/ryyzn9/AuON", "AI": {"tldr": "\u63d0\u51faAuON\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u7f29\u653e\u548c\u5f52\u4e00\u5316\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u6b63\u4ea4\u68af\u5ea6\u66f4\u65b0\uff0c\u907f\u514d\u4e86\u4f20\u7edfO(n^3)\u6b63\u4ea4\u5316\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u6027\u80fd\u4e0eAdamW\u548cMuon\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u6b63\u4ea4\u68af\u5ea6\u66f4\u65b0\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8(O(n^3))\u4e14\u6027\u80fd\u4e0d\u5982\u5e26\u52a8\u91cf\u7684SGD\uff0c\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u5982Muon\u4ecd\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u8c31\u8303\u6570\u4fe1\u4efb\u533a\u57df\u9650\u5236\u52a8\u91cf\u66f4\u65b0\uff0c\u7ed3\u5408\u53cc\u66f2\u4f59\u5f26RMS\u7f29\u653e\u53d8\u6362\u548c\u5f52\u4e00\u5316\uff0c\u907f\u514d\u663e\u5f0f\u534a\u6b63\u4ea4\u5316\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAuON\u53ca\u5176\u6df7\u5408\u53d8\u4f53\u6027\u80fd\u4e0eAdamW\u548cMuon\u7b49\u5f3a\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "AuON\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u6b63\u4ea4\u68af\u5ea6\u66f4\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2509.24330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24330", "abs": "https://arxiv.org/abs/2509.24330", "authors": ["Shiyuan Zuo", "Rongfei Fan", "Cheng Zhan", "Jie Xu", "Puning Zhao", "Han Hu"], "title": "H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables decentralized model training without sharing\nraw data. However, it remains vulnerable to Byzantine attacks, which can\ncompromise the aggregation of locally updated parameters at the central server.\nSimilarity-aware aggregation has emerged as an effective strategy to mitigate\nsuch attacks by identifying and filtering out malicious clients based on\nsimilarity between client model parameters and those derived from clean data,\ni.e., data that is uncorrupted and trustworthy. However, existing methods adopt\nthis strategy only in FL systems with clean data, making them inapplicable to\nsettings where such data is unavailable. In this paper, we propose H+, a novel\nsimilarity-aware aggregation approach that not only outperforms existing\nmethods in scenarios with clean data, but also extends applicability to FL\nsystems without any clean data. Specifically, H+ randomly selects\n$r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to\nthe server and applies a similarity check function $H$ to compare each segment\nagainst a reference vector, preserving the most similar client vectors for\naggregation. The reference vector is derived either from existing robust\nalgorithms when clean data is unavailable or directly from clean data.\nRepeating this process $K$ times enables effective identification of honest\nclients. Moreover, H+ maintains low computational complexity, with an\nanalytical time complexity of $\\mathcal{O}(KMr)$, where $M$ is the number of\nclients and $Kr \\ll p$. Comprehensive experiments validate H+ as a\nstate-of-the-art (SOTA) method, demonstrating substantial robustness\nimprovements over existing approaches under varying Byzantine attack ratios and\nmultiple types of traditional Byzantine attacks, across all evaluated scenarios\nand benchmark datasets.", "AI": {"tldr": "H+\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u4f3c\u6027\u611f\u77e5\u805a\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u62dc\u5360\u5ead\u653b\u51fb\u9632\u5fa1\uff0c\u65e0\u9700\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\uff0c\u901a\u8fc7\u968f\u673a\u9009\u62e9\u53c2\u6570\u5411\u91cf\u6bb5\u8fdb\u884c\u76f8\u4f3c\u6027\u6bd4\u8f83\u6765\u8bc6\u522b\u8bda\u5b9e\u5ba2\u6237\u7aef\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5bb9\u6613\u53d7\u5230\u62dc\u5360\u5ead\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684\u76f8\u4f3c\u6027\u611f\u77e5\u805a\u5408\u65b9\u6cd5\u9700\u8981\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u65e0\u5e72\u51c0\u6570\u636e\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "method": "H+\u968f\u673a\u9009\u62e9p\u7ef4\u53c2\u6570\u5411\u91cf\u4e2d\u7684r\u7ef4\u6bb5\uff0c\u5e94\u7528\u76f8\u4f3c\u6027\u68c0\u67e5\u51fd\u6570H\u4e0e\u53c2\u8003\u5411\u91cf\u6bd4\u8f83\uff0c\u4fdd\u7559\u6700\u76f8\u4f3c\u7684\u5ba2\u6237\u7aef\u5411\u91cf\u8fdb\u884c\u805a\u5408\uff0c\u91cd\u590dK\u6b21\u4ee5\u6709\u6548\u8bc6\u522b\u8bda\u5b9e\u5ba2\u6237\u7aef\u3002", "result": "H+\u5728\u591a\u79cd\u62dc\u5360\u5ead\u653b\u51fb\u6bd4\u4f8b\u548c\u7c7b\u578b\u4e0b\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u573a\u666f\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u6210\u4e3a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "H+\u4e0d\u4ec5\u5728\u6709\u5e72\u51c0\u6570\u636e\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u6269\u5c55\u4e86\u5728\u65e0\u5e72\u51c0\u6570\u636e\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2509.24332", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24332", "abs": "https://arxiv.org/abs/2509.24332", "authors": ["Siyang Li", "Yize Chen", "Yan Guo", "Ming Huang", "Hui Xiong"], "title": "Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning", "comment": "27 pages, 13 figues. In Submission", "summary": "Advanced deep learning-based approaches have been actively applied to\nforecast the spatiotemporal physical dynamics governed by partial differential\nequations (PDEs), which acts as a critical procedure in tackling many science\nand engineering problems. As real-world physical environments like PDE system\nparameters are always capricious, how to generalize across unseen\nout-of-distribution (OOD) forecasting scenarios using limited training data is\nof great importance. To bridge this barrier, existing methods focus on\ndiscovering domain-generalizable representations across various PDE dynamics\ntrajectories. However, their zero-shot OOD generalization capability remains\ndeficient, since extra test-time samples for domain-specific adaptation are\nstill required. This is because the fundamental physical invariance in PDE\ndynamical systems are yet to be investigated or integrated. To this end, we\nfirst explicitly define a two-fold PDE invariance principle, which points out\nthat ingredient operators and their composition relationships remain invariant\nacross different domains and PDE system evolution. Next, to capture this\ntwo-fold PDE invariance, we propose a physics-guided invariant learning method\ntermed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert\narchitecture and a frequency-enriched invariant learning objective. Extensive\nexperiments across simulated benchmarks and real-world applications validate\niMOOE's superior in-distribution performance and zero-shot generalization\ncapabilities on diverse OOD forecasting scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86iMOOE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u4e0d\u53d8\u5b66\u4e60\u6765\u89e3\u51b3PDE\u7cfb\u7edf\u5728\u672a\u77e5\u5206\u5e03\u573a\u666f\u4e0b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8ePDE\u4e0d\u53d8\u6027\u539f\u7406\uff0c\u5305\u542b\u4e0d\u53d8\u6027\u5bf9\u9f50\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u9891\u7387\u589e\u5f3a\u5b66\u4e60\u76ee\u6807\u3002", "motivation": "\u73b0\u5b9e\u7269\u7406\u73af\u5883\u4e2d\u7684PDE\u7cfb\u7edf\u53c2\u6570\u591a\u53d8\uff0c\u5982\u4f55\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u8de8\u672a\u89c1\u5206\u5e03\u573a\u666f\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672cOOD\u6cdb\u5316\u80fd\u529b\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u989d\u5916\u7684\u6d4b\u8bd5\u65f6\u6837\u672c\u8fdb\u884c\u9886\u57df\u9002\u5e94\u3002", "method": "\u9996\u5148\u660e\u786e\u5b9a\u4e49\u4e86PDE\u7684\u4e24\u91cd\u4e0d\u53d8\u6027\u539f\u7406\uff0c\u7136\u540e\u63d0\u51fa\u4e86iMOOE\u65b9\u6cd5\uff0c\u5305\u542b\u4e0d\u53d8\u6027\u5bf9\u9f50\u7684\u7b97\u5b50\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u9891\u7387\u589e\u5f3a\u7684\u4e0d\u53d8\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86iMOOE\u5728\u5206\u5e03\u5185\u6027\u80fd\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u5e94\u5bf9\u591a\u6837\u5316\u7684OOD\u9884\u6d4b\u573a\u666f\u3002", "conclusion": "iMOOE\u901a\u8fc7\u6355\u6349PDE\u7cfb\u7edf\u7684\u57fa\u672c\u7269\u7406\u4e0d\u53d8\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u5206\u5e03\u573a\u666f\u4e0b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7269\u7406\u52a8\u529b\u5b66\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24341", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24341", "abs": "https://arxiv.org/abs/2509.24341", "authors": ["Qingquan Zhang", "Ziqi Wang", "Yuchen Li", "Keyuan Zhang", "Bo Yuan", "Jialin Liu"], "title": "Expanding Horizons of Level Diversity via Multi-objective Evolutionary Learning", "comment": "12 pages,6 figures", "summary": "In recent years, the generation of diverse game levels has gained increasing\ninterest, contributing to a richer and more engaging gaming experience. A\nnumber of level diversity metrics have been proposed in literature, which are\nnaturally multi-dimensional, leading to conflicted, complementary, or both\nrelationships among these dimensions. However, existing level generation\napproaches often fail to comprehensively assess diversity across those\ndimensions. This paper aims to expand horizons of level diversity by\nconsidering multi-dimensional diversity when training generative models. We\nformulate the model training as a multi-objective learning problem, where each\ndiversity metric is treated as a distinct objective. Furthermore, a\nmulti-objective evolutionary learning framework that optimises multiple\ndiversity metrics simultaneously throughout the model training process is\nproposed. Our case study on the commonly used benchmark Super Mario Bros.\ndemonstrates that our proposed framework can enhance multi-dimensional\ndiversity and identify a Pareto front of generative models, which provides a\nrange of tradeoffs among playability and two representative diversity metrics,\nincluding a content-based one and a player-centered one. Such capability\nenables decision-makers to make informed choices when selecting generators\naccommodating a variety of scenarios and the diverse needs of players and\ndesigners.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u8fdb\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6e38\u620f\u5173\u5361\u751f\u6210\u4e2d\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u591a\u6837\u6027\u6307\u6807\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u8bad\u7ec3\u6784\u5efa\u4e3a\u591a\u76ee\u6807\u5b66\u4e60\u95ee\u9898\u6765\u589e\u5f3a\u591a\u7ef4\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u6e38\u620f\u5173\u5361\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u591a\u7ef4\u591a\u6837\u6027\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\u3001\u4e92\u8865\u6216\u6df7\u5408\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6e38\u620f\u4f53\u9a8c\u7684\u4e30\u5bcc\u6027\u548c\u53c2\u4e0e\u5ea6\u3002", "method": "\u5c06\u6a21\u578b\u8bad\u7ec3\u6784\u5efa\u4e3a\u591a\u76ee\u6807\u5b66\u4e60\u95ee\u9898\uff0c\u6bcf\u4e2a\u591a\u6837\u6027\u6307\u6807\u88ab\u89c6\u4e3a\u72ec\u7acb\u76ee\u6807\uff0c\u5e76\u63d0\u51fa\u591a\u76ee\u6807\u8fdb\u5316\u5b66\u4e60\u6846\u67b6\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u591a\u6837\u6027\u6307\u6807\u3002", "result": "\u5728\u8d85\u7ea7\u9a6c\u91cc\u5965\u5144\u5f1f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86\u591a\u7ef4\u591a\u6837\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u751f\u6210\u6a21\u578b\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u63d0\u4f9b\u4e86\u53ef\u73a9\u6027\u4e0e\u4e24\u4e2a\u4ee3\u8868\u6027\u591a\u6837\u6027\u6307\u6807\uff08\u57fa\u4e8e\u5185\u5bb9\u7684\u548c\u73a9\u5bb6\u4e2d\u5fc3\u7684\uff09\u4e4b\u95f4\u7684\u6743\u8861\u8303\u56f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u51b3\u7b56\u8005\u80fd\u591f\u6839\u636e\u5404\u79cd\u573a\u666f\u4ee5\u53ca\u73a9\u5bb6\u548c\u8bbe\u8ba1\u5e08\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u5728\u751f\u6210\u5668\u9009\u62e9\u65f6\u505a\u51fa\u660e\u667a\u51b3\u7b56\u3002"}}
{"id": "2509.24368", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24368", "abs": "https://arxiv.org/abs/2509.24368", "authors": ["Thibaud Gloaguen", "Robin Staab", "Nikola Jovanovi\u0107", "Martin Vechev"], "title": "Watermarking Diffusion Language Models", "comment": null, "summary": "We introduce the first watermark tailored for diffusion language models\n(DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in\ncontrast to standard autoregressive language models (ARLMs) which generate\ntokens sequentially. While there has been much work in ARLM watermarking, a key\nchallenge when attempting to apply these schemes directly to the DLM setting is\nthat they rely on previously generated tokens, which are not always available\nwith DLM generation. In this work we address this challenge by: (i) applying\nthe watermark in expectation over the context even when some context tokens are\nyet to be determined, and (ii) promoting tokens which increase the watermark\nstrength when used as context for other tokens. This is accomplished while\nkeeping the watermark detector unchanged. Our experimental evaluation\ndemonstrates that the DLM watermark leads to a >99% true positive rate with\nminimal quality impact and achieves similar robustness to existing ARLM\nwatermarks, enabling for the first time reliable DLM watermarking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b(DLMs)\u7684\u6c34\u5370\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eDLMs\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u671f\u671b\u4e0a\u4e0b\u6587\u548c\u63d0\u5347\u6c34\u5370\u5f3a\u5ea6\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6c34\u5370\u68c0\u6d4b\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b(DLMs)\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684LLM\u8303\u5f0f\uff0c\u80fd\u591f\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210token\uff0c\u8fd9\u4e0e\u6807\u51c6\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b(ARLMs)\u7684\u987a\u5e8f\u751f\u6210\u65b9\u5f0f\u4e0d\u540c\u3002\u73b0\u6709\u7684ARLM\u6c34\u5370\u65b9\u6848\u4f9d\u8d56\u4e8e\u5148\u524d\u751f\u6210\u7684token\uff0c\u8fd9\u5728DLM\u751f\u6210\u4e2d\u5e76\u4e0d\u603b\u662f\u53ef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u4e3aDLMs\u8bbe\u8ba1\u6c34\u5370\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\u89e3\u51b3DLM\u6c34\u5370\u6311\u6218\uff1a(1)\u5728\u4e0a\u4e0b\u6587token\u5c1a\u672a\u786e\u5b9a\u65f6\uff0c\u5728\u671f\u671b\u4e0a\u5e94\u7528\u6c34\u5370\uff1b(2)\u63d0\u5347\u90a3\u4e9b\u4f5c\u4e3a\u5176\u4ed6token\u4e0a\u4e0b\u6587\u65f6\u80fd\u589e\u52a0\u6c34\u5370\u5f3a\u5ea6\u7684token\u3002\u540c\u65f6\u4fdd\u6301\u6c34\u5370\u68c0\u6d4b\u5668\u4e0d\u53d8\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cDLM\u6c34\u5370\u5b9e\u73b0\u4e86>99%\u7684\u771f\u9633\u6027\u7387\uff0c\u5bf9\u8d28\u91cf\u5f71\u54cd\u6781\u5c0f\uff0c\u5e76\u4e14\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709ARLM\u6c34\u5370\u76f8\u4f3c\u7684\u9c81\u68d2\u6027\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u53ef\u9760\u7684DLM\u6c34\u5370\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6c34\u5370\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6c34\u5370\u65b9\u6cd5\u5728DLM\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\uff0c\u4e3aDLMs\u7684\u53ef\u9760\u6c34\u5370\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24372", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24372", "abs": "https://arxiv.org/abs/2509.24372", "authors": ["Xin Qiu", "Yulu Gan", "Conor F. Hayes", "Qiyao Liang", "Elliot Meyerson", "Babak Hodjat", "Risto Miikkulainen"], "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning", "comment": "24 pages, including the appendix", "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is\na critical step in the AI deployment pipeline. Reinforcement learning (RL) is\narguably the most prominent fine-tuning method, contributing to the birth of\nmany state-of-the-art LLMs. In contrast, evolution strategies (ES), which once\nshowed comparable performance to RL on models with a few million parameters,\nwas neglected due to the pessimistic perception of its scalability to larger\nmodels. In this work, we report the first successful attempt to scale up ES for\nfine-tuning the full parameters of LLMs, showing the surprising fact that ES\ncan search efficiently over billions of parameters and outperform existing RL\nfine-tuning methods in multiple respects, including sample efficiency,\ntolerance to long-horizon rewards, robustness to different base LLMs, less\ntendency to reward hacking, and more stable performance across runs. It\ntherefore serves as a basis to unlock a new direction in LLM fine-tuning beyond\nwhat current RL techniques provide. The source codes are provided at:\nhttps://github.com/VsonicV/es-fine-tuning-paper.", "AI": {"tldr": "\u672c\u6587\u6210\u529f\u5c06\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b8c\u6574\u53c2\u6570\u5fae\u8c03\uff0c\u8bc1\u660eES\u5728\u6570\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0b\u4ecd\u80fd\u9ad8\u6548\u641c\u7d22\uff0c\u5e76\u5728\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u66fe\u5728\u767e\u4e07\u53c2\u6570\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u5bf9\u5176\u53ef\u6269\u5c55\u6027\u7684\u60b2\u89c2\u9884\u671f\uff0c\u5728LLM\u5fae\u8c03\u4e2d\u88ab\u5ffd\u89c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u8bc4\u4f30ES\u5728\u5927\u89c4\u6a21LLM\u5fae\u8c03\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u65b9\u6cd5\u5bf9\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5168\u90e8\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8ba4\u4e3aES\u65e0\u6cd5\u6269\u5c55\u5230\u6570\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u9650\u5236\u3002", "result": "ES\u5728\u6570\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0b\u8868\u73b0\u51fa\u9ad8\u6548\u641c\u7d22\u80fd\u529b\uff0c\u5728\u6837\u672c\u6548\u7387\u3001\u957f\u5468\u671f\u5956\u52b1\u5bb9\u5fcd\u5ea6\u3001\u5bf9\u4e0d\u540c\u57fa\u7840LLM\u7684\u9c81\u68d2\u6027\u3001\u5956\u52b1\u7834\u89e3\u503e\u5411\u6027\u4ee5\u53ca\u8de8\u8fd0\u884c\u7a33\u5b9a\u6027\u7b49\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709RL\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u8fdb\u5316\u7b56\u7565\u4e3aLLM\u5fae\u8c03\u5f00\u8f9f\u4e86\u8d85\u8d8a\u5f53\u524dRL\u6280\u672f\u7684\u65b0\u65b9\u5411\uff0c\u8bc1\u660e\u4e86ES\u5728\u5927\u89c4\u6a21\u53c2\u6570\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.24378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24378", "abs": "https://arxiv.org/abs/2509.24378", "authors": ["Tian Lan", "Hao Duong Le", "Jinbo Li", "Wenjun He", "Meng Wang", "Chenghao Liu", "Chen Zhang"], "title": "AXIS: Explainable Time Series Anomaly Detection with Large Language Models", "comment": null, "summary": "Time-series anomaly detection (TSAD) increasingly demands explanations that\narticulate not only if an anomaly occurred, but also what pattern it exhibits\nand why it is anomalous. Leveraging the impressive explanatory capabilities of\nLarge Language Models (LLMs), recent works have attempted to treat time series\nas text for explainable TSAD. However, this approach faces a fundamental\nchallenge: LLMs operate on discrete tokens and struggle to directly process\nlong, continuous signals. Consequently, naive time-to-text serialization\nsuffers from a lack of contextual grounding and representation alignment\nbetween the two modalities. To address this gap, we introduce AXIS, a framework\nthat conditions a frozen LLM for nuanced time-series understanding. Instead of\ndirect serialization, AXIS enriches the LLM's input with three complementary\nhints derived from the series: (i) a symbolic numeric hint for numerical\ngrounding, (ii) a context-integrated, step-aligned hint distilled from a\npretrained time-series encoder to capture fine-grained dynamics, and (iii) a\ntask-prior hint that encodes global anomaly characteristics. Furthermore, to\nfacilitate robust evaluation of explainability, we introduce a new benchmark\nfeaturing multi-format questions and rationales that supervise contextual\ngrounding and pattern-level semantics. Extensive experiments, including both\nLLM-based and human evaluations, demonstrate that AXIS yields explanations of\nsignificantly higher quality and achieves competitive detection accuracy\ncompared to general-purpose LLMs, specialized time-series LLMs, and time-series\nVision Language Models.", "AI": {"tldr": "AXIS\u6846\u67b6\u901a\u8fc7\u4e3a\u51bb\u7ed3\u7684LLM\u63d0\u4f9b\u4e09\u79cd\u4e92\u8865\u63d0\u793a\u6765\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5305\u62ec\u7b26\u53f7\u6570\u503c\u63d0\u793a\u3001\u4e0a\u4e0b\u6587\u96c6\u6210\u7684\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u63d0\u793a\u548c\u4efb\u52a1\u5148\u9a8c\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9762\u4e34\u8fde\u7eed\u4fe1\u53f7\u96be\u4ee5\u76f4\u63a5\u5904\u7406\u7684\u95ee\u9898\uff0c\u6734\u7d20\u7684\u65f6\u95f4\u5230\u6587\u672c\u5e8f\u5217\u5316\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u57fa\u7840\u548c\u8868\u793a\u5bf9\u9f50\uff0c\u9700\u8981\u66f4\u597d\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "AXIS\u6846\u67b6\u4e0d\u76f4\u63a5\u5e8f\u5217\u5316\u65f6\u95f4\u5e8f\u5217\uff0c\u800c\u662f\u4e3a\u51bb\u7ed3\u7684LLM\u63d0\u4f9b\u4e09\u79cd\u63d0\u793a\uff1a\u7b26\u53f7\u6570\u503c\u63d0\u793a\u7528\u4e8e\u6570\u503c\u57fa\u7840\u3001\u4e0a\u4e0b\u6587\u96c6\u6210\u7684\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u63d0\u793a\u6355\u6349\u7ec6\u7c92\u5ea6\u52a8\u6001\u3001\u4efb\u52a1\u5148\u9a8c\u63d0\u793a\u7f16\u7801\u5168\u5c40\u5f02\u5e38\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAXIS\u4ea7\u751f\u7684\u89e3\u91ca\u8d28\u91cf\u663e\u8457\u66f4\u9ad8\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u901a\u7528LLM\u3001\u4e13\u7528\u65f6\u95f4\u5e8f\u5217LLM\u548c\u65f6\u95f4\u5e8f\u5217\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "AXIS\u901a\u8fc7\u591a\u63d0\u793a\u6761\u4ef6\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u4e0e\u6587\u672c\u6a21\u6001\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24406", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24406", "abs": "https://arxiv.org/abs/2509.24406", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Muon: Training and Trade-offs with Latent Attention and MoE", "comment": null, "summary": "We present a comprehensive theoretical and empirical study of the Muon\noptimizer for training transformers only with a small to medium decoder (30M -\n200M parameters), with an emphasis on its mathematical foundations, convergence\nproperties and synergistic interactions with modern architectural\noptimizations. Building on recent work showing Muon's scalability, we provide\nrigorous theoretical analysis including: (i)showing the convergence rate under\nstandard assumptions, (ii) spectral regularization properties that prevent\ngradient explosion, (iii) connection to natural gradient descent on the Stiefel\nmanifold, and (iv) equivalence to steepest gradient descent under the spectral\nnorm. Crucially, we demonstrate that Muon expands the Pareto frontier in the\ncompute-time trade-off by maintaining superior data efficiency at large batch\nsizes, a key finding of~\\cite{essentialai2025muon} that we validate across our\nmodel scales. Empirically, Muon reaches the target loss with 48-52\\% of the\ntraining calculated by AdamW while maintaining or improving the final\nperplexity, consistent with larger-scale results. When combined with Multi-Head\nLatent Attention (MLA) and Mixture-of-Experts (MoE), we observe multiplicative\nefficiency gains: MLA+MoE+Muon achieves 68\\% memory reduction and 3.2$\\times$\ninference speedup, while improving perplexity by 8-12\\%. We provide detailed\nprocedures on 15 architectural and optimizer components, stability analyzes\nacross 100+ training runs, and practical implementation guidelines including\nNewton-Schulz coefficients $(3.4445, -4.7750, 2.0315)$ optimized\nby~\\cite{su2024muonblog}. Our theoretical analysis and comprehensive\nexperiments establish Muon as a principled, robust alternative to AdamW that\nparticularly excels when combined with modern efficiency techniques and\nlarge-batch training regimes.", "AI": {"tldr": "Muon\u4f18\u5316\u5668\u5728\u5c0f\u5230\u4e2d\u7b49\u89c4\u6a21\u89e3\u7801\u5668\uff0830M-200M\u53c2\u6570\uff09\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u5206\u6790\u548c\u5b9e\u9645\u6548\u7387\u63d0\u5347\uff0c\u4e0eAdamW\u76f8\u6bd4\u8282\u770148-52%\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff0c\u7ed3\u5408MLA\u548cMoE\u6280\u672f\u53ef\u83b7\u5f9768%\u5185\u5b58\u51cf\u5c11\u548c3.2\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u7814\u7a76Muon\u4f18\u5316\u5668\u5728\u8bad\u7ec3transformer\u89e3\u7801\u5668\u65f6\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u63a2\u7d22\u5176\u4e0e\u73b0\u4ee3\u67b6\u6784\u4f18\u5316\u7684\u534f\u540c\u6548\u5e94\uff0c\u9a8c\u8bc1\u5176\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u5305\u62ec\u6536\u655b\u7387\u3001\u8c31\u6b63\u5219\u5316\u7279\u6027\u3001Stiefel\u6d41\u5f62\u4e0a\u7684\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u8fde\u63a5\u3001\u8c31\u8303\u6570\u4e0b\u7684\u6700\u901f\u68af\u5ea6\u4e0b\u964d\u7b49\u4ef7\u6027\uff0c\u5e76\u8fdb\u884c100+\u8bad\u7ec3\u8fd0\u884c\u7684\u7a33\u5b9a\u6027\u5206\u6790\u548c15\u79cd\u67b6\u6784\u4e0e\u4f18\u5316\u5668\u7ec4\u4ef6\u7684\u8be6\u7ec6\u8bc4\u4f30\u3002", "result": "Muon\u4f7f\u752848-52%\u7684AdamW\u8bad\u7ec3\u8ba1\u7b97\u91cf\u8fbe\u5230\u76ee\u6807\u635f\u5931\uff0c\u4fdd\u6301\u6216\u6539\u5584\u6700\u7ec8\u56f0\u60d1\u5ea6\uff1b\u7ed3\u5408MLA+MoE\u65f6\u5b9e\u73b068%\u5185\u5b58\u51cf\u5c11\u30013.2\u500d\u63a8\u7406\u52a0\u901f\u548c8-12%\u56f0\u60d1\u5ea6\u6539\u5584\uff1b\u5728\u5927\u578b\u6279\u6b21\u8bad\u7ec3\u4e2d\u4fdd\u6301\u4f18\u8d8a\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "Muon\u662f\u57fa\u4e8e\u539f\u7406\u7684\u3001\u7a33\u5065\u7684AdamW\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u5728\u4e0e\u73b0\u4ee3\u6548\u7387\u6280\u672f\u548c\u5927\u578b\u6279\u6b21\u8bad\u7ec3\u65b9\u6848\u7ed3\u5408\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u6269\u5c55\u4e86\u8ba1\u7b97-\u65f6\u95f4\u6743\u8861\u7684Pareto\u524d\u6cbf\u3002"}}
{"id": "2509.24414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24414", "abs": "https://arxiv.org/abs/2509.24414", "authors": ["Tao Yin", "Xiaohong Zhang", "Shaochen Fu", "Zhibin Zhang", "Li Huang", "Yiyuan Yang", "Kaixiang Yang", "Meng Yan"], "title": "ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "One main challenge in time series anomaly detection for industrial IoT lies\nin the complex spatio-temporal couplings within multivariate data. However,\ntraditional anomaly detection methods focus on modeling spatial or temporal\ndependencies independently, resulting in suboptimal representation learning and\nlimited sensitivity to anomalous dispersion in high-dimensional spaces. In this\nwork, we conduct an empirical analysis showing that both normal and anomalous\nsamples tend to scatter in high-dimensional space, especially anomalous samples\nare markedly more dispersed. We formalize this dispersion phenomenon as\nscattering, quantified by the mean pairwise distance among sample\nrepresentations, and leverage it as an inductive signal to enhance\nspatio-temporal anomaly detection. Technically, we propose ScatterAD to model\nrepresentation scattering across temporal and topological dimensions. ScatterAD\nincorporates a topological encoder for capturing graph-structured scattering\nand a temporal encoder for constraining over-scattering through mean squared\nerror minimization between neighboring time steps. We introduce a contrastive\nfusion mechanism to ensure the complementarity of the learned temporal and\ntopological representations. Additionally, we theoretically show that\nmaximizing the conditional mutual information between temporal and topological\nviews improves cross-view consistency and enhances more discriminative\nrepresentations. Extensive experiments on multiple public benchmarks show that\nScatterAD achieves state-of-the-art performance on multivariate time series\nanomaly detection. Code is available at this repository:\nhttps://github.com/jk-sounds/ScatterAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86ScatterAD\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u6837\u672c\u5206\u6563\u73b0\u8c61\u4f5c\u4e3a\u5f52\u7eb3\u4fe1\u53f7\uff0c\u901a\u8fc7\u5efa\u6a21\u65f6\u95f4\u548c\u62d3\u6251\u7ef4\u5ea6\u7684\u8868\u793a\u5206\u6563\u6765\u589e\u5f3a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u590d\u6742\u65f6\u7a7a\u8026\u5408\u5173\u7cfb\uff0c\u4f20\u7edf\u65b9\u6cd5\u72ec\u7acb\u5efa\u6a21\u7a7a\u95f4\u6216\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u8868\u793a\u5b66\u4e60\u4e0d\u7406\u60f3\u4e14\u5bf9\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5f02\u5e38\u5206\u6563\u7684\u654f\u611f\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faScatterAD\u65b9\u6cd5\uff0c\u5305\u542b\u62d3\u6251\u7f16\u7801\u5668\u6355\u83b7\u56fe\u7ed3\u6784\u5206\u6563\uff0c\u65f6\u95f4\u7f16\u7801\u5668\u901a\u8fc7\u6700\u5c0f\u5316\u76f8\u90bb\u65f6\u95f4\u6b65\u7684\u5747\u65b9\u8bef\u5dee\u6765\u7ea6\u675f\u8fc7\u5ea6\u5206\u6563\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u878d\u5408\u673a\u5236\u786e\u4fdd\u65f6\u95f4\u548c\u62d3\u6251\u8868\u793a\u7684\u4e92\u8865\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cScatterAD\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u6700\u5927\u5316\u65f6\u95f4\u548c\u62d3\u6251\u89c6\u56fe\u4e4b\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u53ef\u4ee5\u63d0\u9ad8\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u5e76\u589e\u5f3a\u66f4\u5177\u533a\u5206\u6027\u7684\u8868\u793a\uff0cScatterAD\u6709\u6548\u5229\u7528\u4e86\u8868\u793a\u5206\u6563\u73b0\u8c61\u6765\u6539\u8fdb\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2509.24431", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24431", "abs": "https://arxiv.org/abs/2509.24431", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Aurelio Uncini", "Danilo Comminiello"], "title": "Semantic Compression via Multimodal Representation Learning", "comment": null, "summary": "Multimodal representation learning produces high-dimensional embeddings that\nalign diverse modalities in a shared latent space. While this enables strong\ngeneralization, it also introduces scalability challenges, both in terms of\nstorage and downstream processing. A key open problem is how to achieve\nsemantic compression, reducing the memory footprint of multimodal embeddings\nwhile preserving their ability to represent shared semantic content across\nmodalities. In this paper, we prove a strong connection between reducing the\nmodality gap, which is the residual separation of embeddings from different\nmodalities, and the feasibility of post-training semantic compression. When the\ngap is sufficiently reduced, embeddings from different modalities but\nexpressing the same semantics share a common portion of the space. Therefore,\ntheir centroid is a faithful representation of such a semantic concept. This\nenables replacing multiple embeddings with a single centroid, yielding\nsignificant memory savings. We propose a novel approach for semantic\ncompression grounded on the latter intuition, operating directly on pretrained\nencoders. We demonstrate its effectiveness across diverse large-scale\nmultimodal downstream tasks. Our results highlight that modality alignment is a\nkey enabler for semantic compression, showing that the proposed approach\nachieves significant compression without sacrificing performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u6001\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u5d4c\u5165\u8bed\u4e49\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c0f\u6a21\u6001\u95f4\u9699\u6765\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4ea7\u751f\u7684\u9ad8\u7ef4\u5d4c\u5165\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\uff0c\u4f46\u5e26\u6765\u4e86\u5b58\u50a8\u548c\u4e0b\u6e38\u5904\u7406\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u9700\u8981\u627e\u5230\u5728\u51cf\u5c11\u5185\u5b58\u5360\u7528\u7684\u540c\u65f6\u4fdd\u6301\u8de8\u6a21\u6001\u8bed\u4e49\u5185\u5bb9\u8868\u793a\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u6001\u5bf9\u9f50\u7684\u8bed\u4e49\u538b\u7f29\u65b9\u6cd5\uff0c\u5f53\u6a21\u6001\u95f4\u9699\u5145\u5206\u51cf\u5c0f\u65f6\uff0c\u4e0d\u540c\u6a21\u6001\u4f46\u8868\u8fbe\u76f8\u540c\u8bed\u4e49\u7684\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\u4e2d\u7684\u5171\u540c\u90e8\u5206\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528\u5355\u4e2a\u8d28\u5fc3\u66ff\u6362\u591a\u4e2a\u5d4c\u5165\u6765\u5b9e\u73b0\u538b\u7f29\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u5728\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e0a\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u6837\u5316\u5927\u89c4\u6a21\u591a\u6a21\u6001\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u6548\u679c\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002", "conclusion": "\u6a21\u6001\u5bf9\u9f50\u662f\u5b9e\u73b0\u8bed\u4e49\u538b\u7f29\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u538b\u7f29\u3002"}}
{"id": "2509.24436", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24436", "abs": "https://arxiv.org/abs/2509.24436", "authors": ["Yingshi Chen"], "title": "EOE: Evolutionary Optimization of Experts for Training Language Models", "comment": "6 pages, 2 figures", "summary": "This paper presents an evolutionary framework for the training of large\nlanguage models(LLM). The models are divided into several\nexperts(sub-networks), which have the same structure but different parameter\nvalues. Only one expert is trained at each step. After the classical AdamW\noptimization, some evolutionary operators(crossover, PSO, and mutation) act on\nthe tensor weights between the current expert and the best expert. So current\nexpert would learn the experience of best expert. The direction of best expert\nwould help current expert's loss decrease faster. Finally, only save the weight\nof the best expert. Experiments show that best expert would achieve nearly the\nsame accuracy as the full model. This would greatly reduce the size of the\nmodel for inference. Since only one expert is trained at each step, the\ntraining needs much less memory and has much higher throughput. Experiments\nshow that the throughput would accelerate more than ten times! Our source code\nis available. It's a pure c++/cu framework, which is suitable for easy\ndeployment on PCs and edge computing devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u4e13\u5bb6\u5b50\u7f51\u7edc\uff0c\u7ed3\u5408AdamW\u4f18\u5316\u548c\u8fdb\u5316\u7b97\u5b50\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u5185\u5b58\u9700\u6c42\u9ad8\u3001\u6a21\u578b\u4f53\u79ef\u5927\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8fdb\u5316\u5b66\u4e60\u673a\u5236\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u548c\u8bad\u7ec3\u52a0\u901f\u3002", "method": "\u5c06\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u7ed3\u6784\u76f8\u540c\u4f46\u53c2\u6570\u4e0d\u540c\u7684\u4e13\u5bb6\u5b50\u7f51\u7edc\uff0c\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u53ea\u8bad\u7ec3\u4e00\u4e2a\u4e13\u5bb6\uff0c\u7ed3\u5408AdamW\u4f18\u5316\u548c\u8fdb\u5316\u7b97\u5b50\uff08\u4ea4\u53c9\u3001\u7c92\u5b50\u7fa4\u4f18\u5316\u3001\u53d8\u5f02\uff09\u6765\u5b66\u4e60\u6700\u4f73\u4e13\u5bb6\u7684\u7ecf\u9a8c\u3002", "result": "\u6700\u4f73\u4e13\u5bb6\u80fd\u8fbe\u5230\u4e0e\u5b8c\u6574\u6a21\u578b\u51e0\u4e4e\u76f8\u540c\u7684\u7cbe\u5ea6\uff0c\u6a21\u578b\u5927\u5c0f\u663e\u8457\u51cf\u5c0f\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534710\u500d\u4ee5\u4e0a\uff0c\u5185\u5b58\u9700\u6c42\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u8be5\u8fdb\u5316\u8bad\u7ec3\u6846\u67b6\u80fd\u6709\u6548\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u9002\u5408\u5728PC\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2509.24462", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24462", "abs": "https://arxiv.org/abs/2509.24462", "authors": ["Zifan Wang", "Xinlei Yi", "Xenia Konti", "Michael M. Zavlanos", "Karl H. Johansson"], "title": "Distributionally Robust Federated Learning with Outlier Resilience", "comment": null, "summary": "Federated learning (FL) enables collaborative model training without direct\ndata sharing, but its performance can degrade significantly in the presence of\ndata distribution perturbations. Distributionally robust optimization (DRO)\nprovides a principled framework for handling this by optimizing performance\nagainst the worst-case distributions within a prescribed ambiguity set.\nHowever, existing DRO-based FL methods often overlook the detrimental impact of\noutliers in local datasets, which can disproportionately bias the learned\nmodels. In this work, we study distributionally robust federated learning with\nexplicit outlier resilience. We introduce a novel ambiguity set based on the\nunbalanced Wasserstein distance, which jointly captures geometric\ndistributional shifts and incorporates a non-geometric Kullback--Leibler\npenalization to mitigate the influence of outliers. This formulation naturally\nleads to a challenging min--max--max optimization problem. To enable\ndecentralized training, we reformulate the problem as a tractable Lagrangian\npenalty optimization, which admits robustness certificates. Building on this\nreformulation, we propose the distributionally outlier-robust federated\nlearning algorithm and establish its convergence guarantees. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u5e73\u8861Wasserstein\u8ddd\u79bb\u7684\u5206\u5e03\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6563\u5ea6\u60e9\u7f5a\u673a\u5236\u589e\u5f3a\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u5206\u5e03\u6270\u52a8\u548c\u5f02\u5e38\u503c\u5f71\u54cd\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u5206\u5e03\u6270\u52a8\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u73b0\u6709\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u672c\u5730\u6570\u636e\u96c6\u4e2d\u5f02\u5e38\u503c\u5bf9\u5b66\u4e60\u6a21\u578b\u7684\u6709\u5bb3\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u660e\u786e\u5f02\u5e38\u503c\u9c81\u68d2\u6027\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4e0d\u5e73\u8861Wasserstein\u8ddd\u79bb\u7684\u65b0\u578b\u6a21\u7cca\u96c6\uff0c\u8054\u5408\u6355\u6349\u51e0\u4f55\u5206\u5e03\u504f\u79fb\u5e76\u7eb3\u5165\u975e\u51e0\u4f55KL\u60e9\u7f5a\u6765\u51cf\u8f7b\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u53ef\u5904\u7406\u7684\u62c9\u683c\u6717\u65e5\u60e9\u7f5a\u4f18\u5316\uff0c\u63d0\u51fa\u5206\u5e03\u5f02\u5e38\u503c\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7b97\u6cd5\u5177\u6709\u6536\u655b\u4fdd\u8bc1\uff0c\u80fd\u591f\u63d0\u4f9b\u9c81\u68d2\u6027\u8bc1\u660e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u6709\u6548\u5904\u7406\u4e86\u6570\u636e\u5206\u5e03\u6270\u52a8\u548c\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u5e73\u8861Wasserstein\u8ddd\u79bb\u548cKL\u60e9\u7f5a\u673a\u5236\u5b9e\u73b0\u4e86\u5206\u5e03\u9c81\u68d2\u6027\u548c\u5f02\u5e38\u503c\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2509.24467", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24467", "abs": "https://arxiv.org/abs/2509.24467", "authors": ["Maedeh Zarvandi", "Michael Timothy", "Theresa Wasserer", "Debarghya Ghoshdastidar"], "title": "Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nystr\u00f6m Approximation", "comment": "19 Pages, 3 figures", "summary": "Kernel methods provide a theoretically grounded framework for non-linear and\nnon-parametric learning, with strong analytic foundations and statistical\nguarantees. Yet, their scalability has long been limited by prohibitive time\nand memory costs. While progress has been made in scaling kernel regression, no\nframework exists for scalable kernel-based representation learning, restricting\ntheir use in the era of foundation models where representations are learned\nfrom massive unlabeled data. We introduce KREPES -- a unified, scalable\nframework for kernel-based representation learning via Nystr\\\"om approximation.\nKREPES accommodates a wide range of unsupervised and self-supervised losses,\nand experiments on large image and tabular datasets demonstrate its efficiency.\nCrucially, KREPES enables principled interpretability of the learned\nrepresentations, an immediate benefit over deep models, which we substantiate\nthrough dedicated analysis.", "AI": {"tldr": "KREPES\u662f\u4e00\u4e2a\u901a\u8fc7Nystr\u00f6m\u8fd1\u4f3c\u5b9e\u73b0\u53ef\u6269\u5c55\u6838\u8868\u793a\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6838\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u6838\u65b9\u6cd5\u867d\u7136\u7406\u8bba\u624e\u5b9e\uff0c\u4f46\u957f\u671f\u4ee5\u6765\u53d7\u9650\u4e8e\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u5728\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u6838\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528Nystr\u00f6m\u8fd1\u4f3c\u6784\u5efa\u7edf\u4e00\u7684\u6838\u8868\u793a\u5b66\u4e60\u6846\u67b6KREPES\uff0c\u652f\u6301\u591a\u79cd\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u5927\u578b\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86KREPES\u7684\u9ad8\u6548\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u5bf9\u5b66\u4e60\u5230\u7684\u8868\u793a\u8fdb\u884c\u539f\u5219\u6027\u89e3\u91ca\u3002", "conclusion": "KREPES\u4e3a\u6838\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u8868\u793a\u5b66\u4e60\u4e2d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u6df1\u5ea6\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002"}}
{"id": "2509.24472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24472", "abs": "https://arxiv.org/abs/2509.24472", "authors": ["Ran Elbaz", "Guy Bar-Shalom", "Yam Eitan", "Fabrizio Frasca", "Haggai Maron"], "title": "FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing", "comment": null, "summary": "Permutation equivariant neural networks employing parameter-sharing schemes\nhave emerged as powerful models for leveraging a wide range of data symmetries,\nsignificantly enhancing the generalization and computational efficiency of the\nresulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated\npromise through their improved interpretability and expressivity compared to\ntraditional architectures based on MLPs. While equivariant KANs have been\nexplored in recent literature for a few specific data types, a principled\nframework for applying them to data with permutation symmetries in a general\ncontext remains absent. This paper introduces Function Sharing KAN (FS-KAN), a\nprincipled approach to constructing equivariant and invariant KA layers for\narbitrary permutation symmetry groups, unifying and significantly extending\nprevious work in this domain. We derive the basic construction of these FS-KAN\nlayers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup\nand provide a theoretical analysis demonstrating that FS-KANs have the same\nexpressive power as networks that use standard parameter-sharing layers,\nallowing us to transfer well-known and important expressivity results from\nparameter-sharing networks to FS-KANs. Empirical evaluations on multiple data\ntypes and symmetry groups show that FS-KANs exhibit superior data efficiency\ncompared to standard parameter-sharing layers, by a wide margin in certain\ncases, while preserving the interpretability and adaptability of KANs, making\nthem an excellent architecture choice in low-data regimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FS-KAN\uff0c\u4e00\u79cd\u7528\u4e8e\u4efb\u610f\u7f6e\u6362\u5bf9\u79f0\u7fa4\u7684\u7b49\u53d8\u548c\u4e0d\u53d8KA\u5c42\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u7edf\u4e00\u5e76\u663e\u8457\u6269\u5c55\u4e86\u8be5\u9886\u57df\u5148\u524d\u7684\u5de5\u4f5c\u3002", "motivation": "\u867d\u7136\u7b49\u53d8KAN\u5728\u6700\u8fd1\u6587\u732e\u4e2d\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u4e86\u63a2\u7d22\uff0c\u4f46\u5728\u4e00\u822c\u80cc\u666f\u4e0b\u5bf9\u5177\u6709\u7f6e\u6362\u5bf9\u79f0\u6027\u7684\u6570\u636e\u5e94\u7528\u7684\u539f\u5219\u6027\u6846\u67b6\u4ecd\u7136\u7f3a\u5931\u3002", "method": "\u901a\u8fc7\u5c06\u53c2\u6570\u5171\u4eab\u65b9\u6848\u63a8\u5e7f\u5230Kolmogorov-Arnold\u8bbe\u7f6e\uff0c\u63a8\u5bfc\u51fa\u8fd9\u4e9bFS-KAN\u5c42\u7684\u57fa\u672c\u6784\u9020\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cFS-KAN\u5728\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u5bf9\u79f0\u7fa4\u4e0a\u8868\u73b0\u51fa\u6bd4\u6807\u51c6\u53c2\u6570\u5171\u4eab\u5c42\u66f4\u4f18\u8d8a\u7684\u6570\u636e\u6548\u7387\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "FS-KAN\u5728\u4fdd\u6301KAN\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u7684\u540c\u65f6\uff0c\u5728\u4f4e\u6570\u636e\u72b6\u6001\u4e0b\u6210\u4e3a\u4f18\u79c0\u7684\u67b6\u6784\u9009\u62e9\u3002"}}
{"id": "2509.24483", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24483", "abs": "https://arxiv.org/abs/2509.24483", "authors": ["Minh Le", "Bao-Ngoc Dao", "Huy Nguyen", "Quyen Tran", "Anh Nguyen", "Nhat Ho"], "title": "One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning", "comment": "40 pages, 9 figures", "summary": "Prompt-based methods have recently gained prominence in Continual Learning\n(CL) due to their strong performance and memory efficiency. A prevalent\nstrategy in this paradigm assigns a dedicated subset of prompts to each task,\nwhich, while effective, incurs substantial computational overhead and causes\nmemory requirements to scale linearly with the number of tasks. Conversely,\napproaches employing a single shared prompt across tasks offer greater\nefficiency but often suffer from degraded performance due to knowledge\ninterference. To reconcile this trade-off, we propose SMoPE, a novel framework\nthat integrates the benefits of both task-specific and shared prompt\nstrategies. Inspired by recent findings on the relationship between Prefix\nTuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into\nmultiple \"prompt experts\" within a sparse MoE architecture. For each input,\nonly a select subset of relevant experts is activated, effectively mitigating\ninterference. To facilitate expert selection, we introduce a prompt-attention\nscore aggregation mechanism that computes a unified proxy score for each\nexpert, enabling dynamic and sparse activation. Additionally, we propose an\nadaptive noise mechanism to encourage balanced expert utilization while\npreserving knowledge from prior tasks. To further enhance expert\nspecialization, we design a prototype-based loss function that leverages prefix\nkeys as implicit memory representations. Extensive experiments across multiple\nCL benchmarks demonstrate that SMoPE consistently outperforms task-specific\nprompt methods and achieves performance competitive with state-of-the-art\napproaches, all while significantly reducing parameter counts and computational\ncosts.", "AI": {"tldr": "SMoPE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7ec4\u7ec7\u5171\u4eab\u63d0\u793a\uff0c\u52a8\u6001\u6fc0\u6d3b\u76f8\u5173\u4e13\u5bb6\u6765\u5e73\u8861\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u548c\u5171\u4eab\u63d0\u793a\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u5728\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u63d0\u793a\u65b9\u6cd5\u7684\u6743\u8861\u95ee\u9898\uff1a\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u5185\u5b58\u9700\u6c42\u968f\u4efb\u52a1\u6570\u91cf\u7ebf\u6027\u589e\u957f\uff0c\u800c\u5171\u4eab\u63d0\u793a\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u5b58\u5728\u77e5\u8bc6\u5e72\u6270\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "1. \u5728\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u4e2d\u7ec4\u7ec7\u5171\u4eab\u63d0\u793a\u4e3a\u591a\u4e2a\"\u63d0\u793a\u4e13\u5bb6\"\uff1b2. \u5f15\u5165\u63d0\u793a\u6ce8\u610f\u529b\u5206\u6570\u805a\u5408\u673a\u5236\u8fdb\u884c\u52a8\u6001\u7a00\u758f\u6fc0\u6d3b\uff1b3. \u63d0\u51fa\u81ea\u9002\u5e94\u566a\u58f0\u673a\u5236\u5e73\u8861\u4e13\u5bb6\u5229\u7528\uff1b4. \u8bbe\u8ba1\u57fa\u4e8e\u539f\u578b\u7684\u635f\u5931\u51fd\u6570\u589e\u5f3a\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSMoPE\u59cb\u7ec8\u4f18\u4e8e\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "SMoPE\u6210\u529f\u5e73\u8861\u4e86\u6548\u7387\u548c\u6027\u80fd\u7684\u6743\u8861\uff0c\u901a\u8fc7\u7a00\u758f\u4e13\u5bb6\u6fc0\u6d3b\u673a\u5236\u6709\u6548\u7f13\u89e3\u77e5\u8bc6\u5e72\u6270\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u63d0\u793a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24492", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24492", "abs": "https://arxiv.org/abs/2509.24492", "authors": ["Charmaine Barker", "Daniel Bethell", "Simos Gerasimou"], "title": "Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model", "comment": null, "summary": "Reliable uncertainty quantification remains a major obstacle to the\ndeployment of deep learning models under distributional shift. Existing\npost-hoc approaches that retrofit pretrained models either inherit misplaced\nconfidence or merely reshape predictions, without teaching the model when to be\nuncertain. We introduce GUIDE, a lightweight evidential learning meta-model\napproach that attaches to a frozen deep learning model and explicitly learns\nhow and when to be uncertain. GUIDE identifies salient internal features via a\ncalibration stage, and then employs these features to construct a noise-driven\ncurriculum that teaches the model how and when to express uncertainty. GUIDE\nrequires no retraining, no architectural modifications, and no manual\nintermediate-layer selection to the base deep learning model, thus ensuring\nbroad applicability and minimal user intervention. The resulting model avoids\ndistilling overconfidence from the base model, improves out-of-distribution\ndetection by ~77% and adversarial attack detection by ~80%, while preserving\nin-distribution performance. Across diverse benchmarks, GUIDE consistently\noutperforms state-of-the-art approaches, evidencing the need for actively\nguiding uncertainty to close the gap between predictive confidence and\nreliability.", "AI": {"tldr": "GUIDE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8bc1\u636e\u5b66\u4e60\u5143\u6a21\u578b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u9644\u52a0\u5230\u51bb\u7ed3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0a\uff0c\u660e\u786e\u5b66\u4e60\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u3002", "motivation": "\u53ef\u9760\u7684\u5206\u5e03\u504f\u79fb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ecd\u7136\u662f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7ee7\u627f\u9519\u8bef\u7f6e\u4fe1\u5ea6\uff0c\u8981\u4e48\u4ec5\u91cd\u5851\u9884\u6d4b\uff0c\u800c\u6ca1\u6709\u6559\u4f1a\u6a21\u578b\u4f55\u65f6\u5e94\u8be5\u4e0d\u786e\u5b9a\u3002", "method": "GUIDE\u901a\u8fc7\u6821\u51c6\u9636\u6bb5\u8bc6\u522b\u663e\u8457\u5185\u90e8\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u7279\u5f81\u6784\u5efa\u566a\u58f0\u9a71\u52a8\u7684\u8bfe\u7a0b\uff0c\u6559\u4f1a\u6a21\u578b\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3001\u67b6\u6784\u4fee\u6539\u6216\u624b\u52a8\u4e2d\u95f4\u5c42\u9009\u62e9\u3002", "result": "GUIDE\u907f\u514d\u4e86\u4ece\u57fa\u7840\u6a21\u578b\u84b8\u998f\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5c06\u5206\u5e03\u5916\u68c0\u6d4b\u63d0\u9ad8\u4e86\u7ea677%\uff0c\u5bf9\u6297\u653b\u51fb\u68c0\u6d4b\u63d0\u9ad8\u4e86\u7ea680%\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5e03\u5185\u6027\u80fd\u3002\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GUIDE\u8bc1\u660e\u4e86\u4e3b\u52a8\u5f15\u5bfc\u4e0d\u786e\u5b9a\u6027\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u7f29\u5c0f\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.24496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24496", "abs": "https://arxiv.org/abs/2509.24496", "authors": ["Zhaomin Wu", "Haodong Zhao", "Ziyang Wang", "Jizhou Guo", "Qian Wang", "Bingsheng He"], "title": "LLM DNA: Tracing Model Evolution via Functional Representations", "comment": null, "summary": "The explosive growth of large language models (LLMs) has created a vast but\nopaque landscape: millions of models exist, yet their evolutionary\nrelationships through fine-tuning, distillation, or adaptation are often\nundocumented or unclear, complicating LLM management. Existing methods are\nlimited by task specificity, fixed model sets, or strict assumptions about\ntokenizers or architectures. Inspired by biological DNA, we address these\nlimitations by mathematically defining LLM DNA as a low-dimensional,\nbi-Lipschitz representation of functional behavior. We prove that LLM DNA\nsatisfies inheritance and genetic determinism properties and establish the\nexistence of DNA. Building on this theory, we derive a general, scalable,\ntraining-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA\naligns with prior studies on limited subsets and achieves superior or\ncompetitive performance on specific tasks. Beyond these tasks, DNA comparisons\nuncover previously undocumented relationships among LLMs. We further construct\nthe evolutionary tree of LLMs using phylogenetic algorithms, which align with\nshifts from encoder-decoder to decoder-only architectures, reflect temporal\nprogression, and reveal distinct evolutionary speeds across LLM families.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LLM DNA\u7684\u6982\u5ff5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u529f\u80fd\u884c\u4e3a\u8868\u793a\u4e3a\u4f4e\u7ef4\u3001\u53ccLipschitz\u8868\u793a\uff0c\u7528\u4e8e\u8ffd\u8e2a\u6a21\u578b\u95f4\u7684\u8fdb\u5316\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e86LLM\u8fdb\u5316\u6811\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6570\u767e\u4e07LLM\u6a21\u578b\u4e4b\u95f4\u901a\u8fc7\u5fae\u8c03\u3001\u84b8\u998f\u6216\u9002\u5e94\u4ea7\u751f\u7684\u8fdb\u5316\u5173\u7cfb\u7f3a\u4e4f\u6587\u6863\u8bb0\u5f55\u6216\u6a21\u7cca\u4e0d\u6e05\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u4efb\u52a1\u7279\u5f02\u6027\u3001\u56fa\u5b9a\u6a21\u578b\u96c6\u6216\u5bf9\u5206\u8bcd\u5668\u548c\u67b6\u6784\u7684\u4e25\u683c\u5047\u8bbe\u3002", "method": "\u6570\u5b66\u5b9a\u4e49LLM DNA\u4f5c\u4e3a\u529f\u80fd\u884c\u4e3a\u7684\u4f4e\u7ef4\u53ccLipschitz\u8868\u793a\uff0c\u8bc1\u660e\u5176\u6ee1\u8db3\u7ee7\u627f\u548c\u9057\u4f20\u51b3\u5b9a\u8bba\u7279\u6027\uff0c\u5e76\u5efa\u7acbDNA\u5b58\u5728\u6027\u3002\u57fa\u4e8e\u6b64\u7406\u8bba\uff0c\u63a8\u5bfc\u51fa\u901a\u7528\u3001\u53ef\u6269\u5c55\u3001\u65e0\u9700\u8bad\u7ec3\u7684DNA\u63d0\u53d6\u6d41\u7a0b\u3002", "result": "\u5728305\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDNA\u4e0e\u5148\u524d\u6709\u9650\u5b50\u96c6\u7814\u7a76\u4e00\u81f4\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u6216\u5177\u7ade\u4e89\u529b\u3002DNA\u6bd4\u8f83\u63ed\u793a\u4e86LLM\u95f4\u5148\u524d\u672a\u8bb0\u5f55\u7684\u5173\u8054\uff0c\u6784\u5efa\u7684\u8fdb\u5316\u6811\u53cd\u6620\u4e86\u4ece\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5230\u4ec5\u89e3\u7801\u5668\u67b6\u6784\u7684\u8f6c\u53d8\u3001\u65f6\u95f4\u8fdb\u5c55\u4ee5\u53ca\u4e0d\u540cLLM\u5bb6\u65cf\u7684\u8fdb\u5316\u901f\u5ea6\u5dee\u5f02\u3002", "conclusion": "LLM DNA\u4e3a\u7406\u89e3\u548c\u7ba1\u7406LLM\u8fdb\u5316\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u80fd\u591f\u63ed\u793a\u6a21\u578b\u95f4\u7684\u9690\u542b\u5173\u8054\u5e76\u6784\u5efa\u7cfb\u7edf\u53d1\u80b2\u6811\uff0c\u6709\u52a9\u4e8eLLM\u751f\u6001\u7cfb\u7edf\u7684\u900f\u660e\u5316\u548c\u7cfb\u7edf\u5316\u7ba1\u7406\u3002"}}
{"id": "2509.24510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24510", "abs": "https://arxiv.org/abs/2509.24510", "authors": ["Jonas H\u00fcbotter", "Patrik Wolf", "Alexander Shevchenko", "Dennis J\u00fcni", "Andreas Krause", "Gil Kur"], "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models", "comment": null, "summary": "Recent empirical studies have explored the idea of continuing to train a\nmodel at test-time for a given task, known as test-time training (TTT), and\nhave found it to yield significant performance improvements. However, there is\nlimited understanding of why and when TTT is effective. Earlier explanations\nmostly focused on the observation that TTT may help when applied to\nout-of-distribution adaptation or used with privileged data. However, the\ngrowing scale of foundation models with most test data being in-distribution\nquestions these explanations. We instead posit that foundation models remain\nglobally underparameterized, with TTT providing a mechanism for specialization\nafter generalization, focusing capacity on concepts relevant to the test task.\nSpecifically, under the linear representation hypothesis, we propose a model in\nwhich TTT achieves a substantially smaller in-distribution test error than\nglobal training. We empirically validate our model's key assumptions by\ntraining a sparse autoencoder on ImageNet, showing that semantically related\ndata points are explained by only a few shared concepts. Finally, we perform\nscaling studies across image and language tasks that confirm the practical\nimplications of our model, identifying the regimes where specialization is most\neffective.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6d4b\u8bd5\u65f6\u8bad\u7ec3(TTT)\u7684\u6709\u6548\u6027\uff0c\u8ba4\u4e3a\u57fa\u7840\u6a21\u578b\u5728\u5168\u5c40\u4e0a\u4ecd\u7136\u6b20\u53c2\u6570\u5316\uff0cTTT\u901a\u8fc7\u4e13\u95e8\u5316\u673a\u5236\u5728\u901a\u7528\u5316\u540e\u805a\u7126\u4e8e\u6d4b\u8bd5\u4efb\u52a1\u76f8\u5173\u6982\u5ff5\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u5206\u5e03\u5185\u6d4b\u8bd5\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9TTT\u4e3a\u4f55\u6709\u6548\u4ee5\u53ca\u4f55\u65f6\u6709\u6548\u7f3a\u4e4f\u7406\u89e3\uff0c\u4f20\u7edf\u89e3\u91ca\u4e3b\u8981\u5173\u6ce8\u5206\u5e03\u5916\u9002\u5e94\u6216\u7279\u6743\u6570\u636e\u4f7f\u7528\uff0c\u4f46\u968f\u7740\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u6269\u5927\u4e14\u5927\u591a\u6570\u6d4b\u8bd5\u6570\u636e\u4e3a\u5206\u5e03\u5185\u6570\u636e\uff0c\u8fd9\u4e9b\u89e3\u91ca\u53d7\u5230\u8d28\u7591\u3002", "method": "\u5728\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u4e0b\u63d0\u51fa\u7406\u8bba\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728ImageNet\u4e0a\u9a8c\u8bc1\u5173\u952e\u5047\u8bbe\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u8fdb\u884c\u6269\u5c55\u7814\u7a76\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u8bed\u4e49\u76f8\u5173\u6570\u636e\u70b9\u4ec5\u7531\u5c11\u6570\u5171\u4eab\u6982\u5ff5\u89e3\u91ca\uff0c\u6269\u5c55\u7814\u7a76\u786e\u8ba4\u4e86\u4e13\u95e8\u5316\u6700\u6709\u6548\u7684\u673a\u5236\u3002", "conclusion": "TTT\u901a\u8fc7\u4e13\u95e8\u5316\u673a\u5236\u5728\u901a\u7528\u5316\u540e\u805a\u7126\u6d4b\u8bd5\u4efb\u52a1\u76f8\u5173\u6982\u5ff5\uff0c\u662f\u57fa\u7840\u6a21\u578b\u5728\u5168\u5c40\u6b20\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.24517", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24517", "abs": "https://arxiv.org/abs/2509.24517", "authors": ["Sophia N. Wilson", "Jens Hesselbjerg Christensen", "Raghavendra Selvan"], "title": "Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting", "comment": "Source code available at\n  https://github.com/sophiawilson18/FlowMatching", "summary": "Development of modern deep learning methods has been driven primarily by the\npush for improving model efficacy (accuracy metrics). This sole focus on\nefficacy has steered development of large-scale models that require massive\nresources, and results in considerable carbon footprint across the model\nlife-cycle. In this work, we explore how physics inductive biases can offer\nuseful trade-offs between model efficacy and model efficiency (compute, energy,\nand carbon). We study a variety of models for spatio-temporal forecasting, a\ntask governed by physical laws and well-suited for exploring different levels\nof physics inductive bias. We show that embedding physics inductive biases into\nthe model design can yield substantial efficiency gains while retaining or even\nimproving efficacy for the tasks under consideration. In addition to using\nstandard physics-informed spatio-temporal models, we demonstrate the usefulness\nof more recent models like flow matching as a general purpose method for\nspatio-temporal forecasting. Our experiments show that incorporating physics\ninductive biases offer a principled way to improve the efficiency and reduce\nthe carbon footprint of machine learning models. We argue that model\nefficiency, along with model efficacy, should become a core consideration\ndriving machine learning model development and deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u5728\u6a21\u578b\u6548\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u8bc1\u660e\u5d4c\u5165\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u5e76\u51cf\u5c11\u78b3\u8db3\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u8fc7\u5ea6\u5173\u6ce8\u6a21\u578b\u6548\u80fd\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u5de8\u5927\u3001\u78b3\u8db3\u8ff9\u663e\u8457\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u5982\u4f55\u63d0\u4f9b\u6548\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6709\u76ca\u6743\u8861\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\uff0c\u5305\u62ec\u6807\u51c6\u7269\u7406\u4fe1\u606f\u6a21\u578b\u548c\u66f4\u8fd1\u671f\u7684\u6d41\u5339\u914d\u6a21\u578b\uff0c\u63a2\u7d22\u4e0d\u540c\u5c42\u6b21\u7684\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u5d4c\u5165\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5d4c\u5165\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6548\u7387\uff08\u8ba1\u7b97\u3001\u80fd\u8017\u3001\u78b3\u8db3\u8ff9\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u8fdb\u4efb\u52a1\u6548\u80fd\u3002\u6d41\u5339\u914d\u6a21\u578b\u88ab\u8bc1\u660e\u662f\u65f6\u7a7a\u9884\u6d4b\u7684\u901a\u7528\u6709\u6548\u65b9\u6cd5\u3002", "conclusion": "\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u4e3a\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6548\u7387\u548c\u51cf\u5c11\u78b3\u8db3\u8ff9\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u6a21\u578b\u6548\u7387\u5e94\u4e0e\u6a21\u578b\u6548\u80fd\u4e00\u8d77\u6210\u4e3a\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6838\u5fc3\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2509.24547", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24547", "abs": "https://arxiv.org/abs/2509.24547", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Linh Ngo Van"], "title": "LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection", "comment": null, "summary": "Few-shot Continual Event Detection (FCED) poses the dual challenges of\nlearning from limited data and mitigating catastrophic forgetting across\nsequential tasks. Existing approaches often suffer from severe forgetting due\nto the full fine-tuning of a shared base model, which leads to knowledge\ninterference between tasks. Moreover, they frequently rely on data augmentation\nstrategies that can introduce unnatural or semantically distorted inputs. To\naddress these limitations, we propose LEAF, a novel and robust expert-based\nframework for FCED. LEAF integrates a specialized mixture of experts\narchitecture into the base model, where each expert is parameterized with\nlow-rank adaptation (LoRA) matrices. A semantic-aware expert selection\nmechanism dynamically routes instances to the most relevant experts, enabling\nexpert specialization and reducing knowledge interference. To improve\ngeneralization in limited-data settings, LEAF incorporates a contrastive\nlearning objective guided by label descriptions, which capture high-level\nsemantic information about event types. Furthermore, to prevent overfitting on\nthe memory buffer, our framework employs a knowledge distillation strategy that\ntransfers knowledge from previous models to the current one. Extensive\nexperiments on multiple FCED benchmarks demonstrate that LEAF consistently\nachieves state-of-the-art performance.", "AI": {"tldr": "LEAF\u662f\u4e00\u4e2a\u7528\u4e8e\u5c11\u6837\u672c\u6301\u7eed\u4e8b\u4ef6\u68c0\u6d4b\u7684\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u4f4e\u79e9\u9002\u914d\u77e9\u9635\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u53ef\u80fd\u5f15\u5165\u8bed\u4e49\u626d\u66f2\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4f7f\u7528LoRA\u77e9\u9635\u53c2\u6570\u5316\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u52a8\u6001\u8def\u7531\u5b9e\u4f8b\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728\u591a\u4e2aFCED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LEAF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u6301\u7eed\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2509.24550", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24550", "abs": "https://arxiv.org/abs/2509.24550", "authors": ["Eleonora Grassucci", "Giuliano Galadini", "Giordano Cicchetti", "Aurelio Uncini", "Fabio Antonacci", "Danilo Comminiello"], "title": "Training-Free Multimodal Guidance for Video to Audio Generation", "comment": null, "summary": "Video-to-audio (V2A) generation aims to synthesize realistic and semantically\naligned audio from silent videos, with potential applications in video editing,\nFoley sound design, and assistive multimedia. Although the excellent results,\nexisting approaches either require costly joint training on large-scale paired\ndatasets or rely on pairwise similarities that may fail to capture global\nmultimodal coherence. In this work, we propose a novel training-free multimodal\nguidance mechanism for V2A diffusion that leverages the volume spanned by the\nmodality embeddings to enforce unified alignment across video, audio, and text.\nThe proposed multimodal diffusion guidance (MDG) provides a lightweight,\nplug-and-play control signal that can be applied on top of any pretrained audio\ndiffusion model without retraining. Experiments on VGGSound and AudioCaps\ndemonstrate that our MDG consistently improves perceptual quality and\nmultimodal alignment compared to baselines, proving the effectiveness of a\njoint multimodal guidance for V2A.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5f15\u5bfc\u673a\u5236MDG\uff0c\u7528\u4e8e\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u6765\u589e\u5f3a\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u7684\u7edf\u4e00\u5bf9\u9f50\uff0c\u53ef\u5728\u9884\u8bad\u7ec3\u97f3\u9891\u6269\u6563\u6a21\u578b\u4e0a\u5373\u63d2\u5373\u7528\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5728\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6602\u8d35\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u8981\u4e48\u4f9d\u8d56\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u591a\u6a21\u6001\u4e00\u81f4\u6027\u7684\u6210\u5bf9\u76f8\u4f3c\u6027\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6269\u6563\u5f15\u5bfc(MDG)\uff0c\u5229\u7528\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u6765\u5f3a\u5236\u6267\u884c\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u4e4b\u95f4\u7684\u7edf\u4e00\u5bf9\u9f50\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u63a7\u5236\u4fe1\u53f7\u3002", "result": "\u5728VGGSound\u548cAudioCaps\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMDG\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "MDG\u8bc1\u660e\u4e86\u8054\u5408\u591a\u6a21\u6001\u5f15\u5bfc\u5728\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24552", "abs": "https://arxiv.org/abs/2509.24552", "authors": ["Lo\u00efc Cabannes", "Maximilian Beck", "Gergely Szilvasy", "Matthijs Douze", "Maria Lomeli", "Jade Copet", "Pierre-Emmanuel Mazar\u00e9", "Gabriel Synnaeve", "Herv\u00e9 J\u00e9gou"], "title": "Short window attention enables long-term memorization", "comment": null, "summary": "Recent works show that hybrid architectures combining sliding window softmax\nattention layers with linear recurrent neural network (RNN) layers outperform\nboth of these architectures taken separately. However, the impact of the window\nlength and the interplay between softmax attention and linear RNN layers remain\nunder-studied. In this work, we introduce SWAX, a hybrid architecture\nconsisting of sliding-window attention and xLSTM linear RNN layers.\n  A counter-intuitive finding with SWAX is that larger sliding windows do not\nimprove the long-context performance. In fact, short window attention\nencourages the model to better train the long-term memory of the xLSTM, by\nrelying less on the softmax attention mechanism for long context-retrieval.\n  The issue with small sliding windows is that they are detrimental for\nshort-context tasks, which could be solved with information from moderately\nlarger sliding windows otherwise. Therefore, we train SWAX by stochastically\nchanging the sliding window size, forcing the model to leverage both a longer\ncontext window and the xLSTM memory. SWAX trained with stochastic window sizes\nsignificantly outperforms regular window attention both on short and\nlong-context problems.", "AI": {"tldr": "SWAX\u662f\u4e00\u79cd\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u548cxLSTM\u7ebf\u6027RNN\u5c42\u7684\u6df7\u5408\u67b6\u6784\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5927\u7684\u6ed1\u52a8\u7a97\u53e3\u5e76\u4e0d\u4f1a\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u800c\u77ed\u7a97\u53e3\u6ce8\u610f\u529b\u80fd\u66f4\u597d\u5730\u8bad\u7ec3xLSTM\u7684\u957f\u671f\u8bb0\u5fc6\u3002\u901a\u8fc7\u968f\u673a\u6539\u53d8\u7a97\u53e3\u5927\u5c0f\u8fdb\u884c\u8bad\u7ec3\uff0cSWAX\u5728\u77ed\u4e0a\u4e0b\u6587\u548c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u5e38\u89c4\u7a97\u53e3\u6ce8\u610f\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u6df7\u5408\u67b6\u6784\uff08\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b+\u7ebf\u6027RNN\u5c42\uff09\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u8fd9\u4e24\u79cd\u67b6\u6784\uff0c\u4f46\u7a97\u53e3\u957f\u5ea6\u7684\u5f71\u54cd\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u7ebf\u6027RNN\u5c42\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faSWAX\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548cxLSTM\u7ebf\u6027RNN\u5c42\u3002\u91c7\u7528\u968f\u673a\u6539\u53d8\u6ed1\u52a8\u7a97\u53e3\u5927\u5c0f\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5f3a\u5236\u6a21\u578b\u540c\u65f6\u5229\u7528\u66f4\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u548cxLSTM\u8bb0\u5fc6\u3002", "result": "\u53cd\u76f4\u89c9\u53d1\u73b0\uff1a\u66f4\u5927\u7684\u6ed1\u52a8\u7a97\u53e3\u4e0d\u4f1a\u6539\u5584\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u3002\u77ed\u7a97\u53e3\u6ce8\u610f\u529b\u901a\u8fc7\u51cf\u5c11\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f9d\u8d56\uff0c\u80fd\u66f4\u597d\u5730\u8bad\u7ec3xLSTM\u7684\u957f\u671f\u8bb0\u5fc6\u3002\u968f\u673a\u7a97\u53e3\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u77ed\u4e0a\u4e0b\u6587\u548c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "SWAX\u901a\u8fc7\u968f\u673a\u7a97\u53e3\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u77ed\u4e0a\u4e0b\u6587\u548c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u5e38\u89c4\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u67b6\u6784\u548c\u52a8\u6001\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.24556", "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2509.24556", "abs": "https://arxiv.org/abs/2509.24556", "authors": ["Hussam Sababha", "Bernat Font", "Mohammed Daqaq"], "title": "Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations", "comment": null, "summary": "This study showcases an experimental deployment of deep reinforcement\nlearning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV)\nin a circular cylinder at a high Reynolds number (Re = 3000) using rotary\nactuation. Departing from prior work that relied on low-Reynolds-number\nnumerical simulations, this research demonstrates real-time control in a\nchallenging experimental setting, successfully addressing practical constraints\nsuch as actuator delay. When the learning algorithm is provided with state\nfeedback alone (displacement and velocity of the oscillating cylinder), the DRL\nagent learns a low-frequency rotary control strategy that achieves up to 80%\nvibration suppression which leverages the traditional lock-on phenomenon. While\nthis level of suppression is significant, it remains below the performance\nachieved using high-frequency rotary actuation. The reduction in performance is\nattributed to actuation delays and can be mitigated by augmenting the learning\nalgorithm with past control actions. This enables the agent to learn a\nhigh-frequency rotary control strategy that effectively modifies vortex\nshedding and achieves over 95% vibration attenuation. These results demonstrate\nthe adaptability of DRL for AFC in real-world experiments and its ability to\novercome instrumental limitations such as actuation lag.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u9ad8\u96f7\u8bfa\u6570(Re=3000)\u4e0b\u5bf9\u5706\u67f1\u4f53\u6da1\u6fc0\u632f\u52a8\u8fdb\u884c\u4e3b\u52a8\u6d41\u52a8\u63a7\u5236\u7684\u5b9e\u9a8c\u90e8\u7f72\uff0c\u901a\u8fc7\u65cb\u8f6c\u9a71\u52a8\u5b9e\u73b0\u4e86\u9ad8\u8fbe95%\u7684\u632f\u52a8\u6291\u5236\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4f4e\u96f7\u8bfa\u6570\u6570\u503c\u6a21\u62df\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u5177\u6709\u5b9e\u9645\u7ea6\u675f(\u5982\u6267\u884c\u5668\u5ef6\u8fdf)\u7684\u6311\u6218\u6027\u5b9e\u9a8c\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u63a7\u5236\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8e\u5706\u67f1\u4f53\u4f4d\u79fb\u548c\u901f\u5ea6\u7684\u72b6\u6001\u53cd\u9988\uff0c\u7ed3\u5408\u65cb\u8f6c\u9a71\u52a8\u8fdb\u884c\u4e3b\u52a8\u6d41\u52a8\u63a7\u5236\u3002\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u7b97\u6cd5\u4ee5\u5305\u542b\u5386\u53f2\u63a7\u5236\u52a8\u4f5c\u6765\u7f13\u89e3\u6267\u884c\u5ef6\u8fdf\u95ee\u9898\u3002", "result": "\u4ec5\u4f7f\u7528\u72b6\u6001\u53cd\u9988\u65f6\uff0cDRL\u667a\u80fd\u4f53\u5b66\u4e60\u5230\u4f4e\u9891\u65cb\u8f6c\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b080%\u632f\u52a8\u6291\u5236\uff1b\u589e\u5f3a\u7b97\u6cd5\u540e\uff0c\u667a\u80fd\u4f53\u5b66\u4e60\u5230\u9ad8\u9891\u65cb\u8f6c\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u8d85\u8fc795%\u7684\u632f\u52a8\u8870\u51cf\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u5b9e\u9a8c\u73af\u5883\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u80fd\u591f\u514b\u670d\u4eea\u5668\u9650\u5236\u5982\u6267\u884c\u5ef6\u8fdf\uff0c\u4e3a\u4e3b\u52a8\u6d41\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24559", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24559", "abs": "https://arxiv.org/abs/2509.24559", "authors": ["Marco Molinari", "Leonardo Nevali", "Saharsha Navani", "Omar G. Younis"], "title": "Emergent World Representations in OpenVLA", "comment": null, "summary": "Vision Language Action models (VLAs) trained with policy-based reinforcement\nlearning (RL) encode complex behaviors without explicitly modeling\nenvironmental dynamics. However, it remains unclear whether VLAs implicitly\nlearn world models, a hallmark of model-based RL. We propose an experimental\nmethodology using embedding arithmetic on state representations to probe\nwhether OpenVLA, the current state of the art in VLAs, contains latent\nknowledge of state transitions. Specifically, we measure the difference between\nembeddings of sequential environment states and test whether this transition\nvector is recoverable from intermediate model activations. Using linear and non\nlinear probes trained on activations across layers, we find statistically\nsignificant predictive ability on state transitions exceeding baselines\n(embeddings), indicating that OpenVLA encodes an internal world model (as\nopposed to the probes learning the state transitions). We investigate the\npredictive ability of an earlier checkpoint of OpenVLA, and uncover hints that\nthe world model emerges as training progresses. Finally, we outline a pipeline\nleveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.", "AI": {"tldr": "\u901a\u8fc7\u5d4c\u5165\u7b97\u672f\u65b9\u6cd5\u7814\u7a76\u53d1\u73b0\uff0cOpenVLA\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9690\u5f0f\u5b66\u4e60\u4e86\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u72b6\u6001\u8f6c\u79fb\uff0c\u4e14\u8fd9\u79cd\u80fd\u529b\u968f\u8bad\u7ec3\u8fdb\u5c55\u800c\u589e\u5f3a\u3002", "motivation": "\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b(VLAs)\u662f\u5426\u5728\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u9690\u5f0f\u5b66\u4e60\u4e86\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u662f\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u5178\u578b\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u72b6\u6001\u8868\u793a\u7684\u5d4c\u5165\u7b97\u672f\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u63a2\u9488\u5206\u6790\u6a21\u578b\u5404\u5c42\u6fc0\u6d3b\uff0c\u6d4b\u91cf\u5e8f\u5217\u73af\u5883\u72b6\u6001\u5d4c\u5165\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u68c0\u9a8c\u72b6\u6001\u8f6c\u79fb\u5411\u91cf\u662f\u5426\u53ef\u4ece\u4e2d\u95f4\u6a21\u578b\u6fc0\u6d3b\u4e2d\u6062\u590d\u3002", "result": "\u53d1\u73b0OpenVLA\u5728\u72b6\u6001\u8f6c\u79fb\u9884\u6d4b\u4e0a\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u6c34\u5e73\uff0c\u8868\u660e\u6a21\u578b\u7f16\u7801\u4e86\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002\u65e9\u671f\u68c0\u67e5\u70b9\u7684\u5206\u6790\u663e\u793a\u4e16\u754c\u6a21\u578b\u80fd\u529b\u968f\u8bad\u7ec3\u8fdb\u5c55\u800c\u6d8c\u73b0\u3002", "conclusion": "OpenVLA\u786e\u5b9e\u9690\u5f0f\u5b66\u4e60\u4e86\u4e16\u754c\u6a21\u578b\uff0c\u4e14\u63d0\u51fa\u4e86\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u5176\u4e16\u754c\u6a21\u578b\u7684\u7ba1\u9053\u3002"}}
{"id": "2509.24573", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24573", "abs": "https://arxiv.org/abs/2509.24573", "authors": ["Yusuf Guven", "Vincenzo Di Vito", "Ferdinando Fioretto"], "title": "Learning to Solve Optimization Problems Constrained with Partial Differential Equations", "comment": null, "summary": "Partial differential equation (PDE)-constrained optimization arises in many\nscientific and engineering domains, such as energy systems, fluid dynamics and\nmaterial design. In these problems, the decision variables (e.g., control\ninputs or design parameters) are tightly coupled with the PDE state variables,\nand the feasible set is implicitly defined by the governing PDE constraints.\nThis coupling makes the problems computationally demanding, as it requires\nhandling high dimensional discretization and dynamic constraints. To address\nthese challenges, this paper introduces a learning-based framework that\nintegrates a dynamic predictor with an optimization surrogate. The dynamic\npredictor, a novel time-discrete Neural Operator (Lu et al.), efficiently\napproximate system trajectories governed by PDE dynamics, while the\noptimization surrogate leverages proxy optimizer techniques (Kotary et al.) to\napproximate the associated optimal decisions. This dual-network design enables\nreal-time approximation of optimal strategies while explicitly capturing the\ncoupling between decisions and PDE dynamics. We validate the proposed approach\non benchmark PDE-constrained optimization tasks inlacing Burgers' equation,\nheat equation and voltage regulation, and demonstrate that it achieves solution\nquality comparable to classical control-based algorithms, such as the Direct\nMethod and Model Predictive Control (MPC), while providing up to four orders of\nmagnitude improvement in computational speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9884\u6d4b\u5668\u548c\u4f18\u5316\u4ee3\u7406\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u9ad8\u6548\u89e3\u51b3PDE\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u56db\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "PDE\u7ea6\u675f\u4f18\u5316\u5728\u79d1\u5b66\u5de5\u7a0b\u9886\u57df\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u51b3\u7b56\u53d8\u91cf\u4e0ePDE\u72b6\u6001\u53d8\u91cf\u7d27\u5bc6\u8026\u5408\uff0c\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u9700\u8981\u5904\u7406\u9ad8\u7ef4\u79bb\u6563\u5316\u548c\u52a8\u6001\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u8bbe\u8ba1\uff1a\u52a8\u6001\u9884\u6d4b\u5668\uff08\u65f6\u95f4\u79bb\u6563\u795e\u7ecf\u7b97\u5b50\uff09\u8fd1\u4f3cPDE\u52a8\u529b\u5b66\u7cfb\u7edf\u8f68\u8ff9\uff0c\u4f18\u5316\u4ee3\u7406\u5229\u7528\u4ee3\u7406\u4f18\u5316\u5668\u6280\u672f\u8fd1\u4f3c\u6700\u4f18\u51b3\u7b56\uff0c\u663e\u5f0f\u6355\u6349\u51b3\u7b56\u4e0ePDE\u52a8\u529b\u5b66\u7684\u8026\u5408\u3002", "result": "\u5728Burgers\u65b9\u7a0b\u3001\u70ed\u65b9\u7a0b\u548c\u7535\u538b\u8c03\u8282\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u89e3\u8d28\u91cf\u4e0e\u7ecf\u5178\u63a7\u5236\u7b97\u6cd5\u76f8\u5f53\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u56db\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u5b66\u4e60\u6846\u67b6\u80fd\u5b9e\u65f6\u8fd1\u4f3c\u6700\u4f18\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u89e3\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.24580", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24580", "abs": "https://arxiv.org/abs/2509.24580", "authors": ["Lingyu Wang", "Xiangming Meng"], "title": "SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems", "comment": null, "summary": "Solving inverse problems with diffusion models has shown promise in tasks\nsuch as image restoration. A common approach is to formulate the problem in a\nBayesian framework and sample from the posterior by combining the prior score\nwith the likelihood score. Since the likelihood term is often intractable,\nestimators like DPS, DMPS, and $\\pi$GDM are widely adopted. However, these\nmethods rely on a fixed, manually tuned scale to balance prior and likelihood\ncontributions. Such a static design is suboptimal, as the ideal balance varies\nacross timesteps and tasks, limiting performance and generalization. To address\nthis issue, we propose SAIP, a plug-and-play module that adaptively refines the\nscale at each timestep without retraining or altering the diffusion backbone.\nSAIP integrates seamlessly into existing samplers and consistently improves\nreconstruction quality across diverse image restoration tasks, including\nchallenging scenarios.", "AI": {"tldr": "\u63d0\u51faSAIP\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u4e0e\u4f3c\u7136\u9879\u7684\u5e73\u8861\u5c3a\u5ea6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u56fe\u50cf\u6062\u590d\u8d28\u91cf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u624b\u52a8\u8c03\u6574\u5c3a\u5ea6\u6765\u5e73\u8861\u5148\u9a8c\u4e0e\u4f3c\u7136\u9879\uff0c\u8fd9\u79cd\u9759\u6001\u8bbe\u8ba1\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b", "method": "SAIP\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u81ea\u9002\u5e94\u5730\u4f18\u5316\u5c3a\u5ea6\u53c2\u6570\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6269\u6563\u4e3b\u5e72\u7f51\u7edc\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u91c7\u6837\u5668\u4e2d", "result": "SAIP\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u5305\u62ec\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f", "conclusion": "SAIP\u901a\u8fc7\u81ea\u9002\u5e94\u5c3a\u5ea6\u8c03\u6574\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.24601", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24601", "abs": "https://arxiv.org/abs/2509.24601", "authors": ["Jae-Bum Seo", "Muhammad Salman", "Lismer Andres Caceres-Najarro"], "title": "CURA: Size Isnt All You Need -- A Compact Universal Architecture for On-Device Intelligence", "comment": "14 pages, 3 figures, 8 tables", "summary": "Existing on-device AI architectures for resource-constrained environments\nface two critical limitations: they lack compactness, with parameter\nrequirements scaling proportionally to task complexity, and they exhibit poor\ngeneralizability, performing effectively only on specific application domains\n(e.g., models designed for regression tasks cannot adapt to natural language\nprocessing (NLP) applications). In this paper, we propose CURA, an architecture\ninspired by analog audio signal processing circuits that provides a compact and\nlightweight solution for diverse machine learning tasks across multiple\ndomains. Our architecture offers three key advantages over existing approaches:\n(1) Compactness: it requires significantly fewer parameters regardless of task\ncomplexity; (2) Generalizability: it adapts seamlessly across regression,\nclassification, complex NLP, and computer vision tasks; and (3) Complex pattern\nrecognition: it can capture intricate data patterns while maintaining extremely\nlow model complexity. We evaluated CURA across diverse datasets and domains.\nFor compactness, it achieved equivalent accuracy using up to 2,500 times fewer\nparameters compared to baseline models. For generalizability, it demonstrated\nconsistent performance across four NLP benchmarks and one computer vision\ndataset, nearly matching specialized existing models (achieving F1-scores up to\n90%). Lastly, it delivers superior forecasting accuracy for complex patterns,\nachieving 1.6 times lower mean absolute error and 2.1 times lower mean squared\nerror than competing models.", "AI": {"tldr": "CURA\u662f\u4e00\u79cd\u53d7\u6a21\u62df\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u7535\u8def\u542f\u53d1\u7684\u7d27\u51d1\u578bAI\u67b6\u6784\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u8de8\u57df\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u9700\u6c42\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bbe\u5907\u7aefAI\u67b6\u6784\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u53c2\u6570\u9700\u6c42\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u7ebf\u6027\u589e\u957f\u5bfc\u81f4\u7f3a\u4e4f\u7d27\u51d1\u6027\uff0c\u4ee5\u53ca\u7279\u5b9a\u9886\u57df\u6a21\u578b\u65e0\u6cd5\u9002\u5e94\u5176\u4ed6\u5e94\u7528\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faCURA\u67b6\u6784\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u6a21\u62df\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u7535\u8def\uff0c\u63d0\u4f9b\u8de8\u56de\u5f52\u3001\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u7d27\u51d1\u8f7b\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u7d27\u51d1\u6027\u65b9\u9762\uff0c\u4f7f\u7528\u6bd4\u57fa\u7ebf\u6a21\u578b\u5c112500\u500d\u7684\u53c2\u6570\u5b9e\u73b0\u540c\u7b49\u7cbe\u5ea6\uff1b\u5728\u6cdb\u5316\u6027\u65b9\u9762\uff0c\u5728\u56db\u4e2aNLP\u57fa\u51c6\u548c\u4e00\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e00\u81f4\uff0cF1\u5206\u6570\u8fbe90%\uff1b\u5728\u590d\u6742\u6a21\u5f0f\u8bc6\u522b\u65b9\u9762\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e1.6\u500d\uff0c\u5747\u65b9\u8bef\u5dee\u964d\u4f4e2.1\u500d\u3002", "conclusion": "CURA\u67b6\u6784\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u80fd\u8bc6\u522b\u590d\u6742\u6a21\u5f0f\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.24608", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24608", "abs": "https://arxiv.org/abs/2509.24608", "authors": ["Louise AC Millard", "Peter A Flach"], "title": "Evaluating classification performance across operating contexts: A comparison of decision curve analysis and cost curves", "comment": null, "summary": "Classification models typically predict a score and use a decision threshold\nto produce a classification. Appropriate model evaluation should carefully\nconsider the context in which a model will be used, including the relative\nvalue of correct classifications of positive versus negative examples, which\naffects the threshold that should be used. Decision curve analysis (DCA) and\ncost curves are model evaluation approaches that assess the expected utility\nand expected loss of prediction models, respectively, across decision\nthresholds. We compared DCA and cost curves to determine how they are related,\nand their strengths and limitations. We demonstrate that decision curves are\nclosely related to a specific type of cost curve called a Brier curve. Both\ncurves are derived assuming model scores are calibrated and setting the\nclassification threshold using the relative value of correct positive and\nnegative classifications, and the x-axis of both curves are equivalent. Net\nbenefit (used for DCA) and Brier loss (used for Brier curves) will always\nchoose the same model as optimal at any given threshold. Across thresholds,\ndifferences in Brier loss are comparable whereas differences in net benefit\ncannot be compared. Brier curves are more generally applicable (when a wider\nrange of thresholds are plausible), and the area under the Brier curve is the\nBrier score. We demonstrate that reference lines common in each space can be\nincluded in either and suggest the upper envelope decision curve as a useful\ncomparison for DCA showing the possible gain in net benefit that could be\nachieved through recalibration alone.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u51b3\u7b56\u66f2\u7ebf\u5206\u6790(DCA)\u548c\u6210\u672c\u66f2\u7ebf\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u51b3\u7b56\u66f2\u7ebf\u4e0eBrier\u66f2\u7ebf\u5bc6\u5207\u76f8\u5173\uff0c\u4e24\u8005\u5728\u76f8\u540c\u9608\u503c\u4e0b\u4f1a\u9009\u62e9\u76f8\u540c\u7684\u6700\u4f18\u6a21\u578b\uff0c\u4f46Brier\u66f2\u7ebf\u9002\u7528\u8303\u56f4\u66f4\u5e7f\u4e14\u9762\u79ef\u7b49\u4e8eBrier\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u7c7b\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u6a21\u578b\u4f7f\u7528\u573a\u666f\u548c\u51b3\u7b56\u9608\u503c\u7684\u5f71\u54cd\uff0c\u9700\u8981\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u53cd\u6620\u6a21\u578b\u5728\u4e0d\u540c\u9608\u503c\u4e0b\u7684\u9884\u671f\u6548\u7528\u548c\u635f\u5931\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6bd4\u8f83\u51b3\u7b56\u66f2\u7ebf\u5206\u6790(DCA)\u4e0e\u6210\u672c\u66f2\u7ebf(\u7279\u522b\u662fBrier\u66f2\u7ebf)\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u7814\u7a76\u4e24\u8005\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u5f02\u540c\u70b9\u3002", "result": "\u51b3\u7b56\u66f2\u7ebf\u4e0eBrier\u66f2\u7ebf\u5728\u5047\u8bbe\u6a21\u578b\u5f97\u5206\u5df2\u6821\u51c6\u4e14\u57fa\u4e8e\u6b63\u8d1f\u4f8b\u5206\u7c7b\u76f8\u5bf9\u4ef7\u503c\u8bbe\u7f6e\u9608\u503c\u65f6\u5bc6\u5207\u76f8\u5173\uff0c\u4e24\u8005x\u8f74\u7b49\u4ef7\uff0c\u5728\u4efb\u4f55\u7ed9\u5b9a\u9608\u503c\u4e0b\u90fd\u4f1a\u9009\u62e9\u76f8\u540c\u7684\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "Brier\u66f2\u7ebf\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5176\u9762\u79ef\u7b49\u4e8eBrier\u5206\u6570\uff0c\u5efa\u8bae\u4f7f\u7528\u4e0a\u5305\u7edc\u51b3\u7b56\u66f2\u7ebf\u6765\u663e\u793a\u4ec5\u901a\u8fc7\u91cd\u65b0\u6821\u51c6\u53ef\u80fd\u5b9e\u73b0\u7684\u51c0\u6536\u76ca\u589e\u76ca\u3002"}}
{"id": "2509.24610", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24610", "abs": "https://arxiv.org/abs/2509.24610", "authors": ["Liang Lin", "Zhihao Xu", "Junhao Dong", "Jian Zhao", "Yuchen Yuan", "Guibin Zhang", "Miao Yu", "Yiming Zhang", "Zhengtao Yao", "Huahui Yi", "Dongrui Liu", "Xinfeng Li", "Kun Wang"], "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment", "comment": null, "summary": "Large language model (LLM) alignment faces a critical dilemma when addressing\nmultiple human preferences: improvements in one dimension frequently come at\nthe expense of others, creating unavoidable trade-offs between competing\nobjectives like helpfulness and harmlessness. While prior work mainly focuses\non constraint-based optimization algorithms and data selection strategies to\nmitigate conflicts, these approaches overlook the fundamental issue of\nresolving conflicts directly at the parameter level. In this paper, we present\nOrthAlign, an innovative approach that pioneers a new paradigm by leveraging\northogonal subspace decomposition to fundamentally resolve gradient-level\nconflicts in multi-objective preference alignment. OrthAlign strategically\ndecomposes parameter update spaces into orthogonal subspaces, ensuring that\noptimization toward different preferences occurs in mathematically\nnon-interfering directions. Building upon this, we provide theoretical\nguarantees demonstrating that when parameter increments satisfy both orthogonal\nsubspace constraints and spectral norm bounds, the resulting updates exhibit\nlinear Lipschitz growth rather than exponential instability, ensuring stable\nconvergence across all preference dimensions. Extensive experiments show that:\nI. OrthAlign achieves maximum single-preference improvements ranging from\n34.61% to 50.89% after multiple-objective alignment across helpful, harmless,\nand truthful dimensions. II. With an average overall reward improvement of\n13.96%.", "AI": {"tldr": "OrthAlign\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u89e3\u51b3\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\uff0c\u786e\u4fdd\u4e0d\u540c\u504f\u597d\u7684\u4f18\u5316\u5728\u6570\u5b66\u4e0a\u4e92\u4e0d\u5e72\u6270\u7684\u65b9\u5411\u4e0a\u8fdb\u884c\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u9762\u4e34\u591a\u76ee\u6807\u504f\u597d\u7684\u5173\u952e\u56f0\u5883\uff1a\u6539\u8fdb\u4e00\u4e2a\u7ef4\u5ea6\u5f80\u5f80\u4ee5\u727a\u7272\u5176\u4ed6\u7ef4\u5ea6\u4e3a\u4ee3\u4ef7\uff0c\u5728\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\u7b49\u7ade\u4e89\u76ee\u6807\u95f4\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u6743\u8861\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7ea6\u675f\u4f18\u5316\u7b97\u6cd5\u548c\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u4f46\u5ffd\u89c6\u4e86\u5728\u53c2\u6570\u5c42\u9762\u76f4\u63a5\u89e3\u51b3\u51b2\u7a81\u7684\u6839\u672c\u95ee\u9898\u3002", "method": "OrthAlign\u5229\u7528\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\uff0c\u5c06\u53c2\u6570\u66f4\u65b0\u7a7a\u95f4\u5206\u89e3\u4e3a\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u786e\u4fdd\u4e0d\u540c\u504f\u597d\u7684\u4f18\u5316\u5728\u6570\u5b66\u4e0a\u4e92\u4e0d\u5e72\u6270\u7684\u65b9\u5411\u4e0a\u8fdb\u884c\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5f53\u53c2\u6570\u589e\u91cf\u6ee1\u8db3\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7ea6\u675f\u548c\u8c31\u8303\u6570\u8fb9\u754c\u65f6\uff0c\u66f4\u65b0\u5448\u73b0\u7ebf\u6027Lipschitz\u589e\u957f\u800c\u975e\u6307\u6570\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1aI. OrthAlign\u5728\u591a\u76ee\u6807\u5bf9\u9f50\u540e\uff0c\u5728\u5e2e\u52a9\u6027\u3001\u65e0\u5bb3\u6027\u548c\u771f\u5b9e\u6027\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u5355\u504f\u597d\u6700\u5927\u6539\u8fdb34.61%\u81f350.89%\uff1bII. \u5e73\u5747\u603b\u4f53\u5956\u52b1\u6539\u8fdb\u8fbe\u523013.96%\u3002", "conclusion": "OrthAlign\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u4e2d\u7684\u68af\u5ea6\u7ea7\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u5728\u6240\u6709\u504f\u597d\u7ef4\u5ea6\u4e0a\u7684\u7a33\u5b9a\u6536\u655b\u548c\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.24627", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24627", "abs": "https://arxiv.org/abs/2509.24627", "authors": ["Katharina Friedl", "No\u00e9mie Jaquier", "Mika Liao", "Danica Kragic"], "title": "Learning Hamiltonian Dynamics at Scale: A Differential-Geometric Approach", "comment": "28 pages, 15 figures", "summary": "By embedding physical intuition, network architectures enforce fundamental\nproperties, such as energy conservation laws, leading to plausible predictions.\nYet, scaling these models to intrinsically high-dimensional systems remains a\nsignificant challenge. This paper introduces Geometric Reduced-order\nHamiltonian Neural Network (RO-HNN), a novel physics-inspired neural network\nthat combines the conservation laws of Hamiltonian mechanics with the\nscalability of model order reduction. RO-HNN is built on two core components: a\nnovel geometrically-constrained symplectic autoencoder that learns a\nlow-dimensional, structure-preserving symplectic submanifold, and a geometric\nHamiltonian neural network that models the dynamics on the submanifold. Our\nexperiments demonstrate that RO-HNN provides physically-consistent, stable, and\ngeneralizable predictions of complex high-dimensional dynamics, thereby\neffectively extending the scope of Hamiltonian neural networks to\nhigh-dimensional physical systems.", "AI": {"tldr": "\u63d0\u51fa\u51e0\u4f55\u964d\u9636\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc(RO-HNN)\uff0c\u7ed3\u5408\u54c8\u5bc6\u987f\u529b\u5b66\u5b88\u6052\u5b9a\u5f8b\u548c\u6a21\u578b\u964d\u9636\u53ef\u6269\u5c55\u6027\uff0c\u7528\u4e8e\u9ad8\u7ef4\u7269\u7406\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u76f4\u89c9\u7f51\u7edc\u67b6\u6784\u867d\u7136\u80fd\u5f3a\u5236\u6267\u884c\u57fa\u672c\u7269\u7406\u5c5e\u6027\uff0c\u4f46\u96be\u4ee5\u6269\u5c55\u5230\u5185\u5728\u9ad8\u7ef4\u7cfb\u7edf\u3002\u9700\u8981\u7ed3\u5408\u54c8\u5bc6\u987f\u529b\u5b66\u5b88\u6052\u5b9a\u5f8b\u4e0e\u6a21\u578b\u964d\u9636\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u51e0\u4f55\u7ea6\u675f\u8f9b\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4f4e\u7ef4\u7ed3\u6784\u4fdd\u6301\u8f9b\u5b50\u6d41\u5f62\uff0c\u4ee5\u53ca\u51e0\u4f55\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc\u5728\u5b50\u6d41\u5f62\u4e0a\u5efa\u6a21\u52a8\u529b\u5b66\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRO-HNN\u80fd\u63d0\u4f9b\u7269\u7406\u4e00\u81f4\u3001\u7a33\u5b9a\u4e14\u53ef\u6cdb\u5316\u7684\u590d\u6742\u9ad8\u7ef4\u52a8\u529b\u5b66\u9884\u6d4b\u3002", "conclusion": "RO-HNN\u6709\u6548\u6269\u5c55\u4e86\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc\u5728\u9ad8\u7ef4\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.24653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24653", "abs": "https://arxiv.org/abs/2509.24653", "authors": ["Pengxiao Lin", "Zheng-An Chen", "Zhi-Qin John Xu"], "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory", "comment": null, "summary": "Despite remarkable advances, large language models often fail at\ncompositional reasoning tasks, a phenomenon exemplified by the ``curse of\ntwo-hop reasoning''. This paper introduces the Identity Bridge, a simple yet\npowerful mechanism that resolves this compositionality gap by supervising the\nmodel on a zero-hop identity task. We demonstrate empirically that this\naddition enables models to successfully perform out-of-distribution two-hop\nreasoning, a task they otherwise completely fail. To explain this phenomenon,\nwe provide a theoretical analysis using a simplified Emb-MLP model, proving\nthat identity supervision reshapes the model's latent geometry. We show this\nalignment is induced by an implicit nuclear-norm regularization during\noptimization, which favors low-rank solutions that share structure across\ntasks. For complex tasks, we use small initialization or weight decay to\nenhance the regularization effect, which enhances the latent space alignment\neffect and slows down the generalization decay. Finally, we extend our\ninvestigation to large-scale models, observing that they still achieve two-hop\nreasoning through the latent memory, which provides crucial inspiration for\nenhancing their implicit reasoning abilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Identity Bridge\u673a\u5236\uff0c\u901a\u8fc7\u5728\u96f6\u8df3\u8eab\u4efd\u4efb\u52a1\u4e0a\u76d1\u7763\u6a21\u578b\uff0c\u89e3\u51b3\u4e86LLM\u5728\u7ec4\u5408\u63a8\u7406\u4e2d\u7684\u4e24\u8df3\u63a8\u7406\u8bc5\u5492\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u7279\u522b\u662f\u5b58\u5728\"\u4e24\u8df3\u63a8\u7406\u8bc5\u5492\"\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u65e0\u6cd5\u8fdb\u884c\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u4e24\u6b65\u63a8\u7406\u3002", "method": "\u5f15\u5165Identity Bridge\u673a\u5236\uff0c\u5728\u96f6\u8df3\u8eab\u4efd\u4efb\u52a1\u4e0a\u76d1\u7763\u6a21\u578b\uff1b\u4f7f\u7528\u7b80\u5316\u7684Emb-MLP\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff1b\u901a\u8fc7\u5c0f\u521d\u59cb\u5316\u6216\u6743\u91cd\u8870\u51cf\u589e\u5f3a\u6b63\u5219\u5316\u6548\u679c\uff1b\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6a21\u578b\u7814\u7a76\u3002", "result": "\u8eab\u4efd\u76d1\u7763\u4f7f\u6a21\u578b\u80fd\u591f\u6210\u529f\u6267\u884c\u8d85\u51fa\u5206\u5e03\u7684\u4e24\u8df3\u63a8\u7406\u4efb\u52a1\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\u8eab\u4efd\u76d1\u7763\u901a\u8fc7\u9690\u5f0f\u6838\u8303\u6570\u6b63\u5219\u5316\u91cd\u5851\u6a21\u578b\u7684\u6f5c\u5728\u51e0\u4f55\u7ed3\u6784\uff1b\u5c0f\u521d\u59cb\u5316\u589e\u5f3a\u4e86\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "Identity Bridge\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u7684\u7ec4\u5408\u63a8\u7406\u95ee\u9898\uff0c\u8eab\u4efd\u76d1\u7763\u901a\u8fc7\u9690\u5f0f\u6b63\u5219\u5316\u4fc3\u8fdb\u8de8\u4efb\u52a1\u7ed3\u6784\u5171\u4eab\uff0c\u4e3a\u589e\u5f3a\u6a21\u578b\u9690\u542b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2509.24655", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.24655", "abs": "https://arxiv.org/abs/2509.24655", "authors": ["Max van Spengler", "Artem Moskalev", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "title": "HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling", "comment": null, "summary": "Language models are increasingly applied to biological sequences like\nproteins and mRNA, yet their default Euclidean geometry may mismatch the\nhierarchical structures inherent to biological data. While hyperbolic geometry\nprovides a better alternative for accommodating hierarchical data, it has yet\nto find a way into language modeling for mRNA sequences. In this work, we\nintroduce HyperHELM, a framework that implements masked language model\npre-training in hyperbolic space for mRNA sequences. Using a hybrid design with\nhyperbolic layers atop Euclidean backbone, HyperHELM aligns learned\nrepresentations with the biological hierarchy defined by the relationship\nbetween mRNA and amino acids. Across multiple multi-species datasets, it\noutperforms Euclidean baselines on 9 out of 10 tasks involving property\nprediction, with 10% improvement on average, and excels in out-of-distribution\ngeneralization to long and low-GC content sequences; for antibody region\nannotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation\naccuracy. Our results highlight hyperbolic geometry as an effective inductive\nbias for hierarchical language modeling of mRNA sequences.", "AI": {"tldr": "HyperHELM\u662f\u4e00\u4e2a\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u4e3amRNA\u5e8f\u5217\u5b9e\u73b0\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u8bbe\u8ba1\u5c06\u53cc\u66f2\u5c42\u7f6e\u4e8e\u6b27\u51e0\u91cc\u5f97\u9aa8\u5e72\u7f51\u7edc\u4e4b\u4e0a\uff0c\u4f7f\u5b66\u4e60\u8868\u793a\u4e0emRNA\u548c\u6c28\u57fa\u9178\u4e4b\u95f4\u7684\u751f\u7269\u5c42\u6b21\u7ed3\u6784\u5bf9\u9f50\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u86cb\u767d\u8d28\u548cmRNA\u7b49\u751f\u7269\u5e8f\u5217\uff0c\u4f46\u5176\u9ed8\u8ba4\u7684\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u53ef\u80fd\u4e0e\u751f\u7269\u6570\u636e\u56fa\u6709\u7684\u5c42\u6b21\u7ed3\u6784\u4e0d\u5339\u914d\u3002\u53cc\u66f2\u51e0\u4f55\u4e3a\u5bb9\u7eb3\u5c42\u6b21\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8emRNA\u5e8f\u5217\u7684\u8bed\u8a00\u5efa\u6a21\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8bbe\u8ba1\uff0c\u5728\u6b27\u51e0\u91cc\u5f97\u9aa8\u5e72\u7f51\u7edc\u4e4b\u4e0a\u6dfb\u52a0\u53cc\u66f2\u5c42\uff0c\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u4f7f\u5b66\u4e60\u8868\u793a\u4e0emRNA\u548c\u6c28\u57fa\u9178\u4e4b\u95f4\u7684\u751f\u7269\u5c42\u6b21\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u591a\u7269\u79cd\u6570\u636e\u96c6\u4e0a\uff0c\u572810\u4e2a\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u67099\u4e2a\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u534710%\uff1b\u5728\u957f\u5e8f\u5217\u548c\u4f4eGC\u542b\u91cf\u5e8f\u5217\u7684\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff1b\u5728\u6297\u4f53\u533a\u57df\u6ce8\u91ca\u4efb\u52a1\u4e2d\uff0c\u6bd4\u5c42\u6b21\u611f\u77e5\u7684\u6b27\u51e0\u91cc\u5f97\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad83%\u3002", "conclusion": "\u53cc\u66f2\u51e0\u4f55\u662fmRNA\u5e8f\u5217\u5c42\u6b21\u8bed\u8a00\u5efa\u6a21\u7684\u6709\u6548\u5f52\u7eb3\u504f\u7f6e\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u751f\u7269\u6570\u636e\u7684\u5c42\u6b21\u7ed3\u6784\u7279\u6027\u3002"}}
{"id": "2509.24696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24696", "abs": "https://arxiv.org/abs/2509.24696", "authors": ["Zikun Qu", "Min Zhang", "Mingze Kong", "Xiang Li", "Zhiwei Shang", "Zhiyong Wang", "Yikun Ban", "Shuang Qiu", "Yao Shu", "Zhongxiang Dai"], "title": "T-POP: Test-Time Personalization with Online Preference Feedback", "comment": "Preprint", "summary": "Personalizing large language models (LLMs) to individual user preferences is\na critical step beyond generating generically helpful responses. However,\ncurrent personalization methods are ill-suited for new users, as they typically\nrequire either slow, resource-intensive fine-tuning or a substantial amount of\npre-existing user data, creating a significant cold-start problem. To address\nthis challenge, we introduce a new paradigm for real-time personalization by\nlearning from online pairwise preference feedback collected during text\ngeneration. We propose T-POP (Test-Time Personalization with Online Preference\nFeedback}), a novel algorithm that synergistically combines test-time alignment\nwith dueling bandits. Without updating the LLM parameters, T-POP steers the\ndecoding process of a frozen LLM by learning a reward function online that\ncaptures user preferences. By leveraging dueling bandits, T-POP intelligently\nqueries the user to efficiently balance between exploring their preferences and\nexploiting the learned knowledge to generate personalized text. Extensive\nexperiments demonstrate that T-POP achieves rapid and data-efficient\npersonalization, significantly outperforming existing baselines and showing\nconsistent improvement with more user interactions.", "AI": {"tldr": "T-POP\u662f\u4e00\u79cd\u5b9e\u65f6\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u6536\u96c6\u7528\u6237\u504f\u597d\u53cd\u9988\u6765\u4e2a\u6027\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7528\u6237\u6570\u636e\u6216\u8d44\u6e90\u5bc6\u96c6\u7684\u5fae\u8c03\uff0c\u5bfc\u81f4\u65b0\u7528\u6237\u9762\u4e34\u4e25\u91cd\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5b9e\u65f6\u4e2a\u6027\u5316\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u548c\u51b3\u6597\u8d4c\u535a\u673a\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u6765\u5f15\u5bfc\u51bb\u7ed3LLM\u7684\u89e3\u7801\u8fc7\u7a0b\uff0c\u667a\u80fd\u67e5\u8be2\u7528\u6237\u4ee5\u5e73\u8861\u63a2\u7d22\u504f\u597d\u548c\u5229\u7528\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eT-POP\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u6570\u636e\u9ad8\u6548\u7684\u4e2a\u6027\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u968f\u7740\u7528\u6237\u4ea4\u4e92\u589e\u52a0\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "T-POP\u4e3aLLM\u5b9e\u65f6\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u7ebf\u504f\u597d\u53cd\u9988\u514b\u670d\u4e86\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u6709\u9650\u4ea4\u4e92\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4e2a\u6027\u5316\u7684\u80fd\u529b\u3002"}}
{"id": "2509.24701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24701", "abs": "https://arxiv.org/abs/2509.24701", "authors": ["Pingchen Lu", "Zhi Hong", "Zhiwei Shang", "Zhiyong Wang", "Yikun Ban", "Yao Shu", "Min Zhang", "Shuang Qiu", "Zhongxiang Dai"], "title": "FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits", "comment": "Preprint", "summary": "The performance of large language models (LLMs) is highly sensitive to the\ninput prompt, making prompt optimization a critical task. However, real-world\napplication is hindered by three major challenges: (1) the black-box nature of\npowerful proprietary LLMs, (2) the need for high sample efficiency due to query\ncosts, and (3) the desire for privacy-preserving collaboration among multiple\nusers. To address these challenges simultaneously, we introduce a novel\nframework for sample-efficient federated prompt optimization based on\nmulti-armed bandits (MABs). The MAB framework is uniquely suited for this\nproblem as it is (1) inherently a black-box optimization method, (2)\npractically sample-efficient, and (3) enables collaborative learning with\ntheoretically guaranteed benefit from more participating agents. We first\npropose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a\nfederated variant of the Linear UCB algorithm, where agents collaborate by\nsharing model parameters instead of raw data. We then extend our approach to\nthe practical setting of comparative user feedback by introducing FedPOB with\nPreference Feedback (FedPOB-Pref), an efficient algorithm based on federated\ndueling bandits. Extensive experiments demonstrate that both FedPOB and\nFedPOB-Pref significantly outperform existing baselines and that their\nperformance consistently improves as more agents participate in the\ncollaboration, validating the effectiveness of our federated approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u7684\u8054\u90a6\u63d0\u793a\u4f18\u5316\u6846\u67b6FedPOB\u548cFedPOB-Pref\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u4f18\u5316\u7684\u9ed1\u76d2\u6027\u3001\u6837\u672c\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u4f18\u5316\u9762\u4e34\u7684\u4e09\u5927\u6311\u6218\uff1a(1) \u5f3a\u5927\u4e13\u6709LLM\u7684\u9ed1\u76d2\u6027\u8d28\uff0c(2) \u67e5\u8be2\u6210\u672c\u5bfc\u81f4\u7684\u6837\u672c\u6548\u7387\u9700\u6c42\uff0c(3) \u591a\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u63d0\u51faFedPOB\u7b97\u6cd5\uff08\u8054\u90a6\u7ebf\u6027UCB\u53d8\u4f53\uff09\u548cFedPOB-Pref\u7b97\u6cd5\uff08\u57fa\u4e8e\u8054\u90a6\u5bf9\u51b3\u8001\u864e\u673a\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u6a21\u578b\u53c2\u6570\u800c\u975e\u539f\u59cb\u6570\u636e\u5b9e\u73b0\u534f\u4f5c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eFedPOB\u548cFedPOB-Pref\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u968f\u7740\u53c2\u4e0e\u4ee3\u7406\u6570\u91cf\u589e\u52a0\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u8054\u90a6\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u63d0\u793a\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u4f18\u5316\u3002"}}
{"id": "2509.24713", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24713", "abs": "https://arxiv.org/abs/2509.24713", "authors": ["Jing Liu"], "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit\nsystematic failures on longtail distributions, leading to reward hacking and\nmisalignment. We propose a mechanistic interpretability framework that\nidentifies specialized neural circuits responsible for rare-event processing in\nreward models. Drawing from recent advances showing distributed specialization\nfor rare tokens in language models\\citep{liu2025no, liu2025emergent}, we\nhypothesize that reward models also develop functionally distinct circuits for\nlongtail scenarios. Our theoretical framework establishes formal connections\nbetween circuit specialization, reward generalization bounds, and longtail\nperformance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which\nuses circuit analysis to guide data augmentation, regularization, and ensemble\nstrategies. This approach provides both theoretical insights into reward model\nfailures and practical interventions for improving longtail robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u8bc6\u522b\u5956\u52b1\u6a21\u578b\u4e2d\u8d1f\u8d23\u7f55\u89c1\u4e8b\u4ef6\u5904\u7406\u7684\u4e13\u7528\u795e\u7ecf\u56de\u8def\uff0c\u5e76\u5f15\u5165Circuit-Aware Reward Training (CART)\u65b9\u6cd5\u6765\u63d0\u9ad8\u957f\u5c3e\u5206\u5e03\u7684\u9c81\u68d2\u6027\u3002", "motivation": "RLHF\u5956\u52b1\u6a21\u578b\u5728\u957f\u5c3e\u5206\u5e03\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u9519\u4f4d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u8bc6\u522b\u4e13\u7528\u795e\u7ecf\u56de\u8def\uff0c\u63d0\u51faCART\u65b9\u6cd5\uff0c\u901a\u8fc7\u56de\u8def\u5206\u6790\u6307\u5bfc\u6570\u636e\u589e\u5f3a\u3001\u6b63\u5219\u5316\u548c\u96c6\u6210\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u56de\u8def\u4e13\u4e1a\u5316\u3001\u5956\u52b1\u6cdb\u5316\u8fb9\u754c\u548c\u957f\u5c3e\u6027\u80fd\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u8054\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u5956\u52b1\u6a21\u578b\u5931\u8d25\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u89c1\uff0c\u5e76\u4e3a\u63d0\u9ad8\u957f\u5c3e\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2509.24716", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24716", "abs": "https://arxiv.org/abs/2509.24716", "authors": ["Michael Drolet", "Firas Al-Hafez", "Aditya Bhatt", "Jan Peters", "Oleg Arenz"], "title": "Discrete Variational Autoencoding via Policy Search", "comment": null, "summary": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit\nefficiency and can be modeled with autoregressive discrete distributions,\nenabling parameter-efficient multimodal search with transformers. However,\ndiscrete random variables do not allow for exact differentiable\nparameterization; therefore, discrete VAEs typically rely on approximations,\nsuch as Gumbel-Softmax reparameterization or straight-through gradient\nestimates, or employ high-variance gradient-free methods such as REINFORCE that\nhave had limited success on high-dimensional tasks such as image\nreconstruction. Inspired by popular techniques in policy search, we propose a\ntraining framework for discrete VAEs that leverages the natural gradient of a\nnon-parametric encoder to update the parametric encoder without requiring\nreparameterization. Our method, combined with automatic step size adaptation\nand a transformer-based encoder, scales to challenging datasets such as\nImageNet and outperforms both approximate reparameterization methods and\nquantization-based discrete autoencoders in reconstructing high-dimensional\ndata from compact latent spaces, achieving a 20% improvement on FID Score for\nImageNet 256.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u6563VAE\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u975e\u53c2\u6570\u7f16\u7801\u5668\u7684\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\u7f16\u7801\u5668\uff0c\u65e0\u9700\u91cd\u53c2\u6570\u5316\uff0c\u5728ImageNet\u7b49\u9ad8\u7ef4\u6570\u636e\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u79bb\u6563VAE\u867d\u7136\u5177\u6709\u9ad8\u6bd4\u7279\u6548\u7387\uff0c\u4f46\u79bb\u6563\u968f\u673a\u53d8\u91cf\u65e0\u6cd5\u7cbe\u786e\u5fae\u5206\u53c2\u6570\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u5982Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u6216REINFORCE\u5728\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650", "method": "\u7ed3\u5408\u7b56\u7565\u641c\u7d22\u6280\u672f\uff0c\u5229\u7528\u975e\u53c2\u6570\u7f16\u7801\u5668\u7684\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\u7f16\u7801\u5668\uff0c\u65e0\u9700\u91cd\u53c2\u6570\u5316\uff0c\u914d\u5408\u81ea\u52a8\u6b65\u957f\u9002\u5e94\u548c\u57fa\u4e8etransformer\u7684\u7f16\u7801\u5668", "result": "\u5728ImageNet\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91cd\u5efa\u9ad8\u7ef4\u6570\u636e\u65b9\u9762\u4f18\u4e8e\u8fd1\u4f3c\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\u548c\u57fa\u4e8e\u91cf\u5316\u7684\u79bb\u6563\u81ea\u7f16\u7801\u5668\uff0cImageNet 256\u7684FID\u5206\u6570\u63d0\u534720%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79bb\u6563VAE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u9ad8\u7ef4\u6570\u636e\u91cd\u5efa\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb"}}
{"id": "2509.24725", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24725", "abs": "https://arxiv.org/abs/2509.24725", "authors": ["Ting Gao", "Elvin Isufi", "Winnie Daamen", "Erik-Sander Smits", "Serge Hoogendoorn"], "title": "Q-Net: Transferable Queue Length Estimation via Kalman-based Neural Networks", "comment": null, "summary": "Estimating queue lengths at signalized intersections remains a challenge in\ntraffic management, especially under partially observed conditions where\nvehicle flows are not fully captured. This paper introduces Q-Net, a\ndata-efficient and interpretable framework for queue length estimation that\nperforms robustly even when traffic conservation assumptions are violated.\nQ-Net integrates two widely available and privacy-friendly data sources: (i)\nvehicle counts from loop detectors near stop lines, and (ii) aggregated\nfloating car data (aFCD), which divides each road section into segments and\nprovides segment-wise average speed measurements. These data sources often\ndiffer in spatial and temporal resolution, creating fusion challenges. Q-Net\naddresses this by employing a tailored state-space model and an AI-augmented\nKalman filter, KalmanNet, which learns the Kalman gain from data without\nrequiring prior knowledge of noise covariances or full system dynamics. We\nbuild on the vanilla KalmanNet pipeline to decouple measurement dimensionality\nfrom section length, enabling spatial transferability across road segments.\nUnlike black-box models, Q-Net maintains physical interpretability, with\ninternal variables linked to real-world traffic dynamics. Evaluations on main\nroads in Rotterdam, the Netherlands, demonstrate that Q-Net outperforms\nbaseline methods by over 60\\% in Root Mean Square Error (RMSE), accurately\ntracking queue formation and dissipation while correcting aFCD-induced delays.\nQ-Net also demonstrates strong spatial and temporal transferability, enabling\ndeployment without costly sensing infrastructure like cameras or radar.\nAdditionally, we propose a real-time variant of Q-Net, highlighting its\npotential for integration into dynamic, queue-based traffic control systems.", "AI": {"tldr": "Q-Net\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u961f\u5217\u957f\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u878d\u5408\u73af\u68c0\u6d4b\u5668\u8f66\u8f86\u8ba1\u6570\u548c\u805a\u5408\u6d6e\u52a8\u8f66\u6570\u636e\uff0c\u4f7f\u7528AI\u589e\u5f3a\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7a33\u5065\u8fd0\u884c\u3002", "motivation": "\u89e3\u51b3\u4fe1\u53f7\u4ea4\u53c9\u53e3\u961f\u5217\u957f\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\uff0c\u5f53\u4ea4\u901a\u5b88\u6052\u5047\u8bbe\u88ab\u8fdd\u53cd\u65f6\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u73af\u68c0\u6d4b\u5668\u8f66\u8f86\u8ba1\u6570\u548c\u805a\u5408\u6d6e\u52a8\u8f66\u6570\u636e\uff0c\u91c7\u7528\u5b9a\u5236\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cAI\u589e\u5f3a\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(KalmanNet)\uff0c\u65e0\u9700\u5148\u9a8c\u566a\u58f0\u534f\u65b9\u5dee\u6216\u5b8c\u6574\u7cfb\u7edf\u52a8\u6001\u77e5\u8bc6\u3002", "result": "\u5728\u9e7f\u7279\u4e39\u4e3b\u8981\u9053\u8def\u4e0a\u8bc4\u4f30\uff0cQ-Net\u5728\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc760%\uff0c\u51c6\u786e\u8ddf\u8e2a\u961f\u5217\u5f62\u6210\u548c\u6d88\u6563\uff0c\u5e76\u4fee\u6b63aFCD\u5f15\u8d77\u7684\u5ef6\u8fdf\u3002", "conclusion": "Q-Net\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u5177\u6709\u5f3a\u53ef\u8f6c\u79fb\u6027\uff0c\u65e0\u9700\u6602\u8d35\u7684\u611f\u77e5\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u65f6\u53d8\u4f53\uff0c\u9002\u5408\u96c6\u6210\u5230\u52a8\u6001\u961f\u5217\u4ea4\u901a\u63a7\u5236\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2509.24728", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24728", "abs": "https://arxiv.org/abs/2509.24728", "authors": ["Alessandro Manenti", "Cesare Alippi"], "title": "Beyond Softmax: A Natural Parameterization for Categorical Random Variables", "comment": null, "summary": "Latent categorical variables are frequently found in deep learning\narchitectures. They can model actions in discrete reinforcement-learning\nenvironments, represent categories in latent-variable models, or express\nrelations in graph neural networks. Despite their widespread use, their\ndiscrete nature poses significant challenges to gradient-descent learning\nalgorithms. While a substantial body of work has offered improved gradient\nestimation techniques, we take a complementary approach. Specifically, we: 1)\nrevisit the ubiquitous $\\textit{softmax}$ function and demonstrate its\nlimitations from an information-geometric perspective; 2) replace the\n$\\textit{softmax}$ with the $\\textit{catnat}$ function, a function composed of\na sequence of hierarchical binary splits; we prove that this choice offers\nsignificant advantages to gradient descent due to the resulting diagonal Fisher\nInformation Matrix. A rich set of experiments - including graph structure\nlearning, variational autoencoders, and reinforcement learning - empirically\nshow that the proposed function improves the learning efficiency and yields\nmodels characterized by consistently higher test performance. $\\textit{Catnat}$\nis simple to implement and seamlessly integrates into existing codebases.\nMoreover, it remains compatible with standard training stabilization techniques\nand, as such, offers a better alternative to the $\\textit{softmax}$ function.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528catnat\u51fd\u6570\u66ff\u4ee3softmax\u51fd\u6570\u6765\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u5206\u7c7b\u53d8\u91cf\uff0c\u8bc1\u660e\u5176\u5728\u4fe1\u606f\u51e0\u4f55\u89d2\u5ea6\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u4ea7\u751f\u5bf9\u89d2Fisher\u4fe1\u606f\u77e9\u9635\uff0c\u4ece\u800c\u63d0\u5347\u68af\u5ea6\u4e0b\u964d\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u5206\u7c7b\u53d8\u91cf\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u5176\u79bb\u6563\u6027\u8d28\u7ed9\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u68af\u5ea6\u4f30\u8ba1\u6280\u672f\u4e0a\uff0c\u672c\u6587\u4ece\u51fd\u6570\u8bbe\u8ba1\u89d2\u5ea6\u63d0\u4f9b\u8865\u5145\u65b9\u6848\u3002", "method": "1) \u4ece\u4fe1\u606f\u51e0\u4f55\u89d2\u5ea6\u5206\u6790softmax\u51fd\u6570\u7684\u5c40\u9650\u6027\uff1b2) \u63d0\u51facatnat\u51fd\u6570\uff0c\u7531\u4e00\u7cfb\u5217\u5c42\u6b21\u5316\u4e8c\u5206\u7ec4\u6210\uff0c\u80fd\u4ea7\u751f\u5bf9\u89d2Fisher\u4fe1\u606f\u77e9\u9635\u3002", "result": "\u5728\u56fe\u7ed3\u6784\u5b66\u4e60\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u5b9e\u9a8c\u4e2d\uff0ccatnat\u51fd\u6570\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6d4b\u8bd5\u6027\u80fd\u3002", "conclusion": "catnat\u51fd\u6570\u6613\u4e8e\u5b9e\u73b0\uff0c\u4e0e\u73b0\u6709\u4ee3\u7801\u5e93\u517c\u5bb9\uff0c\u4e14\u4e0e\u6807\u51c6\u8bad\u7ec3\u7a33\u5b9a\u6280\u672f\u517c\u5bb9\uff0c\u662fsoftmax\u51fd\u6570\u7684\u66f4\u597d\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.24732", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24732", "abs": "https://arxiv.org/abs/2509.24732", "authors": ["Juergen Schmidhuber"], "title": "Who invented deep residual learning?", "comment": "12 pages, 2 illustrations, circa 100 partially annotated references", "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the\nmost cited scientific article of the 21st century is an NN paper on deep\nresidual learning with residual connections. Who invented this? We present a\ntimeline of the evolution of deep residual learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u56de\u987e\u4e86\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60\u7684\u6f14\u5316\u65f6\u95f4\u7ebf\uff0c\u63a2\u8ba8\u4e86\u8c01\u53d1\u660e\u4e86\u6b8b\u5dee\u8fde\u63a5\u8fd9\u4e00\u91cd\u8981\u6982\u5ff5", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60\u8bba\u6587\u5df2\u6210\u4e3a21\u4e16\u7eaa\u88ab\u5f15\u7528\u6700\u591a\u7684\u79d1\u5b66\u6587\u7ae0\uff0c\u4f5c\u8005\u5e0c\u671b\u8ffd\u6eaf\u8fd9\u4e00\u91cd\u8981\u6982\u5ff5\u7684\u53d1\u660e\u8005\u548c\u6f14\u5316\u5386\u7a0b", "method": "\u901a\u8fc7\u6784\u5efa\u65f6\u95f4\u7ebf\u7684\u65b9\u5f0f\u6765\u8ffd\u8e2a\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60\u7684\u6f14\u5316\u8fc7\u7a0b", "result": "\u8bba\u6587\u5448\u73b0\u4e86\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60\u7684\u5b8c\u6574\u53d1\u5c55\u65f6\u95f4\u7ebf\uff0c\u660e\u786e\u4e86\u76f8\u5173\u6982\u5ff5\u7684\u53d1\u660e\u8005", "conclusion": "\u901a\u8fc7\u65f6\u95f4\u7ebf\u5206\u6790\uff0c\u8bba\u6587\u5398\u6e05\u4e86\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60\u7684\u53d1\u5c55\u8109\u7edc\u548c\u5173\u952e\u8d21\u732e\u8005"}}
{"id": "2509.24734", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24734", "abs": "https://arxiv.org/abs/2509.24734", "authors": ["Giordano Cicchetti", "Eleonora Grassucci", "Danilo Comminiello"], "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity", "comment": "NeurIPS 2025", "summary": "Multimodal learning plays a pivotal role in advancing artificial intelligence\nsystems by incorporating information from multiple modalities to build a more\ncomprehensive representation. Despite its importance, current state-of-the-art\nmodels still suffer from severe limitations that prevent the successful\ndevelopment of a fully multimodal model. Such methods may not provide\nindicators that all the involved modalities are effectively aligned. As a\nresult, some modalities may not be aligned, undermining the effectiveness of\nthe model in downstream tasks where multiple modalities should provide\nadditional information that the model fails to exploit. In this paper, we\npresent TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed\nsimilarity measure that is directly computed in the higher-dimensional space\nspanned by the modality embeddings. TRIANGLE improves the joint alignment of\nthree modalities via a triangle-area similarity, avoiding additional fusion\nlayers or pairwise similarities. When incorporated in contrastive losses\nreplacing cosine similarity, TRIANGLE significantly boosts the performance of\nmultimodal modeling, while yielding interpretable alignment rationales.\nExtensive evaluation in three-modal tasks such as video-text and audio-text\nretrieval or audio-video classification, demonstrates that TRIANGLE achieves\nstate-of-the-art results across different datasets improving the performance of\ncosine-based methods up to 9 points of Recall@1.", "AI": {"tldr": "\u63d0\u51faTRIANGLE\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u89d2\u5f62\u9762\u79ef\u76f8\u4f3c\u6027\u6539\u8fdb\u4e09\u6a21\u6001\u5bf9\u9f50\uff0c\u5728\u5bf9\u6bd4\u635f\u5931\u4e2d\u66ff\u4ee3\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u6a21\u6001\u5bf9\u9f50\u4e0d\u5145\u5206\u7684\u95ee\u9898\uff0c\u67d0\u4e9b\u6a21\u6001\u53ef\u80fd\u672a\u88ab\u6709\u6548\u5bf9\u9f50\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51faTRIANGLE\u76f8\u4f3c\u5ea6\u5ea6\u91cf\uff0c\u76f4\u63a5\u5728\u6a21\u6001\u5d4c\u5165\u7684\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u4e09\u89d2\u5f62\u9762\u79ef\u76f8\u4f3c\u6027\uff0c\u907f\u514d\u989d\u5916\u7684\u878d\u5408\u5c42\u6216\u6210\u5bf9\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002", "result": "\u5728\u89c6\u9891-\u6587\u672c\u548c\u97f3\u9891-\u6587\u672c\u68c0\u7d22\u3001\u97f3\u9891-\u89c6\u9891\u5206\u7c7b\u7b49\u4e09\u6a21\u6001\u4efb\u52a1\u4e2d\uff0cTRIANGLE\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u76f8\u6bd4\u57fa\u4e8e\u4f59\u5f26\u7684\u65b9\u6cd5Recall@1\u63d0\u5347\u9ad8\u8fbe9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "TRIANGLE\u901a\u8fc7\u51e0\u4f55\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u6539\u8fdb\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5bf9\u9f50\u539f\u7406\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5efa\u6a21\u6027\u80fd\u3002"}}
{"id": "2509.24748", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24748", "abs": "https://arxiv.org/abs/2509.24748", "authors": ["Longxiang He", "Deheng Ye", "Junbo Tan", "Xueqian Wang", "Li Shen"], "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption", "comment": "39th Conference on Neural Information Processing Systems", "summary": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.", "AI": {"tldr": "\u63d0\u51fa\u4e86RPEX\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9006\u6982\u7387\u52a0\u6743\u6765\u7f13\u89e3\u6570\u636e\u6c61\u67d3\u5bfc\u81f4\u7684\u7b56\u7565\u91cd\u5c3e\u73b0\u8c61\uff0c\u63d0\u5347\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u548c\u5728\u7ebf\u4ea4\u4e92\u5f80\u5f80\u5b58\u5728\u566a\u58f0\u751a\u81f3\u6076\u610f\u6c61\u67d3\uff0c\u8fd9\u4f1a\u4e25\u91cd\u964d\u4f4e\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u5728\u7ebf\u63a2\u7d22\u7f13\u89e3\u79bb\u7ebf\u7b56\u7565\u7684\u4fdd\u5b88\u6027\uff0c\u4f46\u5728\u6570\u636e\u6c61\u67d3\u4e0b\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51faRPEX\u65b9\u6cd5\uff0c\u5c06\u9006\u6982\u7387\u52a0\u6743\uff08IPW\uff09\u6574\u5408\u5230\u5728\u7ebf\u63a2\u7d22\u7b56\u7565\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u6570\u636e\u6c61\u67d3\u5f15\u8d77\u7684\u7b56\u7565\u91cd\u5c3e\u884c\u4e3a\u3002", "result": "\u5728D4RL\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRPEX\u5728\u591a\u79cd\u6570\u636e\u6c61\u67d3\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u6027\u80fd\u3002", "conclusion": "RPEX\u662f\u4e00\u79cd\u65b0\u9896\u3001\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u636e\u6c61\u67d3\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.24762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24762", "abs": "https://arxiv.org/abs/2509.24762", "authors": ["David Berghaus", "Patrick Seifner", "Kostadin Cvejoski", "C\u00e9sar Ojeda", "Rams\u00e9s J. S\u00e1nchez"], "title": "In-Context Learning of Temporal Point Processes with Foundation Inference Models", "comment": null, "summary": "Modeling event sequences of multiple event types with marked temporal point\nprocesses (MTPPs) provides a principled way to uncover governing dynamical\nrules and predict future events. Current neural network approaches to MTPP\ninference rely on training separate, specialized models for each target system.\nWe pursue a radically different approach: drawing on amortized inference and\nin-context learning, we pretrain a deep neural network to infer, in-context,\nthe conditional intensity functions of event histories from a context defined\nby sets of event sequences. Pretraining is performed on a large synthetic\ndataset of MTPPs sampled from a broad distribution of Hawkes processes. Once\npretrained, our Foundation Inference Model for Point Processes (FIM-PP) can\nestimate MTPPs from real-world data without any additional training, or be\nrapidly finetuned to target systems. Experiments show that this amortized\napproach matches the performance of specialized models on next-event prediction\nacross common benchmark datasets.\n  Our pretrained model, repository and tutorials will soon be available online", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u70b9\u8fc7\u7a0b\u57fa\u7840\u63a8\u7406\u6a21\u578bFIM-PP\uff0c\u901a\u8fc7\u5728\u5927\u91cf\u5408\u6210\u7684Hawkes\u8fc7\u7a0b\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5feb\u901f\u5fae\u8c03\u5373\u53ef\u63a8\u65ad\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u5e8f\u5217\u7684\u6761\u4ef6\u5f3a\u5ea6\u51fd\u6570\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u76ee\u6807\u7cfb\u7edf\u8bad\u7ec3\u4e13\u95e8\u7684\u6a21\u578b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u672c\u6587\u8ffd\u6c42\u4e00\u79cd\u5b8c\u5168\u4e0d\u540c\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u644a\u9500\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u6784\u5efa\u901a\u7528\u7684\u70b9\u8fc7\u7a0b\u63a8\u7406\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u644a\u9500\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u5408\u6210\u7684Hawkes\u8fc7\u7a0b\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u5176\u80fd\u591f\u4ece\u4e8b\u4ef6\u5e8f\u5217\u4e0a\u4e0b\u6587\u63a8\u65ad\u6761\u4ef6\u5f3a\u5ea6\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u644a\u9500\u65b9\u6cd5\u5728\u5e38\u89c1\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u80fd\u591f\u8fbe\u5230\u4e0e\u4e13\u95e8\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "FIM-PP\u6a21\u578b\u5c55\u793a\u4e86\u901a\u8fc7\u9884\u8bad\u7ec3\u6784\u5efa\u901a\u7528\u70b9\u8fc7\u7a0b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5feb\u901f\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u3002"}}
{"id": "2509.24770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24770", "abs": "https://arxiv.org/abs/2509.24770", "authors": ["Fabrizio Frasca", "Guy Bar-Shalom", "Yftah Ziser", "Haggai Maron"], "title": "Neural Message-Passing on Attention Graphs for Hallucination Detection", "comment": "Preprint. 25 pages, 2 figures", "summary": "Large Language Models (LLMs) often generate incorrect or unsupported content,\nknown as hallucinations. Existing detection methods rely on heuristics or\nsimple models over isolated computational traces such as activations, or\nattention maps. We unify these signals by representing them as attributed\ngraphs, where tokens are nodes, edges follow attentional flows, and both carry\nfeatures from attention scores and activations. Our approach, CHARM, casts\nhallucination detection as a graph learning task and tackles it by applying\nGNNs over the above attributed graphs. We show that CHARM provably subsumes\nprior attention-based heuristics and, experimentally, it consistently\noutperforms other leading approaches across diverse benchmarks. Our results\nshed light on the relevant role played by the graph structure and on the\nbenefits of combining computational traces, whilst showing CHARM exhibits\npromising zero-shot performance on cross-dataset transfer.", "AI": {"tldr": "CHARM\u901a\u8fc7\u5c06LLM\u7684\u8ba1\u7b97\u8f68\u8ff9\u8868\u793a\u4e3a\u5c5e\u6027\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u5e7b\u89c9\uff0c\u7edf\u4e00\u4e86\u6ce8\u610f\u529b\u5206\u6570\u548c\u6fc0\u6d3b\u503c\u7b49\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u7b80\u5355\u6a21\u578b\u5904\u7406\u5b64\u7acb\u8ba1\u7b97\u8f68\u8ff9\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u6765\u6574\u5408\u6ce8\u610f\u529b\u56fe\u548c\u6fc0\u6d3b\u503c\u7b49\u591a\u79cd\u4fe1\u53f7\u3002", "method": "\u5c06\u8ba1\u7b97\u8f68\u8ff9\u8868\u793a\u4e3a\u5c5e\u6027\u56fe\uff0c\u5176\u4e2dtoken\u662f\u8282\u70b9\uff0c\u8fb9\u9075\u5faa\u6ce8\u610f\u529b\u6d41\uff0c\u8282\u70b9\u548c\u8fb9\u643a\u5e26\u6ce8\u610f\u529b\u5206\u6570\u548c\u6fc0\u6d3b\u503c\u7279\u5f81\uff0c\u7136\u540e\u5e94\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5e7b\u89c9\u68c0\u6d4b\u3002", "result": "CHARM\u5728\u7406\u8bba\u4e0a\u5305\u542b\u5148\u524d\u7684\u6ce8\u610f\u529b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u4e0a\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u5176\u4ed6\u9886\u5148\u65b9\u6cd5\uff0c\u5e76\u663e\u793a\u51fa\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "\u56fe\u7ed3\u6784\u5728\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u7ed3\u5408\u8ba1\u7b97\u8f68\u8ff9\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cCHARM\u65b9\u6cd5\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2509.24779", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.24779", "abs": "https://arxiv.org/abs/2509.24779", "authors": ["Kacper Kapu\u015bniak", "Cristian Gabellini", "Michael Bronstein", "Prudencio Tossou", "Francesco Di Giovanni"], "title": "MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models", "comment": null, "summary": "Molecular Dynamics (MD) is a powerful computational microscope for probing\nprotein functions. However, the need for fine-grained integration and the long\ntimescales of biomolecular events make MD computationally expensive. To address\nthis, several generative models have been proposed to generate surrogate\ntrajectories at lower cost. Yet, these models typically learn a fixed-lag\ntransition density, causing the training signal to be dominated by frequent but\nuninformative transitions. We introduce a new class of generative models, MSM\nEmulators, which instead learn to sample transitions across discrete states\ndefined by an underlying Markov State Model (MSM). We instantiate this class\nwith Markov Space Flow Matching (MarS-FM), whose sampling offers more than two\norders of magnitude speedup compared to implicit- or explicit-solvent MD\nsimulations. We benchmark Mars-FM ability to reproduce MD statistics through\nstructural observables such as RMSD, radius of gyration, and secondary\nstructure content. Our evaluation spans protein domains (up to 500 residues)\nwith significant chemical and structural diversity, including unfolding events,\nand enforces strict sequence dissimilarity between training and test sets to\nassess generalization. Across all metrics, MarS-FM outperforms existing\nmethods, often by a substantial margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578bMSM Emulators\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u6a21\u578b\u5b66\u4e60\u79bb\u6563\u72b6\u6001\u95f4\u7684\u8f6c\u79fb\uff0c\u76f8\u6bd4\u4f20\u7edf\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5b9e\u73b0\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u5728\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5b66\u4e60\u56fa\u5b9a\u6ede\u540e\u8f6c\u79fb\u5bc6\u5ea6\uff0c\u8bad\u7ec3\u4fe1\u53f7\u88ab\u9891\u7e41\u4f46\u65e0\u4fe1\u606f\u7684\u8f6c\u79fb\u4e3b\u5bfc\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u86cb\u767d\u8d28\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5f15\u5165MSM Emulators\u7c7b\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u6a21\u578b\u5b66\u4e60\u79bb\u6563\u72b6\u6001\u95f4\u7684\u8f6c\u79fb\u91c7\u6837\u3002\u5177\u4f53\u5b9e\u73b0\u4e3aMarkov Space Flow Matching (MarS-FM)\u65b9\u6cd5\u3002", "result": "MarS-FM\u76f8\u6bd4\u9690\u5f0f\u6216\u663e\u5f0f\u6eb6\u5242\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5b9e\u73b0\u4e86\u8d85\u8fc7\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u5728RMSD\u3001\u56de\u8f6c\u534a\u5f84\u3001\u4e8c\u7ea7\u7ed3\u6784\u542b\u91cf\u7b49\u7ed3\u6784\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728500\u4e2a\u6b8b\u57fa\u4ee5\u5185\u7684\u86cb\u767d\u8d28\u57df\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MSM Emulators\u7279\u522b\u662fMarS-FM\u65b9\u6cd5\u4e3a\u86cb\u767d\u8d28\u52a8\u529b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7edf\u8ba1\u7279\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.24784", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24784", "abs": "https://arxiv.org/abs/2509.24784", "authors": ["Nathan Gavenski", "Odinaldo Rodrigues"], "title": "Quantifying Generalisation in Imitation Learning", "comment": "NeurIPS 2025 Datasets and Benchmarks Track poster", "summary": "Imitation learning benchmarks often lack sufficient variation between\ntraining and evaluation, limiting meaningful generalisation assessment. We\nintroduce Labyrinth, a benchmarking environment designed to test generalisation\nwith precise control over structure, start and goal positions, and task\ncomplexity. It enables verifiably distinct training, evaluation, and test\nsettings. Labyrinth provides a discrete, fully observable state space and known\noptimal actions, supporting interpretability and fine-grained evaluation. Its\nflexible setup allows targeted testing of generalisation factors and includes\nvariants like partial observability, key-and-door tasks, and ice-floor hazards.\nBy enabling controlled, reproducible experiments, Labyrinth advances the\nevaluation of generalisation in imitation learning and provides a valuable tool\nfor developing more robust agents.", "AI": {"tldr": "Labyrinth\u662f\u4e00\u4e2a\u4e13\u4e3a\u6d4b\u8bd5\u6a21\u4eff\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u73af\u5883\uff0c\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u7ed3\u6784\u3001\u8d77\u59cb\u4f4d\u7f6e\u3001\u76ee\u6807\u4f4d\u7f6e\u548c\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u4e0d\u540c\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u6d4b\u8bd5\u8bbe\u7f6e\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u57fa\u51c6\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e4b\u95f4\u7f3a\u4e4f\u8db3\u591f\u7684\u53d8\u5316\uff0c\u9650\u5236\u4e86\u6709\u610f\u4e49\u7684\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1Labyrinth\u73af\u5883\uff0c\u63d0\u4f9b\u79bb\u6563\u3001\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u72b6\u6001\u7a7a\u95f4\u548c\u5df2\u77e5\u6700\u4f18\u52a8\u4f5c\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u6027\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002\u73af\u5883\u8bbe\u7f6e\u7075\u6d3b\uff0c\u5305\u62ec\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u3001\u94a5\u5319\u95e8\u4efb\u52a1\u548c\u51b0\u9762\u5371\u9669\u7b49\u53d8\u4f53\u3002", "result": "Labyrinth\u80fd\u591f\u5b9e\u73b0\u53d7\u63a7\u3001\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\uff0c\u652f\u6301\u5bf9\u6cdb\u5316\u56e0\u7d20\u7684\u9488\u5bf9\u6027\u6d4b\u8bd5\u3002", "conclusion": "Labyrinth\u63a8\u8fdb\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2509.24788", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2509.24788", "abs": "https://arxiv.org/abs/2509.24788", "authors": ["Felix Strnad", "Jonathan Schmidt", "Fabian Mockert", "Philipp Hennig", "Nicole Ludwig"], "title": "Assessing the risk of future Dunkelflaute events for Germany using generative deep learning", "comment": null, "summary": "The European electricity power grid is transitioning towards renewable energy\nsources, characterized by an increasing share of off- and onshore wind and\nsolar power. However, the weather dependency of these energy sources poses a\nchallenge to grid stability, with so-called Dunkelflaute events -- periods of\nlow wind and solar power generation -- being of particular concern due to their\npotential to cause electricity supply shortages. In this study, we investigate\nthe impact of these events on the German electricity production in the years\nand decades to come. For this purpose, we adapt a recently developed generative\ndeep learning framework to downscale climate simulations from the CMIP6\nensemble. We first compare their statistics to the historical record taken from\nERA5 data. Next, we use these downscaled simulations to assess plausible future\noccurrences of Dunkelflaute events in Germany under the optimistic low\n(SSP2-4.5) and high (SSP5-8.5) emission scenarios. Our analysis indicates that\nboth the frequency and duration of Dunkelflaute events in Germany in the\nensemble mean are projected to remain largely unchanged compared to the\nhistorical period. This suggests that, under the considered climate scenarios,\nthe associated risk is expected to remain stable throughout the century.", "AI": {"tldr": "\u7814\u7a76\u5fb7\u56fd\u672a\u6765Dunkelflaute\u4e8b\u4ef6\uff08\u98ce\u80fd\u548c\u592a\u9633\u80fd\u53d1\u7535\u4f4e\u8c37\u671f\uff09\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u964d\u5c3a\u5ea6CMIP6\u6c14\u5019\u6a21\u62df\uff0c\u53d1\u73b0\u5728SSP2-4.5\u548cSSP5-8.5\u6392\u653e\u60c5\u666f\u4e0b\uff0c\u8fd9\u7c7b\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\u9884\u8ba1\u5c06\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u6b27\u6d32\u7535\u7f51\u5411\u53ef\u518d\u751f\u80fd\u6e90\u8f6c\u578b\uff0c\u4f46\u98ce\u80fd\u548c\u592a\u9633\u80fd\u7684\u5929\u6c14\u4f9d\u8d56\u6027\u5bf9\u7535\u7f51\u7a33\u5b9a\u6027\u6784\u6210\u6311\u6218\uff0c\u7279\u522b\u662fDunkelflaute\u4e8b\u4ef6\u53ef\u80fd\u5bfc\u81f4\u7535\u529b\u4f9b\u5e94\u77ed\u7f3a\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u964d\u5c3a\u5ea6CMIP6\u96c6\u5408\u7684\u6c14\u5019\u6a21\u62df\uff0c\u5e76\u4e0eERA5\u5386\u53f2\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5fb7\u56fd\u672a\u6765Dunkelflaute\u4e8b\u4ef6\u7684\u53d1\u751f\u60c5\u51b5\u3002", "result": "\u5728\u4e50\u89c2\u7684\u4f4e\u6392\u653e\uff08SSP2-4.5\uff09\u548c\u9ad8\u6392\u653e\uff08SSP5-8.5\uff09\u60c5\u666f\u4e0b\uff0c\u5fb7\u56fdDunkelflaute\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\u5728\u96c6\u5408\u5e73\u5747\u503c\u4e2d\u9884\u8ba1\u4e0e\u5386\u53f2\u65f6\u671f\u57fa\u672c\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u5728\u6240\u8003\u8651\u7684\u6c14\u5019\u60c5\u666f\u4e0b\uff0c\u4e0eDunkelflaute\u4e8b\u4ef6\u76f8\u5173\u7684\u98ce\u9669\u9884\u8ba1\u5728\u6574\u4e2a\u4e16\u7eaa\u5185\u4fdd\u6301\u7a33\u5b9a\u3002"}}
{"id": "2509.24789", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24789", "abs": "https://arxiv.org/abs/2509.24789", "authors": ["Zhijian Xu", "Wanxu Cai", "Xilin Dai", "Zhaorong Deng", "Qiang Xu"], "title": "Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series Forecasting", "comment": null, "summary": "The evaluation of time series forecasting models is hindered by a critical\nlack of high-quality benchmarks, leading to a potential illusion of progress.\nExisting datasets suffer from issues ranging from pre-training data\ncontamination in the age of LLMs to the causal and description leakage\nprevalent in early multimodal designs. To address this, we formalize the core\nprinciples of high-fidelity benchmarking, focusing on data sourcing integrity,\nstrict causal soundness, and structural clarity. We introduce Fidel-TS, a new\nlarge-scale benchmark built from the ground up on these principles by sourcing\ndata from live APIs. Our extensive experiments validate this approach by\nexposing the critical biases and design limitations of prior benchmarks.\nFurthermore, we conclusively demonstrate that the causal relevance of textual\ninformation is the key factor in unlocking genuine performance gains in\nmultimodal forecasting.", "AI": {"tldr": "Fidel-TS\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\uff0c\u901a\u8fc7\u4ece\u5b9e\u65f6API\u83b7\u53d6\u6570\u636e\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u6570\u636e\u6c61\u67d3\u548c\u56e0\u679c\u6cc4\u9732\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u6587\u672c\u4fe1\u606f\u7684\u56e0\u679c\u76f8\u5173\u6027\u662f\u63d0\u5347\u591a\u6a21\u6001\u9884\u6d4b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u8bc4\u4f30\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u5b58\u5728\u9884\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u548c\u56e0\u679c\u63cf\u8ff0\u6cc4\u9732\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8fdb\u5c55\u7684\u5047\u8c61\u3002", "method": "\u5236\u5b9a\u4e86\u9ad8\u4fdd\u771f\u57fa\u51c6\u7684\u6838\u5fc3\u539f\u5219\uff08\u6570\u636e\u6e90\u5b8c\u6574\u6027\u3001\u4e25\u683c\u56e0\u679c\u5408\u7406\u6027\u3001\u7ed3\u6784\u6e05\u6670\u6027\uff09\uff0c\u901a\u8fc7\u4ece\u5b9e\u65f6API\u83b7\u53d6\u6570\u636e\u6784\u5efaFidel-TS\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5148\u524d\u57fa\u51c6\u7684\u5173\u952e\u504f\u5dee\u548c\u8bbe\u8ba1\u9650\u5236\uff0c\u5e76\u8bc1\u660e\u6587\u672c\u4fe1\u606f\u7684\u56e0\u679c\u76f8\u5173\u6027\u662f\u591a\u6a21\u6001\u9884\u6d4b\u83b7\u5f97\u771f\u6b63\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "Fidel-TS\u57fa\u51c6\u901a\u8fc7\u4e25\u683c\u7684\u6570\u636e\u6e90\u548c\u56e0\u679c\u8bbe\u8ba1\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u56e0\u679c\u76f8\u5173\u6587\u672c\u4fe1\u606f\u5728\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.24800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24800", "abs": "https://arxiv.org/abs/2509.24800", "authors": ["Zixu Wang", "Hongbin Dong", "Xiaoping Zhang"], "title": "DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting", "comment": "10 pages, 5 figures", "summary": "Time series forecasting is crucial for various applications, such as weather,\ntraffic, electricity, and energy predictions. Currently, common time series\nforecasting methods are based on Transformers. However, existing approaches\nprimarily model limited time series or fixed scales, making it more challenging\nto capture diverse features cross different ranges. Additionally, traditional\nmethods like STL for complex seasonality-trend decomposition require\npre-specified seasonal periods and typically handle only single, fixed\nseasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive\nTransformer (DSAT-HD), which integrates three key innovations to address the\nlimitations of existing methods: 1) A hybrid decomposition mechanism combining\nEMA and Fourier decomposition with RevIN normalization, dynamically balancing\nseasonal and trend components through noise Top-k gating; 2) A multi-scale\nadaptive pathway leveraging a sparse allocator to route features to four\nparallel Transformer layers, followed by feature merging via a sparse combiner,\nenhanced by hybrid attention combining local CNNs and global interactions; 3) A\ndual-stream residual learning framework where CNN and MLP branches separately\nprocess seasonal and trend components, coordinated by a balanced loss function\nminimizing expert collaboration variance. Extensive experiments on nine\ndatasets demonstrate that DSAT-HD outperforms existing methods overall and\nachieves state-of-the-art performance on some datasets. Notably, it also\nexhibits stronger generalization capabilities across various transfer\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86DSAT-HD\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u5206\u89e3\u673a\u5236\u3001\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u901a\u8def\u548c\u53cc\u6d41\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u6355\u83b7\u8de8\u8303\u56f4\u591a\u6837\u7279\u5f81\u548c\u5904\u7406\u590d\u6742\u5b63\u8282\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5efa\u6a21\u6709\u9650\u65f6\u95f4\u5e8f\u5217\u6216\u56fa\u5b9a\u5c3a\u5ea6\uff0c\u96be\u4ee5\u6355\u83b7\u8de8\u4e0d\u540c\u8303\u56f4\u7684\u591a\u6837\u7279\u5f81\u3002\u4f20\u7edf\u5206\u89e3\u65b9\u6cd5\u5982STL\u9700\u8981\u9884\u5b9a\u4e49\u5b63\u8282\u6027\u5468\u671f\u4e14\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u56fa\u5b9a\u5b63\u8282\u6027\u3002", "method": "1) \u6df7\u5408\u5206\u89e3\u673a\u5236\uff1a\u7ed3\u5408EMA\u548c\u5085\u91cc\u53f6\u5206\u89e3\u4e0eRevIN\u5f52\u4e00\u5316\uff0c\u901a\u8fc7\u566a\u58f0Top-k\u95e8\u63a7\u52a8\u6001\u5e73\u8861\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff1b2) \u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u901a\u8def\uff1a\u5229\u7528\u7a00\u758f\u5206\u914d\u5668\u5c06\u7279\u5f81\u8def\u7531\u5230\u56db\u4e2a\u5e76\u884cTransformer\u5c42\uff0c\u901a\u8fc7\u7a00\u758f\u7ec4\u5408\u5668\u5408\u5e76\u7279\u5f81\uff0c\u7ed3\u5408\u5c40\u90e8CNN\u548c\u5168\u5c40\u4ea4\u4e92\u7684\u6df7\u5408\u6ce8\u610f\u529b\uff1b3) \u53cc\u6d41\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff1aCNN\u548cMLP\u5206\u652f\u5206\u522b\u5904\u7406\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4e13\u5bb6\u534f\u4f5c\u65b9\u5dee\u7684\u5e73\u8861\u635f\u5931\u51fd\u6570\u534f\u8c03\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDSAT-HD\u6574\u4f53\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728\u591a\u79cd\u8fc1\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DSAT-HD\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u5206\u89e3\u3001\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u548c\u53cc\u6d41\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u590d\u6742\u5b63\u8282\u6027\u548c\u8de8\u5c3a\u5ea6\u7279\u5f81\u6355\u83b7\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.24801", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.24801", "abs": "https://arxiv.org/abs/2509.24801", "authors": ["Anna Scampicchio", "Leonardo F. Toso", "Rahel Rickenbach", "James Anderson", "Melanie N. Zeilinger"], "title": "Physics-informed learning under mixing: How physical knowledge speeds up learning", "comment": null, "summary": "A major challenge in physics-informed machine learning is to understand how\nthe incorporation of prior domain knowledge affects learning rates when data\nare dependent. Focusing on empirical risk minimization with physics-informed\nregularization, we derive complexity-dependent bounds on the excess risk in\nprobability and in expectation. We prove that, when the physical prior\ninformation is aligned, the learning rate improves from the (slow) Sobolev\nminimax rate to the (fast) optimal i.i.d. one without any sample-size deflation\ndue to data dependence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4e2d\u6570\u636e\u4f9d\u8d56\u5bf9\u5b66\u4e60\u901f\u7387\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86\u5f53\u7269\u7406\u5148\u9a8c\u4fe1\u606f\u5bf9\u9f50\u65f6\uff0c\u5b66\u4e60\u901f\u7387\u53ef\u4ee5\u4ece\u7f13\u6162\u7684Sobolev\u6781\u5c0f\u6781\u5927\u901f\u7387\u63d0\u5347\u5230\u5feb\u901f\u7684\u6700\u4f18i.i.d.\u901f\u7387\uff0c\u4e14\u4e0d\u4f1a\u56e0\u6570\u636e\u4f9d\u8d56\u6027\u800c\u635f\u5931\u6837\u672c\u91cf\u3002", "motivation": "\u7406\u89e3\u5728\u6570\u636e\u4f9d\u8d56\u60c5\u51b5\u4e0b\uff0c\u878d\u5165\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u901f\u7387\u662f\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u63a8\u5bfc\u4e86\u6982\u7387\u548c\u671f\u671b\u4e0b\u7684\u8d85\u989d\u98ce\u9669\u590d\u6742\u6027\u76f8\u5173\u8fb9\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u5f53\u7269\u7406\u5148\u9a8c\u4fe1\u606f\u5bf9\u9f50\u65f6\uff0c\u5b66\u4e60\u901f\u7387\u4ece\u7f13\u6162\u7684Sobolev\u6781\u5c0f\u6781\u5927\u901f\u7387\u63d0\u5347\u5230\u5feb\u901f\u7684\u6700\u4f18i.i.d.\u901f\u7387\uff0c\u4e14\u6ca1\u6709\u56e0\u6570\u636e\u4f9d\u8d56\u6027\u5bfc\u81f4\u7684\u6837\u672c\u91cf\u7f29\u51cf\u3002", "conclusion": "\u7269\u7406\u5148\u9a8c\u4fe1\u606f\u7684\u6b63\u786e\u5bf9\u9f50\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6570\u636e\u4f9d\u8d56\u60c5\u51b5\u4e0b\u7684\u5b66\u4e60\u6027\u80fd\uff0c\u5b9e\u73b0\u4e0e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u76f8\u5f53\u7684\u6700\u4f18\u5b66\u4e60\u901f\u7387\u3002"}}
{"id": "2509.24804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24804", "abs": "https://arxiv.org/abs/2509.24804", "authors": ["Boxuan Zhang", "Runqing Wang", "Wei Xiao", "Weipu Zhang", "Jian Sun", "Gao Huang", "Jie Chen", "Gang Wang"], "title": "DyMoDreamer: World Modeling with Dynamic Modulation", "comment": null, "summary": "A critical bottleneck in deep reinforcement learning (DRL) is sample\ninefficiency, as training high-performance agents often demands extensive\nenvironmental interactions. Model-based reinforcement learning (MBRL) mitigates\nthis by building world models that simulate environmental dynamics and generate\nsynthetic experience, improving sample efficiency. However, conventional world\nmodels process observations holistically, failing to decouple dynamic objects\nand temporal features from static backgrounds. This approach is computationally\ninefficient, especially for visual tasks where dynamic objects significantly\ninfluence rewards and decision-making performance. To address this, we\nintroduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic\nmodulation mechanism to improve the extraction of dynamic features and enrich\nthe temporal information. DyMoDreamer employs differential observations derived\nfrom a novel inter-frame differencing mask, explicitly encoding object-level\nmotion cues and temporal dynamics. Dynamic modulation is modeled as stochastic\ncategorical distributions and integrated into a recurrent state-space model\n(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments\ndemonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k\nbenchmark with a $156.6$\\% mean human-normalized score, establishes a new\nrecord of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\\%\nperformance improvement after $1$M steps on the Crafter benchmark. Our code is\nreleased at https://github.com/Ultraman-Tiga1/DyMoDreamer.", "AI": {"tldr": "DyMoDreamer\u662f\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u673a\u5236\u6539\u8fdb\u52a8\u6001\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u4fe1\u606f\u4e30\u5bcc\u5ea6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u4f20\u7edf\u4e16\u754c\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u89e3\u8026\u52a8\u6001\u5bf9\u8c61\u548c\u9759\u6001\u80cc\u666f\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u52a8\u6001\u5bf9\u8c61\u5bf9\u5956\u52b1\u548c\u51b3\u7b56\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u52a8\u6001\u8c03\u5236\u673a\u5236\uff0c\u4f7f\u7528\u57fa\u4e8e\u5e27\u95f4\u5dee\u5206\u63a9\u7801\u7684\u5dee\u5206\u89c2\u6d4b\u6765\u663e\u5f0f\u7f16\u7801\u5bf9\u8c61\u7ea7\u8fd0\u52a8\u7ebf\u7d22\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5c06\u52a8\u6001\u8c03\u5236\u5efa\u6a21\u4e3a\u968f\u673a\u5206\u7c7b\u5206\u5e03\u5e76\u96c6\u6210\u5230\u5faa\u73af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u3002", "result": "\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230156.6%\u7684\u5e73\u5747\u4eba\u7c7b\u6807\u51c6\u5316\u5206\u6570\uff0c\u5728DeepMind\u89c6\u89c9\u63a7\u5236\u5957\u4ef6\u4e2d\u521b\u4e0b832\u7684\u65b0\u8bb0\u5f55\uff0c\u5728Crafter\u57fa\u51c6\u6d4b\u8bd5\u4e2d100\u4e07\u6b65\u540e\u6027\u80fd\u63d0\u53479.5%\u3002", "conclusion": "DyMoDreamer\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.24827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24827", "abs": "https://arxiv.org/abs/2509.24827", "authors": ["Bartosz Bieganowski", "Daniel Strzelecki", "Robert Skiba", "Mateusz Topolewski"], "title": "Putnam-like dataset summary: LLMs as mathematical competition contestants", "comment": "11 pages, 11 figures", "summary": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests.", "AI": {"tldr": "\u5206\u6790Google DeepMind\u53d1\u5e03\u7684\u666e\u7279\u5357\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8bc4\u4f30LLMs\u5728\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e0a\u7684\u89e3\u51b3\u80fd\u529b", "motivation": "\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u666e\u7279\u5357\u7ade\u8d5b\u98ce\u683c\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u5305\u542b96\u4e2a\u539f\u521b\u666e\u7279\u5357\u98ce\u683c\u95ee\u9898\u548c576\u4e2aLLM\u89e3\u51b3\u65b9\u6848\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790", "result": "\u8bc4\u4f30\u4e86\u4e0d\u540cLLM\u6a21\u578b\u5728\u8fd9\u7ec4\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e0a\u7684\u8868\u73b0", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30LLMs\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6"}}
{"id": "2509.24840", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.24840", "abs": "https://arxiv.org/abs/2509.24840", "authors": ["Oussama Kharouiche", "Aris Markogiannakis", "Xiao Fei", "Michail Chatzianastasis", "Michalis Vazirgiannis"], "title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data", "comment": null, "summary": "Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells.", "AI": {"tldr": "Cell2Text\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u5c06\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u7ed3\u5408\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u5305\u542b\u7ec6\u80de\u8eab\u4efd\u3001\u7ec4\u7ec7\u6765\u6e90\u3001\u75be\u75c5\u5173\u8054\u548c\u901a\u8def\u6d3b\u52a8\u7684\u8fde\u8d2f\u6458\u8981\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u5c40\u9650\u4e8e\u79bb\u6563\u9884\u6d4b\u5934\uff0c\u5c06\u7ec6\u80de\u590d\u6742\u6027\u538b\u7f29\u4e3a\u9884\u5b9a\u4e49\u6807\u7b7e\uff0c\u65e0\u6cd5\u6355\u6349\u751f\u7269\u5b66\u5bb6\u6240\u9700\u7684\u66f4\u4e30\u5bcc\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u6574\u5408\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u57fa\u56e0\u7ea7\u5d4c\u5165\u4e0e\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u5c06scRNA-seq\u56fe\u8c31\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "Cell2Text\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8ePageRank\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u663e\u793a\u51fa\u5f3a\u5927\u7684\u672c\u4f53\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u6587\u672c\u751f\u6210\u4e2d\u5b9e\u73b0\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u5c06\u8868\u8fbe\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u76f8\u7ed3\u5408\u4e0d\u4ec5\u63d0\u4f9b\u66f4\u5f3a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u4ea7\u751f\u56fa\u6709\u7684\u53ef\u89e3\u91ca\u8f93\u51fa\uff0c\u4e3a\u672a\u89c1\u7ec6\u80de\u7684\u6807\u7b7e\u9ad8\u6548\u8868\u5f81\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2509.24856", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24856", "abs": "https://arxiv.org/abs/2509.24856", "authors": ["Christos Mountzouris"], "title": "Beyond the Hook: Predicting Billboard Hot 100 Chart Inclusion with Machine Learning from Streaming, Audio Signals, and Perceptual Features", "comment": "17 pages, 6 figures, 3 tables", "summary": "The advent of digital streaming platforms have recently revolutionized the\nlandscape of music industry, with the ensuing digitalization providing\nstructured data collections that open new research avenues for investigating\npopularity dynamics and mainstream success. The present work explored which\ndeterminants hold the strongest predictive influence for a track's inclusion in\nthe Billboard Hot 100 charts, including streaming popularity, measurable audio\nsignal attributes, and probabilistic indicators of human listening. The\nanalysis revealed that popularity was by far the most decisive predictor of\nBillboard Hot 100 inclusion, with considerable contribution from\ninstrumentalness, valence, duration and speechiness. Logistic Regression\nachieved 90.0% accuracy, with very high recall for charting singles (0.986) but\nlower recall for non-charting ones (0.813), yielding balanced F1-scores around\n0.90. Random Forest slightly improved performance to 90.4% accuracy,\nmaintaining near-perfect precision for non-charting singles (0.990) and high\nrecall for charting ones (0.992), with F1-scores up to 0.91. Gradient Boosting\n(XGBoost) reached 90.3% accuracy, delivering a more balanced trade-off by\nimproving recall for non-charting singles (0.837) while sustaining high recall\nfor charting ones (0.969), resulting in F1-scores comparable to the other\nmodels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u51b3\u5b9a\u6b4c\u66f2\u80fd\u5426\u8fdb\u5165Billboard Hot 100\u699c\u5355\u7684\u5173\u952e\u56e0\u7d20\uff0c\u53d1\u73b0\u6d41\u884c\u5ea6\u662f\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u5176\u6b21\u662f\u4e50\u5668\u5ea6\u3001\u60c5\u611f\u4ef7\u3001\u65f6\u957f\u548c\u8bed\u97f3\u5ea6\u3002\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90fd\u8fbe\u5230\u4e86\u7ea690%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u6570\u5b57\u6d41\u5a92\u4f53\u5e73\u53f0\u7684\u5174\u8d77\u4e3a\u7814\u7a76\u97f3\u4e50\u6d41\u884c\u5ea6\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u673a\u4f1a\uff0c\u63a2\u7d22\u54ea\u4e9b\u56e0\u7d20\u6700\u80fd\u9884\u6d4b\u6b4c\u66f2\u8fdb\u5165Billboard Hot 100\u699c\u5355\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347(XGBoost)\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5206\u6790\u6d41\u5a92\u4f53\u6d41\u884c\u5ea6\u3001\u97f3\u9891\u4fe1\u53f7\u5c5e\u6027\u548c\u4eba\u7c7b\u6536\u542c\u6982\u7387\u6307\u6807\u5bf9\u699c\u5355\u5165\u9009\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u903b\u8f91\u56de\u5f52\u51c6\u786e\u738790.0%\uff0c\u968f\u673a\u68ee\u679790.4%\uff0cXGBoost 90.3%\u3002\u6d41\u884c\u5ea6\u662f\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u4e50\u5668\u5ea6\u3001\u60c5\u611f\u4ef7\u3001\u65f6\u957f\u548c\u8bed\u97f3\u5ea6\u4e5f\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u6d41\u884c\u5ea6\u662f\u9884\u6d4bBillboard Hot 100\u5165\u9009\u7684\u6700\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u9884\u6d4b\u6b4c\u66f2\u7684\u5546\u4e1a\u6210\u529f\u3002"}}
{"id": "2509.24868", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.24868", "abs": "https://arxiv.org/abs/2509.24868", "authors": ["Jiayi Li", "Flora D. Salim"], "title": "DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning", "comment": null, "summary": "Learning PDE dynamics with neural solvers can significantly improve\nwall-clock efficiency and accuracy compared with classical numerical solvers.\nIn recent years, foundation models for PDEs have largely adopted multi-scale\nwindowed self-attention, with the scOT backbone in \\textsc{Poseidon} serving as\na representative example.\n  However, because of their locality, truly globally consistent spectral\ncoupling can only be propagated gradually through deep stacking and window\nshifting. This weakens global coupling and leads to error accumulation and\ndrift during closed-loop rollouts. To address this, we propose\n\\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral\nbranch and an image branch. The spectral branch is responsible for capturing\nglobal, large-scale low-frequency information, whereas the image branch focuses\non local details and nonstationary structures. Specifically, we first perform\ncontrolled, lightweight mixing within the low-frequency range. Then we fuse the\nspectral and image paths at each layer via bandwise weighting, which avoids the\nwidth inflation and training instability caused by naive concatenation. The\nfused result is transformed back into the spatial domain and added to the image\nbranch, thereby preserving both global structure and high-frequency details\nacross scales. Compared with strong attention-based baselines, DRIFT-Net\nachieves lower error and higher throughput with fewer parameters under\nidentical training settings and budget. On Navier--Stokes benchmarks, the\nrelative $L_{1}$ error is reduced by 7\\%--54\\%, the parameter count decreases\nby about 15\\%, and the throughput remains higher than scOT. Ablation studies\nand theoretical analyses further demonstrate the stability and effectiveness of\nthis design. The code is available at\nhttps://github.com/cruiseresearchgroup/DRIFT-Net.", "AI": {"tldr": "DRIFT-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u8c31\u5206\u652f\u6355\u83b7\u5168\u5c40\u4f4e\u9891\u4fe1\u606f\uff0c\u56fe\u50cf\u5206\u652f\u5904\u7406\u5c40\u90e8\u7ec6\u8282\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PDE\u57fa\u7840\u6a21\u578b\u4e2d\u5168\u5c40\u8026\u5408\u5f31\u5316\u7684\u95ee\u9898\uff0c\u5728Navier-Stokes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684PDE\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u91c7\u7528\u591a\u5c3a\u5ea6\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u7531\u4e8e\u5176\u5c40\u90e8\u6027\uff0c\u771f\u6b63\u7684\u5168\u5c40\u8c31\u8026\u5408\u53ea\u80fd\u901a\u8fc7\u6df1\u5ea6\u5806\u53e0\u548c\u7a97\u53e3\u79fb\u52a8\u9010\u6b65\u4f20\u64ad\uff0c\u8fd9\u524a\u5f31\u4e86\u5168\u5c40\u8026\u5408\uff0c\u5bfc\u81f4\u95ed\u73af\u63a8\u6f14\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u6f02\u79fb\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u8bbe\u8ba1\uff1a\u8c31\u5206\u652f\u8d1f\u8d23\u6355\u83b7\u5168\u5c40\u5927\u5c3a\u5ea6\u4f4e\u9891\u4fe1\u606f\uff0c\u56fe\u50cf\u5206\u652f\u4e13\u6ce8\u4e8e\u5c40\u90e8\u7ec6\u8282\u548c\u975e\u5e73\u7a33\u7ed3\u6784\u3002\u9996\u5148\u5728\u4f4e\u9891\u8303\u56f4\u5185\u8fdb\u884c\u53d7\u63a7\u8f7b\u91cf\u7ea7\u6df7\u5408\uff0c\u7136\u540e\u901a\u8fc7\u5e26\u6743\u52a0\u6743\u878d\u5408\u8c31\u8def\u5f84\u548c\u56fe\u50cf\u8def\u5f84\uff0c\u907f\u514d\u7b80\u5355\u62fc\u63a5\u5bfc\u81f4\u7684\u5bbd\u5ea6\u81a8\u80c0\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "result": "\u5728Navier-Stokes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u5bf9L1\u8bef\u5dee\u964d\u4f4e\u4e867%-54%\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u7ea615%\uff0c\u541e\u5410\u91cf\u4ecd\u9ad8\u4e8escOT\u57fa\u51c6\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u548c\u7406\u8bba\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u8bbe\u8ba1\u7684\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DRIFT-Net\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86PDE\u6c42\u89e3\u5668\u4e2d\u5168\u5c40\u8026\u5408\u5f31\u5316\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\uff0c\u4e3aPDE\u52a8\u529b\u5b66\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24873", "abs": "https://arxiv.org/abs/2509.24873", "authors": ["Teodor Chiaburu", "Vipin Singh", "Frank Hau\u00dfer", "Felix Bie\u00dfmann"], "title": "Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation", "comment": "11 pages, 7 figures, presented at ECAI 2025, CLEAR-AI Workshop,\n  Bologna", "summary": "Uncertainty quantification is essential in human-machine collaboration, as\nhuman agents tend to adjust their decisions based on the confidence of the\nmachine counterpart. Reliably calibrated model uncertainties, hence, enable\nmore effective collaboration, targeted expert intervention and more responsible\nusage of Machine Learning (ML) systems. Conformal prediction has become a well\nestablished model-agnostic framework for uncertainty calibration of ML models,\noffering statistically valid confidence estimates for both regression and\nclassification tasks. In this work, we apply conformal prediction to\n$\\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.\nWe design a simulated human-in-the-loop (HIL) annotation pipeline, where a\nlimited budget for obtaining ground truth annotations from domain experts is\navailable when model uncertainty is high. Our experiments show that\nconformalizing SoilNet leads to more efficient annotation in regression tasks\nand comparable performance scores in classification tasks under the same\nannotation budget when tested against its non-conformal counterpart. All code\nand experiments can be found in our repository:\nhttps://github.com/calgo-lab/BGR", "AI": {"tldr": "\u5c06\u4fdd\u5f62\u9884\u6d4b\u5e94\u7528\u4e8eSoilNet\u571f\u58e4\u5256\u9762\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u901a\u8fc7\u6a21\u62df\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\uff0c\u63d0\u5347\u56de\u5f52\u4efb\u52a1\u6807\u6ce8\u6548\u7387\u5e76\u4fdd\u6301\u5206\u7c7b\u4efb\u52a1\u6027\u80fd", "motivation": "\u4eba\u7c7b-\u673a\u5668\u534f\u4f5c\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4eba\u7c7b\u4f1a\u6839\u636e\u673a\u5668\u7f6e\u4fe1\u5ea6\u8c03\u6574\u51b3\u7b56\u3002\u53ef\u9760\u6821\u51c6\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u53ef\u5b9e\u73b0\u66f4\u6709\u6548\u534f\u4f5c\u3001\u9488\u5bf9\u6027\u4e13\u5bb6\u5e72\u9884\u548c\u66f4\u8d1f\u8d23\u4efb\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4f7f\u7528", "method": "\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\u6821\u51c6SoilNet\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u8bbe\u8ba1\u6a21\u62df\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\uff0c\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u9ad8\u65f6\u4f7f\u7528\u6709\u9650\u9884\u7b97\u83b7\u53d6\u9886\u57df\u4e13\u5bb6\u6807\u6ce8", "result": "\u4fdd\u5f62\u5316\u540e\u7684SoilNet\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6807\u6ce8\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5728\u76f8\u540c\u6807\u6ce8\u9884\u7b97\u4e0b\u83b7\u5f97\u4e0e\u672a\u4fdd\u5f62\u7248\u672c\u76f8\u5f53\u7684\u6027\u80fd\u5206\u6570", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u80fd\u6709\u6548\u63d0\u5347\u571f\u58e4\u5256\u9762\u6a21\u578b\u5728\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u4e2d\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457"}}
{"id": "2509.24882", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24882", "abs": "https://arxiv.org/abs/2509.24882", "authors": ["Leonardo Defilippis", "Yizhou Xu", "Julius Girardin", "Emanuele Troiani", "Vittorio Erba", "Lenka Zdeborov\u00e1", "Bruno Loureiro", "Florent Krzakala"], "title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime", "comment": null, "summary": "Neural scaling laws underlie many of the recent advances in deep learning,\nyet their theoretical understanding remains largely confined to linear models.\nIn this work, we present a systematic analysis of scaling laws for quadratic\nand diagonal neural networks in the feature learning regime. Leveraging\nconnections with matrix compressed sensing and LASSO, we derive a detailed\nphase diagram for the scaling exponents of the excess risk as a function of\nsample complexity and weight decay. This analysis uncovers crossovers between\ndistinct scaling regimes and plateau behaviors, mirroring phenomena widely\nreported in the empirical neural scaling literature. Furthermore, we establish\na precise link between these regimes and the spectral properties of the trained\nnetwork weights, which we characterize in detail. As a consequence, we provide\na theoretical validation of recent empirical observations connecting the\nemergence of power-law tails in the weight spectrum with network generalization\nperformance, yielding an interpretation from first principles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u4e8c\u6b21\u548cdiagonal\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5f81\u5b66\u4e60\u673a\u5236\u4e0b\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u63ed\u793a\u4e86\u98ce\u9669\u7f29\u653e\u6307\u6570\u968f\u6837\u672c\u590d\u6742\u5ea6\u548c\u6743\u91cd\u8870\u51cf\u53d8\u5316\u7684\u76f8\u56fe\uff0c\u53d1\u73b0\u4e86\u4e0d\u540c\u7f29\u653e\u673a\u5236\u4e4b\u95f4\u7684\u4ea4\u53c9\u548c\u5e73\u53f0\u884c\u4e3a\u3002", "motivation": "\u795e\u7ecf\u7f29\u653e\u89c4\u5f8b\u662f\u6df1\u5ea6\u5b66\u4e60\u8fd1\u671f\u8fdb\u5c55\u7684\u57fa\u7840\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4e3b\u8981\u5c40\u9650\u4e8e\u7ebf\u6027\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u7406\u89e3\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5f81\u5b66\u4e60\u673a\u5236\u4e0b\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "method": "\u5229\u7528\u4e0e\u77e9\u9635\u538b\u7f29\u611f\u77e5\u548cLASSO\u7684\u8054\u7cfb\uff0c\u5206\u6790\u4e8c\u6b21\u548cdiagonal\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5f81\u5b66\u4e60\u673a\u5236\u4e0b\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u63a8\u5bfc\u98ce\u9669\u7f29\u653e\u6307\u6570\u7684\u76f8\u56fe\u3002", "result": "\u53d1\u73b0\u4e86\u4e0d\u540c\u7f29\u653e\u673a\u5236\u4e4b\u95f4\u7684\u4ea4\u53c9\u548c\u5e73\u53f0\u884c\u4e3a\uff0c\u8fd9\u4e9b\u73b0\u8c61\u4e0e\u7ecf\u9a8c\u795e\u7ecf\u7f29\u653e\u6587\u732e\u4e2d\u5e7f\u6cdb\u62a5\u544a\u7684\u73b0\u8c61\u76f8\u7b26\u3002\u5efa\u7acb\u4e86\u8fd9\u4e9b\u673a\u5236\u4e0e\u8bad\u7ec3\u7f51\u7edc\u6743\u91cd\u8c31\u7279\u6027\u4e4b\u95f4\u7684\u7cbe\u786e\u8054\u7cfb\u3002", "conclusion": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63d0\u4f9b\u4e86\u5bf9\u6743\u91cd\u8c31\u4e2d\u51fa\u73b0\u5e42\u5f8b\u5c3e\u4e0e\u7f51\u7edc\u6cdb\u5316\u6027\u80fd\u4e4b\u95f4\u8054\u7cfb\u7684\u7406\u8bba\u9a8c\u8bc1\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f29\u653e\u89c4\u5f8b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.24886", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24886", "abs": "https://arxiv.org/abs/2509.24886", "authors": ["Ya-Wei Eileen Lin", "Ron Levie"], "title": "Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks", "comment": null, "summary": "Canonicalization is a widely used strategy in equivariant machine learning,\nenforcing symmetry in neural networks by mapping each input to a standard form.\nYet, it often introduces discontinuities that can affect stability during\ntraining, limit generalization, and complicate universal approximation\ntheorems. In this paper, we address this by introducing \\emph{adaptive\ncanonicalization}, a general framework in which the canonicalization depends\nboth on the input and the network. Specifically, we present the adaptive\ncanonicalization based on prior maximization, where the standard form of the\ninput is chosen to maximize the predictive confidence of the network. We prove\nthat this construction yields continuous and symmetry-respecting models that\nadmit universal approximation properties.\n  We propose two applications of our setting: (i) resolving eigenbasis\nambiguities in spectral graph neural networks, and (ii) handling rotational\nsymmetries in point clouds. We empirically validate our methods on molecular\nand protein classification, as well as point cloud classification tasks. Our\nadaptive canonicalization outperforms the three other common solutions to\nequivariant machine learning: data augmentation, standard canonicalization, and\nequivariant architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u6b63\u5219\u5316\u8fc7\u7a0b\u4f9d\u8d56\u4e8e\u8f93\u5165\u548c\u7f51\u7edc\u672c\u8eab\u6765\u89e3\u51b3\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6cd5\u4e2d\u7684\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u7b49\u53d8\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4f1a\u5f15\u5165\u4e0d\u8fde\u7eed\u6027\uff0c\u5f71\u54cd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u7684\u8bc1\u660e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5148\u9a8c\u6700\u5927\u5316\u7684\u81ea\u9002\u5e94\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u9009\u62e9\u4f7f\u7f51\u7edc\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6700\u5927\u5316\u7684\u6807\u51c6\u5f62\u5f0f\u6765\u5b9e\u73b0\u8fde\u7eed\u4e14\u4fdd\u6301\u5bf9\u79f0\u6027\u7684\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u8fde\u7eed\u4e14\u4fdd\u6301\u5bf9\u79f0\u6027\u7684\u6a21\u578b\uff0c\u5e76\u5177\u6709\u901a\u7528\u903c\u8fd1\u6027\u8d28\u3002\u5728\u5206\u5b50\u5206\u7c7b\u3001\u86cb\u767d\u8d28\u5206\u7c7b\u548c\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6570\u636e\u589e\u5f3a\u3001\u6807\u51c6\u6b63\u5219\u5316\u548c\u7b49\u53d8\u67b6\u6784\u3002", "conclusion": "\u81ea\u9002\u5e94\u6b63\u5219\u5316\u662f\u89e3\u51b3\u7b49\u53d8\u673a\u5668\u5b66\u4e60\u4e2d\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.24895", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24895", "abs": "https://arxiv.org/abs/2509.24895", "authors": ["Kosio Beshkov", "Anders Malthe-S\u00f8renssen"], "title": "Towards Understanding the Shape of Representations in Protein Language Models", "comment": null, "summary": "While protein language models (PLMs) are one of the most promising avenues of\nresearch for future de novo protein design, the way in which they transform\nsequences to hidden representations, as well as the information encoded in such\nrepresentations is yet to be fully understood. Several works have attempted to\npropose interpretability tools for PLMs, but they have focused on understanding\nhow individual sequences are transformed by such models. Therefore, the way in\nwhich PLMs transform the whole space of sequences along with their relations is\nstill unknown. In this work we attempt to understand this transformed space of\nsequences by identifying protein structure and representation with square-root\nvelocity (SRV) representations and graph filtrations. Both approaches naturally\nlead to a metric space in which pairs of proteins or protein representations\ncan be compared with each other.\n  We analyze different types of proteins from the SCOP dataset and show that\nthe Karcher mean and effective dimension of the SRV shape space follow a\nnon-linear pattern as a function of the layers in ESM2 models of different\nsizes. Furthermore, we use graph filtrations as a tool to study the context\nlengths at which models encode the structural features of proteins. We find\nthat PLMs preferentially encode immediate as well as local relations between\nresidues, but start to degrade for larger context lengths. The most\nstructurally faithful encoding tends to occur close to, but before the last\nlayer of the models, indicating that training a folding model ontop of these\nlayers might lead to improved folding performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5e73\u65b9\u6839\u901f\u5ea6\u8868\u793a\u548c\u56fe\u5f62\u8fc7\u6ee4\u65b9\u6cd5\u5206\u6790\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u8f6c\u6362\u5e8f\u5217\u7a7a\u95f4\uff0c\u53d1\u73b0PLMs\u4f18\u5148\u7f16\u7801\u6b8b\u57fa\u95f4\u7684\u5c40\u90e8\u5173\u7cfb\uff0c\u6700\u7ed3\u6784\u5fe0\u5b9e\u7684\u7f16\u7801\u51fa\u73b0\u5728\u63a5\u8fd1\u4f46\u65e9\u4e8e\u6700\u540e\u4e00\u5c42\u7684\u4f4d\u7f6e\u3002", "motivation": "\u7406\u89e3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u6574\u4e2a\u5e8f\u5217\u7a7a\u95f4\u53ca\u5176\u5173\u7cfb\u8f6c\u6362\u4e3a\u9690\u85cf\u8868\u793a\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9PLMs\u8f6c\u6362\u6574\u4e2a\u5e8f\u5217\u7a7a\u95f4\u65b9\u5f0f\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5e73\u65b9\u6839\u901f\u5ea6\u8868\u793a\u548c\u56fe\u5f62\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5728SCOP\u6570\u636e\u96c6\u4e0a\u5206\u6790\u4e0d\u540c\u86cb\u767d\u8d28\uff0c\u7814\u7a76ESM2\u6a21\u578b\u4e2dKarcher\u5747\u503c\u548c\u6709\u6548\u7ef4\u5ea6\u7684\u53d8\u5316\u6a21\u5f0f\u3002", "result": "PLMs\u4f18\u5148\u7f16\u7801\u6b8b\u57fa\u95f4\u7684\u5373\u65f6\u548c\u5c40\u90e8\u5173\u7cfb\uff0c\u4f46\u5728\u8f83\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u6027\u80fd\u4e0b\u964d\uff1b\u6700\u7ed3\u6784\u5fe0\u5b9e\u7684\u7f16\u7801\u51fa\u73b0\u5728\u63a5\u8fd1\u4f46\u65e9\u4e8e\u6700\u540e\u4e00\u5c42\u7684\u4f4d\u7f6e\u3002", "conclusion": "\u5728PLMs\u63a5\u8fd1\u4f46\u65e9\u4e8e\u6700\u540e\u4e00\u5c42\u7684\u8868\u793a\u4e0a\u8bad\u7ec3\u6298\u53e0\u6a21\u578b\u53ef\u80fd\u4f1a\u63d0\u9ad8\u6298\u53e0\u6027\u80fd\u3002"}}
{"id": "2509.24923", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24923", "abs": "https://arxiv.org/abs/2509.24923", "authors": ["Sanxing Chen", "Xiaoyin Chen", "Yukun Huang", "Roy Xie", "Bhuwan Dhingra"], "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training", "comment": null, "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4e24\u79cd\u65b9\u6cd5\u5728\u63d0\u5347LLMs\u591a\u81c2\u8001\u864e\u673a\u4efb\u52a1\u63a2\u7d22\u80fd\u529b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u7ecf\u5178\u7b97\u6cd5\uff0c\u4f46\u5b58\u5728\u66f4\u8d2a\u5a6a\u7684\u5229\u7528\u503e\u5411\u3002", "motivation": "\u867d\u7136LLMs\u6709\u6f5c\u529b\u6210\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u4f46\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u7814\u7a76\u901a\u8fc7SFT\u6216RL\u6539\u8fdb\u591a\u81c2\u8001\u864e\u673a\u4efb\u52a1\u4e2d\u7684\u9057\u61be\u503c\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u5851\u9020\u63a2\u7d22\u7b56\u7565\u53ca\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u8f68\u8ff9\u8fdb\u884cSFT\u8bad\u7ec3\uff0c\u4ee5\u53ca\u4f7f\u7528\u5305\u62ec\u7b56\u7565\u6027\u9057\u61be\u5f62\u5956\u52b1\u548c\u7b97\u6cd5\u5956\u52b1\u5728\u5185\u7684\u591a\u79cd\u5b9a\u5236\u5956\u52b1\u4fe1\u53f7\u8fdb\u884cRL\u8bad\u7ec3\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u667a\u80fd\u4f53\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6027\u80fd\u4e0eUCB\u548cThompson Sampling\u76f8\u5f53\uff0c\u57286\u500d\u66f4\u957f\u65f6\u57df\u548c\u8de8\u8001\u864e\u673a\u5bb6\u65cf\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6cdb\u5316\u80fd\u529b\u3002\u884c\u4e3a\u5206\u6790\u663e\u793a\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u81ea\u66f4\u590d\u6742\u4f46\u66f4\u8d2a\u5a6a\u7684\u5229\u7528\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u6bcf\u79cd\u8bad\u7ec3\u8303\u5f0f\u7684\u9002\u7528\u573a\u666f\uff0c\u5efa\u8bae\u8d85\u8d8a\u5e73\u5747\u9057\u61be\u7684\u5b9a\u5236\u5956\u52b1\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u7a33\u5065\u7684\u63a2\u7d22\u884c\u4e3a\u3002"}}
{"id": "2509.24933", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24933", "abs": "https://arxiv.org/abs/2509.24933", "authors": ["Sebastian W. Ober", "Calvin McCarter", "Aniruddh Raghu", "Yucen Lily Li", "Alan N. Amin", "Andrew Gordon Wilson", "Hunter Elliott"], "title": "Is Sequence Information All You Need for Bayesian Optimization of Antibodies?", "comment": "Accepted into the AI for Science Workshop, NeurIPS 2025", "summary": "Bayesian optimization is a natural candidate for the engineering of antibody\ntherapeutic properties, which is often iterative and expensive. However,\nfinding the optimal choice of surrogate model for optimization over the highly\nstructured antibody space is difficult, and may differ depending on the\nproperty being optimized. Moreover, to the best of our knowledge, no prior\nworks have attempted to incorporate structural information into antibody\nBayesian optimization. In this work, we explore different approaches to\nincorporating structural information into Bayesian optimization, and compare\nthem to a variety of sequence-only approaches on two different antibody\nproperties, binding affinity and stability. In addition, we propose the use of\na protein language model-based ``soft constraint,'' which helps guide the\noptimization to promising regions of the space. We find that certain types of\nstructural information improve data efficiency in early optimization rounds for\nstability, but have equivalent peak performance. Moreover, when incorporating\nthe protein language model soft constraint we find that the data efficiency gap\nis diminished for affinity and eliminated for stability, resulting in\nsequence-only methods that match the performance of structure-based methods,\nraising questions about the necessity of structure in Bayesian optimization for\nantibodies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u6297\u4f53\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u6574\u5408\u7ed3\u6784\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4ec5\u4f7f\u7528\u5e8f\u5217\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u53d1\u73b0\u7ed3\u6784\u4fe1\u606f\u5728\u65e9\u671f\u4f18\u5316\u9636\u6bb5\u80fd\u63d0\u9ad8\u7a33\u5b9a\u6027\u4f18\u5316\u7684\u6570\u636e\u6548\u7387\uff0c\u4f46\u5cf0\u503c\u6027\u80fd\u76f8\u5f53\u3002\u901a\u8fc7\u5f15\u5165\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u7ea6\u675f\uff0c\u5e8f\u5217\u65b9\u6cd5\u53ef\u4ee5\u8fbe\u5230\u4e0e\u7ed3\u6784\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u6297\u4f53\u6cbb\u7597\u7279\u6027\u7684\u5de5\u7a0b\u5316\u901a\u5e38\u662f\u8fed\u4ee3\u4e14\u6602\u8d35\u7684\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u81ea\u7136\u9009\u62e9\u3002\u7136\u800c\uff0c\u5728\u9ad8\u5ea6\u7ed3\u6784\u5316\u7684\u6297\u4f53\u7a7a\u95f4\u4e2d\u4e3a\u4f18\u5316\u9009\u62e9\u5408\u9002\u7684\u4ee3\u7406\u6a21\u578b\u5f88\u56f0\u96be\uff0c\u4e14\u53ef\u80fd\u56e0\u4f18\u5316\u7279\u6027\u800c\u5f02\u3002\u76ee\u524d\u6ca1\u6709\u5de5\u4f5c\u5c1d\u8bd5\u5c06\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u5230\u6297\u4f53\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u3002", "method": "\u63a2\u7d22\u4e86\u5c06\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u5230\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u4e0e\u591a\u79cd\u4ec5\u5e8f\u5217\u65b9\u6cd5\u5728\u6297\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u548c\u7a33\u5b9a\u6027\u4e24\u4e2a\u7279\u6027\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\"\u8f6f\u7ea6\u675f\"\u6765\u5f15\u5bfc\u4f18\u5316\u5230\u6709\u524d\u666f\u7684\u7a7a\u95f4\u533a\u57df\u3002", "result": "\u67d0\u4e9b\u7c7b\u578b\u7684\u7ed3\u6784\u4fe1\u606f\u5728\u65e9\u671f\u4f18\u5316\u8f6e\u6b21\u4e2d\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u7684\u6570\u636e\u6548\u7387\uff0c\u4f46\u5cf0\u503c\u6027\u80fd\u76f8\u5f53\u3002\u5f53\u52a0\u5165\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8f6f\u7ea6\u675f\u540e\uff0c\u4eb2\u548c\u529b\u7684\u6570\u636e\u6548\u7387\u5dee\u8ddd\u51cf\u5c0f\uff0c\u7a33\u5b9a\u6027\u7684\u5dee\u8ddd\u6d88\u9664\uff0c\u5e8f\u5217\u65b9\u6cd5\u80fd\u591f\u5339\u914d\u7ed3\u6784\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7ed3\u6784\u4fe1\u606f\u5bf9\u4e8e\u6297\u4f53\u7684\u5fc5\u8981\u6027\u503c\u5f97\u8d28\u7591\uff0c\u56e0\u4e3a\u901a\u8fc7\u9002\u5f53\u7684\u8f6f\u7ea6\u675f\uff0c\u4ec5\u4f7f\u7528\u5e8f\u5217\u4fe1\u606f\u7684\u65b9\u6cd5\u53ef\u4ee5\u8fbe\u5230\u4e0e\u7ed3\u6784\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2509.24936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24936", "abs": "https://arxiv.org/abs/2509.24936", "authors": ["Angxiao Yue", "Anqi Dong", "Hongteng Xu"], "title": "OAT-FM: Optimal Acceleration Transport for Improved Flow Matching", "comment": null, "summary": "As a powerful technique in generative modeling, Flow Matching (FM) aims to\nlearn velocity fields from noise to data, which is often explained and\nimplemented as solving Optimal Transport (OT) problems. In this study, we\nbridge FM and the recent theory of Optimal Acceleration Transport (OAT),\ndeveloping an improved FM method called OAT-FM and exploring its benefits in\nboth theory and practice. In particular, we demonstrate that the straightening\nobjective hidden in existing OT-based FM methods is mathematically equivalent\nto minimizing the physical action associated with acceleration defined by OAT.\nAccordingly, instead of enforcing constant velocity, OAT-FM optimizes the\nacceleration transport in the product space of sample and velocity, whose\nobjective corresponds to a necessary and sufficient condition of flow\nstraightness. An efficient algorithm is designed to achieve OAT-FM with low\ncomplexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative\nmodel trained by an arbitrary FM method, whose velocity information has been\nrelatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm\neliminates the risk of data distribution drift and the need to generate a large\nnumber of noise data pairs, which consistently improves model performance in\nvarious generative tasks. Code is available at:\nhttps://github.com/AngxiaoYue/OAT-FM", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOAT-FM\u65b9\u6cd5\uff0c\u5c06\u6d41\u5339\u914d\u4e0e\u6700\u4f18\u52a0\u901f\u4f20\u8f93\u7406\u8bba\u7ed3\u5408\uff0c\u901a\u8fc7\u4f18\u5316\u52a0\u901f\u5ea6\u4f20\u8f93\u800c\u975e\u6052\u5b9a\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u66f4\u76f4\u7684\u6d41\u8f68\u8ff9\u548c\u66f4\u597d\u7684\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6d41\u5339\u914d\u65b9\u6cd5\u9690\u542b\u4e86\u76f4\u7ebf\u5316\u76ee\u6807\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u652f\u6491\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u6d41\u5339\u914d\u4e0e\u6700\u4f18\u52a0\u901f\u4f20\u8f93\u7406\u8bba\u7684\u8054\u7cfb\uff0c\u5f00\u53d1\u6539\u8fdb\u7684\u6d41\u5339\u914d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOAT-FM\u65b9\u6cd5\uff0c\u5728\u6837\u672c\u548c\u901f\u5ea6\u7684\u4e58\u79ef\u7a7a\u95f4\u4e2d\u4f18\u5316\u52a0\u901f\u5ea6\u4f20\u8f93\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u8ba1\u7b97\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u5148\u7528\u4efb\u610f\u6d41\u5339\u914d\u65b9\u6cd5\u8bad\u7ec3\uff0c\u518d\u7528OAT-FM\u5fae\u8c03\u3002", "result": "OAT-FM\u5728\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u4e00\u81f4\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u907f\u514d\u4e86\u6570\u636e\u5206\u5e03\u6f02\u79fb\u98ce\u9669\uff0c\u65e0\u9700\u751f\u6210\u5927\u91cf\u566a\u58f0\u6570\u636e\u5bf9\u3002", "conclusion": "OAT-FM\u4e3a\u6d41\u5339\u914d\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u5b9e\u8df5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u52a0\u901f\u4f20\u8f93\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2509.24947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24947", "abs": "https://arxiv.org/abs/2509.24947", "authors": ["Sooraj Sathish", "Keshav Goyal", "Raghuram Bharadwaj Diddigi"], "title": "Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer", "comment": null, "summary": "Deep Reinforcement Learning (RL) has demonstrated success in solving complex\nsequential decision-making problems by integrating neural networks with the RL\nframework. However, training deep RL models poses several challenges, such as\nthe need for extensive hyperparameter tuning and high computational costs.\nTransfer learning has emerged as a promising strategy to address these\nchallenges by enabling the reuse of knowledge from previously learned tasks for\nnew, related tasks. This avoids the need for retraining models entirely from\nscratch. A commonly used approach for transfer learning in RL is to leverage\nthe internal representations learned by the neural network during training.\nSpecifically, the activations from the last hidden layer can be viewed as\nrefined state representations that encapsulate the essential features of the\ninput. In this work, we investigate whether these representations can be used\nas input for training simpler models, such as linear function approximators, on\nnew tasks. We observe that the representations learned by standard deep RL\nmodels can be highly correlated, which limits their effectiveness when used\nwith linear function approximation. To mitigate this problem, we propose a\nnovel deep Q-learning approach that introduces a regularization term to reduce\npositive correlations between feature representation of states. By leveraging\nthese reduced correlated features, we enable more effective use of linear\nfunction approximation in transfer learning. Through experiments and ablation\nstudies on standard RL benchmarks and MinAtar games, we demonstrate the\nefficacy of our approach in improving transfer learning performance and thereby\nreducing computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6Q\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u51cf\u5c11\u72b6\u6001\u7279\u5f81\u8868\u793a\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u6027\uff0c\u4ece\u800c\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u66f4\u6709\u6548\u5730\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9762\u4e34\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\uff0c\u8fc1\u79fb\u5b66\u4e60\u901a\u8fc7\u91cd\u7528\u5148\u524d\u4efb\u52a1\u7684\u77e5\u8bc6\u53ef\u4ee5\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u3002\u4f46\u6807\u51c6\u6df1\u5ea6RL\u6a21\u578b\u5b66\u5230\u7684\u8868\u793a\u53ef\u80fd\u9ad8\u5ea6\u76f8\u5173\uff0c\u9650\u5236\u4e86\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6Q\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f15\u5165\u6b63\u5219\u5316\u9879\u6765\u51cf\u5c11\u72b6\u6001\u7279\u5f81\u8868\u793a\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u6027\uff0c\u5229\u7528\u8fd9\u4e9b\u51cf\u5c11\u76f8\u5173\u6027\u7684\u7279\u5f81\u6765\u66f4\u6709\u6548\u5730\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u5728\u6807\u51c6RL\u57fa\u51c6\u548cMinAtar\u6e38\u620f\u4e0a\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u51cf\u5c11\u7279\u5f81\u8868\u793a\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6548\u679c\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.24957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24957", "abs": "https://arxiv.org/abs/2509.24957", "authors": ["Weifan Jiang", "Rana Shahout", "Yilun Du", "Michael Mitzenmacher", "Minlan Yu"], "title": "Intra-request branch orchestration for efficient LLM reasoning", "comment": "15 pages, 6 figures", "summary": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates.", "AI": {"tldr": "DUCHESS\u662f\u4e00\u4e2aLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u652f\u7f16\u6392\u548c\u590d\u6742\u5ea6\u9884\u6d4b\u6765\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u548c\u4ee4\u724c\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u7b97\u6cd5\uff08\u5982\u601d\u7ef4\u94fe\u548c\u591a\u5206\u652f\u63a8\u7406\uff09\u663e\u8457\u589e\u52a0\u4ee4\u724c\u4f7f\u7528\u548c\u5ef6\u8fdf\uff0c\u800c\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u4f46\u727a\u7272\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u63a2\u6d4b\u6a21\u578b\u9884\u6d4b\u5206\u652f\u6b63\u786e\u6027\uff0c\u7f16\u6392\u7b56\u7565\u51b3\u5b9a\u7ec8\u6b62\u3001\u590d\u5236\u6216\u7ee7\u7eed\u5206\u652f\uff1b\u5728\u591a\u8bf7\u6c42\u65f6\u6839\u636e\u63d0\u793a\u590d\u6742\u5ea6\u4f18\u5148\u5904\u7406\u7b80\u5355\u4efb\u52a1\u3002", "result": "\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDUCHESS\u5c06\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1142-63%\uff0c\u5728vLLM\u670d\u52a1\u4e2d\u5e73\u5747\u5ef6\u8fdf\u51cf\u5c1157-81%\uff0c\u4e2d\u4f4d\u6570\u5ef6\u8fdf\u51cf\u5c1158-85%\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u51cf\u5c1152-84%\u3002", "conclusion": "DUCHESS\u901a\u8fc7\u667a\u80fd\u5206\u652f\u7f16\u6392\u548c\u96be\u5ea6\u611f\u77e5\u8c03\u5ea6\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u4e86LLM\u63a8\u7406\u7684\u4ee4\u724c\u4f7f\u7528\u548c\u5ef6\u8fdf\u3002"}}
{"id": "2509.24962", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24962", "abs": "https://arxiv.org/abs/2509.24962", "authors": ["Valentyn Melnychuk", "Dennis Frauen", "Jonas Schweisthal", "Stefan Feuerriegel"], "title": "Overlap-Adaptive Regularization for Conditional Average Treatment Effect Estimation", "comment": null, "summary": "The conditional average treatment effect (CATE) is widely used in\npersonalized medicine to inform therapeutic decisions. However,\nstate-of-the-art methods for CATE estimation (so-called meta-learners) often\nperform poorly in the presence of low overlap. In this work, we introduce a new\napproach to tackle this issue and improve the performance of existing\nmeta-learners in the low-overlap regions. Specifically, we introduce\nOverlap-Adaptive Regularization (OAR) that regularizes target models\nproportionally to overlap weights so that, informally, the regularization is\nhigher in regions with low overlap. To the best of our knowledge, our OAR is\nthe first approach to leverage overlap weights in the regularization terms of\nthe meta-learners. Our OAR approach is flexible and works with any existing\nCATE meta-learner: we demonstrate how OAR can be applied to both parametric and\nnon-parametric second-stage models. Furthermore, we propose debiased versions\nof our OAR that preserve the Neyman-orthogonality of existing meta-learners and\nthus ensure more robust inference. Through a series of (semi-)synthetic\nexperiments, we demonstrate that our OAR significantly improves CATE estimation\nin low-overlap settings in comparison to constant regularization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cd\u53e0\u81ea\u9002\u5e94\u6b63\u5219\u5316(OAR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u91cd\u53e0\u6743\u91cd\u6bd4\u4f8b\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u5728\u4f4e\u91cd\u53e0\u533a\u57df\u63d0\u9ad8CATE\u4f30\u8ba1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u73b0\u6709\u5143\u5b66\u4e60\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u4fdd\u6301Neyman\u6b63\u4ea4\u6027\u7684\u53bb\u504f\u7248\u672c\u3002", "motivation": "\u73b0\u6709CATE\u4f30\u8ba1\u65b9\u6cd5\u5728\u4f4e\u91cd\u53e0\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u5143\u5b66\u4e60\u5668\u5728\u4f4e\u91cd\u53e0\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u91cd\u53e0\u81ea\u9002\u5e94\u6b63\u5219\u5316(OAR)\uff0c\u6839\u636e\u91cd\u53e0\u6743\u91cd\u6bd4\u4f8b\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u5728\u4f4e\u91cd\u53e0\u533a\u57df\u65bd\u52a0\u66f4\u5f3a\u7684\u6b63\u5219\u5316\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u53c2\u6570\u548c\u975e\u53c2\u6570\u4e8c\u9636\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u53bb\u504f\u7248\u672c\u4ee5\u4fdd\u6301Neyman\u6b63\u4ea4\u6027\u3002", "result": "\u901a\u8fc7(\u534a)\u5408\u6210\u5b9e\u9a8c\u8bc1\u660e\uff0cOAR\u76f8\u6bd4\u6052\u5b9a\u6b63\u5219\u5316\u5728\u4f4e\u91cd\u53e0\u8bbe\u7f6e\u4e0b\u663e\u8457\u6539\u5584\u4e86CATE\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "OAR\u662f\u9996\u4e2a\u5728\u5143\u5b66\u4e60\u5668\u6b63\u5219\u5316\u9879\u4e2d\u5229\u7528\u91cd\u53e0\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u91cd\u53e0\u533a\u57df\u7684CATE\u4f30\u8ba1\u6548\u679c\u3002"}}
{"id": "2509.24974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24974", "abs": "https://arxiv.org/abs/2509.24974", "authors": ["Ahmad Fraij", "Sam Dauncey"], "title": "Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models", "comment": null, "summary": "Data scarcity drives the need for more sample-efficient large language\nmodels. In this work, we use the double descent phenomenon to holistically\ncompare the sample efficiency of discrete diffusion and autoregressive models.\nWe show that discrete diffusion models require larger capacity and more\ntraining epochs to escape their underparameterized regime and reach the\ninterpolation threshold. In the strongly overparameterized regime, both models\nexhibit similar behavior, with neither exhibiting a pronounced second descent\nin test loss across a large range of model sizes. Overall, our results indicate\nthat autoregressive models are more sample-efficient on small-scale datasets,\nwhile discrete diffusion models only become competitive when given sufficient\ncapacity and compute.", "AI": {"tldr": "\u6bd4\u8f83\u79bb\u6563\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u66f4\u9ad8\u6548\uff0c\u800c\u79bb\u6563\u6269\u6563\u6a21\u578b\u9700\u8981\u66f4\u5927\u5bb9\u91cf\u548c\u8ba1\u7b97\u624d\u80fd\u8fbe\u5230\u7ade\u4e89\u529b", "motivation": "\u6570\u636e\u7a00\u7f3a\u6027\u63a8\u52a8\u4e86\u5bf9\u66f4\u9ad8\u6548\u6837\u672c\u5229\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9700\u6c42\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u6837\u672c\u6548\u7387", "method": "\u5229\u7528\u53cc\u4e0b\u964d\u73b0\u8c61\u5168\u9762\u6bd4\u8f83\u79bb\u6563\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\uff0c\u5206\u6790\u5b83\u4eec\u5728\u6b20\u53c2\u6570\u5316\u548c\u8fc7\u53c2\u6570\u5316\u9636\u6bb5\u7684\u8868\u73b0", "result": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u9700\u8981\u66f4\u5927\u5bb9\u91cf\u548c\u66f4\u591a\u8bad\u7ec3\u8f6e\u6b21\u624d\u80fd\u8fbe\u5230\u63d2\u503c\u9608\u503c\uff1b\u5728\u5f3a\u8fc7\u53c2\u6570\u5316\u9636\u6bb5\uff0c\u4e24\u79cd\u6a21\u578b\u8868\u73b0\u76f8\u4f3c\uff0c\u90fd\u6ca1\u6709\u660e\u663e\u7684\u6d4b\u8bd5\u635f\u5931\u7b2c\u4e8c\u4e0b\u964d\uff1b\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u66f4\u6837\u672c\u9ad8\u6548", "conclusion": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u66f4\u6837\u672c\u9ad8\u6548\uff0c\u79bb\u6563\u6269\u6563\u6a21\u578b\u53ea\u6709\u5728\u83b7\u5f97\u8db3\u591f\u5bb9\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u65f6\u624d\u80fd\u5177\u6709\u7ade\u4e89\u529b"}}
{"id": "2509.24981", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24981", "abs": "https://arxiv.org/abs/2509.24981", "authors": ["Haoran He", "Yuxiao Ye", "Qingpeng Cai", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Ling Pan"], "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards", "comment": "32 pages", "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods.", "AI": {"tldr": "\u63d0\u51faROVER\u7b97\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u7b56\u7565Q\u503c\u8bc4\u4f30\u7684\u7b80\u5316RL\u65b9\u6cd5\uff0c\u7528\u4e8e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u590d\u6742\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56PPO\u7b49\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u590d\u6742\u542f\u53d1\u5f0f\u6280\u5de7\u3002\u4f5c\u8005\u53d1\u73b0\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u5177\u6709\u7279\u6b8a\u7684MDP\u7ed3\u6784\uff0c\u53ef\u4ee5\u7b80\u5316\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u63d0\u51faROVER\u7b97\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u5747\u5300\u968f\u673a\u7b56\u7565\u7684Q\u51fd\u6570\u6062\u590d\u6700\u4f18\u52a8\u4f5c\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7684\u7b56\u7565\u8fed\u4ee3\u5faa\u73af\uff0c\u4f7f\u7528\u57fa\u4e8e\u8fd9\u4e9bQ\u503c\u7684softmax\u91c7\u6837\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROVER\u5728\u8d28\u91cf\uff08pass@1 +8.2\uff0cpass@256 +16.8\uff09\u548c\u591a\u6837\u6027\uff08+17.6%\uff09\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ROVER\u8bc1\u660e\u4e86\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u53ef\u4ee5\u5927\u5e45\u7b80\u5316RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u7b56\u7565\u8bc4\u4f30\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u591a\u6837\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.24991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24991", "abs": "https://arxiv.org/abs/2509.24991", "authors": ["Lu Zou", "Wendi Ren", "Weizhong Zhang", "Liang Ding", "Shuang Li"], "title": "Sampling Complexity of TD and PPO in RKHS", "comment": null, "summary": "We revisit Proximal Policy Optimization (PPO) from a function-space\nperspective. Our analysis decouples policy evaluation and improvement in a\nreproducing kernel Hilbert space (RKHS): (i) A kernelized temporal-difference\n(TD) critic performs efficient RKHS-gradient updates using only one-step\nstate-action transition samples; (ii) a KL-regularized, natural-gradient policy\nstep exponentiates the evaluated action-value, recovering a PPO/TRPO-style\nproximal update in continuous state-action spaces. We provide non-asymptotic,\ninstance-adaptive guarantees whose rates depend on RKHS entropy, unifying\ntabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes,\nand we derive a sampling rule for the proximal update that ensures the optimal\n$k^{-1/2}$ convergence rate for stochastic optimization. Empirically, the\ntheory-aligned schedule improves stability and sample efficiency on common\ncontrol tasks (e.g., CartPole, Acrobot), while our TD-based critic attains\nfavorable throughput versus a GAE baseline. Altogether, our results place PPO\non a firmer theoretical footing beyond finite-dimensional assumptions and\nclarify when RKHS-proximal updates with kernel-TD critics yield global policy\nimprovement with practical efficiency.", "AI": {"tldr": "\u4ece\u51fd\u6570\u7a7a\u95f4\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6PPO\u7b97\u6cd5\uff0c\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4(RKHS)\u4e2d\u89e3\u8026\u7b56\u7565\u8bc4\u4f30\u548c\u6539\u8fdb\uff0c\u63d0\u51fa\u6838\u5316TD\u8bc4\u8bba\u5bb6\u548cKL\u6b63\u5219\u5316\u81ea\u7136\u68af\u5ea6\u7b56\u7565\u66f4\u65b0\uff0c\u83b7\u5f97\u975e\u6e10\u8fd1\u7406\u8bba\u4fdd\u8bc1\u548c\u6700\u4f18\u6536\u655b\u901f\u7387\u3002", "motivation": "\u4e3aPPO\u7b97\u6cd5\u63d0\u4f9b\u8d85\u8d8a\u6709\u9650\u7ef4\u5047\u8bbe\u7684\u66f4\u575a\u5b9e\u7406\u8bba\u57fa\u7840\uff0c\u9610\u660eRKHS\u8fd1\u7aef\u66f4\u65b0\u4e0e\u6838TD\u8bc4\u8bba\u5bb6\u4f55\u65f6\u80fd\u5b9e\u73b0\u5168\u5c40\u7b56\u7565\u6539\u8fdb\u5e76\u4fdd\u6301\u5b9e\u9645\u6548\u7387\u3002", "method": "\u5728RKHS\u4e2d\u89e3\u8026\u7b56\u7565\u8bc4\u4f30\u548c\u6539\u8fdb\uff1a(1)\u4f7f\u7528\u6838\u5316TD\u8bc4\u8bba\u5bb6\u8fdb\u884c\u9ad8\u6548RKHS\u68af\u5ea6\u66f4\u65b0\uff1b(2)KL\u6b63\u5219\u5316\u81ea\u7136\u68af\u5ea6\u7b56\u7565\u6b65\u957f\uff0c\u6062\u590dPPO/TRPO\u98ce\u683c\u7684\u8fd1\u7aef\u66f4\u65b0\u3002", "result": "\u83b7\u5f97\u975e\u6e10\u8fd1\u3001\u5b9e\u4f8b\u81ea\u9002\u5e94\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u6536\u655b\u901f\u7387\u4f9d\u8d56\u4e8eRKHS\u71b5\uff1b\u7ecf\u9a8c\u8bc1\u660e\u7406\u8bba\u5bf9\u9f50\u7684\u8c03\u5ea6\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\uff0cTD\u8bc4\u8bba\u5bb6\u5728\u541e\u5410\u91cf\u4e0a\u4f18\u4e8eGAE\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aPPO\u5efa\u7acb\u4e86\u66f4\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u9610\u660e\u4e86RKHS\u8fd1\u7aef\u66f4\u65b0\u4e0e\u6838TD\u8bc4\u8bba\u5bb6\u5728\u5168\u5c40\u7b56\u7565\u6539\u8fdb\u548c\u5b9e\u9645\u6548\u7387\u65b9\u9762\u7684\u9002\u7528\u6761\u4ef6\u3002"}}
{"id": "2509.25003", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25003", "abs": "https://arxiv.org/abs/2509.25003", "authors": ["Mingxing Rao", "Bowen Qu", "Daniel Moyer"], "title": "Score-based Membership Inference on Diffusion Models", "comment": null, "summary": "Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess", "AI": {"tldr": "\u63d0\u51faSimA\u5355\u67e5\u8be2\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u9884\u6d4b\u566a\u58f0\u5411\u91cf\u7684\u8303\u6570\u6765\u68c0\u6d4b\u8bad\u7ec3\u6837\u672c\u6210\u5458\u8eab\u4efd\uff0c\u53d1\u73b0\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6bd4\u50cf\u7d20\u7a7a\u95f4\u6a21\u578b\u66f4\u5b89\u5168", "motivation": "\u6269\u6563\u6a21\u578b\u53ef\u80fd\u65e0\u610f\u4e2d\u6cc4\u9732\u8bad\u7ec3\u6837\u672c\u7684\u6210\u5458\u4fe1\u606f\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u7814\u7a76\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6548\u679c", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u9884\u6d4b\u566a\u58f0\u5411\u91cf\u6307\u5411\u8bad\u7ec3\u6837\u672c\u5c40\u90e8\u5747\u503c\u7684\u7406\u8bba\u89c2\u5bdf\uff0c\u63d0\u51faSimA\u5355\u67e5\u8be2\u653b\u51fb\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u6269\u6563\u6a21\u578b\u53d8\u4f53\u7684\u5b89\u5168\u6027\u5dee\u5f02", "result": "SimA\u5728DDPM\u548cLDM\u7b49\u6a21\u578b\u4e0a\u8868\u73b0\u4e00\u81f4\u5f3a\u52b2\uff0c\u53d1\u73b0\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7531\u4e8eVAE\u7684\u4fe1\u606f\u74f6\u9888\u800c\u66f4\u5b89\u5168\uff0c\u901a\u8fc7\u8c03\u6574\u03b2-VAE\u8d85\u53c2\u6570\u53ef\u589e\u5f3aLDM\u8bad\u7ec3\u5bf9MIA\u7684\u9c81\u68d2\u6027", "conclusion": "\u5de9\u56fa\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u7406\u8bba\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u597d\u7406\u89e3VAE\u53cd\u6f14\u800c\u975e\u4ec5\u6269\u6563\u8fc7\u7a0b\u53cd\u6f14\uff0c\u4e3a\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3"}}
{"id": "2509.25017", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25017", "abs": "https://arxiv.org/abs/2509.25017", "authors": ["Spyros Kondylatos", "Gustau Camps-Valls", "Ioannis Papoutsis"], "title": "Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting", "comment": null, "summary": "Wildfires are among the most severe natural hazards, posing a significant\nthreat to both humans and natural ecosystems. The growing risk of wildfires\nincreases the demand for forecasting models that are not only accurate but also\nreliable. Deep Learning (DL) has shown promise in predicting wildfire danger;\nhowever, its adoption is hindered by concerns over the reliability of its\npredictions, some of which stem from the lack of uncertainty quantification. To\naddress this challenge, we present an uncertainty-aware DL framework that\njointly captures epistemic (model) and aleatoric (data) uncertainty to enhance\nshort-term wildfire danger forecasting. In the next-day forecasting, our\nbest-performing model improves the F1 Score by 2.3% and reduces the Expected\nCalibration Error by 2.1% compared to a deterministic baseline, enhancing both\npredictive skill and calibration. Our experiments confirm the reliability of\nthe uncertainty estimates and illustrate their practical utility for decision\nsupport, including the identification of uncertainty thresholds for rejecting\nlow-confidence predictions and the generation of well-calibrated wildfire\ndanger maps with accompanying uncertainty layers. Extending the forecast\nhorizon up to ten days, we observe that aleatoric uncertainty increases with\ntime, showing greater variability in environmental conditions, while epistemic\nuncertainty remains stable. Finally, we show that although the two uncertainty\ntypes may be redundant in low-uncertainty cases, they provide complementary\ninsights under more challenging conditions, underscoring the value of their\njoint modeling for robust wildfire danger prediction. In summary, our approach\nsignificantly improves the accuracy and reliability of wildfire danger\nforecasting, advancing the development of trustworthy wildfire DL systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u5347\u77ed\u671f\u91ce\u706b\u5371\u9669\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u91ce\u706b\u662f\u4e25\u91cd\u7684\u81ea\u7136\u707e\u5bb3\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u91ce\u706b\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5176\u53ef\u9760\u6027\u53d7\u5230\u8d28\u7591\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u6355\u6349\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff08\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff09\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff08\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff09\uff0c\u7528\u4e8e\u77ed\u671f\u91ce\u706b\u5371\u9669\u9884\u6d4b\u3002", "result": "\u5728\u6b21\u65e5\u9884\u6d4b\u4e2d\uff0c\u6700\u4f73\u6a21\u578b\u76f8\u6bd4\u786e\u5b9a\u6027\u57fa\u7ebf\u5c06F1\u5206\u6570\u63d0\u9ad8\u4e862.3%\uff0c\u9884\u671f\u6821\u51c6\u8bef\u5dee\u964d\u4f4e\u4e862.1%\u3002\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u9760\uff0c\u53ef\u7528\u4e8e\u51b3\u7b56\u652f\u6301\uff0c\u5305\u62ec\u8bc6\u522b\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u62d2\u7edd\u9608\u503c\u548c\u751f\u6210\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u5c42\u7684\u6821\u51c6\u91ce\u706b\u5371\u9669\u5730\u56fe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u91ce\u706b\u5371\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u63a8\u8fdb\u4e86\u53ef\u4fe1\u8d56\u91ce\u706b\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u7c7b\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u63d0\u4f9b\u4e92\u8865\u89c1\u89e3\u3002"}}
{"id": "2509.25020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25020", "abs": "https://arxiv.org/abs/2509.25020", "authors": ["Jiayu Liu", "Zhenya Huang", "Anya Sims", "Enhong Chen", "Yee Whye Teh", "Ning Miao"], "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts", "comment": null, "summary": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs.", "AI": {"tldr": "MARCOS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u63a8\u7406\u8303\u5f0f\uff0c\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u8fde\u7eed\u9ad8\u7ef4\u601d\u60f3\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684token\u5e8f\u5217\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u7075\u6d3b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u63a8\u7406\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53d7\u9650\u4e8e\u79bb\u6563token\u7a7a\u95f4\u3001\u63a8\u7406\u4e0etoken\u751f\u6210\u8026\u5408\u5bfc\u81f4\u77ed\u89c6\u63a8\u7406\u3002", "method": "\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u8fde\u7eed\u9ad8\u7ef4\u601d\u60f3\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u53d8\u5206\u8bad\u7ec3\u65b9\u6848\uff0c\u8ba9\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u4f5c\u4e3a\u9690\u5f0f\u601d\u60f3\u7684\u89c2\u5bdf\u7a97\u53e3\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMARCOS\u6027\u80fd\u8fbe\u5230token-based CoT\u6c34\u5e73\uff0c\u5728GSM8K\u4e0a\u751a\u81f3\u8d85\u8d8a4.7%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe15.7\u500d\u3002", "conclusion": "MARCOS\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8fde\u7eed\u63a8\u7406\u8303\u5f0f\uff0c\u652f\u6301\u6b65\u9aa4\u7ea7\u63a7\u5236\u968f\u673a\u6027\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u5f00\u8f9f\u4e86\u91cd\u8981\u673a\u4f1a\u3002"}}
{"id": "2509.25031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25031", "abs": "https://arxiv.org/abs/2509.25031", "authors": ["Sophia V. Kuhn", "Rafael Bischof", "Marius Weber", "Antoine Binggeli", "Michael A. Kraus", "Walter Kaufmann", "Fernando P\u00e9rez-Cruz"], "title": "Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios", "comment": "Accepted at the NeurIPS 2025 Workshop on MLxOR: Mathematical\n  Foundations and Operational Integration of Machine Learning for\n  Uncertainty-Aware Decision-Making", "summary": "Aging infrastructure portfolios pose a critical resource allocation\nchallenge: deciding which structures require intervention and which can safely\nremain in service. Structural assessments must balance the trade-off between\ncheaper, conservative analysis methods and accurate but costly simulations that\ndo not scale portfolio-wide. We propose Bayesian neural network (BNN)\nsurrogates for rapid structural pre-assessment of worldwide common bridge\ntypes, such as reinforced concrete frame bridges. Trained on a large-scale\ndatabase of non-linear finite element analyses generated via a parametric\npipeline and developed based on the Swiss Federal Railway's bridge portfolio,\nthe models accurately and efficiently estimate high-fidelity structural\nanalysis results by predicting code compliance factors with calibrated\nepistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware\ntriage: flagging likely critical structures and providing guidance where\nrefined analysis is pertinent. We demonstrate the framework's effectiveness in\na real-world case study of a railway underpass, showing its potential to\nsignificantly reduce costs and emissions by avoiding unnecessary analyses and\nphysical interventions across entire infrastructure portfolios.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u5feb\u901f\u8bc4\u4f30\u6865\u6881\u7ed3\u6784\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u9884\u6d4b\u89c4\u8303\u7b26\u5408\u6027\u56e0\u5b50\u6765\u8bc6\u522b\u9700\u8981\u5e72\u9884\u7684\u5173\u952e\u7ed3\u6784\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u5206\u6790\u548c\u7269\u7406\u5e72\u9884\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u8001\u5316\u57fa\u7840\u8bbe\u65bd\u7ec4\u5408\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u5173\u952e\u6311\u6218\uff1a\u5e73\u8861\u5ec9\u4ef7\u4fdd\u5b88\u5206\u6790\u65b9\u6cd5\u4e0e\u51c6\u786e\u4f46\u6602\u8d35\u6a21\u62df\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5b9e\u73b0\u7ec4\u5408\u8303\u56f4\u5185\u7684\u5feb\u901f\u7ed3\u6784\u9884\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u6709\u9650\u5143\u5206\u6790\u6570\u636e\u5e93\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u7ba1\u9053\u751f\u6210\u6570\u636e\uff0c\u5f00\u53d1\u57fa\u4e8e\u745e\u58eb\u8054\u90a6\u94c1\u8def\u6865\u6881\u7ec4\u5408\u7684\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u5730\u4f30\u8ba1\u9ad8\u4fdd\u771f\u7ed3\u6784\u5206\u6790\u7ed3\u679c\uff0c\u9884\u6d4b\u89c4\u8303\u7b26\u5408\u6027\u56e0\u5b50\u5e76\u6821\u51c6\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u94c1\u8def\u4e0b\u7a7f\u901a\u9053\u7684\u5b9e\u9645\u6848\u4f8b\u4e2d\u8bc1\u660e\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u6392\u653e\uff0c\u901a\u8fc7\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5206\u6790\u548c\u7269\u7406\u5e72\u9884\uff0c\u4e3a\u6574\u4e2a\u57fa\u7840\u8bbe\u65bd\u7ec4\u5408\u63d0\u4f9b\u5feb\u901f\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5206\u7c7b\u7b5b\u9009\u3002"}}
{"id": "2509.25040", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25040", "abs": "https://arxiv.org/abs/2509.25040", "authors": ["Giuseppe Bruno", "Federico Pasqualotto", "Andrea Agazzi"], "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime", "comment": "30 pages, 4 figures", "summary": "In this paper, we study the evolution of tokens through the depth of\nencoder-only transformer models at inference time by modeling them as a system\nof particles interacting in a mean-field way and studying the corresponding\ndynamics. More specifically, we consider this problem in the moderate\ninteraction regime, where the number $N$ of tokens is large and the inverse\ntemperature parameter $\\beta$ of the model scales together with $N$. In this\nregime, the dynamics of the system displays a multiscale behavior: a fast\nphase, where the token empirical measure collapses on a low-dimensional space,\nan intermediate phase, where the measure further collapses into clusters, and a\nslow one, where such clusters sequentially merge into a single one. We provide\na rigorous characterization of the limiting dynamics in each of these phases\nand prove convergence in the above mentioned limit, exemplifying our results\nwith some simulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u7f16\u7801\u5668transformer\u6a21\u578b\u4e2d\u7684token\u6f14\u5316\u5efa\u6a21\u4e3a\u5e73\u5747\u573a\u76f8\u4e92\u4f5c\u7528\u7c92\u5b50\u7cfb\u7edf\uff0c\u5728\u4e2d\u7b49\u76f8\u4e92\u4f5c\u7528\u673a\u5236\u4e0b\u7814\u7a76\u5176\u591a\u5c3a\u5ea6\u52a8\u6001\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76transformer\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2dtoken\u968f\u7f51\u7edc\u6df1\u5ea6\u6f14\u5316\u7684\u52a8\u6001\u7279\u6027\uff0c\u901a\u8fc7\u7269\u7406\u7cfb\u7edf\u7c7b\u6bd4\u6765\u7406\u89e3\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u5c06token\u5efa\u6a21\u4e3a\u5e73\u5747\u573a\u76f8\u4e92\u4f5c\u7528\u7c92\u5b50\u7cfb\u7edf\uff0c\u5728N\u4e2atoken\u6570\u91cf\u5927\u4e14\u9006\u6e29\u5ea6\u53c2\u6570\u03b2\u4e0eN\u5171\u540c\u7f29\u653e\u7684\u673a\u5236\u4e0b\uff0c\u5206\u6790\u7cfb\u7edf\u7684\u591a\u5c3a\u5ea6\u52a8\u6001\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u52a8\u6001\u5448\u73b0\u4e09\u4e2a\u5c3a\u5ea6\uff1a\u5feb\u901f\u9636\u6bb5token\u7ecf\u9a8c\u6d4b\u5ea6\u574d\u7f29\u5230\u4f4e\u7ef4\u7a7a\u95f4\uff0c\u4e2d\u95f4\u9636\u6bb5\u8fdb\u4e00\u6b65\u574d\u7f29\u6210\u7c07\uff0c\u6162\u901f\u9636\u6bb5\u7c07\u987a\u5e8f\u5408\u5e76\u6210\u5355\u4e00\u7c07\u3002", "conclusion": "\u4e25\u683c\u523b\u753b\u4e86\u6bcf\u4e2a\u9636\u6bb5\u7684\u6781\u9650\u52a8\u6001\uff0c\u8bc1\u660e\u4e86\u4e0a\u8ff0\u6781\u9650\u4e0b\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002"}}
{"id": "2509.25049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25049", "abs": "https://arxiv.org/abs/2509.25049", "authors": ["Bingrui Li", "Jiaxin Wen", "Zhanpeng Zhou", "Jun Zhu", "Jianfei Chen"], "title": "Efficient Hyperparameter Tuning via Trajectory Invariance Principle", "comment": null, "summary": "As hyperparameter tuning becomes increasingly costly at scale, efficient\ntuning methods are essential. Yet principles for guiding hyperparameter tuning\nremain limited. In this work, we seek to establish such principles by\nconsidering a broad range of hyperparameters, including batch size, learning\nrate, and weight decay. We identify a phenomenon we call trajectory invariance,\nwhere pre-training loss curves, gradient noise, and gradient norm exhibit\ninvariance--closely overlapping--with respect to a quantity that combines\nlearning rate and weight decay. This phenomenon effectively reduces the\noriginal two-dimensional hyperparameter space to one dimension, yielding an\nefficient tuning rule: follow the salient direction revealed by trajectory\ninvariance. Furthermore, we refine previous scaling laws and challenge several\nexisting viewpoints. Overall, our work proposes new principles for efficient\ntuning and inspires future research on scaling laws.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u8f68\u8ff9\u4e0d\u53d8\u6027\u73b0\u8c61\uff0c\u5c06\u5b66\u4e60\u7387\u548c\u6743\u91cd\u8870\u51cf\u4e24\u4e2a\u8d85\u53c2\u6570\u5408\u5e76\u4e3a\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u8c03\u4f18\u89c4\u5219\uff1a\u6cbf\u7740\u8f68\u8ff9\u4e0d\u53d8\u6027\u63ed\u793a\u7684\u663e\u8457\u65b9\u5411\u8fdb\u884c\u8c03\u4f18\u3002", "motivation": "\u968f\u7740\u8d85\u53c2\u6570\u8c03\u4f18\u5728\u89c4\u6a21\u5316\u5e94\u7528\u4e2d\u6210\u672c\u8d8a\u6765\u8d8a\u9ad8\uff0c\u9700\u8981\u5efa\u7acb\u6307\u5bfc\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u539f\u5219\uff0c\u4f46\u73b0\u6709\u539f\u5219\u6709\u9650\u3002", "method": "\u7814\u7a76\u591a\u79cd\u8d85\u53c2\u6570\uff08\u6279\u91cf\u5927\u5c0f\u3001\u5b66\u4e60\u7387\u3001\u6743\u91cd\u8870\u51cf\uff09\uff0c\u8bc6\u522b\u8f68\u8ff9\u4e0d\u53d8\u6027\u73b0\u8c61\uff0c\u5373\u9884\u8bad\u7ec3\u635f\u5931\u66f2\u7ebf\u3001\u68af\u5ea6\u566a\u58f0\u548c\u68af\u5ea6\u8303\u6570\u5728\u7ed3\u5408\u5b66\u4e60\u7387\u548c\u6743\u91cd\u8870\u51cf\u7684\u67d0\u4e2a\u91cf\u4e0a\u8868\u73b0\u51fa\u4e0d\u53d8\u6027\u3002", "result": "\u8f68\u8ff9\u4e0d\u53d8\u6027\u5c06\u539f\u59cb\u4e8c\u7ef4\u8d85\u53c2\u6570\u7a7a\u95f4\u964d\u4e3a\u4e00\u7ef4\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u8c03\u4f18\u89c4\u5219\uff0c\u5e76\u6539\u8fdb\u4e86\u4e4b\u524d\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u6311\u6218\u4e86\u591a\u4e2a\u73b0\u6709\u89c2\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u65b0\u7684\u9ad8\u6548\u8c03\u4f18\u539f\u5219\uff0c\u5e76\u4e3a\u672a\u6765\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2509.25050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25050", "abs": "https://arxiv.org/abs/2509.25050", "authors": ["Shuchen Xue", "Chongjian Ge", "Shilong Zhang", "Yichen Li", "Zhi-Ming Ma"], "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86Advantage Weighted Matching (AWM)\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\uff0c\u901a\u8fc7\u4f18\u52bf\u52a0\u6743\u964d\u4f4e\u65b9\u5dee\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8624\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DDPO\uff09\u4f7f\u7528\u4e0e\u9884\u8bad\u7ec3\u4e0d\u540c\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5bfc\u81f4\u65b9\u5dee\u589e\u52a0\u548c\u6536\u655b\u7f13\u6162\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u3002", "method": "AWM\u4f7f\u7528\u4e0e\u9884\u8bad\u7ec3\u76f8\u540c\u7684score/flow-matching\u635f\u5931\uff0c\u901a\u8fc7\u4f18\u52bf\u51fd\u6570\u5bf9\u6837\u672c\u8fdb\u884c\u52a0\u6743\uff0c\u63d0\u5347\u9ad8\u5956\u52b1\u6837\u672c\u7684\u5f71\u54cd\uff0c\u6291\u5236\u4f4e\u5956\u52b1\u6837\u672c\u3002", "result": "\u5728GenEval\u3001OCR\u548cPickScore\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAWM\u76f8\u6bd4Flow-GRPO\u5b9e\u73b0\u4e86\u9ad8\u8fbe24\u500d\u7684\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "AWM\u901a\u8fc7\u7edf\u4e00\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.25080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25080", "abs": "https://arxiv.org/abs/2509.25080", "authors": ["Bogdan Raoni\u0107", "Siddhartha Mishra", "Samuel Lanthaler"], "title": "Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific AI", "comment": null, "summary": "Data-driven models are increasingly adopted in critical scientific fields\nlike weather forecasting and fluid dynamics. These methods can fail on\nout-of-distribution (OOD) data, but detecting such failures in regression tasks\nis an open challenge. We propose a new OOD detection method based on estimating\njoint likelihoods using a score-based diffusion model. This approach considers\nnot just the input but also the regression model's prediction, providing a\ntask-aware reliability score. Across numerous scientific datasets, including\nPDE datasets, satellite imagery and brain tumor segmentation, we show that this\nlikelihood strongly correlates with prediction error. Our work provides a\nfoundational step towards building a verifiable 'certificate of trust', thereby\noffering a practical tool for assessing the trustworthiness of AI-based\nscientific predictions. Our code is publicly available at\nhttps://github.com/bogdanraonic3/OOD_Detection_ScientificML", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u8054\u5408\u4f3c\u7136\u7684\u65b0OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u8f93\u5165\u548c\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u6765\u63d0\u4f9b\u4efb\u52a1\u611f\u77e5\u7684\u53ef\u9760\u6027\u8bc4\u5206\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u5173\u952e\u79d1\u5b66\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u53ef\u80fd\u5931\u8d25\uff0c\u800c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684OOD\u68c0\u6d4b\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u8054\u5408\u4f3c\u7136\uff0c\u540c\u65f6\u8003\u8651\u8f93\u5165\u548c\u56de\u5f52\u6a21\u578b\u9884\u6d4b\uff0c\u63d0\u4f9b\u4efb\u52a1\u611f\u77e5\u7684\u53ef\u9760\u6027\u8bc4\u5206\u3002", "result": "\u5728\u591a\u4e2a\u79d1\u5b66\u6570\u636e\u96c6\uff08\u5305\u62ecPDE\u6570\u636e\u96c6\u3001\u536b\u661f\u56fe\u50cf\u548c\u8111\u80bf\u7624\u5206\u5272\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f30\u8ba1\u7684\u4f3c\u7136\u4e0e\u9884\u6d4b\u8bef\u5dee\u5f3a\u76f8\u5173\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684'\u4fe1\u4efb\u8bc1\u4e66'\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u8bc4\u4f30\u57fa\u4e8eAI\u7684\u79d1\u5b66\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.25087", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25087", "abs": "https://arxiv.org/abs/2509.25087", "authors": ["Shane Bergsma", "Bin Claire Zhang", "Nolan Dey", "Shaheer Muhammad", "Gurpreet Gosal", "Joel Hestness"], "title": "Scaling with Collapse: Efficient and Predictable Training of LLM Families", "comment": null, "summary": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u4f18\u5316\u8d85\u53c2\u6570\u8bbe\u7f6e\u6700\u4f18\u65f6\uff0c\u4e0d\u540c\u89c4\u6a21\u7684LLM\u8bad\u7ec3\u635f\u5931\u66f2\u7ebf\u53ef\u4ee5\u574d\u7f29\u5230\u7edf\u4e00\u8f68\u8ff9\uff0c\u8fd9\u6210\u4e3a\u8ba1\u7b97\u9ad8\u6548\u8bad\u7ec3\u7684\u6807\u5fd7\uff0c\u5e76\u5c55\u793a\u4e86\u4e24\u4e2a\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u5728\u5b9e\u7528\u6269\u5c55\u914d\u65b9\u4e0b\uff0cLLM\u8bad\u7ec3\u635f\u5931\u66f2\u7ebf\u7684\u574d\u7f29\u73b0\u8c61\u662f\u5426\u6210\u7acb\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u8fd9\u79cd\u73b0\u8c61\u6765\u6539\u8fdb\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u89c4\u6a21LLM\u5728\u6700\u4f18\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u7684\u8bad\u7ec3\u635f\u5931\u66f2\u7ebf\uff0c\u9a8c\u8bc1\u574d\u7f29\u73b0\u8c61\uff0c\u5e76\u5e94\u7528\u4e8e\u8bad\u7ec3\u75c5\u7406\u8bca\u65ad\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u53d1\u73b0\u5f53\u4f18\u5316\u8d85\u53c2\u6570\u6839\u636e\u6570\u636e\u9884\u7b97\u6700\u4f18\u8bbe\u7f6e\u65f6\uff0c\u635f\u5931\u66f2\u7ebf\u786e\u5b9e\u4f1a\u574d\u7f29\uff0c\u8fd9\u53ef\u4ee5\u4f5c\u4e3a\u8ba1\u7b97\u9ad8\u6548\u8bad\u7ec3\u7684\u6807\u5fd7\u3002", "conclusion": "\u635f\u5931\u66f2\u7ebf\u574d\u7f29\u73b0\u8c61\u662f\u5f00\u53d1\u9ad8\u6548LLM\u7684\u6709\u6548\u5de5\u5177\uff0c\u901a\u8fc7\u8bad\u7ec3Celerity LLM\u5bb6\u65cf\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.25100", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25100", "abs": "https://arxiv.org/abs/2509.25100", "authors": ["Aasheesh Singh", "Vishal Vaddina", "Dagnachew Birru"], "title": "ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation", "comment": "Accepted at NeurIPS 2025, Efficient Reasoning Workshop", "summary": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.", "AI": {"tldr": "ORPO-Distill\u662f\u4e00\u79cd\u8de8\u67b6\u6784LLM\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u504f\u597d\u4f18\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6559\u5e08\u548c\u5b66\u751f\u63a8\u7406\u8f68\u8ff9\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u4f20\u7edfCoT\u84b8\u998f\u65b9\u6cd5\u5728\u77e5\u8bc6\u8fc1\u79fb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8de8\u67b6\u6784\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528Odds-Ratio\u504f\u597d\u4f18\u5316\u76ee\u6807\u5bf9\u6bd4\u6559\u5e08\u548c\u5b66\u751f\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u7b56\u7565\u5229\u7528\u5b66\u751f\u751f\u6210\u7684\u8f93\u51fa\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u591a\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u9ed1\u76d2\u77e5\u8bc6\u84b8\u998f\u57fa\u7ebf\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "ORPO-Distill\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8de8\u67b6\u6784LLM\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u548c\u6df7\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u84b8\u998f\u6027\u80fd\u3002"}}
{"id": "2509.25104", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25104", "abs": "https://arxiv.org/abs/2509.25104", "authors": ["Albert Vong", "Steven Henke", "Oliver Hoidn", "Hanna Ruth", "Junjing Deng", "Alexander Hexemer", "Apurva Mehta", "Arianna Gleason", "Levi Hancock", "Nicholas Schwarz"], "title": "Towards generalizable deep ptychography neural networks", "comment": "Submitted to scientific journal for peer review", "summary": "X-ray ptychography is a data-intensive imaging technique expected to become\nubiquitous at next-generation light sources delivering many-fold increases in\ncoherent flux. The need for real-time feedback under accelerated acquisition\nrates motivates surrogate reconstruction models like deep neural networks,\nwhich offer orders-of-magnitude speedup over conventional methods. However,\nexisting deep learning approaches lack robustness across diverse experimental\nconditions. We propose an unsupervised training workflow emphasizing probe\nlearning by combining experimentally-measured probes with synthetic,\nprocedurally generated objects. This probe-centric approach enables a single\nphysics-informed neural network to reconstruct unseen experiments across\nmultiple beamlines; among the first demonstrations of multi-probe\ngeneralization. We find probe learning is equally important as in-distribution\nlearning; models trained using this synthetic workflow achieve reconstruction\nfidelity comparable to those trained exclusively on experimental data, even\nwhen changing the type of synthetic training object. The proposed approach\nenables training of experiment-steering models that provide real-time feedback\nunder dynamic experimental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a2\u9488\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5b9e\u9a8c\u6d4b\u91cf\u7684\u63a2\u9488\u548c\u7a0b\u5e8f\u751f\u6210\u7684\u5408\u6210\u7269\u4f53\uff0c\u4f7f\u5355\u4e2a\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5728\u591a\u4e2a\u675f\u7ebf\u4e0a\u91cd\u5efa\u672a\u89c1\u8fc7\u7684\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u591a\u63a2\u9488\u6cdb\u5316\u3002", "motivation": "X\u5c04\u7ebfptychography\u662f\u4e00\u79cd\u6570\u636e\u5bc6\u96c6\u578b\u6210\u50cf\u6280\u672f\uff0c\u9700\u8981\u5b9e\u65f6\u53cd\u9988\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4e0d\u540c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u5b9e\u9a8c\u6761\u4ef6\u6cdb\u5316\u7684\u91cd\u5efa\u6a21\u578b\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u8bad\u7ec3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u91cd\u70b9\u8fdb\u884c\u63a2\u9488\u5b66\u4e60\uff0c\u5c06\u5b9e\u9a8c\u6d4b\u91cf\u7684\u63a2\u9488\u4e0e\u7a0b\u5e8f\u751f\u6210\u7684\u5408\u6210\u7269\u4f53\u76f8\u7ed3\u5408\uff0c\u8bad\u7ec3\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u5355\u4e2a\u6a21\u578b\u80fd\u591f\u5728\u591a\u4e2a\u675f\u7ebf\u4e0a\u91cd\u5efa\u672a\u89c1\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u591a\u63a2\u9488\u6cdb\u5316\u3002\u4f7f\u7528\u5408\u6210\u5de5\u4f5c\u6d41\u7a0b\u8bad\u7ec3\u7684\u6a21\u578b\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0e\u4ec5\u4f7f\u7528\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u5373\u4f7f\u6539\u53d8\u5408\u6210\u8bad\u7ec3\u7269\u4f53\u7c7b\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a2\u9488\u4e2d\u5fc3\u65b9\u6cd5\u80fd\u591f\u8bad\u7ec3\u5b9e\u9a8c\u5bfc\u5411\u6a21\u578b\uff0c\u5728\u52a8\u6001\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u5b9e\u9a8c\u6761\u4ef6\u9c81\u68d2\u6027\u7684\u95ee\u9898\u3002"}}
{"id": "2509.25133", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25133", "abs": "https://arxiv.org/abs/2509.25133", "authors": ["Yuxian Jiang", "Yafu Li", "Guanxu Chen", "Dongrui Liu", "Yu Cheng", "Jing Shao"], "title": "Rethinking Entropy Regularization in Large Reasoning Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise\nin enhancing the reasoning abilities of large reasoning models (LRMs). However,\nit suffers from a critical issue: entropy collapse and premature convergence.\nNaive entropy regularization, a common approach for encouraging exploration in\nthe traditional RL literature, fails to address this problem in the context of\nLRM. Our analysis reveals that this failure stems from the vast action space\nand long trajectories in LRMs, which easily trigger a global entropy explosion\nas the model indiscriminately explores all possible actions and states. To\naddress this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method\nthat confines exploration to a meaningful subset of actions and states. SIREN\nachieves this through a two-step entropy masking mechanism, consisting of a\ntop-p mask and a peak-entropy mask. In addition, regularization is transformed\ninto a self-anchored form to stabilize training. Across five mathematical\nbenchmarks, SIREN attains superior average performance over previous\nentropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on\nAIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes\ngreater response diversity and maintains entropy at an appropriate level, which\nhelps to preserve the validation pass@k throughout training. This effectively\nmitigates the premature convergence problem common in RLVR for LRM.", "AI": {"tldr": "\u63d0\u51fa\u4e86SIREN\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u7684\u71b5\u5d29\u6e83\u548c\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6734\u7d20\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u5927\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u4e2d\u5931\u6548\uff0c\u56e0\u4e3a\u5de8\u5927\u7684\u52a8\u4f5c\u7a7a\u95f4\u548c\u957f\u8f68\u8ff9\u4f1a\u5bfc\u81f4\u5168\u5c40\u71b5\u7206\u70b8\uff0c\u9700\u8981\u9650\u5236\u63a2\u7d22\u5230\u6709\u610f\u4e49\u7684\u52a8\u4f5c\u548c\u72b6\u6001\u5b50\u96c6\u3002", "method": "SIREN\u65b9\u6cd5\u91c7\u7528\u4e24\u6b65\u71b5\u63a9\u7801\u673a\u5236\uff08top-p\u63a9\u7801\u548c\u5cf0\u503c\u71b5\u63a9\u7801\uff09\uff0c\u5c06\u6b63\u5219\u5316\u8f6c\u6362\u4e3a\u81ea\u951a\u5b9a\u5f62\u5f0f\u4ee5\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5c06\u63a2\u7d22\u9650\u5236\u5728\u6709\u610f\u4e49\u7684\u52a8\u4f5c\u548c\u72b6\u6001\u5b50\u96c6\u5185\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIREN\u5728\u71b5\u76f8\u5173\u7684RLVR\u65b9\u6cd5\u4e2d\u53d6\u5f97\u6700\u4f18\u5e73\u5747\u6027\u80fd\uff0c\u5728AIME24/25\u4e0a\u4f7f\u7528Qwen2.5-Math-7B\u5b9e\u73b0\u4e86+6.6 maj@k\u7684\u63d0\u5347\uff0c\u4fc3\u8fdb\u4e86\u66f4\u5927\u7684\u54cd\u5e94\u591a\u6837\u6027\u5e76\u4fdd\u6301\u9002\u5f53\u7684\u71b5\u6c34\u5e73\u3002", "conclusion": "SIREN\u6709\u6548\u7f13\u89e3\u4e86LRM\u4e2dRLVR\u5e38\u89c1\u7684\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u7ef4\u6301\u4e86\u9a8c\u8bc1pass@k\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.25135", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25135", "abs": "https://arxiv.org/abs/2509.25135", "authors": ["Daniil Dmitriev", "Harald Eskelund Franck", "Carolin Heinzler", "Amartya Sanyal"], "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary", "comment": null, "summary": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u91cd\u653e\u5b66\u4e60\u6846\u67b6\uff0c\u7814\u7a76\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u81ea\u6211\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\u5982\u4f55\u907f\u514d\u9519\u8bef\u5f3a\u5316\u3002\u901a\u8fc7\u5f15\u5165\u6269\u5c55\u9608\u503c\u7ef4\u5ea6\u4f5c\u4e3a\u7cbe\u786e\u7684\u53ef\u5b66\u4e60\u6027\u5ea6\u91cf\uff0c\u8bc1\u660e\u4e86\u95ed\u5305\u5b66\u4e60\u5668\u7684\u6700\u4f18\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u91cd\u653e\u8bbe\u7f6e\u6bd4\u4f20\u7edf\u9519\u8bef\u8fb9\u754c\u8bbe\u7f6e\u66f4\u56f0\u96be\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u81ea\u6211\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5b83\u4eec\u9762\u4e34\u5f3a\u5316\u9519\u8bef\u3001\u6210\u4e3a\u81ea\u8eab\u4fe1\u5ff5\u56de\u97f3\u5ba4\u7684\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u5efa\u6a21\u8fd9\u79cd\u73b0\u8c61\uff0c\u5206\u6790\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5b66\u4e60\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u5728\u7ebf\u91cd\u653e\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u6bcf\u4e00\u8f6e\u4e2d\u5b66\u4e60\u8005\u8f93\u51fa\u5047\u8bbe\uff0c\u5bf9\u624b\u63ed\u793a\u771f\u5b9e\u6807\u7b7e\u6216\u65e9\u671f\u8f6e\u6b21\u7684\u56de\u653e\u6807\u7b7e\u3002\u5f15\u5165\u4e86\u6269\u5c55\u9608\u503c\u7ef4\u5ea6\u4f5c\u4e3a\u53ef\u5b66\u4e60\u6027\u7684\u7cbe\u786e\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u95ed\u5305\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u6269\u5c55\u9608\u503c\u7ef4\u5ea6\u662f\u91cd\u653e\u8bbe\u7f6e\u4e2d\u53ef\u5b66\u4e60\u6027\u7684\u7cbe\u786e\u5ea6\u91cf\uff0c\u95ed\u5305\u5b66\u4e60\u5668\u6700\u591a\u72af\u6269\u5c55\u9608\u503c\u7ef4\u5ea6\u4e2a\u9519\u8bef\uff0c\u4e14\u6ca1\u6709\u7b97\u6cd5\u80fd\u505a\u5f97\u66f4\u597d\u3002\u5bf9\u4e8e\u968f\u673a\u5bf9\u624b\uff0c\u8bc1\u660e\u4e86\u6bcf\u4e2a\u4ea4\u96c6\u95ed\u7c7b\u90fd\u6709\u7c7b\u4f3c\u8fb9\u754c\u3002", "conclusion": "\u91cd\u653e\u8bbe\u7f6e\u6bd4\u4f20\u7edf\u9519\u8bef\u8fb9\u754c\u8bbe\u7f6e\u66f4\u56f0\u96be\uff0c\u67d0\u4e9b\u7c7b\u5177\u6709\u5e38\u6570Littlestone\u7ef4\u5ea6\u4f46\u4efb\u610f\u5927\u7684\u6269\u5c55\u9608\u503c\u7ef4\u5ea6\u3002\u9002\u5f53\u5b66\u4e60\u8868\u73b0\u51fa\u66f4\u5c16\u9510\u7684\u5206\u79bb\uff1a\u7c7b\u5728\u91cd\u653e\u4e0b\u53ef\u9002\u5f53\u5b66\u4e60\u5f53\u4e14\u4ec5\u5f53\u5b83\uff08\u51e0\u4e4e\uff09\u662f\u4ea4\u96c6\u95ed\u7684\u3002"}}
{"id": "2509.25136", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25136", "abs": "https://arxiv.org/abs/2509.25136", "authors": ["David Gonz\u00e1lez Mart\u00ednez"], "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression", "comment": null, "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.", "AI": {"tldr": "BALF\u662f\u4e00\u4e2a\u65e0\u9700\u5fae\u8c03\u7684\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u611f\u77e5\u5206\u89e3\u548c\u53ef\u6269\u5c55\u7684\u9884\u7b97\u79e9\u5206\u914d\u5668\uff0c\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u6216\u641c\u7d22\u8fc7\u7a0b\uff0c\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u5fae\u8c03\u7684\u538b\u7f29\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6fc0\u6d3b\u611f\u77e5\u5206\u89e3\u6846\u67b6\u548c\u53ef\u6269\u5c55\u9884\u7b97\u79e9\u5206\u914d\u5668\uff0c\u5f62\u6210BALF\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63a7\u5236\u538b\u7f29\u76ee\u6807\u3002", "result": "\u5728ResNet-20\u3001ResNeXt-101\u548c\u89c6\u89c9\u53d8\u6362\u5668\u7b49\u591a\u79cd\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cResNeXt-101\u7684FLOPs\u51cf\u5c1145%\u65f6\u4ec5\u635f\u59311%\u7684top-1\u51c6\u786e\u7387\u3002", "conclusion": "BALF\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u9ad8\u6548\u538b\u7f29\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u826f\u597d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2509.25153", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25153", "abs": "https://arxiv.org/abs/2509.25153", "authors": ["Nicholas Barnfield", "Hugo Cui", "Yue M. Lu"], "title": "High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification", "comment": null, "summary": "When and how can an attention mechanism learn to selectively attend to\ninformative tokens, thereby enabling detection of weak, rare, and sparsely\nlocated features? We address these questions theoretically in a sparse-token\nclassification model in which positive samples embed a weak signal vector in a\nrandomly chosen subset of tokens, whereas negative samples are pure noise. In\nthe long-sequence limit, we show that a simple single-layer attention\nclassifier can in principle achieve vanishing test error when the signal\nstrength grows only logarithmically in the sequence length $L$, whereas linear\nclassifiers require $\\sqrt{L}$ scaling. Moving from representational power to\nlearnability, we study training at finite $L$ in a high-dimensional regime,\nwhere sample size and embedding dimension grow proportionally. We prove that\njust two gradient updates suffice for the query weight vector of the attention\nclassifier to acquire a nontrivial alignment with the hidden signal, inducing\nan attention map that selectively amplifies informative tokens. We further\nderive an exact asymptotic expression for the test error and training loss of\nthe trained attention-based classifier, and quantify its capacity -- the\nlargest dataset size that is typically perfectly separable -- thereby\nexplaining the advantage of adaptive token selection over nonadaptive linear\nbaselines.", "AI": {"tldr": "\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u5728\u4fe1\u53f7\u5f3a\u5ea6\u4ec5\u968f\u5e8f\u5217\u957f\u5ea6\u5bf9\u6570\u589e\u957f\u65f6\u5b9e\u73b0\u6d88\u5931\u7684\u6d4b\u8bd5\u8bef\u5dee\uff0c\u800c\u7ebf\u6027\u5206\u7c7b\u5668\u9700\u8981\u5e73\u65b9\u6839\u589e\u957f\u3002\u5728\u6709\u9650\u5e8f\u5217\u957f\u5ea6\u7684\u9ad8\u7ef4\u673a\u5236\u4e0b\uff0c\u4ec5\u9700\u4e24\u6b21\u68af\u5ea6\u66f4\u65b0\u5c31\u80fd\u4f7f\u6ce8\u610f\u529b\u5206\u7c7b\u5668\u4e0e\u9690\u85cf\u4fe1\u53f7\u5bf9\u9f50\uff0c\u9009\u62e9\u6027\u5730\u653e\u5927\u4fe1\u606f\u4e30\u5bcc\u7684token\u3002", "motivation": "\u7814\u7a76\u6ce8\u610f\u529b\u673a\u5236\u5982\u4f55\u5b66\u4e60\u9009\u62e9\u6027\u5730\u5173\u6ce8\u4fe1\u606f\u4e30\u5bcc\u7684token\uff0c\u4ece\u800c\u68c0\u6d4b\u5f31\u3001\u7a00\u758f\u4e14\u7a00\u758f\u5206\u5e03\u7684\u7279\u5f81\uff0c\u7279\u522b\u662f\u5728\u957f\u5e8f\u5217\u4e2d\u7406\u89e3\u6ce8\u610f\u529b\u76f8\u5bf9\u4e8e\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u7a00\u758ftoken\u5206\u7c7b\u6a21\u578b\uff0c\u5176\u4e2d\u6b63\u6837\u672c\u5728\u968f\u673a\u9009\u62e9\u7684token\u5b50\u96c6\u4e2d\u5d4c\u5165\u5f31\u4fe1\u53f7\u5411\u91cf\uff0c\u8d1f\u6837\u672c\u662f\u7eaf\u566a\u58f0\u3002\u5728\u957f\u5e8f\u5217\u6781\u9650\u548c\u9ad8\u7ef4\u673a\u5236\u4e0b\u5206\u6790\u5355\u5c42\u6ce8\u610f\u529b\u5206\u7c7b\u5668\u7684\u7406\u8bba\u6027\u80fd\u3002", "result": "\u6ce8\u610f\u529b\u5206\u7c7b\u5668\u5728\u4fe1\u53f7\u5f3a\u5ea6\u4ec5\u9700\u5bf9\u6570\u589e\u957f\u65f6\u5c31\u80fd\u5b9e\u73b0\u6d88\u5931\u7684\u6d4b\u8bd5\u8bef\u5dee\uff0c\u800c\u7ebf\u6027\u5206\u7c7b\u5668\u9700\u8981\u5e73\u65b9\u6839\u589e\u957f\u3002\u4ec5\u4e24\u6b21\u68af\u5ea6\u66f4\u65b0\u5c31\u80fd\u4f7f\u6ce8\u610f\u529b\u6743\u91cd\u4e0e\u9690\u85cf\u4fe1\u53f7\u5bf9\u9f50\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u81ea\u9002\u5e94token\u9009\u62e9\u5728\u68c0\u6d4b\u5f31\u7a00\u758f\u7279\u5f81\u65b9\u9762\u4f18\u4e8e\u975e\u81ea\u9002\u5e94\u7ebf\u6027\u57fa\u7ebf\uff0c\u89e3\u91ca\u4e86\u5176\u5bb9\u91cf\u4f18\u52bf\u3002"}}
{"id": "2509.25157", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25157", "abs": "https://arxiv.org/abs/2509.25157", "authors": ["Jinhao Liang", "Yixuan Sun", "Anirban Samaddar", "Sandeep Madireddy", "Ferdinando Fioretto"], "title": "Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation", "comment": null, "summary": "Generative models excel at synthesizing high-fidelity samples from complex\ndata distributions, but they often violate hard constraints arising from\nphysical laws or task specifications. A common remedy is to project\nintermediate samples onto the feasible set; however, repeated projection can\ndistort the learned distribution and induce a mismatch with the data manifold.\nThus, recent multi-stage procedures attempt to defer projection to clean\nsamples during sampling, but they increase algorithmic complexity and\naccumulate errors across steps. This paper addresses these challenges by\nproposing a novel training-free method, Chance-constrained Flow Matching\n(CCFM), that integrates stochastic optimization into the sampling process,\nenabling effective enforcement of hard constraints while maintaining\nhigh-fidelity sample generation. Importantly, CCFM guarantees feasibility in\nthe same manner as conventional repeated projection, yet, despite operating\ndirectly on noisy intermediate samples, it is theoretically equivalent to\nprojecting onto the feasible set defined by clean samples. This yields a\nsampler that mitigates distributional distortion. Empirical experiments show\nthat CCFM outperforms current state-of-the-art constrained generative models in\nmodeling complex physical systems governed by partial differential equations\nand molecular docking problems, delivering higher feasibility and fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5CCFM\uff0c\u5c06\u968f\u673a\u4f18\u5316\u96c6\u6210\u5230\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u6709\u6548\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u6837\u672c\u751f\u6210\uff0c\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u548c\u5206\u5b50\u5bf9\u63a5\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u9ad8\u4fdd\u771f\u6837\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u6216\u4efb\u52a1\u89c4\u8303\u4ea7\u751f\u7684\u786c\u7ea6\u675f\u3002\u73b0\u6709\u6295\u5f71\u65b9\u6cd5\u4f1a\u626d\u66f2\u5b66\u4e60\u5206\u5e03\uff0c\u800c\u591a\u9636\u6bb5\u65b9\u6cd5\u4f1a\u589e\u52a0\u7b97\u6cd5\u590d\u6742\u5ea6\u548c\u7d2f\u79ef\u8bef\u5dee\u3002", "method": "CCFM\u65b9\u6cd5\u5c06\u968f\u673a\u4f18\u5316\u96c6\u6210\u5230\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u76f4\u63a5\u5728\u566a\u58f0\u4e2d\u95f4\u6837\u672c\u4e0a\u64cd\u4f5c\uff0c\u4f46\u7406\u8bba\u4e0a\u7b49\u540c\u4e8e\u5728\u5e72\u51c0\u6837\u672c\u4e0a\u6295\u5f71\u5230\u53ef\u884c\u96c6\uff0c\u4ece\u800c\u51cf\u8f7b\u5206\u5e03\u626d\u66f2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCCFM\u5728\u5efa\u6a21\u7531\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u590d\u6742\u7269\u7406\u7cfb\u7edf\u548c\u5206\u5b50\u5bf9\u63a5\u95ee\u9898\u65f6\uff0c\u5728\u53ef\u884c\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7ea6\u675f\u751f\u6210\u6a21\u578b\u3002", "conclusion": "CCFM\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u6837\u672c\u751f\u6210\uff0c\u5728\u590d\u6742\u7ea6\u675f\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.25158", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25158", "abs": "https://arxiv.org/abs/2509.25158", "authors": ["Ehimare Okoyomon", "Arbel Yaniv", "Christoph Goebel"], "title": "Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids", "comment": null, "summary": "Voltage prediction in distribution grids is a critical yet difficult task for\nmaintaining power system stability. Machine learning approaches, particularly\nGraph Neural Networks (GNNs), offer significant speedups but suffer from poor\ngeneralization when trained on limited or incomplete data. In this work, we\nsystematically investigate the role of inductive biases in improving a model's\nability to reliably learn power flow. Specifically, we evaluate three\nphysics-informed strategies: (i) power-flow-constrained loss functions, (ii)\ncomplex-valued neural networks, and (iii) residual-based task reformulation.\nUsing the ENGAGE dataset, which spans multiple low- and medium-voltage grid\nconfigurations, we conduct controlled experiments to isolate the effect of each\ninductive bias and assess both standard predictive performance and\nout-of-distribution generalization. Our study provides practical insights into\nwhich model assumptions most effectively guide learning for reliable and\nefficient voltage prediction in modern distribution networks.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u4e09\u79cd\u7269\u7406\u4fe1\u606f\u7b56\u7565\u5bf9\u914d\u7535\u7f51\u7535\u538b\u9884\u6d4b\u4e2d\u56fe\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c", "motivation": "\u914d\u7535\u7f51\u7535\u538b\u9884\u6d4b\u5bf9\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6709\u9650\u6216\u4e0d\u5b8c\u6574\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\u6cdb\u5316\u80fd\u529b\u5dee", "method": "\u8bc4\u4f30\u4e09\u79cd\u7269\u7406\u4fe1\u606f\u7b56\u7565\uff1a\u529f\u7387\u6d41\u7ea6\u675f\u635f\u5931\u51fd\u6570\u3001\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u3001\u57fa\u4e8e\u6b8b\u5dee\u7684\u4efb\u52a1\u91cd\u6784\uff0c\u4f7f\u7528ENGAGE\u6570\u636e\u96c6\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u54ea\u79cd\u6a21\u578b\u5047\u8bbe\u80fd\u6700\u6709\u6548\u6307\u5bfc\u73b0\u4ee3\u914d\u7535\u7f51\u53ef\u9760\u9ad8\u6548\u7535\u538b\u9884\u6d4b\u5b66\u4e60\u7684\u5b9e\u7528\u89c1\u89e3", "conclusion": "\u7269\u7406\u4fe1\u606f\u5f52\u7eb3\u504f\u7f6e\u80fd\u663e\u8457\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u914d\u7535\u7f51\u7535\u538b\u9884\u6d4b\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027"}}
{"id": "2509.25170", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25170", "abs": "https://arxiv.org/abs/2509.25170", "authors": ["Peter Holderrieth", "Uriel Singer", "Tommi Jaakkola", "Ricky T. Q. Chen", "Yaron Lipman", "Brian Karrer"], "title": "GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models", "comment": null, "summary": "The performance of flow matching and diffusion models can be greatly improved\nat inference time using reward alignment algorithms, yet efficiency remains a\nmajor limitation. While several algorithms were proposed, we demonstrate that a\ncommon bottleneck is the sampling method these algorithms rely on: many\nalgorithms require to sample Markov transitions via SDE sampling, which is\nsignificantly less efficient and often less performant than ODE sampling. To\nremove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that\nsimulates a \"flow matching model within a flow matching model\" to sample Markov\ntransitions. As we show in this work, this \"inner\" flow matching model can be\nretrieved from a pre-trained model without any re-training, combining the\nefficiency of ODEs with the stochastic evolution of SDEs. On large-scale\ntext-to-image models, we show that GLASS Flows eliminate the trade-off between\nstochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS\nFlows improve state-of-the-art performance in text-to-image generation, making\nit a simple, drop-in solution for inference-time scaling of flow and diffusion\nmodels.", "AI": {"tldr": "GLASS Flows\u662f\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\"\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u6d41\u5339\u914d\u6a21\u578b\"\u6765\u91c7\u6837\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\uff0c\u5c06ODE\u7684\u6548\u7387\u4e0eSDE\u7684\u968f\u673a\u6f14\u5316\u76f8\u7ed3\u5408\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u83b7\u53d6\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u5bf9\u9f50\u7b97\u6cd5\u5728\u63a8\u7406\u65f6\u4f9d\u8d56SDE\u91c7\u6837\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6027\u80fd\u8f83\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408ODE\u6548\u7387\u548cSDE\u968f\u673a\u6f14\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGLASS Flows\u91c7\u6837\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u5185\u90e8\u6d41\u5339\u914d\u6a21\u578b\u6765\u91c7\u6837\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\uff0c\u53ef\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u76f4\u63a5\u83b7\u53d6\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6587\u751f\u56fe\u6a21\u578b\u4e2d\uff0cGLASS Flows\u6d88\u9664\u4e86\u968f\u673a\u6f14\u5316\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u7ed3\u5408Feynman-Kac Steering\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u7684\u6587\u751f\u56fe\u6027\u80fd\u3002", "conclusion": "GLASS Flows\u4e3a\u6d41\u548c\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25171", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.25171", "abs": "https://arxiv.org/abs/2509.25171", "authors": ["Sophia Tang", "Yuchen Zhu", "Molei Tao", "Pranam Chatterjee"], "title": "TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion", "comment": null, "summary": "Reinforcement learning with stochastic optimal control offers a promising\nframework for diffusion fine-tuning, where a pre-trained diffusion model is\noptimized to generate paths that lead to a reward-tilted distribution. While\nthese approaches enable optimization without access to explicit samples from\nthe optimal distribution, they require training on rollouts under the current\nfine-tuned model, making them susceptible to reinforcing sub-optimal\ntrajectories that yield poor rewards. To overcome this challenge, we introduce\nTRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion\n(TR2-D2), a novel framework that optimizes reward-guided discrete diffusion\ntrajectories with tree search to construct replay buffers for trajectory-aware\nfine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS)\nand subsequently used to fine-tune a pre-trained discrete diffusion model under\na stochastic optimal control objective. We validate our framework on single-\nand multi-objective fine-tuning of biological sequence diffusion models,\nhighlighting the overall effectiveness of TR2-D2 for reliable reward-guided\nfine-tuning in discrete sequence generation.", "AI": {"tldr": "TR2-D2\u662f\u4e00\u4e2a\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u56de\u653e\u7f13\u51b2\u533a\uff0c\u5b9e\u73b0\u5956\u52b1\u5f15\u5bfc\u7684\u8f68\u8ff9\u611f\u77e5\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u4e0e\u968f\u673a\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u5fae\u8c03\u4e2d\u5bb9\u6613\u5f3a\u5316\u6b21\u4f18\u8f68\u8ff9\uff0c\u5bfc\u81f4\u5956\u52b1\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u56de\u653e\u7f13\u51b2\u533a\uff0c\u5728\u968f\u673a\u6700\u4f18\u63a7\u5236\u76ee\u6807\u4e0b\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u751f\u7269\u5e8f\u5217\u6269\u6563\u6a21\u578b\u7684\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u5fae\u8c03\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "TR2-D2\u4e3a\u79bb\u6563\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5956\u52b1\u5f15\u5bfc\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25174", "abs": "https://arxiv.org/abs/2509.25174", "authors": ["Daniel Palenicek", "Florian Vogt", "Joe Watson", "Ingmar Posner", "Jan Peters"], "title": "XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning", "comment": null, "summary": "Sample efficiency is a central property of effective deep reinforcement\nlearning algorithms. Recent work has improved this through added complexity,\nsuch as larger models, exotic network architectures, and more complex\nalgorithms, which are typically motivated purely by empirical performance. We\ntake a more principled approach by focusing on the optimization landscape of\nthe critic network. Using the eigenspectrum and condition number of the\ncritic's Hessian, we systematically investigate the impact of common\narchitectural design decisions on training dynamics. Our analysis reveals that\na novel combination of batch normalization (BN), weight normalization (WN), and\na distributional cross-entropy (CE) loss produces condition numbers orders of\nmagnitude smaller than baselines. This combination also naturally bounds\ngradient norms, a property critical for maintaining a stable effective learning\nrate under non-stationary targets and bootstrapping. Based on these insights,\nwe introduce XQC: a well-motivated, sample-efficient deep actor-critic\nalgorithm built upon soft actor-critic that embodies these optimization-aware\nprinciples. We achieve state-of-the-art sample efficiency across 55\nproprioception and 15 vision-based continuous control tasks, all while using\nsignificantly fewer parameters than competing methods.", "AI": {"tldr": "\u63d0\u51faXQC\u7b97\u6cd5\uff0c\u901a\u8fc7\u6279\u5f52\u4e00\u5316\u3001\u6743\u91cd\u5f52\u4e00\u5316\u548c\u5206\u5e03\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u7ec4\u5408\uff0c\u663e\u8457\u6539\u5584\u8bc4\u8bba\u5bb6\u7f51\u7edc\u7684\u4f18\u5316\u6761\u4ef6\u6570\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u901a\u8fc7\u589e\u52a0\u590d\u6742\u6027\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u3002\u672c\u6587\u5173\u6ce8\u8bc4\u8bba\u5bb6\u7f51\u7edc\u7684\u4f18\u5316\u666f\u89c2\uff0c\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u5e38\u89c1\u67b6\u6784\u8bbe\u8ba1\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Hessian\u77e9\u9635\u7684\u7279\u5f81\u8c31\u548c\u6761\u4ef6\u6570\u7cfb\u7edf\u5206\u6790\u67b6\u6784\u8bbe\u8ba1\uff0c\u63d0\u51fa\u7ed3\u5408\u6279\u5f52\u4e00\u5316\u3001\u6743\u91cd\u5f52\u4e00\u5316\u548c\u5206\u5e03\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaXQC\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7ec4\u5408\u4f7f\u6761\u4ef6\u6570\u6bd4\u57fa\u7ebf\u5c0f\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u572855\u4e2a\u672c\u4f53\u611f\u77e5\u548c15\u4e2a\u89c6\u89c9\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6837\u672c\u6548\u7387\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "\u4f18\u5316\u611f\u77e5\u7684\u8bbe\u8ba1\u539f\u5219\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\uff0cXQC\u7b97\u6cd5\u8bc1\u660e\u4e86\u8fd9\u79cd\u539f\u5219\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.25176", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25176", "abs": "https://arxiv.org/abs/2509.25176", "authors": ["Haoming Wen", "Yushi Bai", "Juanzi Li", "Jie Tang"], "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression", "comment": "In submission", "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.", "AI": {"tldr": "SIRI\u662f\u4e00\u79cd\u901a\u8fc7\u4ea4\u66ff\u538b\u7f29\u548c\u6269\u5c55\u63a8\u7406\u9884\u7b97\u6765\u8bad\u7ec3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u73b0\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u91cd\u590d\u601d\u8003\u6a21\u5f0f\uff0c\u51cf\u5c11\u8fd9\u4e9b\u91cd\u590d\u5f80\u5f80\u4ee5\u6027\u80fd\u4e0b\u964d\u4e3a\u4ee3\u4ef7\uff0c\u9700\u8981\u514b\u670d\u8fd9\u79cd\u6743\u8861\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u8bad\u7ec3\u673a\u5236\uff0c\u5728\u538b\u7f29\u9636\u6bb5\u9650\u5236\u63a8\u7406\u957f\u5ea6\u8feb\u4f7f\u6a21\u578b\u505a\u51fa\u7cbe\u786e\u51b3\u7b56\uff0c\u5728\u6269\u5c55\u9636\u6bb5\u653e\u5bbd\u957f\u5ea6\u9650\u5236\u8ba9\u6a21\u578b\u63a2\u7d22\u957f\u89c6\u91ce\u63a8\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6700\u5927\u63a8\u7406\u957f\u5ea6\u5b9e\u73b0\u4ea4\u66ff\u8bad\u7ec3\u3002", "result": "\u5728DeepSeek-R1-Distill-Qwen-1.5B\u4e0a\u8bad\u7ec3\uff0cSIRI-low\u5728AIME24\u4e0a\u6027\u80fd\u63d0\u534743.2%\u540c\u65f6\u51cf\u5c1146.9%\u7684token\u4f7f\u7528\uff0cSIRI-high\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5468\u671f\u6027\u8c03\u6574\u63a8\u7406\u6a21\u578b\u7684\u8f93\u51fa\u622a\u65ad\u957f\u5ea6\uff0c\u53ef\u4ee5\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u6548\u7387\uff0c\u6536\u655b\u5230\u4e24\u8005\u4e4b\u95f4\u7684\u6700\u4f18\u5e73\u8861\u70b9\u3002"}}
