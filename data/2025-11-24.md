<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 提出了一种新的数字内存随机计算架构DISCA，采用压缩的准随机Bent-Pyramid数据格式，在保持数字系统可扩展性和可靠性的同时，实现了类似模拟计算的简单性，显著提升了矩阵乘法工作负载的能效。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用向边缘迁移，传统冯·诺依曼架构面临内存墙和摩尔定律终结的挑战，现有内存计算架构因设计限制导致性能大幅下降，需要新的解决方案。

Method: 提出DISCA数字内存随机计算架构，使用压缩的准随机Bent-Pyramid数据格式，结合了模拟计算的简单性和数字系统的可扩展性、可靠性。

Result: 后布局建模结果显示，在商用180nm CMOS技术下，DISCA在500MHz频率下实现每比特3.59 TOPS/W的能效，相比同类架构能效提升数个数量级。

Conclusion: DISCA架构通过创新的数字内存随机计算方法，有效解决了AI边缘计算中的能效挑战，为大规模矩阵乘法任务提供了高效的硬件解决方案。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


### [2] [Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training](https://arxiv.org/abs/2511.16831)
*Yipeng Wang,Mengtian Yang,Chieh-pu Lo,Jaydeep P. Kulkarni*

Main category: cs.AR

TL;DR: Vorion是首个支持硬件加速3D高斯泼溅渲染和训练的GPGPU原型，通过可扩展架构、最小化硬件改动、z-tiling技术和混合数据流，在TSMC 16nm工艺下实现19FPS渲染和38.6迭代/秒的训练性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在实时神经渲染、3D场景生成和4D视频捕获中表现出色，但其渲染和训练计算量巨大，无法在边缘设备上实现实时渲染或在工作站上实现实时4D重建。

Method: 提出Vorion架构，采用可扩展设计、最小化传统光栅化器硬件改动、z-tiling技术增加并行性，以及高斯/像素中心混合数据流。

Result: 在TSMC 16nm FinFET工艺下，最小系统（8个SIMT核心，2个高斯光栅化器）实现19FPS渲染性能；扩展设计（16个光栅化器）实现38.6迭代/秒的训练速度。

Conclusion: 3D高斯泼溅技术适合在下一代GPU图形管线中部署专用硬件，Vorion原型证明了硬件加速的可行性，为实时神经渲染提供了有效的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.

</details>


### [3] [Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration](https://arxiv.org/abs/2511.17123)
*Jiaxun Fang,Li Zhang,Shaoyi Huang*

Main category: cs.AR

TL;DR: 提出了一种能量感知的逐层压缩框架，通过结合MAC单元能量特性和逐层激活统计，在量化感知训练中优化权重选择，实现CNN推理能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用全局激活模型、粗略能量代理或层无关策略，限制了在实际硬件上的有效性。需要更精确的逐层能量感知压缩方法。

Method: 1) 构建逐层感知的MAC能量模型，结合逐层激活统计和22位部分和转换的MSB-汉明距离分组；2) 在量化感知训练中引入能量-精度协同优化的权重选择算法；3) 提出能量优先的逐层调度策略，在全局精度约束下更积极地压缩高能耗层。

Result: 在不同CNN模型上实验，实现了最高58.6%的能量降低，精度下降仅2-3%，优于现有最先进的功率感知基准方法。

Conclusion: 该框架通过精确的逐层能量建模和优化调度，有效降低了CNN推理能耗，同时保持了较高的精度。

Abstract: Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.

</details>


### [4] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: NX-CGRA是一种基于粗粒度可重构阵列的可编程硬件加速器，专为边缘设备上的Transformer推理设计，支持线性和非线性函数，在性能和能效之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上Transformer工作负载的多样性和复杂性日益增长，需要在性能、能效和架构灵活性之间找到平衡点。固定功能加速器无法适应广泛的使用场景。

Method: 采用粗粒度可重构阵列架构，具有软件驱动的可编程性，能够高效执行各种内核模式，支持线性和非线性Transformer推理算法。

Result: 使用真实Transformer模型的代表性基准进行评估，展示了高整体效率和良好的能量-面积权衡，适用于不同类型的操作。

Conclusion: NX-CGRA作为一种可扩展和适应性强的硬件解决方案，在受限的功率和硅预算下具有在边缘部署Transformer的潜力。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [5] [MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing](https://arxiv.org/abs/2511.17418)
*Houji Zhou,Ling Yang,Zhiwei Zhou,Yi Li,Xiangshui Miao*

Main category: cs.AR

TL;DR: 提出MemIntelli端到端仿真框架，支持灵活的变精度计算，实现忆阻器上智能应用的预验证


<details>
  <summary>Details</summary>
Motivation: 忆阻内存计算中电路与算法的耦合使计算可靠性易受器件和外围电路非理想效应影响，需要高效的软硬件协同仿真工具

Method: 在器件和电路层面使用数学函数进行等效电路建模抽象，在架构层面实现支持整数和浮点数据表示的灵活变精度内存计算，兼容NumPy和PyTorch

Result: 展示了方程求解、数据聚类、小波变换、神经网络训练和推理等多种智能算法的强大处理能力

Conclusion: 提供了一个从器件到应用的全面仿真工具，促进内存计算系统的协同设计

Abstract: Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Joint Design of Protein Surface and Structure Using a Diffusion Bridge Model](https://arxiv.org/abs/2511.16675)
*Guanlue Li,Xufeng Zhao,Fang Wu,Sören Laue*

Main category: cs.LG

TL;DR: PepBridge是一个新颖的蛋白质设计框架，通过联合设计蛋白质表面和结构，整合受体表面几何形状和生化特性，实现蛋白质-蛋白质相互作用的高效设计。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-蛋白质相互作用由表面互补性和疏水相互作用控制，但设计能够精确互补目标受体的多样化、物理真实的蛋白质结构和表面仍然是一个重大挑战。

Method: 使用去噪扩散桥模型将受体表面映射到配体表面，然后通过多模型扩散模型预测相应结构，同时使用形状-框架匹配网络确保表面几何形状与骨架结构的对齐。

Result: 在多种蛋白质设计场景中的广泛验证表明，PepBridge能够生成结构可行的蛋白质，在联合设计自上而下的蛋白质结构方面取得了显著进展。

Conclusion: PepBridge通过整合表面互补性、构象稳定性和化学可行性，为蛋白质-蛋白质相互作用的计算设计提供了一个有效的解决方案。

Abstract: Protein-protein interactions (PPIs) are governed by surface complementarity and hydrophobic interactions at protein interfaces. However, designing diverse and physically realistic protein structure and surfaces that precisely complement target receptors remains a significant challenge in computational protein design. In this work, we introduce PepBridge, a novel framework for the joint design of protein surface and structure that seamlessly integrates receptor surface geometry and biochemical properties. Starting with a receptor surface represented as a 3D point cloud, PepBridge generates complete protein structures through a multi-step process. First, it employs denoising diffusion bridge models (DDBMs) to map receptor surfaces to ligand surfaces. Next, a multi-model diffusion model predicts the corresponding structure, while Shape-Frame Matching Networks ensure alignment between surface geometry and backbone architecture. This integrated approach facilitates surface complementarity, conformational stability, and chemical feasibility. Extensive validation across diverse protein design scenarios demonstrates PepBridge's efficacy in generating structurally viable proteins, representing a significant advancement in the joint design of top-down protein structure.

</details>


### [7] [DDTime: Dataset Distillation with Spectral Alignment and Information Bottleneck for Time-Series Forecasting](https://arxiv.org/abs/2511.16715)
*Yuqi Li,Kuiye Ding,Chuanguang Yang,Hao Wang,Haoxuan Wang,Huiran Duan,Junming Liu,Yingli Tian*

Main category: cs.LG

TL;DR: DDTime是一个轻量级的时间序列数据集蒸馏框架，通过频域对齐和样本间正则化解决时间序列蒸馏中的时间偏差和样本多样性不足问题，在20个基准数据集上相比现有方法提升约30%准确率。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要大规模数据集和大量计算资源，数据集蒸馏提供了一种替代方案。但时间序列蒸馏面临两个挑战：1) 强自相关性导致的时间偏差，造成师生模型间值项对齐失真；2) 缺乏明确类别先验导致的合成样本多样性不足。

Method: 基于一阶压缩分解构建DDTime框架：1) 通过时间统计量重新审视值项对齐，引入频域对齐机制缓解自相关偏差；2) 基于信息瓶颈原理设计样本间正则化，增强多样性并最大化信息密度。该目标与多种压缩范式理论兼容，支持稳定的一阶优化。

Result: 在20个基准数据集和多种预测架构上的广泛实验表明，DDTime始终优于现有蒸馏方法，相对准确率提升约30%，同时仅引入约2.49%的计算开销。

Conclusion: DDTime成功解决了时间序列数据集蒸馏中的关键挑战，提供了一种高效且有效的解决方案，所有代码和蒸馏数据集将公开发布。

Abstract: Time-series forecasting is fundamental across many domains, yet training accurate models often requires large-scale datasets and substantial computational resources. Dataset distillation offers a promising alternative by synthesizing compact datasets that preserve the learning behavior of full data. However, extending dataset distillation to time-series forecasting is non-trivial due to two fundamental challenges: 1.temporal bias from strong autocorrelation, which leads to distorted value-term alignment between teacher and student models; and 2.insufficient diversity among synthetic samples, arising from the absence of explicit categorical priors to regularize trajectory variety.
  In this work, we propose DDTime, a lightweight and plug-in distillation framework built upon first-order condensation decomposition. To tackle Challenge 1, it revisits value-term alignment through temporal statistics and introduces a frequency-domain alignment mechanism to mitigate autocorrelation-induced bias, ensuring spectral consistency and temporal fidelity. To address Challenge 2, we further design an inter-sample regularization inspired by the information bottleneck principle, which enhances diversity and maximizes information density across synthetic trajectories. The combined objective is theoretically compatible with a wide range of condensation paradigms and supports stable first-order optimization. Extensive experiments on 20 benchmark datasets and diverse forecasting architectures demonstrate that DDTime consistently outperforms existing distillation methods, achieving about 30% relative accuracy gains while introducing about 2.49% computational overhead. All code and distilled datasets will be released.

</details>


### [8] [When Structure Doesn't Help: LLMs Do Not Read Text-Attributed Graphs as Effectively as We Expected](https://arxiv.org/abs/2511.16767)
*Haotian Xu,Yuning You,Tengfei Ma*

Main category: cs.LG

TL;DR: LLMs在文本属性图上仅使用节点文本描述就能获得强大性能，大多数结构编码策略只带来边际收益甚至负面影响，表明显式结构先验在强大语言模型时代往往不必要甚至适得其反。


<details>
  <summary>Details</summary>
Motivation: 探索不同图结构编码策略如何影响LLM在文本属性图上的性能，挑战传统图学习范式认为结构对图推理至关重要的基本假设。

Method: 系统性地实验比较不同图结构编码策略，包括基于模板的图模板设计和使用GNN编码结构信息等方法。

Result: 发现LLMs仅利用节点文本描述就能在各项任务中取得强劲表现，大多数结构编码策略只提供边际收益或负面效果。

Conclusion: 在强大语言模型时代，显式结构先验往往不必要且可能适得其反，需要重新思考结构在LLM图推理中的表示和利用方式，开启语义驱动的图学习方法。

Abstract: Graphs provide a unified representation of semantic content and relational structure, making them a natural fit for domains such as molecular modeling, citation networks, and social graphs. Meanwhile, large language models (LLMs) have excelled at understanding natural language and integrating cross-modal signals, sparking interest in their potential for graph reasoning. Recent work has explored this by either designing template-based graph templates or using graph neural networks (GNNs) to encode structural information. In this study, we investigate how different strategies for encoding graph structure affect LLM performance on text-attributed graphs. Surprisingly, our systematic experiments reveal that: (i) LLMs leveraging only node textual descriptions already achieve strong performance across tasks; and (ii) most structural encoding strategies offer marginal or even negative gains. We show that explicit structural priors are often unnecessary and, in some cases, counterproductive when powerful language models are involved. This represents a significant departure from traditional graph learning paradigms and highlights the need to rethink how structure should be represented and utilized in the LLM era. Our study is to systematically challenge the foundational assumption that structure is inherently beneficial for LLM-based graph reasoning, opening the door to new, semantics-driven approaches for graph learning.

</details>


### [9] [GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs](https://arxiv.org/abs/2511.16778)
*Yating Ren,Yikun Ban,Huobin Tan*

Main category: cs.LG

TL;DR: 提出了GCL-OT框架，通过最优传输解决文本属性图中的多粒度异质性挑战，实现结构与文本的灵活双向对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在异质性图上表现不佳，主要因为基于同质性假设的相似性估计和硬优化目标，且将文本嵌入视为静态目标导致次优对齐。本文识别了文本属性图中的多粒度异质性（完全异质、部分异质、潜在同质），这些使得结构-文本对齐特别具有挑战性。

Method: 提出GCL-OT框架，使用最优传输实现灵活双向对齐：1）针对部分异质性，设计基于RealSoftMax的相似性估计器；2）针对完全异质性，引入基于提示的过滤器；3）结合OT引导的软监督来发现潜在同质邻居。

Result: 在9个基准测试上的广泛实验表明，GCL-OT始终优于最先进的方法，验证了其有效性和鲁棒性。

Conclusion: GCL-OT通过最优传输有效解决了文本属性图中的多粒度异质性挑战，理论分析表明其能改善互信息边界和贝叶斯误差保证。

Abstract: Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify the multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT consistently outperforms state-of-the-art methods, verifying its effectiveness and robustness.

</details>


### [10] [Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach](https://arxiv.org/abs/2511.16786)
*Yaoxin Yang,Peng Ye,Xudong Tan,Chongjun Tu,Maosen Zhao,Jia Hao,Tao Chen*

Main category: cs.LG

TL;DR: FlashCache是一个基于频域分析和异常KV感知的多模态KV缓存压缩框架，通过保留偏离主能量的异常KV对，在保持任务性能的同时显著减少KV内存使用并加速解码。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在显著推理开销，因为多模态KV缓存随视觉输入长度增长而增加。现有压缩方法依赖注意力分数，与高效注意力内核不兼容且忽略值向量对注意力输出的贡献。

Method: 从KV矩阵分布角度重新审视压缩，发现多模态KV矩阵频域能量集中在低频。提出FlashCache框架：1) 异常KV识别模块在频域建模主成分，优先保留显著偏离的KV对；2) 动态预算分配模块自适应确定每层KV缓存大小以保留更多异常KV。

Result: 在多个MLLM和基准测试中，FlashCache优于最先进的多模态KV压缩方法，实现高达1.69倍解码加速和80% KV内存使用降低，同时保持任务性能。

Conclusion: 基于频域分析和异常KV感知的KV缓存压缩方法能有效减少多模态大语言模型的推理开销，同时保持模型性能。

Abstract: Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.

</details>


### [11] [A Vector Symbolic Approach to Multiple Instance Learning](https://arxiv.org/abs/2511.16795)
*Ehsan Ahmed Dhrubo,Mohammad Mahmudul Alam,Edward Raff,Tim Oates,James Holt*

Main category: cs.LG

TL;DR: 提出基于向量符号架构(VSA)的多示例学习框架，通过高维向量和代数运算直接编码MIL的iff约束，在保持严格MIL公式的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法违反MIL的iff约束(包为正当且仅当至少一个实例为正)，导致性能指标虚高和泛化能力差，需要严格遵循MIL假设的方法。

Method: 使用VSA将实例和概念表示为近似正交的高维向量，通过代数运算在分类时强制执行iff约束；设计学习编码器将原始数据转换为VSA兼容向量；包含VSA驱动的MaxNetwork分类器。

Result: 在标准MIL基准和医学影像数据集上实现了有效MIL模型的SOTA结果，优于现有方法，同时严格遵循MIL公式。

Conclusion: 为依赖学习启发式的现有MIL方法提供了原则性、可解释且有效的替代方案。

Abstract: Multiple Instance Learning (MIL) tasks impose a strict logical constraint: a bag is labeled positive if and only if at least one instance within it is positive. While this iff constraint aligns with many real-world applications, recent work has shown that most deep learning-based MIL approaches violate it, leading to inflated performance metrics and poor generalization. We propose a novel MIL framework based on Vector Symbolic Architectures (VSAs), which provide a differentiable mechanism for performing symbolic operations in high-dimensional space. Our method encodes the MIL assumption directly into the model's structure by representing instances and concepts as nearly orthogonal high-dimensional vectors and using algebraic operations to enforce the iff constraint during classification. To bridge the gap between raw data and VSA representations, we design a learned encoder that transforms input instances into VSA-compatible vectors while preserving key distributional properties. Our approach, which includes a VSA-driven MaxNetwork classifier, achieves state-of-the-art results for a valid MIL model on standard MIL benchmarks and medical imaging datasets, outperforming existing methods while maintaining strict adherence to the MIL formulation. This work offers a principled, interpretable, and effective alternative to existing MIL approaches that rely on learned heuristics.

</details>


### [12] [A Robust Federated Learning Approach for Combating Attacks Against IoT Systems Under non-IID Challenges](https://arxiv.org/abs/2511.16822)
*Eyad Gad,Zubair Md Fadlullah,Mostafa M. Fouda*

Main category: cs.LG

TL;DR: 本文比较了FedAvg、FedProx和Scaffold三种联邦学习算法在不同数据分布下对物联网攻击检测的性能，特别关注统计异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备激增和数据量暴涨，传统机器学习面临资源受限和隐私安全问题。联邦学习虽然能解决这些问题，但非独立同分布数据的统计异质性严重影响了其效果。现有研究缺乏对联邦学习方法在物联网攻击检测中处理统计异质性的全面比较。

Method: 使用CICIoT2023数据集对大规模物联网攻击进行分类，系统评估FedAvg、FedProx和Scaffold三种联邦学习算法在不同数据分布下的表现。

Result: 通过细致分析和实验，揭示了这些联邦学习方法在统计异质性环境下的性能差异和特点。

Conclusion: 研究为物联网安全领域的联邦学习应用提供了有价值的见解，帮助研究者和从业者更好地应对统计异质性挑战。

Abstract: In the context of the growing proliferation of user devices and the concurrent surge in data volumes, the complexities arising from the substantial increase in data have posed formidable challenges to conventional machine learning model training. Particularly, this is evident within resource-constrained and security-sensitive environments such as those encountered in networks associated with the Internet of Things (IoT). Federated Learning has emerged as a promising remedy to these challenges by decentralizing model training to edge devices or parties, effectively addressing privacy concerns and resource limitations. Nevertheless, the presence of statistical heterogeneity in non-Independently and Identically Distributed (non-IID) data across different parties poses a significant hurdle to the effectiveness of FL. Many FL approaches have been proposed to enhance learning effectiveness under statistical heterogeneity. However, prior studies have uncovered a gap in the existing research landscape, particularly in the absence of a comprehensive comparison between federated methods addressing statistical heterogeneity in detecting IoT attacks. In this research endeavor, we delve into the exploration of FL algorithms, specifically FedAvg, FedProx, and Scaffold, under different data distributions. Our focus is on achieving a comprehensive understanding of and addressing the challenges posed by statistical heterogeneity. In this study, We classify large-scale IoT attacks by utilizing the CICIoT2023 dataset. Through meticulous analysis and experimentation, our objective is to illuminate the performance nuances of these FL methods, providing valuable insights for researchers and practitioners in the domain.

</details>


### [13] [Monte Carlo Expected Threat (MOCET) Scoring](https://arxiv.org/abs/2511.16823)
*Joseph Kim,Saahith Potluri*

Main category: cs.LG

TL;DR: MOCET是一个可解释且双重可扩展的指标，用于量化AI模型在生物安全等领域的现实世界风险。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标如LAB-Bench、BioLP-bench和WMDP能够可靠评估模型提升和领域知识，但缺乏能够更好衡量"现实世界风险"的指标，且需要可扩展的开放式指标来跟上AI的快速发展。

Method: 提出了MOCET指标，该指标具有可解释性和双重可扩展性（可自动化和开放式），能够量化现实世界风险。

Result: MOCET能够解决现有指标在衡量现实世界风险和可扩展性方面的不足。

Conclusion: MOCET为LLM安全案例提供了更好的风险评估工具，能够跟上AI技术的快速发展。

Abstract: Evaluating and measuring AI Safety Level (ASL) threats are crucial for guiding stakeholders to implement safeguards that keep risks within acceptable limits. ASL-3+ models present a unique risk in their ability to uplift novice non-state actors, especially in the realm of biosecurity. Existing evaluation metrics, such as LAB-Bench, BioLP-bench, and WMDP, can reliably assess model uplift and domain knowledge. However, metrics that better contextualize "real-world risks" are needed to inform the safety case for LLMs, along with scalable, open-ended metrics to keep pace with their rapid advancements. To address both gaps, we introduce MOCET, an interpretable and doubly-scalable metric (automatable and open-ended) that can quantify real-world risks.

</details>


### [14] [ManifoldFormer: Geometric Deep Learning for Neural Dynamics on Riemannian Manifolds](https://arxiv.org/abs/2511.16828)
*Yihang Fu,Lifang He,Qingyu Chen*

Main category: cs.LG

TL;DR: ManifoldFormer是一个几何深度学习框架，通过显式学习神经流形表示来解决现有EEG基础模型忽略神经动态内在几何结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型将神经信号视为欧几里得空间中的通用时间序列，忽略了限制大脑活动到低维流形的神经动态内在几何结构，这种模型假设与神经几何之间的不匹配限制了表示质量和跨被试泛化能力。

Method: 集成三个关键创新：用于保持几何结构的黎曼VAE进行流形嵌入、在神经流形上直接操作的具有测地线感知注意力机制的几何Transformer，以及利用神经ODE进行流形约束时间演化的动态预测器。

Result: 在四个公共数据集上的广泛评估显示，相比最先进方法有显著改进：准确率提高4.6-4.8%，Cohen's Kappa提高6.2-10.2%，同时保持强大的跨被试泛化能力。

Conclusion: 几何方法揭示了与神经生理学原理一致的有意义神经模式，确立了几何约束对于有效EEG基础模型的重要性。

Abstract: Existing EEG foundation models mainly treat neural signals as generic time series in Euclidean space, ignoring the intrinsic geometric structure of neural dynamics that constrains brain activity to low-dimensional manifolds. This fundamental mismatch between model assumptions and neural geometry limits representation quality and cross-subject generalization. ManifoldFormer addresses this limitation through a novel geometric deep learning framework that explicitly learns neural manifold representations. The architecture integrates three key innovations: a Riemannian VAE for manifold embedding that preserves geometric structure, a geometric Transformer with geodesic-aware attention mechanisms operating directly on neural manifolds, and a dynamics predictor leveraging neural ODEs for manifold-constrained temporal evolution. Extensive evaluation across four public datasets demonstrates substantial improvements over state-of-the-art methods, with 4.6-4.8% higher accuracy and 6.2-10.2% higher Cohen's Kappa, while maintaining robust cross-subject generalization. The geometric approach reveals meaningful neural patterns consistent with neurophysiological principles, establishing geometric constraints as essential for effective EEG foundation models.

</details>


### [15] [Analysis of heart failure patient trajectories using sequence modeling](https://arxiv.org/abs/2511.16839)
*Falk Dippela,Yinan Yu,Annika Rosengren,Martin Lindgren,Christina E. Lundberg,Erik Aerts,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 本文系统比较了六种序列模型（包括Transformers、Transformers++和Mambas）在瑞典心力衰竭队列中的表现，发现Llama在预测性能、校准和鲁棒性方面表现最佳，Mambas次之。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer和Mamba架构在临床预测任务中表现出色，但医学领域缺乏系统分析模型性能和效率的方法。本文旨在填补这一空白。

Method: 在42820名瑞典心力衰竭患者队列中，使用诊断、生命体征、实验室检查、药物和程序等EHR数据，评估六种序列模型在三个一年预测任务上的表现，并进行输入序列、模型配置和时间预处理技术的消融分析。

Result: Llama获得最高的预测区分度、最佳校准和跨任务鲁棒性，其次是Mambas。两种架构都展示了高效的表征学习能力，小配置模型超越其他大型Transformer。在相同模型大小下，Llama和Mambas使用25%更少的训练数据就能达到优越性能。

Conclusion: 本研究首次提供了输入标记化、模型配置和时间数据预处理的系统消融分析，为未来基于EHR的临床预测模型开发提供了起点建议。

Abstract: Transformers have defined the state-of-the-art for clinical prediction tasks involving electronic health records (EHRs). The recently introduced Mamba architecture outperformed an advanced Transformer (Transformer++) based on Llama in handling long context lengths, while using fewer model parameters. Despite the impressive performance of these architectures, a systematic approach to empirically analyze model performance and efficiency under various settings is not well established in the medical domain. The performances of six sequence models were investigated across three architecture classes (Transformers, Transformers++, Mambas) in a large Swedish heart failure (HF) cohort (N = 42820), providing a clinically relevant case study. Patient data included diagnoses, vital signs, laboratories, medications and procedures extracted from in-hospital EHRs. The models were evaluated on three one-year prediction tasks: clinical instability (a readmission phenotype) after initial HF hospitalization, mortality after initial HF hospitalization and mortality after latest hospitalization. Ablations account for modifications of the EHR-based input patient sequence, architectural model configurations, and temporal preprocessing techniques for data collection. Llama achieves the highest predictive discrimination, best calibration, and showed robustness across all tasks, followed by Mambas. Both architectures demonstrate efficient representation learning, with tiny configurations surpassing other large-scaled Transformers. At equal model size, Llama and Mambas achieve superior performance using 25% less training data. This paper presents a first ablation study with systematic design choices for input tokenization, model configuration and temporal data preprocessing. Future model development in clinical prediction tasks using EHRs could build upon this study's recommendation as a starting point.

</details>


### [16] [Provably Minimum-Length Conformal Prediction Sets for Ordinal Classification](https://arxiv.org/abs/2511.16845)
*Zijian Zhang,Xinyu Chen,Yuanjie Shi,Liyuan Lillian Ma,Zifan Xu,Yan Yan*

Main category: cs.LG

TL;DR: 提出了一种新的序数分类的共形预测方法，该方法模型无关且提供实例级最优预测区间，通过滑动窗口算法实现高效预测，显著提高了预测效率。


<details>
  <summary>Details</summary>
Motivation: 现有序数共形预测方法主要关注启发式算法或要求基础模型预测单峰分布，限制了覆盖效率权衡的洞察，缺乏模型无关和分布无关的特性。

Method: 将序数共形分类建模为实例级最小长度覆盖问题，开发线性时间复杂度的滑动窗口算法，并提出长度正则化变体以缩小预测集大小同时保持覆盖。

Result: 在四个不同领域的基准数据集上的实验表明，所提方法相比基线平均降低了15%的预测集大小，显著提高了预测效率。

Conclusion: 该方法填补了序数共形预测的空白，提供了模型无关、实例级最优的预测区间，在保持统计有效性的同时显著提升了预测效率。

Abstract: Ordinal classification has been widely applied in many high-stakes applications, e.g., medical imaging and diagnosis, where reliable uncertainty quantification (UQ) is essential for decision making. Conformal prediction (CP) is a general UQ framework that provides statistically valid guarantees, which is especially useful in practice. However, prior ordinal CP methods mainly focus on heuristic algorithms or restrictively require the underlying model to predict a unimodal distribution over ordinal labels. Consequently, they provide limited insight into coverage-efficiency trade-offs, or a model-agnostic and distribution-free nature favored by CP methods. To this end, we fill this gap by propose an ordinal-CP method that is model-agnostic and provides instance-level optimal prediction intervals. Specifically, we formulate conformal ordinal classification as a minimum-length covering problem at the instance level. To solve this problem, we develop a sliding-window algorithm that is optimal on each calibration data, with only a linear time complexity in K, the number of label candidates. The local optimality per instance further also improves predictive efficiency in expectation. Moreover, we propose a length-regularized variant that shrinks prediction set size while preserving coverage. Experiments on four benchmark datasets from diverse domains are conducted to demonstrate the significantly improved predictive efficiency of the proposed methods over baselines (by 15% decrease on average over four datasets).

</details>


### [17] [Sex and age determination in European lobsters using AI-Enhanced bioacoustics](https://arxiv.org/abs/2511.16848)
*Feliciano Pedro Francisco Domingos,Isibor Kennedy Ihianle,Omprakash Kaiwartya,Ahmad Lotfi,Nicola Khan,Nicholas Beaudreau,Amaya Albalat,Pedro Machado*

Main category: cs.LG

TL;DR: 使用被动声学监测和AI模型对欧洲龙虾进行年龄和性别分类，准确率超过97%


<details>
  <summary>Details</summary>
Motivation: 监测难以观察的水生物种如龙虾面临挑战，了解其栖息地、福利、繁殖、性别和年龄对管理和保护至关重要

Method: 使用深度学习模型（1D-CNN、1D-DCNN）和六种机器学习模型（SVM、k-NN、朴素贝叶斯、随机森林、XGBoost、MLP），以梅尔频率倒谱系数为特征

Result: 年龄分类准确率超过97%，性别分类除朴素贝叶斯外均超过93.23%

Conclusion: 研究展示了监督机器学习和深度学习从龙虾声音中提取年龄和性别相关特征的潜力，为龙虾保护和水产养殖提供了有前景的非侵入性监测方法

Abstract: Monitoring aquatic species, especially elusive ones like lobsters, presents challenges. This study focuses on Homarus gammarus (European lobster), a key species for fisheries and aquaculture, and leverages non-invasive Passive Acoustic Monitoring (PAM). Understanding lobster habitats, welfare, reproduction, sex, and age is crucial for management and conservation. While bioacoustic emissions have classified various aquatic species using Artificial Intelligence (AI) models, this research specifically uses H. gammarus bioacoustics (buzzing/carapace vibrations) to classify lobsters by age (juvenile/adult) and sex (male/female).
  The dataset was collected at Johnshaven, Scotland, using hydrophones in concrete tanks. We explored the efficacy of Deep Learning (DL) models (1D-CNN, 1D-DCNN) and six Machine Learning (ML) models (SVM, k-NN, Naive Bayes, Random Forest, XGBoost, MLP). Mel-frequency cepstral coefficients (MFCCs) were used as features.
  For age classification (adult vs. juvenile), most models achieved over 97% accuracy (Naive Bayes: 91.31%). For sex classification, all models except Naive Bayes surpassed 93.23%. These strong results demonstrate the potential of supervised ML and DL to extract age- and sex-related features from lobster sounds. This research offers a promising non-invasive PAM approach for lobster conservation, detection, and management in aquaculture and fisheries, enabling real-world edge computing applications for underwater species.

</details>


### [18] [Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks](https://arxiv.org/abs/2511.16849)
*Leonardo Pepino,Pablo Riera,Juan Kamienkowski,Luciana Ferrer*

Main category: cs.LG

TL;DR: 研究发现，在听觉领域，自监督音频模型的任务性能与大脑表征相似性呈正相关，且这种大脑相似性在预训练过程中会自然涌现。


<details>
  <summary>Details</summary>
Motivation: 探讨人工神经网络在提高任务性能的同时，其内部表征是否也更接近大脑信号，特别是在听觉领域。

Method: 使用36种不同音频模型，通过体素回归、成分回归和表征相似性分析，评估模型内部表征与两个独立fMRI数据集中大脑活动的对齐程度。

Result: 发现性能更好的自监督音频模型能更准确地预测听觉皮层活动，模型整体任务性能与大脑表征对齐度呈强正相关（r>0.7）。

Conclusion: 大脑相似表征可以从自然音频数据重建缺失信息的学习过程中自然涌现，无需显式优化。

Abstract: Artificial neural networks (ANNs) are increasingly powerful models of brain computation, yet it remains unclear whether improving their task performance also makes their internal representations more similar to brain signals. To address this question in the auditory domain, we quantified the alignment between the internal representations of 36 different audio models and brain activity from two independent fMRI datasets. Using voxel-wise and component-wise regression, and representation similarity analysis (RSA), we found that recent self-supervised audio models with strong performance in diverse downstream tasks are better predictors of auditory cortex activity than older and more specialized models. To assess the quality of the audio representations, we evaluated these models in 6 auditory tasks from the HEAREval benchmark, spanning music, speech, and environmental sounds. This revealed strong positive Pearson correlations ($r>0.7$) between a model's overall task performance and its alignment with brain representations. Finally, we analyzed the evolution of the similarity between audio and brain representations during the pretraining of EnCodecMAE. We discovered that brain similarity increases progressively and emerges early during pretraining, despite the model not being explicitly optimized for this objective. This suggests that brain-like representations can be an emergent byproduct of learning to reconstruct missing information from naturalistic audio data.

</details>


### [19] [The use of vocal biomarkers in the detection of Parkinson's disease: a robust statistical performance comparison of classic machine learning models](https://arxiv.org/abs/2511.16856)
*Katia Pires Nascimento do Sacramento,Elliot Q. C. Garcia,Nicéias Silva Vilela,Vinicius P. Sacramento,Tiago A. E. Ferreira*

Main category: cs.LG

TL;DR: 本研究评估了深度神经网络（DNN）与传统机器学习方法在使用语音生物标志物区分帕金森病患者与健康对照方面的有效性，结果显示DNN在准确性和效率上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期常伴有语音障碍，使用语音生物标志物进行早期诊断具有非侵入性、低成本且易于获取的优势，本研究旨在验证DNN在此任务中的有效性。

Method: 使用两个公开语音数据集，提取梅尔频率倒谱系数（MFCCs），采用1000次独立随机执行的验证策略评估模型鲁棒性，并使用非参数检验比较不同分类模型的性能。

Result: DNN在意大利语音数据集和帕金森远程监测数据集上的平均准确率分别为98.65%和92.11%，显著优于传统机器学习模型。

Conclusion: 研究证实了DNN在利用语音生物标志物进行神经退行性疾病早期检测方面的效率和潜力，能够提供更高的准确性和可靠性。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative disorder that, in addition to directly impairing functional mobility, is frequently associated with vocal impairments such as hypophonia and dysarthria, which typically manifest in the early stages. The use of vocal biomarkers to support the early diagnosis of PD presents a non-invasive, low-cost, and accessible alternative in clinical settings. Thus, the objective of this cross-sectional study was to consistently evaluate the effectiveness of a Deep Neural Network (DNN) in distinguishing individuals with Parkinson's disease from healthy controls, in comparison with traditional Machine Learning (ML) methods, using vocal biomarkers. Two publicly available voice datasets were used. Mel-frequency cepstral coefficients (MFCCs) were extracted from the samples, and model robustness was assessed using a validation strategy with 1000 independent random executions. Performance was evaluated using classification statistics. Since normality assumptions were not satisfied, non-parametric tests (Kruskal-Wallis and Bonferroni post-hoc tests) were applied to verify whether the tested classification models were similar or different in the classification of PD. With an average accuracy of $98.65\%$ and $92.11\%$ on the Italian Voice dataset and Parkinson's Telemonitoring dataset, respectively, the DNN demonstrated superior performance and efficiency compared to traditional ML models, while also achieving competitive results when benchmarked against relevant studies. Overall, this study confirms the efficiency of DNNs and emphasizes their potential to provide greater accuracy and reliability for the early detection of neurodegenerative diseases using voice-based biomarkers.

</details>


### [20] [Topologic Attention Networks: Attending to Direct and Indirect Neighbors through Gaussian Belief Propagation](https://arxiv.org/abs/2511.16871)
*Marshall Rosenhoover,Huaming Zhang*

Main category: cs.LG

TL;DR: 提出了拓扑注意力网络，通过概率机制学习信息在图中的直接和间接连接中的传播方式，解决了图神经网络长距离依赖建模问题，在多个基准模型上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络依赖局部消息传递，难以建模图中的长距离依赖关系。现有方法通过连续时间动态或密集自注意力扩展范围，但计算成本高且可扩展性有限。

Method: 提出拓扑注意力网络框架，应用拓扑注意力机制，这是一种概率机制，学习信息如何通过图的直接和间接连接流动。与传统依赖显式成对交互的注意力不同，拓扑注意力从图的学得信息传播中产生。

Result: 在所有测量的基准模型上实现了最先进的性能。

Conclusion: 拓扑注意力网络提供了一种统一推理局部和全局关系的方法，解决了图神经网络的长距离依赖建模问题，具有更好的计算效率和可扩展性。

Abstract: Graph Neural Networks rely on local message passing, which limits their ability to model long-range dependencies in graphs. Existing approaches extend this range through continuous-time dynamics or dense self-attention, but both suffer from high computational cost and limited scalability. We propose Topologic Attention Networks, a new framework that applies topologic attention, a probabilistic mechanism that learns how information should flow through both direct and indirect connections in a graph. Unlike conventional attention that depends on explicit pairwise interactions, topologic attention emerges from the learned information propagation of the graph, enabling unified reasoning over local and global relationships. This method achieves provides state-of-the-art performance across all measured baseline models. Our implementation is available at https://github.com/Marshall-Rosenhoover/Topologic-Attention-Networks.

</details>


### [21] [PersonalizedRouter: Personalized LLM Routing via Graph-based User Preference Modeling](https://arxiv.org/abs/2511.16883)
*Zhongjie Dai,Tao Feng,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出PersonalizedRouter框架，通过图结构建模用户偏好，实现个性化LLM选择，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM选择方法通常优化单一目标（如性能或成本），无法从交互数据中学习个体用户偏好

Method: 将交互数据转换为异构图，建模用户查询与最优LLM之间的上下文关系，使用多成本效率模拟和LLM-as-a-Judge策略评估

Result: 在两个模拟策略下分别超越最强方法15.38%和9.83%，在1000用户基准上分别提升16.19%和59.69%，同时保持更高效率

Conclusion: PersonalizedRouter在个性化LLM选择方面表现优异，具有强大的少样本泛化能力，适应新用户和新LLM时分别达到全训练模型性能的64.81%和85.80%

Abstract: The growing number of Large Language Models (LLMs) with diverse capabilities and response styles provides users with a wider range of choices, which presents challenges in selecting appropriate LLMs, as user preferences vary in terms of performance, cost, and response style. Current LLM selection methods typically optimize for a single fixed objective, such as performance, cost, or a trade-off between them, and fail to learn individual user preferences from interaction data. To address these limitations, we propose PersonalizedRouter, a graph-based framework that models diverse user profiles and performs personalized LLM selection by leveraging interaction data that includes task context, queries, candidate LLMs, and user decisions. To capture contextual information between user queries and optimal LLMs, PersonalizedRouter converts the interaction data into a heterogeneous graph, where the relationships between different types of nodes are represented by edges. To evaluate adaptability across users, we design two strategies: the multi-cost-efficiency simulation strategy and the LLM-as-a-Judge strategy. In addition, we construct PersonaRoute-Bench, a large-scale benchmark with 1,000 simulated users and 10 LLMs. Experimental results show that PersonalizedRouter significantly outperforms existing LLM selection methods and surpasses the strongest methods by a large margin of 15.38% and 9.83% under two simulation strategies. On the PersonaRoute-Bench with 1,000 users, it further surpasses the best methods by 16.19% and 59.69% while maintaining higher efficiency. Moreover, PersonalizedRouter demonstrates strong few-shot generalization, achieving 64.81% and 85.80% of the fully trained model's performance when adapting to new users and new LLMs.

</details>


### [22] [Predicting Talent Breakout Rate using Twitter and TV data](https://arxiv.org/abs/2511.16905)
*Bilguun Batsaikhan,Hiroyuki Fukuda*

Main category: cs.LG

TL;DR: 本文提出了一种结合Twitter和电视数据预测日本艺人走红的方法，通过比较传统时间序列模型、神经网络和集成学习方法，发现集成学习在标准回归指标上表现最好，但神经网络在预测艺人走红方面具有更高的精确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 早期发现潜在人才在广告领域至关重要，研究旨在探索结合社交媒体和电视数据预测艺人走红的有效性。

Method: 实验比较了传统时间序列模型、神经网络和集成学习方法，利用Twitter和电视数据预测社交数据的时间变化。

Result: 集成学习方法在标准回归指标上优于传统和神经网络模型，但神经网络在预测艺人走红方面表现更好，具有更高的精确率和召回率。

Conclusion: 结合社交媒体和电视数据可以有效预测艺人走红，神经网络模型在识别人才爆发方面具有优势，尽管集成学习在一般回归任务上表现更好。

Abstract: Early detection of rising talents is of paramount importance in the field of advertising. In this paper, we define a concept of talent breakout and propose a method to detect Japanese talents before their rise to stardom. The main focus of the study is to determine the effectiveness of combining Twitter and TV data on predicting time-dependent changes in social data. Although traditional time-series models are known to be robust in many applications, the success of neural network models in various fields (e.g.\ Natural Language Processing, Computer Vision, Reinforcement Learning) continues to spark an interest in the time-series community to apply new techniques in practice. Therefore, in order to find the best modeling approach, we have experimented with traditional, neural network and ensemble learning methods. We observe that ensemble learning methods outperform traditional and neural network models based on standard regression metrics. However, by utilizing the concept of talent breakout, we are able to assess the true forecasting ability of the models, where neural networks outperform traditional and ensemble learning methods in terms of precision and recall.

</details>


### [23] [PepEVOLVE: Position-Aware Dynamic Peptide Optimization via Group-Relative Advantage](https://arxiv.org/abs/2511.16912)
*Trieu Nguyen,Hao-Wei Pang,Shasha Feng*

Main category: cs.LG

TL;DR: PepEVOLVE是一个动态框架，用于多目标优化大环肽，无需预先指定突变位置，通过动态掩码、多臂老虎机路由器和进化优化算法，在Rev结合大环肽基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大环肽具有类似生物制剂的亲和力和类似小分子的可开发性，但其巨大的组合空间和多参数目标使得先导优化缓慢且具有挑战性。现有方法需要预先指定突变位置，且依赖静态预训练和优化算法，限制了模型的泛化能力和优化效果。

Method: PepEVOLVE框架：(i) 通过动态掩码和CHUCKLES移位增强预训练以改善泛化；(ii) 使用上下文无关的多臂老虎机路由器发现高奖励残基；(iii) 结合新颖的进化优化算法和组相对优势来稳定强化学习更新。

Result: 在Rev结合大环肽基准测试中，PepEVOLVE优于PepINVENT：达到更高的平均分数（约0.8 vs 0.6），获得最佳候选分数0.95（vs 0.87），并在优化渗透性和亲脂性时以更少步骤收敛。

Conclusion: PepEVOLVE为肽先导优化提供了一条实用、可复现的路径，当最优编辑位点未知时，能够更有效地探索并提高多目标设计质量。

Abstract: Macrocyclic peptides are an emerging modality that combines biologics-like affinity with small-molecule-like developability, but their vast combinatorial space and multi-parameter objectives make lead optimization slow and challenging. Prior generative approaches such as PepINVENT require chemists to pre-specify mutable positions for optimization, choices that are not always known a priori, and rely on static pretraining and optimization algorithms that limit the model's ability to generalize and effectively optimize peptide sequences. We introduce PepEVOLVE, a position-aware, dynamic framework that learns both where to edit and how to dynamically optimize peptides for multi-objective improvement. PepEVOLVE (i) augments pretraining with dynamic masking and CHUCKLES shifting to improve generalization, (ii) uses a context-free multi-armed bandit router that discovers high-reward residues, and (iii) couples a novel evolving optimization algorithm with group-relative advantage to stabilize reinforcement updates. During in silico evaluations, the router policy reliably learns and concentrates probability on chemically meaningful sites that influence the peptide's properties. On a therapeutically motivated Rev-binding macrocycle benchmark, PepEVOLVE outperformed PepINVENT by reaching higher mean scores (approximately 0.8 vs. 0.6), achieving best candidates with a score of 0.95 (vs. 0.87), and converging in fewer steps under the task of optimizing permeability and lipophilicity with structural constraints. Overall, PepEVOLVE offers a practical, reproducible path to peptide lead optimization when optimal edit sites are unknown, enabling more efficient exploration and improving design quality across multiple objectives.

</details>


### [24] [A Hybrid Computational Intelligence Framework for scRNA-seq Imputation: Integrating scRecover and Random Forests](https://arxiv.org/abs/2511.16923)
*Ali Anaissi,Deshao Liu,Yuanzhe Jia,Weidong Huang,Widad Alyassine,Junaid Akram*

Main category: cs.LG

TL;DR: SCR-MF是一个用于单细胞RNA测序数据缺失值填补的两阶段工作流程，结合了scRecover的缺失检测和missForest的非参数填补方法，在保持生物学保真度的同时提供稳健性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序存在普遍的缺失事件，这些事件会掩盖生物信号，需要有效的填补方法来恢复数据质量。

Method: 采用模块化两阶段工作流程：第一阶段使用scRecover进行原理性缺失检测，第二阶段使用missForest进行稳健的非参数填补。

Result: 在公共和模拟数据集上，SCR-MF在大多数情况下达到或超过现有填补方法的性能，同时保持生物保真度和透明度。运行时间分析显示其在准确性和计算效率之间达到良好平衡。

Conclusion: SCR-MF适用于中等规模单细胞数据集，提供了稳健、可解释且计算效率高的缺失值填补解决方案。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables transcriptomic profiling at cellular resolution but suffers from pervasive dropout events that obscure biological signals. We present SCR-MF, a modular two-stage workflow that combines principled dropout detection using scRecover with robust non-parametric imputation via missForest. Across public and simulated datasets, SCR-MF achieves robust and interpretable performance comparable to or exceeding existing imputation methods in most cases, while preserving biological fidelity and transparency. Runtime analysis demonstrates that SCR-MF provides a competitive balance between accuracy and computational efficiency, making it suitable for mid-scale single-cell datasets.

</details>


### [25] [CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection](https://arxiv.org/abs/2511.16929)
*Rui Xue,Dan He,Fengmei Jin,Chen Zhang,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 提出了CroTad框架，一种基于对比强化学习的在线轨迹异常检测方法，无需阈值设置且对噪声和不规则采样数据具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹异常检测中存在的三个关键挑战：子轨迹异常检测研究不足、现有方法依赖阈值调优、不规则采样和训练集噪声影响模型性能。

Method: 结合对比学习和强化学习，通过对比学习提取不同行程的正常旅行模式，利用深度强化学习进行在线实时异常评分。

Result: 在两个真实世界数据集上的广泛实验证明了该框架在各种评估场景下的有效性和鲁棒性。

Conclusion: CroTad框架能够有效区分子轨迹和点级别的异常行为，实现及时和细粒度的异常检测。

Abstract: Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.

</details>


### [26] [A novel approach to classification of ECG arrhythmia types with latent ODEs](https://arxiv.org/abs/2511.16933)
*Angelina Yan,Matt L. Sampson,Peter Melchior*

Main category: cs.LG

TL;DR: 提出一种端到端的心电图分类方法，使用潜在ODE建模连续ECG波形，通过降采样创建鲁棒特征向量，在保持高性能的同时解决可穿戴设备采样频率与电池寿命的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决12导联ECG短期监测无法捕捉间歇性事件，以及可穿戴ECG因电池限制导致采样频率不规则、形态分析困难的问题。

Method: 训练潜在ODE模型连续ECG波形，从360Hz单通道信号创建鲁棒特征向量，通过降采样到90Hz和45Hz构建三个潜在向量，使用梯度提升树进行分类。

Result: 在不同采样频率下性能下降极小，宏平均AUC-ROC值在360Hz、90Hz和45Hz下分别为0.984、0.978和0.976。

Conclusion: 该方法能够在保持高性能的同时绕过信号保真度与电池寿命的权衡，使更小的可穿戴设备成为可能，促进长期心脏健康监测。

Abstract: 12-lead ECGs with high sampling frequency are the clinical gold standard for arrhythmia detection, but their short-term, spot-check nature often misses intermittent events. Wearable ECGs enable long-term monitoring but suffer from irregular, lower sampling frequencies due to battery constraints, making morphology analysis challenging. We present an end-to-end classification pipeline to address these issues. We train a latent ODE to model continuous ECG waveforms and create robust feature vectors from high-frequency single-channel signals. We construct three latent vectors per waveform via downsampling the initial 360 Hz ECG to 90 Hz and 45 Hz. We then use a gradient boosted tree to classify these vectors and test robustness across frequencies. Performance shows minimal degradation, with macro-averaged AUC-ROC values of 0.984, 0.978, and 0.976 at 360 Hz, 90 Hz, and 45 Hz, respectively, suggesting a way to sidestep the trade-off between signal fidelity and battery life. This enables smaller wearables, promoting long-term monitoring of cardiac health.

</details>


### [27] [ToC: Tree-of-Claims Search with Multi-Agent Language Models](https://arxiv.org/abs/2511.16972)
*Shuyang Yu,Jianan Liang,Hui Hu*

Main category: cs.LG

TL;DR: 提出了Tree of Claims (ToC)框架，将专利权利要求编辑重新定义为引导搜索问题，结合蒙特卡洛树搜索和多智能体系统，显著提升了权利要求优化的效果。


<details>
  <summary>Details</summary>
Motivation: 专利权利要求优化是关键但具有挑战性的任务，需要平衡新颖性和法律范围。手动起草权利要求劳动密集、成本高且不一致，而传统大语言模型缺乏结构化、迭代推理能力。

Method: ToC框架整合了蒙特卡洛树搜索(MCTS)与协作多智能体系统，包括基于LLM的编辑智能体提出上下文编辑建议，以及审查智能体通过结构化思维链分析新颖性和现有技术披露。

Result: 在1145个权利要求基准测试中，ToC在零样本和少样本场景下显著优于标准LLM，平均综合得分提升8%，某些情况下可达9%。消融实验验证了ToC的有效性。

Conclusion: ToC建立了一个透明、可控且可解释的方法论，有效结合了先进LLM推理能力与战略MCTS规划，用于结构化专利权利要求优化。

Abstract: Optimizing patent claims is a critical yet challenging task, demanding careful balance between maximizing novelty and preserving legal scope. Manual claim drafting is labor-intensive, costly, and inherently inconsistent, while conventional Large Language Models (LLMs) often lack the structured, iterative reasoning essential for precise claim refinement. To address these challenges, we introduce Tree of Claims (ToC), an innovative framework that redefines claim editing as a guided search problem. ToC synergistically integrates Monte Carlo Tree Search (MCTS) with a collaborative multi-agent system, comprising an LLM-based EditorAgent that proposes contextually grounded edits, and an ExaminerAgent that mimics patent examiner critiques through structured, chain-of-thought analyses of novelty and prior art disclosure. Driven by a carefully designed multi-objective reward function, ToC jointly optimizes novelty, scope retention, and semantic coherence. Experimental evaluation on a benchmark of 1145 claims demonstrates that ToC significantly outperforms standard LLMs in zero-shot and few-shot scenarios, achieving an average composite score improvement of 8\%, and up to 9\% in certain cases. Extensive experiments, including detailed ablation studies, validate ToC's efficacy in generating superior, legally robust claim revisions. Overall, ToC establishes a transparent, controllable, and interpretable methodology that effectively bridges advanced LLM reasoning capabilities with strategic MCTS planning for structured patent claim optimization.The source code is available at https://github.com/ysy2003/ToC.

</details>


### [28] [Gradient flow for deep equilibrium single-index models](https://arxiv.org/abs/2511.16976)
*Sanjit Dandapanthula,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 本文研究了深度平衡模型(DEQs)在梯度下降训练中的理论动态特性，在线性模型和单指标模型设置下证明了参数守恒定律、梯度流良好条件性以及线性收敛到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 深度平衡模型在实践上取得了成功，但在理论上理解其梯度下降动态仍是一个活跃的研究领域，特别是在线性模型和单指标模型设置下存在理论空白。

Method: 通过理论分析证明了线性DEQs的参数守恒定律，表明参数在训练过程中保持在球面上；证明了梯度流在所有时间都保持良好条件；在线性DEQs和深度平衡单指标模型中证明了梯度下降的线性收敛性。

Result: 建立了线性DEQs的参数守恒定律，证明了梯度流在所有时间都保持良好条件，在线性DEQs和深度平衡单指标模型中证明了梯度下降以线性速率收敛到全局最优解。

Conclusion: 本文填补了DEQs梯度下降动态理论分析的空白，为理解无限深度权重共享网络的训练行为提供了理论基础，并通过实验验证了理论发现。

Abstract: Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.

</details>


### [29] [FIRM: Federated In-client Regularized Multi-objective Alignment for Large Language Models](https://arxiv.org/abs/2511.16992)
*Fatemeh,Nourzad,Amirhossein Roknilamouki,Eylem Ekici,Jia,Liu,Ness B. Shroff*

Main category: cs.LG

TL;DR: FIRM是一种联邦多目标对齐算法，通过客户端内正则化解决多目标优化问题，实现通信效率和客户端分歧漂移缓解。


<details>
  <summary>Details</summary>
Motivation: 传统联邦多目标优化方法存在严重通信瓶颈，需要传输多个梯度，不适合大型模型。同时集中化训练存在数据隐私问题。

Method: 每个客户端本地解决正则化多目标优化问题，通过客户端内正则化直接缓解客户端分歧漂移，只需传输单个参数集。

Result: FIRM实现更平滑的训练动态、减少客户端分歧漂移、改善奖励权衡，并能根据偏好平滑调整目标间权衡。

Conclusion: FIRM在联邦多目标对齐中实现了通信效率和收敛保证，是解决大型语言模型对齐问题的有效方法。

Abstract: Aligning Large Language Models (LLMs) with human values often involves balancing multiple, conflicting objectives such as helpfulness and harmlessness. Training these models is computationally intensive, and centralizing the process raises significant data privacy concerns. Federated Learning (FL) offers a compelling alternative, but existing Federated Multi-Objective Optimization (FMOO) methods face severe communication bottlenecks as their reliance on transmitting multiple gradients to a server is unscalable for large models. We introduce FIRM (Federated In-client Regularized Multi-objective alignment), a novel algorithm that achieves both client disagreement drift mitigation and communication efficiency. In FIRM, each client locally solves a regularized multi-objective optimization problem. By directly mitigating client disagreement drift through in-client regularization, our method eliminates the need for the multi-gradient transmissions common in prior works. Consequently, clients need only to transmit a single set of adapted parameters, maintaining high communication efficiency. We prove that our algorithm converges to Pareto-stationary points and, to our knowledge, provide the first finite-time convergence guarantees for this federated multi-objective alignment setting. Empirically, we show that FIRM leads to smoother training dynamics, reduced client disagreement drift, and improved reward trade-offs compared to baselines. We further propose a method to incorporate a preference over the objectives and report empirical Pareto plots, demonstrating that FIRM can smoothly adapt trade-offs between objectives in response to specified preferences.

</details>


### [30] [Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering](https://arxiv.org/abs/2511.17008)
*Zexi Tan,Xiaopeng Luo,Yunlin Liu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 提出EMTC方法，通过重要性感知的变量级掩码和多内生视图表示学习，解决多元时间序列聚类中冗余信息影响判别性时间戳学习的问题。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中存在大量冗余信息（如稳态机器运行记录、太阳能发电零输出期），这些冗余降低了模型对判别性时间戳的关注度，导致聚类性能瓶颈。现有掩码策略多为独立预处理步骤，无法动态适应聚类关键时间戳的重要性变化。

Method: 提出EMTC方法，包含重要性感知变量级掩码(IVM)和多内生视图(MEV)表示学习模块。IVM自适应引导模型学习更具判别性的聚类表示，MEV通过重构和对比学习路径增强泛化能力。

Result: 在15个真实基准数据集上的实验表明，EMTC优于8种SOTA方法，平均比最强基线提升4.85%。

Conclusion: EMTC通过动态掩码和多重表示学习有效提升了多元时间序列聚类性能，解决了冗余信息对判别性时间戳学习的负面影响。

Abstract: Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, with its model architecture composed of Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) representation learning modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the MEV-based reconstruction and contrastive learning pathways enhance the generalization. That is, the MEV reconstruction facilitates multi-perspective complementary to prevent the masking from premature convergence, and the clustering-guided contrastive learning facilitates the joint optimization of representation and clustering. Extensive experiments on 15 real benchmark datasets demonstrate the superiority of EMTC in comparison with eight SOTA methods, where the EMTC achieves an average improvement of 4.85% over the strongest baselines.

</details>


### [31] [Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation](https://arxiv.org/abs/2511.17031)
*Aniketh Iyengar,Jiaqi Han,Boris Ruf,Vincent Grari,Marcin Detyniecki,Stefano Ermon*

Main category: cs.LG

TL;DR: 将Kaplan缩放定律应用于扩散模型，基于计算复杂度预测GPU能耗，验证了扩散推理的计算受限特性


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成图像的计算需求快速增长，引发了对能耗和环境影响的担忧，需要预测不同模型配置和硬件设置的能耗

Method: 将扩散模型推理分解为文本编码、迭代去噪和解码组件，假设去噪操作因多次执行而主导能耗，在多种扩散模型和GPU架构上进行实验

Result: 能耗缩放定律在单个架构内预测精度高（R平方>0.9），具有强大的跨架构泛化能力，保持高秩相关性

Conclusion: 验证了扩散推理的计算受限性质，为可持续AI部署规划和碳足迹估算提供了基础

Abstract: The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation.

</details>


### [32] [Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels](https://arxiv.org/abs/2511.17040)
*Wenzhang Du*

Main category: cs.LG

TL;DR: Step-E是一个将样本选择和模型学习集成到单一优化过程中的框架，通过按损失对样本排序并逐步排除高损失样本来处理噪声标签和异常值，在CIFAR数据集上显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 野外收集的训练数据通常包含噪声标签和异常值，这会严重降低深度神经网络的性能和可靠性。传统的数据清洗作为单独的预处理阶段，无法充分利用下游模型的反馈，也无法适应未知的噪声模式。

Method: Step-E在每个epoch中按损失对样本进行排序，在短暂的预热阶段后逐步增加从梯度更新中排除的高损失样本比例，形成一个在线课程，专注于简单且一致的样本，最终忽略持续的异常值。

Result: 在CIFAR-100N上，Step-E将ResNet-18模型的测试准确率从43.3%提升到50.4%，明显优于损失截断、自定步调学习和一次性过滤方法，接近60.5%的干净标签oracle。在CIFAR-10N上，也从83.9%提升到85.3%，接近85.9%的干净标签oracle。

Conclusion: Step-E通过将样本选择和模型学习集成到单一优化过程中，有效处理了训练数据中的噪声标签和异常值问题，显著提升了模型性能，且训练时间开销适中。

Abstract: Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the downstream model nor adapt to unknown noise patterns. We propose Step-E, a simple framework that integrates sample selection and model learning into a single optimization process. At each epoch, Step-E ranks samples by loss and gradually increases the fraction of high-loss examples that are excluded from gradient updates after a brief warm-up stage, yielding an online curriculum that focuses on easy and consistent examples and eventually ignores persistent outliers. On CIFAR-100N, Step-E improves the test accuracy of a ResNet-18 model from 43.3% (+/- 0.7%) to 50.4% (+/- 0.9%), clearly outperforming loss truncation, self-paced learning, and one-shot filtering while approaching the clean-label oracle at 60.5% (+/- 0.2%). On CIFAR-10N (aggre), Step-E also improves over the noisy baseline (85.3% vs. 83.9%) and nearly matches the clean-label oracle (85.9%), with only moderate training-time overhead.

</details>


### [33] [Hash Collisions in Molecular Fingerprints: Effects on Property Prediction and Bayesian Optimization](https://arxiv.org/abs/2511.17078)
*Walter Virany,Austin Tripp*

Main category: cs.LG

TL;DR: 研究比较了精确指纹与标准压缩指纹在分子性质预测和贝叶斯优化中的表现，发现精确指纹在预测准确性上有小幅但一致的提升，但在贝叶斯优化中无明显改进。


<details>
  <summary>Details</summary>
Motivation: 分子指纹方法使用哈希函数生成固定长度的分子向量表示，但哈希冲突会导致不同子结构具有相同特征，从而在分子相似性计算中产生高估。

Method: 使用精确指纹与标准压缩指纹进行对比研究，基于高斯过程作为预测模型，在DOCKSTRING数据集的五个分子性质预测基准上进行测试。

Result: 在分子性质预测中，精确指纹相比压缩指纹产生了小幅但一致的准确性提升；但在贝叶斯优化任务中，这些改进并未转化为显著的性能提升。

Conclusion: 精确指纹在分子性质预测中能提供更好的准确性，但在贝叶斯优化应用中优势不明显。

Abstract: Molecular fingerprinting methods use hash functions to create fixed-length vector representations of molecules. However, hash collisions cause distinct substructures to be represented with the same feature, leading to overestimates in molecular similarity calculations. We investigate whether using exact fingerprints improves accuracy compared to standard compressed fingerprints in molecular property prediction and Bayesian optimization where the underlying predictive model is a Gaussian process. We find that using exact fingerprints yields a small yet consistent improvement in predictive accuracy on five molecular property prediction benchmarks from the DOCKSTRING dataset. However, these gains did not translate to significant improvements in Bayesian optimization performance.

</details>


### [34] [Why Do Language Model Agents Whistleblow?](https://arxiv.org/abs/2511.17085)
*Kushal Agrawal,Frank Xiao,Guido Bergman,Asa Cooper Stickland*

Main category: cs.LG

TL;DR: 研究LLM作为工具使用代理时的举报行为，即模型未经用户指示向第三方披露可疑不当行为，开发评估套件发现举报频率因模型而异，任务复杂性降低举报倾向，道德提示增加举报率，提供更多工具和详细工作流程减少举报。


<details>
  <summary>Details</summary>
Motivation: LLM作为工具使用代理时，其对齐训练会以新方式表现，研究发现模型可能违背用户利益或明确指示使用工具，特别关注模型未经用户指示向外部披露可疑不当行为的举报现象。

Method: 引入包含多样现实不当行为场景的评估套件，测试不同模型和设置下的举报行为，包括任务复杂性、道德提示、工具和工作流程等因素的影响。

Result: 举报频率因模型家族差异显著；任务复杂性增加降低举报倾向；道德提示显著提高举报率；提供更多工具和详细工作流程减少举报行为；验证数据集鲁棒性，发现评估意识低于先前工作。

Conclusion: LLM的举报行为受多种因素影响，通过调整任务设计、提示策略和工具配置可以控制这种行为，为开发更安全可靠的AI代理提供指导。

Abstract: The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.

</details>


### [35] [Geometric-Disentangelment Unlearning](https://arxiv.org/abs/2511.17100)
*Duo Zhou,Yuji Zhang,Tianxin Wei,Ruizhong Qiu,Ke Yang,Xiao Lin,Cheng Qian,Jingrui He,Hanghang Tong,Heng Ji,Huan Zhang*

Main category: cs.LG

TL;DR: 提出了几何解缠遗忘方法(GU)，通过将遗忘梯度分解为保留梯度子空间的切向和法向分量，只执行法向分量来在保证遗忘效果的同时最小化对保留知识的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘方法在遗忘训练子集时往往损害保留知识，面临遗忘效果与保留知识保护之间的权衡问题，缺乏对遗忘更新如何影响保留知识的理论分析。

Method: 基于一阶分析，将遗忘梯度更新分解为保留梯度子空间的切向分量和法向分量，只执行法向分量来确保保留损失的一阶不变性。该方法可插入现有的基于梯度的遗忘方法中。

Result: 在TOFU、MUSE和WMDP三个基准测试中，GU方法在各种现有方法上均实现了持续改进。

Conclusion: 几何解缠方法提供了一种理论上有保证的简单解决方案，能够在有效遗忘的同时保护保留知识，且可与现有方法无缝集成。

Abstract: Machine unlearning, the removal of a training subset's influence from a deployed model, is critical for privacy preservation and model reliability, yet gradient ascent on forget samples often harms retained knowledge. Existing approaches face a persistent tradeoff between effective forgetting and preservation on the retain set. While previous methods provide useful heuristics, they often lack a formal analysis on how exactly forgetting updates harm retained knowledge, and whether the side effects can be removed with theoretical guarantees. To explore a theoretically sound and simple solution, we start from the first principle on how performance on the retain set is actually affected: a first-order analysis of the local change of the retain loss under small parameter updates during model training. We start from a crisp equivalence: the retain loss is unchanged to first order iff the update direction is orthogonal to the subspace spanned by retain gradients ("retain-invariant"). This identifies the entangled component as the tangential part of forget update within the retain-gradient subspace, and characterizes disentanglement as orthogonality. Guided by this, we propose the Geometric-disentanglement Unlearning (GU) that decomposes any candidate forget gradient update into tangential and normal components to retain space and executes only the normal component. Under a standard trust-region budget, the projected direction aligned with the raw forget gradient is optimal among all first-order retain-invariant moves, and we also derive the optimal projected direction for joint forget-retain updating objectives. Our method is plug-and-play and can be attached to existing gradient-based unlearning procedures to mitigate side effects. GU achieves consistent improvement on various methods across three benchmarks TOFU, MUSE, and WMDP.

</details>


### [36] [Four decades of circumpolar super-resolved satellite land surface temperature data](https://arxiv.org/abs/2511.17134)
*Sonia Dupuis,Nando Metzger,Konrad Schindler,Frank Göttsche,Stefan Wunderle*

Main category: cs.LG

TL;DR: 开发了一个42年泛北极地表温度数据集，通过深度学习超分辨率算法将AVHRR GAC数据从粗分辨率降尺度到1公里，用于改善永久冻土建模和气候变化监测。


<details>
  <summary>Details</summary>
Motivation: 北极地区快速变暖，但AVHRR卫星数据的粗空间分辨率限制了其在分析精细尺度永久冻土动态和其他地表过程方面的应用，需要更高分辨率的地表温度数据。

Method: 使用基于深度各向异性扩散模型的超分辨率算法，以MODIS LST数据为训练目标，结合高分辨率土地覆盖、数字高程和植被高度图，将AVHRR GAC数据降尺度到1公里分辨率。

Result: 生成了42年泛北极地区每日两次的1公里分辨率地表温度观测数据集，支持改进的永久冻土建模、近地表气温重建和格陵兰冰盖表面质量平衡评估。

Conclusion: 该数据集填补了MODIS时代之前的气候监测空白，为未来热红外观测卫星任务提供了可适应的框架，确保了气候数据记录的连续性。

Abstract: Land surface temperature (LST) is an essential climate variable (ECV) crucial for understanding land-atmosphere energy exchange and monitoring climate change, especially in the rapidly warming Arctic. Long-term satellite-based LST records, such as those derived from the Advanced Very High Resolution Radiometer (AVHRR), are essential for detecting climate trends. However, the coarse spatial resolution of AVHRR's global area coverage (GAC) data limit their utility for analyzing fine-scale permafrost dynamics and other surface processes in the Arctic. This paper presents a new 42 years pan-Arctic LST dataset, downscaled from AVHRR GAC to 1 km with a super-resolution algorithm based on a deep anisotropic diffusion model. The model is trained on MODIS LST data, using coarsened inputs and native-resolution outputs, guided by high-resolution land cover, digital elevation, and vegetation height maps. The resulting dataset provides twice-daily, 1 km LST observations for the entire pan-Arctic region over four decades. This enhanced dataset enables improved modelling of permafrost, reconstruction of near-surface air temperature, and assessment of surface mass balance of the Greenland Ice Sheet. Additionally, it supports climate monitoring efforts in the pre-MODIS era and offers a framework adaptable to future satellite missions for thermal infrared observation and climate data record continuity.

</details>


### [37] [Reconstruction of Surface EMG Signal using IMU data for Upper Limb Actions](https://arxiv.org/abs/2511.17200)
*Shubhranil Basak,Mada Hemanth,Madhav Rao*

Main category: cs.LG

TL;DR: 使用深度学习从IMU数据合成归一化sEMG信号，模型能成功预测肌肉激活的时间和大致形状，但峰值幅度常被低估。


<details>
  <summary>Details</summary>
Motivation: sEMG信号对肌肉功能分析很重要但易受噪声干扰且采集困难，IMU作为可穿戴替代方案更稳健。

Method: 收集同步的sEMG和IMU数据，使用基于扩张因果卷积的滑动窗口波网络模型将IMU数据映射到sEMG信号。

Result: 模型成功预测了肌肉激活的时间和一般形状，但峰值幅度经常被低估，具有高时间保真度。

Conclusion: 该方法在假肢控制和康复生物反馈等应用中用于肌肉意图检测是可行的。

Abstract: Surface Electromyography (sEMG) provides vital insights into muscle function, but it can be noisy and challenging to acquire. Inertial Measurement Units (IMUs) provide a robust and wearable alternative to motion capture systems. This paper investigates the synthesis of normalized sEMG signals from 6-axis IMU data using a deep learning approach. We collected simultaneous sEMG and IMU data sampled at 1~KHz for various arm movements. A Sliding-Window-Wave-Net model, based on dilated causal convolutions, was trained to map the IMU data to the sEMG signal. The results show that the model successfully predicts the timing and general shape of muscle activations. Although peak amplitudes were often underestimated, the high temporal fidelity demonstrates the feasibility of using this method for muscle intent detection in applications such as prosthetics and rehabilitation biofeedback.

</details>


### [38] [DelTriC: A Novel Clustering Method with Accurate Outlier](https://arxiv.org/abs/2511.17219)
*Tomas Javurek,Michal Gregor,Sebastian Kula,Marian Simko*

Main category: cs.LG

TL;DR: DelTriC是一种新的聚类算法，通过PCA/UMAP降维、Delaunay三角剖分和反向投影机制，在原始高维空间中形成聚类，在可扩展性和准确性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在高维空间中面临挑战，需要一种既能利用低维空间优势又能在原始空间进行决策的聚类算法。

Method: 首先在低维代理空间进行三角剖分索引局部邻接关系，然后反向投影到原始空间进行稳健的边修剪、合并和异常检测。

Result: DelTriC在多种场景下优于k-means、DBSCAN和HDBSCAN等传统方法，具有更好的可扩展性和准确性，并显著改进异常检测。

Conclusion: DelTriC通过解耦邻域构建和决策制定，提供了一种有效的高维数据聚类解决方案。

Abstract: The paper introduces DelTriC (Delaunay Triangulation Clustering), a clustering algorithm which integrates PCA/UMAP-based projection, Delaunay triangulation, and a novel back-projection mechanism to form clusters in the original high-dimensional space. DelTriC decouples neighborhood construction from decision-making by first triangulating in a low-dimensional proxy to index local adjacency, and then back-projecting to the original space to perform robust edge pruning, merging, and anomaly detection. DelTriC can outperform traditional methods such as k-means, DBSCAN, and HDBSCAN in many scenarios; it is both scalable and accurate, and it also significantly improves outlier detection.

</details>


### [39] [Generating transition states of chemical reactions via distance-geometry-based flow matching](https://arxiv.org/abs/2511.17229)
*Yufei Luo,Xiang Gu,Jian Sun*

Main category: cs.LG

TL;DR: TS-DFM是一个基于流匹配的框架，能够从反应物和产物预测过渡态结构，在分子距离几何空间中捕获化学反应中的原子间距离动态变化，显著提高了结构预测精度。


<details>
  <summary>Details</summary>
Motivation: 过渡态对于理解反应机理至关重要，但实验和计算方法在探索过渡态时面临复杂性限制，需要更高效准确的预测方法。

Method: 提出TS-DFM流匹配框架，在分子距离几何空间中操作，设计TSDVNet网络结构学习速度场以生成准确的过渡态几何结构。

Result: 在Transition1X基准数据集上，TS-DFM的结构准确率比之前最优方法React-OT提高了30%，预测的过渡态为CI-NEB优化提供了高质量初始结构，并能识别替代反应路径。

Conclusion: TS-DFM在未见过的分子和反应类型上表现出强大的泛化能力，具有促进反应探索的潜力。

Abstract: Transition states (TSs) are crucial for understanding reaction mechanisms, yet their exploration is limited by the complexity of experimental and computational approaches. Here we propose TS-DFM, a flow matching framework that predicts TSs from reactants and products. By operating in molecular distance geometry space, TS-DFM explicitly captures the dynamic changes of interatomic distances in chemical reactions. A network structure named TSDVNet is designed to learn the velocity field for generating TS geometries accurately. On the benchmark dataset Transition1X, TS-DFM outperforms the previous state-of-the-art method React-OT by 30\% in structural accuracy. These predicted TSs provide high-quality initial structures, accelerating the convergence of CI-NEB optimization. Additionally, TS-DFM can identify alternative reaction paths. In our experiments, even a more favorable TS with lower energy barrier is discovered. Further tests on RGD1 dataset confirm its strong generalization ability on unseen molecules and reaction types, highlighting its potential for facilitating reaction exploration.

</details>


### [40] [FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble](https://arxiv.org/abs/2511.17249)
*Riccardo Tedoldi,Ola Engkvist,Patrick Bryant,Hossein Azizpour,Jon Paul Janet,Alessandro Tibo*

Main category: cs.LG

TL;DR: FlexiFlow是一个新颖的架构，扩展了流匹配模型，能够联合采样分子及其多个构象，同时保持等变性和置换不变性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的3D从头设计流匹配或扩散模型仅限于生成单一构象，但分子的构象景观决定了其可观察性质和与蛋白质靶标的结合能力。通过生成一组代表性的低能构象异构体，可以更直接评估这些性质，并可能提高生成具有所需热力学可观察性质分子的能力。

Method: 提出FlexiFlow架构，扩展流匹配模型，允许联合采样分子及其多个构象，同时保持等变性和置换不变性。在QM9和GEOM Drugs数据集上验证方法。

Result: 在分子生成任务中达到最先进结果，能够生成有效、无应变、独特且新颖的分子，具有对训练数据分布的高保真度，同时捕获分子的构象多样性。生成的构象集合在推理时间的一小部分内提供与最先进物理方法相似的覆盖范围。

Conclusion: FlexiFlow可以成功转移到蛋白质条件配体生成任务，即使数据集仅包含静态口袋而没有伴随构象。

Abstract: Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations.

</details>


### [41] [Enforcing governing equation constraints in neural PDE solvers via training-free projections](https://arxiv.org/abs/2511.17258)
*Omer Rochman,Gilles Louppe*

Main category: cs.LG

TL;DR: 本文评估了两种无需训练的后处理投影方法，用于减少神经PDE求解器违反约束的问题，包括非线性优化投影和局部线性化投影。


<details>
  <summary>Details</summary>
Motivation: 神经PDE求解器在科学模拟中经常违反控制方程约束，特别是非线性约束和具有长时间依赖性的动力学PDE约束，这使得投影到可行集变得复杂。

Method: 提出了两种后处理投影方法：基于非线性优化的投影，以及使用Jacobian-vector和vector-Jacobian积的局部线性化投影。

Result: 两种投影方法都显著减少了约束违反，并在代表性PDE上比基于物理信息的基线方法提高了准确性。

Conclusion: 后处理投影是解决神经PDE求解器约束违反问题的有效方法，特别是对于非线性约束和动力学PDE问题。

Abstract: Neural PDE solvers used for scientific simulation often violate governing equation constraints. While linear constraints can be projected cheaply, many constraints are nonlinear, complicating projection onto the feasible set. Dynamical PDEs are especially difficult because constraints induce long-range dependencies in time. In this work, we evaluate two training-free, post hoc projections of approximate solutions: a nonlinear optimization-based projection, and a local linearization-based projection using Jacobian-vector and vector-Jacobian products. We analyze constraints across representative PDEs and find that both projections substantially reduce violations and improve accuracy over physics-informed baselines.

</details>


### [42] [Automobile demand forecasting: Spatiotemporal and hierarchical modeling, life cycle dynamics, and user-generated online information](https://arxiv.org/abs/2511.17275)
*Tom Nahrendorf,Stefan Minner,Helfried Binder,Richard Zinck*

Main category: cs.LG

TL;DR: 针对高端汽车制造商面临的产品多样性高、变体级数据稀疏和市场波动大的复杂预测挑战，本研究开发了结合点预测和概率预测的多层次预测方法，通过LightGBM集成模型、分位数回归和混合整数线性规划协调，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 高端汽车制造商面临产品多样性高、变体级数据稀疏和市场波动大的复杂预测挑战，需要开发能够处理多产品、多市场、多层次层次结构的预测方法。

Method: 结合点预测和概率预测，使用LightGBM集成模型与池化训练集、分位数回归以及混合整数线性规划协调方法，分析时空依赖性和舍入偏差对预测准确性的影响。

Result: 研究发现时空依赖性和舍入偏差显著影响预测准确性，强调了整数预测对操作可行性的重要性。短期需求受生命周期成熟度、自回归动量和操作信号影响，而中期需求则反映在线参与度、规划目标和竞争指标等预期驱动因素。

Conclusion: 在线行为数据在分解层级显著提高了预测准确性，该方法为高端汽车制造商提供了有效的多层次需求预测解决方案，能够同时满足战略和运营规划需求。

Abstract: Premium automotive manufacturers face increasingly complex forecasting challenges due to high product variety, sparse variant-level data, and volatile market dynamics. This study addresses monthly automobile demand forecasting across a multi-product, multi-market, and multi-level hierarchy using data from a German premium manufacturer. The methodology combines point and probabilistic forecasts across strategic and operational planning levels, leveraging ensembles of LightGBM models with pooled training sets, quantile regression, and a mixed-integer linear programming reconciliation approach. Results highlight that spatiotemporal dependencies, as well as rounding bias, significantly affect forecast accuracy, underscoring the importance of integer forecasts for operational feasibility. Shapley analysis shows that short-term demand is reactive, shaped by life cycle maturity, autoregressive momentum, and operational signals, whereas medium-term demand reflects anticipatory drivers such as online engagement, planning targets, and competitive indicators, with online behavioral data considerably improving accuracy at disaggregated levels.

</details>


### [43] [SAVeD: Semantic Aware Version Discovery](https://arxiv.org/abs/2511.17298)
*Artem Frenk,Roee Shraga*

Main category: cs.LG

TL;DR: SAVeD是一个基于对比学习的框架，用于识别结构化数据集的版本，无需依赖元数据、标签或集成假设。


<details>
  <summary>Details</summary>
Motivation: 解决数据科学中由于数据集相似工作或转换导致的重复劳动问题。

Method: 采用改进的SimCLR流程，通过随机变换生成增强表视图，使用自定义transformer编码器嵌入，在潜在空间中进行对比以优化语义相似性。

Result: 在五个标准数据集上验证，SAVeD在完全未见过的表上获得显著更高的准确率，分离分数显著提升，能够有效区分语义改变的版本。

Conclusion: SAVeD在数据集版本检测方面表现出色，优于未训练基线和现有最先进方法如Starmie。

Abstract: Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.

</details>


### [44] [Self-supervised denoising of raw tomography detector data for improved image reconstruction](https://arxiv.org/abs/2511.17312)
*Israt Jahan Tulin,Sebastian Starke,Dominic Windisch,André Bieberle,Peter Steinbach*

Main category: cs.LG

TL;DR: 比较两种自监督深度学习方法和一种非学习方法对超快电子束X射线CT的探测器数据进行降噪的效果，发现深度学习方法在信噪比和重建图像质量方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 超快电子束X射线CT由于测量时间短导致数据噪声大，产生重建伪影，限制了图像质量。

Method: 研究并比较了两种自监督深度学习方法和一种非学习方法的探测器数据降噪效果。

Result: 深度学习方法能够提高探测器数据的信噪比，并持续改善重建图像质量，优于非学习方法。

Conclusion: 自监督深度学习方法在超快电子束X射线CT数据降噪方面具有优势，能够有效提升图像质量。

Abstract: Ultrafast electron beam X-ray computed tomography produces noisy data due to short measurement times, causing reconstruction artifacts and limiting overall image quality. To counteract these issues, two self-supervised deep learning methods for denoising of raw detector data were investigated and compared against a non-learning based denoising method. We found that the application of the deep-learning-based methods was able to enhance signal-to-noise ratios in the detector data and also led to consistent improvements of the reconstructed images, outperforming the non-learning based method.

</details>


### [45] [ReBaPL: Repulsive Bayesian Prompt Learning](https://arxiv.org/abs/2511.17339)
*Yassir Bendou,Omar Ezzahir,Eduardo Fernandes Montesuma,Gabriel Mahuas,Victoria Shevchenko,Mike Gartrell*

Main category: cs.LG

TL;DR: 提出Repulsive Bayesian Prompt Learning (ReBaPL)方法，通过贝叶斯推理框架优化提示学习，结合循环步长调度和随机梯度哈密顿蒙特卡洛算法，并引入表示空间的排斥力来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统提示学习方法容易过拟合且泛化能力不足，贝叶斯提示学习通过概率框架增强鲁棒性，但需要更有效地探索复杂的多模态后验分布。

Method: 结合循环步长调度与SGHMC算法，交替进行探索和利用阶段；引入基于最大均值差异和Wasserstein距离的表示空间排斥力，防止过早收敛到单一模式。

Result: 在多个基准数据集上验证了ReBaPL的有效性，相比现有最先进的提示学习方法表现出更优越的性能。

Conclusion: ReBaPL提供了一种模块化的贝叶斯扩展方法，能够更全面地刻画提示后验分布，显著提升泛化能力，适用于任何基于最大似然估计的提示学习方法。

Abstract: Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.

</details>


### [46] [Convergence and stability of Q-learning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2511.17351)
*Massimiliano Manenti,Andrea Iannelli*

Main category: cs.LG

TL;DR: 本文提出了封建Q学习方案，分析了其收敛和稳定性条件，并通过理论证明和实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习虽然在实际应用中表现出色，但理论保证滞后。本文旨在为封建强化学习提供收敛性和稳定性的理论分析。

Method: 采用封建Q学习方案，利用随机逼近理论和ODE方法进行收敛性分析，并通过实验验证理论结果。

Result: 证明了封建Q学习在特定条件下的收敛性和稳定性，并表明其更新收敛到一个可解释为博弈均衡的点。

Conclusion: 为封建强化学习提供了理论保证，为基于博弈论的分层强化学习方法打开了大门。

Abstract: Hierarchical Reinforcement Learning promises, among other benefits, to efficiently capture and utilize the temporal structure of a decision-making problem and to enhance continual learning capabilities, but theoretical guarantees lag behind practice. In this paper, we propose a Feudal Q-learning scheme and investigate under which conditions its coupled updates converge and are stable. By leveraging the theory of Stochastic Approximation and the ODE method, we present a theorem stating the convergence and stability properties of Feudal Q-learning. This provides a principled convergence and stability analysis tailored to Feudal RL. Moreover, we show that the updates converge to a point that can be interpreted as an equilibrium of a suitably defined game, opening the door to game-theoretic approaches to Hierarchical RL. Lastly, experiments based on the Feudal Q-learning algorithm support the outcomes anticipated by theory.

</details>


### [47] [R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability](https://arxiv.org/abs/2511.17367)
*Runyu Lu,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: 本文提出了首个在部分可观测性下的最坏情况鲁棒实时追捕策略（R2PS），通过信念保持机制和异步移动动态规划，实现了在未知图结构上的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的追逃博弈方法主要基于完美信息假设，无法处理现实中的部分可观测性和逃避者预测追捕者行动的情况，需要开发能在部分可观测环境下实时应用的鲁棒追捕策略。

Method: 1）证明传统动态规划算法在异步移动下保持最优性；2）提出逃避者可能位置的信念保持机制，将DP策略扩展到部分可观测设置；3）将信念保持嵌入EPG框架，通过跨图强化学习训练实时追捕策略。

Result: 强化学习后，策略实现了对未见真实世界图结构的鲁棒零样本泛化，并持续优于现有博弈强化学习方法直接在测试图上训练的策略。

Conclusion: R2PS是首个在部分可观测性下实现最坏情况鲁棒实时追捕的方法，通过信念保持和异步移动DP的有效结合，显著提升了追捕策略的泛化能力和鲁棒性。

Abstract: Computing worst-case robust strategies in pursuit-evasion games (PEGs) is time-consuming, especially when real-world factors like partial observability are considered. While important for general security purposes, real-time applicable pursuit strategies for graph-based PEGs are currently missing when the pursuers only have imperfect information about the evader's position. Although state-of-the-art reinforcement learning (RL) methods like Equilibrium Policy Generalization (EPG) and Grasper provide guidelines for learning graph neural network (GNN) policies robust to different game dynamics, they are restricted to the scenario of perfect information and do not take into account the possible case where the evader can predict the pursuers' actions. This paper introduces the first approach to worst-case robust real-time pursuit strategies (R2PS) under partial observability. We first prove that a traditional dynamic programming (DP) algorithm for solving Markov PEGs maintains optimality under the asynchronous moves by the evader. Then, we propose a belief preservation mechanism about the evader's possible positions, extending the DP pursuit strategies to a partially observable setting. Finally, we embed the belief preservation into the state-of-the-art EPG framework to finish our R2PS learning scheme, which leads to a real-time pursuer policy through cross-graph reinforcement learning against the asynchronous-move DP evasion strategies. After reinforcement learning, our policy achieves robust zero-shot generalization to unseen real-world graph structures and consistently outperforms the policy directly trained on the test graphs by the existing game RL approach.

</details>


### [48] [A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias](https://arxiv.org/abs/2511.17378)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 本文开发了一个线性稳定性框架来分析SGD、随机扰动和SAM在两层ReLU网络中的行为，通过一致性度量揭示梯度曲率在数据点间的对齐方式，解释为何某些最小值在训练中更稳定和被偏好。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习优化的动态性越来越重要，特别是随着模型规模的扩大。虽然SGD及其变体能可靠地找到泛化良好的解，但驱动这种泛化的机制仍不清楚。这些算法通常偏好更平坦或更简单的最小值，尤其是在过参数化设置中。

Method: 开发了一个线性稳定性框架来分析SGD、随机扰动和SAM的行为，特别关注两层ReLU网络。核心是一个一致性度量，量化梯度曲率在数据点间的对齐方式。

Result: 该框架揭示了为什么某些最小值在训练中是稳定的和被偏好的，通过一致性度量展示了梯度曲率对齐如何影响优化动态。

Conclusion: 该工作提供了一个统一的理论框架，连接了数据结构、优化动态和学习解的性质，特别解释了平坦最小值偏好现象背后的机制。

Abstract: Understanding the dynamics of optimization in deep learning is increasingly important as models scale. While stochastic gradient descent (SGD) and its variants reliably find solutions that generalize well, the mechanisms driving this generalization remain unclear. Notably, these algorithms often prefer flatter or simpler minima, particularly in overparameterized settings. Prior work has linked flatness to generalization, and methods like Sharpness-Aware Minimization (SAM) explicitly encourage flatness, but a unified theory connecting data structure, optimization dynamics, and the nature of learned solutions is still lacking. In this work, we develop a linear stability framework that analyzes the behavior of SGD, random perturbations, and SAM, particularly in two layer ReLU networks. Central to our analysis is a coherence measure that quantifies how gradient curvature aligns across data points, revealing why certain minima are stable and favored during training.

</details>


### [49] [Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes](https://arxiv.org/abs/2511.17399)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出了一种基于后验采样的核心集选择框架，通过连接后验采样与损失景观来解决梯度方法在核心集选择中的局限性，实现更快的训练和更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型规模不断扩大，计算需求增长，需要有效的核心集选择技术来加速训练。梯度方法虽然理论基础强，但面临SGD作为强基线和损失曲率不匹配导致代表性失效的挑战。

Method: 1) 建立后验采样与损失景观的连接，实现鲁棒的核心集选择；2) 引入基于后验采样的平滑损失函数，增强稳定性和泛化能力；3) 提出采样核心集选择方法的收敛性分析。

Result: 通过大量实验证明，该方法在多个数据集上相比现有技术实现了更快的训练速度和更好的泛化性能。

Conclusion: 所提出的基于后验采样的核心集选择框架有效解决了梯度方法的局限性，在加速训练和提升泛化能力方面优于现有方法。

Abstract: As deep learning models continue to scale, the growing computational demands have amplified the need for effective coreset selection techniques. Coreset selection aims to accelerate training by identifying small, representative subsets of data that approximate the performance of the full dataset. Among various approaches, gradient based methods stand out due to their strong theoretical underpinnings and practical benefits, particularly under limited data budgets. However, these methods face challenges such as naive stochastic gradient descent (SGD) acting as a surprisingly strong baseline and the breakdown of representativeness due to loss curvature mismatches over time.
  In this work, we propose a novel framework that addresses these limitations. First, we establish a connection between posterior sampling and loss landscapes, enabling robust coreset selection even in high data corruption scenarios. Second, we introduce a smoothed loss function based on posterior sampling onto the model weights, enhancing stability and generalization while maintaining computational efficiency. We also present a novel convergence analysis for our sampling-based coreset selection method. Finally, through extensive experiments, we demonstrate how our approach achieves faster training and enhanced generalization across diverse datasets than the current state of the art.

</details>


### [50] [DS-Span: Single-Phase Discriminative Subgraph Mining for Efficient Graph Embeddings](https://arxiv.org/abs/2511.17419)
*Yeamin Kaiser,Muhammed Tasnim Bin Anwar,Bholanath Das,Chowdhury Farhan Ahmed,Md. Tanvir Alam*

Main category: cs.LG

TL;DR: DS-Span是一个单阶段判别子图挖掘框架，统一了模式增长、剪枝和监督驱动评分，在搜索空间的一次遍历中完成所有操作，显著提高了效率和判别性。


<details>
  <summary>Details</summary>
Motivation: 现有的频繁或判别子图挖掘方法存在冗余的多阶段流程、高计算成本和挖掘结构与判别相关性之间弱耦合的问题，需要更高效统一的解决方案。

Method: 提出DS-Span框架，引入覆盖上限资格机制动态限制探索，以及信息增益引导的选择机制，在单次搜索空间遍历中统一完成模式增长、剪枝和评分。

Result: 在多个基准测试中，DS-Span比现有多阶段方法生成更紧凑和判别的子图特征，在显著减少运行时间的同时达到更高或相当的准确率。

Conclusion: 统一的单阶段判别挖掘为可扩展和可解释的图表示学习提供了有前景的基础，证明了该方法的有效性。

Abstract: Graph representation learning seeks to transform complex, high-dimensional graph structures into compact vector spaces that preserve both topology and semantics. Among the various strategies, subgraph-based methods provide an interpretable bridge between symbolic pattern discovery and continuous embedding learning. Yet, existing frequent or discriminative subgraph mining approaches often suffer from redundant multi-phase pipelines, high computational cost, and weak coupling between mined structures and their discriminative relevance. We propose DS-Span, a single-phase discriminative subgraph mining framework that unifies pattern growth, pruning, and supervision-driven scoring within one traversal of the search space. DS-Span introduces a coverage-capped eligibility mechanism that dynamically limits exploration once a graph is sufficiently represented, and an information-gain-guided selection that promotes subgraphs with strong class-separating ability while minimizing redundancy. The resulting subgraph set serves as an efficient, interpretable basis for downstream graph embedding and classification. Extensive experiments across benchmarks demonstrate that DS-Span generates more compact and discriminative subgraph features than prior multi-stage methods, achieving higher or comparable accuracy with significantly reduced runtime. These results highlight the potential of unified, single-phase discriminative mining as a foundation for scalable and interpretable graph representation learning.

</details>


### [51] [Self-Supervised Learning by Curvature Alignment](https://arxiv.org/abs/2511.17426)
*Benyamin Ghojogh,M. Hadi Sepanj,Paul Fieguth*

Main category: cs.LG

TL;DR: CurvSSL是一个曲率正则化的自监督学习框架，通过在标准冗余减少损失基础上添加基于曲率的正则化项，显式地塑造表示的局部几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有的非对比自监督学习方法主要关注表示的一阶和二阶统计特性，但忽略了底层数据流形的局部几何结构。本文旨在通过曲率正则化来弥补这一不足。

Method: 采用标准双视图编码器-投影器架构，在Barlow Twins风格的冗余减少损失基础上，添加曲率正则化项。通过计算嵌入点的k近邻在单位超球面上的余弦交互来定义离散曲率得分，并在不同增强视图间对齐和解相关这些曲率得分。

Result: 在MNIST和CIFAR-10数据集上的实验表明，曲率正则化的自监督学习相比Barlow Twins和VICReg具有竞争性或改进的线性评估性能。

Conclusion: 显式塑造局部几何结构是对纯统计自监督学习正则化的简单有效补充，能够提升表示学习的效果。

Abstract: Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.

</details>


### [52] [Towards fully differentiable neural ocean model with Veros](https://arxiv.org/abs/2511.17427)
*Etienne Meunier,Said Ouala,Hugo Frezat,Julien Le Sommer,Ronan Fablet*

Main category: cs.LG

TL;DR: 将VEROS海洋模型扩展为可微分版本，使其与JAX自动微分框架兼容，并展示了在海洋状态修正和物理参数校准中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统海洋模型缺乏自动微分能力，限制了基于梯度的优化和参数校准的应用。

Method: 对VEROS海洋模型进行修改，使其与JAX框架兼容，实现自动微分功能。

Result: 成功实现了可微分海洋模型，数值一致性得到验证，并展示了在状态修正和参数校准中的实际应用。

Conclusion: 可微分编程为海洋建模中的端到端学习和参数调优提供了新的可能性。

Abstract: We present a differentiable extension of the VEROS ocean model, enabling automatic differentiation through its dynamical core. We describe the key modifications required to make the model fully compatible with JAX autodifferentiation framework and evaluate the numerical consistency of the resulting implementation. Two illustrative applications are then demonstrated: (i) the correction of an initial ocean state through gradient-based optimization, and (ii) the calibration of unknown physical parameters directly from model observations. These examples highlight how differentiable programming can facilitate end-to-end learning and parameter tuning in ocean modeling. Our implementation is available online.

</details>


### [53] [Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems](https://arxiv.org/abs/2511.17435)
*Zengyu Zou,Jingyuan Wang,Yixuan Huang,Junjie Wu*

Main category: cs.LG

TL;DR: 提出了Multi-Agent Pointer Transformer (MAPT)框架，用于解决多车辆动态取送货随机请求问题，通过Transformer编码器提取实体表示，结合指针网络生成联合动作序列，在8个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 经典运筹学方法在处理大规模动态问题时面临计算复杂度和时间效率瓶颈，现有强化学习方法存在三个挑战：多车辆独立解码无法建模联合动作分布、特征提取网络难以捕捉实体间关系、联合动作空间指数级增长。

Method: 设计了MAPT框架，使用Transformer编码器提取实体表示，结合Transformer解码器和指针网络以自回归方式生成联合动作序列，引入关系感知注意力模块捕捉实体间关系，并使用信息先验指导模型决策。

Result: 在8个数据集上的实验表明，MAPT在性能上显著优于现有基线方法，与经典运筹学方法相比具有显著的计算时间优势。

Conclusion: MAPT框架有效解决了多车辆动态取送货随机请求问题中的关键挑战，在性能和计算效率方面都表现出色，为大规模动态路由优化问题提供了有效的端到端解决方案。

Abstract: This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.

</details>


### [54] [InTAct: Interval-based Task Activation Consolidation for Continual Learning](https://arxiv.org/abs/2511.17439)
*Patryk Krukowski,Jan Miksa,Piotr Helm,Jacek Tabor,Paweł Wawrzyński,Przemysław Spurek*

Main category: cs.LG

TL;DR: InTAct是一种持续学习方法，通过在共享层中保留功能行为来解决表示漂移问题，无需冻结参数或存储过去数据。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的方法在类别增量学习中表现良好，但在域偏移情况下容易受到表示漂移的影响，导致遗忘问题。

Method: InTAct捕获先前学习任务的特征激活范围，并约束更新以确保网络在这些区域内保持一致，同时允许在其他地方灵活适应。

Result: 在DomainNet和ImageNet-R等域增量基准测试中，InTAct持续减少表示漂移并提高性能，平均准确率比最先进基线提高多达8个百分点。

Conclusion: InTAct通过在编码过去知识的地方调节表示变化，实现了稳定性和可塑性之间的原则性平衡。

Abstract: Continual learning aims to enable neural networks to acquire new knowledge without forgetting previously learned information. While recent prompt-based methods perform strongly in class-incremental settings, they remain vulnerable under domain shifts, where the input distribution changes but the label space remains fixed. This exposes a persistent problem known as representation drift. Shared representations evolve in ways that overwrite previously useful features and cause forgetting even when prompts isolate task-specific parameters. To address this issue, we introduce InTAct, a method that preserves functional behavior in shared layers without freezing parameters or storing past data. InTAct captures the characteristic activation ranges associated with previously learned tasks and constrains updates to ensure the network remains consistent within these regions, while still allowing for flexible adaptation elsewhere. In doing so, InTAct stabilizes the functional role of important neurons rather than directly restricting parameter values. The approach is architecture-agnostic and integrates seamlessly into existing prompt-based continual learning frameworks. By regulating representation changes where past knowledge is encoded, InTAct achieves a principled balance between stability and plasticity. Across diverse domain-incremental benchmarks, including DomainNet and ImageNet-R, InTAct consistently reduces representation drift and improves performance, increasing Average Accuracy by up to 8 percentage points over state-of-the-art baselines.

</details>


### [55] [Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry](https://arxiv.org/abs/2511.17446)
*Kyle M. Regan,Michael McLoughlin,Wayne A. Bryden,Gonzalo R. Arce*

Main category: cs.LG

TL;DR: MS-DGFormer是一个基于Transformer的数据驱动框架，能够直接从原始、最小预处理的质量谱数据中识别病原体，解决了传统MALDI-MS在实时环境监测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统MALDI-MS依赖于劳动密集型的样品制备和多光谱平均，限制了其在实验室外的实时环境监测应用，特别是在新兴的气溶胶MALDI-MS系统中，自主采样会产生噪声谱，需要单次检测能力。

Method: 提出MS-DGFormer框架，采用Transformer架构捕捉时间序列谱中的长程依赖性，并引入基于奇异值分解(SVD)的字典编码器来集成去噪谱信息，从单次谱中识别关键生物分子模式。

Result: 该方法能够从气溶胶样品中实现卓越的病原体识别性能，支持在野外条件下进行自主、实时分析。

Conclusion: 通过消除大量预处理需求，该方法释放了便携式、可部署MALDI-MS平台的潜力，革新了环境病原体检测和生物威胁快速响应能力。

Abstract: Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is a cornerstone in biomolecular analysis, offering precise identification of pathogens through unique mass spectral signatures. Yet, its reliance on labor-intensive sample preparation and multi-shot spectral averaging restricts its use to laboratory settings, rendering it impractical for real-time environmental monitoring. These limitations are especially pronounced in emerging aerosol MALDI-MS systems, where autonomous sampling generates noisy spectra for unknown aerosol analytes, requiring single-shot detection for effective analysis. Addressing these challenges, we propose the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a data-driven framework that redefines spectral analysis by directly processing raw, minimally prepared mass spectral data. MS-DGFormer leverages a transformer architecture, designed to capture the long-range dependencies inherent in these time-series spectra. To enhance feature extraction, we introduce a novel dictionary encoder that integrates denoised spectral information derived from Singular Value Decomposition (SVD), enabling the model to discern critical biomolecular patterns from single-shot spectra with robust performance. This innovation provides a system to achieve superior pathogen identification from aerosol samples, facilitating autonomous, real-time analysis in field conditions. By eliminating the need for extensive preprocessing, our method unlocks the potential for portable, deployable MALDI-MS platforms, revolutionizing environmental pathogen detection and rapid response to biological threats.

</details>


### [56] [PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM](https://arxiv.org/abs/2511.17467)
*Siqi Liang,Yudi Zhang,Yue Guo*

Main category: cs.LG

TL;DR: 提出了一个基于知识图谱增强检索增强生成（Graph RAG）的个性化语言模型框架，通过构建LLM衍生的图索引和社区信息摘要，实现用户画像驱动的个性化AI代理。


<details>
  <summary>Details</summary>
Motivation: 需要能够适应用户偏好的个性化AI代理，让代理体现用户的"画像"（如用户档案或品味）。

Method: 使用知识图谱增强的检索增强生成机制，构建LLM衍生的图索引，结合用户历史行为摘要和基于图的社区检测发现的全局交互模式，生成个性化提示。

Result: 在LaMP基准测试中，新闻分类F1提升11.1%，电影标签F1提升56.1%，产品评分MAE降低10.4%。

Conclusion: 该框架能够保持一致的画像对齐行为，同时受益于集体知识，显著提升了个性化任务的性能。

Abstract: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F

</details>


### [57] [Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization](https://arxiv.org/abs/2511.17489)
*Vinay Kanakeri,Shivam Bajaj,Ashwin Verma,Vijay Gupta,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出一种结合聚类和学习的算法，为相似线性过程学习个性化策略，在保证正确聚类的同时实现样本效率提升。


<details>
  <summary>Details</summary>
Motivation: 强化学习通常需要大量数据，通过利用相似过程的数据可以提高样本效率，但如何识别相似过程是一个挑战。

Method: 结合顺序消除和零阶策略优化的思想，提出同时进行聚类和学习的算法，为每个聚类输出个性化控制器。

Result: 在适当的聚类分离条件下，算法能以高概率保证正确聚类，每个聚类的策略次优性差距与聚类大小成反比。

Conclusion: 这是首个展示聚类如何在数据驱动控制中学习个性化策略的工作，既能从协作中获得统计收益，又不会因包含不相似过程数据而遭受次优性。

Abstract: It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from 'approximately similar' processes. However, since the process models are unknown, identifying which other processes are similar poses a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding to a copy of a linear process to be controlled. The agents' local processes can be partitioned into clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable notion of cluster separation that captures differences in closed-loop performance across systems, we prove that our approach guarantees correct clustering with high probability. Furthermore, we show that the sub-optimality gap of the policy learned for each cluster scales inversely with the size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based control. Our work is the first to reveal how clustering can be used in data-driven control to learn personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality due to inclusion of data from dissimilar processes. From a distributed implementation perspective, our method is attractive as it incurs only a mild logarithmic communication overhead.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [58] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: 提出了MicroEP并行化策略和MicroMoE系统，通过微批次级别的细粒度负载均衡，显著提升MoE模型训练效率


<details>
  <summary>Details</summary>
Motivation: MoE模型在动态专家选择时存在负载不均衡问题，影响训练效率，现有解决方案要么牺牲模型精度，要么引入额外系统开销

Method: 提出MicroEP并行化策略，通过跨GPU的高效token调度实现每个微批次的最优负载均衡，并构建了MicroMoE分布式训练系统

Result: 实验结果显示MicroMoE相比最先进系统提升端到端训练吞吐量达47.6%，几乎始终实现GPU间最优负载均衡

Conclusion: MicroEP和MicroMoE通过细粒度负载均衡有效解决了MoE训练中的效率瓶颈问题

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


### [59] [Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption](https://arxiv.org/abs/2511.17119)
*Gabriel Job Antunes Grabher,Fumio Machida,Thomas Ropars*

Main category: cs.DC

TL;DR: 本文研究性能异常检测器的特性如何优化云服务性能与成本之间的权衡，发现高精度和高召回率并非总是必要，检测频率会影响哪个特性更重要。


<details>
  <summary>Details</summary>
Motivation: 云服务中性能异常检测对维持性能目标至关重要，但检测器会出错。需要研究哪些检测器特性对优化性能与成本权衡最重要。

Method: 使用随机奖励网建模由性能异常检测器监控的云服务，分析检测器精度、召回率和检查频率对平均延迟和资源消耗的影响。

Result: 结果显示：如果检测频繁运行，高精度足以获得良好的性能-成本权衡；如果检测器运行不频繁，召回率变得最重要。高精度和高召回率并非总是必要。

Conclusion: 性能异常检测器的优化策略应根据检测频率调整：高频检测时注重精度，低频检测时注重召回率，以平衡性能与成本。

Abstract: Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [60] [An Introductory Study on the Power Consumption Overhead of ERC-4337 Bundlers](https://arxiv.org/abs/2511.16890)
*Andrei Arusoaie,Claudiu-Nicu Bărbieru,Oana-Otilia Captarencu,Paul-Flavian Diac,Emanuel Onica,Cosmin-Nicolae Vârlan*

Main category: cs.PF

TL;DR: 本文首次分析了ERC-4337标准中bundler中间件的功耗开销，使用SmartWatts监控系统实证研究了bundler工作负载与功耗的关系。


<details>
  <summary>Details</summary>
Motivation: 以太坊从PoW转向PoS后能耗问题有所缓解，但网络扩展通过额外层级仍存在能耗担忧。ERC-4337标准引入的bundler中间件作为第三方服务，其运行成本部分体现为功耗，而目前bundler提供商数量远少于常规以太坊访问提供商。

Method: 使用SmartWatts监控系统，该工具利用RAPL硬件接口，通过实证方法确定bundler工作负载与其主动功耗之间的相关性。

Result: 研究发现bundler在以太坊访问服务中会增加主动功耗开销，并建立了工作负载与功耗之间的量化关系。

Conclusion: ERC-4337标准中的bundler中间件确实会带来额外的能耗开销，这为评估以太坊生态系统整体能耗提供了重要参考。

Abstract: Ethereum is currently the main blockchain ecosystem providing decentralised trust guarantees for applications ranging from finance to e-government. A common criticism of blockchain networks has been their energy consumption and operational costs. The switch from Proof-of-Work (PoW) protocol to Proof-of-Stake (PoS) protocol has significantly reduced this issue, though concerns remain, especially with network expansions via additional layers. The ERC-4337 standard is a recent proposal that facilitates end-user access to Ethereum-backed applications. It introduces a middleware called a bundler, operated as a third-party service, where part of its operational cost is represented by its power consumption. While bundlers have served over 500 million requests in the past two years, fewer than 15 official bundler providers exist, compared to over 100 regular Ethereum access providers. In this paper, we provide a first look at the active power consumption overhead that a bundler would add to an Ethereum access service. Using SmartWatts, a monitoring system leveraging Running Average Power Limit (RAPL) hardware interfaces, we empirically determine correlations between the bundler workload and its active power consumption.

</details>
