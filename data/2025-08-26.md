<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 157]
- [cs.PF](#cs.PF) [Total: 6]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.ET](#cs.ET) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems](https://arxiv.org/abs/2508.17341)
*Muhammet Anil Yagiz,Zeynep Sude Cengiz,Polat Goktas*

Main category: cs.LG

TL;DR: MetaFed是一个去中心化的联邦学习框架，通过多智能体强化学习、同态加密和碳感知调度，为元宇宙应用提供可持续的智能资源编排，在降低碳排放25%的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 元宇宙应用的快速发展带来了性能、隐私和环境可持续性方面的复杂挑战，集中式架构无法满足这些需求，导致高能耗、延迟和隐私问题。

Method: 提出MetaFed框架，整合：(1)多智能体强化学习进行动态客户端选择；(2)使用同态加密的隐私保护联邦学习；(3)与可再生能源可用性对齐的碳感知调度。

Result: 在MNIST和CIFAR-10数据集上使用轻量级ResNet架构进行评估，结果显示相比传统方法减少25%碳排放，同时保持高精度和最小通信开销。

Conclusion: MetaFed是构建环境友好且符合隐私要求的元宇宙基础设施的可扩展解决方案。

Abstract: The rapid expansion of immersive Metaverse applications introduces complex
challenges at the intersection of performance, privacy, and environmental
sustainability. Centralized architectures fall short in addressing these
demands, often resulting in elevated energy consumption, latency, and privacy
concerns. This paper proposes MetaFed, a decentralized federated learning (FL)
framework that enables sustainable and intelligent resource orchestration for
Metaverse environments. MetaFed integrates (i) multi-agent reinforcement
learning for dynamic client selection, (ii) privacy-preserving FL using
homomorphic encryption, and (iii) carbon-aware scheduling aligned with
renewable energy availability. Evaluations on MNIST and CIFAR-10 using
lightweight ResNet architectures demonstrate that MetaFed achieves up to 25\%
reduction in carbon emissions compared to conventional approaches, while
maintaining high accuracy and minimal communication overhead. These results
highlight MetaFed as a scalable solution for building environmentally
responsible and privacy-compliant Metaverse infrastructures.

</details>


### [2] [Quantum-Inspired DRL Approach with LSTM and OU Noise for Cut Order Planning Optimization](https://arxiv.org/abs/2508.16611)
*Yulison Herry Chrisnanto,Julian Evan Chrisnanto*

Main category: cs.LG

TL;DR: 提出量子启发的深度强化学习框架(QI-DRL)，结合LSTM和Ornstein-Uhlenbeck噪声，用于纺织行业裁切订单规划优化，相比传统方法节省13%的布料成本。


<details>
  <summary>Details</summary>
Motivation: 纺织行业裁切订单规划直接影响布料利用率和生产成本，传统基于静态启发式和目录估算的方法难以适应动态生产环境，导致次优解和浪费增加。

Method: 量子启发的深度强化学习框架，整合LSTM网络捕捉序列依赖关系，使用Ornstein-Uhlenbeck噪声实现平滑探索和快速收敛。

Result: 经过1000轮训练，平均奖励0.81(±0.03)，预测损失降至0.15(±0.02)，相比传统方法节省高达13%的布料成本，统计评估显示低变异性和稳定收敛。

Conclusion: 尽管模拟模型做了简化假设，但结果证明了该可扩展自适应框架在提升制造效率方面的潜力，为COP优化领域的未来创新铺平了道路。

Abstract: Cut order planning (COP) is a critical challenge in the textile industry,
directly impacting fabric utilization and production costs. Conventional
methods based on static heuristics and catalog-based estimations often struggle
to adapt to dynamic production environments, resulting in suboptimal solutions
and increased waste. In response, we propose a novel Quantum-Inspired Deep
Reinforcement Learning (QI-DRL) framework that integrates Long Short-Term
Memory (LSTM) networks with Ornstein-Uhlenbeck noise. This hybrid approach is
designed to explicitly address key research questions regarding the benefits of
quantum-inspired probabilistic representations, the role of LSTM-based memory
in capturing sequential dependencies, and the effectiveness of OU noise in
facilitating smooth exploration and faster convergence. Extensive training over
1000 episodes demonstrates robust performance, with an average reward of 0.81
(-+0.03) and a steady decrease in prediction loss to 0.15 (-+0.02). A
comparative analysis reveals that the proposed approach achieves fabric cost
savings of up to 13% compared to conventional methods. Furthermore, statistical
evaluations indicate low variability and stable convergence. Despite the fact
that the simulation model makes several simplifying assumptions, these
promising results underscore the potential of the scalable and adaptive
framework to enhance manufacturing efficiency and pave the way for future
innovations in COP optimization.

</details>


### [3] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: AQ-PCDSys是一个专门为行星探测任务设计的自适应量化陨石坑检测系统，通过量化神经网络和多传感器融合技术，在资源受限的计算硬件上实现实时、高精度的陨石坑检测。


<details>
  <summary>Details</summary>
Motivation: 行星自主探测任务严重依赖实时准确的环境感知进行导航和危险规避，但在资源受限的计算硬件上部署深度学习模型仍是一个重大挑战。

Method: 结合量化神经网络架构（使用量化感知训练）和自适应多传感器融合模块，在特征层面智能融合光学图像和数字高程模型数据，并采用自适应权重机制动态优先处理最相关的传感器模态。

Result: 系统显著优化了模型大小和推理延迟，适合在空间探测任务中实时部署，同时保持高精度，增强了在不同行星景观中的检测鲁棒性。

Conclusion: AQ-PCDSys为行星陨石坑检测提供了一个计算高效、可靠且准确的解决方案，是实现下一代自主行星着陆、导航和科学探索的关键能力。

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


### [4] [CrystalDiT: A Diffusion Transformer for Crystal Generation](https://arxiv.org/abs/2508.16614)
*Xiaohan Yi,Guikun Xu,Xi Xiao,Zhong Zhang,Liu Liu,Yatao Bian,Peilin Zhao*

Main category: cs.LG

TL;DR: CrystalDiT是一个用于晶体结构生成的扩散变换器，通过简单的统一变换器架构实现了最先进的性能，在MP-20数据集上达到9.62%的SUN率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 挑战当前架构复杂化的趋势，证明在数据有限的科学领域中，精心设计的简单架构比容易过拟合的复杂替代方案更有效。

Method: 采用统一的变换器架构，将晶格和原子属性视为单一相互依赖系统，结合基于元素周期表的原子表示和平衡训练策略。

Result: 在MP-20数据集上达到9.62%的SUN率（稳定、独特、新颖），生成63.28%的独特新颖结构，同时保持相当的稳定性，显著优于FlowMM（4.38%）和MatterGen（3.42%）。

Conclusion: 架构简单性在材料发现中可能比复杂性更有效，特别是在数据有限的科学领域，精心设计的简单架构能够避免过拟合问题。

Abstract: We present CrystalDiT, a diffusion transformer for crystal structure
generation that achieves state-of-the-art performance by challenging the trend
of architectural complexity. Instead of intricate, multi-stream designs,
CrystalDiT employs a unified transformer that imposes a powerful inductive
bias: treating lattice and atomic properties as a single, interdependent
system. Combined with a periodic table-based atomic representation and a
balanced training strategy, our approach achieves 9.62% SUN (Stable, Unique,
Novel) rate on MP-20, substantially outperforming recent methods including
FlowMM (4.38%) and MatterGen (3.42%). Notably, CrystalDiT generates 63.28%
unique and novel structures while maintaining comparable stability rates,
demonstrating that architectural simplicity can be more effective than
complexity for materials discovery. Our results suggest that in data-limited
scientific domains, carefully designed simple architectures outperform
sophisticated alternatives that are prone to overfitting.

</details>


### [5] [Leveraging the Christoffel Function for Outlier Detection in Data Streams](https://arxiv.org/abs/2508.16617)
*Kévin Ducharlet,Louise Travé-Massuyès,Jean-Bernard Lasserre,Marie-Véronique Le Lann,Youssef Miloudi*

Main category: cs.LG

TL;DR: 本文提出了两种新的数据流异常检测方法DyCF和DyCG，基于Christoffel函数理论，无需复杂参数调优，在低维数据处理和内存效率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测在数据挖掘中至关重要，但现有方法普遍存在参数调优复杂的问题，且难以处理非平稳分布和海量数据流的挑战。

Method: DyCF利用近似理论和正交多项式中的Christoffel函数，DyCG则利用Christoffel函数的增长特性，两种方法都基于严格的代数框架，无需调参且无内存成本地维护数据历史。

Result: 实验表明DyCF在执行时间和内存使用方面优于精细调优方法，DyCG虽然性能稍逊但完全无需调参，在合成和真实工业数据流上都进行了全面对比。

Conclusion: DyCF提供了卓越的性能表现，而DyCG具有完全无需调参的重要优势，两种方法都能有效满足数据流处理的关键需求，特别是在低维数据处理方面。

Abstract: Outlier detection holds significant importance in the realm of data mining,
particularly with the growing pervasiveness of data acquisition methods. The
ability to identify outliers in data streams is essential for maintaining data
quality and detecting faults. However, dealing with data streams presents
challenges due to the non-stationary nature of distributions and the
ever-increasing data volume. While numerous methods have been proposed to
tackle this challenge, a common drawback is the lack of straightforward
parameterization in many of them. This article introduces two novel methods:
DyCF and DyCG. DyCF leverages the Christoffel function from the theory of
approximation and orthogonal polynomials. Conversely, DyCG capitalizes on the
growth properties of the Christoffel function, eliminating the need for tuning
parameters. Both approaches are firmly rooted in a well-defined algebraic
framework, meeting crucial demands for data stream processing, with a specific
focus on addressing low-dimensional aspects and maintaining data history
without memory cost. A comprehensive comparison between DyCF, DyCG, and
state-of-the-art methods is presented, using both synthetic and real industrial
data streams. The results show that DyCF outperforms fine-tuning methods,
offering superior performance in terms of execution time and memory usage. DyCG
performs less well, but has the considerable advantage of requiring no tuning
at all.

</details>


### [6] [STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction with Future Spatiotemporal Contexts](https://arxiv.org/abs/2508.16620)
*Bangchao Deng,Lianhua Ji,Chunhua Chen,Xin Jing,Ling Ding,Bingqing QU,Pengyang Wang,Dingqi Yang*

Main category: cs.LG

TL;DR: STRelay是一个时空中继框架，通过显式建模未来时空上下文来提升位置预测模型的性能，在四个真实轨迹数据集上平均提升3.19%-11.56%


<details>
  <summary>Details</summary>
Motivation: 现有位置预测方法主要依赖历史轨迹数据，但忽略了未来时空上下文的重要性，而未来时空信息（如旅行时间和距离）对预测下一个位置具有关键线索作用

Method: STRelay以中继方式建模未来时空上下文，与基础位置预测模型的历史表示集成，通过多任务学习同时预测下一个时间间隔、移动距离间隔和最终位置

Result: 在四个真实轨迹数据集上，STRelay与四种最先进的基础模型集成后，预测性能一致提升3.19%-11.56%，特别对娱乐相关位置和长途旅行用户群体效果显著

Conclusion: 未来时空上下文对非日常例行活动（如娱乐）具有特别帮助，这与擅长建模日常规律模式的基础模型形成互补，提高了对不确定性较高活动的预测能力

Abstract: Next location prediction is a critical task in human mobility modeling,
enabling applications like travel planning and urban mobility management.
Existing methods mainly rely on historical spatiotemporal trajectory data to
train sequence models that directly forecast future locations. However, they
often overlook the importance of the future spatiotemporal contexts, which are
highly informative for the future locations. For example, knowing how much time
and distance a user will travel could serve as a critical clue for predicting
the user's next location. Against this background, we propose \textbf{STRelay},
a universal \textbf{\underline{S}}patio\textbf{\underline{T}}emporal
\textbf{\underline{Relay}}ing framework explicitly modeling the future
spatiotemporal context given a human trajectory, to boost the performance of
different location prediction models. Specifically, STRelay models future
spatiotemporal contexts in a relaying manner, which is subsequently integrated
with the encoded historical representation from a base location prediction
model, enabling multi-task learning by simultaneously predicting the next time
interval, next moving distance interval, and finally the next location. We
evaluate STRelay integrated with four state-of-the-art location prediction base
models on four real-world trajectory datasets. Results demonstrate that STRelay
consistently improves prediction performance across all cases by
3.19\%-11.56\%. Additionally, we find that the future spatiotemporal contexts
are particularly helpful for entertainment-related locations and also for user
groups who prefer traveling longer distances. The performance gain on such
non-daily-routine activities, which often suffer from higher uncertainty, is
indeed complementary to the base location prediction models that often excel at
modeling regular daily routine patterns.

</details>


### [7] [A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction](https://arxiv.org/abs/2508.16623)
*Weilin Ruan,Xilin Dang,Ziyu Zhou,Sisuo Lyu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了RAST框架，通过检索增强机制解决交通预测中复杂时空依赖建模和细粒度时空点预测精度低的挑战


<details>
  <summary>Details</summary>
Motivation: 现有时空图神经网络和预训练模型在交通预测中仍面临两个关键挑战：(i)建模复杂时空依赖时的上下文容量有限，(ii)由于异质模式导致细粒度时空点的可预测性低

Method: 基于检索增强生成(RAG)思想，提出RAST框架，包含三个关键设计：解耦编码器和查询生成器、时空检索存储和检索器、通用骨干预测器

Result: 在六个真实世界交通网络（包括大规模数据集）上的广泛实验表明，RAST实现了优越性能同时保持计算效率

Conclusion: RAST是一个将检索增强机制与时空建模相结合的通用框架，能有效提升交通预测性能

Abstract: Traffic prediction is a cornerstone of modern intelligent transportation
systems and a critical task in spatio-temporal forecasting. Although advanced
Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have
achieved significant progress in traffic prediction, two key challenges remain:
(i) limited contextual capacity when modeling complex spatio-temporal
dependencies, and (ii) low predictability at fine-grained spatio-temporal
points due to heterogeneous patterns. Inspired by Retrieval-Augmented
Generation (RAG), we propose RAST, a universal framework that integrates
retrieval-augmented mechanisms with spatio-temporal modeling to address these
challenges. Our framework consists of three key designs: 1) Decoupled Encoder
and Query Generator to capture decoupled spatial and temporal features and
construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval
Store and Retrievers to maintain and retrieve vectorized fine-grained patterns;
and 3) Universal Backbone Predictor that flexibly accommodates pre-trained
STGNNs or simple MLP predictors. Extensive experiments on six real-world
traffic networks, including large-scale datasets, demonstrate that RAST
achieves superior performance while maintaining computational efficiency.

</details>


### [8] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的自适应记忆框架，通过建模记忆周期来优化LLM智能体的记忆能力，包括MoE门控检索、可学习聚合和任务特定反思机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的记忆机制由人工预定义，导致高人工成本和次优性能，且忽视了交互场景中的记忆周期效应。

Method: 设计MoE门控函数促进记忆检索，提出可学习聚合过程改进记忆利用，开发任务特定反思机制适应记忆存储，支持离线和在线策略优化。

Result: 在多个方面进行了全面实验验证方法有效性，并开源了项目代码。

Conclusion: 该记忆框架使LLM智能体能够在特定环境中有效学习记忆信息，解决了现有方法的局限性。

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [9] [Recurrent Transformer U-Net Surrogate for Flow Modeling and Data Assimilation in Subsurface Formations with Faults](https://arxiv.org/abs/2508.16631)
*Yifu Han,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 基于重复变换器U-Net的代模型，用于效极预测地层碳存储中的压力和CO2饱和度，对带有不确定断层的地质系统进行效率高的流体流动模拟。


<details>
  <summary>Details</summary>
Motivation: 地下层形成物包含大量断层，这些断层强烈影响流体流动，特别是在地质碳存储中。传统数值模拟计算成本高，需要快速准确的代模型来支持不确定性分析和数据同化。

Method: 开发了重复变换器U-Net代模型，训练使用4000个随机采样的地质实现。模型考虑了层次不确定性，包括地质参数和细胞属性的不确定性，断层测逐率也作为不确定参数。

Result: 新代模模型比之前的重复殊差U-Net更准确，并在不同漏泉场景下保持准确性。通过全局敏感性分析和层次Markov链蒙特卡洛数据同化程序，证明在所有三个水层中测量压力和饱和度可显著降低不确定性。

Conclusion: 重复变换器U-Net代模型能够提供准确且高效的预测，在地质碳存储监测策略中具有重要价值，特别是在多水层监测方案中显示出显著的不确定性减少效果。

Abstract: Many subsurface formations, including some of those under consideration for
large-scale geological carbon storage, include extensive faults that can
strongly impact fluid flow. In this study, we develop a new recurrent
transformer U-Net surrogate model to provide very fast predictions for pressure
and CO2 saturation in realistic faulted subsurface aquifer systems. The
geomodel includes a target aquifer (into which supercritical CO2 is injected),
surrounding regions, caprock, two extensive faults, and two overlying aquifers.
The faults can act as leakage pathways between the three aquifers. The
heterogeneous property fields in the target aquifer are characterized by
hierarchical uncertainty, meaning both the geological metaparameters (e.g.,
mean and standard deviation of log-permeability) and the detailed cell
properties of each realization, are uncertain. Fault permeabilities are also
treated as uncertain. The model is trained with simulation results for (up to)
4000 randomly sampled realizations. Error assessments show that this model is
more accurate than a previous recurrent residual U-Net, and that it maintains
accuracy for qualitatively different leakage scenarios. The new surrogate is
then used for global sensitivity analysis and data assimilation. A hierarchical
Markov chain Monte Carlo data assimilation procedure is applied. Different
monitoring strategies, corresponding to different amounts and types of observed
data collected at monitoring wells, are considered for three synthetic true
models. Detailed results demonstrate the degree of uncertainty reduction
achieved with the various monitoring strategies. Posterior results for 3D
saturation plumes and leakage volumes indicate the benefits of measuring
pressure and saturation in all three aquifers.

</details>


### [10] [Adaptive Variance-Penalized Continual Learning with Fisher Regularization](https://arxiv.org/abs/2508.16632)
*Krisanu Sarkar*

Main category: cs.LG

TL;DR: 提出了一种新颖的持续学习框架，通过Fisher加权的非对称正则化结合变分学习，动态调节正则化强度，有效解决神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络中的灾难性遗忘问题是持续学习领域的主要挑战，需要开发能够保持先前任务知识同时学习新任务的方法。

Method: 将Fisher加权的参数方差非对称正则化集成到变分学习范式中，根据参数不确定性动态调节正则化强度。

Result: 在SplitMNIST、PermutedMNIST和SplitFashionMNIST等标准基准测试中，相比VCL和EWC等现有方法有显著提升，有效维持了跨序列任务的知识并提高了模型准确性。

Conclusion: 该方法不仅提升了即时任务性能，还显著减轻了随时间推移的知识退化，成功解决了神经网络灾难性遗忘的根本挑战。

Abstract: The persistent challenge of catastrophic forgetting in neural networks has
motivated extensive research in continual learning . This work presents a novel
continual learning framework that integrates Fisher-weighted asymmetric
regularization of parameter variances within a variational learning paradigm.
Our method dynamically modulates regularization intensity according to
parameter uncertainty, achieving enhanced stability and performance.
Comprehensive evaluations on standard continual learning benchmarks including
SplitMNIST, PermutedMNIST, and SplitFashionMNIST demonstrate substantial
improvements over existing approaches such as Variational Continual Learning
and Elastic Weight Consolidation . The asymmetric variance penalty mechanism
proves particularly effective in maintaining knowledge across sequential tasks
while improving model accuracy. Experimental results show our approach not only
boosts immediate task performance but also significantly mitigates knowledge
degradation over time, effectively addressing the fundamental challenge of
catastrophic forgetting in neural networks

</details>


### [11] [A Novel Unified Extended Matrix for Graph Signal Processing: Theory and Application](https://arxiv.org/abs/2508.16633)
*Yunyan Zheng,Zhichao Zhang,Wei Yao*

Main category: cs.LG

TL;DR: 本文提出统一扩展矩阵（UEM）框架，通过参数化设计结合扩展邻接矩阵和统一图表示矩阵，解决传统图移移操作符无法模型非相邻节点依赖关系的问题，并基于此提出UEM-GFT图普量变换方法，在异常检测任务中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统图移移操作符（GSOs）在模型非相邻节点之间的依赖关系时缺乏灵活性，限制了对复杂图结构的表示能力，因此需要一种更灵活的框架来揭示更多的图信号信息。

Method: 提出统一扩展矩阵（UEM）框架，通过参数化设计集成扩展邻接矩阵和统一图表示矩阵，并进行理论分析证明其在特定条件下的半正定性和特征值单调性。基于UEM提出图普量变换（UEM-GFT），能够自适应地调整谱性质来提升信号处理性能。

Result: 在合成和实际数据集上的实验结果显示，UEM-GFT在异常检测任务中表现超过现有的GSO基方法，在不同网络拓扑结构下均取得优异性能。

Conclusion: UEM框架提供了一种灵活的方法来模型复杂图结构中的节点依赖关系，UEM-GFT通过自适应调整谱性质，显著提升了图信号处理的性能，为图信号处理领域提供了有效的新方法。

Abstract: Graph signal processing has become an essential tool for analyzing data
structured on irregular domains. While conventional graph shift operators
(GSOs) are effective for certain tasks, they inherently lack flexibility in
modeling dependencies between non-adjacent nodes, limiting their ability to
represent complex graph structures. To address this limitation, this paper
proposes the unified extended matrix (UEM) framework, which integrates the
extended-adjacency matrix and the unified graph representation matrix through
parametric design, so as to be able to flexibly adapt to different graph
structures and reveal more graph signal information. Theoretical analysis of
the UEM is conducted, demonstrating positive semi-definiteness and eigenvalue
monotonicity under specific conditions. Then, we propose graph Fourier
transform based on UEM (UEM-GFT), which can adaptively tune spectral properties
to enhance signal processing performance. Experimental results on synthetic and
real-world datasets demonstrate that the UEM-GFT outperforms existing GSO-based
methods in anomaly detection tasks, achieving superior performance across
varying network topologies.

</details>


### [12] [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models](https://arxiv.org/abs/2508.17467)
*Krishna Teja Chitty-Venkata,Sylvia Howland,Golara Azar,Daria Soboleva,Natalia Vassilieva,Siddhisanket Raskar,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: MoE-Inference-Bench是一个系统性评估混合专家模型推理性能的基准测试，分析了不同配置和优化技术对MoE模型吞吐量的影响。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型虽然通过大规模参数实现了LLM和VLM的扩展，但在推理时存在负载不均衡和路由计算开销等问题，需要系统性的硬件加速技术评估。

Method: 在Nvidia H100 GPU上评估不同批大小、序列长度、FFN维度和专家数量等超参数，测试剪枝、融合MoE操作、推测解码、量化和并行化等多种优化技术，涵盖Mixtral、DeepSeek、OLMoE和Qwen等MoE模型家族。

Result: 研究揭示了不同配置下的性能差异，为MoE模型的高效部署提供了重要见解。

Conclusion: 该基准测试为混合专家模型的推理性能优化提供了系统性评估框架，有助于实现MoE模型的高效部署和应用。

Abstract: Mixture of Experts (MoE) models have enabled the scaling of Large Language
Models (LLMs) and Vision Language Models (VLMs) by achieving massive parameter
counts while maintaining computational efficiency. However, MoEs introduce
several inference-time challenges, including load imbalance across experts and
the additional routing computational overhead. To address these challenges and
fully harness the benefits of MoE, a systematic evaluation of hardware
acceleration techniques is essential. We present MoE-Inference-Bench, a
comprehensive study to evaluate MoE performance across diverse scenarios. We
analyze the impact of batch size, sequence length, and critical MoE
hyperparameters such as FFN dimensions and number of experts on throughput. We
evaluate several optimization techniques on Nvidia H100 GPUs, including
pruning, Fused MoE operations, speculative decoding, quantization, and various
parallelization strategies. Our evaluation includes MoEs from the Mixtral,
DeepSeek, OLMoE and Qwen families. The results reveal performance differences
across configurations and provide insights for the efficient deployment of
MoEs.

</details>


### [13] [Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations](https://arxiv.org/abs/2508.16634)
*Zhendong Yang,Jie Wang,Liansong Zong,Xiaorong Liu,Quan Qian,Shiqian Chen*

Main category: cs.LG

TL;DR: 提出DGGN框架解决少样本类增量故障诊断问题，通过双粒度表征和边界感知策略有效缓解灾难性遗忘和新数据过拟合


<details>
  <summary>Details</summary>
Motivation: 工业系统中需要持续学习新故障类别但样本稀少，传统方法面临灾难性遗忘和过拟合问题

Method: 双粒度表征网络：细粒度流捕获类别特征，粗粒度流保留通用知识；多语义交叉注意力融合；边界感知样本优先策略；解耦平衡随机森林分类器

Result: 在TEP基准和真实MFF数据集上优于现有FSC-FD方法，诊断性能和稳定性显著提升

Conclusion: DGGN框架通过双粒度表征和动态融合机制，有效解决了少样本增量学习中的关键挑战

Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to
continuously learn from new fault classes with only a few samples without
forgetting old ones, is critical for real-world industrial systems. However,
this challenging task severely amplifies the issues of catastrophic forgetting
of old knowledge and overfitting on scarce new data. To address these
challenges, this paper proposes a novel framework built upon Dual-Granularity
Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN
explicitly decouples feature learning into two parallel streams: 1) a
fine-grained representation stream, which utilizes a novel Multi-Order
Interaction Aggregation module to capture discriminative, class-specific
features from the limited new samples. 2) a coarse-grained representation
stream, designed to model and preserve general, class-agnostic knowledge shared
across all fault types. These two representations are dynamically fused by a
multi-semantic cross-attention mechanism, where the stable coarse-grained
knowledge guides the learning of fine-grained features, preventing overfitting
and alleviating feature conflicts. To further mitigate catastrophic forgetting,
we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a
decoupled Balanced Random Forest classifier is employed to counter the decision
boundary bias caused by data imbalance. Extensive experiments on the TEP
benchmark and a real-world MFF dataset demonstrate that our proposed DGGN
achieves superior diagnostic performance and stability compared to
state-of-the-art FSC-FD approaches. Our code is publicly available at
https://github.com/MentaY/DGGN

</details>


### [14] [Enhancing Transformer-Based Foundation Models for Time Series Forecasting via Bagging, Boosting and Statistical Ensembles](https://arxiv.org/abs/2508.16641)
*Dhruv D. Modi,Rong Pan*

Main category: cs.LG

TL;DR: 本文研究统计和集成增强技术来改进时间序列基础模型的预测性能，通过bagging、stacking、残差建模等方法在电力负荷预测数据集上显著提升了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测、异常检测等方面表现出强大的泛化能力，但在实际应用中仍存在方差大、领域偏差和不确定性量化有限等问题，需要统计方法来增强其可靠性。

Method: 采用了一套统计和集成增强技术，包括基于bootstrap的bagging、回归stacking、预测区间构建、统计残差建模和迭代误差反馈等方法，将统计推理与现代基础模型相结合。

Result: 在比利时电力短期负荷预测数据集上的实验表明，所提出的混合方法在多个预测范围内持续优于独立基础模型：回归集成获得最低均方误差，bootstrap聚合显著减少长上下文误差，残差建模纠正系统偏差，预测区间实现接近名义覆盖率。

Conclusion: 将统计推理与现代基础模型集成可以在准确性、可靠性和可解释性方面带来可衡量的收益，为现实世界时间序列应用提供了有效的增强方案。

Abstract: Time series foundation models (TSFMs) such as Lag-Llama, TimeGPT, Chronos,
MOMENT, UniTS, and TimesFM have shown strong generalization and zero-shot
capabilities for time series forecasting, anomaly detection, classification,
and imputation. Despite these advantages, their predictions still suffer from
variance, domain-specific bias, and limited uncertainty quantification when
deployed on real operational data. This paper investigates a suite of
statistical and ensemble-based enhancement techniques, including
bootstrap-based bagging, regression-based stacking, prediction interval
construction, statistical residual modeling, and iterative error feedback, to
improve robustness and accuracy. Using the Belgium Electricity Short-Term Load
Forecasting dataset as a case study, we demonstrate that the proposed hybrids
consistently outperform standalone foundation models across multiple horizons.
Regression-based ensembles achieve the lowest mean squared error; bootstrap
aggregation markedly reduces long-context errors; residual modeling corrects
systematic bias; and the resulting prediction intervals achieve near nominal
coverage with widths shrinking as context length increases. The results
indicate that integrating statistical reasoning with modern foundation models
yields measurable gains in accuracy, reliability, and interpretability for
real-world time series applications.

</details>


### [15] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: 本文评估了基于Mamba的状态空间模型在GPU上的训练行为，构建了代表性工作负载套件，分析了GPU微架构影响，并提出了潜在优化方向。


<details>
  <summary>Details</summary>
Motivation: Mamba状态空间模型作为Transformer的有前途替代方案，具有线性计算复杂度优势，但需要了解其在GPU上的行为特征以指导硬件设计优化。

Method: 构建包含不同模型架构的Mamba SSM工作负载套件，在GPU上进行训练行为分析，研究微架构层面的性能特征。

Result: 系统性地表征了Mamba SSM在GPU上的运行行为，识别了关键性能瓶颈和计算模式，为GPU架构优化提供了数据支撑。

Conclusion: 研究揭示了Mamba SSM在GPU上的独特行为特征，为未来GPU微架构设计提供了优化方向，有助于继续提升此类模型的性能扩展性。

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


### [16] [From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective](https://arxiv.org/abs/2508.16643)
*Tianhua Chen*

Main category: cs.LG

TL;DR: 本文提供了一个统一的概率潜变量模型(PLVM)视角，将经典和现代生成方法都纳入该范式，揭示了生成式AI的共同理论基础和方法论脉络。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI系统架构多样，但许多都基于概率潜变量模型的共同基础。本文旨在通过PLVM范式统一理解从经典到现代的生成方法，揭示共享原理和推理策略。

Method: 通过概率潜变量模型框架，系统分析从经典平面模型(概率PCA、高斯混合模型等)到序列扩展(HMM、LDS等)，再到深度架构(VAE、归一化流、扩散模型、自回归模型、GAN)的演进路径。

Result: 建立了生成式AI的统一概率分类体系，揭示了不同架构的共享原则、推理策略差异和表征权衡，提供了概念路线图。

Conclusion: PLVM范式为理解生成式AI提供了统一的理论基础，澄清了方法论谱系，能够指导未来创新并将新兴架构植根于其概率传统中。

Abstract: From large language models to multi-modal agents, Generative Artificial
Intelligence (AI) now underpins state-of-the-art systems. Despite their varied
architectures, many share a common foundation in probabilistic latent variable
models (PLVMs), where hidden variables explain observed data for density
estimation, latent reasoning, and structured inference. This paper presents a
unified perspective by framing both classical and modern generative methods
within the PLVM paradigm. We trace the progression from classical flat models
such as probabilistic PCA, Gaussian mixture models, latent class analysis, item
response theory, and latent Dirichlet allocation, through their sequential
extensions including Hidden Markov Models, Gaussian HMMs, and Linear Dynamical
Systems, to contemporary deep architectures: Variational Autoencoders as Deep
PLVMs, Normalizing Flows as Tractable PLVMs, Diffusion Models as Sequential
PLVMs, Autoregressive Models as Explicit Generative Models, and Generative
Adversarial Networks as Implicit PLVMs. Viewing these architectures under a
common probabilistic taxonomy reveals shared principles, distinct inference
strategies, and the representational trade-offs that shape their strengths. We
offer a conceptual roadmap that consolidates generative AI's theoretical
foundations, clarifies methodological lineages, and guides future innovation by
grounding emerging architectures in their probabilistic heritage.

</details>


### [17] [AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training](https://arxiv.org/abs/2508.16647)
*Boran Zhao,Hetian Liu,Zihang Yuan,Li Zhu,Fan Yang,Lina Xie Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: 这篇论文提出了AdapSNE方法，通过火算法和熵指导优化改善了边缘设备上的样本采样效果，并设计了专用加速器降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上训练深度神经网络时面临的大规模数据集负担问题，尤其是现有NMS方法存在的异常倾向和不均匀采样问题。

Method: 结合火算法(FWA)压制异常倾向，使用熵指导优化实现均匀采样，设计专用加速器降低迭代计算成本。

Result: AdapSNE能够提供更代表性的训练样本，提高训练准确性，同时显著降低了边缘设备的能耗和面积。

Conclusion: 该方法有效解决了边缘设备上的样本采样问题，为边缘训练提供了更高效的解决方案。

Abstract: Training deep neural networks (DNNs) directly on edge devices has attracted
increasing attention, as it offers promising solutions to challenges such as
domain adaptation and privacy preservation. However, conventional DNN training
typically requires large-scale datasets, which imposes prohibitive overhead on
edge devices-particularly for emerging large language model (LLM) tasks. To
address this challenge, a DNN-free method (ie., dataset sampling without DNN),
named NMS (Near-Memory Sampling), has been introduced. By first conducting
dimensionality reduction of the dataset and then performing exemplar sampling
in the reduced space, NMS avoids the architectural bias inherent in DNN-based
methods and thus achieves better generalization. However, The state-of-the-art,
NMS, suffers from two limitations: (1) The mismatch between the search method
and the non-monotonic property of the perplexity error function leads to the
emergence of outliers in the reduced representation; (2) Key parameter (ie.,
target perplexity) is selected empirically, introducing arbitrariness and
leading to uneven sampling. These two issues lead to representative bias of
examplars, resulting in degraded accuracy. To address these issues, we propose
AdapSNE, which integrates an efficient non-monotonic search method-namely, the
Fireworks Algorithm (FWA)-to suppress outliers, and employs entropy-guided
optimization to enforce uniform sampling, thereby ensuring representative
training samples and consequently boosting training accuracy. To cut the
edge-side cost arising from the iterative computations of FWA search and
entropy-guided optimization, we design an accelerator with custom dataflow and
time-multiplexing markedly reducing on-device training energy and area.

</details>


### [18] [LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping](https://arxiv.org/abs/2508.16648)
*Junle Liu,Chang Liu,Yanyu Ke,Qiuxiang Huang,Jiachen Zhao,Wenliang Chen,K. T. Tse,Gang Hu*

Main category: cs.LG

TL;DR: LatentFlow是一个跨模态时间上采样框架，通过融合低频流场和压力数据，仅使用高频壁面压力信号就能重建高频湍流尾流场


<details>
  <summary>Details</summary>
Motivation: 解决PIV实验中获取时空高分辨率湍流尾流场的硬件限制和测量噪声问题，利用更易获得的高频壁面压力测量来重建流场

Method: 两阶段方法：1) 训练压力条件β-VAE学习流场紧凑潜在表示；2) 次级网络将低频压力映射到潜在空间，推理时使用高频压力通过解码器生成高频流场

Result: 能够从稀疏壁面压力输入重建512Hz高频湍流尾流场，实现了时空解耦的流场重建

Conclusion: LatentFlow为数据受限的实验环境提供了可扩展且鲁棒的高频湍流尾流重建解决方案

Abstract: Acquiring temporally high-frequency and spatially high-resolution turbulent
wake flow fields in particle image velocimetry (PIV) experiments remains a
significant challenge due to hardware limitations and measurement noise. In
contrast, temporal high-frequency measurements of spatially sparse wall
pressure are more readily accessible in wind tunnel experiments. In this study,
we propose a novel cross-modal temporal upscaling framework, LatentFlow, which
reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing
synchronized low-frequency (15 Hz) flow field and pressure data during
training, and high-frequency wall pressure signals during inference. The first
stage involves training a pressure-conditioned $\beta$-variation autoencoder
($p$C-$\beta$-VAE) to learn a compact latent representation that captures the
intrinsic dynamics of the wake flow. A secondary network maps synchronized
low-frequency wall pressure signals into the latent space, enabling
reconstruction of the wake flow field solely from sparse wall pressure. Once
trained, the model utilizes high-frequency, spatially sparse wall pressure
inputs to generate corresponding high-frequency flow fields via the
$p$C-$\beta$-VAE decoder. By decoupling the spatial encoding of flow dynamics
from temporal pressure measurements, LatentFlow provides a scalable and robust
solution for reconstructing high-frequency turbulent wake flows in
data-constrained experimental settings.

</details>


### [19] [HiCL: Hippocampal-Inspired Continual Learning](https://arxiv.org/abs/2508.16651)
*Kushal Kapoor,Wyatt Mackey,Yiannis Aloimonos,Xiaomin Lin*

Main category: cs.LG

TL;DR: HiCL是一个受海马体启发的双记忆持续学习架构，通过网格细胞层、齿状回稀疏模式分离、CA3自联想记忆和基于余弦相似度的专家路由机制，有效缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘问题，从生物海马体结构中获取灵感，设计更高效、可扩展的神经网络架构。

Method: 使用网格细胞层编码输入，齿状回模块进行稀疏模式分离，CA3自联想记忆存储情景记忆，基于余弦相似度的DG门控专家混合机制进行任务路由，结合弹性权重巩固和优先回放机制。

Result: 在标准持续学习基准测试中表现出色，减少了任务干扰，以较低计算成本实现了接近最先进水平的性能。

Conclusion: HiCL架构通过生物启发的设计原则，提供了可微分、可扩展的任务路由方案，在持续学习任务中展现出优越的性能和效率。

Abstract: We propose HiCL, a novel hippocampal-inspired dual-memory continual learning
architecture designed to mitigate catastrophic forgetting by using elements
inspired by the hippocampal circuitry. Our system encodes inputs through a
grid-cell-like layer, followed by sparse pattern separation using a dentate
gyrus-inspired module with top-k sparsity. Episodic memory traces are
maintained in a CA3-like autoassociative memory. Task-specific processing is
dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs
are routed to experts based on cosine similarity between their normalized
sparse DG representations and learned task-specific DG prototypes computed
through online exponential moving averages. This biologically grounded yet
mathematically principled gating strategy enables differentiable, scalable
task-routing without relying on a separate gating network, and enhances the
model's adaptability and efficiency in learning multiple sequential tasks.
Cortical outputs are consolidated using Elastic Weight Consolidation weighted
by inter-task similarity. Crucially, we incorporate prioritized replay of
stored patterns to reinforce essential past experiences. Evaluations on
standard continual learning benchmarks demonstrate the effectiveness of our
architecture in reducing task interference, achieving near state-of-the-art
results in continual learning tasks at lower computational costs.

</details>


### [20] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: 使用Transformer模型结合Laplace扩散技术，通过运动上下文嵌入和注意机制来模拟心率波动，在真实病人数据集上实现了更高的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 远程患者监测中心率变化受多种因素影响，需要结合运动上下文来准确评估心率变化的意义，而现有模型很少集成运动数据。

Method: 提出了基于Transformer的模型，结合Laplace扩散技术，使用专门的运动上下文嵌入和注意机制来建模心率波动，并通过专门编码器捕捉长期模式和运动特异心率动态。

Result: 在29名病人4个月的真实数据集上验证，模型在平均绝对误差上比基线模型减少43%，决定系数R2达到0.97，显示预测与实际心率值高度一致。

Conclusion: 该模型是一种实用有效的工具，能够支持医疗服务提供者和远程患者监测系统。

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [21] [OASIS: Open-world Adaptive Self-supervised and Imbalanced-aware System](https://arxiv.org/abs/2508.16656)
*Miru Kim,Mugon Joe,Minhae Kwon*

Main category: cs.LG

TL;DR: 提出了一种针对类别不平衡数据的开放世界问题解决方案，通过对比式预训练和选择性激活的后训练机制，显著提升了模型在开放环境中的性能和效率


<details>
  <summary>Details</summary>
Motivation: 机器学习在动态环境中面临开放世界挑战，包括标签偏移、协变量偏移和未知类别出现。现有后训练方法在处理类别不平衡的预训练数据时泛化能力有限，特别是对少数类别的处理效果不佳

Method: 采用对比式预训练方法增强分类性能，特别是对少数类别的表示学习；设计后训练机制生成可靠伪标签；引入选择性激活准则优化后训练过程，减少不必要计算

Result: 大量实验表明，该方法在多种开放世界场景下，在准确性和效率方面均显著优于最先进的适应技术

Conclusion: 该方法有效解决了类别不平衡预训练数据下的开放世界问题，通过对比学习和选择性后训练机制，实现了更好的泛化性能和计算效率

Abstract: The expansion of machine learning into dynamic environments presents
challenges in handling open-world problems where label shift, covariate shift,
and unknown classes emerge. Post-training methods have been explored to address
these challenges, adapting models to newly emerging data. However, these
methods struggle when the initial pre-training is performed on class-imbalanced
datasets, limiting generalization to minority classes. To address this, we
propose a method that effectively handles open-world problems even when
pre-training is conducted on imbalanced data. Our contrastive-based
pre-training approach enhances classification performance, particularly for
underrepresented classes. Our post-training mechanism generates reliable
pseudo-labels, improving model robustness against open-world problems. We also
introduce selective activation criteria to optimize the post-training process,
reducing unnecessary computation. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art adaptation techniques in both
accuracy and efficiency across diverse open-world scenarios.

</details>


### [22] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: 提出了WISCA权重缩放方法，通过优化神经网络权重模式来提升训练效率和模型质量，无需改变网络结构


<details>
  <summary>Details</summary>
Motivation: 现有Transformer训练优化方法主要关注架构修改或优化器调整，缺乏对权重模式的系统性优化

Method: WISCA权重缩放方法，通过重新缩放权重同时保持模型输出不变，间接优化训练轨迹

Result: 显著提升收敛质量（泛化能力和损失减少），在GQA架构和LoRA微调任务中表现突出，零样本验证任务平均提升5.6%，训练困惑度平均降低2.12%

Conclusion: WISCA方法通过系统性优化权重模式，有效提升了LLM训练效率和模型性能

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [23] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: 提出RED方法，通过控制探索空间和优化离线数据整合来增强小语言模型的推理能力，解决探索不足和蒸馏冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型的推理能力提升，而小语言模型的推理能力增强尚未充分探索，需要解决蒸馏数据与在线强化学习的平衡问题。

Method: 设计RED框架，通过监控熵变比例调节离线SFT权重，并基于样本准确性的策略转移机制动态选择模仿离线数据或学习自身策略。

Result: 该方法有效解决了小模型探索空间不足和蒸馏过程冗余复杂的问题，缓解了离线数据与当前策略的分布差异。

Conclusion: RED方法为小语言模型的推理能力提升提供了有效的解决方案，通过控制探索和优化离线整合实现了性能改进。

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [24] [CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression](https://arxiv.org/abs/2508.16680)
*Muchammad Daniyal Kautsar,Afra Majida Hariono,Widyawan,Syukron Abu Ishaq Alfarozi,Kuntpong Wararatpanya*

Main category: cs.LG

TL;DR: CALR是一种新的LLM压缩方法，通过SVD主路径和可学习的低秩校正模块相结合，在显著减少参数的同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法主要关注矩阵重构误差最小化，但会导致模型功能性能显著下降，需要解决压缩过程中的功能信息损失问题

Method: CALR采用双组件压缩方法：主路径为SVD压缩层，并行添加可学习的低秩校正模块，专门训练以恢复功能残差误差

Result: 在多个模型上测试，CALR可减少26.93%-51.77%参数，同时保持59.45%-90.42%的原始性能，优于LaCo、ShortGPT和LoSparse等方法

Conclusion: 将功能信息损失视为可学习信号是一种高效的压缩范式，CALR方法能够创建更小更高效的LLM，提升实际部署的可行性

Abstract: Large Language Models (LLMs) present significant deployment challenges due to
their immense size and computational requirements. Model compression techniques
are essential for making these models practical for resource-constrained
environments. A prominent compression strategy is low-rank factorization via
Singular Value Decomposition (SVD) to reduce model parameters by approximating
weight matrices. However, standard SVD focuses on minimizing matrix
reconstruction error, often leading to a substantial loss of the model's
functional performance. This performance degradation occurs because existing
methods do not adequately correct for the functional information lost during
compression. To address this gap, we introduce Corrective Adaptive Low-Rank
Decomposition (CALR), a two-component compression approach. CALR combines a
primary path of SVD-compressed layers with a parallel, learnable, low-rank
corrective module that is explicitly trained to recover the functional residual
error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and
Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to
51.77% while retaining 59.45% to 90.42% of the original model's performance,
consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows
that treating functional information loss as a learnable signal is a highly
effective compression paradigm. This approach enables the creation of
significantly smaller, more efficient LLMs, advancing their accessibility and
practical deployment in real-world applications.

</details>


### [25] [STGAtt: A Spatial-Temporal Unified Graph Attention Network for Traffic Flow Forecasting](https://arxiv.org/abs/2508.16685)
*Zhuding Liang,Jianxun Cui,Qingshuang Zeng,Feng Liu,Nenad Filipovic,Tijana Geroski*

Main category: cs.LG

TL;DR: 提出STGAtt模型，通过时空统一图注意力网络有效捕捉交通流中的复杂时空依赖关系，在多个数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 准确及时的交通流预测对智能交通系统至关重要，现有方法在时空依赖建模方面存在分离处理的问题

Method: 使用时空统一图表示和注意力机制，直接建模时空相关性，动态权衡跨维度连接，并采用分区和交换机制捕捉短程和长程相关性

Result: 在PEMS-BAY和SHMetro数据集上的实验显示，STGAtt在不同预测时间范围内均优于最先进的基线方法

Conclusion: 可视化注意力权重证实了模型适应动态交通模式和捕捉长程依赖的能力，具有实际应用的潜力

Abstract: Accurate and timely traffic flow forecasting is crucial for intelligent
transportation systems. This paper presents a novel deep learning model, the
Spatial-Temporal Unified Graph Attention Network (STGAtt). By leveraging a
unified graph representation and an attention mechanism, STGAtt effectively
captures complex spatial-temporal dependencies. Unlike methods relying on
separate spatial and temporal dependency modeling modules, STGAtt directly
models correlations within a Spatial-Temporal Unified Graph, dynamically
weighing connections across both dimensions. To further enhance its
capabilities, STGAtt partitions traffic flow observation signal into
neighborhood subsets and employs a novel exchanging mechanism, enabling
effective capture of both short-range and long-range correlations. Extensive
experiments on the PEMS-BAY and SHMetro datasets demonstrate STGAtt's superior
performance compared to state-of-the-art baselines across various prediction
horizons. Visualization of attention weights confirms STGAtt's ability to adapt
to dynamic traffic patterns and capture long-range dependencies, highlighting
its potential for real-world traffic flow forecasting applications.

</details>


### [26] [Multidimensional Distributional Neural Network Output Demonstrated in Super-Resolution of Surface Wind Speed](https://arxiv.org/abs/2508.16686)
*Harrison J. Goldwyn,Mitchell Krock,Johann Rudi,Daniel Getter,Julie Bessac*

Main category: cs.LG

TL;DR: 提出了一种基于多维高斯损失的神经网络框架，能够生成闭式预测分布，同时捕捉空间相关性和异方差结构，适用于科学应用中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉偶然不确定性和认知不确定性，且缺乏能够保持空间相关性并提供闭式多维分布的计算可行方案。科学应用需要准确量化高维相关数据预测中的不确定性。

Method: 使用多维高斯损失训练神经网络，通过迭代估计均值和协方差矩阵来捕捉偶然不确定性。采用傅里叶表示稳定训练并保持空间相关性，引入信息共享正则化策略在图像特定和全局协方差估计间插值。

Result: 在超分辨率降尺度任务中验证了方法的有效性，能够实现高效采样、显式相关建模，且不影响预测性能。在表面风速降尺度任务中展示了应用效果。

Conclusion: 该框架为科学模型中的不确定性感知预测提供了有效解决方案，能够生成闭式预测分布，保持空间相关性，并支持扩展到更复杂的分布族。

Abstract: Accurate quantification of uncertainty in neural network predictions remains
a central challenge for scientific applications involving high-dimensional,
correlated data. While existing methods capture either aleatoric or epistemic
uncertainty, few offer closed-form, multidimensional distributions that
preserve spatial correlation while remaining computationally tractable. In this
work, we present a framework for training neural networks with a
multidimensional Gaussian loss, generating closed-form predictive distributions
over outputs with non-identically distributed and heteroscedastic structure.
Our approach captures aleatoric uncertainty by iteratively estimating the means
and covariance matrices, and is demonstrated on a super-resolution example. We
leverage a Fourier representation of the covariance matrix to stabilize network
training and preserve spatial correlation. We introduce a novel regularization
strategy -- referred to as information sharing -- that interpolates between
image-specific and global covariance estimates, enabling convergence of the
super-resolution downscaling network trained on image-specific distributional
loss functions. This framework allows for efficient sampling, explicit
correlation modeling, and extensions to more complex distribution families all
without disrupting prediction performance. We demonstrate the method on a
surface wind speed downscaling task and discuss its broader applicability to
uncertainty-aware prediction in scientific models.

</details>


### [27] [Native Logical and Hierarchical Representations with Subspace Embeddings](https://arxiv.org/abs/2508.16687)
*Gabriel Moreira,Zita Marinho,Manuel Marques,João Paulo Costeira,Chenyan Xiong*

Main category: cs.LG

TL;DR: 提出将概念表示为线性子空间而非点的新嵌入范式，通过子空间维度建模泛化性，子空间包含建模层次关系，支持集合运算和逻辑操作，在多项任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统神经嵌入将概念表示为点，擅长相似性计算但在高级推理和非对称关系建模上存在局限，需要新的表示范式来更好地支持逻辑运算和层次关系

Method: 引入线性子空间嵌入框架，使用子空间维度表示泛化程度，子空间包含表示层次关系；提出正交投影算子的平滑松弛方法，可微分地学习子空间方向和维度

Result: 在WordNet的重建和链接预测任务上达到state-of-the-art；在自然语言推理基准测试中超越双编码器基线，提供可解释的蕴含关系几何表示

Conclusion: 子空间嵌入为概念表示提供了新的几何基础，既保持了几何直观性又支持逻辑运算，在保持可解释性的同时提升了推理性能

Abstract: Traditional neural embeddings represent concepts as points, excelling at
similarity but struggling with higher-level reasoning and asymmetric
relationships. We introduce a novel paradigm: embedding concepts as linear
subspaces. This framework inherently models generality via subspace
dimensionality and hierarchy through subspace inclusion. It naturally supports
set-theoretic operations like intersection (conjunction), linear sum
(disjunction) and orthogonal complements (negations), aligning with classical
formal semantics. To enable differentiable learning, we propose a smooth
relaxation of orthogonal projection operators, allowing for the learning of
both subspace orientation and dimension. Our method achieves state-of-the-art
results in reconstruction and link prediction on WordNet. Furthermore, on
natural language inference benchmarks, our subspace embeddings surpass
bi-encoder baselines, offering an interpretable formulation of entailment that
is both geometrically grounded and amenable to logical operations.

</details>


### [28] [A novel auxiliary equation neural networks method for exactly explicit solutions of nonlinear partial differential equations](https://arxiv.org/abs/2508.16702)
*Shanhao Yuan,Yanqin Liu,Runfa Zhang,Limei Yan,Shunjun Wu,Libo Feng*

Main category: cs.LG

TL;DR: 提出辅助方程神经网络方法(AENNM)，结合神经网络和辅助方程法求解非线性偏微分方程，通过Riccati方程解构造新型激活函数，显著提高计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法求解非线性偏微分方程存在计算效率低和精度不足的问题，需要结合神经网络的强大逼近能力和符号计算的高精度优势。

Method: 提出AENNM方法，将辅助方程法嵌入神经网络框架，使用Riccati方程解构造新型激活函数，构建"2-2-2-1"和"3-2-2-1"神经网络模型。

Result: 成功求解了非线性演化方程、KdV-Burgers方程和(2+1)维Boussinesq方程，获得了包含双曲函数、三角函数和有理函数的精确解析解，并通过三维图、等高线图和密度图展示了解的动态特性。

Conclusion: AENNM为求解非线性偏微分方程提供了新的方法论框架，在科学和工程领域具有广泛的应用前景，建立了微分方程理论与深度学习之间的新数学联系。

Abstract: In this study, we firstly propose an auxiliary equation neural networks
method (AENNM), an innovative analytical method that integrates neural networks
(NNs) models with the auxiliary equation method to obtain exact solutions of
nonlinear partial differential equations (NLPDEs). A key novelty of this method
is the introduction of a novel activation function derived from the solutions
of the Riccati equation, establishing a new mathematical link between
differential equations theory and deep learning. By combining the strong
approximation capability of NNs with the high precision of symbolic
computation, AENNM significantly enhances computational efficiency and
accuracy. To demonstrate the effectiveness of the AENNM in solving NLPDEs,
three numerical examples are investigated, including the nonlinear evolution
equation, the Korteweg-de Vries-Burgers equation, and the (2+1)-dimensional
Boussinesq equation. Furthermore, some new trial functions are constructed by
setting specific activation functions within the "2-2-2-1" and "3-2-2-1" NNs
models. By embedding the auxiliary equation method into the NNs framework, we
derive previously unreported solutions. The exact analytical solutions are
expressed in terms of hyperbolic functions, trigonometric functions, and
rational functions. Finally, three-dimensional plots, contour plots, and
density plots are presented to illustrate the dynamic characteristics of the
obtained solutions. This research provides a novel methodological framework for
addressing NLPDEs, with broad applicability across scientific and engineering
fields.

</details>


### [29] [Aligning Distributionally Robust Optimization with Practical Deep Learning Needs](https://arxiv.org/abs/2508.16734)
*Dmitrii Feoktistov,Igor Ignashin,Andrey Veprikov,Nikita Borovko,Alexander Bogdanov,Savelii Chezhegov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: ALSO是一种自适应损失缩放优化器，通过改进的分布鲁棒优化目标，能够为样本组分配权重，在深度学习任务中优于传统优化器和现有DRO方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法平等对待所有训练样本，而分布鲁棒优化(DRO)虽然能自适应分配样本重要性权重，但与当前深度学习实践存在显著差距，需要适应随机梯度和支持样本组权重分配的能力。

Method: 提出ALSO算法，针对改进的DRO目标，能够处理样本组的权重分配，并证明了在非凸目标（典型深度学习模型）上的收敛性。

Result: 在从表格深度学习到分割学习等多样化深度学习任务中的实证评估表明，ALSO优于传统优化器和现有DRO方法。

Conclusion: ALSO成功弥合了DRO与深度学习实践之间的差距，提供了自适应、支持样本组权重分配的高效优化算法。

Abstract: While traditional Deep Learning (DL) optimization methods treat all training
samples equally, Distributionally Robust Optimization (DRO) adaptively assigns
importance weights to different samples. However, a significant gap exists
between DRO and current DL practices. Modern DL optimizers require adaptivity
and the ability to handle stochastic gradients, as these methods demonstrate
superior performance. Additionally, for practical applications, a method should
allow weight assignment not only to individual samples, but also to groups of
objects (for example, all samples of the same class). This paper aims to bridge
this gap by introducing ALSO $\unicode{x2013}$ Adaptive Loss Scaling Optimizer
$\unicode{x2013}$ an adaptive algorithm for a modified DRO objective that can
handle weight assignment to sample groups. We prove the convergence of our
proposed algorithm for non-convex objectives, which is the typical case for DL
models. Empirical evaluation across diverse Deep Learning tasks, from Tabular
DL to Split Learning tasks, demonstrates that ALSO outperforms both traditional
optimizers and existing DRO methods.

</details>


### [30] [Deep Learning for Markov Chains: Lyapunov Functions, Poisson's Equation, and Stationary Distributions](https://arxiv.org/abs/2508.16737)
*Yanlin Qu,Jose Blanchet,Peter Glynn*

Main category: cs.LG

TL;DR: 使用深度学习自动构建Lyapunov函数，通过神经网络满足首次转移分析导出的积分方程，用于马尔可夫模型的稳定性分析、泊松方程求解和稳态分布估计。


<details>
  <summary>Details</summary>
Motivation: Lyapunov函数的传统构造需要大量创造性和分析工作，深度学习可以自动化这一过程，提高效率并扩展到非紧致状态空间。

Method: 训练神经网络满足从首次转移分析导出的积分方程，通过深度学习近似Lyapunov函数，适用于紧致和非紧致状态空间。

Result: 方法在排队论等多个示例中证明有效，能够成功构建Lyapunov函数并应用于稳定性分析、泊松方程求解和稳态分布估计。

Conclusion: 深度学习可以自动化Lyapunov函数的构造过程，为马尔可夫模型的稳定性分析提供了一种有效且通用的方法，特别是在传统方法难以处理的非紧致状态空间中表现出色。

Abstract: Lyapunov functions are fundamental to establishing the stability of Markovian
models, yet their construction typically demands substantial creativity and
analytical effort. In this paper, we show that deep learning can automate this
process by training neural networks to satisfy integral equations derived from
first-transition analysis. Beyond stability analysis, our approach can be
adapted to solve Poisson's equation and estimate stationary distributions.
While neural networks are inherently function approximators on compact domains,
it turns out that our approach remains effective when applied to Markov chains
on non-compact state spaces. We demonstrate the effectiveness of this
methodology through several examples from queueing theory and beyond.

</details>


### [31] [WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning](https://arxiv.org/abs/2508.16741)
*Haosen Ge,Shuo Li,Lianghuan Huang*

Main category: cs.LG

TL;DR: 弱到强迁移(WST)是一种自动提示工程框架，通过小型教师模型生成指令来增强大型学生模型的性能，无需强教师模型即可实现高效提示优化


<details>
  <summary>Details</summary>
Motivation: 传统提示工程具有挑战性，特别是在大型模型闭源或难以微调的情况下，需要一种高效且广泛适用的自动提示优化方法

Method: 使用强化学习，教师模型的指令基于学生模型的结果进行迭代改进，小型教师模型为大型学生模型生成优化指令

Result: 在推理(MATH-500, GSM8K)和对齐(HH-RLHF)基准测试中取得显著提升：MATH-500提升98%，HH-RLHF提升134%，超越GPT-4o-mini和Llama-70B等基线模型

Conclusion: 小模型能够可靠地支撑大模型，解锁潜在能力同时避免强教师可能引入的误导性提示，WST为高效安全的LLM提示优化提供了可扩展解决方案

Abstract: Effective prompt engineering remains a challenging task for many
applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt
engineering framework where a small "Teacher" model generates instructions that
enhance the performance of a much larger "Student" model. Unlike prior work,
WST requires only a weak teacher, making it efficient and broadly applicable in
settings where large models are closed-source or difficult to fine-tune. Using
reinforcement learning, the Teacher Model's instructions are iteratively
improved based on the Student Model's outcomes, yielding substantial gains
across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on
MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and
Llama-70B. These results demonstrate that small models can reliably scaffold
larger ones, unlocking latent capabilities while avoiding misleading prompts
that stronger teachers may introduce, establishing WST as a scalable solution
for efficient and safe LLM prompt refinement.

</details>


### [32] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 研究探索双曲网络是否能作为生物分类学中层次结构模型的更好嵌入空间，通过多模态对比学习和新颖的堆叠蕴含目标，在BIOSCAN-1M数据集上取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 生物多样性研究中的分类学需要将生物标本组织成基于多模态证据（如图像和遗传信息）的结构化层次体系，需要寻找更适合层次结构的嵌入空间。

Method: 使用对比学习和新颖的堆叠蕴含目标，将多模态输入嵌入到共享的双曲空间中。

Result: 在BIOSCAN-1M数据集上，双曲嵌入与欧几里得基线性能相当，在使用DNA条形码进行未见物种分类方面优于所有其他模型，但细粒度分类和开放世界泛化仍具挑战性。

Conclusion: 该框架为生物多样性建模提供了结构感知的基础，在物种发现、生态监测和保护工作方面具有潜在应用价值。

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [33] [Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling](https://arxiv.org/abs/2508.16745)
*Ivan Rodkin,Daniil Orel,Konstantin Smirnov,Arman Bolatov,Bilal Elbouardi,Besher Hassan,Yuri Kuratov,Aydar Bulatov,Preslav Nakov,Timothy Baldwin,Artem Shelmanov,Mikhail Burtsev*

Main category: cs.LG

TL;DR: 研究表明大多数神经网络架构能够学习抽象底层规则，但在多步推理任务中性能显著下降。增加模型深度、引入循环机制、记忆和测试时计算扩展能显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索不同架构和训练方法如何影响模型在多步推理任务中的表现，理解大语言模型如何学习和执行多步推理这一核心能力。

Method: 在细胞自动机框架中，使用随机布尔函数生成状态序列进行训练，排除记忆化影响，测试不同神经架构的多步推理能力。

Result: 模型在单步状态预测上达到高准确率，但在需要多步推理时性能急剧下降。增加模型深度对序列计算至关重要，循环机制、记忆和测试时计算扩展能显著提升推理性能。

Conclusion: 多步推理能力需要更深的模型架构和扩展的计算机制，单纯的单步预测训练不足以支持复杂的推理任务，需要专门的设计来增强模型的序列推理能力。

Abstract: Reasoning is a core capability of large language models, yet understanding
how they learn and perform multi-step reasoning remains an open problem. In
this study, we explore how different architectures and training methods affect
model multi-step reasoning capabilities within a cellular automata framework.
By training on state sequences generated with random Boolean functions for
random initial conditions to exclude memorization, we demonstrate that most
neural architectures learn to abstract the underlying rules. While models
achieve high accuracy in next-state prediction, their performance declines
sharply if multi-step reasoning is required. We confirm that increasing model
depth plays a crucial role for sequential computations. We demonstrate that an
extension of the effective model depth with recurrence, memory, and test-time
compute scaling substantially enhances reasoning capabilities.

</details>


### [34] [FAIRWELL: Fair Multimodal Self-Supervised Learning for Wellbeing Prediction](https://arxiv.org/abs/2508.16748)
*Jiaee Cheong,Abtin Mogharabin,Paul Liang,Hatice Gunes,Sinan Kalkan*

Main category: cs.LG

TL;DR: 提出了FAIRWELL损失函数，通过多模态自监督学习改善机器学习公平性，在三个医疗数据集上验证了其有效性


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习改善机器学习公平性的方法尚未在多模态环境下探索，而多模态数据包含独特的互补信息，可以用于学习更公平的表征

Method: 基于VICReg方法提出FAIRWELL损失函数，包含三个机制：方差项减少对保护属性的依赖、不变性项确保相似个体的预测一致性、协方差项最小化与保护属性的相关性

Result: 在三个医疗数据集（D-Vlog、MIMIC、MODMA）上验证，框架提高了整体公平性性能，分类性能下降最小，显著改善了性能-公平性帕累托前沿

Conclusion: FAIRWELL方法能够有效学习主体无关的表征，在多模态预测任务中强制实现公平性，为多模态环境下的公平机器学习提供了新思路

Abstract: Early efforts on leveraging self-supervised learning (SSL) to improve machine
learning (ML) fairness has proven promising. However, such an approach has yet
to be explored within a multimodal context. Prior work has shown that, within a
multimodal setting, different modalities contain modality-unique information
that can complement information of other modalities. Leveraging on this, we
propose a novel subject-level loss function to learn fairer representations via
the following three mechanisms, adapting the variance-invariance-covariance
regularization (VICReg) method: (i) the variance term, which reduces reliance
on the protected attribute as a trivial solution; (ii) the invariance term,
which ensures consistent predictions for similar individuals; and (iii) the
covariance term, which minimizes correlational dependence on the protected
attribute. Consequently, our loss function, coined as FAIRWELL, aims to obtain
subject-independent representations, enforcing fairness in multimodal
prediction tasks. We evaluate our method on three challenging real-world
heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain
different modalities of varying length and different prediction tasks. Our
findings indicate that our framework improves overall fairness performance with
minimal reduction in classification performance and significantly improves on
the performance-fairness Pareto frontier.

</details>


### [35] [DR-CircuitGNN: Training Acceleration of Heterogeneous Circuit Graph Neural Network on GPUs](https://arxiv.org/abs/2508.16769)
*Yuebo Luo,Shiyang Li,Junran Tao,Kiran Thorat,Xi Xie,Hongwu Peng,Nuo Xu,Caiwen Ding,Shaoyi Huang*

Main category: cs.LG

TL;DR: 通过利用行向稀疏感知的Dynamic-ReLU和SpMM内核优化，结合CPU-GPU并行处理策略，实现了异构图神经网络在EDA电路图数据集上的高效训练加速。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络(HGNNs)能更好地处理EDA电路图的拓扑和几何特征，但其串行消息传递机制导致计算复杂度和处理成本高，成为性能瓶颈。

Method: 设计DR-CircuitGNN，采用行向稀疏感知的Dynamic-ReLU和SpMM内核优化，并通过多线程CPU初始化和多cudaStreams实现CPU-GPU并行处理独立子图。

Result: 在三个代表性CircuitNet设计上，前向传播和反向传播分别达到了3.51x和4.09x的加速比；在全尺寸CircuitNet和Mini-CircuitNet上，并行设计实现了2.71x的速度提升，而相关分数和错误率受影响微小。

Conclusion: DR-CircuitGNN通过核心算法优化和并行处理策略，有效解决了HGNNs在EDA电路图上的计算性能瓶颈，为大规模复杂电路设计提供了高效的神经网络加速方案。

Abstract: The increasing scale and complexity of integrated circuit design have led to
increased challenges in Electronic Design Automation (EDA). Graph Neural
Networks (GNNs) have emerged as a promising approach to assist EDA design as
circuits can be naturally represented as graphs. While GNNs offer a foundation
for circuit analysis, they often fail to capture the full complexity of EDA
designs. Heterogeneous Graph Neural Networks (HGNNs) can better interpret EDA
circuit graphs as they capture both topological relationships and geometric
features. However, the improved representation capability comes at the cost of
even higher computational complexity and processing cost due to their serial
module-wise message-passing scheme, creating a significant performance
bottleneck. In this paper, we propose DR-CircuitGNN, a fast GPU kernel design
by leveraging row-wise sparsity-aware Dynamic-ReLU and optimizing SpMM kernels
during heterogeneous message-passing to accelerate HGNNs training on
EDA-related circuit graph datasets. To further enhance performance, we propose
a parallel optimization strategy that maximizes CPU-GPU concurrency by
concurrently processing independent subgraphs using multi-threaded CPU
initialization and GPU kernel execution via multiple cudaStreams. Our
experiments show that on three representative CircuitNet designs (small,
medium, large), the proposed method can achieve up to 3.51x and 4.09x speedup
compared to the SOTA for forward and backward propagation, respectively. On
full-size CircuitNet and sampled Mini-CircuitNet, our parallel design enables
up to 2.71x speed up over the official DGL implementation cuSPARSE with
negligible impact on correlation scores and error rates.

</details>


### [36] [Latent Graph Learning in Generative Models of Neural Signals](https://arxiv.org/abs/2508.16776)
*Nathan X. Kodama,Kenneth A. Loparo*

Main category: cs.LG

TL;DR: 该研究探索从神经信号生成模型中学习潜在图结构的方法，通过数值模拟验证提取的网络表示与真实连接性的对齐程度，发现共输入图表示具有强对齐性。


<details>
  <summary>Details</summary>
Motivation: 推断时间交互图和从神经信号中提取高阶结构是构建系统神经科学生成模型的关键问题，但目前在大规模神经数据基础模型中提取可解释的潜在图表示仍然具有挑战性。

Method: 通过测试已知真实连接性的神经回路数值模拟，评估多种解释学习模型权重的假设，比较提取的网络表示与底层有向图的对齐程度。

Result: 发现提取的网络表示与底层有向图存在适度对齐，而共输入图表示则表现出强对齐性。

Conclusion: 这些发现为在构建大规模神经数据基础模型时融入基于图的几何约束提供了路径和动机。

Abstract: Inferring temporal interaction graphs and higher-order structure from neural
signals is a key problem in building generative models for systems
neuroscience. Foundation models for large-scale neural data represent shared
latent structures of neural signals. However, extracting interpretable latent
graph representations in foundation models remains challenging and unsolved.
Here we explore latent graph learning in generative models of neural signals.
By testing against numerical simulations of neural circuits with known
ground-truth connectivity, we evaluate several hypotheses for explaining
learned model weights. We discover modest alignment between extracted network
representations and the underlying directed graphs and strong alignment in the
co-input graph representations. These findings motivate paths towards
incorporating graph-based geometric constraints in the construction of
large-scale foundation models for neural data.

</details>


### [37] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: 量化对LLM内部表示的影响研究：通过多种可解释性技术分析4位和8位量化，发现量化对模型校准影响较小，死亡神经元数量保持稳定，神经元贡献模式因模型而异，但总体支持量化作为可靠的模型压缩技术


<details>
  <summary>Details</summary>
Motivation: 量化虽然在资源受限环境中部署LLM提供了实用解决方案，但其对内部表示的影响尚未充分研究，引发了对量化模型可靠性的质疑

Method: 采用多种可解释性技术分析多个LLM在4位和8位量化下的模型和神经元行为，包括模型校准分析、神经元激活分析和神经元贡献分析

Result: 量化对模型校准影响较小；死亡神经元数量在不同量化条件下保持一致；小型全精度模型具有较少显著神经元，大型模型则更多（Llama-2-7B除外）；量化对神经元冗余的影响因模型而异

Conclusion: 量化的影响可能因模型和任务而异，但未观察到任何剧烈变化，支持量化作为可靠的模型压缩技术的使用

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [38] [Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression](https://arxiv.org/abs/2508.16802)
*Baozhuo Su,Zhengxian Qu*

Main category: cs.LG

TL;DR: Anchor-MoE是一个处理概率回归和点回归的统一模型，通过锚点预测、混合密度网络专家和可学习核函数实现异方差校正，在理论和实验上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 回归问题在科学和工程中至关重要，但现有方法往往无法同时处理概率回归和点回归，需要一种统一的框架来处理不确定性。

Method: 使用梯度提升模型提供锚点均值，将锚点预测投影到潜在空间，通过可学习的度量窗口核函数评分局部性，软路由将样本分配到混合密度网络专家进行异方差校正和方差预测。

Result: 理论证明达到最小最大最优风险率，实验在UCI回归数据集上匹配或超越NGBoost基线，多个数据集达到新的最先进概率回归结果。

Conclusion: Anchor-MoE提供了一个统一且理论保证的概率回归框架，在保持点回归精度的同时提供可靠的不确定性估计，代码已开源。

Abstract: Regression under uncertainty is fundamental across science and engineering.
We present an Anchored Mixture of Experts (Anchor-MoE), a model that handles
both probabilistic and point regression. For simplicity, we use a tuned
gradient-boosting model to furnish the anchor mean; however, any off-the-shelf
point regressor can serve as the anchor. The anchor prediction is projected
into a latent space, where a learnable metric-window kernel scores locality and
a soft router dispatches each sample to a small set of mixture-density-network
experts; the experts produce a heteroscedastic correction and predictive
variance. We train by minimizing negative log-likelihood, and on a disjoint
calibration split fit a post-hoc linear map on predicted means to improve point
accuracy. On the theory side, assuming a H\"older smooth regression function of
order~$\alpha$ and fixed Lipschitz partition-of-unity weights with bounded
overlap, we show that Anchor-MoE attains the minimax-optimal $L^2$ risk rate
$O\!\big(N^{-2\alpha/(2\alpha+d)}\big)$. In addition, the CRPS test
generalization gap scales as
$\widetilde{O}\!\Big(\sqrt{(\log(Mh)+P+K)/N}\Big)$; it is logarithmic in $Mh$
and scales as the square root in $P$ and $K$. Under bounded-overlap routing,
$K$ can be replaced by $k$, and any dependence on a latent dimension is
absorbed into $P$. Under uniformly bounded means and variances, an analogous
$\widetilde{O}\!\big(\sqrt{(\log(Mh)+P+K)/N}\big)$ scaling holds for the test
NLL up to constants. Empirically, across standard UCI regressions, Anchor-MoE
consistently matches or surpasses the strong NGBoost baseline in RMSE and NLL;
on several datasets it achieves new state-of-the-art probabilistic regression
results on our benchmark suite. Code is available at
https://github.com/BaozhuoSU/Probabilistic_Regression.

</details>


### [39] [Uncertainty Propagation Networks for Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.16815)
*Hadi Jahanshahi,Zheng H. Zhu*

Main category: cs.LG

TL;DR: UPN是一种新型神经微分方程，通过耦合的均值和协方差微分方程同时建模状态演化和不确定性量化，在连续时间建模中自然集成不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有神经ODE仅预测状态轨迹，缺乏不确定性量化能力。UPN旨在解决这一问题，为连续时间建模提供原则性的不确定性量化方法。

Method: 参数化耦合的均值和协方差微分方程，通过求解状态和协方差演化的耦合ODE来有效传播非线性动力学中的不确定性，支持状态相关、可学习的过程噪声。

Result: 实验证明UPN在多个领域有效：具有不确定性量化的连续归一化流、具有良好校准置信区间的时间序列预测，以及在稳定和混沌动力系统中的鲁棒轨迹预测。

Conclusion: UPN提供了一种连续深度建模框架，能够自适应输入复杂度，提供原则性不确定性量化，并自然处理不规则采样观测。

Abstract: This paper introduces Uncertainty Propagation Network (UPN), a novel family
of neural differential equations that naturally incorporate uncertainty
quantification into continuous-time modeling. Unlike existing neural ODEs that
predict only state trajectories, UPN simultaneously model both state evolution
and its associated uncertainty by parameterizing coupled differential equations
for mean and covariance dynamics. The architecture efficiently propagates
uncertainty through nonlinear dynamics without discretization artifacts by
solving coupled ODEs for state and covariance evolution while enabling
state-dependent, learnable process noise. The continuous-depth formulation
adapts its evaluation strategy to each input's complexity, provides principled
uncertainty quantification, and handles irregularly-sampled observations
naturally. Experimental results demonstrate UPN's effectiveness across multiple
domains: continuous normalizing flows (CNFs) with uncertainty quantification,
time-series forecasting with well-calibrated confidence intervals, and robust
trajectory prediction in both stable and chaotic dynamical systems.

</details>


### [40] [Understanding and Tackling Over-Dilution in Graph Neural Networks](https://arxiv.org/abs/2508.16829)
*Junhyun Lee,Veronika Thost,Bumsoo Kim,Jaewoo Kang,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文提出了MPNN中的过稀释问题，包括节点内属性级稀释和节点间表示级稀释，并提出了基于Transformer的解决方案来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: MPNN在图机器学习中占据重要地位，但存在过平滑和过挤压等问题。作者发现即使在单层中，节点特定信息也会显著稀释，这一现象之前被忽视。

Method: 提出了过稀释概念，用两个稀释因子（节点内属性级稀释和节点间节点级表示稀释）进行形式化描述，并引入了基于Transformer的解决方案。

Result: 提出的方法能够有效缓解过稀释问题，补充现有的节点嵌入方法如MPNN，为构建更具信息性的图表示提供了新见解。

Conclusion: 过稀释是MPNN的一个重要限制因素，基于Transformer的解决方案可以有效解决这个问题，推动信息性图表示的发展。

Abstract: Message Passing Neural Networks (MPNNs) hold a key position in machine
learning on graphs, but they struggle with unintended behaviors, such as
over-smoothing and over-squashing, due to irregular data structures. The
observation and formulation of these limitations have become foundational in
constructing more informative graph representations. In this paper, we delve
into the limitations of MPNNs, focusing on aspects that have previously been
overlooked. Our observations reveal that even within a single layer, the
information specific to an individual node can become significantly diluted. To
delve into this phenomenon in depth, we present the concept of Over-dilution
and formulate it with two dilution factors: intra-node dilution for
attribute-level and inter-node dilution for node-level representations. We also
introduce a transformer-based solution that alleviates over-dilution and
complements existing node embedding methods like MPNNs. Our findings provide
new insights and contribute to the development of informative representations.
The implementation and supplementary materials are publicly available at
https://github.com/LeeJunHyun/NATR.

</details>


### [41] [Out of Distribution Detection for Efficient Continual Learning in Quality Prediction for Arc Welding](https://arxiv.org/abs/2508.16832)
*Yannik Hahn,Jan Voets,Antonin Koenigsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 基于VQ-VAE Transformer架构，利用自回归损失进行OOD检测，结合持续学习策略，在动态焊接环境中实现稳健的质量预测


<details>
  <summary>Details</summary>
Motivation: 解决当前机器学习模型在动态制造环境中面对分布偏移时的局限性，特别是在焊接质量预测领域

Method: 扩展VQ-VAE Transformer架构，利用其自回归损失作为OOD检测机制，结合持续学习策略进行模型优化

Result: 相比传统重建方法和嵌入误差技术，表现出更优越的性能，在真实焊接场景中有效维持质量预测能力

Conclusion: 为动态制造过程提供了可解释且自适应的质量保证解决方案，是工业环境中稳健实用AI系统的重要进展

Abstract: Modern manufacturing relies heavily on fusion welding processes, including
gas metal arc welding (GMAW). Despite significant advances in machine
learning-based quality prediction, current models exhibit critical limitations
when confronted with the inherent distribution shifts that occur in dynamic
manufacturing environments. In this work, we extend the VQ-VAE Transformer
architecture - previously demonstrating state-of-the-art performance in weld
quality prediction - by leveraging its autoregressive loss as a reliable
out-of-distribution (OOD) detection mechanism. Our approach exhibits superior
performance compared to conventional reconstruction methods, embedding
error-based techniques, and other established baselines. By integrating OOD
detection with continual learning strategies, we optimize model adaptation,
triggering updates only when necessary and thereby minimizing costly labeling
requirements. We introduce a novel quantitative metric that simultaneously
evaluates OOD detection capability while interpreting in-distribution
performance. Experimental validation in real-world welding scenarios
demonstrates that our framework effectively maintains robust quality prediction
capabilities across significant distribution shifts, addressing critical
challenges in dynamic manufacturing environments where process parameters
frequently change. This research makes a substantial contribution to applied
artificial intelligence by providing an explainable and at the same time
adaptive solution for quality assurance in dynamic manufacturing processes - a
crucial step towards robust, practical AI systems in the industrial
environment.

</details>


### [42] [Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience](https://arxiv.org/abs/2508.16836)
*Bicheng Wang,Junping Wang,Yibo Xue*

Main category: cs.LG

TL;DR: 提出了一种物理信息神经符号方法来描述复杂网络的演化动力学，用于预测产业链弹性，通过联合学习物理符号动力学和时空协同演化拓扑来实现更准确的预测。


<details>
  <summary>Details</summary>
Motivation: 产业链在国民经济可持续发展中日益重要，但数据驱动的深度学习在描述和分析复杂网络弹性方面仍处于起步阶段，核心问题是缺乏描述系统动力学的理论框架。

Method: 学习物理实体活动状态的动力学，并将其整合到多层时空协同演化网络中，使用物理信息方法实现物理符号动力学和时空协同演化拓扑的联合学习。

Result: 实验结果表明该模型能够获得更好的结果，更准确有效地预测产业链弹性，对行业发展具有一定的实际意义。

Conclusion: 提出的物理信息神经符号方法为复杂网络弹性预测提供了有效的理论框架和实践解决方案，在产业链分析中具有重要应用价值。

Abstract: Industrial chain plays an increasingly important role in the sustainable
development of national economy. However, as a typical complex network,
data-driven deep learning is still in its infancy in describing and analyzing
the resilience of complex networks, and its core is the lack of a theoretical
framework to describe the system dynamics. In this paper, we propose a
physically informative neural symbolic approach to describe the evolutionary
dynamics of complex networks for resilient prediction. The core idea is to
learn the dynamics of the activity state of physical entities and integrate it
into the multi-layer spatiotemporal co-evolution network, and use the physical
information method to realize the joint learning of physical symbol dynamics
and spatiotemporal co-evolution topology, so as to predict the industrial chain
resilience. The experimental results show that the model can obtain better
results and predict the elasticity of the industry chain more accurately and
effectively, which has certain practical significance for the development of
the industry.

</details>


### [43] [Neural Contrast Expansion for Explainable Structure-Property Prediction and Random Microstructure Design](https://arxiv.org/abs/2508.16857)
*Guangyu Nie,Yang Jiao,Yi Ren*

Main category: cs.LG

TL;DR: 提出Neural Contrast Expansion (NCE)方法，结合强对比展开理论和神经网络，为双相复合材料建立既高效又可解释的结构-性能预测模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法存在效率与可解释性的矛盾：PDE求解成本高但可提供敏感度信息，数据驱动模型效率高但可解释性差。需要一种既能保持计算效率又能提供可解释敏感度信息的方法。

Method: 基于强对比展开(SCE)理论框架，提出神经对比展开(NCE)架构，从结构-性能数据中学习替代PDE核函数。该方法仅需宏观性能测量数据，无需PDE解场信息。

Result: 在静态传导和电磁波传播案例中，NCE模型能够提供准确且具有洞察力的敏感度信息，对材料设计具有实用价值。

Conclusion: NCE方法成功解决了复合材料性能预测中效率与可解释性的平衡问题，为材料开发提供了既经济又具有物理洞察力的建模工具。

Abstract: Effective properties of composite materials are defined as the ensemble
average of property-specific PDE solutions over the underlying microstructure
distributions. Traditionally, predicting such properties can be done by solving
PDEs derived from microstructure samples or building data-driven models that
directly map microstructure samples to properties. The former has a higher
running cost, but provides explainable sensitivity information that may guide
material design; the latter could be more cost-effective if the data overhead
is amortized, but its learned sensitivities are often less explainable. With a
focus on properties governed by linear self-adjoint PDEs (e.g., Laplace,
Helmholtz, and Maxwell curl-curl) defined on bi-phase microstructures, we
propose a structure-property model that is both cost-effective and explainable.
Our method is built on top of the strong contrast expansion (SCE) formalism,
which analytically maps $N$-point correlations of an unbounded random field to
its effective properties. Since real-world material samples have finite sizes
and analytical PDE kernels are not always available, we propose Neural Contrast
Expansion (NCE), an SCE-inspired architecture to learn surrogate PDE kernels
from structure-property data. For static conduction and electromagnetic wave
propagation cases, we show that NCE models reveal accurate and insightful
sensitivity information useful for material design. Compared with other PDE
kernel learning methods, our method does not require measurements about the PDE
solution fields, but rather only requires macroscopic property measurements
that are more accessible in material development contexts.

</details>


### [44] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种无监督的图匹配框架，通过伪坐标、自适应相似度平衡和瓦片后处理，实现了大规模地图数据的高精度匹配。


<details>
  <summary>Details</summary>
Motivation: 解决地图到地图匹配中缺乏真实对应关系、节点特征稀疏和可扩展性需求等挑战，特别是在大规模地图数据中难以获得标注训练样本的问题。

Method: 采用无监督学习方法，引入伪坐标捕捉节点相对空间布局，设计自适应特征与几何相似度平衡机制和几何一致性损失函数，并使用基于瓦片的后处理管道实现并行处理。

Result: 在真实数据集上实现了最先进的匹配精度，大幅超越现有方法，特别是在高噪声和大规模场景中表现优异。

Conclusion: 该框架为地图对齐提供了可扩展且实用的解决方案，是传统方法的稳健高效替代方案。

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>


### [45] [Quantifying Out-of-Training Uncertainty of Neural-Network based Turbulence Closures](https://arxiv.org/abs/2508.16891)
*Cody Grogan,Som Dhulipala,Mauricio Tano,Izabela Gutowska,Som Dutta*

Main category: cs.LG

TL;DR: 本文比较了三种神经网络方法和高斯过程在湍流闭合模型中的不确定性量化性能，发现在训练区域内GP精度最高，但在训练区域外深度集成方法表现优异且计算成本更低


<details>
  <summary>Details</summary>
Motivation: 神经网络湍流闭合模型缺乏不确定性量化，特别是在训练数据之外的输入区域，这限制了其在CFD模拟中的广泛应用

Method: 使用已发布的代数湍流闭合模型，比较了深度集成(DE)、蒙特卡洛dropout(MCD)、随机变分推断(SVI)和高斯过程(GP)四种方法在不确定性量化方面的性能

Result: 在训练区域内GP精度最高(RMSE=2.14e-5)，DE次之(4.59e-4)；在训练区域外GP和DE性能相似，但DE在负对数似然方面表现最佳；计算成本方面GP显著高于神经网络方法

Conclusion: 深度集成方法在提供直观不确定性估计方面表现稳健，计算效率高，是湍流闭合不确定性量化的实用选择，而GP虽然精度最高但计算成本过高

Abstract: Neural-Network (NN) based turbulence closures have been developed for being
used as pre-trained surrogates for traditional turbulence closures, with the
aim to increase computational efficiency and prediction accuracy of CFD
simulations. The bottleneck to the widespread adaptation of these ML-based
closures is the relative lack of uncertainty quantification (UQ) for these
models. Especially, quantifying uncertainties associated with out-of-training
inputs, that is when the ML-based turbulence closures are queried on inputs
outside their training data regime. In the current paper, a published algebraic
turbulence closure1 has been utilized to compare the quality of epistemic UQ
between three NN-based methods and Gaussian Process (GP). The three NN-based
methods explored are Deep Ensembles (DE), Monte-Carlo Dropout (MCD), and
Stochastic Variational Inference (SVI). In the in-training results, we find the
exact GP performs the best in accuracy with a Root Mean Squared Error (RMSE) of
$2.14 \cdot 10^{-5}$ followed by the DE with an RMSE of $4.59 \cdot 10^{-4}$.
Next, the paper discusses the performance of the four methods for quantifying
out-of-training uncertainties. For performance, the Exact GP yet again is the
best in performance, but has similar performance to the DE in the
out-of-training regions. In UQ accuracy for the out-of-training case, SVI and
DE hold the best miscalibration error for one of the cases. However, the DE
performs the best in Negative Log-Likelihood for both out-of-training cases. We
observe that for the current problem, in terms of accuracy GP > DE > SV I >
MCD. The DE results are relatively robust and provide intuitive UQ estimates,
despite performing naive ensembling. In terms of computational cost, the GP is
significantly higher than the NN-based methods with a $O(n^3)$ computational
complexity for each training step

</details>


### [46] [Tri-Accel: Curvature-Aware Precision-Adaptive and Memory-Elastic Optimization for Efficient GPU Usage](https://arxiv.org/abs/2508.16905)
*Mohsen Sheibanian,Pouya Shaeri,Alimohammad Beigi,Ryan T. Woo,Aryan Keluskar*

Main category: cs.LG

TL;DR: Tri-Accel是一个统一的优化框架，通过协同调整三种加速策略（精度自适应更新、稀疏二阶信号、内存弹性批处理缩放）来减少深度神经网络训练的GPU内存和计算时间成本。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在优化过程中面临GPU内存和计算时间的瓶颈，现有的加速技术通常单独使用，缺乏统一的协同优化框架。

Method: 提出Tri-Accel框架，包含：(1)基于曲率和梯度方差动态分配混合精度级别的精度自适应更新；(2)利用Hessian/Fisher稀疏模式指导精度和步长决策的稀疏二阶信号；(3)根据VRAM可用性实时调整批处理大小的内存弹性批处理缩放。

Result: 在CIFAR-10上使用ResNet-18和EfficientNet-B0，训练时间减少9.9%，内存使用降低13.3%，准确率比FP32基线提高1.1个百分点。内存占用从0.35GB降至0.31GB，保持78.1%准确率。

Conclusion: 该框架展示了算法自适应性和硬件感知相结合可以改善资源受限环境下的可扩展性，为边缘设备和成本敏感的云部署提供更高效的神经网络训练方案。

Abstract: Deep neural networks are increasingly bottlenecked by the cost of
optimization, both in terms of GPU memory and compute time. Existing
acceleration techniques, such as mixed precision, second-order methods, and
batch size scaling, are typically used in isolation. We present Tri-Accel, a
unified optimization framework that co-adapts three acceleration strategies
along with adaptive parameters during training: (1) Precision-Adaptive Updates
that dynamically assign mixed-precision levels to layers based on curvature and
gradient variance; (2) Sparse Second-Order Signals that exploit Hessian/Fisher
sparsity patterns to guide precision and step size decisions; and (3)
Memory-Elastic Batch Scaling that adjusts batch size in real time according to
VRAM availability. On CIFAR-10 with ResNet-18 and EfficientNet-B0, Tri-Accel
achieves up to 9.9% reduction in training time and 13.3% lower memory usage,
while improving accuracy by +1.1 percentage points over FP32 baselines. Tested
on CIFAR-10/100, our approach demonstrates adaptive learning behavior, with
efficiency gradually improving over the course of training as the system learns
to allocate resources more effectively. Compared to static mixed-precision
training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint
from 0.35GB to 0.31GB on standard hardware. The framework is implemented with
custom Triton kernels, whose hardware-aware adaptation enables automatic
optimization without manual hyperparameter tuning, making it practical for
deployment across diverse computational environments. This work demonstrates
how algorithmic adaptivity and hardware awareness can be combined to improve
scalability in resource-constrained settings, paving the way for more efficient
neural network training on edge devices and cost-sensitive cloud deployments.

</details>


### [47] [Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection](https://arxiv.org/abs/2508.16915)
*Sadman Mohammad Nasif,Md Abrar Jahin,M. F. Mridha*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲神经网络和强化学习超启发式优化的新型欺诈检测框架，在保持高准确率的同时实现了公平性和可解释性，在银行欺诈数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 家庭银行系统的普及增加了网络欺诈风险，需要既准确又公平且可解释的欺诈检测机制。现有AI模型存在计算效率低、脉冲神经网络可解释性差、以及强化学习超参数优化复杂不稳定等问题。

Method: 提出了一个集成皮层脉冲神经网络与群体编码(CSNPC)和强化引导超启发式优化器(RHOSS)的新框架。CSNPC使用群体编码进行分类，RHOSS使用Q学习在公平性和召回率约束下动态选择超参数优化启发式方法，并整合了可解释AI技术。

Result: 在银行账户欺诈数据集上，模型在5%误报率下达到90.8%的召回率，优于最先进的脉冲和非脉冲模型，同时在关键人口属性上保持超过98%的预测公平性。可解释性模块证实了显著性归因与脉冲动态的一致性。

Conclusion: 该研究展示了结合群体编码脉冲神经网络和强化引导超启发式优化在现实金融应用中实现公平、透明和高性能欺诈检测的潜力。

Abstract: The growing adoption of home banking systems has heightened the risk of
cyberfraud, necessitating fraud detection mechanisms that are not only accurate
but also fair and explainable. While AI models have shown promise in this
domain, they face key limitations, including computational inefficiency, the
interpretability challenges of spiking neural networks (SNNs), and the
complexity and convergence instability of hyper-heuristic reinforcement
learning (RL)-based hyperparameter optimization. To address these issues, we
propose a novel framework that integrates a Cortical Spiking Network with
Population Coding (CSNPC) and a Reinforcement-Guided Hyper-Heuristic Optimizer
for Spiking Systems (RHOSS). The CSNPC, a biologically inspired SNN, employs
population coding for robust classification, while RHOSS uses Q-learning to
dynamically select low-level heuristics for hyperparameter optimization under
fairness and recall constraints. Embedded within the Modular Supervisory
Framework for Spiking Network Training and Interpretation (MoSSTI), the system
incorporates explainable AI (XAI) techniques, specifically, saliency-based
attribution and spike activity profiling, to increase transparency. Evaluated
on the Bank Account Fraud (BAF) dataset suite, our model achieves a $90.8\%$
recall at a strict $5\%$ false positive rate (FPR), outperforming
state-of-the-art spiking and non-spiking models while maintaining over $98\%$
predictive equality across key demographic attributes. The explainability
module further confirms that saliency attributions align with spiking dynamics,
validating interpretability. These results demonstrate the potential of
combining population-coded SNNs with reinforcement-guided hyper-heuristics for
fair, transparent, and high-performance fraud detection in real-world financial
applications.

</details>


### [48] [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
*Junxuan Wang,Xuyang Ge,Wentao Shu,Zhengfu He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 研究发现注意力输出存在于低维子空间，这是导致稀疏字典学习中死特征问题的根本原因，提出了子空间约束训练方法将死特征从87%降至1%以下


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型被认为在高维隐藏空间中运行，但研究发现注意力输出实际上被限制在低维子空间中，这种结构导致了稀疏字典学习中普遍存在的死特征问题

Method: 提出子空间约束训练方法，将特征方向初始化到激活的活跃子空间中，适用于注意力输出SAE和其他稀疏字典学习方法

Result: 在具有100万个特征的注意力输出SAE中，死特征从87%减少到1%以下，方法可扩展到其他稀疏字典学习方法

Conclusion: 研究提供了对注意力几何结构的新见解，并为改进大型语言模型中的稀疏字典学习提供了实用工具

Abstract: While transformer models are widely believed to operate in high-dimensional
hidden spaces, we show that attention outputs are confined to a surprisingly
low-dimensional subspace, where about 60\% of the directions account for 99\%
of the variance--a phenomenon that is induced by the attention output
projection matrix and consistently observed across diverse model families and
datasets. Critically, we find this low-rank structure as a fundamental cause of
the prevalent dead feature problem in sparse dictionary learning, where it
creates a mismatch between randomly initialized features and the intrinsic
geometry of the activation space. Building on this insight, we propose a
subspace-constrained training method for sparse autoencoders (SAEs),
initializing feature directions into the active subspace of activations. Our
approach reduces dead features from 87\% to below 1\% in Attention Output SAEs
with 1M features, and can further extend to other sparse dictionary learning
methods. Our findings provide both new insights into the geometry of attention
and practical tools for improving sparse dictionary learning in large language
models.

</details>


### [49] [Degree of Staleness-Aware Data Updating in Federated Learning](https://arxiv.org/abs/2508.16931)
*Tao Liu,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出了DUFL激励机制，通过三个控制参数协调数据陈旧性和数据量，使用DoS指标量化数据陈旧性，采用两阶段Stackelberg博弈优化联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据陈旧性问题，现有工作未同时考虑数据陈旧性和数据量对模型性能的影响

Method: 提出DUFL激励机制，使用DoS指标量化数据陈旧性，建立两阶段Stackelberg博弈模型，推导客户端最优数据更新策略和服务器近似最优策略

Result: 在真实数据集上的实验结果表明该方法具有显著性能优势

Conclusion: DUFL机制能有效协调数据陈旧性和数据量，提升联邦学习在时间敏感任务中的性能

Abstract: Handling data staleness remains a significant challenge in federated learning
with highly time-sensitive tasks, where data is generated continuously and data
staleness largely affects model performance. Although recent works attempt to
optimize data staleness by determining local data update frequency or client
selection strategy, none of them explore taking both data staleness and data
volume into consideration. In this paper, we propose DUFL(Data Updating in
Federated Learning), an incentive mechanism featuring an innovative local data
update scheme manipulated by three knobs: the server's payment, outdated data
conservation rate, and clients' fresh data collection volume, to coordinate
staleness and volume of local data for best utilities. To this end, we
introduce a novel metric called DoS(the Degree of Staleness) to quantify data
staleness and conduct a theoretic analysis illustrating the quantitative
relationship between DoS and model performance. We model DUFL as a two-stage
Stackelberg game with dynamic constraint, deriving the optimal local data
update strategy for each client in closed-form and the approximately optimal
strategy for the server. Experimental results on real-world datasets
demonstrate the significant performance of our approach.

</details>


### [50] [Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter](https://arxiv.org/abs/2508.16939)
*Lei Jiang,Wen Ge,Niels Cariou-Kotlarek,Mingxuan Yi,Po-Yu Chen,Lingyi Yang,Francois Buet-Golfouse,Gaurav Mittal,Hao Ni*

Main category: cs.LG

TL;DR: Sig-DEG是一种基于签名的扩散模型蒸馏方法，通过粗粒度时间分辨率近似反向扩散过程，将推理步骤减少一个数量级的同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中达到最先进效果，但推理时计算密集，通常需要数千个离散化步骤，需要高效的推理方法

Method: 利用偏签名高效总结布朗运动子区间，采用循环结构实现SDE解的全局精确近似，将蒸馏构建为监督学习任务，在粗时间网格上匹配细分辨率扩散模型输出

Result: Sig-DEG在减少推理步骤一个数量级的同时实现了有竞争力的生成质量

Conclusion: 基于签名的近似方法对高效生成建模具有有效性

Abstract: Diffusion models have achieved state-of-the-art results in generative
modelling but remain computationally intensive at inference time, often
requiring thousands of discretization steps. To this end, we propose Sig-DEG
(Signature-based Differential Equation Generator), a novel generator for
distilling pre-trained diffusion models, which can universally approximate the
backward diffusion process at a coarse temporal resolution. Inspired by
high-order approximations of stochastic differential equations (SDEs), Sig-DEG
leverages partial signatures to efficiently summarize Brownian motion over
sub-intervals and adopts a recurrent structure to enable accurate global
approximation of the SDE solution. Distillation is formulated as a supervised
learning task, where Sig-DEG is trained to match the outputs of a
fine-resolution diffusion model on a coarse time grid. During inference,
Sig-DEG enables fast generation, as the partial signature terms can be
simulated exactly without requiring fine-grained Brownian paths. Experiments
demonstrate that Sig-DEG achieves competitive generation quality while reducing
the number of inference steps by an order of magnitude. Our results highlight
the effectiveness of signature-based approximations for efficient generative
modeling.

</details>


### [51] [Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning](https://arxiv.org/abs/2508.16949)
*Yang Zhou,Sunzhu Li,Shunyu Liu,Wenkai Fang,Jiale Zhao,Jingwen Yang,Jianwei Lv,Kongcheng Zhang,Yihe Zhou,Hengtong Lu,Wei Chen,Yan Xie,Mingli Song*

Main category: cs.LG

TL;DR: RuscaRL通过引入检查表式评分标准作为外部指导，打破LLM推理中的探索瓶颈，在生成阶段提供多样化高质量样本，在训练阶段提供可验证奖励，显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM强化学习中存在的困境：RL改进需要高质量样本，但LLM的固有局限性限制了探索能力，形成无法探索就无法学习的恶性循环。

Method: 提出RuscaRL框架：1）在生成阶段使用评分标准作为显式脚手架指导多样化高质量响应生成；2）在训练阶段使用评分标准作为参考获得可靠的LLM-as-a-Judge奖励分数；3）逐步衰减外部指导以内部化推理模式。

Result: 在多个基准测试中表现优异，显著扩展推理边界：Qwen-2.5-7B-Instruct在HealthBench-500上从23.6提升至50.3，超越GPT-4.1；Qwen3-30B-A3B-Instruct达到61.1，超越包括OpenAI-o3在内的领先LLM。

Conclusion: RuscaRL有效解决了LLM推理中的探索瓶颈问题，通过评分标准脚手架实现了探索与利用的平衡，显著提升了通用推理任务的性能表现。

Abstract: Recent advances in Large Language Models (LLMs) have underscored the
potential of Reinforcement Learning (RL) to facilitate the emergence of
reasoning capabilities. Despite the encouraging results, a fundamental dilemma
persists as RL improvement relies on learning from high-quality samples, yet
the exploration for such samples remains bounded by the inherent limitations of
LLMs. This, in effect, creates an undesirable cycle in which what cannot be
explored cannot be learned. In this work, we propose Rubric-Scaffolded
Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework
designed to break the exploration bottleneck for general LLM reasoning.
Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit
scaffolding for exploration during rollout generation, where different rubrics
are provided as external guidance within task instructions to steer diverse
high-quality responses. This guidance is gradually decayed over time,
encouraging the model to internalize the underlying reasoning patterns; (2)
verifiable rewards for exploitation during model training, where we can obtain
robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL
on general reasoning tasks. Extensive experiments demonstrate the superiority
of the proposed RuscaRL across various benchmarks, effectively expanding
reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL
significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,
surpassing GPT-4.1. Furthermore, our fine-tuned variant on
Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading
LLMs including OpenAI-o3.

</details>


### [52] [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
*Manan Gupta,Dhruv Kumar*

Main category: cs.LG

TL;DR: 提出了Polysemanticity Index (PSI)指标，用于量化神经网络中多义神经元的语义聚类特征，发现深层网络比浅层网络具有更高的多义性。


<details>
  <summary>Details</summary>
Motivation: 神经网络中存在响应多个不相关特征的多义神经元，这给机制可解释性带来了挑战，需要一种量化方法来识别和研究这些多义单元。

Method: 开发了PSI指标，包含三个校准组件：几何聚类质量(S)、与标注类别的对齐度(Q)、通过CLIP的开放词汇语义区分度(D)。在预训练的ResNet-50上使用Tiny-ImageNet图像进行评估。

Result: PSI成功识别出激活集可分解为连贯、可命名原型的神经元，发现深层网络比浅层网络具有显著更高的PSI值。通过因果补丁交换干预验证了方法的有效性。

Conclusion: PSI提供了一个原则性和实用性的工具，用于发现、量化和研究神经网络中的多义单元，有助于提升神经网络的可解释性。

Abstract: Neural networks often contain polysemantic neurons that respond to multiple,
sometimes unrelated, features, complicating mechanistic interpretability. We
introduce the Polysemanticity Index (PSI), a null-calibrated metric that
quantifies when a neuron's top activations decompose into semantically distinct
clusters. PSI multiplies three independently calibrated components: geometric
cluster quality (S), alignment to labeled categories (Q), and open-vocabulary
semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with
Tiny-ImageNet images, PSI identifies neurons whose activation sets split into
coherent, nameable prototypes, and reveals strong depth trends: later layers
exhibit substantially higher PSI than earlier layers. We validate our approach
with robustness checks (varying hyperparameters, random seeds, and
cross-encoder text heads), breadth analyses (comparing class-only vs.
open-vocabulary concepts), and causal patch-swap interventions. In particular,
aligned patch replacements increase target-neuron activation significantly more
than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI
thus offers a principled and practical lever for discovering, quantifying, and
studying polysemantic units in neural networks.

</details>


### [53] [Unveiling the Latent Directions of Reflection in Large Language Models](https://arxiv.org/abs/2508.16989)
*Fu-Chieh Chang,Yu-Ting Lee,Pei-Yuan Wu*

Main category: cs.LG

TL;DR: 本文通过激活导向技术研究大语言模型的反思机制，发现可以通过激活干预直接增强或抑制反思行为，且抑制反思比激发更容易。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注设计反思提示策略或强化学习目标，但对反思的内在机制探索不足，需要从模型激活的潜在方向来深入理解反思的工作原理。

Method: 提出基于激活导向的方法论，构建不同反思级别（无反思、内在反思、触发反思）之间的导向向量，通过激活干预来系统识别反思诱导指令并控制反思行为。

Result: 在GSM8k-adv数据集上的实验显示，不同反思级别存在明显分层，激活干预证实了反思的可控性，抑制反思比激发反思更容易实现。

Conclusion: 这项工作为理解LLMs中反思推理的机制开辟了道路，既提供了反思增强防御等机会，也揭示了越狱攻击中对抗性抑制反思的风险。

Abstract: Reflection, the ability of large language models (LLMs) to evaluate and
revise their own reasoning, has been widely used to improve performance on
complex reasoning tasks. Yet, most prior work emphasizes designing reflective
prompting strategies or reinforcement learning objectives, leaving the inner
mechanisms of reflection underexplored. In this paper, we investigate
reflection through the lens of latent directions in model activations. We
propose a methodology based on activation steering to characterize how
instructions with different reflective intentions: no reflection, intrinsic
reflection, and triggered reflection. By constructing steering vectors between
these reflection levels, we demonstrate that (1) new reflection-inducing
instructions can be systematically identified, (2) reflective behavior can be
directly enhanced or suppressed through activation interventions, and (3)
suppressing reflection is considerably easier than stimulating it. Experiments
on GSM8k-adv with Qwen2.5-3B and Gemma3-4B reveal clear stratification across
reflection levels, and steering interventions confirm the controllability of
reflection. Our findings highlight both opportunities (e.g.,
reflection-enhancing defenses) and risks (e.g., adversarial inhibition of
reflection in jailbreak attacks). This work opens a path toward mechanistic
understanding of reflective reasoning in LLMs.

</details>


### [54] [Online Learning for Approximately-Convex Functions with Long-term Adversarial Constraints](https://arxiv.org/abs/2508.16992)
*Dhruv Sarkar,Samrat Mukhopadhyay,Abhishek Sinha*

Main category: cs.LG

TL;DR: 本文研究具有长期预算约束的对抗性在线学习问题，提出了一种高效的一阶在线算法，在完全信息和赌博机反馈设置下都能实现O(√T) α-遗憾，同时资源消耗最多为O(B_T log T) + ˜O(√T)。


<details>
  <summary>Details</summary>
Motivation: 研究具有长期预算约束的在线学习问题，特别是在对抗性设置下，处理α-近似凸函数这一广泛函数类，该类别化了凸性并包含许多常见的非凸优化问题。

Method: 提出一种高效的一阶在线算法，处理α-近似凸函数，在完全信息和赌博机反馈设置下工作，通过优化累积成本同时近似满足长期预算约束。

Result: 算法保证O(√T) α-遗憾对抗最优固定可行基准，同时消耗最多O(B_T log T) + ˜O(√T)资源。在赌博机反馈设置中为"对抗性赌博机与背包"问题提供改进保证的解决方案。

Conclusion: 证明了匹配的下界，展示了结果的紧性，并刻画了α-近似凸函数类，表明结果适用于广泛的问题家族。

Abstract: We study an online learning problem with long-term budget constraints in the
adversarial setting. In this problem, at each round $t$, the learner selects an
action from a convex decision set, after which the adversary reveals a cost
function $f_t$ and a resource consumption function $g_t$. The cost and
consumption functions are assumed to be $\alpha$-approximately convex - a broad
class that generalizes convexity and encompasses many common non-convex
optimization problems, including DR-submodular maximization, Online Vertex
Cover, and Regularized Phase Retrieval. The goal is to design an online
algorithm that minimizes cumulative cost over a horizon of length $T$ while
approximately satisfying a long-term budget constraint of $B_T$. We propose an
efficient first-order online algorithm that guarantees $O(\sqrt{T})$
$\alpha$-regret against the optimal fixed feasible benchmark while consuming at
most $O(B_T \log T)+ \tilde{O}(\sqrt{T})$ resources in both full-information
and bandit feedback settings. In the bandit feedback setting, our approach
yields an efficient solution for the $\texttt{Adversarial Bandits with
Knapsacks}$ problem with improved guarantees. We also prove matching lower
bounds, demonstrating the tightness of our results. Finally, we characterize
the class of $\alpha$-approximately convex functions and show that our results
apply to a broad family of problems.

</details>


### [55] [Learned Structure in CARTRIDGES: Keys as Shareable Routers in Self-Studied Representations](https://arxiv.org/abs/2508.17032)
*Maurizio Diaz*

Main category: cs.LG

TL;DR: CARTRIDGE方法通过离线训练大幅压缩KV缓存（最多减少40倍内存使用），本文首次机制性探索其结构，发现keys作为稳定检索路由器，values承担主要压缩，并提出SCI初始化改进加速收敛。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理的瓶颈在于KV缓存线性增长，需要研究CARTRIDGE压缩方法的内部工作机制以优化性能。

Method: 通过实证分析CARTRIDGE结构，提出keys作为检索路由器、values负责压缩的理论，并设计Sampled Chunk Initialization(SCI)改进初始化方法。

Result: 实验证明CARTRIDGE keys在不同任务间可共享且性能损失小，SCI能加速训练收敛，为后续优化奠定基础。

Conclusion: CARTRIDGE keys作为稳定路由器、values负责压缩的机制得到验证，SCI初始化改进有助于提升训练效率，为大规模应用提供理论支持。

Abstract: A bottleneck for long-context LLM inference is the linearly growing KV cache.
Recent work has proposed CARTRIDGES, an approach which leverages offline
compute to train a much smaller KV cache than is typically required for a full
document (up to 40x less memory usage at inference time). In this paper, we
present the first mechanistic exploration of the learned CARTRIDGE key-value
cache structure. In particular, we propose that (1) CARTRIDGE keys act as
stable, shareable retrieval routers for the compressed corpora and (2) most of
the learned compression occurs within the CARTRIDGE value vectors. We present
empirical evidence of our routing theory across tasks, model families, and
model sizes; for example, we can ablate the learned CARTRIDGE key vectors
between tasks with little performance loss. Finally, we propose a slight
improvement in initialization called Sampled Chunk Initialization (SCI). We
suggest that SCI can lead to faster CARTRIDGE convergence than previously
demonstrated in the literature. Our findings lay the groundwork for broader
empirical study of CARTRIDGE training optimization which may be crucial for
further scaling.

</details>


### [56] [TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression](https://arxiv.org/abs/2508.17056)
*Kiran Madhusudhanan,Vijaya Krishna Yalavarthi,Jonas Sonntag,Maximilian Stubbemann,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: TabResFlow是一个用于表格回归的概率模型，通过归一化样条流处理复杂的目标分布，在似然分数和推理速度上优于现有方法，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法主要关注点估计，容易产生过度自信的预测，这在需要可信决策的工业自动化中尤为关键。现有概率方法通常假设固定形状分布（如高斯分布），但现实世界中的目标分布往往非常复杂。

Method: TabResFlow包含三个关键组件：(1)每个数值特征的MLP编码器；(2)用于表达性特征提取的全连接ResNet主干网络；(3)基于条件样条的归一化流，用于灵活且可处理的密度估计。

Result: 在9个公共基准数据集上，TabResFlow在似然分数上持续超越现有概率回归模型，相比最强的概率回归模型(TreeFlow)提升9.64%，相比最强的深度学习替代方案(NodeFlow)推理速度平均提升5.6倍。在实际二手车价格预测任务中，TabResFlow在新提出的AURC指标上表现出色。

Conclusion: TabResFlow为表格回归提供了一个灵活且高效的概率建模框架，能够处理复杂的真实世界分布，在性能和实用性方面都优于现有方法。

Abstract: Tabular regression is a well-studied problem with numerous industrial
applications, yet most existing approaches focus on point estimation, often
leading to overconfident predictions. This issue is particularly critical in
industrial automation, where trustworthy decision-making is essential.
Probabilistic regression models address this challenge by modeling prediction
uncertainty. However, many conventional methods assume a fixed-shape
distribution (typically Gaussian), and resort to estimating distribution
parameters. This assumption is often restrictive, as real-world target
distributions can be highly complex. To overcome this limitation, we introduce
TabResFlow, a Normalizing Spline Flow model designed specifically for
univariate tabular regression, where commonly used simple flow networks like
RealNVP and Masked Autoregressive Flow (MAF) are unsuitable. TabResFlow
consists of three key components: (1) An MLP encoder for each numerical
feature. (2) A fully connected ResNet backbone for expressive feature
extraction. (3) A conditional spline-based normalizing flow for flexible and
tractable density estimation. We evaluate TabResFlow on nine public benchmark
datasets, demonstrating that it consistently surpasses existing probabilistic
regression models on likelihood scores. Our results demonstrate 9.64%
improvement compared to the strongest probabilistic regression model
(TreeFlow), and on average 5.6 times speed-up in inference time compared to the
strongest deep learning alternative (NodeFlow). Additionally, we validate the
practical applicability of TabResFlow in a real-world used car price prediction
task under selective regression. To measure performance in this setting, we
introduce a novel Area Under Risk Coverage (AURC) metric and show that
TabResFlow achieves superior results across this metric.

</details>


### [57] [Learning ON Large Datasets Using Bit-String Trees](https://arxiv.org/abs/2508.17083)
*Prashant Gupta*

Main category: cs.LG

TL;DR: 该论文开发了相似性保持哈希、分类和癌症基因组学的计算方法，包括高效的ComBI哈希方法、GRAF分类器以及CRCS深度学习框架，在十亿级数据集上实现显著性能提升和生物医学应用。


<details>
  <summary>Details</summary>
Motivation: 解决标准空间分区哈希方法中二叉树指数增长和稀疏性导致的效率问题，以及癌症基因组学中突变解释的挑战，需要开发高效、可扩展的计算工具。

Method: 提出ComBI压缩二叉树哈希方法实现快速近似最近邻搜索；开发GRAF集成分类器结合全局和局部分区；创建CRCS深度学习框架将遗传变化嵌入数值向量。

Result: 在十亿样本数据集上，ComBI达到0.90精度，速度提升4-296倍；GRAF在115个数据集上表现优异；CRCS成功识别体细胞突变、发现驱动基因并进行生存预测。

Conclusion: 这些方法为大规模数据分析和生物医学应用提供了高效、可扩展且可解释的工具，在哈希、分类和基因组学领域均有重要贡献。

Abstract: This thesis develops computational methods in similarity-preserving hashing,
classification, and cancer genomics. Standard space partitioning-based hashing
relies on Binary Search Trees (BSTs), but their exponential growth and sparsity
hinder efficiency. To overcome this, we introduce Compressed BST of Inverted
hash tables (ComBI), which enables fast approximate nearest-neighbor search
with reduced memory. On datasets of up to one billion samples, ComBI achieves
0.90 precision with 4X-296X speed-ups over Multi-Index Hashing, and also
outperforms Cellfishing.jl on single-cell RNA-seq searches with 2X-13X gains.
Building on hashing structures, we propose Guided Random Forest (GRAF), a
tree-based ensemble classifier that integrates global and local partitioning,
bridging decision trees and boosting while reducing generalization error.
Across 115 datasets, GRAF delivers competitive or superior accuracy, and its
unsupervised variant (uGRAF) supports guided hashing and importance sampling.
We show that GRAF and ComBI can be used to estimate per-sample classifiability,
which enables scalable prediction of cancer patient survival. To address
challenges in interpreting mutations, we introduce Continuous Representation of
Codon Switches (CRCS), a deep learning framework that embeds genetic changes
into numerical vectors. CRCS allows identification of somatic mutations without
matched normals, discovery of driver genes, and scoring of tumor mutations,
with survival prediction validated in bladder, liver, and brain cancers.
Together, these methods provide efficient, scalable, and interpretable tools
for large-scale data analysis and biomedical applications.

</details>


### [58] [Convolutional Neural Networks for Accurate Measurement of Train Speed](https://arxiv.org/abs/2508.17096)
*Haitao Tian,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.LG

TL;DR: 本研究探索使用卷积神经网络提高列车速度估计精度，比较了三种CNN架构与传统方法，发现多分支模型在复杂工况下表现最优


<details>
  <summary>Details</summary>
Motivation: 解决现代铁路系统中列车速度估计的复杂挑战，提高铁路安全性和运营效率

Method: 研究三种CNN架构（单分支2D、单分支1D、多分支模型），与自适应卡尔曼滤波进行比较，使用带/不带轮滑保护激活的模拟列车运行数据集

Result: 基于CNN的方法，特别是多分支模型，在准确性和鲁棒性方面优于传统方法，在具有挑战性的运行条件下表现尤为突出

Conclusion: 深度学习技术通过更有效地捕捉复杂运输数据集中的精细模式，具有提升铁路安全和运营效率的潜力

Abstract: In this study, we explore the use of Convolutional Neural Networks for
improving train speed estimation accuracy, addressing the complex challenges of
modern railway systems. We investigate three CNN architectures - single-branch
2D, single-branch 1D, and multiple-branch models - and compare them with the
Adaptive Kalman Filter. We analyse their performance using simulated train
operation datasets with and without Wheel Slide Protection activation. Our
results reveal that CNN-based approaches, especially the multiple-branch model,
demonstrate superior accuracy and robustness compared to traditional methods,
particularly under challenging operational conditions. These findings highlight
the potential of deep learning techniques to enhance railway safety and
operational efficiency by more effectively capturing intricate patterns in
complex transportation datasets.

</details>


### [59] [Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process](https://arxiv.org/abs/2508.17097)
*Lingkai Kong,Haotian Sun,Yuchen Zhuang,Haorui Wang,Wenhao Mu,Chao Zhang*

Main category: cs.LG

TL;DR: 基于图神经网络的不确定性量化和可解释性图分类模型，结合图功能性神经过程和图生成模型，通过隐藏理由映射到概率嵌入空间实现了优异的不确定性量化和解释能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络预测存在误检测和缺乏可解释性的问题，限制了其在关键应用中的采用。需要发展新的不确定性感知和可解释性图分类模型来解决这些挑战。

Method: 提出了一种新的不确定性感知和可解释性图分类模型，结合图功能性神经过程和图生成模型。核心思想是假设一组隐藏理由，将其映射到概率嵌入空间，分类器的预测分布通过学习随机相关矩阵来条件化理由嵌入。图生成器用于从嵌入空间解码理由的图结构。采用交替优化过程模仿EM算法进行高效模型训练。

Result: 在5个图分类数据集上的大量实验表明，该框架在不确定性量化和GNN可解释性方面都超过了最新的方法。案例研究显示，解码的理由结构能够提供有意义的解释。

Conclusion: 该方法具有普遍性，可以应用于任何现有的GNN架构，为图神经网络提供了有效的不确定性量化和可解释性解决方案。

Abstract: Graph neural networks (GNNs) are powerful tools on graph data. However, their
predictions are mis-calibrated and lack interpretability, limiting their
adoption in critical applications. To address this issue, we propose a new
uncertainty-aware and interpretable graph classification model that combines
graph functional neural process and graph generative model. The core of our
method is to assume a set of latent rationales which can be mapped to a
probabilistic embedding space; the predictive distribution of the classifier is
conditioned on such rationale embeddings by learning a stochastic correlation
matrix. The graph generator serves to decode the graph structure of the
rationales from the embedding space for model interpretability. For efficient
model training, we adopt an alternating optimization procedure which mimics the
well known Expectation-Maximization (EM) algorithm. The proposed method is
general and can be applied to any existing GNN architecture. Extensive
experiments on five graph classification datasets demonstrate that our
framework outperforms state-of-the-art methods in both uncertainty
quantification and GNN interpretability. We also conduct case studies to show
that the decoded rationale structure can provide meaningful explanations.

</details>


### [60] [Reconciling Communication Compression and Byzantine-Robustness in Distributed Learning](https://arxiv.org/abs/2508.17129)
*Diksha Gupta,Nirupam Gupta,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 提出RoSDHB算法，结合Polyak动量和协调压缩机制，在分布式学习中同时解决拜占庭容错和通信压缩问题，比现有方法依赖更少假设且性能相当。


<details>
  <summary>Details</summary>
Motivation: 分布式学习面临拜占庭故障和高通信成本的双重挑战，现有方法在结合通信压缩和拜占庭鲁棒聚合时会导致容错性下降，需要新的解决方案。

Method: 提出RoSDHB算法，整合经典Polyak动量和新型协调压缩机制，仅需诚实工作者平均损失函数的Lipschitz平滑性假设。

Result: 在标准(G,B)梯度差异异构模型下，RoSDHB与Byz-DASHA-PAGE性能相当，但依赖更少假设。基准图像分类实验显示其具有强鲁棒性和显著通信节省。

Conclusion: RoSDHB算法有效解决了分布式学习中拜占庭容错和通信压缩的协同问题，在减少假设依赖的同时保持了良好的性能表现。

Abstract: Distributed learning (DL) enables scalable model training over decentralized
data, but remains challenged by Byzantine faults and high communication costs.
While both issues have been studied extensively in isolation, their interaction
is less explored. Prior work shows that naively combining communication
compression with Byzantine-robust aggregation degrades resilience to faulty
nodes (or workers). The state-of-the-art algorithm, namely Byz-DASHA-PAGE [29],
makes use of the momentum variance reduction scheme to mitigate the detrimental
impact of compression noise on Byzantine-robustness. We propose a new
algorithm, named RoSDHB, that integrates the classic Polyak's momentum with a
new coordinated compression mechanism. We show that RoSDHB performs comparably
to Byz-DASHA-PAGE under the standard (G, B)-gradient dissimilarity
heterogeneity model, while it relies on fewer assumptions. In particular, we
only assume Lipschitz smoothness of the average loss function of the honest
workers, in contrast to [29]that additionally assumes a special smoothness of
bounded global Hessian variance. Empirical results on benchmark image
classification task show that RoSDHB achieves strong robustness with
significant communication savings.

</details>


### [61] [MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices](https://arxiv.org/abs/2508.17137)
*Nishant Gavhane,Arush Mehrotra,Rohit Chawla,Peter Proenca*

Main category: cs.LG

TL;DR: MoE-Beyond是一个基于学习的专家激活预测器，通过训练轻量级Transformer模型来预测MoE模型在自回归解码过程中的专家激活，显著提高了GPU缓存命中率


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型在边缘设备部署面临内存限制挑战，传统启发式专家缓存策略在模型参数扩展时难以维持高缓存命中率

Method: 将任务构建为多标签序列预测问题，使用从DeepSeek-V2-Chat-Lite MoE提取的6600万专家激活轨迹训练轻量级Transformer模型

Result: 在WebGLM-QA数据集上达到97.5%准确率和86.6% F1分数，当仅10%专家能放入GPU缓存时，将缓存命中率从17%提升至72%

Conclusion: MoE-Beyond通过学习预测专家激活，有效解决了MoE模型在资源受限环境中的内存管理问题，显著优于传统启发式方法

Abstract: The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices
presents significant challenges due to memory constraints. While MoE
architectures enable efficient utilization of computational resources by
activating only a subset of experts per inference, they require careful memory
management to operate efficiently in resource-constrained environments.
Traditional heuristic-based expert caching strategies such as MoE-Infinity
struggle to maintain high cache hit rates as models parameters scale. In this
work, we introduce MoE-Beyond, a learning-based expert activation predictor
trained to predict expert activations during autoregressive decoding. By
framing the task as a multi-label sequence prediction problem, we train a
lightweight transformer model on 66 million expert activation traces extracted
from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor
generalizes effectively across unseen prompts from WebGLM-QA dataset [6],
achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that
MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts
fit in GPU cache, outperforming heuristic baselines.

</details>


### [62] [Stochastic Gradient Descent with Strategic Querying](https://arxiv.org/abs/2508.17144)
*Nanfei Jiang,Hoi-To Wai,Mahnoosh Alizadeh*

Main category: cs.LG

TL;DR: 本文研究有限和优化问题中的战略梯度查询策略，相比均匀查询能提升随机梯度方法的性能。提出了理想化的OGQ算法和实用的SGQ算法，在满足PL条件的平滑目标函数下，OGQ能改善瞬态性能和降低稳态方差，SGQ在瞬态性能上优于SGD。


<details>
  <summary>Details</summary>
Motivation: 研究在有限和优化问题中，通过战略性地选择梯度查询（而非均匀随机查询）来提升随机梯度方法的性能，特别是在实际应用中如何实现这种战略查询。

Method: 首先提出理想化的Oracle Gradient Querying (OGQ)算法，假设可以访问所有用户的梯度来选择期望改进最大的梯度；然后提出实用的Strategic Gradient Querying (SGQ)算法，每轮只进行一次查询但在瞬态性能上优于SGD。

Result: 理论分析表明，在满足Polyak-Lojasiewicz条件的平滑目标函数下，OGQ能提升瞬态性能并降低稳态方差，SGQ在瞬态性能上优于标准SGD。数值实验验证了理论发现。

Conclusion: 战略梯度查询策略相比均匀查询能显著提升优化算法的性能，特别是SGQ算法在实际应用中具有很好的实用价值，能够在只进行一次查询的情况下获得更好的优化效果。

Abstract: This paper considers a finite-sum optimization problem under first-order
queries and investigates the benefits of strategic querying on stochastic
gradient-based methods compared to uniform querying strategy. We first
introduce Oracle Gradient Querying (OGQ), an idealized algorithm that selects
one user's gradient yielding the largest possible expected improvement (EI) at
each step. However, OGQ assumes oracle access to the gradients of all users to
make such a selection, which is impractical in real-world scenarios. To address
this limitation, we propose Strategic Gradient Querying (SGQ), a practical
algorithm that has better transient-state performance than SGD while making
only one query per iteration. For smooth objective functions satisfying the
Polyak-Lojasiewicz condition, we show that under the assumption of EI
heterogeneity, OGQ enhances transient-state performance and reduces
steady-state variance, while SGQ improves transient-state performance over SGD.
Our numerical experiments validate our theoretical findings.

</details>


### [63] [SACA: Selective Attention-Based Clustering Algorithm](https://arxiv.org/abs/2508.17150)
*Meysam Shirdel Bilehsavar,Razieh Ghaedi,Samira Seyed Taheri,Xinqi Fan,Christian O'Reilly*

Main category: cs.LG

TL;DR: 提出了一种基于选择性注意力的新型密度聚类方法，无需用户定义参数即可运行，简化了传统密度聚类算法的参数优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统密度聚类算法（如DBSCAN）依赖用户定义参数，需要领域专业知识进行优化，这在实际应用中存在挑战。

Method: 受选择性注意力启发，算法首先无需参数即可运行，如需调整则引入单个整数参数；通过计算阈值过滤稀疏点和异常值，形成初步聚类结构后重新整合排除点完成最终聚类。

Result: 在多样化数据集上的实验评估表明，该方法具有易用性和鲁棒性能。

Conclusion: 该方法为密度聚类任务提供了有效的替代方案，显著降低了参数调优的复杂性。

Abstract: Clustering algorithms are widely used in various applications, with
density-based methods such as Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) being particularly prominent. These algorithms identify
clusters in high-density regions while treating sparser areas as noise.
However, reliance on user-defined parameters often poses optimization
challenges that require domain expertise. This paper presents a novel
density-based clustering method inspired by the concept of selective attention,
which minimizes the need for user-defined parameters under standard conditions.
Initially, the algorithm operates without requiring user-defined parameters. If
parameter adjustment is needed, the method simplifies the process by
introducing a single integer parameter that is straightforward to tune. The
approach computes a threshold to filter out the most sparsely distributed
points and outliers, forms a preliminary cluster structure, and then
reintegrates the excluded points to finalize the results. Experimental
evaluations on diverse data sets highlight the accessibility and robust
performance of the method, providing an effective alternative for density-based
clustering tasks.

</details>


### [64] [Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks](https://arxiv.org/abs/2508.17158)
*Jack Youstra,Mohammed Mahfoud,Yang Yan,Henry Sleight,Ethan Perez,Mrinank Sharma*

Main category: cs.LG

TL;DR: 提出了CIFR基准测试来评估防御策略在对抗密码编码攻击时保持模型安全性的能力，并展示了探测监控器能达到99%以上的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调API虽然支持模型定制化，但也带来了严重的安全风险，攻击者可以通过在看似无害的微调数据中编码有害内容来绕过安全机制。

Method: 引入CIFR基准测试，包含多样化的密码编码和家族，评估防御策略；训练基于模型内部激活的探测监控器来检测恶意微调。

Result: 探测监控器实现了超过99%的检测准确率，能够泛化到未见过的密码变体和家族，优于现有监控方法。

Conclusion: CIFR基准和探测监控器为微调API安全防御提供了有效的评估工具和解决方案，有助于进一步研究这一关键领域。

Abstract: Large language model fine-tuning APIs enable widespread model customization,
yet pose significant safety risks. Recent work shows that adversaries can
exploit access to these APIs to bypass model safety mechanisms by encoding
harmful content in seemingly harmless fine-tuning data, evading both human
monitoring and standard content filters. We formalize the fine-tuning API
defense problem, and introduce the Cipher Fine-tuning Robustness benchmark
(CIFR), a benchmark for evaluating defense strategies' ability to retain model
safety in the face of cipher-enabled attackers while achieving the desired
level of fine-tuning functionality. We include diverse cipher encodings and
families, with some kept exclusively in the test set to evaluate for
generalization across unseen ciphers and cipher families. We then evaluate
different defenses on the benchmark and train probe monitors on model internal
activations from multiple fine-tunes. We show that probe monitors achieve over
99% detection accuracy, generalize to unseen cipher variants and families, and
compare favorably to state-of-the-art monitoring approaches. We open-source
CIFR and the code to reproduce our experiments to facilitate further research
in this critical area. Code and data are available online
https://github.com/JackYoustra/safe-finetuning-api

</details>


### [65] [ONG: Orthogonal Natural Gradient Descent](https://arxiv.org/abs/2508.17169)
*Yajat Yadav,Jathin Korrapati,Patrick Mendoza*

Main category: cs.LG

TL;DR: ONG算法将正交梯度下降与自然梯度相结合，通过EKFAC近似Fisher信息矩阵的逆来预处理梯度，在黎曼度量下实现最速下降，同时保持对先前任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的正交梯度下降方法忽略了神经网络参数化分布空间的信息几何结构，可能导致学习任务中的收敛不理想。

Method: 结合自然梯度思想，使用EKFAC高效近似Fisher信息矩阵的逆来预处理新任务梯度，然后将这些自然梯度投影到先前任务梯度的正交补空间上。

Result: 在Permuted和Rotated MNIST数据集上进行了基准测试，证明了算法的有效性。

Conclusion: ONG算法通过结合自然梯度和正交投影，在保持先前任务性能的同时，改善了新任务的学习效率，为持续学习提供了更优的解决方案。

Abstract: Orthogonal gradient descent has emerged as a powerful method for continual
learning tasks. However, its Euclidean projections overlook the underlying
information-geometric structure of the space of distributions parametrized by
neural networks, which can lead to suboptimal convergence in learning tasks. To
counteract this, we combine it with the idea of the natural gradient and
present ONG (Orthogonal Natural Gradient Descent). ONG preconditions each new
task gradient with an efficient EKFAC approximation of the inverse Fisher
information matrix, yielding updates that follow the steepest descent direction
under a Riemannian metric. To preserve performance on previously learned tasks,
ONG projects these natural gradients onto the orthogonal complement of prior
task gradients. We provide a theoretical justification for this procedure,
introduce the ONG algorithm, and benchmark its performance on the Permuted and
Rotated MNIST datasets. All code for our experiments/reproducibility can be
found at https://github.com/yajatyadav/orthogonal-natural-gradient.

</details>


### [66] [Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection](https://arxiv.org/abs/2508.17174)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.LG

TL;DR: 提出SaGD框架，通过平滑对抗训练的尖锐损失景观来改进OOD检测，在对抗攻击下能准确区分对抗性ID样本和真正的OOD样本。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将对抗性ID样本误判为OOD样本，且缺乏在对抗攻击下同时处理ID和OOD数据的鲁棒检测方法。

Method: 提出Sharpness-aware Geometric Defense (SaGD)框架，使用Jitter-based扰动在对抗训练中平滑损失景观，提升几何嵌入收敛质量。

Result: 在CIFAR-100与六个OOD数据集的各种攻击下，SaGD显著改善了FPR和AUC指标，优于现有最先进防御方法。

Conclusion: SaGD框架通过处理尖锐损失景观问题，有效提升了对抗环境下OOD检测的准确性，揭示了损失景观锐度与对抗性OOD检测之间的关系。

Abstract: Out-of-distribution (OOD) detection ensures safe and reliable model
deployment. Contemporary OOD algorithms using geometry projection can detect
OOD or adversarial samples from clean in-distribution (ID) samples. However,
this setting regards adversarial ID samples as OOD, leading to incorrect OOD
predictions. Existing efforts on OOD detection with ID and OOD data under
attacks are minimal. In this paper, we develop a robust OOD detection method
that distinguishes adversarial ID samples from OOD ones. The sharp loss
landscape created by adversarial training hinders model convergence, impacting
the latent embedding quality for OOD score calculation. Therefore, we introduce
a {\bf Sharpness-aware Geometric Defense (SaGD)} framework to smooth out the
rugged adversarial loss landscape in the projected latent geometry. Enhanced
geometric embedding convergence enables accurate ID data characterization,
benefiting OOD detection against adversarial attacks. We use Jitter-based
perturbation in adversarial training to extend the defense ability against
unseen attacks. Our SaGD framework significantly improves FPR and AUC over the
state-of-the-art defense approaches in differentiating CIFAR-100 from six other
OOD datasets under various attacks. We further examine the effects of
perturbations at various adversarial training levels, revealing the
relationship between the sharp loss landscape and adversarial OOD detection.

</details>


### [67] [Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention](https://arxiv.org/abs/2508.17175)
*Leon Dimitrov*

Main category: cs.LG

TL;DR: 比较图transformer中密集和稀疏注意力机制的优缺点，分析适用场景，并讨论当前设计挑战


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络难以捕获长距离节点依赖关系，图transformer通过注意力机制实现全局信息交换，但存在密集和稀疏两种注意力机制需要对比分析

Method: 对比分析密集注意力和稀疏注意力机制，评估它们在计算效率、表达能力、可扩展性等方面的权衡

Result: 明确了两种注意力机制各自的优势和局限性，提供了选择指导原则，识别了当前设计中的关键挑战

Conclusion: 需要根据具体应用场景选择合适的注意力机制，图transformer的注意力设计仍面临计算复杂度、表达能力平衡等挑战

Abstract: Graphs have become a central representation in machine learning for capturing
relational and structured data across various domains. Traditional graph neural
networks often struggle to capture long-range dependencies between nodes due to
their local structure. Graph transformers overcome this by using attention
mechanisms that allow nodes to exchange information globally. However, there
are two types of attention in graph transformers: dense and sparse. In this
paper, we compare these two attention mechanisms, analyze their trade-offs, and
highlight when to use each. We also outline current challenges and problems in
designing attention for graph transformers.

</details>


### [68] [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
*Hikaru Tsujimura,Arush Tagade*

Main category: cs.LG

TL;DR: 通过机制可解释性方法分析LLM过度自信的内部机制，发现自信表征可分解为情感和逻辑两个正交子成分，类似于心理学中的双路径精细加工可能性模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在关键场景中经常表现出过度自信，以不必要的确定性呈现信息，需要研究其内部机制来理解这一行为。

Method: 使用基于人类标注自信度数据集微调的Llama 3.2模型，提取所有层的残差激活，计算相似性指标来定位自信表征，并分析各层对自信度对比的敏感性。

Result: 识别出对自信度最敏感的层，发现高自信表征可分解为情感和逻辑两个正交子成分。情感向量广泛影响预测准确性，而逻辑向量产生更局部化的效应。

Conclusion: 研究为LLM自信度的多组件结构提供了机制性证据，并为缓解过度自信行为指明了途径。

Abstract: Large Language Models (LLMs) often display overconfidence, presenting
information with unwarranted certainty in high-stakes contexts. We investigate
the internal basis of this behavior via mechanistic interpretability. Using
open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness
datasets, we extract residual activations across all layers, and compute
similarity metrics to localize assertive representations. Our analysis
identifies layers most sensitive to assertiveness contrasts and reveals that
high-assertive representations decompose into two orthogonal sub-components of
emotional and logical clusters-paralleling the dual-route Elaboration
Likelihood Model in Psychology. Steering vectors derived from these
sub-components show distinct causal effects: emotional vectors broadly
influence prediction accuracy, while logical vectors exert more localized
effects. These findings provide mechanistic evidence for the multi-component
structure of LLM assertiveness and highlight avenues for mitigating
overconfident behavior.

</details>


### [69] [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens](https://arxiv.org/abs/2508.17196)
*Hao Wen,Xinrui Wu,Yi Sun,Feifei Zhang,Liye Chen,Jie Wang,Yunxin Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.LG

TL;DR: BudgetThinker是一个新颖的框架，通过在推理过程中插入特殊控制令牌来让LLM感知剩余令牌预算，结合两阶段训练（SFT+课程RL）来优化准确性和预算遵守。


<details>
  <summary>Details</summary>
Motivation: 现有LLM通过增加测试时计算来提升推理能力，但带来了显著的延迟和资源成本，限制了在实时或成本敏感场景中的应用。

Method: 提出周期性插入预算控制令牌的方法，配合两阶段训练：监督微调使模型熟悉预算约束，课程强化学习使用长度感知奖励函数优化准确性和预算遵守。

Result: 在多个数学基准测试中，BudgetThinker在各种推理预算下显著超越强基线，保持性能表现。

Conclusion: 该方法为开发高效可控的LLM推理提供了可扩展的解决方案，使先进模型更适合在资源受限和实时环境中部署。

Abstract: Recent advancements in Large Language Models (LLMs) have leveraged increased
test-time computation to enhance reasoning capabilities, a strategy that, while
effective, incurs significant latency and resource costs, limiting their
applicability in real-world time-constrained or cost-sensitive scenarios. This
paper introduces BudgetThinker, a novel framework designed to empower LLMs with
budget-aware reasoning, enabling precise control over the length of their
thought processes. We propose a methodology that periodically inserts special
control tokens during inference to continuously inform the model of its
remaining token budget. This approach is coupled with a comprehensive two-stage
training pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize
the model with budget constraints, followed by a curriculum-based Reinforcement
Learning (RL) phase that utilizes a length-aware reward function to optimize
for both accuracy and budget adherence. We demonstrate that BudgetThinker
significantly surpasses strong baselines in maintaining performance across a
variety of reasoning budgets on challenging mathematical benchmarks. Our method
provides a scalable and effective solution for developing efficient and
controllable LLM reasoning, making advanced models more practical for
deployment in resource-constrained and real-time environments.

</details>


### [70] [How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System](https://arxiv.org/abs/2508.17215)
*Kaiwen Zuo,Zelin Liu,Raman Dutt,Ziyang Wang,Zhongtian Sun,Yeming Wang,Fan Mo,Pietro Liò*

Main category: cs.LG

TL;DR: 这篇论文提出了MedThreatRAG多模态毒性攻击框架，通过注入对立性图像-文本对来攻击医学RAG系统，尤其是通过跨模态冲突注入(CMCI)技术造成严重性能下降。


<details>
  <summary>Details</summary>
Motivation: 医学AI中应用的大视觉-语言模型通过RAG增强事实基础，但这种外部检索依赖创造了重要攻击面，需要系统性地探测安全漏洞。

Method: 构建模拟半开攻击环境，通过注入对立性图像-文本对来攻击RAG系统，重点是跨模态冲突注入(CMCI)技术，在医学图像和报告之间嵌入细微语义矛盾。

Result: 在IU-Xray和MIMIC-CXR QA任务上，MedThreatRAG导致回答F1分数最高下降27.66%，将LLaVA-Med-1.5的F1率降至低于51.36%，CMCI攻击效果最为严重。

Conclusion: 临床RAG系统存在根本性安全漏洞，强调了威胁感知设计和健壮多模态一致性检查的紧迫性，并提供了安全开发指南。

Abstract: Large Vision-Language Models (LVLMs) augmented with Retrieval-Augmented
Generation (RAG) are increasingly employed in medical AI to enhance factual
grounding through external clinical image-text retrieval. However, this
reliance creates a significant attack surface. We propose MedThreatRAG, a novel
multimodal poisoning framework that systematically probes vulnerabilities in
medical RAG systems by injecting adversarial image-text pairs. A key innovation
of our approach is the construction of a simulated semi-open attack
environment, mimicking real-world medical systems that permit periodic
knowledge base updates via user or pipeline contributions. Within this setting,
we introduce and emphasize Cross-Modal Conflict Injection (CMCI), which embeds
subtle semantic contradictions between medical images and their paired reports.
These mismatches degrade retrieval and generation by disrupting cross-modal
alignment while remaining sufficiently plausible to evade conventional filters.
While basic textual and visual attacks are included for completeness, CMCI
demonstrates the most severe degradation. Evaluations on IU-Xray and MIMIC-CXR
QA tasks show that MedThreatRAG reduces answer F1 scores by up to 27.66% and
lowers LLaVA-Med-1.5 F1 rates to as low as 51.36%. Our findings expose
fundamental security gaps in clinical RAG systems and highlight the urgent need
for threat-aware design and robust multimodal consistency checks. Finally, we
conclude with a concise set of guidelines to inform the safe development of
future multimodal medical RAG systems.

</details>


### [71] [GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning](https://arxiv.org/abs/2508.17218)
*Xing Wei,Yuqi Ouyang*

Main category: cs.LG

TL;DR: 本文提出了一种基于决策Transformer和广义策略梯度框架的路径规划解决方案，用于解决随机交通网络中的可靠最短路径问题，在Sioux Falls网络上实验证明优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着城市车辆数量快速增长，现有道路基础设施难以满足现代交通需求，导致拥堵问题。现有导航模型大多只关注确定性或时间依赖网络，忽略了交通流的关联性和随机性特征。

Method: 提出了一种路径规划解决方案，将决策Transformer与广义策略梯度(GPG)框架相结合。利用决策Transformer建模长期依赖关系的能力，提高路径决策的准确性和稳定性。

Result: 在Sioux Falls网络(SFN)上的实验结果表明，该方法在准时到达概率方面优于之前的基线方法，提供了更准确的路径规划解决方案。

Conclusion: 通过整合决策Transformer和GPG框架，能够有效处理随机交通网络中具有依赖关系的可靠最短路径问题，为城市交通拥堵问题提供了更优的路径规划方案。

Abstract: With the rapidly increased number of vehicles in urban areas, existing road
infrastructure struggles to accommodate modern traffic demands, resulting in
the issue of congestion. This highlights the importance of efficient path
planning strategies. However, most recent navigation models focus solely on
deterministic or time-dependent networks, while overlooking the correlations
and the stochastic nature of traffic flows. In this work, we address the
reliable shortest path problem within stochastic transportation networks under
certain dependencies. We propose a path planning solution that integrates the
decision Transformer with the Generalized Policy Gradient (GPG) framework.
Based on the decision Transformer's capability to model long-term dependencies,
our proposed solution improves the accuracy and stability of path decisions.
Experimental results on the Sioux Falls Network (SFN) demonstrate that our
approach outperforms previous baselines in terms of on-time arrival
probability, providing more accurate path planning solutions.

</details>


### [72] [Curvature Learning for Generalization of Hyperbolic Neural Networks](https://arxiv.org/abs/2508.17232)
*Xiaomeng Fan,Yuwei Wu,Zhi Gao,Mehrtash Harandi,Yunde Jia*

Main category: cs.LG

TL;DR: 本文提出了基于PAC-Bayesian泛化界限的曲率学习方法，通过优化双曲神经网络中的曲率参数来平滑损失景观，提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 双曲神经网络在处理层次结构数据方面表现出色，但曲率选择不当会导致次优参数和性能下降。目前缺乏关于曲率对HNNs影响的理论基础。

Method: 提出锐度感知曲率学习方法：设计曲率范围锐度度量，通过双层优化最小化；引入隐式微分算法近似曲率梯度；提供近似误差和收敛性分析。

Result: 在分类、长尾数据学习、噪声数据学习和少样本学习四个设置上的实验表明，该方法能有效提升HNNs的性能。

Conclusion: 通过理论推导和实验验证，证明了曲率在HNNs泛化中的关键作用，提出的曲率学习方法能有效改善网络性能。

Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in
representing real-world data with hierarchical structures via exploiting the
geometric properties of hyperbolic spaces characterized by negative curvatures.
Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may
cause HNNs to converge to suboptimal parameters, degrading overall performance.
So far, the theoretical foundation of the effect of curvatures on HNNs has not
been developed. In this paper, we derive a PAC-Bayesian generalization bound of
HNNs, highlighting the role of curvatures in the generalization of HNNs via
their effect on the smoothness of the loss landscape. Driven by the derived
bound, we propose a sharpness-aware curvature learning method to smooth the
loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized
through a bi-level optimization process. Then, we introduce an implicit
differentiation algorithm that efficiently solves the bi-level optimization by
approximating gradients of curvatures. We present the approximation error and
convergence analyses of the proposed method, showing that the approximation
error is upper-bounded, and the proposed method can converge by bounding
gradients of HNNs. Experiments on four settings: classification, learning from
long-tailed data, learning from noisy data, and few-shot learning show that our
method can improve the performance of HNNs.

</details>


### [73] [Module-Aware Parameter-Efficient Machine Unlearning on Transformers](https://arxiv.org/abs/2508.17233)
*Wenjie Bao,Jian Lou,Yuke Hu,Xiaochen Li,Zhihao Liu,Jiaqi Liu,Zhan Qin,Kui Ren*

Main category: cs.LG

TL;DR: MAPE-Unlearn是一种模块感知的参数高效机器学习遗忘方法，通过可学习的掩码对来精确定位Transformer中影响关键参数，实现高效的数据影响移除。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效遗忘方法以模块无关的方式设计，往往无法准确识别影响关键参数，导致Transformer的遗忘性能不佳。需要一种模块感知的方法来更好地处理Transformer架构的遗忘需求。

Method: 使用可学习的掩码对来精确定位Transformer头部和过滤器中的影响关键参数，通过贪婪搜索和热启动的高效算法优化掩码学习目标。

Result: 在各种Transformer模型和数据集上的广泛实验证明了MAPE-Unlearn在遗忘任务中的有效性和鲁棒性。

Conclusion: MAPE-Unlearn提供了一种有效的模块感知参数高效遗忘方法，能够更好地处理Transformer架构的机器学习遗忘需求，满足隐私法规要求。

Abstract: Transformer has become fundamental to a vast series of pre-trained large
models that have achieved remarkable success across diverse applications.
Machine unlearning, which focuses on efficiently removing specific data
influences to comply with privacy regulations, shows promise in restricting
updates to influence-critical parameters. However, existing parameter-efficient
unlearning methods are largely devised in a module-oblivious manner, which
tends to inaccurately identify these parameters and leads to inferior
unlearning performance for Transformers. In this paper, we propose {\tt
MAPE-Unlearn}, a module-aware parameter-efficient machine unlearning approach
that uses a learnable pair of masks to pinpoint influence-critical parameters
in the heads and filters of Transformers. The learning objective of these masks
is derived by desiderata of unlearning and optimized through an efficient
algorithm featured by a greedy search with a warm start. Extensive experiments
on various Transformer models and datasets demonstrate the effectiveness and
robustness of {\tt MAPE-Unlearn} for unlearning.

</details>


### [74] [Provable Generalization in Overparameterized Neural Nets](https://arxiv.org/abs/2508.17256)
*Aviral Dhingra*

Main category: cs.LG

TL;DR: 通过分析注意力矩阵的有效秩来量化模型容量，提出了一种更适用于过参数化模型的统一性理论方法


<details>
  <summary>Details</summary>
Motivation: 传统的VC维数或PAC-Bayes约束在过参数化模型中失效，无法解释Transformer等模型的统一性能力

Method: 提出以注意力矩阵的有效秩作为容量量度，并得出了相应的统一性约束

Result: 该方法得到的统一性约束与大语言模型的实验缩放律相符，只差对数因子

Conclusion: 注意力的谱特性而非原始参数数量可能是理解过参数化模型统一性的关键

Abstract: Deep neural networks often contain far more parameters than training
examples, yet they still manage to generalize well in practice. Classical
complexity measures such as VC-dimension or PAC-Bayes bounds usually become
vacuous in this overparameterized regime, offering little explanation for the
empirical success of models like Transformers. In this work, I explore an
alternative notion of capacity for attention-based models, based on the
effective rank of their attention matrices. The intuition is that, although the
parameter count is enormous, the functional dimensionality of attention is
often much lower. I show that this quantity leads to a generalization bound
whose dependence on sample size matches empirical scaling laws observed in
large language models, up to logarithmic factors. While the analysis is not a
complete theory of overparameterized learning, it provides evidence that
spectral properties of attention, rather than raw parameter counts, may be the
right lens for understanding why these models generalize.

</details>


### [75] [DeepCFD: Efficient near-ground airfoil lift coefficient approximation with deep convolutional neural networks](https://arxiv.org/abs/2508.17278)
*Mohammad Amin Esabat,Saeed Jaamei,Fatemeh Asadi*

Main category: cs.LG

TL;DR: 使用VGG神经网络方法预测近地面翼型升阻比系数，通过CFD数据和翼型截面图像矩阵进行训练，相比其他CNN方法精度更高


<details>
  <summary>Details</summary>
Motivation: 传统CFD软件计算近地面翼型气动系数耗时较长，而CFD模拟数据的可用性和新神经网络方法的发展使得使用VGG等CNN方法呈现模拟结果成为可能

Method: 采用VGG CNN神经网络方法，通过提供包含升阻比信息的原始数据和翼型截面图像（转换为矩阵）进行训练和学习，实现近地面翼型升阻系数的预测

Result: VGG方法相比其他CNN方法具有更高的预测精度

Conclusion: 神经网络方法特别是VGG CNN可以有效预测近地面翼型的气动性能，为快速气动分析提供了有效替代方案

Abstract: . Predicting and calculating the aerodynamic coefficients of airfoils near
the ground with CFD software requires much time. However, the availability of
data from CFD simulation results and the development of new neural network
methods have made it possible to present the simulation results using methods
like VGG, a CCN neural network method. In this article, lift-to-drag
coefficients of airfoils near the ground surface are predicted with the help of
a neural network. This prediction can only be realized by providing data for
training and learning the code that contains information on the lift-to-drag
ratio of the primary data and images related to the airfoil cross-section,
which are converted into a matrix. One advantage of the VGG method over other
methods is that its results are more accurate than those of other CNN methods.

</details>


### [76] [Explainable AI (XAI) for Arrhythmia detection from electrocardiograms](https://arxiv.org/abs/2508.17294)
*Joschka Beck,Arlene John*

Main category: cs.LG

TL;DR: 这篇论文研究了适用于心电图分析的可解释人工智能技术，发现显著性地图比反事实可视化更符合医疗专业人员的工作流程。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心律失常检测中准确度高但可解释性有限，影响临床应用。需要研究适合时间序列ECG分析的XAI技术。

Method: 使用MIT-BIH心律失常数据集和12导联ECG数据集，构建卷积神经网络模型。进行用户需求评估，并实现四种SHAP基础的解释方法：排列重要性、KernelSHAP、梯度基础方法和DeepLIFT。

Result: 模型在MIT-BIH数据集上达到98.3%的验证准确率，但在结合数据集上性能下降。显著性地图方法比反事实可视化更受医疗专业人员偏好，梯度基础和DeepLIFT方法能够突出与临床推理一致的波形区域。

Conclusion: 心电图分析需要领域特定的XAI适配，显著性地图是更具临床直观性的解释方法，但不同方法在不同样本上存在变异性。

Abstract: Advancements in deep learning have enabled highly accurate arrhythmia
detection from electrocardiogram (ECG) signals, but limited interpretability
remains a barrier to clinical adoption. This study investigates the application
of Explainable AI (XAI) techniques specifically adapted for time-series ECG
analysis. Using the MIT-BIH arrhythmia dataset, a convolutional neural
network-based model was developed for arrhythmia classification, with
R-peak-based segmentation via the Pan-Tompkins algorithm. To increase the
dataset size and to reduce class imbalance, an additional 12-lead ECG dataset
was incorporated. A user needs assessment was carried out to identify what kind
of explanation would be preferred by medical professionals. Medical
professionals indicated a preference for saliency map-based explanations over
counterfactual visualisations, citing clearer correspondence with ECG
interpretation workflows. Four SHapley Additive exPlanations (SHAP)-based
approaches: permutation importance, KernelSHAP, gradient-based methods, and
Deep Learning Important FeaTures (DeepLIFT), were implemented and compared. The
model achieved 98.3% validation accuracy on MIT-BIH but showed performance
degradation on the combined dataset, underscoring dataset variability
challenges. Permutation importance and KernelSHAP produced cluttered visual
outputs, while gradient-based and DeepLIFT methods highlighted waveform regions
consistent with clinical reasoning, but with variability across samples.
Findings emphasize the need for domain-specific XAI adaptations in ECG analysis
and highlight saliency mapping as a more clinically intuitive approach

</details>


### [77] [Physics-informed neural network for fatigue life prediction of irradiated austenitic and ferritic/martensitic steels](https://arxiv.org/abs/2508.17303)
*Dhiraj S Kori,Abhinav Chandraker,Syed Abdur Rahman,Punit Rathore,Ankur Chauhan*

Main category: cs.LG

TL;DR: 提出了基于物理信息的神经网络(PINN)框架来预测核反应堆用辐照奥氏体和铁素体/马氏体钢的低周疲劳寿命，该模型在495个数据点上训练，优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 核反应堆材料在高温循环载荷和辐照条件下会发生复杂退化，传统经验模型无法准确预测其疲劳寿命。

Method: 开发了PINN模型，在损失函数中引入物理疲劳寿命约束，使用495个数据点（包括辐照和未辐照条件）进行训练。

Result: PINN模型在预测精度和泛化能力上优于随机森林、梯度提升、XGBoost和传统神经网络，识别出应变幅值、辐照剂量和测试温度为关键特征。

Conclusion: PINN框架为辐照合金的疲劳寿命预测提供了可靠且可解释的方法，有助于明智的合金选择。

Abstract: This study proposes a Physics-Informed Neural Network (PINN) framework to
predict the low-cycle fatigue (LCF) life of irradiated austenitic and
ferritic/martensitic (F/M) steels used in nuclear reactors. These materials
experience cyclic loading and irradiation at elevated temperatures, causing
complex degradation that traditional empirical models fail to capture
accurately. The developed PINN model incorporates physical fatigue life
constraints into its loss function, improving prediction accuracy and
generalizability. Trained on 495 data points, including both irradiated and
unirradiated conditions, the model outperforms traditional machine learning
models like Random Forest, Gradient Boosting, eXtreme Gradient Boosting, and
the conventional Neural Network. SHapley Additive exPlanations analysis
identifies strain amplitude, irradiation dose, and testing temperature as
dominant features, each inversely correlated with fatigue life, consistent with
physical understanding. PINN captures saturation behaviour in fatigue life at
higher strain amplitudes in F/M steels. Overall, the PINN framework offers a
reliable and interpretable approach for predicting fatigue life in irradiated
alloys, enabling informed alloy selection.

</details>


### [78] [AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations](https://arxiv.org/abs/2508.17320)
*Yifei Yao,Mengnan Du*

Main category: cs.LG

TL;DR: 提出Adaptive Top K稀疏自编码器框架，根据输入语义复杂度动态调整稀疏度，显著优于固定稀疏度方法


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器使用固定稀疏约束，无法适应不同输入的复杂度变化，需要更灵活的特征分配机制

Method: 利用线性探针发现上下文复杂度在线性编码中的线性关系，基于此信号在训练过程中动态调整特征分配的稀疏度水平

Result: 在三个语言模型上的实验表明，该方法在重建保真度、解释方差和余弦相似度指标上显著优于固定稀疏度方法，同时消除了大量超参数调优的计算负担

Conclusion: 基于复杂度驱动的自适应稀疏自编码器为LLM内部表示的可解释性研究提供了更有效的解决方案

Abstract: Understanding the internal representations of large language models (LLMs)
remains a central challenge for interpretability research. Sparse autoencoders
(SAEs) offer a promising solution by decomposing activations into interpretable
features, but existing approaches rely on fixed sparsity constraints that fail
to account for input complexity. We propose Adaptive Top K Sparse Autoencoders
(AdaptiveK), a novel framework that dynamically adjusts sparsity levels based
on the semantic complexity of each input. Leveraging linear probes, we
demonstrate that context complexity is linearly encoded in LLM representations,
and we use this signal to guide feature allocation during training. Experiments
across three language models (Pythia-70M, Pythia-160M, and Gemma-2-2B)
demonstrate that this complexity-driven adaptation significantly outperforms
fixed-sparsity approaches on reconstruction fidelity, explained variance, and
cosine similarity metrics while eliminating the computational burden of
extensive hyperparameter tuning.

</details>


### [79] [Is the Frequency Principle always valid?](https://arxiv.org/abs/2508.17323)
*Qijia Zhai*

Main category: cs.LG

TL;DR: 本文研究了浅层ReLU神经网络在球面S²上的学习动力学，发现频率优先原则(FP)在固定权重和可训练权重情况下都存在，但特定条件下可能被违反，FP应被视为趋势而非绝对规则。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在弯曲域（如球面S²）上的频率相关学习动态，特别是频率优先原则(FP)在不同权重条件下的表现和局限性。

Method: 使用球谐函数展开分析固定权重和可训练权重情况下的学习动态，通过数值实验验证理论分析结果。

Result: 固定权重时系数衰减为O(ℓ^{5/2}/2^ℓ)，可训练权重时衰减为O(ℓ^{7/2}/2^ℓ)，都倾向于低频优先学习，但特定初始条件或误差分布下FP可能被违反。

Conclusion: 频率优先原则在球面等弯曲域上应被视为学习趋势而非绝对规则，权重可训练性增加了学习复杂性并可能改变频率学习顺序。

Abstract: We investigate the learning dynamics of shallow ReLU neural networks on the
unit sphere \(S^2\subset\mathbb{R}^3\) in polar coordinates \((\tau,\phi)\),
considering both fixed and trainable neuron directions \(\{w_i\}\). For fixed
weights, spherical harmonic expansions reveal an intrinsic low-frequency
preference with coefficients decaying as \(O(\ell^{5/2}/2^\ell)\), typically
leading to the Frequency Principle (FP) of lower-frequency-first learning.
However, this principle can be violated under specific initial conditions or
error distributions. With trainable weights, an additional rotation term in the
harmonic evolution equations preserves exponential decay with decay order
\(O(\ell^{7/2}/2^\ell)\) factor, also leading to the FP of
lower-frequency-first learning. But like fixed weights case, the principle can
be violated under specific initial conditions or error distributions. Our
numerical results demonstrate that trainable directions increase learning
complexity and can either maintain a low-frequency advantage or enable faster
high-frequency emergence. This analysis suggests the FP should be viewed as a
tendency rather than a rule on curved domains like \(S^2\), providing insights
into how direction updates and harmonic expansions shape frequency-dependent
learning.

</details>


### [80] [ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable Generation](https://arxiv.org/abs/2508.17345)
*Yuxuan Song,Zhe Zhang,Yu Pei,Jingjing Gong,Qiying Yu,Zheng Zhang,Mingxuan Wang,Hao Zhou,Jingjing Liu,Wei-Ying Ma*

Main category: cs.LG

TL;DR: SLM是一种基于单纯形的扩散模型，通过候选剪枝机制在离散变量生成任务中展现出色性能，特别是在生物序列设计和语言建模方面


<details>
  <summary>Details</summary>
Motivation: 离散变量的生成建模在自然语言处理和生物序列设计中具有重要应用价值，但面临挑战。需要开发能够处理复杂离散空间的高效生成模型

Method: 提出Shortlisting Model (SLM)，这是一种基于单纯形质心的扩散模型，采用渐进式候选剪枝机制。模型结合了无分类器引导的灵活实现，降低了生成复杂度并提高了可扩展性

Result: 在DNA启动子和增强子设计、蛋白质设计、字符级和大词汇量语言建模等任务上进行了广泛实验，证明了SLM具有竞争性的性能和强大潜力

Conclusion: SLM为离散变量生成提供了一种有效的解决方案，在多个重要应用领域展现出优越性能，具有很好的应用前景

Abstract: Generative modeling of discrete variables is challenging yet crucial for
applications in natural language processing and biological sequence design. We
introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model
inspired by progressive candidate pruning. SLM operates on simplex centroids,
reducing generation complexity and enhancing scalability. Additionally, SLM
incorporates a flexible implementation of classifier-free guidance, enhancing
unconditional generation performance. Extensive experiments on DNA promoter and
enhancer design, protein design, character-level and large-vocabulary language
modeling demonstrate the competitive performance and strong potential of SLM.
Our code can be found at https://github.com/GenSI-THUAIR/SLM

</details>


### [81] [Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias](https://arxiv.org/abs/2508.17361)
*Shir Bernstein,David Beste,Daniel Ayzenshteyn,Lea Schonherr,Yisroel Mirsky*

Main category: cs.LG

TL;DR: 本文发现并利用LLM代码分析中的抽象偏见漏洞，提出熟悉模式攻击(FPA)，通过最小代码编辑即可劫持LLM的控制流，且攻击具有跨模型和跨语言的通用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化代码审查和静态分析中被广泛信任，但存在抽象偏见导致模型过度泛化熟悉编程模式而忽略小但重要的bug，攻击者可利用这一盲点进行控制流劫持。

Method: 开发了全自动黑盒算法来发现和注入熟悉模式攻击(FPA)，在多种编程语言(Python、C、Rust、Go)和模型(GPT-4o、Claude 3.5、Gemini 2.0)上进行评估。

Result: FPA攻击不仅有效，而且具有跨模型和跨语言的通用性，即使在模型通过鲁棒系统提示明确警告攻击的情况下仍然有效。

Conclusion: 研究揭示了代码导向LLM的可靠性和安全性问题，同时探讨了FPA的积极防御用途及其对LLM代码分析系统的广泛影响。

Abstract: Large Language Models (LLMs) are increasingly trusted to perform automated
code review and static analysis at scale, supporting tasks such as
vulnerability detection, summarization, and refactoring. In this paper, we
identify and exploit a critical vulnerability in LLM-based code analysis: an
abstraction bias that causes models to overgeneralize familiar programming
patterns and overlook small, meaningful bugs. Adversaries can exploit this
blind spot to hijack the control flow of the LLM's interpretation with minimal
edits and without affecting actual runtime behavior. We refer to this attack as
a Familiar Pattern Attack (FPA).
  We develop a fully automated, black-box algorithm that discovers and injects
FPAs into target code. Our evaluation shows that FPAs are not only effective,
but also transferable across models (GPT-4o, Claude 3.5, Gemini 2.0) and
universal across programming languages (Python, C, Rust, Go). Moreover, FPAs
remain effective even when models are explicitly warned about the attack via
robust system prompts. Finally, we explore positive, defensive uses of FPAs and
discuss their broader implications for the reliability and safety of
code-oriented LLMs.

</details>


### [82] [ShaLa: Multimodal Shared Latent Space Modelling](https://arxiv.org/abs/2508.17376)
*Jiali Cui,Yan-Ying Chen,Yanxia Zhang,Matthew Klenk*

Main category: cs.LG

TL;DR: ShaLa是一个新颖的多模态生成框架，通过集成创新的架构推理模型和扩散先验，解决了多模态VAE在共享潜在表示学习和合成质量方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法过于关注模态特定的细节组合，反而模糊了跨模态共享的高层语义概念。多模态VAE虽然旨在捕获共享表示，但在表达性联合变分后验设计和合成质量方面存在困难。

Method: ShaLa整合了新颖的架构推理模型和第二阶段表达性扩散先验，既促进了共享潜在表示的有效推断，又显著提高了下游多模态合成的质量。

Result: 在多个基准测试中验证了ShaLa的优越性能，相比最先进的多模态VAE表现出更好的连贯性和合成质量，并且能够扩展到更多模态。

Conclusion: ShaLa成功解决了多模态VAE的关键挑战，在共享潜在表示学习和多模态合成方面实现了显著改进，具有良好的可扩展性。

Abstract: This paper presents a novel generative framework for learning shared latent
representations across multimodal data. Many advanced multimodal methods focus
on capturing all combinations of modality-specific details across inputs, which
can inadvertently obscure the high-level semantic concepts that are shared
across modalities. Notably, Multimodal VAEs with low-dimensional latent
variables are designed to capture shared representations, enabling various
tasks such as joint multimodal synthesis and cross-modal inference. However,
multimodal VAEs often struggle to design expressive joint variational
posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses
these challenges by integrating a novel architectural inference model and a
second-stage expressive diffusion prior, which not only facilitates effective
inference of shared latent representation but also significantly improves the
quality of downstream multimodal synthesis. We validate ShaLa extensively
across multiple benchmarks, demonstrating superior coherence and synthesis
quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales
to many more modalities while prior multimodal VAEs have fallen short in
capturing the increasing complexity of the shared latent space.

</details>


### [83] [FedERL: Federated Efficient and Robust Learning for Common Corruptions](https://arxiv.org/abs/2508.17381)
*Omar Bekdache,Naresh Shanbhag*

Main category: cs.LG

TL;DR: FedERL是首个在联邦学习中同时解决客户端资源约束和抗数据损坏鲁棒性的方法，通过服务器端数据无关的鲁棒训练技术实现零客户端开销


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端计算资源受限和缺乏对常见数据损坏（噪声、模糊、天气效应）鲁棒性的挑战，现有鲁棒训练方法计算成本高且不适合资源受限的客户端

Method: 提出FedERL框架，采用新颖的数据无关鲁棒训练（DART）方法在服务器端增强模型鲁棒性，无需访问训练数据，确保客户端零鲁棒性开销

Result: 大量实验表明FedERL能够以传统鲁棒训练方法的一小部分时间和能耗成本处理常见数据损坏，在有限时间和能量预算下性能超越传统方法

Conclusion: FedERL为现实世界联邦学习应用提供了一个实用且可扩展的解决方案，在保持数据隐私的同时有效解决了资源约束下的鲁棒性问题

Abstract: Federated learning (FL) accelerates the deployment of deep learning models on
edge devices while preserving data privacy. However, FL systems face challenges
due to client-side constraints on computational resources, and from a lack of
robustness to common corruptions such as noise, blur, and weather effects.
Existing robust training methods are computationally expensive and unsuitable
for resource-constrained clients. We propose FedERL, federated efficient and
robust learning, as the first work to explicitly address corruption robustness
under time and energy constraints on the client side. At its core, FedERL
employs a novel data-agnostic robust training (DART) method on the server to
enhance robustness without access to the training data. In doing so, FedERL
ensures zero robustness overhead for clients. Extensive experiments demonstrate
FedERL's ability to handle common corruptions at a fraction of the time and
energy cost of traditional robust training methods. In scenarios with limited
time and energy budgets, FedERL surpasses the performance of traditional robust
training, establishing it as a practical and scalable solution for real-world
FL applications.

</details>


### [84] [Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning](https://arxiv.org/abs/2508.17387)
*Yicong Wu,Guangyue Lu,Yuan Zuo,Huarong Zhang,Junjie Wu*

Main category: cs.LG

TL;DR: 提出Graph-R1框架，将图任务转化为文本推理问题，使用强化学习指导大型推理模型进行零样本图学习，无需任务特定监督。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络固定标签空间限制和大型语言模型缺乏结构归纳偏置的问题，探索通过显式推理链实现零样本图任务泛化。

Method: 将节点分类、链接预测和图分类任务重新表述为文本推理问题，使用强化学习框架和重思考模板指导大型推理模型对线性化图进行推理。

Result: Graph-R1在零样本设置下优于最先进的基线方法，产生可解释且有效的预测结果。

Conclusion: 显式推理为图学习提供了新途径，为未来研究提供了新的资源和方法论。

Abstract: Generalizing to unseen graph tasks without task-pecific supervision remains
challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,
while Large Language Models (LLMs) lack structural inductive biases. Recent
advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via
explicit, long chain-of-thought reasoning. Inspired by this, we propose a
GNN-free approach that reformulates graph tasks--node classification, link
prediction, and graph classification--as textual reasoning problems solved by
LRMs. We introduce the first datasets with detailed reasoning traces for these
tasks and develop Graph-R1, a reinforcement learning framework that leverages
task-specific rethink templates to guide reasoning over linearized graphs.
Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in
zero-shot settings, producing interpretable and effective predictions. Our work
highlights the promise of explicit reasoning for graph learning and provides
new resources for future research.

</details>


### [85] [Effective Clustering for Large Multi-Relational Graphs](https://arxiv.org/abs/2508.17388)
*Xiaoyang Lin,Runhao Jiang,Renchi Yang*

Main category: cs.LG

TL;DR: DEMM和DEMM+是针对多关系图聚类问题的两个高效方法，通过两阶段优化目标解决现有方法质量差或扩展性差的问题，在聚类质量和效率方面都表现优异


<details>
  <summary>Details</summary>
Motivation: 现有多关系图聚类方法要么因异构图结构和属性融合效果差导致结果质量严重受损，要么因采用复杂昂贵的深度学习模型而无法处理大规模图数据

Method: 基于新颖的两阶段优化目标：第一阶段通过优化多关系Dirichlet能量获得高质量节点特征向量，第二阶段在节点亲和图上最小化聚类结果的Dirichlet能量。DEMM+通过高效近似求解器和线性时间聚类优化进一步提升可扩展性

Result: 在11个真实多关系图上与20个基线方法比较，DEMM+在聚类质量（基于真实标签评估）方面始终表现优越，且通常显著更快

Conclusion: DEMM+方法通过技术创新有效解决了多关系图聚类的质量和可扩展性问题，能够处理百万节点和十亿边的大规模图数据，并可扩展到无属性图

Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling
diverse interactions/relations among real objects (i.e., nodes), which pervade
extensive applications and scenarios. Given an MRG G with N nodes, partitioning
the node set therein into K disjoint clusters (MRGC) is a fundamental task in
analyzing MRGs, which has garnered considerable attention. However, the
majority of existing solutions towards MRGC either yield severely compromised
result quality by ineffective fusion of heterogeneous graph structures and
attributes, or struggle to cope with sizable MRGs with millions of nodes and
billions of edges due to the adoption of sophisticated and costly deep learning
models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to
address the limitations above. Specifically, our algorithms are built on novel
two-stage optimization objectives, where the former seeks to derive
high-caliber node feature vectors by optimizing the multi-relational Dirichlet
energy specialized for MRGs, while the latter minimizes the Dirichlet energy of
clustering results over the node affinity graph. In particular, DEMM+ achieves
significantly higher scalability and efficiency over our based method DEMM
through a suite of well-thought-out optimizations. Key technical contributions
include (i) a highly efficient approximation solver for constructing node
feature vectors, and (ii) a theoretically-grounded problem transformation with
carefully-crafted techniques that enable linear-time clustering without
explicitly materializing the NxN dense affinity matrix. Further, we extend
DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive
experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit
that DEMM+ is consistently superior in terms of clustering quality measured
against ground-truth labels, while often being remarkably faster.

</details>


### [86] [Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs](https://arxiv.org/abs/2508.17400)
*Jacob Portes,Connor Jennings,Erica Ji Yuen,Sasha Doubov,Michael Carbin*

Main category: cs.LG

TL;DR: 检索性能随预训练计算量(FLOPs)呈可预测的缩放规律，模型大小、训练时长和FLOPs都与零样本BEIR检索任务表现正相关，且上下文学习能力与检索性能强相关。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLM)的检索性能如何随预训练计算规模变化，为开发基于LLM的检索器提供指导。

Method: 对参数规模从1.25亿到70亿的LLM进行基准测试，使用从10亿到超过2万亿token的数据集进行预训练，评估零样本BEIR任务的检索性能。

Result: 检索性能与模型大小、训练时长和估计FLOPs呈可预测的缩放关系，上下文学习得分与检索得分在检索任务中强相关。

Conclusion: 研究结果为基于LLM的检索器开发提供了重要指导，表明检索性能可以通过预训练计算规模进行预测和优化。

Abstract: How does retrieval performance scale with pretraining FLOPs? We benchmark
retrieval performance across LLM model sizes from 125 million parameters to 7
billion parameters pretrained on datasets ranging from 1 billion tokens to more
than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR
tasks predictably scales with LLM size, training duration, and estimated FLOPs.
We also show that In-Context Learning scores are strongly correlated with
retrieval scores across retrieval tasks. Finally, we highlight the implications
this has for the development of LLM-based retrievers.

</details>


### [87] [Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems](https://arxiv.org/abs/2508.17403)
*Yinsong Wang,Xiao Liu,Quan Zeng,Yu Ding*

Main category: cs.LG

TL;DR: 本文提出了互信息惊喜(MIS)框架，将惊喜重新定义为认知增长的信号而非异常检测，并开发了MISRP策略来动态调整自主系统的行为。


<details>
  <summary>Details</summary>
Motivation: 现有自主实验系统缺乏检测和适应意外情况的机制，传统惊喜度量只能检测偏差但无法捕捉系统是否真正在学习适应。

Method: 引入互信息惊喜(MIS)量化新观测对互信息的影响，开发统计测试序列检测互信息变化，提出MISRP策略通过采样调整和进程分叉动态控制系统行为。

Result: 在合成领域和动态污染图估计任务中，MISRP策略在稳定性、响应性和预测准确性方面显著优于传统基于惊喜的方法。

Conclusion: MIS将惊喜从反应性转变为反思性，为构建更具自我意识和适应性的自主系统提供了新路径。

Abstract: Recent breakthroughs in autonomous experimentation have demonstrated
remarkable physical capabilities, yet their cognitive control remains
limited--often relying on static heuristics or classical optimization. A core
limitation is the absence of a principled mechanism to detect and adapt to the
unexpectedness. While traditional surprise measures--such as Shannon or
Bayesian Surprise--offer momentary detection of deviation, they fail to capture
whether a system is truly learning and adapting. In this work, we introduce
Mutual Information Surprise (MIS), a new framework that redefines surprise not
as anomaly detection, but as a signal of epistemic growth. MIS quantifies the
impact of new observations on mutual information, enabling autonomous systems
to reflect on their learning progression. We develop a statistical test
sequence to detect meaningful shifts in estimated mutual information and
propose a mutual information surprise reaction policy (MISRP) that dynamically
governs system behavior through sampling adjustment and process forking.
Empirical evaluations--on both synthetic domains and a dynamic pollution map
estimation task--show that MISRP-governed strategies significantly outperform
classical surprise-based approaches in stability, responsiveness, and
predictive accuracy. By shifting surprise from reactive to reflective, MIS
offers a path toward more self-aware and adaptive autonomous systems.

</details>


### [88] [FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats](https://arxiv.org/abs/2508.17405)
*Avishag Shapira,Simon Shigol,Asaf Shabtai*

Main category: cs.LG

TL;DR: FRAME是一个全面的自动化框架，用于评估不同机器学习系统的对抗性机器学习风险，通过量化部署环境、攻击技术和实证研究三个维度来提供可操作的风险评估结果。


<details>
  <summary>Details</summary>
Motivation: 传统风险评估框架无法有效应对对抗性机器学习（AML）带来的独特挑战，现有AML威胁评估方法主要关注技术攻击鲁棒性，忽略了部署环境、系统依赖性和攻击可行性等现实因素，且缺乏跨领域应用的通用解决方案。

Method: FRAME框架包含新颖的风险评估方法，系统评估三个关键维度：目标系统部署环境、多样化AML技术特征、先前研究的实证见解。采用可行性评分机制和基于LLM的系统特定评估定制，并开发了全面的结构化AML攻击数据集。

Result: 在六个不同的现实世界应用中进行验证，显示出卓越的准确性和与AML专家分析的高度一致性，能够帮助组织优先处理AML风险。

Conclusion: FRAME是首个全面自动化评估AML风险的框架，使组织能够在现实环境中优先处理AML风险，支持安全AI部署，即使没有AML专业知识的系统所有者也能直接使用。

Abstract: The widespread adoption of machine learning (ML) systems increased attention
to their security and emergence of adversarial machine learning (AML)
techniques that exploit fundamental vulnerabilities in ML systems, creating an
urgent need for comprehensive risk assessment for ML-based systems. While
traditional risk assessment frameworks evaluate conventional cybersecurity
risks, they lack ability to address unique challenges posed by AML threats.
Existing AML threat evaluation approaches focus primarily on technical attack
robustness, overlooking crucial real-world factors like deployment
environments, system dependencies, and attack feasibility. Attempts at
comprehensive AML risk assessment have been limited to domain-specific
solutions, preventing application across diverse systems. Addressing these
limitations, we present FRAME, the first comprehensive and automated framework
for assessing AML risks across diverse ML-based systems. FRAME includes a novel
risk assessment method that quantifies AML risks by systematically evaluating
three key dimensions: target system's deployment environment, characteristics
of diverse AML techniques, and empirical insights from prior research. FRAME
incorporates a feasibility scoring mechanism and LLM-based customization for
system-specific assessments. Additionally, we developed a comprehensive
structured dataset of AML attacks enabling context-aware risk assessment. From
an engineering application perspective, FRAME delivers actionable results
designed for direct use by system owners with only technical knowledge of their
systems, without expertise in AML. We validated it across six diverse
real-world applications. Our evaluation demonstrated exceptional accuracy and
strong alignment with analysis by AML experts. FRAME enables organizations to
prioritize AML risks, supporting secure AI deployment in real-world
environments.

</details>


### [89] [Convergence and Generalization of Anti-Regularization for Parametric Models](https://arxiv.org/abs/2508.17412)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 提出反正则化(AR)方法，通过添加符号反转的奖励项来在小样本情况下增加模型表达能力，并随着样本量增长使用幂律衰减来减弱这种干预。


<details>
  <summary>Details</summary>
Motivation: 解决小样本学习中的欠拟合问题，通过增加模型表达能力来改善泛化性能和校准能力。

Method: 在损失函数中添加符号反转的奖励项，采用幂律衰减调度，结合投影算子和梯度裁剪的稳定性保护机制，在NTK机制下进行分析。

Result: AR方法减少了欠拟合，保持了泛化能力，改善了回归和分类任务的校准性能。消融研究证实衰减调度和稳定性保护对防止过拟合和数值不稳定性至关重要。

Conclusion: AR方法简单易实现，可复现，能无缝集成到标准经验风险最小化流程中，在数据和资源受限的环境中实现稳健学习。

Abstract: We propose Anti-regularization (AR), which adds a sign-reversed reward term
to the loss to intentionally increase model expressivity in the small-sample
regime, and then attenuates this intervention with a power-law decay as the
sample size grows. We formalize spectral safety and trust-region conditions,
and design a lightweight stability safeguard that combines a projection
operator with gradient clipping, ensuring stable intervention under stated
assumptions. Our analysis spans linear smoothers and the Neural Tangent Kernel
(NTK) regime, providing practical guidance on selecting the decay exponent by
balancing empirical risk against variance. Empirically, AR reduces underfitting
while preserving generalization and improving calibration in both regression
and classification. Ablation studies confirm that the decay schedule and the
stability safeguard are critical to preventing overfitting and numerical
instability. We further examine a degrees-of-freedom targeting schedule that
keeps per-sample complexity approximately constant. AR is simple to implement
and reproducible, integrating cleanly into standard empirical risk minimization
pipelines. It enables robust learning in data- and resource-constrained
settings by intervening only when beneficial and fading away when unnecessary.

</details>


### [90] [Modular MeanFlow: Towards Stable and Scalable One-Step Generative Modeling](https://arxiv.org/abs/2508.17426)
*Haochen You,Baojing Liu,Hongyang He*

Main category: cs.LG

TL;DR: MMF是一种一步生成建模方法，通过学习时间平均速度场实现高效数据生成，避免了传统扩散模型的多步采样过程。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型需要多步采样，效率较低。本文旨在开发一种单步生成方法，提高生成效率同时保持样本质量。

Method: 提出Modular MeanFlow (MMF)方法，基于瞬时速度与平均速度的微分恒等式推导损失函数族，引入梯度调制机制稳定训练，并采用课程式预热调度。

Result: 在图像合成和轨迹建模任务中，MMF实现了竞争性的样本质量、稳健的收敛性和强泛化能力，特别是在低数据或分布外设置下表现优异。

Conclusion: MMF统一并推广了现有的基于一致性和流匹配的方法，避免了昂贵的高阶导数计算，为高效生成建模提供了灵活且理论完备的解决方案。

Abstract: One-step generative modeling seeks to generate high-quality data samples in a
single function evaluation, significantly improving efficiency over traditional
diffusion or flow-based models. In this work, we introduce Modular MeanFlow
(MMF), a flexible and theoretically grounded approach for learning
time-averaged velocity fields. Our method derives a family of loss functions
based on a differential identity linking instantaneous and average velocities,
and incorporates a gradient modulation mechanism that enables stable training
without sacrificing expressiveness. We further propose a curriculum-style
warmup schedule to smoothly transition from coarse supervision to fully
differentiable training. The MMF formulation unifies and generalizes existing
consistency-based and flow-matching methods, while avoiding expensive
higher-order derivatives. Empirical results across image synthesis and
trajectory modeling tasks demonstrate that MMF achieves competitive sample
quality, robust convergence, and strong generalization, particularly under
low-data or out-of-distribution settings.

</details>


### [91] [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
*Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: TreePO是一种基于树结构搜索的强化学习对齐方法，通过分段采样和早期剪枝策略，在保持探索多样性的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型对齐方法虽然能解决复杂推理问题，但存在昂贵的在线策略rollout成本高和推理路径探索有限的问题。

Method: 采用树结构搜索过程，包含动态树采样策略和固定长度分段解码，利用局部不确定性保证额外分支，通过分摊计算和早期剪枝低价值路径来减少计算负担。

Result: 在推理基准测试中显示性能提升，GPU时间节省22%-43%，轨迹级采样计算减少40%，token级减少35%。

Conclusion: TreePO为基于强化学习的后训练提供了一条实用路径，以更少的样本和计算实现推理效率的提升。

Abstract: Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

</details>


### [92] [Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality](https://arxiv.org/abs/2508.17448)
*Shaocong Ma,Ziyi Chen,Yi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了RRPO算法，解决了鲁棒约束强化学习中强对偶性不成立的问题，通过纯原始方法找到近似最优的可行策略。


<details>
  <summary>Details</summary>
Motivation: 传统基于对偶的方法在鲁棒约束RL中可能失效，因为强对偶性一般不成立，需要开发不依赖对偶形式的原始算法。

Method: 提出Rectified Robust Policy Optimization (RRPO)算法，直接在原始问题上操作，不依赖于对偶公式化。

Result: 理论证明在温和正则性假设下收敛到近似最优可行策略，迭代复杂度匹配已知下界；实验验证在网格世界环境中实现鲁棒安全性能。

Conclusion: RRPO能有效处理模型不确定性下的鲁棒约束RL问题，相比非鲁棒方法能保证最坏情况安全约束。

Abstract: The goal of robust constrained reinforcement learning (RL) is to optimize an
agent's performance under the worst-case model uncertainty while satisfying
safety or resource constraints. In this paper, we demonstrate that strong
duality does not generally hold in robust constrained RL, indicating that
traditional primal-dual methods may fail to find optimal feasible policies. To
overcome this limitation, we propose a novel primal-only algorithm called
Rectified Robust Policy Optimization (RRPO), which operates directly on the
primal problem without relying on dual formulations. We provide theoretical
convergence guarantees under mild regularity assumptions, showing convergence
to an approximately optimal feasible policy with iteration complexity matching
the best-known lower bound when the uncertainty set diameter is controlled in a
specific level. Empirical results in a grid-world environment validate the
effectiveness of our approach, demonstrating that RRPO achieves robust and safe
performance under model uncertainties while the non-robust method can violate
the worst-case safety constraints.

</details>


### [93] [ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories](https://arxiv.org/abs/2508.17452)
*Dou Jiabao,Nie Jiayi,Yihang Cheng,Jinwei Liu,Yingrui Ji,Canran Xiao,Feixiang Du,Jiaping Xiao*

Main category: cs.LG

TL;DR: ReviBranch是一个新颖的深度强化学习框架，通过构建复活轨迹来解决MILP问题中分支定界算法的分支变量选择问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统分支启发式方法在异构问题实例间泛化能力差，现有学习方法如模仿学习依赖专家演示质量，强化学习面临稀疏奖励和动态状态表示的挑战。

Method: 提出ReviBranch框架：1）构建复活轨迹，复活分支决策与其对应图状态之间的历史对应关系；2）引入重要性加权奖励重分配机制，将稀疏终端奖励转化为密集的逐步反馈。

Result: 在不同MILP基准测试中，ReviBranch优于最先进的RL方法，在大规模实例上减少4.0%的B&B节点和2.2%的LP迭代次数。

Conclusion: ReviBranch在异构MILP问题类别中展现出强大的鲁棒性和泛化能力，为解决分支变量选择问题提供了有效解决方案。

Abstract: The Branch-and-bound (B&B) algorithm is the main solver for Mixed Integer
Linear Programs (MILPs), where the selection of branching variable is essential
to computational efficiency. However, traditional heuristics for branching
often fail to generalize across heterogeneous problem instances, while existing
learning-based methods such as imitation learning (IL) suffers from dependence
on expert demonstration quality, and reinforcement learning (RL) struggles with
limitations in sparse rewards and dynamic state representation challenges. To
address these issues, we propose ReviBranch, a novel deep RL framework that
constructs revived trajectories by reviving explicit historical correspondences
between branching decisions and their corresponding graph states along
search-tree paths. During training, ReviBranch enables agents to learn from
complete structural evolution and temporal dependencies within the branching
process. Additionally, we introduce an importance-weighted reward
redistribution mechanism that transforms sparse terminal rewards into dense
stepwise feedback, addressing the sparse reward challenge. Extensive
experiments on different MILP benchmarks demonstrate that ReviBranch
outperforms state-of-the-art RL methods, reducing B&B nodes by 4.0% and LP
iterations by 2.2% on large-scale instances. The results highlight the
robustness and generalizability of ReviBranch across heterogeneous MILP problem
classes.

</details>


### [94] [A Systematic Literature Review on Multi-label Data Stream Classification](https://arxiv.org/abs/2508.17455)
*H. Freire-Oliveira,E. R. F. Paiva,J. Gama,L. Khan,R. Cerri*

Main category: cs.LG

TL;DR: 本文对多标签数据流分类方法进行了系统性文献综述，分析了该领域的最新进展、方法分类、评估策略和计算复杂度，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流分类在现实世界中具有高度适用性，但面临数据高速连续到达、概念漂移、新标签出现和真实标签延迟到达等挑战，需要系统性的综述来梳理现有方法。

Method: 采用系统性文献综述方法，对多标签数据流分类提案进行深入分析，构建方法层次结构，分析评估策略、渐进复杂度和资源消耗。

Result: 提供了多标签数据流分类方法的全面概述和系统分类，讨论了各种问题处理方式，分析了方法的计算性能和资源需求。

Conclusion: 识别了该领域的主要研究空白，为未来研究提供了方向建议，强调需要进一步解决概念漂移、标签延迟和计算效率等问题。

Abstract: Classification in the context of multi-label data streams represents a
challenge that has attracted significant attention due to its high real-world
applicability. However, this task faces problems inherent to dynamic
environments, such as the continuous arrival of data at high speed and volume,
changes in the data distribution (concept drift), the emergence of new labels
(concept evolution), and the latency in the arrival of ground truth labels.
This systematic literature review presents an in-depth analysis of multi-label
data stream classification proposals. We characterize the latest methods in the
literature, providing a comprehensive overview, building a thorough hierarchy,
and discussing how the proposals approach each problem. Furthermore, we discuss
the adopted evaluation strategies and analyze the methods' asymptotic
complexity and resource consumption. Finally, we identify the main gaps and
offer recommendations for future research directions in the field.

</details>


### [95] [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)
*Liv Gorton,Owen Lewis*

Main category: cs.LG

TL;DR: 本文提出叠加(superposition)机制可能是对抗样本现象的主要成因，通过理论分析、玩具模型实验和ResNet18验证了该假设


<details>
  <summary>Details</summary>
Motivation: 对抗样本作为深度学习中最令人困惑的现象之一，近十年来缺乏共识性的解释机制，叠加这一机制解释性概念可能是主要原因

Method: 通过四条证据链验证假设：(1)理论分析叠加解释对抗现象的能力；(2)在玩具模型中干预叠加控制鲁棒性；(3)在玩具模型中通过对抗训练干预鲁棒性控制叠加；(4)在ResNet18中通过对抗训练验证叠加与鲁棒性的关系

Result: 叠加机制理论上能够解释多种对抗现象，实验表明叠加与模型鲁棒性存在双向因果关系，对抗训练会减少叠加现象

Conclusion: 叠加是导致对抗样本现象的关键因素，这为理解对抗性脆弱性提供了新的机制性解释框架

Abstract: Adversarial examples -- inputs with imperceptible perturbations that fool
neural networks -- remain one of deep learning's most perplexing phenomena
despite nearly a decade of research. While numerous defenses and explanations
have been proposed, there is no consensus on the fundamental mechanism. One
underexplored hypothesis is that superposition, a concept from mechanistic
interpretability, may be a major contributing factor, or even the primary
cause. We present four lines of evidence in support of this hypothesis, greatly
extending prior arguments by Elhage et al. (2022): (1) superposition can
theoretically explain a range of adversarial phenomena, (2) in toy models,
intervening on superposition controls robustness, (3) in toy models,
intervening on robustness (via adversarial training) controls superposition,
and (4) in ResNet18, intervening on robustness (via adversarial training)
controls superposition.

</details>


### [96] [A Human-In-The-Loop Approach for Improving Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.17477)
*Martin Käppel,Julian Neuberger,Felix Möhrlein,Sven Weinzierl,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: 提出了一种新的模型无关方法来识别和纠正预测性业务流程监控模型中的偏见决策，即使同一敏感属性同时被公平和不公平使用。该方法通过人类参与循环，在从原始预测模型提取的决策树上进行简单修改来区分公平与不公平决策。


<details>
  <summary>Details</summary>
Motivation: 现有的预测性业务流程监控模型虽然表现出色，但容易受到数据中不公平、偏见或不道德模式的影响，导致基于敏感属性（如性别、年龄）的偏见预测。现有解决方案通常完全移除敏感属性，但同一敏感属性可能在流程中同时被公平和不公平使用。

Method: 使用模型无关的方法，通过人类参与循环的方式，在从原始预测模型提取的决策树上进行简单修改，从而区分公平和不公平的决策使用敏感属性。

Result: 该方法在存在偏见数据的情况下，实现了公平性和准确性之间的良好平衡。所有源代码和数据都已公开。

Conclusion: 该方法为处理预测性业务流程监控中的偏见问题提供了有效的解决方案，特别是在敏感属性同时被公平和不公平使用的复杂场景下，通过人类参与确保了决策的合理性。

Abstract: Predictive process monitoring enables organizations to proactively react and
intervene in running instances of a business process. Given an incomplete
process instance, predictions about the outcome, next activity, or remaining
time are created. This is done by powerful machine learning models, which have
shown impressive predictive performance. However, the data-driven nature of
these models makes them susceptible to finding unfair, biased, or unethical
patterns in the data. Such patterns lead to biased predictions based on
so-called sensitive attributes, such as the gender or age of process
participants. Previous work has identified this problem and offered solutions
that mitigate biases by removing sensitive attributes entirely from the process
instance. However, sensitive attributes can be used both fairly and unfairly in
the same process instance. For example, during a medical process, treatment
decisions could be based on gender, while the decision to accept a patient
should not be based on gender. This paper proposes a novel, model-agnostic
approach for identifying and rectifying biased decisions in predictive business
process monitoring models, even when the same sensitive attribute is used both
fairly and unfairly. The proposed approach uses a human-in-the-loop approach to
differentiate between fair and unfair decisions through simple alterations on a
decision tree model distilled from the original prediction model. Our results
show that the proposed approach achieves a promising tradeoff between fairness
and accuracy in the presence of biased data. All source code and data are
publicly available at https://doi.org/10.5281/zenodo.15387576.

</details>


### [97] [Multimodal Representation Learning Conditioned on Semantic Relations](https://arxiv.org/abs/2508.17497)
*Yang Qiao,Yuntong Hu,Liang Zhao*

Main category: cs.LG

TL;DR: 提出了RCML框架，通过自然语言关系描述指导多模态表示学习，解决传统对比学习中关系利用不足、缺乏上下文对齐和模态内一致性有限的问题


<details>
  <summary>Details</summary>
Motivation: 传统多模态对比学习模型（如CLIP）存在三个主要局限：1）主要关注图像-文本对，未能充分利用不同对之间的语义关系；2）直接匹配全局嵌入而缺乏上下文对齐；3）强调跨模态对比但模态内一致性支持有限

Method: 构建由语义关系连接的多样化训练对，引入关系引导的交叉注意力机制在关系上下文中调节多模态表示，训练目标结合了跨模态和模态内对比损失

Result: 在不同数据集上的实验表明，RCML在检索和分类任务上均优于强基线方法

Conclusion: 利用语义关系指导多模态表示学习是有效的，RCML框架通过关系条件化学习显著提升了多模态表示的质量和性能

Abstract: Multimodal representation learning has advanced rapidly with contrastive
models such as CLIP, which align image-text pairs in a shared embedding space.
However, these models face limitations: (1) they typically focus on image-text
pairs, underutilizing the semantic relations across different pairs. (2) they
directly match global embeddings without contextualization, overlooking the
need for semantic alignment along specific subspaces or relational dimensions;
and (3) they emphasize cross-modal contrast, with limited support for
intra-modal consistency. To address these issues, we propose
Relation-Conditioned Multimodal Learning RCML, a framework that learns
multimodal representations under natural-language relation descriptions to
guide both feature extraction and alignment. Our approach constructs
many-to-many training pairs linked by semantic relations and introduces a
relation-guided cross-attention mechanism that modulates multimodal
representations under each relation context. The training objective combines
inter-modal and intra-modal contrastive losses, encouraging consistency across
both modalities and semantically related samples. Experiments on different
datasets show that RCML consistently outperforms strong baselines on both
retrieval and classification tasks, highlighting the effectiveness of
leveraging semantic relations to guide multimodal representation learning.

</details>


### [98] [Learning Interpretable Differentiable Logic Networks for Time-Series Classification](https://arxiv.org/abs/2508.17512)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: 首次将可微分逻辑网络(DLNs)应用于单变量时间序列分类，通过特征提取将时间序列转换为适合DLN分类的向量形式，并通过联合超参数搜索优化配置，在51个基准数据集上验证了DLN在准确性、推理效率和可解释性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 将DLNs扩展到时间序列分类领域，探索其在保持准确性、可解释性和计算效率的同时，能否在时间序列数据上同样表现优异。

Method: 使用Catch22和TSFresh特征提取方法将时间序列转换为向量形式，采用联合超参数搜索而非孤立消融实验来优化DLN配置。

Result: 在51个公开单变量时间序列分类基准测试中，DLNs保持了竞争性的准确性、低推理成本和透明的决策逻辑，与表格分类领域的结果一致。

Conclusion: DLNs成功应用于时间序列分类领域，证明了其在保持核心优势的同时能够适应新的数据类型，为时间序列分析提供了准确、高效且可解释的解决方案。

Abstract: Differentiable logic networks (DLNs) have shown promising results in tabular
domains by combining accuracy, interpretability, and computational efficiency.
In this work, we apply DLNs to the domain of TSC for the first time, focusing
on univariate datasets. To enable DLN application in this context, we adopt
feature-based representations relying on Catch22 and TSFresh, converting
sequential time series into vectorized forms suitable for DLN classification.
Unlike prior DLN studies that fix the training configuration and vary various
settings in isolation via ablation, we integrate all such configurations into
the hyperparameter search space, enabling the search process to select jointly
optimal settings. We then analyze the distribution of selected configurations
to better understand DLN training dynamics. We evaluate our approach on 51
publicly available univariate TSC benchmarks. The results confirm that
classification DLNs maintain their core strengths in this new domain: they
deliver competitive accuracy, retain low inference cost, and provide
transparent, interpretable decision logic, thus aligning well with previous DLN
findings in the realm of tabular classification and regression tasks.

</details>


### [99] [GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts](https://arxiv.org/abs/2508.17515)
*Kyrylo Yemets,Mykola Lukashchuk,Ivan Izonin*

Main category: cs.LG

TL;DR: 提出了一种简化训练过程的单变量时间序列预测模型，结合稀疏MoE计算和新型注意力机制门控，无需辅助负载均衡损失即可实现平衡的专家利用和优异预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型需要复杂的训练过程（包括主预测损失和辅助负载均衡损失）以及精细的路由/温度调优，这阻碍了实际应用。需要简化训练过程并有效处理长短期预测需求。

Method: 结合稀疏MoE计算和新型注意力启发式门控机制，替代传统的单层softmax路由器，自然促进专家平衡利用。

Result: 在多个数据集上的实验表明，该模型在仅使用最先进transformer模型（如PatchTST）一小部分参数的情况下获得更好性能，计算效率优于LSTM，支持成本效益推理。

Conclusion: 该方法在准确性和计算效率都至关重要的实际时间序列预测应用中具有巨大潜力，简化了训练过程并实现了优异的预测性能。

Abstract: Accurate univariate forecasting remains a pressing need in real-world
systems, such as energy markets, hydrology, retail demand, and IoT monitoring,
where signals are often intermittent and horizons span both short- and
long-term. While transformers and Mixture-of-Experts (MoE) architectures are
increasingly favored for time-series forecasting, a key gap persists: MoE
models typically require complicated training with both the main forecasting
loss and auxiliary load-balancing losses, along with careful
routing/temperature tuning, which hinders practical adoption. In this paper, we
propose a model architecture that simplifies the training process for
univariate time series forecasting and effectively addresses both long- and
short-term horizons, including intermittent patterns. Our approach combines
sparse MoE computation with a novel attention-inspired gating mechanism that
replaces the traditional one-layer softmax router. Through extensive empirical
evaluation, we demonstrate that our gating design naturally promotes balanced
expert utilization and achieves superior predictive accuracy without requiring
the auxiliary load-balancing losses typically used in classical MoE
implementations. The model achieves better performance while utilizing only a
fraction of the parameters required by state-of-the-art transformer models,
such as PatchTST. Furthermore, experiments across diverse datasets confirm that
our MoE architecture with the proposed gating mechanism is more computationally
efficient than LSTM for both long- and short-term forecasting, enabling
cost-effective inference. These results highlight the potential of our approach
for practical time-series forecasting applications where both accuracy and
computational efficiency are critical.

</details>


### [100] [TANDEM: Temporal Attention-guided Neural Differential Equations for Missingness in Time Series Classification](https://arxiv.org/abs/2508.17519)
*YongKyung Oh,Dong-Young Lim,Sungil Kim,Alex Bui*

Main category: cs.LG

TL;DR: TANDEM是一种基于注意力机制和神经微分方程的框架，用于处理时间序列分类中的缺失数据问题，在30个基准数据集和真实医疗数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 处理时间序列分类中的缺失数据是一个重要挑战，传统插补方法可能引入偏差或无法捕捉时间动态。

Method: 提出TANDEM框架，通过注意力机制整合原始观测值、插补控制路径和连续潜在动态，使用神经微分方程有效分类含缺失值的时间序列数据。

Result: 在30个基准数据集和真实医疗数据集上评估，证明TANDEM优于现有最先进方法，不仅提高了分类准确性，还提供了处理缺失数据的见解。

Conclusion: TANDEM是一个有价值的实用工具，能够有效处理时间序列分类中的缺失数据问题。

Abstract: Handling missing data in time series classification remains a significant
challenge in various domains. Traditional methods often rely on imputation,
which may introduce bias or fail to capture the underlying temporal dynamics.
In this paper, we propose TANDEM (Temporal Attention-guided Neural Differential
Equations for Missingness), an attention-guided neural differential equation
framework that effectively classifies time series data with missing values. Our
approach integrates raw observation, interpolated control path, and continuous
latent dynamics through a novel attention mechanism, allowing the model to
focus on the most informative aspects of the data. We evaluate TANDEM on 30
benchmark datasets and a real-world medical dataset, demonstrating its
superiority over existing state-of-the-art methods. Our framework not only
improves classification accuracy but also provides insights into the handling
of missing data, making it a valuable tool in practice.

</details>


### [101] [Modeling Irregular Astronomical Time Series with Neural Stochastic Delay Differential Equations](https://arxiv.org/abs/2508.17521)
*YongKyung Oh,Seungsu Kam,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: 提出了基于神经随机延迟微分方程的新框架，用于处理不规则采样的天文时间序列数据，提高分类和异常检测性能


<details>
  <summary>Details</summary>
Motivation: 大规模天文调查（如LSST）产生的时间序列数据往往不规则采样且不完整，给分类和异常检测带来挑战

Method: 结合随机建模和神经网络，采用延迟感知神经架构、SDDE数值求解器，以及从噪声稀疏序列中稳健学习的机制

Result: 在不规则采样的天文数据上实验显示，该方法具有强大的分类准确性和有效的新天体物理事件检测能力，即使在部分标签情况下也表现良好

Conclusion: 神经SDDEs为观测约束下的时间序列分析提供了一个原则性和实用的工具

Abstract: Astronomical time series from large-scale surveys like LSST are often
irregularly sampled and incomplete, posing challenges for classification and
anomaly detection. We introduce a new framework based on Neural Stochastic
Delay Differential Equations (Neural SDDEs) that combines stochastic modeling
with neural networks to capture delayed temporal dynamics and handle irregular
observations. Our approach integrates a delay-aware neural architecture, a
numerical solver for SDDEs, and mechanisms to robustly learn from noisy, sparse
sequences. Experiments on irregularly sampled astronomical data demonstrate
strong classification accuracy and effective detection of novel astrophysical
events, even with partial labels. This work highlights Neural SDDEs as a
principled and practical tool for time series analysis under observational
constraints.

</details>


### [102] [Gumbel-MPNN: Graph Rewiring with Gumbel-Softmax](https://arxiv.org/abs/2508.17531)
*Marcel Hoffmann,Lukas Galke,Ansgar Scherp*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Gumbel-Softmax的图重连方法，通过减少邻域分布的偏差来提升消息传递神经网络在节点分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统认为图同配性是MPNN性能的关键，但研究发现性能更依赖于邻域类别分布的一致性。现有方法虽然能提高邻域分布信息量，但未能改善MPNN性能。

Method: 提出Gumbel-Softmax重连方法，通过分解类的邻域分布组件并减少分布偏差，增强邻域信息量并处理长程依赖关系。

Result: 新方法显著提升了邻域信息量，缓解了过度压缩问题，并提高了MPNN的分类性能。

Conclusion: 基于Gumbel-Softmax的重连方法有效解决了MPNN在异配图上的性能瓶颈，为图神经网络提供了新的优化方向。

Abstract: Graph homophily has been considered an essential property for message-passing
neural networks (MPNN) in node classification. Recent findings suggest that
performance is more closely tied to the consistency of neighborhood class
distributions. We demonstrate that the MPNN performance depends on the number
of components of the overall neighborhood distribution within a class. By
breaking down the classes into their neighborhood distribution components, we
increase measures of neighborhood distribution informativeness but do not
observe an improvement in MPNN performance. We propose a Gumbel-Softmax-based
rewiring method that reduces deviations in neighborhood distributions. Our
results show that our new method enhances neighborhood informativeness, handles
long-range dependencies, mitigates oversquashing, and increases the
classification performance of the MPNN. The code is available at
https://github.com/Bobowner/Gumbel-Softmax-MPNN.

</details>


### [103] [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
*Andrzej Szablewski,Marek Masiak*

Main category: cs.LG

TL;DR: 提出了激活传输算子（ATO）方法，通过线性映射分析transformer残差流中特征在前向传播中的线性传输机制，能够区分特征是线性传输还是非线性合成。


<details>
  <summary>Details</summary>
Motivation: 理解transformer残差流中特征的流动机制对于提升模型安全性、错误检测和修正具有重要意义，但目前这一动态过程研究不足。

Method: 使用激活传输算子（ATO）作为上游到下游残差的线性映射，通过下游SAE解码器投影在特征空间中进行评估，计算传输效率并估计线性传输涉及的残差流子空间大小。

Result: 实证证明ATO能够确定特征是从先前层线性传输还是从非线性层计算合成，提供了传输效率的上界估计和线性传输子空间大小的量化结果。

Conclusion: 这种计算轻量（无需微调，<50 GPU小时）的方法为安全性、调试提供了实用工具，并更清晰地展示了LLM中计算行为在何处呈现线性特征。

Abstract: The residual stream mediates communication between transformer decoder layers
via linear reads and writes of non-linear computations. While sparse-dictionary
learning-based methods locate features in the residual stream, and activation
patching methods discover circuits within the model, the mechanism by which
features flow through the residual stream remains understudied. Understanding
this dynamic can better inform jailbreaking protections, enable early detection
of model mistakes, and their correction. In this work, we propose Activation
Transport Operators (ATO), linear maps from upstream to downstream residuals
$k$ layers later, evaluated in feature space using downstream SAE decoder
projections. We empirically demonstrate that these operators can determine
whether a feature has been linearly transported from a previous layer or
synthesised from non-linear layer computation. We develop the notion of
transport efficiency, for which we provide an upper bound, and use it to
estimate the size of the residual stream subspace that corresponds to linear
transport. We empirically demonstrate the linear transport, report transport
efficiency and the size of the residual stream's subspace involved in linear
transport. This compute-light (no finetuning, <50 GPU-h) method offers
practical tools for safety, debugging, and a clearer picture of where
computation in LLMs behaves linearly.

</details>


### [104] [In-Context Algorithm Emulation in Fixed-Weight Transformers](https://arxiv.org/abs/2508.17550)
*Jerry Yao-Chieh Hu,Hude Liu,Jennifer Yuntong Zhang,Han Liu*

Main category: cs.LG

TL;DR: 这篇论文证明了冻结参数的最小Transformer架构通过上下文提示能够模拟广泛类别的算法，包括梯度下降和线性回归等，仅需通过构造特定的提示字符串就可实现算法输出的高精度模拟。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型如何通过上下文学习实现算法模拟的机制，并证明现代Transformer模型具有仅仅通过提示就能切换不同算法的算法普适性。

Method: 构造特殊的提示字符串，将算法参数编码到token表示中，创造尖锐的点积求和间隔，使softmax attention强制按照预定计算流程运行。方法不需要feed-forward层和参数更新。

Result: 证明了两层softmax attention模块甚至单头attention层都能通过提示实现任意精度的算法模拟，达到了架构上的最小化。

Conclusion: 这些发现在上下文学习与算法模拟之间建立了直接联系，提供了大型Transformer作为可提示编程算法库的简单机制，明确了GPT风格基础模型仅通过提示就能切换算法的能力。

Abstract: We prove that a minimal Transformer architecture with frozen weights is
capable of emulating a broad class of algorithms by in-context prompting. In
particular, for any algorithm implementable by a fixed-weight attention head
(e.g. one-step gradient descent or linear/ridge regression), there exists a
prompt that drives a two-layer softmax attention module to reproduce the
algorithm's output with arbitrary precision. This guarantee extends even to a
single-head attention layer (using longer prompts if necessary), achieving
architectural minimality. Our key idea is to construct prompts that encode an
algorithm's parameters into token representations, creating sharp dot-product
gaps that force the softmax attention to follow the intended computation. This
construction requires no feed-forward layers and no parameter updates. All
adaptation happens through the prompt alone. These findings forge a direct link
between in-context learning and algorithmic emulation, and offer a simple
mechanism for large Transformers to serve as prompt-programmable libraries of
algorithms. They illuminate how GPT-style foundation models may swap algorithms
via prompts alone, establishing a form of algorithmic universality in modern
Transformer models.

</details>


### [105] [Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction](https://arxiv.org/abs/2508.17554)
*Shuqi Zi,Haitz Sáez de Ocáriz Borde,Emma Rocheteau,Pietro Lio'*

Main category: cs.LG

TL;DR: S²G-Net：结合状态空间序列建模和多视图图神经网络的新型神经网络架构，用于ICU住院时间预测，在MIMIC-IV数据集上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: ICU住院时间预测对医院资源管理至关重要，但由于电子健康记录(EHRs)的异构性和不规则采样特性，这一任务仍然具有挑战性

Method: 提出S²G-Net架构，包含两个路径：时间路径使用Mamba状态空间模型捕捉患者轨迹，图路径使用优化的GraphGPS骨干网络整合基于诊断、管理和语义特征构建的异构患者相似性图

Result: 在MIMIC-IV数据集上的实验表明，S²G-Net在所有主要指标上都优于序列模型、图模型和混合方法

Conclusion: S²G-Net为多模态临床数据的ICU住院时间预测提供了有效且可扩展的解决方案，消融研究和可解释性分析验证了各组件的重要性和图构建的重要性

Abstract: Predicting a patient's length of stay (LOS) in the intensive care unit (ICU)
is a critical task for hospital resource management, yet remains challenging
due to the heterogeneous and irregularly sampled nature of electronic health
records (EHRs). In this work, we propose S$^2$G-Net, a novel neural
architecture that unifies state-space sequence modeling with multi-view Graph
Neural Networks (GNNs) for ICU LOS prediction. The temporal path employs Mamba
state-space models (SSMs) to capture patient trajectories, while the graph path
leverages an optimized GraphGPS backbone, designed to integrate heterogeneous
patient similarity graphs derived from diagnostic, administrative, and semantic
features. Experiments on the large-scale MIMIC-IV cohort dataset show that
S$^2$G-Net consistently outperforms sequence models (BiLSTM, Mamba,
Transformer), graph models (classic GNNs, GraphGPS), and hybrid approaches
across all primary metrics. Extensive ablation studies and interpretability
analyses highlight the complementary contributions of each component of our
architecture and underscore the importance of principled graph construction.
These results demonstrate that S$^2$G-Net provides an effective and scalable
solution for ICU LOS prediction with multi-modal clinical data.

</details>


### [106] [Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA](https://arxiv.org/abs/2508.17586)
*Daniel Frees,Aditri Bhagirath,Moritz Bolling*

Main category: cs.LG

TL;DR: 本文在小型语言模型minBERT上验证了LoRA和DoRA高效微调方法的有效性，发现即使在小模型上梯度更新也具有低秩特性，并通过AMP等技术显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA和DoRA在大语言模型上表现出色，但其在小型语言模型上的效果尚未验证。本研究旨在探索这些高效微调方法在紧凑模型minBERT上的适用性和性能表现。

Method: 在minBERT模型上应用LoRA和DoRA方法，结合自动混合精度(AMP)技术，测试不同配置下的训练效率和性能。同时探索多种架构、自定义损失函数和超参数，最终训练多任务集成模型。

Result: 研究发现：1) LoRA和DoRA在小模型上同样有效，rank 1分解几乎不影响性能；2) 结合AMP可显著提升训练效率；3) 成功训练出能同时执行情感分析、复述检测和相似度评分的最优多任务集成模型。

Conclusion: 梯度更新的低秩特性不仅存在于大语言模型中，在小模型空间同样适用。LoRA和DoRA配合AMP技术为资源受限的研究团队提供了高效微调小型语言模型的可行方案。

Abstract: While Large Language Models (LLMs) have revolutionized artificial
intelligence, fine-tuning LLMs is extraordinarily computationally expensive,
preventing smaller businesses and research teams with limited GPU resources
from engaging with new research. Hu et al and Liu et al introduce Low-Rank
Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) as highly
efficient and performant solutions to the computational challenges of LLM
fine-tuning, demonstrating huge speedups and memory usage savings for models
such as GPT-3 and RoBERTa. We seek to expand upon the original LoRA and DoRA
papers by benchmarking efficiency and performance of LoRA and DoRA when applied
to a much smaller scale of language model: our case study here is the compact
minBERT model. Our findings reveal that optimal custom configurations of LoRA
and DoRA, coupled with Automatic Mixed Precision (AMP), significantly enhance
training efficiency without compromising performance. Furthermore, while the
parameterization of minBERT is significantly smaller than GPT-3, our results
validate the observation that gradient updates to language models are
inherently low-rank even in small model space, observing that rank 1
decompositions yield negligible performance deficits. Furthermore, aided by our
highly efficient minBERT implementation, we investigate numerous architectures,
custom loss functions, and hyperparameters to ultimately train an optimal
ensembled multitask minBERT model to simultaneously perform sentiment analysis,
paraphrase detection, and similarity scoring.

</details>


### [107] [ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning](https://arxiv.org/abs/2508.17608)
*Wentao Tan,Qiong Cao,Chao Xue,Yibing Zhan,Changxing Ding,Xiaodong He*

Main category: cs.LG

TL;DR: 提出了ReChartPrompt数据集和ChartSimRL强化学习算法，解决了图表到代码生成任务中的数据多样性不足和视觉一致性维护问题，ChartMaster模型在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 图表到代码生成任务面临两个主要挑战：数据多样性有限，以及训练过程中生成的图表与原始图表之间视觉一致性维护不足

Method: 1) 使用arXiv论文中的真实人类设计图表构建ReChartPrompt-240K大规模多样化数据集；2) 提出基于GRPO的ChartSimRL强化学习算法，使用包含属性相似性和视觉相似性的图表相似性奖励

Result: ChartMaster模型在7B参数模型中达到最先进结果，在多个图表到代码生成基准测试中甚至可与GPT-4o相媲美

Conclusion: 通过整合ReChartPrompt多样化数据集和ChartSimRL强化学习算法，有效解决了图表到代码生成任务的关键挑战，显著提升了模型性能

Abstract: The chart-to-code generation task requires MLLMs to convert chart images into
executable code. This task faces two major challenges: limited data diversity
and insufficient maintenance of visual consistency between generated and
original charts during training. Existing datasets mainly rely on seed data to
prompt GPT models for code generation, resulting in homogeneous samples. To
address this, we propose ReChartPrompt, which leverages real-world,
human-designed charts from arXiv papers as prompts instead of synthetic seeds.
Using the diverse styles and rich content of arXiv charts, we construct
ReChartPrompt-240K, a large-scale and highly diverse dataset. Another challenge
is that although SFT effectively improve code understanding, it often fails to
ensure that generated charts are visually consistent with the originals. To
address this, we propose ChartSimRL, a GRPO-based reinforcement learning
algorithm guided by a novel chart similarity reward. This reward consists of
attribute similarity, which measures the overlap of chart attributes such as
layout and color between the generated and original charts, and visual
similarity, which assesses similarity in texture and other overall visual
features using convolutional neural networks. Unlike traditional text-based
rewards such as accuracy or format rewards, our reward considers the multimodal
nature of the chart-to-code task and effectively enhances the model's ability
to accurately reproduce charts. By integrating ReChartPrompt and ChartSimRL, we
develop the ChartMaster model, which achieves state-of-the-art results among
7B-parameter models and even rivals GPT-4o on various chart-to-code generation
benchmarks. All resources are available at
https://github.com/WentaoTan/ChartMaster.

</details>


### [108] [A Proportional-Integral Controller-Incorporated SGD Algorithm for High Efficient Latent Factor Analysis](https://arxiv.org/abs/2508.17609)
*Jinli Li,Shiyu Long,Minglian Han*

Main category: cs.LG

TL;DR: 提出PILF模型，通过PI控制机制整合当前和历史信息来加速SGD算法，提升高维稀疏矩阵的特征提取性能


<details>
  <summary>Details</summary>
Motivation: 现有SGD-LFA方法仅依赖当前样本的瞬时梯度信息，缺乏历史迭代经验和样本间相关性考虑，导致收敛慢和泛化性能不佳

Method: 开发PI加速SGD算法，整合相关实例并通过比例-积分控制机制精炼学习误差，同时利用当前和历史信息

Result: 对比实验证明PILF模型在高维稀疏矩阵上具有优越的表征能力

Conclusion: PILF模型通过PI控制机制有效提升了SGD算法的收敛速度和泛化性能，在高维稀疏矩阵分析中表现优异

Abstract: In industrial big data scenarios, high-dimensional sparse matrices (HDI) are
widely used to characterize high-order interaction relationships among massive
nodes. The stochastic gradient descent-based latent factor analysis (SGD-LFA)
method can effectively extract deep feature information embedded in HDI
matrices. However, existing SGD-LFA methods exhibit significant limitations:
their parameter update process relies solely on the instantaneous gradient
information of current samples, failing to incorporate accumulated experiential
knowledge from historical iterations or account for intrinsic correlations
between samples, resulting in slow convergence speed and suboptimal
generalization performance. Thus, this paper proposes a PILF model by
developing a PI-accelerated SGD algorithm by integrating correlated instances
and refining learning errors through proportional-integral (PI) control
mechanism that current and historical information; Comparative experiments
demonstrate the superior representation capability of the PILF model on HDI
matrices

</details>


### [109] [Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning](https://arxiv.org/abs/2508.17630)
*An Ning,Tai Yue Li,Nan Yow Chen*

Main category: cs.LG

TL;DR: 量子图注意力网络（QGAT）将变分量子电路集成到注意力机制中，通过量子并行性同时生成多个注意力系数，显著降低计算开销和模型复杂度


<details>
  <summary>Details</summary>
Motivation: 将量子计算的优势引入图神经网络，利用量子并行性提高注意力机制的效率，同时增强模型对复杂结构依赖关系的捕捉能力和抗噪声鲁棒性

Method: 使用强纠缠量子电路和振幅编码的节点特征，通过单一量子电路同时生成多个注意力系数，实现参数共享。经典投影权重和量子电路参数端到端联合优化

Result: 实验证明QGAT能有效捕捉复杂结构依赖关系，在归纳场景中具有更好的泛化能力，量子嵌入增强了对抗特征和结构噪声的鲁棒性

Conclusion: QGAT展示了量子增强学习在化学、生物和网络分析等领域的潜力，其模块化设计便于集成到现有架构中，为量子-经典混合模型提供了可行方案

Abstract: We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neural
network that integrates variational quantum circuits into the attention
mechanism. At its core, QGAT employs strongly entangling quantum circuits with
amplitude-encoded node features to enable expressive nonlinear interactions.
Distinct from classical multi-head attention that separately computes each
head, QGAT leverages a single quantum circuit to simultaneously generate
multiple attention coefficients. This quantum parallelism facilitates parameter
sharing across heads, substantially reducing computational overhead and model
complexity. Classical projection weights and quantum circuit parameters are
optimized jointly in an end-to-end manner, ensuring flexible adaptation to
learning tasks. Empirical results demonstrate QGAT's effectiveness in capturing
complex structural dependencies and improved generalization in inductive
scenarios, highlighting its potential for scalable quantum-enhanced learning
across domains such as chemistry, biology, and network analysis. Furthermore,
experiments confirm that quantum embedding enhances robustness against feature
and structural noise, suggesting advantages in handling real-world noisy data.
The modularity of QGAT also ensures straightforward integration into existing
architectures, allowing it to easily augment classical attention-based models.

</details>


### [110] [ControlEchoSynth: Boosting Ejection Fraction Estimation Models via Controlled Video Diffusion](https://arxiv.org/abs/2508.17631)
*Nima Kondori,Hanwen Liang,Hooman Vaseli,Bingyu Xie,Christina Luong,Purang Abolmaesumi,Teresa Tsang,Renjie Liao*

Main category: cs.LG

TL;DR: 该研究提出了一种基于条件生成模型的合成超声心动图视图方法，通过生成合成数据来增强机器学习模型在射血分数估计方面的性能，特别是在数据获取困难的医学影像领域。


<details>
  <summary>Details</summary>
Motivation: 超声心动图数据的获取和标注存在挑战，特别是在POCUS环境中，由于操作者经验水平不同和可用视图数量有限，影响了心脏评估的准确性。传统方法在射血分数估计方面存在局限性。

Method: 使用条件生成模型，基于现有的真实心脏视图合成生成超声心动图视图，重点关注从双平面心尖视图测量射血分数这一关键参数。

Result: 初步结果表明，合成超声心动图数据在增强现有数据集时，不仅提高了射血分数估计的准确性，还显示出开发更稳健、准确和临床相关机器学习模型的潜力。

Conclusion: 该方法在提高临床诊断准确性方面表现出良好前景，预计将推动合成数据在医学影像诊断中的进一步研究和创新应用。

Abstract: Synthetic data generation represents a significant advancement in boosting
the performance of machine learning (ML) models, particularly in fields where
data acquisition is challenging, such as echocardiography. The acquisition and
labeling of echocardiograms (echo) for heart assessment, crucial in
point-of-care ultrasound (POCUS) settings, often encounter limitations due to
the restricted number of echo views available, typically captured by operators
with varying levels of experience. This study proposes a novel approach for
enhancing clinical diagnosis accuracy by synthetically generating echo views.
These views are conditioned on existing, real views of the heart, focusing
specifically on the estimation of ejection fraction (EF), a critical parameter
traditionally measured from biplane apical views. By integrating a conditional
generative model, we demonstrate an improvement in EF estimation accuracy,
providing a comparative analysis with traditional methods. Preliminary results
indicate that our synthetic echoes, when used to augment existing datasets, not
only enhance EF estimation but also show potential in advancing the development
of more robust, accurate, and clinically relevant ML models. This approach is
anticipated to catalyze further research in synthetic data applications, paving
the way for innovative solutions in medical imaging diagnostics.

</details>


### [111] [Longitudinal Progression Prediction of Alzheimer's Disease with Tabular Foundation Model](https://arxiv.org/abs/2508.17649)
*Yilang Ding,Jiawen Ren,Jiaying Lu,Gloria Hyunjung Kwak,Armin Iraji,Alex Fedorov*

Main category: cs.LG

TL;DR: L2C-TabPFN方法通过纵向到横截面转换和预训练表格基础模型，在阿尔茨海默病预测中实现了竞争性性能，特别是在脑室体积预测方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病具有多因素病因和多模态临床数据复杂性，准确预测临床相关生物标志物对疾病进展监测至关重要。

Method: 整合纵向到横截面(L2C)转换和预训练表格基础模型(TabPFN)，将序列患者记录转换为固定长度特征向量，用于预测诊断、认知评分和脑室体积。

Result: 在诊断和认知结果预测上具有竞争性性能，在脑室体积预测方面达到最先进水平，该成像生物标志物反映了阿尔茨海默病的神经退行性变和进展。

Conclusion: 表格基础模型在推进阿尔茨海默病临床相关成像标志物的纵向预测方面具有潜力。

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that remains
challenging to predict due to its multifactorial etiology and the complexity of
multimodal clinical data. Accurate forecasting of clinically relevant
biomarkers, including diagnostic and quantitative measures, is essential for
effective monitoring of disease progression. This work introduces L2C-TabPFN, a
method that integrates a longitudinal-to-cross-sectional (L2C) transformation
with a pre-trained Tabular Foundation Model (TabPFN) to predict Alzheimer's
disease outcomes using the TADPOLE dataset. L2C-TabPFN converts sequential
patient records into fixed-length feature vectors, enabling robust prediction
of diagnosis, cognitive scores, and ventricular volume. Experimental results
demonstrate that, while L2C-TabPFN achieves competitive performance on
diagnostic and cognitive outcomes, it provides state-of-the-art results in
ventricular volume prediction. This key imaging biomarker reflects
neurodegeneration and progression in Alzheimer's disease. These findings
highlight the potential of tabular foundational models for advancing
longitudinal prediction of clinically relevant imaging markers in Alzheimer's
disease.

</details>


### [112] [Heterogeneous co-occurrence embedding for visual information exploration](https://arxiv.org/abs/2508.17663)
*Takuro Ishida,Tetsuo Furukawa*

Main category: cs.LG

TL;DR: 提出了一种用于共现数据可视化的嵌入方法，将异质域元素映射到二维潜在空间，通过最大化互信息来保持依赖结构，支持多域分析和交互式可视化


<details>
  <summary>Details</summary>
Motivation: 处理异质域间的共现概率数据，需要可视化展示不对称关系，传统方法难以有效呈现这种跨域依赖结构

Method: 基于互信息最大化的嵌入方法，将异质元素映射到二维潜在空间，使用总相关性扩展到多域情况，并通过条件概率着色实现交互式可视化

Result: 在形容词-名词、NeurIPS数据集和主谓宾数据集上验证了方法的有效性，成功展示了域内和域间分析能力

Conclusion: 该方法能有效可视化异质域间的共现关系，通过互信息保持原始依赖结构，为信息探索提供了有力的可视化工具

Abstract: This paper proposes an embedding method for co-occurrence data aimed at
visual information exploration. We consider cases where co-occurrence
probabilities are measured between pairs of elements from heterogeneous
domains. The proposed method maps these heterogeneous elements into
corresponding two-dimensional latent spaces, enabling visualization of
asymmetric relationships between the domains. The key idea is to embed the
elements in a way that maximizes their mutual information, thereby preserving
the original dependency structure as much as possible. This approach can be
naturally extended to cases involving three or more domains, using a
generalization of mutual information known as total correlation. For
inter-domain analysis, we also propose a visualization method that assigns
colors to the latent spaces based on conditional probabilities, allowing users
to explore asymmetric relationships interactively. We demonstrate the utility
of the method through applications to an adjective-noun dataset, the NeurIPS
dataset, and a subject-verb-object dataset, showcasing both intra- and
inter-domain analysis.

</details>


### [113] [Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models](https://arxiv.org/abs/2508.17675)
*Victoria Yan,Honor Chotkowski,Fengran Wang,Alex Fedorov*

Main category: cs.LG

TL;DR: 使用多模态大语言模型（GPT-4o和GPT-4o-mini）通过高级提示策略生成认知测试的合成规范数据，替代传统耗时费力的数据收集方法。


<details>
  <summary>Details</summary>
Motivation: 认知评估需要规范数据作为基准，但传统数据收集方法成本高、耗时长且更新不及时，限制了新认知测试的开发。

Method: 采用两种提示策略（基础指令的朴素提示和上下文指导的高级提示），使用GPT-4o和GPT-4o-mini模型生成"Cookie Theft"图片描述任务的合成文本响应，并通过嵌入分析、BLEU、ROUGE、BERTScore和LLM-as-a-judge评估性能。

Result: 高级提示策略生成的响应能更好地区分诊断组和捕捉人口统计多样性，BERTScore是最可靠的上下文相似性评估指标，BLEU对创意输出评估效果较差。

Conclusion: 生成式多模态LLM通过精炼提示方法可以可行地生成稳健的合成规范数据，为开发新型图像认知评估奠定基础，克服传统限制。

Abstract: Cognitive assessments require normative data as essential benchmarks for
evaluating individual performance. Hence, developing new cognitive tests based
on novel image stimuli is challenging due to the lack of readily available
normative data. Traditional data collection methods are costly, time-consuming,
and infrequently updated, limiting their practical utility. Recent advancements
in generative multimodal large language models (MLLMs) offer a new approach to
generate synthetic normative data from existing cognitive test images. We
investigated the feasibility of using MLLMs, specifically GPT-4o and
GPT-4o-mini, to synthesize normative textual responses for established
image-based cognitive assessments, such as the "Cookie Theft" picture
description task. Two distinct prompting strategies-naive prompts with basic
instructions and advanced prompts enriched with contextual guidance-were
evaluated. Responses were analyzed using embeddings to assess their capacity to
distinguish diagnostic groups and demographic variations. Performance metrics
included BLEU, ROUGE, BERTScore, and an LLM-as-a-judge evaluation. Advanced
prompting strategies produced synthetic responses that more effectively
distinguished between diagnostic groups and captured demographic diversity
compared to naive prompts. Superior models generated responses exhibiting
higher realism and diversity. BERTScore emerged as the most reliable metric for
contextual similarity assessment, while BLEU was less effective for evaluating
creative outputs. The LLM-as-a-judge approach provided promising preliminary
validation results. Our study demonstrates that generative multimodal LLMs,
guided by refined prompting methods, can feasibly generate robust synthetic
normative data for existing cognitive tests, thereby laying the groundwork for
developing novel image-based cognitive assessments without the traditional
limitations.

</details>


### [114] [TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training](https://arxiv.org/abs/2508.17677)
*Yifan Wang,Binbin Liu,Fengze Liu,Yuanfan Guo,Jiyao Deng,Xuecheng Wu,Weidong Zhou,Xiaohuan Zhou,Taifeng Wang*

Main category: cs.LG

TL;DR: TiKMiX是一种动态调整语言模型预训练数据混合比例的方法，通过Group Influence指标评估数据域影响，相比静态混合策略显著提升性能并减少计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 静态数据混合策略在语言模型预训练中不是最优的，因为模型对不同数据域的学习偏好会随着训练动态变化，但高效观测这些变化偏好存在挑战

Method: 提出Group Influence指标评估数据域对模型的影响，将数据混合问题转化为寻找最优影响最大化分布的问题。开发了TiKMiX-D（直接优化）和TiKMiX-M（回归模型预测）两种方法

Result: TiKMiX-D仅用20%计算资源就超越了REGMIX等先进方法；TiKMiX-M在9个下游基准测试中平均性能提升2%；模型在1万亿token上训练验证了有效性

Conclusion: 模型的数据偏好随训练进度和规模动态演变，基于Group Influence动态调整数据混合比例能显著提升性能，缓解静态比例导致的数据消化不足问题

Abstract: The data mixture used in the pre-training of a language model is a
cornerstone of its final performance. However, a static mixing strategy is
suboptimal, as the model's learning preferences for various data domains shift
dynamically throughout training. Crucially, observing these evolving
preferences in a computationally efficient manner remains a significant
challenge. To address this, we propose TiKMiX, a method that dynamically
adjusts the data mixture according to the model's evolving preferences. TiKMiX
introduces Group Influence, an efficient metric for evaluating the impact of
data domains on the model. This metric enables the formulation of the data
mixing problem as a search for an optimal, influence-maximizing distribution.
We solve this via two approaches: TiKMiX-D for direct optimization, and
TiKMiX-M, which uses a regression model to predict a superior mixture. We
trained models with different numbers of parameters, on up to 1 trillion
tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like
REGMIX while using just 20% of the computational resources. TiKMiX-M leads to
an average performance gain of 2% across 9 downstream benchmarks. Our
experiments reveal that a model's data preferences evolve with training
progress and scale, and we demonstrate that dynamically adjusting the data
mixture based on Group Influence, a direct measure of these preferences,
significantly improves performance by mitigating the underdigestion of data
seen with static ratios.

</details>


### [115] [Robustness Feature Adapter for Efficient Adversarial Training](https://arxiv.org/abs/2508.17680)
*Quanwei Wu,Jun Guo,Wei Wang,Yi Wang*

Main category: cs.LG

TL;DR: 通过插件基控制的特征空间对抗训练，解决了阻塞性计算开销和突出的健壁过拟合问题，提高效率并改善模型精度。


<details>
  <summary>Details</summary>
Motivation: 对大型基础模型进行对抗性训练时计算开销过大，且存在健壁过拟合问题，需要同时解决这两个挑战以建立更可信的基础模型。

Method: 提出了一种新的基于插件的方法，直接在特征空间进行高效的对抗性训练。该方法通过插件控制来消除健壁过拟合现象。

Result: 插件基方法显著提高了计算效率，改善了内循环收敛质量，并能够将对抗性通用化到未见攻击上，从而提高了模型的准确性。

Conclusion: 新的插件基对抗性训练方法在不同的基础网络架构和大规模训练中都表现出有效性，为建立更可信的基础模型提供了重要技术支撑。

Abstract: Adversarial training (AT) with projected gradient descent is the most popular
method to improve model robustness under adversarial attacks. However,
computational overheads become prohibitively large when AT is applied to large
backbone models. AT is also known to have the issue of robust overfitting. This
paper contributes to solving both problems simultaneously towards building more
trustworthy foundation models. In particular, we propose a new adapter-based
approach for efficient AT directly in the feature space. We show that the
proposed adapter-based approach can improve the inner-loop convergence quality
by eliminating robust overfitting. As a result, it significantly increases
computational efficiency and improves model accuracy by generalizing
adversarial robustness to unseen attacks. We demonstrate the effectiveness of
the new adapter-based approach in different backbone architectures and in AT at
scale.

</details>


### [116] [Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery](https://arxiv.org/abs/2508.17681)
*Robert Yang*

Main category: cs.LG

TL;DR: 本文提出了"反向学习作为剩除法"来测试大语言模型是否真正能够生成新知识，而不仅仅是重组记忆的知识片段。


<details>
  <summary>Details</summary>
Motivation: 当前对AI在科学发现中作用的过分宣传需要一种可验证的方法来区分模型是否真正具备构造性知识创造能力。

Method: 通过系统性移除目标结果及其忘记闭包（包括引理、讽义和多跳含义），然后评估模型是否能从允许的公理和工具重新推导出该结果。

Result: 成功重新推导表明模型具有真正的生成能力，失败则曝露了当前的限制。这种方法可以作为新一代的测试标准。

Conclusion: 反吐学习作为剩除法提供了一个有理论基础的框架，用于追踪和评估AI在科学发现中的真实能力和限制。

Abstract: Bold claims about AI's role in science-from "AGI will cure all diseases" to
promises of radically accelerated discovery-raise a central epistemic question:
do large language models (LLMs) truly generate new knowledge, or do they merely
remix memorized fragments? We propose unlearning-as-ablation as a falsifiable
test of constructive scientific discovery. The method systematically removes a
target result and its entire forget-closure (lemmas, paraphrases, and multi-hop
entailments) and then evaluates whether the model can re-derive the result from
only permitted axioms and tools. Success provides evidence for genuine
generative capability; failure exposes current limits. Unlike prevailing
motivations for unlearning-privacy, copyright, or safety-our framing
repositions it as an epistemic probe for AI-for-Science. We argue that such
tests could serve as the next generation of benchmarks, much as ImageNet
catalyzed progress in vision: distinguishing models that can merely recall from
those that can constructively generate new scientific knowledge. We outline a
minimal pilot in mathematics and algorithms, and discuss extensions to physics,
chemistry, and biology. Whether models succeed or fail, unlearning-as-ablation
provides a principled framework to map the true reach and limits of AI
scientific discovery. This is a position paper: we advance a conceptual and
methodological argument rather than new empirical results.

</details>


### [117] [On the Edge of Memorization in Diffusion Models](https://arxiv.org/abs/2508.17689)
*Sam Buchanan,Druv Pai,Yi Ma,Valentin De Bortoli*

Main category: cs.LG

TL;DR: 本文研究了扩散模型中记忆化与泛化的临界点，提出了一个理论框架来预测模型何时会记忆训练数据而非泛化，并通过实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型何时记忆训练数据、何时能够泛化生成新样本，对于解决版权侵权和数据隐私等实际问题具有重要意义。

Method: 建立了一个科学的数学"实验室"，在合成和自然图像数据上训练扩散模型，理论分析了记忆化模型与泛化模型的训练损失差异，并通过精心设计的实验验证理论预测。

Result: 理论预测了一个临界点，当模型参数化程度低于该临界值时，记忆化行为占主导地位。实验验证了这一相变现象，理论能够准确预测记忆化开始占优的模型大小。

Conclusion: 该研究提供了一个可分析处理且具有实际意义的理论框架，为未来扩散模型中记忆化与泛化现象的理论和实证研究奠定了基础。

Abstract: When do diffusion models reproduce their training data, and when are they
able to generate samples beyond it? A practically relevant theoretical
understanding of this interplay between memorization and generalization may
significantly impact real-world deployments of diffusion models with respect to
issues such as copyright infringement and data privacy. In this work, to
disentangle the different factors that influence memorization and
generalization in practical diffusion models, we introduce a scientific and
mathematical "laboratory" for investigating these phenomena in diffusion models
trained on fully synthetic or natural image-like structured data. Within this
setting, we hypothesize that the memorization or generalization behavior of an
underparameterized trained model is determined by the difference in training
loss between an associated memorizing model and a generalizing model. To probe
this hypothesis, we theoretically characterize a crossover point wherein the
weighted training loss of a fully generalizing model becomes greater than that
of an underparameterized memorizing model at a critical value of model
(under)parameterization. We then demonstrate via carefully-designed experiments
that the location of this crossover predicts a phase transition in diffusion
models trained via gradient descent, validating our hypothesis. Ultimately, our
theory enables us to analytically predict the model size at which memorization
becomes predominant. Our work provides an analytically tractable and
practically meaningful setting for future theoretical and empirical
investigations. Code for our experiments is available at
https://github.com/DruvPai/diffusion_mem_gen.

</details>


### [118] [Rethinking Federated Learning Over the Air: The Blessing of Scaling Up](https://arxiv.org/abs/2508.17697)
*Jiaqi Zhu,Bikramjit Das,Yong Xie,Nikolaos Pappas,Howard H. Yang*

Main category: cs.LG

TL;DR: 本文分析了空中计算联邦学习在大规模客户端场景下的性能，发现增加客户端数量可以增强隐私保护、减轻信道衰落影响并改善收敛性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时进行协作模型训练，但通信资源有限制约了其性能，特别是在支持大量客户端的系统中。空中计算通过模拟信号传输可以缓解通信瓶颈，但会带来信道失真问题。

Method: 开发了一个理论框架来分析大规模客户端场景下空中联邦学习的性能，并通过广泛的实验评估验证理论见解。

Result: 分析揭示了扩大参与客户端数量的三个关键优势：增强隐私保护（客户端本地梯度与服务器聚合梯度之间的互信息减少）、减轻信道衰落影响（信道硬化效应消除小尺度衰落）、改善收敛性能（降低热噪声和梯度估计误差）。

Conclusion: 空中计算模型训练是联邦学习在大规模客户端网络中可行的解决方案，理论分析和实验验证都支持这一结论。

Abstract: Federated learning facilitates collaborative model training across multiple
clients while preserving data privacy. However, its performance is often
constrained by limited communication resources, particularly in systems
supporting a large number of clients. To address this challenge, integrating
over-the-air computations into the training process has emerged as a promising
solution to alleviate communication bottlenecks. The system significantly
increases the number of clients it can support in each communication round by
transmitting intermediate parameters via analog signals rather than digital
ones. This improvement, however, comes at the cost of channel-induced
distortions, such as fading and noise, which affect the aggregated global
parameters. To elucidate these effects, this paper develops a theoretical
framework to analyze the performance of over-the-air federated learning in
large-scale client scenarios. Our analysis reveals three key advantages of
scaling up the number of participating clients: (1) Enhanced Privacy: The
mutual information between a client's local gradient and the server's
aggregated gradient diminishes, effectively reducing privacy leakage. (2)
Mitigation of Channel Fading: The channel hardening effect eliminates the
impact of small-scale fading in the noisy global gradient. (3) Improved
Convergence: Reduced thermal noise and gradient estimation errors benefit the
convergence rate. These findings solidify over-the-air model training as a
viable approach for federated learning in networks with a large number of
clients. The theoretical insights are further substantiated through extensive
experimental evaluations.

</details>


### [119] [Adaptive Ensemble Learning with Gaussian Copula for Load Forecasting](https://arxiv.org/abs/2508.17700)
*Junying Yang,Gang Lu,Xiaoqing Yan,Peng Xia,Di Wu*

Main category: cs.LG

TL;DR: 提出基于高斯Copula的自适应集成学习模型处理稀疏数据下的负荷预测问题，通过数据补全、多模型预测和自适应集成三模块提升预测鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机器学习在完整数据下能准确进行负荷预测，但实际数据收集存在不确定性导致数据稀疏，需要解决稀疏数据下的预测挑战

Method: 采用三模块框架：1)高斯Copula数据补全消除稀疏性 2)五种机器学习模型分别预测 3)自适应集成获得加权最终结果

Result: 实验证明该模型具有鲁棒性，能有效处理稀疏数据下的负荷预测问题

Conclusion: 自适应集成学习结合高斯Copula的方法能有效解决数据稀疏问题，提升负荷预测的准确性和稳定性

Abstract: Machine learning (ML) is capable of accurate Load Forecasting from complete
data. However, there are many uncertainties that affect data collection,
leading to sparsity. This article proposed a model called Adaptive Ensemble
Learning with Gaussian Copula to deal with sparsity, which contains three
modules: data complementation, ML construction, and adaptive ensemble. First,
it applies Gaussian Copula to eliminate sparsity. Then, we utilise five ML
models to make predictions individually. Finally, it employs adaptive ensemble
to get final weighted-sum result. Experiments have demonstrated that our model
are robust.

</details>


### [120] [Copyright Protection for 3D Molecular Structures with Watermarking](https://arxiv.org/abs/2508.17702)
*Runwen Hu,Peilin Chen,Keyan Ding,Shiqi Wang*

Main category: cs.LG

TL;DR: 提出了首个针对分子的鲁棒水印方法，利用原子级特征保持分子完整性，通过不变特征确保对仿射变换的鲁棒性，在保持90%以上基本属性的同时实现95%以上的水印准确率。


<details>
  <summary>Details</summary>
Motivation: 人工智能在分子生成领域的革命性进展加速了发现过程，但同时也带来了知识产权保护的关键问题，需要开发有效的水印技术来保护分子知识产权。

Method: 利用原子级特征来保持分子完整性，采用不变特征确保对仿射变换的鲁棒性，在QM9和GEOM-DRUG数据集上使用GeoBFN和GeoLDM生成模型进行验证。

Result: 水印准确率超过95.00%，基本属性保持率高于90.00%，下游对接模拟显示原始分子和水印分子性能相当，结合亲和力达到-6.00 kcal/mol，均方根偏差低于1.602 Å。

Conclusion: 该水印技术能有效保护分子知识产权而不影响科学实用性，为分子发现和研究应用中安全、负责任的人工智能集成提供了保障。

Abstract: Artificial intelligence (AI) revolutionizes molecule generation in
bioengineering and biological research, significantly accelerating discovery
processes. However, this advancement introduces critical concerns regarding
intellectual property protection. To address these challenges, we propose the
first robust watermarking method designed for molecules, which utilizes
atom-level features to preserve molecular integrity and invariant features to
ensure robustness against affine transformations. Comprehensive experiments
validate the effectiveness of our method using the datasets QM9 and GEOM-DRUG,
and generative models GeoBFN and GeoLDM. We demonstrate the feasibility of
embedding watermarks, maintaining basic properties higher than 90.00\% while
achieving watermark accuracy greater than 95.00\%. Furthermore, downstream
docking simulations reveal comparable performance between original and
watermarked molecules, with binding affinities reaching -6.00 kcal/mol and root
mean square deviations below 1.602 \AA. These results confirm that our
watermarking technique effectively safeguards molecular intellectual property
without compromising scientific utility, enabling secure and responsible AI
integration in molecular discovery and research applications.

</details>


### [121] [Speculative Safety-Aware Decoding](https://arxiv.org/abs/2508.17739)
*Xuekang Wang,Shengyu Zhu,Xueqi Cheng*

Main category: cs.LG

TL;DR: SSD是一种轻量级解码时方法，通过推测采样和小模型集成，为LLMs提供安全属性防御越狱攻击，同时加速推理


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量对齐LLMs与人类价值观的努力，但越狱攻击仍不断出现，需要在不影响性能的情况下增强模型安全性

Method: 使用推测采样技术，集成具备安全属性的小模型，通过匹配比例量化越狱风险，动态切换解码方案，从原始模型和小模型的组合分布中采样输出

Result: 实验结果显示SSD成功为大型模型赋予所需安全属性，保持对良性查询的有用性，同时加速推理时间

Conclusion: SSD提供了一种有效的解码时安全增强方案，既能防御越狱攻击，又能保持模型效用和加速推理

Abstract: Despite extensive efforts to align Large Language Models (LLMs) with human
values and safety rules, jailbreak attacks that exploit certain vulnerabilities
continuously emerge, highlighting the need to strengthen existing LLMs with
additional safety properties to defend against these attacks. However, tuning
large models has become increasingly resource-intensive and may have difficulty
ensuring consistent performance. We introduce Speculative Safety-Aware Decoding
(SSD), a lightweight decoding-time approach that equips LLMs with the desired
safety property while accelerating inference. We assume that there exists a
small language model that possesses this desired property. SSD integrates
speculative sampling during decoding and leverages the match ratio between the
small and composite models to quantify jailbreak risks. This enables SSD to
dynamically switch between decoding schemes to prioritize utility or safety, to
handle the challenge of different model capacities. The output token is then
sampled from a new distribution that combines the distributions of the original
and the small models. Experimental results show that SSD successfully equips
the large model with the desired safety property, and also allows the model to
remain helpful to benign queries. Furthermore, SSD accelerates the inference
time, thanks to the speculative sampling design.

</details>


### [122] [Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks](https://arxiv.org/abs/2508.17744)
*Sotaro Takeshita,Yurina Takeshita,Daniel Ruffinelli,Simone Paolo Ponzetto*

Main category: cs.LG

TL;DR: 研究发现，随机截断文本嵌入向量中高达50%的维度只会导致下游任务性能轻微下降（<10%），这种现象在6个最先进文本编码器和26个下游任务中一致存在。


<details>
  <summary>Details</summary>
Motivation: 探索截断文本嵌入维度对下游性能的影响，以及这种现象背后的原因，特别是在先前研究认为这是表示空间利用效率低下的情况下。

Method: 使用6个state-of-the-art文本编码器和26个下游任务进行实验，随机移除不同比例的嵌入维度，并分析性能变化。同时研究大语言模型中嵌入截断对生成任务的影响。

Result: 截断50%嵌入维度仅导致性能下降不到10%。发现大量均匀分布的维度在被移除时反而会提升性能，这与先前研究结论相反。在生成任务中也观察到类似现象。

Conclusion: 文本嵌入中存在大量冗余维度，截断这些维度不会显著影响性能，这为模型压缩和效率优化提供了重要启示，且这种现象在不同任务类型中普遍存在。

Abstract: In this paper, we study the surprising impact that truncating text embeddings
has on downstream performance. We consistently observe across 6
state-of-the-art text encoders and 26 downstream tasks, that randomly removing
up to 50% of embedding dimensions results in only a minor drop in performance,
less than 10%, in retrieval and classification tasks. Given the benefits of
using smaller-sized embeddings, as well as the potential insights about text
encoding, we study this phenomenon and find that, contrary to what is suggested
in prior work, this is not the result of an ineffective use of representation
space. Instead, we find that a large number of uniformly distributed dimensions
actually cause an increase in performance when removed. This would explain why,
on average, removing a large number of embedding dimensions results in a
marginal drop in performance. We make similar observations when truncating the
embeddings used by large language models to make next-token predictions on
generative tasks, suggesting that this phenomenon is not isolated to
classification or retrieval tasks.

</details>


### [123] [Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2508.17751)
*Alessio Arcudi,Davide Sartor,Alberto Sinigaglia,Vincent François-Lavet,Gian Antonio Susto*

Main category: cs.LG

TL;DR: MANGO是一个新颖的分层强化学习框架，通过多层抽象和嵌套选项来解决长期稀疏奖励环境中的挑战，提高了样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务中长期稀疏奖励环境的挑战，提高强化学习在样本效率和泛化能力方面的表现。

Method: 将复杂任务分解为多层抽象，每层定义抽象状态空间并使用选项将轨迹模块化为宏动作，通过嵌套选项实现学习动作的高效复用，引入层内策略指导智能体在抽象状态空间中的转移。

Result: 在程序生成的网格环境中实验表明，相比标准RL方法，MANGO在样本效率和泛化能力方面都有显著提升，同时增强了决策过程的可解释性。

Conclusion: MANGO框架有效解决了长期稀疏奖励问题，未来工作将探索自动发现抽象和抽象动作、适应连续或模糊环境以及更鲁棒的多层训练策略。

Abstract: This paper introduces MANGO (Multilayer Abstraction for Nested Generation of
Options), a novel hierarchical reinforcement learning framework designed to
address the challenges of long-term sparse reward environments. MANGO
decomposes complex tasks into multiple layers of abstraction, where each layer
defines an abstract state space and employs options to modularize trajectories
into macro-actions. These options are nested across layers, allowing for
efficient reuse of learned movements and improved sample efficiency. The
framework introduces intra-layer policies that guide the agent's transitions
within the abstract state space, and task actions that integrate task-specific
components such as reward functions. Experiments conducted in
procedurally-generated grid environments demonstrate substantial improvements
in both sample efficiency and generalization capabilities compared to standard
RL methods. MANGO also enhances interpretability by making the agent's
decision-making process transparent across layers, which is particularly
valuable in safety-critical and industrial applications. Future work will
explore automated discovery of abstractions and abstract actions, adaptation to
continuous or fuzzy environments, and more robust multi-layer training
strategies.

</details>


### [124] [SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling](https://arxiv.org/abs/2508.17756)
*Fanjiang Ye,Zepeng Zhao,Yi Mu,Jucheng Shen,Renjie Li,Kaijian Wang,Desen Sun,Saurabh Agarwal,Myungjin Lee,Triston Cao,Aditya Akella,Arvind Krishnamurthy,T. S. Eugene Ng,Zhengzhong Tu,Yuke Wang*

Main category: cs.LG

TL;DR: SuperGen是一个基于分块的训练免费框架，用于超高清视频生成，通过分块技术、自适应缓存策略和并行处理，显著降低内存和计算成本，支持多种分辨率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但对超高清视频（如2K/4K）的生成仍面临计算和内存成本过高的问题，需要高效的解决方案。

Method: 采用分块技术，无需重新训练；引入自适应区域感知缓存策略，利用去噪步骤和空间区域的冗余性；集成缓存引导的通信最小化分块并行处理。

Result: 评估显示SuperGen在保持高输出质量的同时，实现了最大性能增益，显著减少了内存占用和计算复杂度。

Conclusion: SuperGen通过创新的分块框架和缓存策略，有效解决了超高清视频生成中的计算和内存挑战，为高质量内容生成提供了高效解决方案。

Abstract: Diffusion models have recently achieved remarkable success in generative
tasks (e.g., image and video generation), and the demand for high-quality
content (e.g., 2K/4K videos) is rapidly increasing across various domains.
However, generating ultra-high-resolution videos on existing
standard-resolution (e.g., 720p) platforms remains challenging due to the
excessive re-training requirements and prohibitively high computational and
memory costs. To this end, we introduce SuperGen, an efficient tile-based
framework for ultra-high-resolution video generation. SuperGen features a novel
training-free algorithmic innovation with tiling to successfully support a wide
range of resolutions without additional training efforts while significantly
reducing both memory footprint and computational complexity. Moreover, SuperGen
incorporates a tile-tailored, adaptive, region-aware caching strategy that
accelerates video generation by exploiting redundancy across denoising steps
and spatial regions. SuperGen also integrates cache-guided,
communication-minimized tile parallelism for enhanced throughput and minimized
latency. Evaluations demonstrate that SuperGen harvests the maximum performance
gains while achieving high output quality across various benchmarks.

</details>


### [125] [Evaluating the Quality of the Quantified Uncertainty for (Re)Calibration of Data-Driven Regression Models](https://arxiv.org/abs/2508.17761)
*Jelke Wibbeke,Nico Schönfisch,Sebastian Rohjans,Andreas Rauh*

Main category: cs.LG

TL;DR: 本文系统分析了回归校准指标的不一致性问题，发现不同指标对同一校准结果的评估经常产生冲突甚至矛盾结论，并推荐了ENCE和CWC作为最可靠的指标。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，数据驱动模型不仅需要准确，还必须提供可靠的uncertainty估计（校准）。然而现有的回归校准指标在定义、假设和尺度上差异很大，难以跨研究比较结果，且大多数重新校准方法仅使用少量指标评估，无法确定改进是否适用于不同的校准概念。

Method: 从文献中系统提取和分类回归校准指标，独立于特定建模方法或重新校准方法对这些指标进行基准测试。通过使用真实世界数据、合成数据和人工错误校准数据进行受控实验。

Result: 实验表明校准指标经常产生冲突结果：许多指标对同一重新校准结果的评估存在分歧，有些甚至指示矛盾的结论。这种不一致性特别令人担忧，因为它可能允许选择指标来制造误导性的成功印象。

Conclusion: 研究发现ENCE（期望归一化校准误差）和CWC（覆盖宽度准则）是测试中最可靠的指标。强调了指标选择在校准研究中的关键作用，指标不一致问题可能影响风险感知决策的可靠性。

Abstract: In safety-critical applications data-driven models must not only be accurate
but also provide reliable uncertainty estimates. This property, commonly
referred to as calibration, is essential for risk-aware decision-making. In
regression a wide variety of calibration metrics and recalibration methods have
emerged. However, these metrics differ significantly in their definitions,
assumptions and scales, making it difficult to interpret and compare results
across studies. Moreover, most recalibration methods have been evaluated using
only a small subset of metrics, leaving it unclear whether improvements
generalize across different notions of calibration. In this work, we
systematically extract and categorize regression calibration metrics from the
literature and benchmark these metrics independently of specific modelling
methods or recalibration approaches. Through controlled experiments with
real-world, synthetic and artificially miscalibrated data, we demonstrate that
calibration metrics frequently produce conflicting results. Our analysis
reveals substantial inconsistencies: many metrics disagree in their evaluation
of the same recalibration result, and some even indicate contradictory
conclusions. This inconsistency is particularly concerning as it potentially
allows cherry-picking of metrics to create misleading impressions of success.
We identify the Expected Normalized Calibration Error (ENCE) and the Coverage
Width-based Criterion (CWC) as the most dependable metrics in our tests. Our
findings highlight the critical role of metric selection in calibration
research.

</details>


### [126] [Puzzle: Scheduling Multiple Deep Learning Models on Mobile Device with Heterogeneous Processors](https://arxiv.org/abs/2508.17764)
*Duseok Kang,Yunseong Lee,Junghoon Kim*

Main category: cs.LG

TL;DR: 通过基因算法将多个深度学习模型分割成子图并在异构处理器上调度，提高移动设备请求处理频率


<details>
  <summary>Details</summary>
Motivation: 解决移动设备深度学习加速器异构性带来的调度挑战，充分利用多模型场景下的硬件资源

Method: 使用基因算法，包含三种不同类型的柯序（分割/映射/优先级），采用设备实时分析进行准确的执行时间估计

Result: 在9个最新网络模型的随机场景中，Puzzle系统满足实时要求的同时，请求处理频率比NPU Only和Best Mapping基准分别高出3.7倍和2.2倍

Conclusion: 该方法有效解决了多模型异构处理器调度问题，显著提升了移动设备的深度学习工作负载处理能力

Abstract: As deep learning models are increasingly deployed on mobile devices, modern
mobile devices incorporate deep learning-specific accelerators to handle the
growing computational demands, thus increasing their hardware heterogeneity.
However, existing works on scheduling deep learning workloads across these
processors have significant limitations: most studies focus on single-model
scenarios rather than realistic multi-model scenarios, overlook performance
variations from different hardware/software configurations, and struggle with
accurate execution time estimation. To address these challenges, we propose a
novel genetic algorithm-based methodology for scheduling multiple deep learning
networks on heterogeneous processors by partitioning the networks into multiple
subgraphs. Our approach incorporates three different types of chromosomes for
partition/mapping/priority exploration, and leverages device-in-the-loop
profiling and evaluation for accurate execution time estimation. Based on this
methodology, our system, Puzzle, demonstrates superior performance in extensive
evaluations with randomly generated scenarios involving nine state-of-the-art
networks. The results demonstrate Puzzle can support 3.7 and 2.2 times higher
request frequency on average compared to the two heuristic baselines, NPU Only
and Best Mapping, respectively, while satisfying the equivalent level of
real-time requirements.

</details>


### [127] [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
*Wenhong Zhu,Ruobing Xie,Rui Wang,Xingwu Sun,Di Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: 提出了Proximal SFT (PSFT)方法，通过引入信任区域约束来防止监督微调中的策略漂移，提高模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统监督微调(SFT)会导致基础模型泛化能力下降，原有能力在新任务或领域上退化

Method: 受强化学习中TRPO和PPO启发，将SFT视为具有恒定正优势的策略梯度方法特例，提出PSFT目标函数来约束策略漂移

Result: 在数学和人类价值观领域的实验显示，PSFT在域内性能与SFT相当，域外泛化更优，训练稳定性更好，不会导致熵崩溃，为后续优化提供更强基础

Conclusion: PSFT通过信任区域约束有效解决了SFT的泛化问题，同时保持竞争力，为后续训练阶段留出优化空间

Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor
generalization, where prior capabilities deteriorate after tuning on new tasks
or domains. Inspired by trust-region policy optimization (TRPO) and proximal
policy optimization (PPO) in reinforcement learning (RL), we propose Proximal
SFT (PSFT). This fine-tuning objective incorporates the benefits of
trust-region, effectively constraining policy drift during SFT while
maintaining competitive tuning. By viewing SFT as a special case of policy
gradient methods with constant positive advantages, we derive PSFT that
stabilizes optimization and leads to generalization, while leaving room for
further optimization in subsequent post-training stages. Experiments across
mathematical and human-value domains show that PSFT matches SFT in-domain,
outperforms it in out-of-domain generalization, remains stable under prolonged
training without causing entropy collapse, and provides a stronger foundation
for the subsequent optimization.

</details>


### [128] [Multi-domain Distribution Learning for De Novo Drug Design](https://arxiv.org/abs/2508.17815)
*Arne Schneuing,Ilia Igashov,Adrian W. Dobbelstein,Thomas Castiglione,Michael Bronstein,Bruno Correia*

Main category: cs.LG

TL;DR: DrugFlow是一个基于结构的药物设计生成模型，结合连续流匹配和离散马尔可夫桥，在3D蛋白质-配体数据学习方面达到最先进性能，并提供不确定性估计和偏好对齐采样机制。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时学习化学、几何和物理特性的3D蛋白质-配体生成模型，并解决传统方法在不确定性估计和采样优化方面的不足。

Method: 整合连续流匹配和离散马尔可夫桥方法，引入不确定性估计机制检测分布外样本，提出联合偏好对齐方案优化采样过程，并扩展模型以联合采样蛋白质侧链角度和分子构象。

Result: 在3D蛋白质-配体数据学习方面实现了state-of-the-art性能，能够有效检测分布外样本，并通过偏好对齐提升采样质量。

Conclusion: DrugFlow提供了一个强大的基于结构的药物设计框架，通过创新的流匹配和马尔可夫桥整合，以及不确定性估计和偏好对齐机制，显著提升了生成模型在药物发现中的应用价值。

Abstract: We introduce DrugFlow, a generative model for structure-based drug design
that integrates continuous flow matching with discrete Markov bridges,
demonstrating state-of-the-art performance in learning chemical, geometric, and
physical aspects of three-dimensional protein-ligand data. We endow DrugFlow
with an uncertainty estimate that is able to detect out-of-distribution
samples. To further enhance the sampling process towards distribution regions
with desirable metric values, we propose a joint preference alignment scheme
applicable to both flow matching and Markov bridge frameworks. Furthermore, we
extend our model to also explore the conformational landscape of the protein by
jointly sampling side chain angles and molecules.

</details>


### [129] [Limitations of Normalization in Attention Mechanism](https://arxiv.org/abs/2508.17821)
*Timur Mudarisov,Mikhail Burtsev,Tatiana Petrova,Radu State*

Main category: cs.LG

TL;DR: 本文分析了注意力机制中softmax归一化的局限性，发现随着选择token数量增加，模型区分信息性token的能力下降，梯度敏感性在低温设置下带来训练挑战。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制中归一化处理的局限性，特别是softmax缩放对token选择能力和几何分离的影响，为改进注意力架构提供理论基础。

Method: 建立理论框架分析token选择能力和几何分离，通过预训练GPT-2模型进行实验验证，包括距离边界和分离标准的定量分析。

Result: 随着选择token数量增加，模型区分信息性token的能力显著下降，趋向均匀选择模式；softmax归一化在低温设置下导致梯度敏感性训练挑战。

Conclusion: 研究揭示了softmax注意力机制的局限性，强调了未来需要开发更鲁棒的归一化和选择策略来改进注意力架构。

Abstract: This paper investigates the limitations of the normalization in attention
mechanisms. We begin with a theoretical framework that enables the
identification of the model's selective ability and the geometric separation
involved in token selection. Our analysis includes explicit bounds on distances
and separation criteria for token vectors under softmax scaling. Through
experiments with pre-trained GPT-2 model, we empirically validate our
theoretical results and analyze key behaviors of the attention mechanism.
Notably, we demonstrate that as the number of selected tokens increases, the
model's ability to distinguish informative tokens declines, often converging
toward a uniform selection pattern. We also show that gradient sensitivity
under softmax normalization presents challenges during training, especially at
low temperature settings. These findings advance current understanding of
softmax-based attention mechanism and motivate the need for more robust
normalization and selection strategies in future attention architectures.

</details>


### [130] [Limits of message passing for node classification: How class-bottlenecks restrict signal-to-noise ratio](https://arxiv.org/abs/2508.17822)
*Jonathan Rubin,Sahil Loomba,Nick S. Jones*

Main category: cs.LG

TL;DR: 本文提出了一个统计框架来分析MPNN在异质性图上的性能限制，通过信噪比分解模型性能，证明了高阶同质性对性能的影响，并提出了BRIDGE图重连算法来优化图结构。


<details>
  <summary>Details</summary>
Motivation: 消息传递神经网络(MPNN)在节点分类任务中表现出色，但在异质性图(低同类连接性)和结构瓶颈下性能受限，需要理论框架来理解这些限制并提出改进方法。

Method: 建立统一的统计框架，通过信噪比(SNR)分解MPNN表示性能，分析高阶同质性与结构瓶颈的关系，提出基于图集分析的BRIDGE重连算法来优化图结构。

Result: 理论证明最优图结构是单类和双类二分簇的并集，BRIDGE算法在合成基准测试中达到近乎完美的分类准确率，在真实世界基准测试中显著改进性能，特别是在MPNN通常表现不佳的'中等同质性陷阱'区域。

Conclusion: 该框架提供了评估MPNN性能的诊断工具和通过原则性图修改增强性能的有效方法，BRIDGE算法超越了现有重连技术，代码已公开供使用。

Abstract: Message passing neural networks (MPNNs) are powerful models for node
classification but suffer from performance limitations under heterophily (low
same-class connectivity) and structural bottlenecks in the graph. We provide a
unifying statistical framework exposing the relationship between heterophily
and bottlenecks through the signal-to-noise ratio (SNR) of MPNN
representations. The SNR decomposes model performance into feature-dependent
parameters and feature-independent sensitivities. We prove that the sensitivity
to class-wise signals is bounded by higher-order homophily -- a generalisation
of classical homophily to multi-hop neighbourhoods -- and show that low
higher-order homophily manifests locally as the interaction between structural
bottlenecks and class labels (class-bottlenecks). Through analysis of graph
ensembles, we provide a further quantitative decomposition of bottlenecking
into underreaching (lack of depth implying signals cannot arrive) and
oversquashing (lack of breadth implying signals arriving on fewer paths) with
closed-form expressions. We prove that optimal graph structures for maximising
higher-order homophily are disjoint unions of single-class and
two-class-bipartite clusters. This yields BRIDGE, a graph ensemble-based
rewiring algorithm that achieves near-perfect classification accuracy across
all homophily regimes on synthetic benchmarks and significant improvements on
real-world benchmarks, by eliminating the ``mid-homophily pitfall'' where MPNNs
typically struggle, surpassing current standard rewiring techniques from the
literature. Our framework, whose code we make available for public use,
provides both diagnostic tools for assessing MPNN performance, and simple yet
effective methods for enhancing performance through principled graph
modification.

</details>


### [131] [Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs](https://arxiv.org/abs/2508.17850)
*Han Zhang,Ruibin Zheng,Zexuan Yi,Hanyang Peng,Hui Wang,Yue Yu*

Main category: cs.LG

TL;DR: HeteroRL是一个异步强化学习架构，通过解耦采样和学习来解决分布式环境中网络延迟导致的KL散度问题，GEPO方法实现了指数级方差降低，在1800秒延迟下性能下降小于3%


<details>
  <summary>Details</summary>
Motivation: 随着单中心计算面临功耗限制，去中心化训练变得至关重要。强化学习后训练能增强大语言模型，但在异构分布式环境中由于紧密耦合的采样-学习交替而面临挑战

Method: 提出HeteroRL异步RL架构，将rollout采样与参数学习解耦；提出Group Expectation Policy Optimization (GEPO)方法，通过改进的采样机制降低重要性权重方差

Result: 理论证明GEPO实现指数级方差降低；实验显示相比GRPO等方法保持优越稳定性，在1800秒延迟下性能下降小于3%

Conclusion: HeteroRL和GEPO展示了在异构网络中部署去中心化强化学习的强大潜力，能够有效应对网络延迟带来的挑战

Abstract: As single-center computing approaches power constraints, decentralized
training is becoming essential. Reinforcement Learning (RL) post-training
enhances Large Language Models (LLMs) but faces challenges in heterogeneous
distributed environments due to its tightly-coupled sampling-learning
alternation. We propose HeteroRL, an asynchronous RL architecture that
decouples rollout sampling from parameter learning, enabling robust deployment
across geographically distributed nodes under network delays. We identify that
latency-induced KL divergence causes importance sampling failure due to high
variance. To address this, we propose Group Expectation Policy Optimization
(GEPO), which reduces importance weight variance through a refined sampling
mechanism. Theoretically, GEPO achieves exponential variance reduction.
Experiments show it maintains superior stability over methods like GRPO, with
less than 3% performance degradation under 1800-second delays, demonstrating
strong potential for decentralized RL in heterogeneous networks.

</details>


### [132] [Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks](https://arxiv.org/abs/2508.17867)
*Dan Wang,Feng Jiang,Zhanquan Wang*

Main category: cs.LG

TL;DR: 提出了Ada-TransGNN模型，结合Transformer和图卷积网络，通过自适应图结构学习和辅助任务学习模块，提高了空气质量预测的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有空气质量预测模型存在预测精度低、实时更新慢的问题，导致预测结果滞后，需要开发更准确高效的时空预测方法。

Method: 使用多头注意力机制和图卷积网络构建时空块集，提出自适应图结构学习模块和辅助任务学习模块，动态提取时空依赖特征并优化图结构表示。

Result: 在基准数据集和新数据集(Mete-air)上的评估表明，该模型在短期和长期预测中都优于现有最先进的预测模型。

Conclusion: Ada-TransGNN模型通过整合全局空间语义和时间行为，有效提升了空气质量预测的准确性和实时性能，为解决环境监测中的预测滞后问题提供了有效方案。

Abstract: Accurate air quality prediction is becoming increasingly important in the
environmental field. To address issues such as low prediction accuracy and slow
real-time updates in existing models, which lead to lagging prediction results,
we propose a Transformer-based spatiotemporal data prediction method
(Ada-TransGNN) that integrates global spatial semantics and temporal behavior.
The model constructs an efficient and collaborative spatiotemporal block set
comprising a multi-head attention mechanism and a graph convolutional network
to extract dynamically changing spatiotemporal dependency features from complex
air quality monitoring data. Considering the interaction relationships between
different monitoring points, we propose an adaptive graph structure learning
module, which combines spatiotemporal dependency features in a data-driven
manner to learn the optimal graph structure, thereby more accurately capturing
the spatial relationships between monitoring points. Additionally, we design an
auxiliary task learning module that enhances the decoding capability of
temporal relationships by integrating spatial context information into the
optimal graph structure representation, effectively improving the accuracy of
prediction results. We conducted comprehensive evaluations on a benchmark
dataset and a novel dataset (Mete-air). The results demonstrate that our model
outperforms existing state-of-the-art prediction models in short-term and
long-term predictions.

</details>


### [133] [Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering](https://arxiv.org/abs/2508.17872)
*Yanghao Qin,Bo Zhou,Guangliang Pan,Qihui Wu,Meixia Tao*

Main category: cs.LG

TL;DR: SFFP框架通过分数傅里叶变换、自适应滤波和复值神经网络，有效分离频谱数据中的可预测趋势和噪声，显著提升频谱预测精度


<details>
  <summary>Details</summary>
Motivation: 现有基于时域或频域的方法难以有效分离频谱数据中的可预测模式和噪声，影响动态频谱接入和资源分配的准确性

Method: 提出SFFP三阶段框架：1) 自适应分数傅里叶变换模块将数据转换到合适分数域；2) 自适应滤波模块选择性抑制噪声；3) 复值神经网络预测模块学习过滤后的趋势成分

Result: 在真实频谱数据上的实验表明，SFFP优于主流频谱预测和通用预测方法

Conclusion: SFFP框架通过创新的分数域处理和自适应滤波技术，成功解决了频谱数据中模式与噪声分离的挑战，为动态频谱接入提供了更准确的预测能力

Abstract: Accurate spectrum prediction is crucial for dynamic spectrum access (DSA) and
resource allocation. However, due to the unique characteristics of spectrum
data, existing methods based on the time or frequency domain often struggle to
separate predictable patterns from noise. To address this, we propose the
Spectral Fractional Filtering and Prediction (SFFP) framework. SFFP first
employs an adaptive fractional Fourier transform (FrFT) module to transform
spectrum data into a suitable fractional Fourier domain, enhancing the
separability of predictable trends from noise. Subsequently, an adaptive Filter
module selectively suppresses noise while preserving critical predictive
features within this domain. Finally, a prediction module, leveraging a
complex-valued neural network, learns and forecasts these filtered trend
components. Experiments on real-world spectrum data show that the SFFP
outperforms leading spectrum and general forecasting methods.

</details>


### [134] [Riemannian Optimization for LoRA on the Stiefel Manifold](https://arxiv.org/abs/2508.17901)
*Juneyoung Park,Minjae Kang,Seongbae Lee,Haegang Lee,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 通过Stiefel流形优化LoRA的B矩阵，实现正交约束，显著提升参数效率和表示能力，在多个基准测试中超越AdamW


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调面临参数效率挑战，现有PEFT方法如LoRA存在优化器效率问题，特别是B矩阵的基础冗余限制了性能

Method: 在Stiefel流形上优化LoRA的B矩阵，施加显式正交约束，实现近乎完美的正交性和完整的有效秩

Result: Stiefel优化器在LoRA和DoRA的各种基准测试中一致优于AdamW，显著提升了参数效率和表示容量

Conclusion: 几何约束是释放LoRA在LLM微调中全部潜力的关键，Stiefel流形优化提供了有效的解决方案

Abstract: While powerful, large language models (LLMs) present significant fine-tuning
challenges due to their size. Parameter-efficient fine-tuning (PEFT) methods
like LoRA provide solutions, yet suffer from critical optimizer inefficiencies;
notably basis redundancy in LoRA's $B$ matrix when using AdamW, which
fundamentally limits performance. We address this by optimizing the $B$ matrix
on the Stiefel manifold, imposing explicit orthogonality constraints that
achieve near-perfect orthogonality and full effective rank. This geometric
approach dramatically enhances parameter efficiency and representational
capacity. Our Stiefel optimizer consistently outperforms AdamW across
benchmarks with both LoRA and DoRA, demonstrating that geometric constraints
are the key to unlocking LoRA's full potential for effective LLM fine-tuning.

</details>


### [135] [Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets](https://arxiv.org/abs/2508.17930)
*Sarina Penquitt,Tobias Riedlinger,Timo Heller,Markus Reischl,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出了一种统一的学习方法来检测目标检测、语义分割和实例分割数据集中的标签错误，通过注入标签错误并将其构建为实例分割问题，在多个任务和数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前标签错误检测方法通常只针对单一计算机视觉任务，且不是基于学习的方法，这限制了其通用性和效果。错误标注数据会导致模型性能下降、基准测试结果偏差等问题。

Method: 通过向真实标签中注入不同类型的标签错误，将标签错误检测问题构建为基于复合输入的实例分割问题，实现跨任务的统一检测方法。

Result: 在多个任务、数据集和基础模型上的模拟标签错误检测实验中，该方法优于各种基线和最先进方法，并在真实标签错误上具有良好的泛化能力。

Conclusion: 该方法提供了一个统一的框架来检测多种计算机视觉任务中的标签错误，提高了标签质量检测的通用性和有效性，并发布了Cityscapes数据集中的459个真实标签错误作为基准。

Abstract: Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.

</details>


### [136] [Choice Outweighs Effort: Facilitating Complementary Knowledge Fusion in Federated Learning via Re-calibration and Merit-discrimination](https://arxiv.org/abs/2508.17954)
*Ming Yang,Dongrun Li,Xin Wang,Xiaoyang Yu,Xiaoming Wu,Shibo He*

Main category: cs.LG

TL;DR: FedMate通过双边优化解决联邦学习中的数据异构性问题：服务端构建动态全局原型并微调分类器保持全局一致性，客户端采用互补分类融合和成本感知特征传输，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中跨客户端数据异构性导致偏见，阻碍无偏共识凝聚和泛化-个性化知识的互补融合。现有方法依赖静态指标和刚性全局对齐，导致共识扭曲和模型适应性下降。

Method: 双边优化方法：服务端构建动态全局原型（聚合权重基于样本量、当前参数和未来预测校准），微调类别分类器；客户端采用互补分类融合进行择优判别训练，并引入成本感知特征传输平衡性能与通信效率。

Result: 在五个不同复杂度数据集上的实验表明，FedMate在协调泛化和适应性方面优于最先进方法。自动驾驶数据集上的语义分割实验验证了方法的实际可扩展性。

Conclusion: FedMate有效解决了联邦学习中的数据异构性问题，通过动态原型构建和互补融合机制，在保持全局一致性的同时提升了模型的适应性和通信效率。

Abstract: Cross-client data heterogeneity in federated learning induces biases that
impede unbiased consensus condensation and the complementary fusion of
generalization- and personalization-oriented knowledge. While existing
approaches mitigate heterogeneity through model decoupling and representation
center loss, they often rely on static and restricted metrics to evaluate local
knowledge and adopt global alignment too rigidly, leading to consensus
distortion and diminished model adaptability. To address these limitations, we
propose FedMate, a method that implements bilateral optimization: On the server
side, we construct a dynamic global prototype, with aggregation weights
calibrated by holistic integration of sample size, current parameters, and
future prediction; a category-wise classifier is then fine-tuned using this
prototype to preserve global consistency. On the client side, we introduce
complementary classification fusion to enable merit-based discrimination
training and incorporate cost-aware feature transmission to balance model
performance and communication efficiency. Experiments on five datasets of
varying complexity demonstrate that FedMate outperforms state-of-the-art
methods in harmonizing generalization and adaptation. Additionally, semantic
segmentation experiments on autonomous driving datasets validate the method's
real-world scalability.

</details>


### [137] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出了一种生成式特征补全框架，通过空间误差集中打包、扩散模型特征重建和语义感知功率分配，提升语义通信在传输错误下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语义通信在6G网络中具有高效传输优势，但在数字系统中面临传输错误导致语义内容失真的挑战，需要确保关键语义内容的鲁棒性。

Method: 采用三阶段方法：1）空间误差集中打包策略；2）基于扩散模型的生成式特征补全；3）语义感知的功率分配方案，根据语义重要性提供不等错误保护。

Result: 实验结果显示，在块衰落条件下，该框架在语义准确性和LPIPS分数方面优于传统的DJSCC和JPEG2000方法。

Conclusion: 所提出的生成式特征补全框架有效解决了语义通信中的传输错误问题，显著提升了系统的鲁棒性和性能表现。

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for
achieving unprecedented communication efficiency in sixth-generation (6G)
networks by leveraging artificial intelligence (AI) to extract and transmit the
underlying meanings of source data. However, deploying SemCom over digital
systems presents new challenges, particularly in ensuring robustness against
transmission errors that may distort semantically critical content. To address
this issue, this paper proposes a novel framework, termed generative feature
imputing, which comprises three key techniques. First, we introduce a spatial
error concentration packetization strategy that spatially concentrates feature
distortions by encoding feature elements based on their channel mappings, a
property crucial for both the effectiveness and reduced complexity of the
subsequent techniques. Second, building on this strategy, we propose a
generative feature imputing method that utilizes a diffusion model to
efficiently reconstruct missing features caused by packet losses. Finally, we
develop a semantic-aware power allocation scheme that enables unequal error
protection by allocating transmission power according to the semantic
importance of each packet. Experimental results demonstrate that the proposed
framework outperforms conventional approaches, such as Deep Joint
Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,
achieving higher semantic accuracy and lower Learned Perceptual Image Patch
Similarity (LPIPS) scores.

</details>


### [138] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的拓扑感知时间变化标量场插值方法，利用持续图和时间关键帧来估计缺失的非关键帧数据


<details>
  <summary>Details</summary>
Motivation: 传统插值方法无法有效处理时间变化标量场的拓扑结构变化，需要一种能够保持拓扑一致性的插值方案

Method: 使用神经网络架构学习时间值与对应标量场的关系，通过拓扑损失函数利用输入持续图来改善几何和拓扑重建质量

Result: 在2D和3D时间变化数据集上的实验表明，该方法在数据和拓扑拟合方面优于参考插值方案

Conclusion: 该方法能够实时生成高质量的插值结果，通过单次网络传播即可产生输出，在保持拓扑一致性方面表现优异

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [139] [A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond](https://arxiv.org/abs/2508.18001)
*Sebastian G. Gruber*

Main category: cs.LG

TL;DR: 本博士论文提出了一种基于合适分数的机器学习不确定性量化框架，通过函数Bregman散度实现偏差-方差分解，并将核分数应用于生成模型评估，在大语言模型不确定性估计方面超越了现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化是可信任机器学习应用的关键，但现有方法多为问题特定的，无法轻松转移到不同任务。需要一种通用的框架来统一处理回归、分类和生成模型中的不确定性量化问题。

Method: 使用合适分数作为损失函数，通过函数Bregman散度实现严格合适分数的偏差-方差分解。采用核分数评估样本基于生成模型，并将棵定性-锐度分解推广到分类以外的任务，定义合适棵定性误差。

Result: 在大语言模型不确定性估计方面超越了现有最优基准方法。提出了合适棵定性误差的新估计器，以及比较平方棵定性误差估计器的风险基方法。还对核球面分数进行了分解，实现了生成图像模型更细粒度和可解释性的评估。

Conclusion: 该研究提供了一个通用的不确定性量化框架，通过合适分数将认知不确定性、偶然不确定性和模型棵定联系起来。该框架在多个领域都取得了超越现有方法的性能，为可靠机器学习提供了重要技术支撑。

Abstract: In this PhD thesis, we propose a novel framework for uncertainty
quantification in machine learning, which is based on proper scores.
Uncertainty quantification is an important cornerstone for trustworthy and
reliable machine learning applications in practice. Usually, approaches to
uncertainty quantification are problem-specific, and solutions and insights
cannot be readily transferred from one task to another. Proper scores are loss
functions minimized by predicting the target distribution. Due to their very
general definition, proper scores apply to regression, classification, or even
generative modeling tasks. We contribute several theoretical results, that
connect epistemic uncertainty, aleatoric uncertainty, and model calibration
with proper scores, resulting in a general and widely applicable framework. We
achieve this by introducing a general bias-variance decomposition for strictly
proper scores via functional Bregman divergences. Specifically, we use the
kernel score, a kernel-based proper score, for evaluating sample-based
generative models in various domains, like image, audio, and natural language
generation. This includes a novel approach for uncertainty estimation of large
language models, which outperforms state-of-the-art baselines. Further, we
generalize the calibration-sharpness decomposition beyond classification, which
motivates the definition of proper calibration errors. We then introduce a
novel estimator for proper calibration errors in classification, and a novel
risk-based approach to compare different estimators for squared calibration
errors. Last, we offer a decomposition of the kernel spherical score, another
kernel-based proper score, allowing a more fine-grained and interpretable
evaluation of generative image models.

</details>


### [140] [Does simple trump complex? Comparing strategies for adversarial robustness in DNNs](https://arxiv.org/abs/2508.18019)
*William Brooks,Marelie H. Davel,Coenraad Mouton*

Main category: cs.LG

TL;DR: 本研究通过分析两种对抗训练方法（简单损失函数修改和复杂的Dynamics-Aware方法）的各个组件，识别出对提升深度神经网络对抗鲁棒性最有效的元素，特别关注输入空间中的边界距离。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在多种应用中表现出色，但对对抗攻击仍然脆弱。需要识别对抗训练技术中哪些组件最能提高对抗鲁棒性，特别是通过输入空间边界距离的视角。

Method: 使用VGG-16模型作为基础，系统性地分离和评估两种对抗训练方法的各个组件：简单的损失函数修改方法和复杂的Dynamics-Aware鲁棒训练方法。在CIFAR-10数据集上测试各种对抗攻击（包括AutoAttack和PGD）下的性能。

Result: 分析揭示了哪些元素最能有效增强对抗鲁棒性，为设计更鲁棒的深度神经网络提供了见解。

Conclusion: 通过组件级别的分析，确定了对抗训练方法中最关键的鲁棒性增强元素，为未来鲁棒神经网络设计提供了重要指导。

Abstract: Deep Neural Networks (DNNs) have shown substantial success in various
applications but remain vulnerable to adversarial attacks. This study aims to
identify and isolate the components of two different adversarial training
techniques that contribute most to increased adversarial robustness,
particularly through the lens of margins in the input space -- the minimal
distance between data points and decision boundaries. Specifically, we compare
two methods that maximize margins: a simple approach which modifies the loss
function to increase an approximation of the margin, and a more complex
state-of-the-art method (Dynamics-Aware Robust Training) which builds upon this
approach. Using a VGG-16 model as our base, we systematically isolate and
evaluate individual components from these methods to determine their relative
impact on adversarial robustness. We assess the effect of each component on the
model's performance under various adversarial attacks, including AutoAttack and
Projected Gradient Descent (PGD). Our analysis on the CIFAR-10 dataset reveals
which elements most effectively enhance adversarial robustness, providing
insights for designing more robust DNNs.

</details>


### [141] [Enhancing Differentially Private Linear Regression via Public Second-Moment](https://arxiv.org/abs/2508.18037)
*Zilong Cao,Hai Zhang*

Main category: cs.LG

TL;DR: 提出一种基于公共二阶矩矩阵变换的差分隐私线性回归方法，通过改进条件数来提升估计精度和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法仅基于私有数据添加噪声会显著降低效用，需要利用公共数据信息来增强差分隐私方法的实用性

Method: 使用公共二阶矩矩阵变换私有数据，计算变换后的充分统计量扰动OLS估计量，从而获得更好的条件数

Result: 理论误差界限分析显示该方法相比标准SSP-OLSE具有更好的鲁棒性和精度，在合成和真实数据集上的实验验证了方法的有效性

Conclusion: 通过利用公共数据信息进行数据变换，可以显著提升差分隐私线性回归估计的准确性和鲁棒性

Abstract: Leveraging information from public data has become increasingly crucial in
enhancing the utility of differentially private (DP) methods. Traditional DP
approaches often require adding noise based solely on private data, which can
significantly degrade utility. In this paper, we address this limitation in the
context of the ordinary least squares estimator (OLSE) of linear regression
based on sufficient statistics perturbation (SSP) under the unbounded data
assumption. We propose a novel method that involves transforming private data
using the public second-moment matrix to compute a transformed SSP-OLSE, whose
second-moment matrix yields a better condition number and improves the OLSE
accuracy and robustness. We derive theoretical error bounds about our method
and the standard SSP-OLSE to the non-DP OLSE, which reveal the improved
robustness and accuracy achieved by our approach. Experiments on synthetic and
real-world datasets demonstrate the utility and effectiveness of our method.

</details>


### [142] [Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation](https://arxiv.org/abs/2508.18045)
*Xiuheng Wang,Ricardo Borsoi,Arnaud Breloy,Cédric Richard*

Main category: cs.LG

TL;DR: 提出了一种基于鲁棒质心的流式时间序列非参数变点检测方法，通过比较Karcher均值与Huber函数定义的鲁棒质心来构建检测统计量，解决了传统方法对步长调优敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统流式时间序列变点检测方法在计算质心更新时需要仔细调优步长参数，这在实际应用中存在困难。针对黎曼流形上的数据，需要开发对估计方法不敏感的鲁棒检测方法。

Method: 利用M估计理论中的鲁棒质心概念，比较对变化敏感的经典Karcher均值与对变化鲁棒的Huber函数定义质心，构建检测统计量，并采用随机黎曼优化算法高效估计两种质心。

Result: 在模拟数据和真实世界数据上的实验表明，该方法在两个代表性流形上表现出优越的性能，对底层估计方法的敏感性降低。

Conclusion: 基于鲁棒质心比较的方法为流式时间序列变点检测提供了有效的解决方案，特别是在黎曼流形数据上表现出更好的鲁棒性和性能。

Abstract: Non-parametric change-point detection in streaming time series data is a
long-standing challenge in signal processing. Recent advancements in statistics
and machine learning have increasingly addressed this problem for data residing
on Riemannian manifolds. One prominent strategy involves monitoring abrupt
changes in the center of mass of the time series. Implemented in a streaming
fashion, this strategy, however, requires careful step size tuning when
computing the updates of the center of mass. In this paper, we propose to
leverage robust centroid on manifolds from M-estimation theory to address this
issue. Our proposal consists of comparing two centroid estimates: the classical
Karcher mean (sensitive to change) versus one defined from Huber's function
(robust to change). This comparison leads to the definition of a test statistic
whose performance is less sensitive to the underlying estimation method. We
propose a stochastic Riemannian optimization algorithm to estimate both robust
centroids efficiently. Experiments conducted on both simulated and real-world
data across two representative manifolds demonstrate the superior performance
of our proposed method.

</details>


### [143] [Training Transformers for Mesh-Based Simulations](https://arxiv.org/abs/2508.18051)
*Paul Garnier,Vincent Lannelongue,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: 基于注意力机制的图变换器架构，通过邻接矩阵作为注意力掩码，提出扩展感受野的新方法，在大规模网格物理模拟中实现了显著的等效性和可扩展性提升


<details>
  <summary>Details</summary>
Motivation: 解决传统消息传递图神经网络在处理大规模复杂网格时遇到的扩展性和效率挑战，包括多级网格方法和K-hop聚合等改进方法带来的复杂性问题

Method: 提出一种新的图变换器架构，利用邻接矩阵作为注意力掩码，采用扩张滑动窗口和全局注意力等创新增强技术来扩展感受野，同时保持计算效率

Result: 在30万节点和300万边的大规模网格上表现出艰图扩展性，最小模型在与MeshGraphNet性能相当的情况下速度提升7倍、模型大小减少6倍，最大模型在全滚出RMSE上超过之前SOTA方法38.8%，超过MeshGraphNet 52%，训练速度相似

Conclusion: 该图变换器架构为大规模网格物理模拟提供了高效、可扩展的解决方案，在保持计算效率的同时显著提升了模型性能，具有重要的应用价值

Abstract: Simulating physics using Graph Neural Networks (GNNs) is predominantly driven
by message-passing architectures, which face challenges in scaling and
efficiency, particularly in handling large, complex meshes. These architectures
have inspired numerous enhancements, including multigrid approaches and $K$-hop
aggregation (using neighbours of distance $K$), yet they often introduce
significant complexity and suffer from limited in-depth investigations. In
response to these challenges, we propose a novel Graph Transformer architecture
that leverages the adjacency matrix as an attention mask. The proposed approach
incorporates innovative augmentations, including Dilated Sliding Windows and
Global Attention, to extend receptive fields without sacrificing computational
efficiency. Through extensive experimentation, we evaluate model size,
adjacency matrix augmentations, positional encoding and $K$-hop configurations
using challenging 3D computational fluid dynamics (CFD) datasets. We also train
over 60 models to find a scaling law between training FLOPs and parameters. The
introduced models demonstrate remarkable scalability, performing on meshes with
up to 300k nodes and 3 million edges. Notably, the smallest model achieves
parity with MeshGraphNet while being $7\times$ faster and $6\times$ smaller.
The largest model surpasses the previous state-of-the-art by $38.8$\% on
average and outperforms MeshGraphNet by $52$\% on the all-rollout RMSE, while
having a similar training speed. Code and datasets are available at
https://github.com/DonsetPG/graph-physics.

</details>


### [144] [Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks](https://arxiv.org/abs/2508.18052)
*Silvia Beddar-Wiesing,Alice Moallemy-Oureh*

Main category: cs.LG

TL;DR: 本文扩展了图神经网络理论，从离散动态图扩展到连续时间动态图，提出了连续时间动态1-WL测试和对应的CGNN架构，保持了区分能力和通用逼近保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统（如通信网络、金融交易网络）通常以异步方式演化并可能分裂为断开连接的组件，而现有GNN理论仅限于离散动态属性图，无法处理连续时间动态图和任意连通性情况。

Method: 引入连续时间动态1-WL测试，证明其与连续时间动态展开树的等价性，基于离散动态GNN架构设计连续时间动态GNN（CGNN），使用分段连续可微时间函数处理异步断开图。

Result: 建立了连续时间动态图的等价理论框架，提出了具有区分能力和通用逼近保证的CGNN架构，为处理现实世界异步动态图提供了理论基础。

Conclusion: 成功将GNN理论扩展到连续时间动态图领域，提出的CGNN架构能够有效处理异步演化和断开连接的图结构，为实际应用提供了理论指导。

Abstract: Graph Neural Networks (GNNs) are known to match the distinguishing power of
the 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with
the unfolding tree equivalence classes of graphs. Preserving this equivalence,
GNNs can universally approximate any target function on graphs in probability
up to any precision. However, these results are limited to attributed
discrete-dynamic graphs represented as sequences of connected graph snapshots.
Real-world systems, such as communication networks, financial transaction
networks, and molecular interactions, evolve asynchronously and may split into
disconnected components. In this paper, we extend the theory of attributed
discrete-dynamic graphs to attributed continuous-time dynamic graphs with
arbitrary connectivity. To this end, we introduce a continuous-time dynamic
1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,
and identify a class of continuous-time dynamic GNNs (CGNNs) based on
discrete-dynamic GNN architectures that retain both distinguishing power and
universal approximation guarantees. Our constructive proofs further yield
practical design guidelines, emphasizing a compact and expressive CGNN
architecture with piece-wise continuously differentiable temporal functions to
process asynchronous, disconnected graphs.

</details>


### [145] [FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning](https://arxiv.org/abs/2508.18060)
*Emmanouil Kritharakis,Antonios Makris,Dusan Jakovetic,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: FedGreed是一种针对联邦学习的鲁棒聚合策略，通过基于服务器可信数据集评估客户端模型损失来排序和选择最优客户端，无需假设恶意客户端比例，在非IID数据分布和拜占庭攻击下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端可能表现出拜占庭攻击行为的问题，同时处理现实部署中普遍存在的异构（非IID）数据分布挑战。

Method: 提出FedGreed聚合策略：基于服务器可信数据集评估客户端本地模型更新的损失指标，按损失排序并贪婪选择损失最小的客户端子集进行聚合。

Result: 在MNIST、FMNIST和CIFAR-10数据集上的实验表明，FedGreed在标签翻转和高斯噪声注入等攻击场景下显著优于Mean、Trimmed Mean、Median、Krum和Multi-Krum等基线方法。

Conclusion: FedGreed具有收敛保证和有界最优性差距，在强对抗行为下能可靠运行，是处理联邦学习中拜占庭攻击的有效解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients while preserving data privacy by keeping local datasets on-device. In
this work, we address FL settings where clients may behave adversarially,
exhibiting Byzantine attacks, while the central server is trusted and equipped
with a reference dataset. We propose FedGreed, a resilient aggregation strategy
for federated learning that does not require any assumptions about the fraction
of adversarial participants. FedGreed orders clients' local model updates based
on their loss metrics evaluated against a trusted dataset on the server and
greedily selects a subset of clients whose models exhibit the minimal
evaluation loss. Unlike many existing approaches, our method is designed to
operate reliably under heterogeneous (non-IID) data distributions, which are
prevalent in real-world deployments. FedGreed exhibits convergence guarantees
and bounded optimality gaps under strong adversarial behavior. Experimental
evaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method
significantly outperforms standard and robust federated learning baselines,
such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of
adversarial scenarios considered, including label flipping and Gaussian noise
injection attacks. All experiments were conducted using the Flower federated
learning framework.

</details>


### [146] [Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing Detection](https://arxiv.org/abs/2508.18085)
*Abyad Enan,Mashrur Chowdhury,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 使用混合量子-经典自动编码器(HQC-AE)的零日骗窃检测方法，仅使用真实GNSS信号进行训练，在检测未见时间推送骗窃攻击方面显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: GNSS系统容易受到骗窃攻击，现有检测方法主要依靠监督学习，无法有效检测新型、未见的攻击

Method: 开发了一种混合量子-经典自动编码器(HQC-AE)，仅使用真实GNSS信号进行训练，利用跟踪阶段提取的特征进行预防性检测

Result: 在检测未见时间推送骗窃攻击方面，HQC-AE的平均检测准确率达到97.71%，平均假阻性率为0.62%；在复杂攻击中准确率达到98.23%，假阻性率为1.85%

Conclusion: 该方法能够在各种静态GNSS接收器平台上有效地预防性检测零日GNSS时间推送骗窃攻击，显著超过传统监督学习和无监督学习方法

Abstract: Global Navigation Satellite Systems (GNSS) are critical for Positioning,
Navigation, and Timing (PNT) applications. However, GNSS are highly vulnerable
to spoofing attacks, where adversaries transmit counterfeit signals to mislead
receivers. Such attacks can lead to severe consequences, including misdirected
navigation, compromised data integrity, and operational disruptions. Most
existing spoofing detection methods depend on supervised learning techniques
and struggle to detect novel, evolved, and unseen attacks. To overcome this
limitation, we develop a zero-day spoofing detection method using a Hybrid
Quantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSS
signals without exposure to spoofed data. By leveraging features extracted
during the tracking stage, our method enables proactive detection before PNT
solutions are computed. We focus on spoofing detection in static GNSS
receivers, which are particularly susceptible to time-push spoofing attacks,
where attackers manipulate timing information to induce incorrect time
computations at the receiver. We evaluate our model against different unseen
time-push spoofing attack scenarios: simplistic, intermediate, and
sophisticated. Our analysis demonstrates that the HQC-AE consistently
outperforms its classical counterpart, traditional supervised learning-based
models, and existing unsupervised learning-based methods in detecting zero-day,
unseen GNSS time-push spoofing attacks, achieving an average detection accuracy
of 97.71% with an average false negative rate of 0.62% (when an attack occurs
but is not detected). For sophisticated spoofing attacks, the HQC-AE attains an
accuracy of 98.23% with a false negative rate of 1.85%. These findings
highlight the effectiveness of our method in proactively detecting zero-day
GNSS time-push spoofing attacks across various stationary GNSS receiver
platforms.

</details>


### [147] [Provable Mixed-Noise Learning with Flow-Matching](https://arxiv.org/abs/2508.18122)
*Paul Hagemann,Robert Gruhlke,Bernhard Stankewitz,Claudia Schillings,Gabriele Steidl*

Main category: cs.LG

TL;DR: 提出基于条件流匹配和EM算法的混合噪声贝叶斯反问题推断框架，能联合估计后验采样器和噪声参数，在高维场景下具有良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实应用（特别是物理和化学领域）中的噪声通常具有未知和异构结构，而传统方法往往假设固定或已知的噪声特性，无法有效处理混合噪声问题。

Method: 将条件流匹配嵌入期望最大化(EM)算法中，使用无模拟ODE流匹配作为生成模型，在E步中估计后验采样器，在M步中更新噪声参数。

Result: 理论证明在无限观测的总体极限下，EM更新收敛到真实噪声参数。数值结果表明该方法在混合噪声贝叶斯反问题中的有效性。

Conclusion: 流匹配与EM推断的结合为处理混合噪声贝叶斯反问题提供了有效的解决方案，特别适用于高维推断场景。

Abstract: We study Bayesian inverse problems with mixed noise, modeled as a combination
of additive and multiplicative Gaussian components. While traditional inference
methods often assume fixed or known noise characteristics, real-world
applications, particularly in physics and chemistry, frequently involve noise
with unknown and heterogeneous structure. Motivated by recent advances in
flow-based generative modeling, we propose a novel inference framework based on
conditional flow matching embedded within an Expectation-Maximization (EM)
algorithm to jointly estimate posterior samplers and noise parameters. To
enable high-dimensional inference and improve scalability, we use
simulation-free ODE-based flow matching as the generative model in the E-step
of the EM algorithm. We prove that, under suitable assumptions, the EM updates
converge to the true noise parameters in the population limit of infinite
observations. Our numerical results illustrate the effectiveness of combining
EM inference with flow matching for mixed-noise Bayesian inverse problems.

</details>


### [148] [CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics](https://arxiv.org/abs/2508.18124)
*Weida Wang,Dongchen Huang,Jiatong Li,Tengchao Yang,Ziyang Zheng,Di Zhang,Dong Han,Benteng Chen,Binzhao Luo,Zhiyu Liu,Kunling Liu,Zhiyuan Gao,Shiqi Geng,Wei Ma,Jiaming Su,Xin Li,Shuchen Pu,Yuhan Shui,Qianjia Cheng,Zhihao Dou,Dongfei Cui,Changyong He,Jin Zeng,Zeke Xie,Mao Su,Dongzhan Zhou,Yuqiang Li,Wanli Ouyang,Lei Bai,Yunqi Cai,Xi Dai,Shufei Zhang,Jinguang Cheng,Zhong Fang,Hongming Weng*

Main category: cs.LG

TL;DR: CMPhysBench是一个专门评估大语言模型在凝聚态物理领域能力的基准测试，包含520多个研究生级别的计算问题，使用SEED评分系统进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注传统物理领域，缺乏对凝聚态物理这种前沿实践领域的专门评估工具，需要开发一个能够深度评估LLMs问题解决能力的专业基准。

Method: 构建包含520+研究生级别计算问题的数据集，涵盖磁学、超导、强关联系统等核心领域；开发SEED评分系统，基于表达式树结构进行细粒度相似度评估，提供非二进制的部分得分。

Result: 即使是表现最好的Grok-4模型，在CMPhysBench上也仅获得36的平均SEED分数和28%的准确率，显示出大语言模型在凝聚态物理领域的显著能力差距。

Conclusion: CMPhysBench揭示了LLMs在凝聚态物理这一重要前沿领域的局限性，为未来模型改进提供了重要基准，代码和数据集已开源供研究使用。

Abstract: We introduce CMPhysBench, designed to assess the proficiency of Large
Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.
CMPhysBench is composed of more than 520 graduate-level meticulously curated
questions covering both representative subfields and foundational theoretical
frameworks of condensed matter physics, such as magnetism, superconductivity,
strongly correlated systems, etc. To ensure a deep understanding of the
problem-solving process,we focus exclusively on calculation problems, requiring
LLMs to independently generate comprehensive solutions. Meanwhile, leveraging
tree-based representations of expressions, we introduce the Scalable Expression
Edit Distance (SEED) score, which provides fine-grained (non-binary) partial
credit and yields a more accurate assessment of similarity between prediction
and ground-truth. Our results show that even the best models, Grok-4, reach
only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a
significant capability gap, especially for this practical and frontier domain
relative to traditional physics. The code anddataset are publicly available at
https://github.com/CMPhysBench/CMPhysBench.

</details>


### [149] [Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics](https://arxiv.org/abs/2508.18130)
*Pradeep Singh,Mehak Sharma,Anupriya Dey,Balasubramanian Raman*

Main category: cs.LG

TL;DR: FreezeTST是一种轻量级混合模型，通过将冻结的随机特征（水库）块与标准可训练Transformer层交错结合，在保持推理复杂度不变的同时显著降低了训练参数和计算成本，在七个长期预测基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在序列建模中存在二次自注意力计算复杂和时间偏差弱的问题，导致长程预测既昂贵又脆弱，需要更高效的解决方案。

Method: 采用混合架构设计：冻结的随机特征块提供丰富的非线性记忆而无需优化成本，可训练的Transformer层通过自注意力学习查询这些记忆。

Result: 在七个标准长期预测基准测试中，FreezeTST始终匹配或超越Informer、Autoformer和PatchTST等专门变体，且计算成本显著降低。

Conclusion: 将水库原理嵌入Transformer中为高效长期时间序列预测提供了一条简单而原则性的途径。

Abstract: Transformers are the de-facto choice for sequence modelling, yet their
quadratic self-attention and weak temporal bias can make long-range forecasting
both expensive and brittle. We introduce FreezeTST, a lightweight hybrid that
interleaves frozen random-feature (reservoir) blocks with standard trainable
Transformer layers. The frozen blocks endow the network with rich nonlinear
memory at no optimisation cost; the trainable layers learn to query this memory
through self-attention. The design cuts trainable parameters and also lowers
wall-clock training time, while leaving inference complexity unchanged. On
seven standard long-term forecasting benchmarks, FreezeTST consistently matches
or surpasses specialised variants such as Informer, Autoformer, and PatchTST;
with substantially lower compute. Our results show that embedding reservoir
principles within Transformers offers a simple, principled route to efficient
long-term time-series prediction.

</details>


### [150] [Unveiling the Actual Performance of Neural-based Models for Equation Discovery on Graph Dynamical Systems](https://arxiv.org/abs/2508.18173)
*Riccardo Cappi,Paolo Frazzetto,Nicolò Navarin,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 本文比较了符号回归技术在网络动态系统方程发现中的表现，引入基于KAN的图网络方法，证明其比传统方法更准确且具有更好的可解释性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的黑盒特性阻碍了其在科学发现中的应用，特别是在网络动态系统的控制方程发现中，需要可解释的方法来理解复杂系统的物理动力学

Method: 评估了稀疏回归和MLP架构等现有方法，并创新性地将Kolmogorov-Arnold Networks (KANs) 适配到图结构，利用其可学习的激活函数实现更好的可解释性

Result: 在合成和真实动态系统测试中，MLP和KAN架构都能成功识别底层符号方程，显著超越现有基线。KANs以更高的简约性和透明度实现这一性能

Conclusion: 研究为研究人员提供了实用指南，阐明了模型表达能力和可解释性之间的权衡，确立了基于神经网络的架构在复杂系统科学发现中的可行性

Abstract: The ``black-box'' nature of deep learning models presents a significant
barrier to their adoption for scientific discovery, where interpretability is
paramount. This challenge is especially pronounced in discovering the governing
equations of dynamical processes on networks or graphs, since even their
topological structure further affects the processes' behavior. This paper
provides a rigorous, comparative assessment of state-of-the-art symbolic
regression techniques for this task. We evaluate established methods, including
sparse regression and MLP-based architectures, and introduce a novel adaptation
of Kolmogorov-Arnold Networks (KANs) for graphs, designed to exploit their
inherent interpretability. Across a suite of synthetic and real-world dynamical
systems, our results demonstrate that both MLP and KAN-based architectures can
successfully identify the underlying symbolic equations, significantly
surpassing existing baselines. Critically, we show that KANs achieve this
performance with greater parsimony and transparency, as their learnable
activation functions provide a clearer mapping to the true physical dynamics.
This study offers a practical guide for researchers, clarifying the trade-offs
between model expressivity and interpretability, and establishes the viability
of neural-based architectures for robust scientific discovery on complex
systems.

</details>


### [151] [Amortized Sampling with Transferable Normalizing Flows](https://arxiv.org/abs/2508.18175)
*Charlie B. Tan,Majdi Hassan,Leon Klein,Saifuddin Syed,Dominique Beaini,Michael M. Bronstein,Alexander Tong,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 提出了Prose，一个2.8亿参数的可转移归一化流模型，用于零样本生成任意肽系统的无相关样本，解决了传统分子采样方法缺乏摊销和跨系统迁移能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学和马尔可夫链蒙特卡洛方法缺乏摊销性，每个系统的采样计算成本都很高。虽然生成模型在单个系统上表现良好，但现有学习采样器在跨系统迁移方面能力有限。

Method: 开发了Prose，一个基于深度学习的大规模可转移归一化流模型，使用包含多达8个残基的肽分子动力学轨迹数据集进行训练，能够进行零样本无相关提案采样。

Result: Prose在未见过的四肽上通过简单的基于重要性采样的微调程序，实现了优于传统方法（如顺序蒙特卡洛）的性能，同时保持了归一化流的高效似然评估能力。

Conclusion: 深度学习能够设计出可扩展和可转移的采样器，Prose证明了在肽系统中实现跨序列长度转移的可行性，为摊销采样方法的研究开辟了新方向。

Abstract: Efficient equilibrium sampling of molecular conformations remains a core
challenge in computational chemistry and statistical inference. Classical
approaches such as molecular dynamics or Markov chain Monte Carlo inherently
lack amortization; the computational cost of sampling must be paid in-full for
each system of interest. The widespread success of generative models has
inspired interest into overcoming this limitation through learning sampling
algorithms. Despite performing on par with conventional methods when trained on
a single system, learned samplers have so far demonstrated limited ability to
transfer across systems. We prove that deep learning enables the design of
scalable and transferable samplers by introducing Prose, a 280 million
parameter all-atom transferable normalizing flow trained on a corpus of peptide
molecular dynamics trajectories up to 8 residues in length. Prose draws
zero-shot uncorrelated proposal samples for arbitrary peptide systems,
achieving the previously intractable transferability across sequence length,
whilst retaining the efficient likelihood evaluation of normalizing flows.
Through extensive empirical evaluation we demonstrate the efficacy of Prose as
a proposal for a variety of sampling algorithms, finding a simple importance
sampling-based finetuning procedure to achieve superior performance to
established methods such as sequential Monte Carlo on unseen tetrapeptides. We
open-source the Prose codebase, model weights, and training dataset, to further
stimulate research into amortized sampling methods and finetuning objectives.

</details>


### [152] [AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models](https://arxiv.org/abs/2508.18182)
*Nikolay Kutuzov,Makar Baderko,Stepan Kulibaba,Artem Dzhalilov,Daniel Bobrov,Maxim Mashtaler,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 一种三阶段分布式训练方法，通过多实例训练、适配批处理和切换机制，提高大语言模型在异构硬件下的训练效率和收敛速度


<details>
  <summary>Details</summary>
Motivation: 解决现有分布式训练方法（如DiLoCo）在动态负载下无法充分利用异构计算集群资源的问题

Method: 三阶段方法：1）多实例训练（MIT）-让单个节点并行运行多个轻量训练流；2）适配批处DiLoCo-动态调整本地批处大小平衡计算与通信；3）切换模式-在批处大小超过硬件限制时引入梯度累积

Result: 提高了系统吞吐量和减少空闲时间，大幅降低同步延迟，收敛速度和系统效率都得到改善，并提供了完全收敛所需通信次数的理论估计

Conclusion: 该三阶段方法能够有效利用异构硬件资源，在动态负载环境下实现更高效的大语言模型分布式训练

Abstract: Scaling distributed training of Large Language Models (LLMs) requires not
only algorithmic advances but also efficient utilization of heterogeneous
hardware resources. While existing methods such as DiLoCo have demonstrated
promising results, they often fail to fully exploit computational clusters
under dynamic workloads. To address this limitation, we propose a three-stage
method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,
and switch mode mechanism. MIT allows individual nodes to run multiple
lightweight training streams with different model instances in parallel and
merge them to combine knowledge, increasing throughput and reducing idle time.
Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance
computation and communication, substantially lowering synchronization delays.
Switch mode further stabilizes training by seamlessly introducing gradient
accumulation once adaptive batch sizes grow beyond hardware-friendly limits.
Together, these innovations improve both convergence speed and system
efficiency. We also provide a theoretical estimate of the number of
communications required for the full convergence of a model trained using our
method.

</details>


### [153] [HypER: Hyperbolic Echo State Networks for Capturing Stretch-and-Fold Dynamics in Chaotic Flows](https://arxiv.org/abs/2508.18196)
*Pradeep Singh,Sutirtha Ghosh,Ashutosh Kumar,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TL;DR: 提出了基于双曲几何的HypER回声状态网络，通过双曲距离的指数衰减连接来匹配混沌系统的拉伸-折叠结构，显著延长了混沌系统的有效预测时间


<details>
  <summary>Details</summary>
Motivation: 现有的ESN使用欧几里得几何，与混沌系统的拉伸-折叠结构不匹配，导致预测时间有限

Method: 在Poincare球中采样神经元，连接权重随双曲距离指数衰减，保持ESN的稀疏性、泄漏积分和谱半径控制等特性，仅训练Tikhonov正则化的读出层

Result: 在Lorenz-63、Roessler系统和Chen-Ueta超混沌吸引子上，HypER显著延长了平均有效预测时间，在30次独立运行中均优于欧几里得和图结构ESN基线；在心率变异性、太阳黑子数等真实世界数据上也表现出优势

Conclusion: 双曲几何嵌入能够更好地匹配混沌动力学，HypER通过指数度量空间直接嵌入潜在空间，实现了与Lyapunov方向对齐的局部扩展-收缩谱，为混沌系统预测提供了更有效的方法

Abstract: Forecasting chaotic dynamics beyond a few Lyapunov times is difficult because
infinitesimal errors grow exponentially. Existing Echo State Networks (ESNs)
mitigate this growth but employ reservoirs whose Euclidean geometry is
mismatched to the stretch-and-fold structure of chaos. We introduce the
Hyperbolic Embedding Reservoir (HypER), an ESN whose neurons are sampled in the
Poincare ball and whose connections decay exponentially with hyperbolic
distance. This negative-curvature construction embeds an exponential metric
directly into the latent space, aligning the reservoir's local
expansion-contraction spectrum with the system's Lyapunov directions while
preserving standard ESN features such as sparsity, leaky integration, and
spectral-radius control. Training is limited to a Tikhonov-regularized readout.
On the chaotic Lorenz-63 and Roessler systems, and the hyperchaotic Chen-Ueta
attractor, HypER consistently lengthens the mean valid-prediction horizon
beyond Euclidean and graph-structured ESN baselines, with statistically
significant gains confirmed over 30 independent runs; parallel results on
real-world benchmarks, including heart-rate variability from the Santa Fe and
MIT-BIH datasets and international sunspot numbers, corroborate its advantage.
We further establish a lower bound on the rate of state divergence for HypER,
mirroring Lyapunov growth.

</details>


### [154] [Deep Learning and Matrix Completion-aided IoT Network Localization in the Outlier Scenarios](https://arxiv.org/abs/2508.18225)
*Sunwoo Kim*

Main category: cs.LG

TL;DR: 提出一种基于深度学习和矩阵补全的方法，用于恢复物联网网络定位中受异常值污染的欧几里得距离矩阵，通过联合恢复距离矩阵和传感器坐标，有效处理异常值。


<details>
  <summary>Details</summary>
Motivation: 传统定位技术在矩阵全集上搜索解，而本方法将搜索限制在欧几里得距离矩阵集合内，利用距离矩阵的独特性质提高定位精度，特别是在存在异常值的情况下。

Method: 将距离矩阵D表示为传感器坐标矩阵X的函数，利用深度神经网络联合恢复D和X；将异常值建模为稀疏矩阵L，在优化问题中加入L的正则化项，通过交替更新X、D和L来求解。

Result: 数值实验表明，即使在存在异常值的情况下，所提出的技术也能准确恢复传感器的位置信息。

Conclusion: 该方法通过限制搜索空间和联合优化策略，有效处理了异常值污染问题，在物联网网络定位中表现出良好的性能。

Abstract: In this paper, we propose a deep learning and matrix completion aided
approach for recovering an outlier contaminated Euclidean distance matrix D in
IoT network localization. Unlike conventional localization techniques that
search the solution over a whole set of matrices, the proposed technique
restricts the search to the set of Euclidean distance matrices. Specifically,
we express D as a function of the sensor coordinate matrix X that inherently
satisfies the unique properties of D, and then jointly recover D and X using a
deep neural network. To handle outliers effectively, we model them as a sparse
matrix L and add a regularization term of L into the optimization problem. We
then solve the problem by alternately updating X, D, and L. Numerical
experiments demonstrate that the proposed technique can recover the location
information of sensors accurately even in the presence of outliers.

</details>


### [155] [Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data](https://arxiv.org/abs/2508.18244)
*Chu-Cheng Lin,Daiyi Peng,Yifeng Lu,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: TACs框架通过将工作流重新定义为类型化概率程序，使用梯度优化方法解决LLM多步工作流组合的脆弱性问题，在结构化任务上显著优于现有提示优化方法


<details>
  <summary>Details</summary>
Motivation: 当前基于离散提示优化的LLM工作流组合方法存在脆弱性问题，难以满足结构化任务的形式合规要求

Method: 将整个工作流（参数高效适配的LLM和确定性逻辑）视为未归一化联合分布，实现基于梯度的原则性训练，即使存在潜在中间结构

Result: 在结构化任务上显著超越最先进的提示优化基线，MGSM-SymPy从57.1%提升到75.9%（27B模型），MGSM从1.6%提升到27.3%（7B模型）

Conclusion: TACs为开发可靠、任务合规的LLM系统提供了一个鲁棒且理论基础坚实的范式

Abstract: Reliably composing Large Language Models (LLMs) for complex, multi-step
workflows remains a significant challenge. The dominant paradigm-optimizing
discrete prompts in a pipeline-is notoriously brittle and struggles to enforce
the formal compliance required for structured tasks. We introduce
Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow
adaptation as learning typed probabilistic programs. TACs treats the entire
workflow, which is composed of parameter-efficiently adapted LLMs and
deterministic logic, as an unnormalized joint distribution. This enables
principled, gradient-based training even with latent intermediate structures.
We provide theoretical justification for our tractable optimization objective,
proving that the optimization bias vanishes as the model learns type
compliance. Empirically, TACs significantly outperforms state-of-the-art
prompt-optimization baselines. Gains are particularly pronounced on structured
tasks, improving MGSM-SymPy from $57.1\%$ to $75.9\%$ for a 27B model, MGSM
from $1.6\%$ to $27.3\%$ for a 7B model. TACs offers a robust and theoretically
grounded paradigm for developing reliable, task-compliant LLM systems.

</details>


### [156] [Aligning the Evaluation of Probabilistic Predictions with Downstream Value](https://arxiv.org/abs/2508.18251)
*Novin Shahroudi,Viacheslav Komisarenko,Meelis Kull*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的方法来学习与下游任务评估对齐的代理评估函数，通过神经网络参数化的加权评分规则来解决预测评估与下游效用之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统的预测性能度量往往与真实世界下游任务的实际影响存在偏差，现有方法要么依赖多个任务特定指标，要么需要事先知道明确的成本结构，这在实际应用中存在局限性。

Method: 基于适当评分规则理论，探索确保适当性保持的评分规则变换，使用神经网络参数化的加权评分规则，通过学习权重来与下游任务性能对齐。

Result: 通过合成和真实数据的回归任务实验展示了该框架的潜力，能够有效弥合预测评估与下游效用之间的差距。

Conclusion: 该方法为模块化预测系统提供了一种快速、可扩展的评估循环方案，能够在权重复杂或事先未知的任务中实现预测评估与下游效用的对齐。

Abstract: Every prediction is ultimately used in a downstream task. Consequently,
evaluating prediction quality is more meaningful when considered in the context
of its downstream use. Metrics based solely on predictive performance often
diverge from measures of real-world downstream impact. Existing approaches
incorporate the downstream view by relying on multiple task-specific metrics,
which can be burdensome to analyze, or by formulating cost-sensitive
evaluations that require an explicit cost structure, typically assumed to be
known a priori. We frame this mismatch as an evaluation alignment problem and
propose a data-driven method to learn a proxy evaluation function aligned with
the downstream evaluation. Building on the theory of proper scoring rules, we
explore transformations of scoring rules that ensure the preservation of
propriety. Our approach leverages weighted scoring rules parametrized by a
neural network, where weighting is learned to align with the performance in the
downstream task. This enables fast and scalable evaluation cycles across tasks
where the weighting is complex or unknown a priori. We showcase our framework
through synthetic and real-data experiments for regression tasks, demonstrating
its potential to bridge the gap between predictive evaluation and downstream
utility in modular prediction systems.

</details>


### [157] [ANO : Faster is Better in Noisy Landscape](https://arxiv.org/abs/2508.18258)
*Adrien Kegreisz*

Main category: cs.LG

TL;DR: Ano优化器通过分离方向和平滑处理，使用动量进行方向平滑，瞬时梯度幅度决定步长，提高了对梯度噪声的鲁棒性。Anolog进一步通过对数调度扩展动量窗口，消除对动量系数的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam和Adan在非平稳或噪声环境中性能下降，主要因为依赖基于动量的幅度估计。需要一种更鲁棒的优化方法。

Method: 提出Ano优化器，将方向和平滑解耦：动量用于方向平滑，瞬时梯度幅度决定步长。进一步提出Anolog，通过对数调度扩展动量窗口。

Result: 建立了非凸收敛保证，收敛速率与其他基于符号的方法相似。在强化学习等噪声和非平稳环境中表现显著提升，在计算机视觉基准测试中保持竞争力。

Conclusion: Ano优化器通过解耦方向和平滑的设计，在保持一阶方法简洁高效的同时，显著提高了在噪声环境中的鲁棒性，为深度学习优化提供了新思路。

Abstract: Stochastic optimizers are central to deep learning, yet widely used methods
such as Adam and Adan can degrade in non-stationary or noisy environments,
partly due to their reliance on momentum-based magnitude estimates. We
introduce Ano, a novel optimizer that decouples direction and magnitude:
momentum is used for directional smoothing, while instantaneous gradient
magnitudes determine step size. This design improves robustness to gradient
noise while retaining the simplicity and efficiency of first-order methods. We
further propose Anolog, which removes sensitivity to the momentum coefficient
by expanding its window over time via a logarithmic schedule. We establish
non-convex convergence guarantees with a convergence rate similar to other
sign-based methods, and empirically show that Ano provides substantial gains in
noisy and non-stationary regimes such as reinforcement learning, while
remaining competitive on low-noise tasks such as standard computer vision
benchmarks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [158] [H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2508.16653)
*Zizhuo Fu,Xiaotian Guo,Wenxuan Zeng,Shuzhang Zhong,Yadong Zhang,Peiyu Chen,Runsheng Wang,Le Ye,Meng Li*

Main category: cs.PF

TL;DR: H2EAL是一种基于混合键合技术的稀疏注意力算法-硬件协同设计加速器，用于边缘设备上的高效LLM推理，通过混合稀疏注意力和内存计算协同布局解决了分布式内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备部署时，KV缓存带来的高能耗和延迟问题限制了应用，特别是长上下文场景。混合键合技术虽然提供了更好的带宽效率和功耗优势，但存在分布式内存瓶颈。

Method: 算法层面提出混合稀疏注意力方案（静态和动态稀疏性结合），硬件层面协同设计支持混合稀疏注意力，采用内存计算协同布局，并开发负载均衡调度器和平铺注意力优化映射策略。

Result: 实验显示H2EAL相比基线HB实现实现了5.20~48.21倍加速和6.22~73.48倍能效提升，在多个基准测试中平均准确率仅下降0.87%。

Conclusion: H2EAL通过算法-硬件协同设计有效解决了混合键合架构下的分布式内存瓶颈问题，为边缘设备上的高效LLM推理提供了可行的解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in a
wide range of natural language processing applications. However, the high
energy and latency overhead induced by the KV cache limits the edge deployment,
especially for long contexts. Emerging hybrid bonding (HB) technology has been
proposed as a promising alternative to conventional near-memory processing
(NMP) architectures, offering improved bandwidth efficiency and lower power
consumption while exhibiting characteristics of distributed memory. In this
paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse
attention algorithm-hardware co-design for efficient LLM inference at the edge.
At the algorithm level, we propose a hybrid sparse attention scheme with static
and dynamic sparsity for different heads to fully leverage the sparsity with
high accuracy. At the hardware level, we co-design the hardware to support
hybrid sparse attention and propose memory-compute co-placement to address the
distributed memory bottleneck. Since different attention heads exhibit
different sparse patterns and the attention structure often mismatches the HB
architecture, we further develop a load-balancing scheduler with parallel tiled
attention to address workload imbalance and optimize the mapping strategy.
Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and
6.22~73.48x energy efficiency improvement over baseline HB implementation, with
a negligible average accuracy drop of 0.87% on multiple benchmarks.

</details>


### [159] [Dynamic Sparse Attention on Mobile SoCs](https://arxiv.org/abs/2508.16703)
*Wangsong Yin,Daliang Xu,Mengwei Xu,Gang Huang,Xuanzhe Liu*

Main category: cs.PF

TL;DR: shadowAttn是一个系统-算法协同设计的稀疏注意力模块，通过在少量token上稀疏计算注意力，减少对CPU/GPU的依赖，提升设备端LLM运行效率。


<details>
  <summary>Details</summary>
Motivation: 现有框架中注意力算子因量化敏感性问题从专用NPU回退到通用CPU/GPU，导致用户体验下降和系统调度复杂度增加。

Method: 采用NPU-based pilot compute隐藏重要token估计开销，提出NPU计算图分桶、头级NPU-CPU/GPU流水线和每头细粒度稀疏比等技术。

Result: 在CPU/GPU资源高度受限的情况下提供最佳性能，以更少的CPU/GPU资源实现与最先进框架相当的性能。

Conclusion: shadowAttn通过系统-算法协同设计有效解决了设备端LLM运行中的注意力计算效率问题，为隐私保护提供了更好的技术支撑。

Abstract: On-device running Large Language Models (LLMs) is nowadays a critical enabler
towards preserving user privacy. We observe that the attention operator falls
back from the special-purpose NPU to the general-purpose CPU/GPU because of
quantization sensitivity in state-of-the-art frameworks. This fallback results
in a degraded user experience and increased complexity in system scheduling. To
this end, this paper presents shadowAttn, a system-algorithm codesigned sparse
attention module with minimal reliance on CPU/GPU by only sparsely calculating
the attention on a tiny portion of tokens. The key idea is to hide the overhead
of estimating the important tokens with a NPU-based pilot compute. Further,
shadowAttn proposes insightful techniques such as NPU compute graph bucketing,
head-wise NPU-CPU/GPU pipeline and per-head fine-grained sparsity ratio to
achieve high accuracy and efficiency. shadowAttn delivers the best performance
with highly limited CPU/GPU resource; it requires much less CPU/GPU resource to
deliver on-par performance of SoTA frameworks.

</details>


### [160] [Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective](https://arxiv.org/abs/2508.16712)
*Tianyao Shi,Yi Ding*

Main category: cs.PF

TL;DR: 对11种LLM后训练量化方法在真实服务条件下的性能、能耗和质量权衡进行了系统性评估，开发了自动化框架qMeter，覆盖4种模型规模和2种GPU架构


<details>
  <summary>Details</summary>
Motivation: 虽然存在多种量化方法，但在真实服务条件下对量化方法的性能、能耗和质量权衡缺乏系统性理解

Method: 开发自动化在线表征框架qMeter，对11种后训练量化方法在7B-70B模型规模和A100/H100 GPU上进行多维度评估，包括应用、工作负载、并行和硬件层面

Result: 揭示了量化权衡高度依赖于任务和方法，对工作负载特征敏感，与并行性和GPU架构存在复杂交互

Conclusion: 这是首个从性能、能耗和质量联合角度对LLM量化进行的全面应用、系统和硬件层面表征研究，为部署优化提供了重要见解

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse domains, but their heavy resource demands make quantization-reducing
precision to lower-bit formats-critical for efficient serving. While many
quantization methods exist, a systematic understanding of their performance,
energy, and quality tradeoffs in realistic serving conditions remains a gap. In
this work, we first develop a fully automated online characterization framework
qMeter, and then conduct an in-depth characterization of 11 post-training LLM
quantization methods across 4 model sizes (7B-70B) and two GPU architectures
(A100, H100). We evaluate quantization at the application, workload,
parallelism, and hardware levels under online serving conditions. Our study
reveals highly task- and method-dependent tradeoffs, strong sensitivity to
workload characteristics, and complex interactions with parallelism and GPU
architecture. We further present three optimization case studies illustrating
deployment challenges in capacity planning, energy-efficient scheduling, and
multi-objective tuning. To the best of our knowledge, this is one of the first
comprehensive application-, system-, and hardware-level characterization of LLM
quantization from a joint performance, energy, and quality perspective.

</details>


### [161] [Evaluación y modelado del rendimiento de los sistemas informáticos](https://arxiv.org/abs/2508.16996)
*Xavier Molero,Carlos Juiz,Miguel Jesus Rodeno*

Main category: cs.PF

TL;DR: 这本书维纳地介绍了计算机系统性能评估和建模的量化技术，通过理论与实践相结合的方式，分类提供了完整解决方案、只给答案和读者自行解决的问题集。


<details>
  <summary>Details</summary>
Motivation: 为了给读者提供实用的量化技术，帮助回答计算机系统性能相关问题，而非纯粹的理论研究。

Method: 每章先进行短暂的理论回顾，然后深入分析大量问题，将其分为三类：完整解决方案的问题、只提供答案的问题、以及需读者自行解决的问题。

Result: 虽然部分问题在复杂性上可能被视为学术性超出实际需求，但它们在缩小规模上展示了解决工业级实际问题所需的处理流程和工具使用。

Conclusion: 该书通过理论与实践相结合的方式，有效地帮助读者掌握解决计算机系统性能问题的量化技术，并为处理真实工业场景问题奠定基础。

Abstract: This book, by Molero, Juiz, and Rodeno, titled Performance Evaluation and
Modeling of Computer Systems, presents a comprehensive summary of simple
quantitative techniques that help answer the above questions. Its approach is
not one of theory for theory's sake; rather, in each chapter, after a brief
theoretical review, it delves deeply into numerous problems grouped into three
categories: those with complete solutions, those for which only the solution is
given, and, finally, those whose resolution is left to the reader's discretion.
Although some of the solved problems may be considered purely academic in terms
of complexity, they should not be underestimated, as they reveal, on a reduced
scale, the process that must be followed with the help of appropriate tools to
solve equivalent real-world problems of an industrial scale.

</details>


### [162] [The Unwritten Contract of Cloud-based Elastic Solid-State Drives](https://arxiv.org/abs/2508.17372)
*Yingjia Wang,Ming-Chang Yang*

Main category: cs.PF

TL;DR: 香港云和阿里巴巴云的弹性块存储(ESSD)性能特征分析，揭示了与本地SSD存在明显性能差异的四个观察结论和五项实践建议


<details>
  <summary>Details</summary>
Motivation: 虽然弹性块存储(EBS)广泛集成到云服务中，但缺乏对弹性固态硬盘(ESSD)性能的详细特征分析，需要确认ESSD是否能够满足云上服务的存储性能需求

Method: 首次对亚马逊AWS和阿里巴巴云的两款ESSD进行性能特征分析，通过实验测试识别其性能特点

Result: 发现了四个违背传统认知的反直觉观察，ESSD与本地SSD在性能上存在显著差异，形成了云端ESSD的“非成文合同”

Conclusion: 研究结果为云存储用户提供了五项实践建议，帮助用户重新设计部署的云软件以更好利用ESSD的独特特性来提升系统性能

Abstract: Elastic block storage (EBS) with the storage-compute disaggregated
architecture stands as a pivotal piece in today's cloud. EBS furnishes users
with storage capabilities through the elastic solid-state drive (ESSD).
Nevertheless, despite the widespread integration into cloud services, the
absence of a thorough ESSD performance characterization raises critical doubt:
when more and more services are shifted onto the cloud, can ESSD satisfactorily
substitute the storage responsibilities of the local SSD and offer comparable
performance?
  In this paper, we for the first time target this question by characterizing
two ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract
of cloud-based ESSDs, encapsulating four observations and five implications for
cloud storage users. Specifically, the observations are counter-intuitive and
contrary to the conventional perceptions of what one would expect from the
local SSD. The implications we hope could guide users in revisiting the designs
of their deployed cloud software, i.e., harnessing the distinct characteristics
of ESSDs for better system performance.

</details>


### [163] [Evaluating Compiler Optimization Impacts on zkVM Performance](https://arxiv.org/abs/2508.17518)
*Thomas Gassmann,Stefanos Chaliasos,Thodoris Sotiropoulos,Zhendong Su*

Main category: cs.PF

TL;DR: 这是首个系统性研究LLVM编译器优化对zkVM性能影响的论文，发现标准优化效果远差于传统CPU，并通过微调少量LLVM通道使其zkVM感知，实现了显著性能提升


<details>
  <summary>Details</summary>
Motivation: 虽然zkVM将零知证明转换为标准编译流程，但标准编译器优化为传统硬件设计，假设了缓存局部性、分支预测等特性，而这些在zkVM中缺失，导致优化效果受到质疑

Method: 系统性评估64个LLVM通道、6个标准优化级别咊未优化基准线，在2个RISC-V基zkVM(RISC Zero和SP1)上运行58个性能测试。通过细粒度通道分析，微调少量LLVM通道使其能感知zkVM特性

Result: 标准LLVM优化级别能提升zkVM性能超过40%，但效果远差于传统CPU。细微调整后的zkVM感知通道能进一步提升执行时间达45%(RISC Zero平均+4.6%，SP1平均+1%)，并实现了一致的证明时间提升

Conclusion: 这项工作呈现了编译器层面优化对zkVM性能的巨大潜力，为开发zkVM专用通道、后端咊超优化器打开了新方向

Abstract: Zero-knowledge proofs (ZKPs) are the cornerstone of programmable
cryptography. They enable (1) privacy-preserving and verifiable computation
across blockchains, and (2) an expanding range of off-chain applications such
as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the
barrier by turning ZKPs into a drop-in backend for standard compilation
pipelines. This lets developers write proof-generating programs in conventional
languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.
However, these VMs inherit compiler infrastructures tuned for traditional
architectures rather than for proof systems. In particular, standard compiler
optimizations assume features that are absent in zkVMs, including cache
locality, branch prediction, or instruction-level parallelism. Therefore, their
impact on proof generation is questionable.
  We present the first systematic study of the impact of compiler optimizations
on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an
unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero
and SP1). While standard LLVM optimization levels do improve zkVM performance
(over 40\%), their impact is far smaller than on traditional CPUs, since their
decisions rely on hardware features rather than proof constraints. Guided by a
fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM
passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average
+4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains.
Our work highlights the potential of compiler-level optimizations for zkVM
performance and opens new direction for zkVM-specific passes, backends, and
superoptimizers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [164] [Performance measurements of modern Fortran MPI applications with Score-P](https://arxiv.org/abs/2508.16592)
*Gregor Corbin*

Main category: cs.DC

TL;DR: Score-P工具为MPI F08绑定提供了完整的性能测量支持，通过Fortran实现的包装器覆盖MPI 4.1标准全部特性，解决了工具支持缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: MPI 3.0标准引入的F08语言绑定虽然提供了类型安全和标准兼容的Fortran调用方式，但近十年来工具支持仍然缺乏，迫使开发者使用不够安全和方便的接口。

Method: 通过在Fortran中实现MPI包装器，使用代码生成器产生约5万行包装代码，利用Python pympistandard模块程序化访问MPI标准数据，全面支持MPI 4.1标准特性。

Result: 新的F08包装器已成功应用于两个流体动力学模拟代码（Neko和EPIC）的性能测量生成，并通过MPICH测试套件进行定期测试。

Conclusion: 该实现为现代Fortran应用开发者提供了完整的MPI F08绑定工具支持，解决了长期存在的工具兼容性问题，使开发者能够充分利用F08绑定的优势。

Abstract: Version 3.0 of the Message-Passing Interface (MPI) standard, released in
2012, introduced a new set of language bindings for Fortran 2008. By making use
of modern language features and the enhanced interoperability with C, there was
finally a type safe and standard conforming method to call MPI from Fortran.
This highly recommended use mpi_f08 language binding has since then been widely
adopted among developers of modern Fortran applications. However, tool support
for the F08 bindings is still lacking almost a decade later, forcing users to
recede to the less safe and convenient interfaces. Full support for the F08
bindings was added to the performance measurement infrastructure Score-P by
implementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard
version 4.1 in its entirety, matching the features of the C wrappers. By
implementing the wrappers in modern Fortran, we can provide full support for
MPI procedures passing attributes, info objects, or callbacks. The
implementation is regularly tested under the MPICH test suite. The new F08
wrappers were already used by two fluid dynamics simulation codes -- Neko, a
spectral finite-element code derived from Nek5000, and EPIC (Elliptical
Parcel-In-Cell) -- to successfully generate performance measurements. In this
work, we additionally present our design considerations and sketch out the
implementation, discussing the challenges we faced in the process. The key
component of the implementation is a code generator that produces approximately
50k lines of MPI wrapper code to be used by Score-P, relying on the Python
pympistandard module to provide programmatic access to the extracted data from
the MPI standard.

</details>


### [165] [GPU Acceleration for Faster Evolutionary Spatial Cyclic Game Systems](https://arxiv.org/abs/2508.16639)
*Louie Sinadjan*

Main category: cs.DC

TL;DR: 开发了基于Metal和CUDA的GPU加速进化空间循环游戏模拟框架，相比单线程实现获得最高28倍加速，使更大规模系统模拟成为可能


<details>
  <summary>Details</summary>
Motivation: 传统的单线程进化空间循环游戏(ESCG)模拟计算成本高且扩展性差，需要高性能实现来支持生态和进化动力学研究

Method: 使用Apple Metal和Nvidia CUDA开发GPU加速框架，以验证过的单线程C++版本作为基准对比

Result: GPU加速带来显著性能提升，CUDA实现最高达28倍加速，可处理最大3200x3200系统规模，Metal存在扩展性限制

Conclusion: 项目提供了可配置的ESCG模拟平台，推进了该研究领域的计算工具集，成果已被欧洲建模与仿真研讨会接受发表

Abstract: This dissertation presents the design, implementation and evaluation of
GPU-accelerated simulation frameworks for Evolutionary Spatial Cyclic Games
(ESCGs), a class of agent-based models used to study ecological and
evolutionary dynamics. Traditional single-threaded ESCG simulations are
computationally expensive and scale poorly. To address this, high-performance
implementations were developed using Apple's Metal and Nvidia's CUDA, with a
validated single-threaded C++ version serving as a baseline comparison point.
  Benchmarking results show that GPU acceleration delivers significant
speedups, with the CUDA maxStep implementation achieving up to a 28x
improvement. Larger system sizes, up to 3200x3200, became tractable, while
Metal faced scalability limits. The GPU frameworks also enabled replication and
critical extension of recent ESCG studies, revealing sensitivities to system
size and runtime not fully explored in prior work.
  Overall, this project provides a configurable ESCG simulation platform that
advances the computational toolkit for this field of research. This
dissertation forms the basis for a paper accepted for publication and
presentation at the European Modelling and Simulation Symposium.

</details>


### [166] [Equinox: Holistic Fair Scheduling in Serving Large Language Models](https://arxiv.org/abs/2508.16646)
*Zhixiang Wei,James Yen,Jingyi Chen,Ziyang Zhang,Zhibai Huang,Chen Chen,Xingzi Yu,Yicheng Gu,Chenggang Wu,Yun Wang,Mingyuan Xia,Jie Wu,Hao Wang,Zhengwei Qi*

Main category: cs.DC

TL;DR: 提出Equinox系统，通过双计数器框架和预测专家模型实现公平感知的LLM服务调度，在保持高GPU利用率的同时提升吞吐量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM服务中用户服务质量与资源效率之间的调度悖论，需要在执行前就能平衡用户公平性和资源公平性

Method: 使用用户公平计数器（加权token和延迟）和资源公平计数器（吞吐量和GPU利用率），通过Mixture of Prediction Experts框架预测关键指标，计算统一的整体公平分数进行主动调度

Result: 在ShareGPT和LMSYS生产迹上实现1.3倍吞吐量提升、60%首token延迟降低、13%公平性提升，保持94% GPU利用率

Conclusion: Equinox系统通过预测驱动的公平感知调度，有效解决了LLM服务中的公平性与效率平衡问题，在异构平台上证明了其有效性

Abstract: We address the limitations of current LLM serving with a dual-counter
framework separating user and operator perspectives. The User Fairness Counter
measures quality of service via weighted tokens and latency; the Resource
Fairness Counter measures operational efficiency through throughput and GPU
utilization. Since these metrics are only available post-execution, creating a
scheduling paradox, we introduce a deterministic Mixture of Prediction Experts
(MoPE) framework to predict user-perceived latency, output tokens, throughput,
and GPU utilization. These predictions enable calculation of a unified Holistic
Fairness score that balances both counters through tunable parameters for
proactive fairness-aware scheduling. We implement this in Equinox, an
open-source system with other optimizations like adaptive batching, and
stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and
synthetic workloads demonstrate Equinox achieves up to $1.3\times$ higher
throughput, 60\% lower time-to-first-token latency, and 13\% higher fairness
versus VTC while maintaining 94\% GPU utilization, proving fairness under
bounded discrepancy across heterogeneous platforms.

</details>


### [167] [Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on Loihi 2](https://arxiv.org/abs/2508.16792)
*Felix Wang,Bradley H. Theilman,Fred Rothganger,William Severa,Craig M. Vineyard,James B. Aimone*

Main category: cs.DC

TL;DR: 首次在神经模式硬件上实现了果蝇全脑连接组，包含14万神经元和5000万突触，通过12块Loihi 2芯片实现，运行速度比传统硬件快数个数级


<details>
  <summary>Details</summary>
Motivation: 真实生物神经网络具有稀疏、循环、不规则的连接特征，不适合传统计算方法，需要神经模式硬件来高效模拟

Method: 使用Intel Loihi 2神经模式平台，解决了连接结构映射中的所输入/输出内存限制问题，将FlyWire果蝇全脑连接组部署到12块芯片上

Result: 实现了生物学上有意义的连接组模拟，运行速度显著优于传统硬件，而且活动越稀疏性能优势越明显

Conclusion: 今天的可扩展神经模式平台能够实现和加速生物实际模型，为推进神经受启发的人工智能和计算神经科学提供了关键技术支撑

Abstract: We demonstrate the first-ever nontrivial, biologically realistic connectome
simulated on neuromorphic computing hardware. Specifically, we implement the
whole-brain connectome of the adult Drosophila melanogaster (fruit fly) from
the FlyWire Consortium containing 140K neurons and 50M synapses on the Intel
Loihi 2 neuromorphic platform. This task is particularly challenging due to the
characteristic connectivity structure of biological networks. Unlike artificial
neural networks and most abstracted neural models, real biological circuits
exhibit sparse, recurrent, and irregular connectivity that is poorly suited to
conventional computing methods intended for dense linear algebra. Though
neuromorphic hardware is architecturally better suited to discrete event-based
biological communication, mapping the connectivity structure to frontier
systems still faces challenges from low-level hardware constraints, such as
fan-in and fan-out memory limitations. We describe solutions to these
challenges that allow for the full FlyWire connectome to fit onto 12 Loihi 2
chips. We statistically validate our implementation by comparing network
behavior across multiple reference simulations. Significantly, we achieve a
neuromorphic implementation that is orders of magnitude faster than numerical
simulations on conventional hardware, and we also find that performance
advantages increase with sparser activity. These results affirm that today's
scalable neuromorphic platforms are capable of implementing and accelerating
biologically realistic models -- a key enabling technology for advancing
neuro-inspired AI and computational neuroscience.

</details>


### [168] [PICO: Performance Insights for Collective Operations](https://arxiv.org/abs/2508.16809)
*Saverio Pasqualoni,Lorenzo Piarulli,Daniele De Sensi*

Main category: cs.DC

TL;DR: PICO是一个轻量级可扩展框架，用于简化集体操作的性能基准测试，解决现有框架在详细性能分析和可重现性方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有的集体操作性能评估框架无法提供足够的详细性能分析信息，且缺乏可重现性和可扩展性，难以满足HPC和AI训练中对集体操作性能评估的需求

Method: 开发了PICO框架，这是一个轻量级、可扩展的框架，专门设计用于集体操作的基准测试，提供详细的性能分析能力

Result: 提出了PICO框架，能够提供更详细的性能分析信息，确保测试的可重现性，并支持框架的扩展性

Conclusion: PICO框架为集体操作的性能基准测试提供了一个有效的解决方案，弥补了现有工具的不足，具有重要的实用价值

Abstract: Collective operations are cornerstones of both HPC application and
large-scale AI training and inference. Yet, comprehensive, systematic and
reproducible performance evaluation and benchmarking of said operations is not
straightforward. Existing frameworks do not provide sufficiently detailed
profiling information, nor they ensure reproducibility and extensibility. In
this paper, we present PICO (Performance Insights for Collective Operations), a
novel lightweight, extensible framework built with the aim of simplifying
collective operations benchmarking.

</details>


### [169] [Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning](https://arxiv.org/abs/2508.17209)
*Yebo Wu,Jingguang Li,Chunlin Tian,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: FedPruner是一种创新的联邦微调范式，通过智能层剪枝解决隐私保护LLM适配中的高内存成本问题，在降低75%峰值内存使用的同时提升模型精度1.98%。


<details>
  <summary>Details</summary>
Motivation: 联邦微调虽然能保护隐私，但其高内存成本限制了资源受限设备的参与，需要一种方法在保持性能的同时降低内存需求。

Method: 采用宏观-微观协同剪枝框架：宏观功能驱动的层编排机制对层进行分组，微观重要性感知的层选择策略在组内剪枝构建设备特定子模型，并提供细粒度变体独立剪枝多头注意力和前馈网络组件。

Result: 实验结果表明FedPruner显著优于最先进方法，平均模型精度提升1.98%，同时峰值内存使用减少75%。

Conclusion: FedPruner通过智能层剪枝有效解决了联邦微调中的内存限制问题，为资源受限设备参与隐私保护的LLM适配提供了可行方案。

Abstract: Federated fine-tuning enables privacy-preserving Large Language Model (LLM)
adaptation, but its high memory cost limits participation from
resource-constrained devices. We propose FedPruner, an innovative federated
fine-tuning paradigm that tackles this via intelligent layer pruning. FedPruner
flexibly prunes the global model, creating personalized submodels based on
device memory constraints. It employs a macro-micro synergistic pruning
framework: a macro-level functionality-driven layer orchestration mechanism
groups layers, while a micro-level importance-aware layer selection strategy
prunes within groups to build device-specific submodels. We further introduce a
fine-grained variant that independently prunes Multi-Head Attention and
Feed-Forward Network components to precisely preserve critical architectural
elements. Extensive experimental results demonstrate that FedPruner
significantly outperforms state-of-the-art approaches, achieving up to a 1.98\%
improvement in average model accuracy while reducing peak memory usage by 75\%.

</details>


### [170] [TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving](https://arxiv.org/abs/2508.17219)
*Bingyang Wu,Zili Zhang,Yinmin Zhong,Guanzhe Huang,Yibo Zhu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: TokenLake是一个统一的段级前缀缓存池系统，通过声明式缓存接口和负载均衡算法，解决了现有前缀缓存系统的负载不均衡、数据冗余和内存碎片问题，显著提升了吞吐量和命中率。


<details>
  <summary>Details</summary>
Motivation: 现有前缀缓存系统与请求调度紧密耦合，导致跨实例的缓存系统出现负载不均衡、数据冗余和内存碎片等问题，无法实现低延迟的内存池化。

Method: 提出TokenLake系统，使用声明式缓存接口暴露查询张量、前缀缓存和缓存感知操作，采用基于热点感知的负载均衡算法进行段级缓存管理，并透明地最小化通信量。

Result: 在真实工作负载评估中，相比最先进的缓存感知路由和缓存中心化PD解耦方案，TokenLake可将吞吐量分别提升2.6倍和2.0倍，命中率分别提升2.0倍和2.1倍。

Conclusion: TokenLake通过统一的段级前缀缓存池实现了更好的缓存负载均衡、去重和碎片整理，使调度器能够专注于计算优化而不必考虑前缀缓存管理。

Abstract: Prefix caching is crucial to accelerate multi-turn interactions and requests
with shared prefixes. At the cluster level, existing prefix caching systems are
tightly coupled with request scheduling to optimize cache efficiency and
computation performance together, leading to load imbalance, data redundancy,
and memory fragmentation of caching systems across instances. To address these
issues, memory pooling is promising to shield the scheduler from the underlying
cache management so that it can focus on the computation optimization. However,
because existing prefix caching systems only transfer increasingly longer
prefix caches between instances, they cannot achieve low-latency memory
pooling.
  To address these problems, we propose a unified segment-level prefix cache
pool, TokenLake. It uses a declarative cache interface to expose requests'
query tensors, prefix caches, and cache-aware operations to TokenLake for
efficient pooling. Powered by this abstraction, TokenLake can manage prefix
cache at the segment level with a heavy-hitter-aware load balancing algorithm
to achieve better cache load balance, deduplication, and defragmentation.
TokenLake also transparently minimizes the communication volume of query
tensors and new caches. Based on TokenLake, the scheduler can schedule requests
elastically by using existing techniques without considering prefix cache
management. Evaluations on real-world workloads show that TokenLake can improve
throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by
2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing
and cache-centric PD-disaggregation solutions, respectively.

</details>


### [171] [Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality](https://arxiv.org/abs/2508.17311)
*Daniele De Sensi,Saverio Pasqualoni,Lorenzo Piarulli,Tommaso Bonato,Seydou Ba,Matteo Turisini,Jens Domke,Torsten Hoefler*

Main category: cs.DC

TL;DR: Bine树是一种改进通信局部性的集合算法家族，在保持通用性的同时减少全局链路流量达33%，在多种大规模超级计算机上实现最高5倍加速


<details>
  <summary>Details</summary>
Motivation: 大规模HPC系统中通信局部性对集合操作性能至关重要，特别是在内部全连接但全局连接稀疏的过载网络中

Method: 提出Bine（二项式负二进制）树算法家族，基于二项式树和蝶形结构改进通信局部性

Result: 在四种大规模超级计算机（Dragonfly、Dragonfly+、过载胖树和环面拓扑）上实现8种Bine集合操作，获得最高5倍加速和一致的全局链路流量减少

Conclusion: Bine树算法能有效提升大规模HPC系统中集合操作的性能，显著减少全局通信流量

Abstract: Communication locality plays a key role in the performance of collective
operations on large HPC systems, especially on oversubscribed networks where
groups of nodes are fully connected internally but sparsely linked through
global connections. We present Bine (binomial negabinary) trees, a family of
collective algorithms that improve communication locality. Bine trees maintain
the generality of binomial trees and butterflies while cutting global-link
traffic by up to 33%. We implement eight Bine-based collectives and evaluate
them on four large-scale supercomputers with Dragonfly, Dragonfly+,
oversubscribed fat-tree, and torus topologies, achieving up to 5x speedups and
consistent reductions in global-link traffic across different vector sizes and
node counts.

</details>


### [172] [Easy Acceleration with Distributed Arrays](https://arxiv.org/abs/2508.17493)
*Jeremy Kepner,Chansup Byun,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel Burrill,Vijay Gadepally,Ryan Haney,Michael Houle,Matthew Hubbell,Hayden Jananthan,Michael Jones,Piotr Luszczek,Lauren Milechin,Guillermo Morales,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Peter Michaleas*

Main category: cs.DC

TL;DR: 分布式数组通过STREAM内存带宽测试在多种硬件上展现了突出的维度扩展性，在百个节点上达到超1PB/s的持续带宽


<details>
  <summary>Details</summary>
Motivation: 探索高级编程语言和GPU加速器如何通过分布式数组抽象实现维度扩展性和生产力的平衡

Method: 使用STREAM内存带宽测试测试分布式数组在不同硬件上的性能，包括CPU核心、CPU节点、GPU节点的垂直、水平和时间维度扩展

Result: 在多个节点上实现线性扩展，在百个MIT SuperCloud节点上达到>1PB/s的持续带宽；硬件带宽在20年内提升10-100倍，GPU在5年内提升5倍

Conclusion: 分布式数组是一种高效的抽象，能够通过数据局部性实现高效并行处理，在多种硬件平台上都能获得良好的扩展性和性能表现

Abstract: High level programming languages and GPU accelerators are powerful enablers
for a wide range of applications. Achieving scalable vertical (within a compute
node), horizontal (across compute nodes), and temporal (over different
generations of hardware) performance while retaining productivity requires
effective abstractions. Distributed arrays are one such abstraction that
enables high level programming to achieve highly scalable performance.
Distributed arrays achieve this performance by deriving parallelism from data
locality, which naturally leads to high memory bandwidth efficiency. This paper
explores distributed array performance using the STREAM memory bandwidth
benchmark on a variety of hardware. Scalable performance is demonstrated within
and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across
multiple nodes was linear. The hardware used spans decades and allows a direct
comparison of hardware improvements for memory bandwidth over this time range;
showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in
CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5
years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a
sustained bandwidth $>$1 PB/s.

</details>


### [173] [Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD NPUs](https://arxiv.org/abs/2508.17593)
*Aadesh Deshmukh,Venkata Yaswanth Raparti,Samuel Hsu*

Main category: cs.DC

TL;DR: Zen-Attention框架通过系统优化注意力层的DRAM带宽利用，在AMD XDNA NPU上实现高达4倍的注意力块延迟提升和32%的端到端网络延迟改进


<details>
  <summary>Details</summary>
Motivation: Transformer模型在能耗和DRAM带宽受限设备上的部署面临延迟挑战，需要高效映射动态注意力层到NPU以提升性能功耗比

Method: 通过系统探索层折叠、分块、互连数据移动和张量布局的复杂设计空间，优化注意力层的DRAM带宽利用率

Result: 在代表性Transformer模型中，注意力块延迟提升高达4倍，端到端网络延迟提升高达32%

Conclusion: Zen-Attention框架有效解决了NPU上注意力层映射的工程挑战，显著提升了性能功耗比

Abstract: Transformer-based deep learning models are increasingly deployed on energy,
and DRAM bandwidth constrained devices such as laptops and gaming consoles,
which presents significant challenges in meeting the latency requirements of
the models. The industry is turning to neural processing units (NPUs) for
superior performance-per-watt (perf/watt); however, efficiently mapping dynamic
attention layers to the NPUs remains a challenging task. For optimizing
perf/watt, AMD XDNA NPUs employ software managed caches and share system memory
with host. This requires substantial engineering effort to unlock efficient
tiling, buffer allocation, and data movement to extract the maximum efficiency
from the device. This paper introduces Zen-Attention, a framework that
optimizes DRAM bandwidth utilization in the attention layer of models by
systematically exploring the complex design space of layer folding, tiling, and
data-movement on the interconnect, and the tensor layouts to come up with an
optimal solution. Our evaluation includes comparative analysis of end-to-end
model latency and specific attention latency in each model. We demonstrate how
the framework enhances mapping capabilities by varying input dimensions, which
require padding and masking in the attention block. For representative
transformer models, the Zen-Attention Framework achieves up to 4x improvement
in the latency of the attention block and up to 32% improvement in end-to-end
network latency compared to the baseline Unfolded- approaches.

</details>


### [174] [ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters at Scale](https://arxiv.org/abs/2508.17624)
*Ge Shi,Hanieh Sadri,Qian Wang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ExpertWeave是一个专门为ESFT适配器设计的服务系统，能够在单个MoE基础模型上同时服务多个专家专用微调模型，大幅减少内存占用并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: ESFT通过选择性微调MoE模型中任务相关的专家来提升性能，但现有服务系统无法有效支持这种专家导向的范式，导致资源消耗巨大。

Method: 系统采用虚拟内存辅助的专家权重管理器来共置基础模型和适配器专家，避免内存碎片化开销；使用融合内核进行批量重路由，实现运行时轻量级令牌重定向。

Result: 在单个加速器上可同时服务16B MoE模型的多个适配器，提供高达94倍的KV缓存容量和18%的吞吐量提升，资源使用相当且不损失模型精度。

Conclusion: ExpertWeave能够高效服务多个ESFT适配器，扩展至20个适配器时仅增加4-11%的延迟，相比单独服务基础模型具有显著优势。

Abstract: Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large
language models to enhance their task-specific performance by selectively
tuning the top-activated experts for the task. Serving these fine-tuned models
at scale is challenging: deploying merged models in isolation is prohibitively
resource-hungry, while existing multi-adapter serving systems with LoRA-style
additive updates are incompatible with ESFT's expert-oriented paradigm. We
present ExpertWeave, a system that serves multiple ESFT adapters concurrently
over a single shared MoE base model, drastically reducing the memory footprint
and improving resource utilization. To seamlessly integrate into existing
inference pipelines for MoE models with non-intrusive modifications and minimal
latency overhead, ExpertWeave introduces a virtual-memory-assisted expert
weight manager that co-locates base-model and adapter experts without incurring
memory overhead from fragmentation, and a fused kernel for batched rerouting to
enable lightweight redirection of tokens to the appropriate experts at runtime.
Our evaluations show that ExpertWeave can simultaneously serve multiple
adapters of a 16B MoE model on a single accelerator where the baseline runs out
of memory, or provides up to 94x more KV cache capacity and achieves up to 18%
higher throughput while using comparable resources, all without compromising
model accuracy. ExpertWeave maintains low overhead even when scaling to 20
adapters, with a 4-11% latency increase compared with serving the base model
alone. Source code will be released soon.

</details>


### [175] [Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture](https://arxiv.org/abs/2508.17814)
*Anderson de Lima Luiz,Shubham Vijay Kurlekar,Munir Georges*

Main category: cs.DC

TL;DR: 基于SLURM的HPC架构，通过动态资源调度和容器化微服务，实现了异构大语言模型的可扩展推理引擎，在多节点集群中高效管理CPU、GPU和内存分配。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模HPC基础设施上部署异构大语言模型时的资源管理、扩展性和性能优化挑战，提供高效、响应快速且容错的LLM推理解决方案。

Method: 采用SLURM资源管理系统，通过动态资源调度和容器化微服务技术，构建REST API端点支持单次和批量推理，包括多步法庭精炼等高级工作流程。

Result: 小模型（Llama 3.2 1B/3B）可处理128个并发请求，延迟低于50ms；大模型（Llama 3.1 8B/70B）在2个并发用户时就达到饱和，延迟超过2秒。容器和调度活动的开销最小，方案在批处理和交互式场景中都能可靠扩展。

Conclusion: 该架构能够在大规模HPC基础设施上实现高效、响应快速且容错的LLM推理，为部署聊天机器人等实际应用场景提供了灵活性和稳健性支撑，为更高效的大模型推理抓基了基础。

Abstract: This work elaborates on a High performance computing (HPC) architecture based
on Simple Linux Utility for Resource Management (SLURM) [1] for deploying
heterogeneous Large Language Models (LLMs) into a scalable inference engine.
Dynamic resource scheduling and seamless integration of containerized
microservices have been leveraged herein to manage CPU, GPU, and memory
allocations efficiently in multi-node clusters. Extensive experiments, using
Llama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe
throughput, latency, and concurrency and show that small models can handle up
to 128 concurrent requests at sub-50 ms latency, while for larger models,
saturation happens with as few as two concurrent users, with a latency of more
than 2 seconds. This architecture includes Representational State Transfer
Application Programming Interfaces (REST APIs) [4] endpoints for single and
bulk inferences, as well as advanced workflows such as multi-step "tribunal"
refinement. Experimental results confirm minimal overhead from container and
scheduling activities and show that the approach scales reliably both for batch
and interactive settings. We further illustrate real-world scenarios, including
the deployment of chatbots with retrievalaugmented generation, which helps to
demonstrate the flexibility and robustness of the architecture. The obtained
results pave ways for significantly more efficient, responsive, and
fault-tolerant LLM inference on large-scale HPC infrastructures.

</details>


### [176] [Wait-free Replicated Data Types and Fair Reconciliation](https://arxiv.org/abs/2508.18193)
*Petr Kuznetsov,Maxence Perion,Sara Tucci-Piergiovanni*

Main category: cs.DC

TL;DR: 本文提出了一个基于DAG的复制数据框架，解决最终一致性系统中的操作撤销和客户端饥饿问题，确保稳定收敛和公平进展


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，强一致性和强可用性无法同时实现（CAP理论），通常采用最终一致性来获得无等待访问。但这种方法面临操作被不断撤销和客户端可能饥饿的问题

Method: 设计基于有向无环图（DAG）的通用复制框架，副本通过交换本地视图并使用协调函数进行合并。每个副本维护客户端操作的增长序列

Result: 实现了无等待最终一致性复制状态机，保证所有副本共享单调增长的稳定操作前缀，并且没有客户端会饥饿

Conclusion: 该框架成功解决了最终一致性系统中的两个关键挑战，为构建既高效又公平的分布式系统提供了理论基础和实践方法

Abstract: Replication is a standard way to maintain availability of shared data in
fault-prone distributed systems. To make sure that the data replicas are
up-to-date, they need to synchronize, which typically means engaging the
replicas in waiting for coherent responses from each other. The amount of
waiting depends on the consistency and availability guarantees we impose on the
system. The folklore CAP theory states that strong consistency (the set of
replicas create an illusion of one correct server) and strong availability (the
replicas' states are reachable despite network partitions) cannot be
implemented in the same system. A popular way to deal with this impossibility
is to relax consistency to be only eventual: the replicas eventually converge
to the same state. In return, the replicas can be wait-free, i.e., the clients
can get the data from the closest replica without waiting for other ones.
  Wait-free data replication faces two important challenges. First, the
operations issued by the clients may be constantly revoked, i.e., their effects
can be repeatedly recomputed due to asynchrony and concurrency. Second, even if
some operations eventually stabilize in their effects, a particular client may
still experience starvation if, from some point onward, each of its operations
is later revoked. In this paper, we address these challenges through a general
DAG-based framework for replicated data types, where replicas exchange their
local views and merge them using a reconciliation function. Within this
framework, we design reconciliation functions that implement a wait-free
eventually consistent replicated state machine ensuring both stable convergence
and fair progress. Specifically, every replica maintains a growing sequence of
client operations, and we guarantee that: (1) all replicas share a common,
monotonically growing stable prefix of operations, and (2) no client starves.

</details>


### [177] [Practical GPU Choices for Earth Observation: ResNet-50 Training Throughput on Integrated, Laptop, and Cloud Accelerators](https://arxiv.org/abs/2508.18206)
*Ritvik Chaturvedi*

Main category: cs.DC

TL;DR: 基于ResNet的Sentinel-2影像土地利用分类系统，在三种GPU上实现2倍训练加速，保持高精度


<details>
  <summary>Details</summary>
Motivation: 解决地理空间分析中深度学习模型在消费级和云GPU上的可扩展部署问题

Method: 采用ResNet架构，构建自动化数据获取、预处理、分块、训练和可视化的容器化流程

Result: 在NVIDIA RTX 3060和Tesla T4上相比Apple M3 Pro实现2倍训练加速，EuroSAT数据集上保持高分类精度

Conclusion: 证明了在消费级和免费云GPU上部署深度学习LULC模型进行可扩展地理空间分析的可行性

Abstract: This project implements a ResNet-based pipeline for land use and land cover
(LULC) classification on Sentinel-2 imagery, benchmarked across three
heterogeneous GPUs. The workflow automates data acquisition, geospatial
preprocessing, tiling, model training, and visualization, and is fully
containerized for reproducibility. Performance evaluation reveals up to a 2x
training speed-up on an NVIDIA RTX 3060 and a Tesla T4 compared to the Apple M3
Pro baseline, while maintaining high classification accuracy on the EuroSAT
dataset. These results demonstrate the feasibility of deploying deep learning
LULC models on consumer and free cloud GPUs for scalable geospatial analytics.

</details>


### [178] [Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel](https://arxiv.org/abs/2508.18224)
*Ran Yan,Youhe Jiang,Binhang Yuan*

Main category: cs.DC

TL;DR: Flash Sparse Attention (FSA) 是一种新的稀疏注意力核设计，解决了现有NSA方法在小GQA组大小下的效率限制，在多种现代LLM上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Native Sparse Attention (NSA) 方法在大GQA组大小时效率很高，但现代LLM通常采用较小的GQA组，这限制了NSA的适用性。需要一种能在小GQA组下保持高效的新方法。

Method: 提出了Flash Sparse Attention (FSA)，采用替代的核设计，使NSA计算能够在现代GPU上对各种小GQA组大小的流行LLM保持高效。

Result: FSA相比原始NSA实现了：1) 核级延迟降低最高3.5倍，平均1.6倍；2) 端到端训练加速最高1.25倍，平均1.09倍；3) 端到端预填充加速最高1.36倍，平均1.11倍。

Conclusion: FSA成功扩展了稀疏注意力技术的适用范围，使其能够在现代LLM常用的较小GQA组配置下保持高效性能，为长上下文训练和推理提供了实用的解决方案。

Abstract: Recent progress in sparse attention mechanisms has demonstrated strong
potential for reducing the computational cost of long-context training and
inference in large language models (LLMs). Native Sparse Attention (NSA), a
state-of-the-art approach, introduces natively trainable, hardware-aligned
sparse attention that delivers substantial system-level performance gains while
maintaining accuracy comparable to full attention. However, the kernel
implementation of NSA relies on a query-grouping strategy that is efficient
only with large Grouped Query Attention (GQA) sizes, whereas modern LLMs
typically adopt much smaller GQA groups, which limits the applicability of this
sparse algorithmic advance. In this work, we propose Flash Sparse Attention
(FSA), which includes an alternative kernel design that enables efficient NSA
computation across a wide range of popular LLMs with varied smaller GQA group
sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our
empirical evaluation demonstrates that FSA achieves (i) up to 3.5$\times$ and
on average 1.6$\times$ kernel-level latency reduction, (ii) up to 1.25$\times$
and 1.09$\times$ on average end-to-end training speedup on state-of-the-art
LLMs, and (iii) up to 1.36$\times$ and 1.11$\times$ on average end-to-end
prefill speedup on state-of-the-art LLMs. The source code is open-sourced and
publicly available at
https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [179] [TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training and Inference on Hopper](https://arxiv.org/abs/2508.16584)
*Zhongling Su,Rong Fu,Weihan Cao,Jianfei Gao,Minxi Jin,Zhilin Pei,Hui Wang*

Main category: cs.AR

TL;DR: 通过动态TMA描述符池和双阶段读写操作，消除FP8分组GEMM中的填充开销，实现更高效的内存使用和计算性能


<details>
  <summary>Details</summary>
Motivation: 现有FP8分组GEMM实现需要将每个分组填充到固定对齐（如128），导致内存和计算开销

Method: 使用TMA描述符池处理变长分组维度，通过动态运行时选择和双阶段读写操作，同时满足16字节全局内存对齐和128字节共享内存对齐要求

Result: 实验显示获得1.7%到20.4%的速度提升，内存减少达23.8%，同时保持完全数值等效性

Conclusion: TMA-Adaptive FP8 Grouped GEMM方案通过消除填充开销，在保持数值准确性的前提下显著提升了计算效率和内存利用率

Abstract: Current FP8 grouped GEMM implementations require padding each group to a
fixed alignment (e.g., 128), incurring memory and computational overhead. We
propose \textit{TMA-Adaptive FP8 Grouped GEMM}, which eliminates padding by
dynamically adapting to variable group dimensions via (1) a TMA descriptor pool
with $\log_2(block_M)$ preconfigured descriptors to handle all residual row
cases through dynamic runtime selection and dual-phase load-store operations,
achieving comprehensive coverage with minimal overhead, and (2)
TMA-alignment-aware management to satisfy 16-byte global memory alignment and
128-byte shared memory alignment. Experiments demonstrate 1.7\% to 20.4\% speed
up with up to 23.8\% memory reduction compared to padding operation plus
state-of-the-art FP8 grouped GEMM, while maintaining full numerical equivalence
for valid data. The source code is publicly available at an anonymous
repository: https://github.com/sukoncon/TMA-Adaptive-FP8-Grouped-GEMM.

</details>


### [180] [GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model](https://arxiv.org/abs/2508.16700)
*Deepak Kumar,Divakar Yadav,Yash Patel*

Main category: cs.AR

TL;DR: GPT-OSS-20B（MoE模型）在单GPU H100上相比密集模型Qwen3-32B和Yi-34B，在解码吞吐量、能效和显存使用方面表现更优，但首token延迟较高


<details>
  <summary>Details</summary>
Motivation: 评估混合专家（MoE）模型在单GPU部署场景下的实际性能优势，特别是在解码吞吐量、能效和显存使用效率方面与密集模型的对比

Method: 在单GPU（H100，bf16）环境下，使用一致的nvidia-smi采样器测量多个性能指标：首token时间（TTFT）、完整解码吞吐量（TPOT）、端到端延迟百分位数、包含历史键值的峰值显存使用以及能耗

Result: GPT-OSS-20B在2048token上下文和64token解码设置下，解码吞吐量比Qwen3-32B高31.8%，每1000个生成token能耗低25.8%，峰值显存使用减少31.7%。虽然首token延迟因MoE路由开销而较高，但按活跃参数计算的效率（APE）显著更强

Conclusion: MoE架构在单GPU部署中具有明显优势，通过仅激活17.3%的参数（3.61B/20.9B）实现了更高的解码吞吐量和更好的能效，证明了MoE模型在实际部署中的价值

Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B
(Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines
Qwen3-32B and Yi-34B across multiple dimensions. We measure true
time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency
percentiles, peak VRAM with past key values (PKV) held, and energy via a
consistent nvidia-smi-based sampler. At a 2048-token context with 64-token
decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than
dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM
and energy per 1000 generated tokens; its TTFT is higher due to MoE routing
overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B
provides about 31.8% higher decode throughput and 25.8% lower energy per 1000
generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM.
Normalized by active parameters, GPT-OSS-20B shows markedly stronger
per-active-parameter efficiency (APE), underscoring MoE's deployment
advantages. We do not evaluate accuracy; this is a deployment-focused study. We
release code and consolidated results to enable replication and extension.

</details>


### [181] [zkPHIRE: A Programmable Accelerator for ZKPs over HIgh-degRee, Expressive Gates](https://arxiv.org/abs/2508.16738)
*Alhad Daftardar,Jianqiao Mo,Joey Ah-kiow,Benedikt Bünz,Siddharth Garg,Brandon Reagen*

Main category: cs.AR

TL;DR: 这篇论文提出了zkPHIRE加速器，通过专门的SumCheck加速单元高效处理复杂的高次门，实现了过CPU 1000倍和相比最新技术11.87倍的性能提升，是首个能处理2^30规模问题的可编程零知证明加速器。


<details>
  <summary>Details</summary>
Motivation: 零知证明(ZKPs)虽然在安全和隐私保护计算方面具有巨大潜力，但由于证明生成阶段的计算开销极高，导致实际部署受限。现代ZKP系统中处理复杂高次门的SumCheck协议是一个特别的挑战。

Method: 设计了一种新题的可编程加速器，通过SumCheck协议高效处理任意自定义门。将这个加速单元集成到完整系统加速器zkPHIRE中。

Result: 加速器在各种门类型上实现了过CPU 1000倍的几何均值加速比。zkPHIRE在同面积下达到过CPU 1486倍和过最新技术11.87倍的加速比。这是首个能处理2^30规模问题的可编程零知证明加速器。

Conclusion: zkPHIRE通过专门的SumCheck加速单元有效解决了现代ZKP系统中处理复杂高次门的挑战，实现了重大性能提升，为零知证明技术的广泛部署提供了重要支撑。

Abstract: Zero-Knowledge Proofs (ZKPs) have emerged as powerful tools for secure and
privacy-preserving computation. ZKPs enable one party to convince another of a
statement's validity without revealing anything else. This capability has
profound implications in many domains, including: machine learning, blockchain,
image authentication, and electronic voting. Despite their potential, ZKPs have
seen limited deployment because of their exceptionally high computational
overhead, which manifests primarily during proof generation. To mitigate these
overheads, a (growing) body of researchers has proposed hardware accelerators
and GPU implementations for kernels and complete protocols. Prior art spans a
wide variety of ZKP schemes that vary significantly in computational overhead,
proof size, verifier cost, protocol setup, and trust. The latest, and widely
used ZKP protocols are intentionally designed to balance these trade-offs. A
particular challenge in modern ZKP systems is supporting complex, high-degree
gates using the SumCheck protocol. We address this challenge with a novel
programmable accelerator that efficiently handles arbitrary custom gates via
SumCheck. Our accelerator achieves upwards of $1000\times$ geomean speedup over
CPU-based SumChecks across a range of gate types. We integrate this unit into a
full-system accelerator, zkPHIRE, which achieves $1486\times$ geomean speedup
over CPU and $11.87\times$ speedup over the state-of-the-art at iso-area.
zkPHIRE is the first accelerator to scale to problem sizes of $2^{30}$ nominal
constraints while maintaining small proof sizes and programmability.

</details>


### [182] [X-HEEP: An Open-Source, Configurable and Extendible RISC-V Platform for TinyAI Applications](https://arxiv.org/abs/2508.16959)
*Simone Machetti,Pasquale Davide Schiavone,Giovanni Ansaloni,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: X-HEEP是一个开源的、可配置的RISC-V超低功耗边缘计算平台，具有可扩展加速器接口，支持多种开发流程，在65nm工艺下实现0.15mm²面积和29μW漏电功耗，与近内存加速器集成可获得7.3倍性能提升和3.6倍能效改善。


<details>
  <summary>Details</summary>
Motivation: 为超低功耗边缘AI应用（TinyAI）开发一个开源、可配置且可扩展的RISC-V平台，解决传统平台在加速器集成和配置灵活性方面的不足。

Method: 设计eXtendible Accelerator InterFace (XAIF)实现无缝加速器集成，提供核心、内存、总线和外设的广泛内部配置，支持FPGA原型设计、ASIC实现和混合SystemC-RTL建模等多种开发流程。

Result: 在TSMC 65nm CMOS工艺下实现300MHz@0.8V，仅0.15mm²面积和29μW漏电功耗。与近内存加速器集成后，相比纯CPU执行获得7.3倍性能加速和3.6倍能效提升。

Conclusion: X-HEEP作为一个高度可配置的低功耗平台，成功展示了其在边缘AI应用中的有效性，为异构系统集成提供了优秀的宿主平台解决方案。

Abstract: In this work, we present X-HEEP, an open-source, configurable, and extendible
RISC-V platform for ultra-low-power edge applications (TinyAI). X-HEEP features
the eXtendible Accelerator InterFace (XAIF), which enables seamless integration
of accelerators with varying requirements along with an extensive internal
configuration of cores, memory, bus, and peripherals. Moreover, it supports
various development flows, including FPGA prototyping, ASIC implementation, and
mixed SystemC-RTL modeling, enabling efficient exploration and optimization.
Implemented in TSMC's 65 nm CMOS technology (300 MHz, 0.8 V), X-HEEP achieves a
minimal footprint of only 0.15 mm2 and consumes just 29 uW of leakage power. As
a demonstrator of the configurability and low overhead of X-HEEP as a host
platform, we present a study integrating it with near-memory accelerators
targeting early-exit dynamic network applications, achieving up to 7.3 x
performance speedup and 3.6 x energy improvement on the resulting heterogeneous
system compared to CPU-only execution.

</details>


### [183] [Invited Paper: FEMU: An Open-Source and Configurable Emulation Framework for Prototyping TinyAI Heterogeneous Systems](https://arxiv.org/abs/2508.16981)
*Simone Machetti,Deniz Kasap,Juan Sapriza,Rubén Rodríguez Álvarez,Hossein Taji,José Miranda,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: FEMU是一个开源的FPGA仿真框架，用于快速原型设计和评估TinyAI异构系统，结合了可重构硬件区域和软件控制区域。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够快速原型设计和评估TinyAI异构系统的框架，利用SoC FPGA的能力来整合硬件和软件组件。

Method: 开发了FEMU框架，在SoC FPGA上实现可重构硬件区域用于硬件原型设计，控制软件区域运行标准操作系统进行监控和通信。通过X-HEEP-FEMU平台实例化该框架，集成X-HEEP主机、Linux Python环境和能量模型。

Result: 成功构建了X-HEEP-FEMU平台，部署在Xilinx Zynq-7020 SoC上，集成了硬件主机、软件环境和基于硅实现的能量模型。

Conclusion: FEMU提供了一个有效的框架，能够加速TinyAI异构系统的开发和评估，通过硬件软件协同仿真实现快速原型设计。

Abstract: In this paper, we present the new FPGA EMUlation (FEMU), an open-source and
configurable emulation framework for prototyping and evaluating TinyAI
heterogeneous systems (HS). FEMU leverages the capability of system-on-chip
(SoC)-based FPGAs to combine the under-development HS implemented in a
reconfigurable hardware region (RH) for quick prototyping with a software
environment running under a standard operating system in a control software
region (CS) for supervision and communication. To evaluate our approach, we
built the X-HEEP FPGA EMUlation (X-HEEP-FEMU) platform by instantiating the
proposed framework with real-world hardware and software components.
X-HEEP-FEMU is deployed on the Xilinx Zynq-7020 SoC and integrates the
eXtendible Heterogeneous Energy Efficient Platform (X-HEEP) host in the RH, a
Linux-based Python environment on the ARM Cortex-A9 CS, and energy models
derived from a TSMC 65 nm CMOS silicon implementation of X-HEEP, called
HEEPocrates.

</details>


### [184] [Optimizing Neural Networks with Learnable Non-Linear Activation Functions via Lookup-Based FPGA Acceleration](https://arxiv.org/abs/2508.17069)
*Mengyuan Yin,Benjamin Chen Ming Choong,Chuping Qu,Rick Siow Mong Goh,Weng-Fai Wong,Tao Luo*

Main category: cs.AR

TL;DR: 通过FPGA可重构查找表架构，解决学习激活函数在边缘AI中的计算复杂性问题，实现了超高10^4倍的能消效率提升


<details>
  <summary>Details</summary>
Motivation: 学习激活函数在KAN等模型中具有更高准确性和可解释性，但计算复杂性导致在能量受限的边缘AI部署中遇到挑战

Method: 采用可重构查找表架构，结合细粒度量化和适配性查找表，在FPGA上实现动态硬件专门化

Result: 在KAN模型中实现了超高计算速度和超10^4倍能消效率，同时保持相同的准确性和最小的资源占用

Conclusion: 该方法为能量关键的边缘AI提供了实用的解决方案，突破了传统上适配性激活网络在计算强度和功耗限制下无法部署的问题

Abstract: Learned activation functions in models like Kolmogorov-Arnold Networks (KANs)
outperform fixed-activation architectures in terms of accuracy and
interpretability; however, their computational complexity poses critical
challenges for energy-constrained edge AI deployments. Conventional CPUs/GPUs
incur prohibitive latency and power costs when evaluating higher order
activations, limiting deployability under ultra-tight energy budgets. We
address this via a reconfigurable lookup architecture with edge FPGAs. By
coupling fine-grained quantization with adaptive lookup tables, our design
minimizes energy-intensive arithmetic operations while preserving activation
fidelity. FPGA reconfigurability enables dynamic hardware specialization for
learned functions, a key advantage for edge systems that require
post-deployment adaptability. Evaluations using KANs - where unique activation
functions play a critical role - demonstrate that our FPGA-based design
achieves superior computational speed and over $10^4$ times higher energy
efficiency compared to edge CPUs and GPUs, while maintaining matching accuracy
and minimal footprint overhead. This breakthrough positions our approach as a
practical enabler for energy-critical edge AI, where computational intensity
and power constraints traditionally preclude the use of adaptive activation
networks.

</details>


### [185] [A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations](https://arxiv.org/abs/2508.17562)
*Shota Konno,Che-Kai Liu,Sigang Ryu,Samuel Spetalnick,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: 提出了一种28nm 6T-SRAM数字/模拟混合存内计算宏，支持复数MAC运算，采用2D加权电容阵列实现高精度和低面积开销


<details>
  <summary>Details</summary>
Motivation: 传统存内计算架构在复数运算和精度方面存在限制，需要设计一种支持复数MAC操作且具有高精度、低面积开销的混合计算架构

Method: 采用数字/模拟混合配置，数字CIM仅应用于高位，模拟CIM应用于低位，引入2D加权电容阵列，无需输入DAC，通过单次转换输出复数的实部和虚部

Result: 实现了1.80 Mb/mm²的内存密度和0.435%的RMS误差，显著降低了延迟和面积开销

Conclusion: 该混合存内计算架构成功实现了高效的复数运算支持，在保持高精度的同时显著降低了硬件开销，为复数信号处理应用提供了有效的硬件解决方案

Abstract: A 28nm dense 6T-SRAM Digital(D)/Analog(A) Hybrid compute-in-memory (CIM)
macro supporting complex num-ber MAC operation is presented. By introducing a
2D-weighted Capacitor Array, a hybrid configuration is adopted where digital
CIM is applied only to the upper bits and ana-log CIM is applied to the rest,
without the need for input DACs resulting in improved accuracy and lower area
overhead. The CIM prototype macro achieves 1.80 Mb/mm2 memory density and
0.435% RMS error. Complex CIM unit outputs real and imaginary part with a
single conversion to reduce latency.

</details>


### [186] [In-Memory Computing Enabled Deep MIMO Detection to Support Ultra-Low-Latency Communications](https://arxiv.org/abs/2508.17820)
*Tingyu Ding,Qunsong Zeng,Kaibin Huang*

Main category: cs.AR

TL;DR: 基于内存计算技术的深度展开MIMO检测器，通过通道依赖/独立模块分离设计和专用训练方法，实现纳秒级延迟和高检测精度，满满6G网络的极端性能要求。


<details>
  <summary>Details</summary>
Motivation: 为满满6G网络的极端延迟和可靠性要求，需要软硬件协同设计的高效MIMO检测方案。传统的深度展开检测器虽然准确性高，但运算延迟仍不能满6G的0.1毫秒标准，需要利用内存计算技术来实现纳秒级矩阵运算。

Method: 提出了深度内存MIMO(IM-MIMO)检测器：1)将流水线计算模块分解为通道依赖和通道独立的神经网络模块，减少存储器重编程延迟；2)发展了利用存储器值统计知识的专用训练方法，提高对编程噪声的耐受性。

Result: 对IM-MIMO检测器进行了全面性能分析，评估了检测准确性、处理延迟和硬件复杂度。量化分析了检测误差与通道噪声、存储器编程噪声和神经网络大小等因素的函数关系。

Conclusion: 该研究提出的IM-MIMO检测器通过软硬件协同设计，在保持高检测准确性的同时实现了纳秒级延迟，为6G网络的极端延迟要求提供了可行的解决方案。

Abstract: The development of sixth-generation (6G) mobile networks imposes
unprecedented latency and reliability demands on multiple-input multiple-output
(MIMO) communication systems, a key enabler of high-speed radio access.
Recently, deep unfolding-based detectors, which map iterative algorithms onto
neural network architectures, have emerged as a promising approach, combining
the strengths of model-driven and data-driven methods to achieve high detection
accuracy with relatively low complexity. However, algorithmic innovation alone
is insufficient; software-hardware co-design is essential to meet the extreme
latency requirements of 6G (i.e., 0.1 milliseconds). This motivates us to
propose leveraging in-memory computing, which is an analog computing technology
that integrates memory and computation within memristor circuits, to perform
the intensive matrix-vector multiplication (MVM) operations inherent in deep
MIMO detection at the nanosecond scale. Specifically, we introduce a novel
architecture, called the deep in-memory MIMO (IM-MIMO) detector, characterized
by two key features. First, each of its cascaded computational blocks is
decomposed into channel-dependent and channel-independent neural network
modules. Such a design minimizes the latency of memristor reprogramming in
response to channel variations, which significantly exceeds computation time.
Second, we develop a customized detector-training method that exploits prior
knowledge of memristor-value statistics to enhance robustness against
programming noise. Furthermore, we conduct a comprehensive analysis of the
IM-MIMO detector's performance, evaluating detection accuracy, processing
latency, and hardware complexity. Our study quantifies detection error as a
function of various factors, including channel noise, memristor programming
noise, and neural network size.

</details>


### [187] [LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow](https://arxiv.org/abs/2508.17826)
*Kaiyan Chang,Wenlong Zhu,Shengwen Liang,Huawei Li,Ying Wang*

Main category: cs.AR

TL;DR: LLMulator是一个基于大语言模型的性能预测框架，通过将性能值视为分类标记序列来处理数据流加速器的性能预测，支持跨架构和应用泛化，并引入强化学习动态校准处理输入相关控制流。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在架构、应用和输入相关控制流之间实现泛化，需要一种能够处理这些挑战的准确快速性能预测方法。

Method: 利用预训练大语言模型的程序语义知识，构建渐进数值建模框架，将性能值作为分类标记序列处理，采用强化学习动态校准方法和渐进数据增强策略。

Result: 动态校准方法比静态模型减少9.7%的周期预测误差，迭代后误差降至11.2%；数据增强策略显著提升了跨架构和配置的预测准确性。

Conclusion: LLMulator框架通过结合LLM语义知识和强化学习技术，实现了对数据流加速器性能的鲁棒预测，有效解决了跨架构泛化和输入相关控制流处理的挑战。

Abstract: Accurate and fast performance prediction for dataflow-based accelerators is
vital for efficient hardware design and design space exploration, yet existing
methods struggle to generalize across architectures, applications, and
input-dependent control flows. We present LLMulator, a progressive numeric
modeling framework leveraging the program semantic knowledge of pre-trained
large language models (LLMs) for robust, hardware- and application-aware
prediction. Our numeric model treats performance values as categorical token
sequences, enabling range-agnostic estimates and confidence-aware predictions
for unseen applications. To handle input-dependent control flows, we introduce
a reinforcement learning-based dynamic calibration method, reducing cycle
prediction error by 9.7% over static models and converging to 11.2% error after
a few iterations. For cross-hardware generalization, we develop a progressive
data augmentation strategy that generates diverse datasets covering multi-level
dataflow structures, memory parameters, and loop mapping primitives,
significantly boosting prediction accuracy across architectures and
configurations.

</details>


### [188] [Anatomy of the gem5 Simulator: AtomicSimpleCPU, TimingSimpleCPU, O3CPU, and Their Interaction with the Ruby Memory System](https://arxiv.org/abs/2508.18043)
*Johan Söderström,Yuan Yao*

Main category: cs.AR

TL;DR: 对gem5模拟器中三种主要CPU模型（AtomicSimpleCPU、TimingSimpleCPU、Out-of-order CPU）与内存子系统交互的解剖分析，使用轻量级分析器识别软件瓶颈，发现Ruby内存子系统在顺序CPU中占用最多时间，而O3 CPU主要时间用于指令实例构建和流水线阶段。


<details>
  <summary>Details</summary>
Motivation: gem5作为流行的计算机系统模拟器，存在模拟时间长和学习曲线陡峭的问题，需要深入分析其CPU模型与内存子系统的交互，以识别性能瓶颈并为优化提供基础。

Method: 使用基于Linux perf_event接口的轻量级分析器，对三种CPU模型在各种基准测试中进行性能分析，详细检查函数调用链和模拟硬件层的时间分配。

Result: 顺序CPU（AS和TS）中Ruby内存子系统在执行时间中占比最大，主要是在指令获取阶段；而O3 CPU在Ruby中花费时间相对较少，主要时间用于指令实例构建和CPU流水线阶段。

Conclusion: 提供了有价值的CPU执行流程解剖视图，可用于教育目的和性能优化，分析框架可扩展到其他gem5组件或新模型的开发评估。

Abstract: gem5 is a popular modular-based computer system simulator, widely used in
computer architecture research and known for its long simulation time and steep
learning curve. This report examines its three major CPU models: the
AtomicSimpleCPU (AS CPU), the TimingSimpleCPU (TS CPU), the Out-of-order (O3)
CPU, and their interactions with the memory subsystem. We provide a detailed
anatomical overview of each CPU's function call-chains and present how gem5
partitions its execution time for each simulated hardware layer.
  We perform our analysis using a lightweight profiler built on Linux's
perf_event interface, with user-configurable options to target specific
functions and examine their interactions in detail. By profiling each CPU
across a wide selection of benchmarks, we identify their software bottlenecks.
Our results show that the Ruby memory subsystem consistently accounts for the
largest share of execution time in the sequential AS and TS CPUs, primarily
during the instruction fetch stage. In contrast, the O3 CPU spends a relatively
smaller fraction of time in Ruby, with most of its time devoted to constructing
instruction instances and the various pipeline stages of the CPU.
  We believe that the anatomical view of each CPU's execution flow is valuable
for educational purposes, as it clearly illustrates the interactions among
simulated components. These insights form a foundation for optimizing gem5's
performance, particularly for the AS, TS, and O3 CPUs. Moreover, our framework
can be readily applied to analyze other gem5 components or to develop and
evaluate new models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [189] [TSPC-PFD: TSPC-Based Low-Power High-Resolution CMOS Phase Frequency Detector](https://arxiv.org/abs/2508.16933)
*Dhandeep Challagundla,Venkata Krishna Vamsi Sundarapu,Ignatius Bezzam,Riadul Islam*

Main category: cs.ET

TL;DR: 这篇论文提出了一种新的低功耗TSPC基相位频率检测器（PFD）设计，完全消除了盲区并将死区降至40ps，在3GHz频率下功耗仅4.41uW。


<details>
  <summary>Details</summary>
Motivation: 传统PFD设计存在显著的死区和盲区问题，影响相位检测精度并增加高速应用中的投弄。

Method: 提出一种新的低功耗True Single-Phase Clock (TSPC)基础PFD设计，采用TSMC 28nm技术实现。

Result: 实现了完全消除盲区，死区仅为40ps，在3GHz输入频率下功耗为4.41uW，布局面积10.42μm²。

Conclusion: 该新型PFD设计有效解决了传统PFD的性能限制，在保持低功耗的同时提高了相位检测精度。

Abstract: Phase Frequency Detectors (PFDs) are essential components in Phase-Locked
Loop (PLL) and Delay-Locked Loop (DLL) systems, responsible for comparing phase
and frequency differences and generating up/down signals to regulate charge
pumps and/or, consequently, Voltage-Controlled Oscillators (VCOs). Conventional
PFD designs often suffer from significant dead zones and blind zones, which
degrade phase detection accuracy and increase jitter in high-speed
applications. This paper addresses PFD design challenges and presents a novel
low-power True Single-Phase Clock (TSPC)-based PFD. The proposed design
eliminates the blind zone entirely while achieving a minimal dead zone of 40
ps. The proposed PFD, implemented using TSMC 28 nm technology, demonstrates a
low-power consumption of 4.41 uW at 3 GHz input frequency with a layout area of
$10.42\mu m^2$.

</details>


### [190] [Quantum Optimization for the Steiner Traveling Salesman Problem with Time Windows and Pickup and Delivery](https://arxiv.org/abs/2508.17896)
*Alessia Ciacco,Francesca Guerriero,Eneko Osaba*

Main category: cs.ET

TL;DR: 提出了结合Steiner旅行商问题、时间窗约束、取送货操作和车辆容量限制的复杂路由优化问题，并开发了两种数学模型和量子混合求解方法


<details>
  <summary>Details</summary>
Motivation: 解决现代物流中的复杂路由优化挑战，包括最后一公里配送、逆向物流和按需服务场景，这些现实问题需要处理时间窗、取送货和容量限制等多重约束

Method: 提出了基于弧的模型和基于节点的模型两种数学规划公式，在D-Wave的LeapCQMHybrid量子混合平台上实现，并引入了预处理缩减方法来消除冗余弧以提高计算性能

Result: 实验结果表明，混合量子方法能够解决现实规模的问题实例，证明了其在下一代路由优化中的变革潜力

Conclusion: 量子混合计算方法为解决复杂的组合优化问题提供了有效途径，特别是在处理具有多重约束的现实物流路由问题方面展现出巨大潜力

Abstract: We present the Steiner Traveling Salesman Problem with Time Windows and
Pickup and Delivery, an advanced and practical extension of classical routing
models. This variant integrates the characteristics of the Steiner Traveling
Salesman Problem with time-window constraints, pickup and delivery operations
and vehicle capacity limitations. These features closely mirror the
complexities of contemporary logistics challenges, including last-mile
distribution, reverse logistics and on-demand service scenarios. To tackle the
inherent computational difficulties of this NP-hard problem, we propose two
specialized mathematical formulations: an arc-based model and a node-oriented
model, each designed to capture distinct structural aspects of the problem.
Both models are implemented on D-Wave's LeapCQMHybrid platform, which combines
quantum and classical techniques for solving constrained optimization tasks. We
further introduce a preprocessing reduction method that eliminates redundant
arcs, significantly enhancing computational performance and scalability.
Experimental results demonstrate that hybrid quantum approaches are capable of
solving problem instances of realistic size, underscoring their potential as a
transformative tool for next-generation routing optimization.

</details>


### [191] [SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study](https://arxiv.org/abs/2508.18250)
*Yang Xiang,Fernando García-Redondo,Arvind Sharma,Van Dai Nguyen,Andrea Fantini,Philippe Matagne,Siddharth Rao,Subhali Subhechha,Lynn Verschueren,Mohammed Aftab Baig,Marie Garcia Bardon,Geert Hellings*

Main category: cs.ET

TL;DR: 本文探索SOT-MRAM在异构系统缩放下的跨节点缓存扩展潜力，通过BEOL读选择器实现低到40%的位单元面积缩减，但以读取延迟和能耗增加为代价。


<details>
  <summary>Details</summary>
Motivation: 解决传统2T1R SOT-MRAM中MTJ路由挑战导致的位单元面积缩放问题，探索在异构系统缩放范弋下SOT-MRAM作为最后一级缓存的扩展潜力。

Method: 进行设计-技术协同优化(DTCO)分析，在7nm技术节点上评估不同单元配置的位单元占地面积，并提出beol读选择器(BEOL RSs)来改善路由问题。

Result: BEOL RSs能够实现10-40%的位单元面积缩减，达到N3以下SRAM的面积水平，但会导致读取延迟增加3-5ns，能耗增加2.5-5倍。

Conclusion: BEOL RSs为SOT-MRAM提供了全面的功耗面缩放潜力，但需要在读取性能和面积优势之间找到平衡点，为异构系统中的应用指明了现实的前景和挑战。

Abstract: This work explores the cross-node scaling potential of SOT-MRAM for
last-level caches (LLCs) under heterogeneous system scaling paradigm. We
perform extensive Design-Technology Co-Optimization (DTCO) exercises to
evaluate the bitcell footprint for different cell configurations at a
representative 7 nm technology and to assess their implications on read and
write power-performance. We crucially identify the MTJ routing struggle in
conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary
bitcell area scaling challenge and propose to use BEOL read selectors (BEOL
RSs) that enable (10 -- 40) % bitcell area reduction and eventually match
sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet
the required SOT switching current, provided the magnetic free layer properties
be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This
is particularly to attribute to their (i) more available Si fins for write
transistor and (ii) lower bitline resistance at reduced cell width. We
nevertheless underscore the read tradeoff associated with BEOL RSs, with the
low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the
imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost
relative to 2T1R. This article thus highlights the realistic prospects and
hurdles of BEOL RSs towards holistic power-performance-area scaling of
SOT-MRAM.

</details>
