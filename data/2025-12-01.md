<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 2]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.DC](#cs.DC) [Total: 15]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Sneak Path Current Modeling in Memristor Crossbar Arrays for Analog In-Memory Computing](https://arxiv.org/abs/2511.21796)
*Shah Zayed Riam,Zhenlin Pei,Kyle Mooney,Chenyun Pan,Na Gong,Jinhui Wang*

Main category: cs.ET

TL;DR: 提出基于IMEC A14技术的忆阻器交叉阵列漏电流分析框架，通过数学模型快速准确估计漏电流，比SPICE仿真快4784倍，误差小于10.9%


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列是下一代非易失性存储器、人工神经网络和模拟内存计算的关键组件，但漏电流问题会降低电气性能、减小噪声容限并限制可靠操作，需要准确的分析工具

Method: 基于IMEC A14（1.4nm）技术建立闭式解析框架，通过数学推导的关系捕捉阵列大小、忆阻器ON/OFF比、读取电压和互连条件等关键设计参数的相互依赖，支持不同数据模式和连接策略

Result: 验证显示模型误差小于10.9%，比完整电路仿真快4784倍，灵敏度分析揭示了设计参数对漏电流和噪声容限损失的影响，突显了交叉阵列扩展中的权衡

Conclusion: 该分析框架为基于忆阻器的模拟内存计算架构提供了强大的定量评估和预设计/实时优化工具

Abstract: Memristor crossbar arrays have emerged as a key component for next-generation non-volatile memories, artificial neural networks, and analog in-memory computing (IMC) systems. By minimizing data transfer between the processor and memory, they offer substantial energy savings. However, a major design challenge in memristor crossbar arrays is the presence of sneak path currents, which degrade electrical performance, reduce noise margins, and limit reliable operations. This work presents a closed-form analytical framework based on IMEC A14 (1.4 nm) Technology for accurately estimating sneak path currents in memristor crossbar arrays. The proposed model captures the interdependence of key design parameters in memristor crossbar arrays, including array size, ON/OFF ratio of memristors, read voltage, and interconnect conditions, through mathematically derived relationships. It supports various practical configurations, such as different data patterns and connection strategies, enabling rapid and comprehensive sneak path current modeling. The sensitivity analysis includes how design parameters influence sneak path current and noise margin loss, underscoring the trade-offs involved in scaling crossbar arrays. Validation through SPICE simulations shows that the model achieves an error of less than 10.9% while being up to 4784 times faster than full circuit simulations. This analytical framework offers a powerful tool for quantitative assessment and pre-design/real-time optimization of memristor-based analog in-memory computing (IMC) architectures.

</details>


### [2] [Quantifying Geometry Effects on Low-Cost Intelligent Reflecting Surfaces](https://arxiv.org/abs/2511.22408)
*Yizhi He,Sayed Amir Hoseini,Mahbub Hassan*

Main category: cs.ET

TL;DR: IRS性能分析：量化列分组和1比特相位量化对性能的影响，在毫米波频段下，两种简化措施各带来约4dB损失，组合约8dB，但32x32简化IRS仍能提供两位数SNR增益。


<details>
  <summary>Details</summary>
Motivation: 智能反射表面(IRS)虽然能提供低功耗覆盖扩展，但实际部署需要考虑硬件复杂度和控制开销。本文旨在量化两种成本节约措施（列分组和1比特相位量化）相对于理想全控制连续相位基线的性能影响。

Method: 在26GHz毫米波频段下，通过单输入单输出链路仿真，研究了三种不同部署几何结构（改变接入点、IRS和用户设备相对高度）。比较了连续相位控制、1比特相位量化、列分组以及两者组合的性能差异。

Result: 从连续相位切换到1比特相位控制使中值SNR增益降低约4dB；采用列分组引入相似损失；两者组合在高度偏移部署中损失约8dB。当所有节点高度相同时，列分组控制带来的性能下降可忽略不计。尽管有损失，32x32列分组二进制IRS在大多数位置仍能提供超过无IRS基线的两位数SNR增益。

Conclusion: 部署几何结构可以抵消控制粒度限制的影响。研究提供了量化指南，说明简化IRS架构何时能满足链路预算目标，以及何时仍需要全元素控制。简化IRS架构在成本受限场景中仍然可行。

Abstract: Intelligent Reflecting Surfaces (IRS) promise low-power coverage extension, yet practical deployments must curb hardware complexity and control overhead. This paper quantifies the performance impact of two cost-saving measures, column-wise element grouping and 1-bit (binary) phase quantization, relative to the ideal fully-controlled, continuous-phase baseline. A single-input single-output link is simulated at 26 GHz (mmWave) across three deployment geometries that vary the relative heights of access point, IRS and user equipment. Results show that switching from continuous to binary phase control reduces median SNR gain by approximately 4 dB, while adopting column-wise grouping introduces a similar penalty; combining both constraints incurs approximately 8 dB loss under height-offset deployments. When all nodes share the same height, the degradation from column-wise control becomes negligible, indicating deployment geometry can offset control-granularity limits. Despite the losses, a 32 x 32 column-wise binary IRS still delivers double-digit SNR gains over the no-IRS baseline in most positions, confirming its viability for cost-constrained scenarios. The study provides quantitative guidelines on when simplified IRS architectures can meet link-budget targets and where full element-wise control remains justified.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Artificial intelligence for methane detection: from continuous monitoring to verified mitigation](https://arxiv.org/abs/2511.21777)
*Anna Allen,Gonzalo Mateo-Garcia,Itziar Irakulis-Loitxate,Manuel Montesino-San Martin,Marc Watine,James Requeima,Javier Gorroño,Cynthia Randles,Tharwat Mokalled,Luis Guanter,Richard E. Turner,Claudio Cifarelli,Manfredi Caltagirone*

Main category: cs.LG

TL;DR: MARS-S2L是一个机器学习模型，利用公开的多光谱卫星图像检测甲烷排放，能够每两天提供高分辨率检测，实现设施级归因，并已在实际部署中促成多个持久排放源的永久减排。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，少数大型点源排放占不成比例的高份额，这为通过针对相对较少站点实现大幅减排创造了机会。然而，大规模检测和归因大型排放以通知资产所有者仍然具有挑战性。

Method: 开发了MARS-S2L机器学习模型，在超过80,000张手动标注的图像数据集上进行训练，利用公开可用的多光谱卫星图像检测甲烷排放。该模型每两天提供高分辨率检测，实现设施级归因。

Result: 模型在697个未见过的站点上以8%的假阳性率检测到78%的羽流。在实际部署中，已向20个国家的利益相关方发出1,015次通知，促成6个持久排放源的验证永久减排，包括利比亚一个先前未知的站点。

Conclusion: 这些结果表明了从卫星检测到可量化甲烷减排的可扩展路径，为大规模甲烷减排提供了有效工具。

Abstract: Methane is a potent greenhouse gas, responsible for roughly 30\% of warming since pre-industrial times. A small number of large point sources account for a disproportionate share of emissions, creating an opportunity for substantial reductions by targeting relatively few sites. Detection and attribution of large emissions at scale for notification to asset owners remains challenging. Here, we introduce MARS-S2L, a machine learning model that detects methane emissions in publicly available multispectral satellite imagery. Trained on a manually curated dataset of over 80,000 images, the model provides high-resolution detections every two days, enabling facility-level attribution and identifying 78\% of plumes with an 8\% false positive rate at 697 previously unseen sites. Deployed operationally, MARS-S2L has issued 1,015 notifications to stakeholders in 20 countries, enabling verified, permanent mitigation of six persistent emitters, including a previously unknown site in Libya. These results demonstrate a scalable pathway from satellite detection to quantifiable methane mitigation.

</details>


### [4] [Physics-Informed Spiking Neural Networks via Conservative Flux Quantization](https://arxiv.org/abs/2511.21784)
*Chi Zhang,Lin Wang*

Main category: cs.LG

TL;DR: 提出PISNN框架，结合物理约束与脉冲神经网络，通过C-LIF神经元和CFQ策略实现严格物理守恒和长期泛化，适用于边缘设备实时物理预测。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要实时、物理一致的预测，但传统PINNs能耗高且难以严格保证物理守恒定律。脉冲神经网络(SNNs)适合边缘计算但直接转换会降低物理保真度。

Method: 提出Physics-Informed Spiking Neural Network (PISNN)框架，包含：1) Conservative Leaky Integrate-and-Fire (C-LIF)神经元，结构上保证局部质量守恒；2) Conservative Flux Quantization (CFQ)策略，将神经脉冲重新定义为物理通量的离散包，学习时间不变的物理演化算子。

Result: 在1D热方程和2D拉普拉斯方程等基准测试中表现出色，准确模拟系统动力学，同时设计上保持完美的质量守恒，优于传统PINNs。

Conclusion: 建立了将科学计算的严谨性与神经形态工程效率融合的框架，为智能系统的复杂、长期、节能物理预测铺平道路。

Abstract: Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.

</details>


### [5] [Dynamical Implicit Neural Representations](https://arxiv.org/abs/2511.21787)
*Yesom Park,Kelvin Kan,Thomas Flynn,Yi Huang,Shinjae Yoo,Stanley Osher,Xihaier Luo*

Main category: cs.LG

TL;DR: DINR将隐式神经表示建模为连续时间动力系统而非离散层堆叠，通过连续特征演化缓解谱偏差，提升高频细节表达能力，在图像表示、场重建和数据压缩中表现优异。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在建模复杂视觉和几何信号方面具有强大能力，但谱偏差问题限制了其捕捉高频细节的能力。现有解决方案效果有限，需要新的建模框架来解决这一根本挑战。

Method: 提出动态隐式神经表示（DINR），将特征演化视为连续时间动力系统而非离散层堆叠。通过基于Rademacher复杂度和神经正切核的理论分析，证明DINR增强表达能力并改善训练动态。通过正则化底层动力学的复杂度来平衡表达能力和泛化能力。

Result: 在图像表示、场重建和数据压缩等任务上的大量实验表明，DINR相比传统静态INRs具有更稳定的收敛性、更高的信号保真度和更强的泛化能力。

Conclusion: DINR通过将INR建模为连续时间动力系统，有效缓解了谱偏差问题，提供了一种平衡表达能力和泛化能力的理论框架，在多个视觉和几何信号建模任务中表现出优越性能。

Abstract: Implicit Neural Representations (INRs) provide a powerful continuous framework for modeling complex visual and geometric signals, but spectral bias remains a fundamental challenge, limiting their ability to capture high-frequency details. Orthogonal to existing remedy strategies, we introduce Dynamical Implicit Neural Representations (DINR), a new INR modeling framework that treats feature evolution as a continuous-time dynamical system rather than a discrete stack of layers. This dynamical formulation mitigates spectral bias by enabling richer, more adaptive frequency representations through continuous feature evolution. Theoretical analysis based on Rademacher complexity and the Neural Tangent Kernel demonstrates that DINR enhances expressivity and improves training dynamics. Moreover, regularizing the complexity of the underlying dynamics provides a principled way to balance expressivity and generalization. Extensive experiments on image representation, field reconstruction, and data compression confirm that DINR delivers more stable convergence, higher signal fidelity, and stronger generalization than conventional static INRs.

</details>


### [6] [Multiclass threshold-based classification and model evaluation](https://arxiv.org/abs/2511.21794)
*Edoardo Legnaro,Sabrina Guastavino,Francesco Marchetti*

Main category: cs.LG

TL;DR: 提出基于阈值的多分类框架，用多维单纯形上的几何解释替代softmax概率解释，通过阈值调优提升分类性能，并引入ROC云和DFP分数进行多类ROC分析。


<details>
  <summary>Details</summary>
Motivation: 标准多分类使用argmax规则，缺乏像二分类那样的阈值调优能力。本文旨在将二分类中的阈值调优概念推广到多分类，以进一步提升训练好的分类网络的预测能力。

Method: 将softmax输出的概率解释改为多维单纯形上的几何解释，引入多维阈值进行分类决策。提出后验阈值调优方法，并基于ROC云进行多类ROC分析，使用DFP分数总结性能。

Result: 实验表明，多维阈值调优能在不同网络和数据集上带来性能提升。提出的ROC云和DFP分数提供了比标准OvR曲线更一致的多类ROC分析方法。

Conclusion: 该框架成功将二分类的阈值调优概念推广到多分类，为任何训练好的分类网络提供了额外的性能优化手段，并提供了更合理的多类ROC评估方法。

Abstract: In this paper, we introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule. This is done by replacing the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex, where the classification depends on a multidimensional threshold. This change of perspective enables for any trained classification network an \textit{a posteriori} optimization of the classification score by means of threshold tuning, as usually carried out in the binary setting, thus allowing for a further refinement of the prediction capability of any network. Our experiments show indeed that multidimensional threshold tuning yields performance improvements across various networks and datasets. Moreover, we derive a multiclass ROC analysis based on \emph{ROC clouds} -- the attainable (FPR,TPR) operating points induced by a single multiclass threshold -- and summarize them via a \emph{Distance From Point} (DFP) score to $(0,1)$. This yields a coherent alternative to standard One-vs-Rest (OvR) curves and aligns with the observed tuning gains.

</details>


### [7] [The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning](https://arxiv.org/abs/2511.21799)
*Ethan Hsu,Harry Chen,Chudi Zhong,Lesia Semenova*

Main category: cs.LG

TL;DR: Rashomon集合（多个近最优模型）重塑了可信ML：稀疏模型保护隐私但易受攻击，而Rashomon集合的多样性提供反应式鲁棒性，但会增加信息泄漏风险


<details>
  <summary>Details</summary>
Motivation: 现实ML管道通常产生多个近最优模型（Rashomon集合），而非单一模型。这种多重性如何影响ML的可信度（鲁棒性、隐私性等）尚未得到充分研究。

Method: 通过理论分析和稀疏决策树、线性模型的实证研究，分析Rashomon集合对鲁棒性和隐私性的影响。在个体模型层面分析稀疏可解释模型，在集合层面分析多样性效应。

Result: 稀疏模型保护隐私但易受对抗攻击；Rashomon集合的多样性提供反应式鲁棒性（攻击破坏一个模型时其他模型仍准确），且在小分布偏移下稳定。但多样性增加信息泄漏风险，披露更多近最优模型让攻击者获得更丰富的训练数据视图。

Conclusion: Rashomon集合在可信ML中具有双重角色：既是资源（提供鲁棒性），也是风险（增加隐私泄漏）。揭示了鲁棒性与隐私性的权衡，强调在ML管道中考虑模型多重性的重要性。

Abstract: Real-world machine learning (ML) pipelines rarely produce a single model; instead, they produce a Rashomon set of many near-optimal ones. We show that this multiplicity reshapes key aspects of trustworthiness. At the individual-model level, sparse interpretable models tend to preserve privacy but are fragile to adversarial attacks. In contrast, the diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate. Rashomon sets are also stable under small distribution shifts. However, this same diversity increases information leakage, as disclosing more near-optimal models provides an attacker with progressively richer views of the training data. Through theoretical analysis and empirical studies of sparse decision trees and linear models, we characterize this robustness-privacy trade-off and highlight the dual role of Rashomon sets as both a resource and a risk for trustworthy ML.

</details>


### [8] [Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation](https://arxiv.org/abs/2511.23440)
*Bernhard Klein,Falk Selker,Hendrik Borras,Sophie Steger,Franz Pernkopf,Holger Fröning*

Main category: cs.LG

TL;DR: PFP-BNNs通过高斯分布假设实现高效不确定性传播，在ARM嵌入式CPU上实现4200倍加速，保持与SVI相当的精度和OOD检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在安全关键应用中因不确定性处理不足而受限，贝叶斯神经网络虽能提供概率估计但计算成本过高，需要高效部署方案。

Method: 提出PFP方法，假设权重和激活值服从高斯分布，实现完全解析的不确定性传播；使用TVM编译器构建高斯传播算子库，结合手动和自动调优策略，在ARM CPU上部署端到端管道。

Result: PFP在计算效率上显著优于SVI，小批量处理时加速达4200倍；在Dirty-MNIST上保持与SVI相当的精度、不确定性估计和OOD检测能力。

Conclusion: 结合贝叶斯近似与代码生成技术，PFP-BNNs能够在资源受限系统上高效部署，为安全关键应用提供可行的不确定性感知解决方案。

Abstract: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

</details>


### [9] [Unsupervised Anomaly Detection for Smart IoT Devices: Performance and Resource Comparison](https://arxiv.org/abs/2511.21842)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 本研究比较了两种无监督异常检测方法（Isolation Forest和One-Class SVM）在IoT环境中的性能，发现Isolation Forest在检测精度和计算效率方面均优于OC-SVM，更适合资源受限的IoT边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备在各行业的快速部署，虽然提升了运营效率，但也增加了网络安全漏洞。传统基于签名的异常检测系统难以识别新兴和零日威胁，因此需要研究更有效的无监督异常检测方法。

Method: 使用TON_IoT恒温器数据集，评估两种无监督异常检测技术：Isolation Forest（IF）和One-Class Support Vector Machine（OC-SVM）。采用标准评估指标（准确率、精确率、召回率、F1分数）以及关键资源利用指标（推理时间、模型大小、峰值RAM使用量）。

Result: 实验结果表明，Isolation Forest在所有性能指标上均优于OC-SVM，具有更高的检测准确率、精确率和召回率，以及显著更好的F1分数。同时，Isolation Forest在计算资源占用方面表现更优，推理时间更短，模型更小，RAM使用量更低。

Conclusion: Isolation Forest在高维和不平衡的IoT环境中表现出更强的鲁棒性，其优越的计算效率使其更适合部署在资源受限的IoT边缘设备上，具有实际实时异常检测的可行性。

Abstract: The rapid expansion of Internet of Things (IoT) deployments across diverse sectors has significantly enhanced operational efficiency, yet concurrently elevated cybersecurity vulnerabilities due to increased exposure to cyber threats. Given the limitations of traditional signature-based Anomaly Detection Systems (ADS) in identifying emerging and zero-day threats, this study investigates the effectiveness of two unsupervised anomaly detection techniques, Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM), using the TON_IoT thermostat dataset. A comprehensive evaluation was performed based on standard metrics (accuracy, precision, recall, and F1-score) alongside critical resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results revealed that IF consistently outperformed OC-SVM, achieving higher detection accuracy, superior precision, and recall, along with a significantly better F1-score. Furthermore, Isolation Forest demonstrated a markedly superior computational footprint, making it more suitable for deployment on resource-constrained IoT edge devices. These findings underscore Isolation Forest's robustness in high-dimensional and imbalanced IoT environments and highlight its practical viability for real-time anomaly detection.

</details>


### [10] [Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics](https://arxiv.org/abs/2511.21848)
*Eric Leonardis,Akira Nagamori,Ayesha Thanawalla,Yuanjia Yang,Joshua Park,Hutton Saunders,Eiman Azim,Talmo Pereira*

Main category: cs.LG

TL;DR: 开发了一个用于行为驱动仿真的通用平台，通过模仿学习框架在物理模拟环境中实现小鼠前肢抓取任务，发现能量和速度的自然约束能更好地预测真实EMG信号。


<details>
  <summary>Details</summary>
Motivation: 大脑进化出有效控制身体的能力，为了理解这种关系，需要建模体现控制背后的感觉运动转换。需要开发一个高保真行为动力学、生物力学和神经回路架构的仿真平台。

Method: 开发了一个通用行为驱动仿真平台，建立从神经科学实验室运动学数据到生物力学模型重现自然运动的流程。采用模仿学习框架在物理模拟环境中执行灵巧的前肢抓取任务，使用JAX和Mujoco-MJX进行GPU加速训练。

Result: 小鼠手臂模型训练速度超过每秒100万步。结果显示，添加能量和速度的自然约束能产生更好地预测真实EMG信号的模拟肌肉骨骼活动。

Conclusion: 能量和控制约束对于建模肌肉骨骼运动控制至关重要，这项工作为理解体现控制提供了重要证据。

Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.

</details>


### [11] [Lightweight ML-Based Air Quality Prediction for IoT and Embedded Applications](https://arxiv.org/abs/2511.21857)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 该研究评估了完整版和轻量版XGBoost回归模型在预测CO和NO2浓度方面的表现，发现完整版精度更高，而轻量版在计算资源消耗上优势明显，适合资源受限的实时应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同版本的XGBoost模型在空气污染物浓度预测中的表现，特别关注在保持预测质量的同时，为资源受限的物联网和嵌入式应用提供可行的解决方案。

Method: 使用AirQualityUCI数据集（城市环境一年数据），评估完整版和轻量版XGBoost回归模型。采用MAE、RMSE、MBE、R2等预测精度指标，以及推理时间、模型大小、峰值RAM使用等资源指标进行综合评估。

Result: 完整版XGBoost模型对两种污染物都获得了更优的预测精度，而轻量版模型虽然精度稍低，但在推理时间、模型存储需求等计算资源方面优势显著，大幅降低了计算开销。

Conclusion: 轻量版XGBoost模型在资源受限环境下部署是可行的，能在不显著牺牲预测质量的前提下，为物联网和嵌入式应用提供实时空气质量监测能力。

Abstract: This study investigates the effectiveness and efficiency of two variants of the XGBoost regression model, the full-capacity and lightweight (tiny) versions, for predicting the concentrations of carbon monoxide (CO) and nitrogen dioxide (NO2). Using the AirQualityUCI dataset collected over one year in an urban environment, we conducted a comprehensive evaluation based on widely accepted metrics, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). In addition, we assessed resource-oriented metrics such as inference time, model size, and peak RAM usage. The full XGBoost model achieved superior predictive accuracy for both pollutants, while the tiny model, though slightly less precise, offered substantial computational benefits with significantly reduced inference time and model storage requirements. These results demonstrate the feasibility of deploying simplified models in resource-constrained environments without compromising predictive quality. This makes the tiny XGBoost model suitable for real-time air-quality monitoring in IoT and embedded applications.

</details>


### [12] [Towards a Foundation Model for Partial Differential Equations Across Physics Domains](https://arxiv.org/abs/2511.21861)
*Eduardo Soares,Emilio Vital Brazil,Victor Shirasuna,Breno W. S. R. de Carvalho,Cristiano Malossi*

Main category: cs.LG

TL;DR: PDE-FM是一个用于物理信息机器学习的模块化基础模型，通过统一空间、频谱和时间推理来处理异质偏微分方程系统，在多个物理领域实现了最先进的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的任务特定神经算子需要针对不同物理系统进行专门设计和训练，缺乏统一的建模框架。需要开发能够跨不同物理领域进行通用建模的基础模型，实现数据高效且可扩展的物理动力学建模。

Method: 结合空间-频谱标记化、物理感知条件化和基于Mamba状态空间的主干网络与算子理论解码器。模型在多样化的PDE数据集上进行一次性预训练，无需架构或数据特定修改即可迁移到新的物理体系。

Result: 在The Well基准测试的12个2D和3D数据集上评估，涵盖流体动力学、辐射、弹性和天体物理现象。PDE-FM在6个领域达到最先进精度，相对于先前的算子学习基线平均VRMSE降低46%。模型展示了强大的跨物理泛化能力，在湍流和辐射系统中表现优异，同时在线性和稳态体系中保持强劲性能。

Conclusion: 跨多样物理过程的大规模预训练可以产生可迁移的动力学表示，这标志着向统一、基础级别的多物理模拟和科学发现替代模型迈出了一步。

Abstract: We present PDE-FM, a modular foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation (PDE) systems. PDE-FM combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. In contrast to task-specific neural operators, PDE-FM is pretrained once on diverse PDE datasets and can be transferred to new physical regimes without architectural or data-specific modifications. Evaluated on twelve 2D and 3D datasets from The Well benchmark - spanning hydrodynamic, radiative, elastic, and astrophysical phenomena - PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines. The model demonstrates robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes. These results suggest that large-scale pretraining across diverse physical processes can yield transferable representations of dynamics, marking a step toward unified, foundation-level surrogates for multi-physics simulation and scientific discovery.

</details>


### [13] [Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium](https://arxiv.org/abs/2511.21882)
*Akbar Anbar Jafari,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出Equilibrium Transformers (EqT)，通过闭循环预测原则让模型在生成每个token前迭代优化隐表示，解决传统自回归模型开环推理的错误传播问题


<details>
  <summary>Details</summary>
Motivation: 传统自回归transformer采用开环推理，隐藏状态一次性计算且不可修正，导致错误在序列中传播，这是长程推理、事实一致性和多步规划等问题的根本架构限制

Method: 引入闭循环预测原则，要求模型在生成每个token前迭代优化隐表示直至达到自洽平衡。具体实现为Equilibrium Transformers (EqT)，在标准transformer层中加入Equilibrium Refinement Module，通过梯度下降最小化学得的能量函数来优化隐空间

Result: 在二进制奇偶性任务上，EqT平均提升3.28%，在标准transformer接近随机性能的困难序列上提升达8.07%，验证了深思熟虑的收益随任务难度增加而增加

Conclusion: 闭循环平衡机制可能解决开环自回归的承诺瓶颈，正如注意力机制解决了循环网络的序列瓶颈一样，这是向语言模型发展的重要基础步骤

Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.

</details>


### [14] [Physically Interpretable Representation Learning with Gaussian Mixture Variational AutoEncoder (GM-VAE)](https://arxiv.org/abs/2511.21883)
*Tiffany Fan,Murray Cutforth,Marta D'Elia,Alexandre Cortiella,Alireza Doostan,Eric Darve*

Main category: cs.LG

TL;DR: 提出GM-VAE框架，结合EM训练方案和谱可解释性度量，用于从高维科学数据中提取物理可解释的紧凑表示，在复杂物理系统中实现稳定训练和物理一致的聚类。


<details>
  <summary>Details</summary>
Motivation: 从高维科学数据中提取紧凑、物理可解释的表示是一个持续挑战，因为物理系统具有复杂的非线性结构。传统VAE在联合优化重构和聚类时存在训练不稳定性问题。

Method: 提出高斯混合变分自编码器(GM-VAE)框架，采用块坐标下降策略，交替执行期望步和最大化步，结合基于图拉普拉斯平滑度的谱可解释性度量来评估学习到的表示。

Result: 在表面反应ODE、Navier-Stokes尾流和实验激光诱导燃烧纹影图像等数据集上验证，GM-VAE能够产生平滑、物理一致的流形和准确的机制聚类，为湍流和反应流系统提供鲁棒的数据驱动解释工具。

Conclusion: GM-VAE框架通过EM启发式训练方案和定量可解释性度量，成功解决了从复杂物理数据中提取物理可解释表示的挑战，为科学数据解释提供了有效工具。

Abstract: Extracting compact, physically interpretable representations from high-dimensional scientific data is a persistent challenge due to the complex, nonlinear structures inherent in physical systems. We propose a Gaussian Mixture Variational Autoencoder (GM-VAE) framework designed to address this by integrating an Expectation-Maximization (EM)-inspired training scheme with a novel spectral interpretability metric. Unlike conventional VAEs that jointly optimize reconstruction and clustering (often leading to training instability), our method utilizes a block-coordinate descent strategy, alternating between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes. To objectively evaluate the learned representations, we introduce a quantitative metric based on graph-Laplacian smoothness, which measures the coherence of physical quantities across the latent manifold. We demonstrate the efficacy of this framework on datasets of increasing complexity: surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. The results show that our GM-VAE yields smooth, physically consistent manifolds and accurate regime clustering, offering a robust data-driven tool for interpreting turbulent and reactive flow systems.

</details>


### [15] [Exploring Fusion Strategies for Multimodal Vision-Language Systems](https://arxiv.org/abs/2511.21889)
*Regan Willis,Jason Bakos*

Main category: cs.LG

TL;DR: 该论文研究了多模态机器学习中不同数据融合策略对准确性和延迟的权衡，通过BERT和视觉网络框架在CMU MOSI数据集上验证了晚期融合准确性最高、早期融合延迟最低的结论。


<details>
  <summary>Details</summary>
Motivation: 多模态机器学习模型需要融合多个输入数据流来提升决策准确性，但融合策略的选择需要在准确性和延迟之间进行权衡，因为早期或晚期融合会导致性能和延迟的不同变化。

Method: 使用混合BERT和视觉网络框架（MobileNetV2和ViT）集成图像和文本数据，为每个视觉网络提出三种模型：晚期融合、中期融合和早期融合，在CMU MOSI数据集上评估准确性，并在NVIDIA Jetson Orin AGX上基准测试延迟。

Result: 实验结果表明，晚期融合获得最高准确性，早期融合提供最低推理延迟，验证了融合策略在准确性和延迟之间的权衡关系。

Conclusion: 模型架构中较早的数据融合会导致更快的推理时间，但以准确性为代价，为多模态应用中的融合策略选择提供了实用指导。

Abstract: Modern machine learning models often combine multiple input streams of data to more accurately capture the information that informs their decisions. In multimodal machine learning, choosing the strategy for fusing data together requires careful consideration of the application's accuracy and latency requirements, as fusing the data at earlier or later stages in the model architecture can lead to performance changes in accuracy and latency. To demonstrate this tradeoff, we investigate different fusion strategies using a hybrid BERT and vision network framework that integrates image and text data. We explore two different vision networks: MobileNetV2 and ViT. We propose three models for each vision network, which fuse data at late, intermediate, and early stages in the architecture. We evaluate the proposed models on the CMU MOSI dataset and benchmark their latency on an NVIDIA Jetson Orin AGX. Our experimental results demonstrate that while late fusion yields the highest accuracy, early fusion offers the lowest inference latency. We describe the three proposed model architectures and discuss the accuracy and latency tradeoffs, concluding that data fusion earlier in the model architecture results in faster inference times at the cost of accuracy.

</details>


### [16] [Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2511.21893)
*Fatemeh Akbarian,Anahita Baninajjar,Yingyi Zhang,Ananth Balashankar,Amir Aminifar*

Main category: cs.LG

TL;DR: 提出一种任务无关的防御机制，通过生成模型重建对抗性扰动输入，结合生成采样和共识聚合策略，有效抵御多模态基础模型中的对抗性幻觉攻击。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型虽然能在共享嵌入空间中对齐图像、文本等模态，但容易受到对抗性幻觉攻击的影响，这些攻击通过微小扰动破坏跨模态对齐并误导下游任务。

Method: 1. 使用生成模型（如变分自编码器）从攻击者的扰动输入中重建原始输入；2. 采用生成采样策略结合基于共识的聚合方案，对生成样本的结果进行聚合。

Result: 实验表明，该方法将幻觉攻击成功率大幅降低至接近零，在未扰动和扰动输入设置下分别将跨模态对齐提高了4%（42到46）和11%（32到43）。

Conclusion: 该方法为对抗性幻觉攻击提供了有效且模型无关的防御机制，显著增强了多模态基础模型的鲁棒性。

Abstract: Multi-modal foundation models align images, text, and other modalities in a shared embedding space but remain vulnerable to adversarial illusions (Zhang et al., 2025), where imperceptible perturbations disrupt cross-modal alignment and mislead downstream tasks. To counteract the effects of adversarial illusions, we propose a task-agnostic mitigation mechanism that reconstructs the input from the attacker's perturbed input through generative models, e.g., Variational Autoencoders (VAEs), to maintain natural alignment. To further enhance our proposed defense mechanism, we adopt a generative sampling strategy combined with a consensus-based aggregation scheme over the outcomes of the generated samples. Our experiments on the state-of-the-art multi-modal encoders show that our approach substantially reduces the illusion attack success rates to near-zero and improves cross-modal alignment by 4% (42 to 46) and 11% (32 to 43) in unperturbed and perturbed input settings respectively, providing an effective and model-agnostic defense against adversarial illusions.

</details>


### [17] [Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning](https://arxiv.org/abs/2511.21900)
*Patricia Suriana,Joshua A. Rackers,Ewa M. Nowara,Pedro O. Pinheiro,John M. Nicoloudis,Vishnu Sresht*

Main category: cs.LG

TL;DR: 比较三种体素输入（原子类型、原始电子密度、密度梯度）在3D CNN中的表现，发现电子密度在低数据量下对蛋白质-配体结合亲和力预测更优，在量子性质预测中即使使用低精度密度也优于原子类型。


<details>
  <summary>Details</summary>
Motivation: 传统的3D分子性质预测模型通常基于原子表示，可能忽略细微的物理信息。电子密度图作为X射线晶体学和冷冻电镜的直接输出，提供了连续、物理基础更强的替代方案。

Method: 使用三种体素输入类型（原子类型、原始电子密度、密度梯度幅度）在3D卷积神经网络中进行比较，应用于两个分子任务：蛋白质-配体结合亲和力预测（PDBbind）和量子性质预测（QM9）。

Result: 在PDBbind任务中，全数据量下所有表示表现相似，但在低数据量下，基于密度的输入优于原子类型。在QM9任务中，即使使用低精度方法（XTB）生成的密度，基于密度的输入在大规模数据下仍优于原子基表示。

Conclusion: 电子密度输入具有任务和机制依赖性优势：在亲和力预测中提高数据效率，在量子性质建模中提高准确性，表明密度编码了丰富的结构和电子信息。

Abstract: Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.

</details>


### [18] [Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals](https://arxiv.org/abs/2511.21908)
*Hamid Shamszare,Avishek Choudhury*

Main category: cs.LG

TL;DR: 多模态机器学习框架结合面部表情和皮肤电反应预测用户对AI/人类建议的早期信任度，在模拟ADHD移动健康场景中取得良好预测效果


<details>
  <summary>Details</summary>
Motivation: 预测人类对AI系统的信任对于安全整合AI决策支持工具至关重要，特别是在医疗健康领域。在心理健康应用中，信任失调可能影响诊断和治疗结果，因此需要实时、客观的信任标记方法。

Method: 提出多模态机器学习框架，结合图像（面部视频）和皮肤电反应数据。面部数据使用OpenCV提取帧，通过预训练transformer模型提取情感特征；GSR信号分解为紧张性和阶段性成分。定义两个时间窗口：早期检测窗口（决策前6-3秒）和邻近检测窗口（决策前3-0秒）。分别使用单模态和多模态特征进行信任预测，并通过多模态堆叠集成方法整合最佳单模态模型。

Result: 结合面部和生理线索显著提高了预测性能。多模态堆叠框架在早期检测窗口达到准确率0.83、F1分数0.88、ROC-AUC 0.87；在邻近检测窗口达到准确率0.75、F1分数0.82、ROC-AUC 0.66。

Conclusion: 生物信号可作为实时、客观的用户信任标记，使自适应AI系统能够动态调整响应以维持校准信任，这在心理健康应用中尤为重要，因为信任失调可能影响诊断和治疗结果。

Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.

</details>


### [19] [A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization](https://arxiv.org/abs/2511.22080)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Chang Liu,Qipeng Xie,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWMSAM：一种解决联邦学习中动量与SAM结合时出现的局部-全局曲率错位和动量回波振荡问题的新方法，通过动量引导的全局扰动和自适应规则实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，模型需要在有限的通信预算下快速收敛，并在非IID客户端分布上良好泛化。虽然动量和SAM分别能加速收敛和寻找平坦解，但简单结合在非IID FL中会导致两个结构性问题：局部-全局曲率错位（局部SAM方向不能反映全局损失几何）和动量回波振荡（累积动量导致的后期不稳定）。

Method: 提出FedWMSAM方法：1）构建动量引导的全局扰动，使用服务器聚合的动量来对齐客户端的SAM方向与全局下降几何，实现单反向传播的SAM近似以保持效率；2）通过余弦相似度自适应规则耦合动量和SAM，形成早期动量、后期SAM的两阶段训练计划。

Result: 在多个数据集和模型架构上的广泛实验验证了方法的有效性、适应性和鲁棒性。理论方面提供了非IID收敛边界，明确建模了扰动引起的方差及其对参数(S, K, R, N)的依赖关系。

Conclusion: FedWMSAM成功解决了联邦学习中动量与SAM结合时的优化挑战，通过协调局部-全局几何对齐和动量-SAM耦合，在保持通信效率的同时实现了更好的收敛和泛化性能。

Abstract: In federated learning (FL), models must \emph{converge quickly} under tight communication budgets while \emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \emph{momentum} to accelerate progress, and \emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.

</details>


### [20] [Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck](https://arxiv.org/abs/2511.21923)
*Xinyu Liu,Xu Zhang,Can Chen,Ren Wang*

Main category: cs.LG

TL;DR: 本文通过信息瓶颈原理分析后门数据对神经网络训练动态的影响，发现后门攻击会产生独特的互信息特征，揭示了视觉明显攻击在信息论层面可能更隐蔽的悖论，并提出基于动态的隐蔽性度量方法。


<details>
  <summary>Details</summary>
Motivation: 理解后门数据如何影响神经网络训练动态是一个复杂且未被充分探索的挑战。现有研究缺乏对后门数据在学习过程中影响的深入分析，特别是在目标类别与其他干净类别之间的差异行为。

Method: 利用信息瓶颈原理结合内部表示的聚类分析，研究后门攻击在训练不同阶段产生的互信息特征。基于这些洞察，提出了一种新颖的基于动态的隐蔽性度量方法，量化攻击在模型层面的整合程度。

Result: 发现后门攻击会产生独特的互信息特征，这些特征随训练阶段演变且因攻击机制而异。揭示了一个令人惊讶的权衡：像BadNets这样视觉明显的攻击在信息论层面可能比许多视觉不可感知的攻击更隐蔽。提出的动态隐蔽性度量方法在多个数据集和攻击类型上得到验证。

Conclusion: 后门攻击在信息瓶颈框架下展现出独特的动态特征，视觉明显攻击可能在模型层面更隐蔽。提出的动态隐蔽性度量为理解和评估后门威胁提供了新的维度，有助于更全面地评估攻击的隐蔽性。

Abstract: Understanding how backdoor data influences neural network training dynamics remains a complex and underexplored challenge. In this paper, we present a rigorous analysis of the impact of backdoor data on the learning process, with a particular focus on the distinct behaviors between the target class and other clean classes. Leveraging the Information Bottleneck (IB) principle connected with clustering of internal representation, We find that backdoor attacks create unique mutual information (MI) signatures, which evolve across training phases and differ based on the attack mechanism. Our analysis uncovers a surprising trade-off: visually conspicuous attacks like BadNets can achieve high stealthiness from an information-theoretic perspective, integrating more seamlessly into the model than many visually imperceptible attacks. Building on these insights, we propose a novel, dynamics-based stealthiness metric that quantifies an attack's integration at the model level. We validate our findings and the proposed metric across multiple datasets and diverse attack types, offering a new dimension for understanding and evaluating backdoor threats. Our code is available in: https://github.com/XinyuLiu71/Information_Bottleneck_Backdoor.git.

</details>


### [21] [Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers](https://arxiv.org/abs/2511.22616)
*Meriem Arbaoui,Mohamed-el-Amine Brahmia,Abdellatif Rahmoun,Mourad Zghal*

Main category: cs.LG

TL;DR: 本文是关于联邦学习(FL)的综述，重点分析了FL的三个主要研究方向：个性化、优化和鲁棒性，通过混合方法（文献计量分析+系统综述）对FL研究进行分类，并讨论了异构性、效率、安全和隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网和人工智能的融合推动了行业创新，但隐私问题和数据孤岛限制了发展。传统的集中式机器学习难以解决这些问题，因此需要联邦学习这种去中心化范式，能够在保护本地原始数据隐私的同时实现协作模型训练。

Method: 采用混合方法学，结合文献计量分析和系统综述，对联邦学习研究进行结构化分类。重点关注三个研究方向：个性化、优化和鲁棒性，并分析聚合策略（包括架构、同步方法和联邦目标）。

Result: 提供了联邦学习的全面概述，包括挑战和技术分析，比较了IID和非IID数据分布下的聚合方法，并提出了实用的评估方法。识别了该领域最有影响力的研究成果。

Conclusion: 联邦学习作为保护隐私的分布式机器学习范式具有巨大潜力，但仍面临异构性等挑战。本文为未来研究提供了方向指导，旨在推动这一快速发展领域的创新。

Abstract: The integration of IoT and AI has unlocked innovation across industries, but growing privacy concerns and data isolation hinder progress. Traditional centralized ML struggles to overcome these challenges, which has led to the rise of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing local raw data. FL ensures data privacy, reduces communication overhead, and supports scalability, yet its heterogeneity adds complexity compared to centralized approaches. This survey focuses on three main FL research directions: personalization, optimization, and robustness, offering a structured classification through a hybrid methodology that combines bibliometric analysis with systematic review to identify the most influential works. We examine challenges and techniques related to heterogeneity, efficiency, security, and privacy, and provide a comprehensive overview of aggregation strategies, including architectures, synchronization methods, and diverse federation objectives. To complement this, we discuss practical evaluation approaches and present experiments comparing aggregation methods under IID and non-IID data distributions. Finally, we outline promising research directions to advance FL, aiming to guide future innovation in this rapidly evolving field.

</details>


### [22] [Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs](https://arxiv.org/abs/2511.21928)
*Yifan Zhou,Sachin Grover,Mohamed El Mistiri,Kamalesh Kalirathnam,Pratyush Kerhalkar,Swaroop Mishra,Neelesh Kumar,Sanket Gaurav,Oya Aran,Heni Ben Amor*

Main category: cs.LG

TL;DR: ProPS是一种新颖的强化学习方法，将大型语言模型置于策略优化循环的核心，直接基于奖励反馈和自然语言输入提出策略更新，在15个Gymnasium任务中优于7种传统RL算法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励信号，无法利用现实任务中丰富的语义知识。相比之下，人类通过结合数值反馈与语言、先验知识和常识来高效学习。需要统一数值和语言推理的框架。

Method: 提出Prompted Policy Search (ProPS)，将大型语言模型置于策略优化循环的中心，直接基于奖励反馈和自然语言输入提出策略更新。LLM能够在上下文中执行数值优化，并整合目标、领域知识和策略提示等语义信号。

Result: 在15个Gymnasium任务（经典控制、Atari游戏、MuJoCo环境）中评估，与PPO、SAC、TRPO等7种广泛采用的RL算法比较。在15个任务中的8个上优于所有基线方法，当提供领域知识时表现出显著优势。

Conclusion: 结果表明统一语义和数值推理对于透明、可泛化且与人类对齐的强化学习具有潜力。ProPS展示了LLM在强化学习中的有效性，为更智能、更高效的RL系统开辟了新方向。

Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.

</details>


### [23] [Closing the Generalization Gap in Parameter-efficient Federated Edge Learning](https://arxiv.org/abs/2511.23282)
*Xinnong Du,Zhonghao Lyu,Xiaowen Cao,Chunyang Wen,Shuguang Cui,Jie Xu*

Main category: cs.LG

TL;DR: 提出一个参数高效的联邦边缘学习框架，通过联合模型剪枝和客户端选择来解决数据异构和资源受限问题，将泛化分析嵌入收敛分析中，并优化剪枝率、客户端选择和资源分配。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习虽然能保护数据隐私，但面临本地数据集有限且异构、资源受限等问题，这会严重降低模型泛化能力和资源利用率，从而影响学习性能。

Method: 提出参数高效的FEEL框架，联合模型剪枝和客户端选择；推导信息论泛化边界并嵌入收敛分析；将问题建模为泛化感知的平均平方梯度范数边界最小化问题，联合优化剪枝率、客户端选择和通信计算资源；使用交替优化算法求解非凸混合整数问题。

Result: 通过大量实验验证，所提设计相比现有基线方法实现了更优的学习性能，证明了将泛化感知分析与系统级优化相结合对高效FEEL的有效性。

Conclusion: 通过联合模型剪枝和客户端选择，并将泛化分析嵌入系统优化，可以有效解决联邦边缘学习中的数据异构和资源约束问题，提升学习性能和资源效率。

Abstract: Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.

</details>


### [24] [Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment](https://arxiv.org/abs/2511.21931)
*Henry Salgado,Meagan Kendall,Martine Ceberio*

Main category: cs.LG

TL;DR: 提出一个简单计算框架，通过比较数据本身特征排序与模型解释，评估机器学习模型是否与数据结构对齐


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法主要关注解释模型行为，但缺乏评估模型是否真正反映数据结构的方法。需要建立直接从数据本身出发的基准来验证模型-数据对齐

Method: 借鉴Rubin潜在结果框架，量化每个特征在二分类任务中分离两个结果组的能力，超越传统描述统计，估计每个特征对结果的影响。通过比较数据驱动的特征排序与模型解释，提供模型无关的可解释方法

Result: 开发了一个简单且计算高效的框架，能够评估模型是否与数据结构对齐，为从业者提供了可解释的模型-数据对齐评估工具

Conclusion: 提出的框架为评估机器学习模型是否"说出数据所说"提供了实用方法，通过建立数据本身基准并与模型解释对比，增强了模型可解释性和可信度

Abstract: In this work, we propose a simple and computationally efficient framework to evaluate whether machine learning models align with the structure of the data they learn from; that is, whether \textit{the model says what the data says}. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings against model-based explanations, we provide practitioners with an interpretable and model-agnostic method to assess model--data alignment.

</details>


### [25] [Modeling Quantum Autoencoder Trainable Kernel for IoT Anomaly Detection](https://arxiv.org/abs/2511.21932)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Swati Kumari,Navneet Singh*

Main category: cs.LG

TL;DR: 提出量子自编码器框架，结合量子支持向量分类进行物联网入侵检测，在NISQ设备上实现实用量子优势


<details>
  <summary>Details</summary>
Motivation: 物联网流量高维复杂性使传统异常检测方法失效，深度学习存在计算瓶颈，需要实时可扩展的解决方案

Method: 量子自编码器压缩网络流量为判别性潜在表示，结合量子支持向量分类进行入侵检测

Result: 在三个数据集上评估，在理想模拟器和IBM量子硬件上实现更高精度，证明当前NISQ设备的实用量子优势

Conclusion: 量子机器学习是解决现实网络安全挑战的可行、硬件就绪方案，适度去极化噪声可作为隐式正则化提升泛化能力

Abstract: Escalating cyber threats and the high-dimensional complexity of IoT traffic have outpaced classical anomaly detection methods. While deep learning offers improvements, computational bottlenecks limit real-time deployment at scale. We present a quantum autoencoder (QAE) framework that compresses network traffic into discriminative latent representations and employs quantum support vector classification (QSVC) for intrusion detection. Evaluated on three datasets, our approach achieves improved accuracy on ideal simulators and on the IBM Quantum hardware demonstrating practical quantum advantage on current NISQ devices. Crucially, moderate depolarizing noise acts as implicit regularization, stabilizing training and enhancing generalization. This work establishes quantum machine learning as a viable, hardware-ready solution for real-world cybersecurity challenges.

</details>


### [26] [Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation](https://arxiv.org/abs/2511.21934)
*Tao Zhe,Huazhen Fang,Kunpeng Liu,Qian Lou,Tamzidul Hoque,Dongjie Wang*

Main category: cs.LG

TL;DR: 提出了一种异构多智能体强化学习框架，用于自动化特征变换，通过共享评论家和多头注意力机制解决动态特征空间和智能体协作问题。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习有进展，但特征变换对于结构化数据仍然至关重要。现有自动化特征变换方法依赖启发式或穷举搜索，效率低下。强化学习方法存在动态特征扩展导致不稳定和智能体间协作不足的问题。

Method: 提出异构多智能体强化学习框架，包含三种异构智能体，分为两类：选择特征和选择操作。采用共享评论家机制促进智能体间通信，使用多头注意力特征智能体处理动态扩展特征空间，引入状态编码技术稳定学习过程。

Result: 通过大量实验验证了模型在有效性、效率、鲁棒性和可解释性方面的优势。

Conclusion: 该框架解决了自动化特征变换中的动态特征扩展和智能体协作问题，实现了更高效、稳定和可解释的特征变换。

Abstract: Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.

</details>


### [27] [Breaking Algorithmic Collusion in Human-AI Ecosystems](https://arxiv.org/abs/2511.21935)
*Natalie Collina,Eshwar Ram Arunachaleswaran,Meena Jagadeesan*

Main category: cs.LG

TL;DR: 研究混合人机生态系统中算法合谋的脆弱性，发现人类参与者的存在会破坏AI代理之间的价格合谋，推动价格趋向竞争水平。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在生态系统中与人类和其他AI反复交互，需要理解这种混合生态系统中算法合谋的稳定性，特别是人类参与者如何影响AI代理之间的价格协调。

Method: 采用重复定价博弈的经典理论框架，构建模型：AI代理采用均衡策略，而人类参与者采用无遗憾策略进行手动定价，分析不同数量人类参与者对价格合谋的影响。

Result: 单个人类参与者的背离就能破坏合谋并压低价格，多个人类参与者会使价格更接近竞争水平；同时揭示了在人类背离意识下AI代理合谋性质的变化。

Conclusion: 算法合谋在混合人机生态系统中的脆弱性取决于人类参与者的存在，这为理解何时算法合谋会持续或崩溃提供了理论特征。

Abstract: AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.

</details>


### [28] [Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection](https://arxiv.org/abs/2511.21940)
*Kiran Nair,Hubert Cecotti*

Main category: cs.LG

TL;DR: 该研究提出多种深度学习架构用于C-VEP解码，其中多类孪生网络在单次试验中达到96.89%的平均准确率，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 基于C-VEP的非侵入式脑机接口需要高度鲁棒的解码方法来应对EEG信号的时间变异性和会话依赖性噪声，传统方法在这方面存在局限性。

Method: 提出并评估了多种深度学习架构：用于63位m序列重建和分类的卷积神经网络、基于相似性解码的孪生网络，以及作为基线的典型相关分析。采用地球移动距离和约束EMD进行距离解码，并通过小位移的时间数据增强提高跨会话泛化能力。

Result: 深度模型显著优于传统方法，基于EMD的距离解码对延迟变化更具鲁棒性。多类孪生网络在所有模型中表现最佳，平均准确率达到96.89%，展示了数据驱动架构在单次试验C-VEP解码中的潜力。

Conclusion: 深度学习架构特别是多类孪生网络，能够实现可靠的单次试验C-VEP解码，为自适应非侵入式脑机接口系统提供了有前景的解决方案。

Abstract: Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.

</details>


### [29] [ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions](https://arxiv.org/abs/2511.21952)
*Krishna Khadka,Sunny Shree,Pujan Budhathoki,Yu Lei,Raghu Kacker,D. Richard Kuhn*

Main category: cs.LG

TL;DR: ABLE方法通过生成对抗点对来定位决策边界，并用线性模型近似局部边界，提高了局部解释的稳定性和保真度。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键应用中缺乏透明度，现有局部解释方法（如LIME）存在不稳定和局部保真度差的问题，需要改进。

Method: 1) 在测试点附近生成带高斯噪声的邻域点；2) 对每个邻域点进行两次对抗攻击，生成跨越决策边界的对抗点对(A,A')；3) 在这些对抗点对上训练线性模型来近似局部决策边界。

Result: 在六个UCI基准数据集和三种深度神经网络架构上的实验表明，ABLE方法在稳定性和保真度方面优于现有最先进方法。

Conclusion: ABLE通过对抗点对定位决策边界的方法，有效提高了局部解释的稳定性和保真度，为黑盒模型提供了更可靠的解释。

Abstract: Machine learning models are increasingly used in critical applications but are mostly "black boxes" due to their lack of transparency. Local explanation approaches, such as LIME, address this issue by approximating the behavior of complex models near a test instance using simple, interpretable models. However, these approaches often suffer from instability and poor local fidelity. In this paper, we propose a novel approach called Adversarially Bracketed Local Explanation (ABLE) to address these limitations. Our approach first generates a set of neighborhood points near the test instance, x_test, by adding bounded Gaussian noise. For each neighborhood point D, we apply an adversarial attack to generate an adversarial point A with minimal perturbation that results in a different label than D. A second adversarial attack is then performed on A to generate a point A' that has the same label as D (and thus different than A). The points A and A' form an adversarial pair that brackets the local decision boundary for x_test. We then train a linear model on these adversarial pairs to approximate the local decision boundary. Experimental results on six UCI benchmark datasets across three deep neural network architectures demonstrate that our approach achieves higher stability and fidelity than the state-of-the-art.

</details>


### [30] [CTR Prediction on Alibaba's Taobao Advertising Dataset Using Traditional and Deep Learning Models](https://arxiv.org/abs/2511.21963)
*Hongyu Yang,Chunxi Wen,Jiyin Zhang,Nanfei Shen,Shijiao Zhang,Xiyan Han*

Main category: cs.LG

TL;DR: 该研究探索了使用淘宝大规模数据集进行CTR预测，从传统机器学习模型到深度学习模型，最终采用Transformer架构处理用户行为序列，取得了显著性能提升，并探讨了该技术在公共卫生等领域的扩展应用。


<details>
  <summary>Details</summary>
Motivation: 点击率预测在现代广告系统中至关重要，直接影响平台效率和商业价值。传统监督学习模型虽然快速可解释，但难以捕捉驱动点击的行为模式，需要更先进的建模方法来理解用户意图和动态兴趣。

Method: 1. 使用大规模淘宝数据集（22天、数亿次交互）；2. 从静态特征（用户人口统计、广告属性、上下文元数据）开始，使用逻辑回归和Light-GBM作为基准；3. 提取和编码用户行为序列，构建时间维度的用户兴趣表示；4. 使用深度学习模型融合行为嵌入和静态特征，包括多层感知机（MLP）；5. 设计基于Transformer的架构，利用自注意力机制学习行为序列的上下文依赖，建模交互内容、时间和频率。

Result: Transformer模型相比基线（逻辑回归）AUC提升了2.81%，对于兴趣多样化或随时间变化的用户群体提升最为显著。深度学习模型（特别是MLP）相比传统监督学习方法取得了显著性能改进。

Conclusion: 该研究为CTR预测提供了从传统方法到深度学习的完整路线图，证明了处理用户行为序列的Transformer架构的有效性。研究还提出了A/B测试策略进行实际评估，并探讨了将个性化广告定向技术扩展到公共卫生等领域的可能性，展示了该技术超越电子商务的潜在价值。

Abstract: Click-through rates prediction is critical in modern advertising systems, where ranking relevance and user engagement directly impact platform efficiency and business value. In this project, we explore how to model CTR more effectively using a large-scale Taobao dataset released by Alibaba. We start with supervised learning models, including logistic regression and Light-GBM, that are trained on static features such as user demographics, ad attributes, and contextual metadata. These models provide fast, interpretable benchmarks, but have limited capabilities to capture patterns of behavior that drive clicks. To better model user intent, we combined behavioral data from hundreds of millions of interactions over a 22-day period. By extracting and encoding user action sequences, we construct representations of user interests over time. We use deep learning models to fuse behavioral embeddings with static features. Among them, multilayer perceptrons (MLPs) have achieved significant performance improvements. To capture temporal dynamics, we designed a Transformer-based architecture that uses a self-attention mechanism to learn contextual dependencies across behavioral sequences, modeling not only what the user interacts with, but also the timing and frequency of interactions. Transformer improves AUC by 2.81 % over the baseline (LR model), with the largest gains observed for users whose interests are diverse or change over time. In addition to modeling, we propose an A/B testing strategy for real-world evaluation. We also think about the broader implications: personalized ad targeting technology can be applied to public health scenarios to achieve precise delivery of health information or behavior guidance. Our research provides a roadmap for advancing click-through rate predictions and extending their value beyond e-commerce.

</details>


### [31] [MOTIF-RF: Multi-template On-chip Transformer Synthesis Incorporating Frequency-domain Self-transfer Learning for RFIC Design Automation](https://arxiv.org/abs/2511.21970)
*Houbo He,Yizhou Xu,Lei Xia,Yaolong Hu,Fan Cai,Taiyun Chi*

Main category: cs.LG

TL;DR: 本文系统研究了多模板机器学习代理模型及其在射频集成电路变压器逆向设计中的应用，提出频率域自迁移学习技术提升建模精度30%-50%，并基于CMA-ES算法开发了逆向设计框架。


<details>
  <summary>Details</summary>
Motivation: 射频集成电路中变压器的设计过程复杂且耗时，传统方法效率低下。需要开发AI辅助的规格到版图自动化工具，为RFIC设计师提供可操作的AI集成工具。

Method: 1. 基准测试四种ML架构（MLP、CNN、UNet、GT）；2. 提出频率域自迁移学习技术，利用相邻频带相关性；3. 基于CMA-ES算法开发逆向设计框架。

Result: 频率域自迁移学习使S参数预测精度提升30%-50%；CMA-ES逆向设计框架在多个阻抗匹配任务中表现出快速收敛和可靠性能。

Conclusion: 该研究推进了RFIC的AI辅助规格到版图自动化目标，为RFIC设计师提供了将AI集成到工作流程中的实用工具，展示了ML在RFIC设计中的有效应用。

Abstract: This paper presents a systematic study on developing multi-template machine learning (ML) surrogate models and applying them to the inverse design of transformers (XFMRs) in radio-frequency integrated circuits (RFICs). Our study starts with benchmarking four widely used ML architectures, including MLP-, CNN-, UNet-, and GT-based models, using the same datasets across different XFMR topologies. To improve modeling accuracy beyond these baselines, we then propose a new frequency-domain self-transfer learning technique that exploits correlations between adjacent frequency bands, leading to around 30%-50% accuracy improvement in the S-parameters prediction. Building on these models, we further develop an inverse design framework based on the covariance matrix adaptation evolutionary strategy (CMA-ES) algorithm. This framework is validated using multiple impedance-matching tasks, all demonstrating fast convergence and trustworthy performance. These results advance the goal of AI-assisted specs-to-GDS automation for RFICs and provide RFIC designers with actionable tools for integrating AI into their workflows.

</details>


### [32] [A Safety and Security Framework for Real-World Agentic Systems](https://arxiv.org/abs/2511.21990)
*Shaona Ghosh,Barnaby Simkin,Kyriacos Shiarlis,Soumili Nandi,Dan Zhao,Matthew Fiedler,Julia Bazinska,Nikki Pope,Roopa Prabhu,Daniel Rohrer,Michael Demoret,Bartley Richardson*

Main category: cs.LG

TL;DR: 提出动态可操作的框架来保护企业级智能体AI系统，将安全视为智能体、编排器、工具和数据动态交互中涌现的属性，通过AI驱动的红队测试发现新型智能体风险并进行缓解。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法主要关注单个模型，而智能体AI系统在部署中面临新型风险，如工具滥用、级联行动链、意外控制放大等，这些风险在传统安全与安全分离的视角下难以识别和应对。

Method: 提出动态智能体安全与安全框架，通过辅助AI模型和智能体在人类监督下进行上下文风险发现、评估和缓解；采用沙盒化、AI驱动的红队测试进行风险发现；定义统一的智能体风险分类法。

Result: 在NVIDIA旗舰智能体研究助手AI-Q Research Assistant上进行详细案例研究，展示了在复杂企业级智能体工作流中端到端的安全评估；发现了新型智能体风险并进行上下文缓解；发布了包含10,000多次真实攻击和防御执行轨迹的数据集。

Conclusion: 智能体AI系统的安全需要动态、上下文感知的方法，将传统安全与安全关注点与新型智能体特有风险统一管理；提出的框架能有效发现和缓解企业部署中的智能体风险，相关数据集将推动智能体安全研究。

Abstract: This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.

</details>


### [33] [Distance-based Learning of Hypertrees](https://arxiv.org/abs/2511.22014)
*Shaun Fallat,Kamyar Khodamoradi,David Kirkpatrick,Valerii Maliuk,S. Ahmad Mojallal,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文研究了通过最短路径查询学习超图的问题，提出了首个可证明最优的在线算法用于学习有序超树，并建立了有界距离查询模型下的复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 超图学习在数据库理论和进化树重建等领域有重要应用。传统方法查询复杂度高，需要更高效的算法。特别是在进化树重建等场景中，距离测量会随距离增加而退化，需要新的学习模型。

Method: 1. 针对有序超树（orderly hypertrees）提出了首个可证明最优的在线算法
2. 该在线算法可转换为可证明最优的离线算法
3. 建立了有界距离查询模型，在该模型下分析了一般超树学习的复杂度界限

Result: 1. 对于有序超树类，算法在最短路径查询复杂度上达到最优
2. 有序超树位于Fagin层次结构中，且严格包含该层次结构中可用亚二次查询复杂度学习的最广泛类别
3. 在有界距离查询模型下，获得了学习一般超树的渐近紧复杂度界限

Conclusion: 本文为超图学习提供了理论突破，首次为有序超树类设计了最优算法，并建立了有界距离查询模型下的完整复杂度理论框架，对数据库理论和进化生物学等领域有重要应用价值。

Abstract: We study the problem of learning hypergraphs with shortest-path queries (SP-queries), and present the first provably optimal online algorithm for a broad and natural class of hypertrees that we call orderly hypertrees. Our online algorithm can be transformed into a provably optimal offline algorithm. Orderly hypertrees can be positioned within the Fagin hierarchy of acyclic hypergraph (well-studied in database theory), and strictly encompass the broadest class in this hierarchy that is learnable with subquadratic SP-query complexity.
  Recognizing that in some contexts, such as evolutionary tree reconstruction, distance measurements can degrade with increased distance, we also consider a learning model that uses bounded distance queries. In this model, we demonstrate asymptotically tight complexity bounds for learning general hypertrees.

</details>


### [34] [Equilibrium Propagation Without Limits](https://arxiv.org/abs/2511.22024)
*Elon Litman*

Main category: cs.LG

TL;DR: 该论文为平衡传播建立了有限扰动理论基础，证明对比赫布学习更新是任意有限扰动的精确梯度估计器，无需无穷小近似或凸性假设。


<details>
  <summary>Details</summary>
Motivation: 传统平衡传播方法依赖于无穷小扰动近似，限制了其在实际应用中的有效性。作者希望将平衡传播从无穷小扰动的限制中解放出来，建立更实用的有限扰动理论基础。

Method: 通过将网络状态建模为吉布斯-玻尔兹曼分布而非确定性点，证明赫姆霍兹自由能差值的梯度等于局部能量导数的期望差值。推导了基于损失-能量协方差路径积分的广义平衡传播算法。

Result: 验证了经典对比赫布学习更新是任意有限扰动的精确梯度估计器，无需无穷小近似或凸性假设。新算法支持强误差信号，而标准无穷小近似无法处理这种情况。

Conclusion: 该研究为平衡传播建立了坚实的有限扰动理论基础，使该算法能够处理更实际的强误差信号场景，扩展了其应用范围。

Abstract: We liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.

</details>


### [35] [Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation](https://arxiv.org/abs/2511.22030)
*Geun-Deok Jang,Dong-Kyun Han,Seo-Hyeon Park,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出基于在线测试时适应的驾驶员疲劳检测框架，通过动态调整批归一化层参数和记忆库管理，解决EEG信号的域偏移问题，在模拟驾驶数据集上取得81.73%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳驾驶是交通事故的主要原因之一，基于EEG的疲劳检测系统面临信号变异性大、需要繁琐校准的问题。特别是EEG信号的受试者间变异性导致域偏移问题，使得模型难以泛化到未见过的目标受试者。

Method: 提出在线测试时适应框架：1）动态调整批归一化层中的可学习参数，同时保留预训练的归一化统计量；2）引入记忆库动态管理流式EEG片段，基于负能量分数和持续时间的可靠性选择样本；3）采用原型学习确保对时间分布偏移的鲁棒预测。

Result: 在模拟环境中的持续注意力驾驶数据集上验证，该方法优于所有基线，平均F1分数达到81.73%，比最佳TTA基线提高了11.73%。

Conclusion: 提出的方法显著增强了EEG-based疲劳检测系统在非独立同分布场景下的适应性，为解决EEG信号变异性问题提供了有效方案。

Abstract: Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\%, an improvement of 11.73\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.

</details>


### [36] [Predicting Public Health Impacts of Electricity Usage](https://arxiv.org/abs/2511.22031)
*Yejia Liu,Zhifeng Wu,Pengfei Li,Shaolei Ren*

Main category: cs.LG

TL;DR: HealthPredictor是一个端到端的AI模型，将电力使用与公共健康结果联系起来，通过优化电力需求管理来减少空气污染对公共健康的影响。


<details>
  <summary>Details</summary>
Motivation: 电力行业是空气污染的主要来源，尽管已有监管措施，但化石燃料仍是能源供应的重要组成部分，需要更先进的需求侧管理方法来减少公共健康影响。

Method: HealthPredictor包含三个组件：燃料组合预测器（估计不同发电来源的贡献）、空气质量转换器（模拟污染物排放和大气扩散）、健康影响评估器（将污染物变化转化为货币化的健康损害）。

Result: 在美国多个地区，基于健康驱动的优化框架在公共健康影响方面的预测误差显著低于基于燃料组合的基线方法。电动汽车充电调度的案例研究展示了该方法带来的公共健康收益和可操作的指导。

Conclusion: 这项工作展示了AI模型如何被明确设计用于支持健康导向的能源管理，以促进公共健康和更广泛的社会福祉。

Abstract: The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.

</details>


### [37] [Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian](https://arxiv.org/abs/2511.22069)
*Yiran Zhang,Weihang Xu,Mo Zhou,Maryam Fazel,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 论文研究了过参数化模型在单高斯分布上的分数匹配优化动态，证明了不同噪声机制下的收敛性结果，包括大噪声下的全局收敛、小噪声下的特定初始化收敛，以及随机初始化下的参数发散但损失收敛现象。


<details>
  <summary>Details</summary>
Motivation: 尽管分数匹配在现代生成建模（特别是扩散模型）中取得了经验成功，但其在过参数化机制下的优化行为理论理解仍然有限。本文旨在填补这一理论空白，通过分析在单高斯分布上训练过参数化模型的梯度下降动态。

Method: 使用具有n个可学习参数的学生模型，在从单真实高斯生成的数据上训练，采用总体分数匹配目标。分析不同机制下的优化动态：大噪声机制、小噪声机制、指数小初始化和随机高斯初始化。

Result: 1) 噪声足够大时，证明了梯度下降的全局收敛性；2) 低噪声机制下存在驻点，但指数小初始化时所有参数收敛到真实值；3) 无指数小初始化时参数可能不收敛；4) 随机高斯初始化时，高概率下只有一个参数收敛而其他发散，但损失以1/τ速率收敛到零，并建立了匹配的下界。

Conclusion: 这是首个在分数匹配框架下为至少三个分量的高斯混合建立全局收敛保证的工作，揭示了过参数化分数匹配优化的复杂动态，包括参数发散但损失收敛的奇特现象，为理解现代生成模型的优化行为提供了理论洞见。

Abstract: Score matching has become a central training objective in modern generative modeling, particularly in diffusion models, where it is used to learn high-dimensional data distributions through the estimation of score functions. Despite its empirical success, the theoretical understanding of the optimization behavior of score matching, particularly in over-parameterized regimes, remains limited. In this work, we study gradient descent for training over-parameterized models to learn a single Gaussian distribution. Specifically, we use a student model with $n$ learnable parameters and train it on data generated from a single ground-truth Gaussian using the population score matching objective. We analyze the optimization dynamics under multiple regimes. When the noise scale is sufficiently large, we prove a global convergence result for gradient descent. In the low-noise regime, we identify the existence of a stationary point, highlighting the difficulty of proving global convergence in this case. Nevertheless, we show convergence under certain initialization conditions: when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters to the ground truth. We further prove that without the exponentially small initialization, the parameters may not converge to the ground truth. Finally, we consider the case where parameters are randomly initialized from a Gaussian distribution far from the ground truth. We prove that, with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero with a $1/τ$ rate, where $τ$ is the number of iterations. We also establish a nearly matching lower bound on the convergence rate in this regime. This is the first work to establish global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework.

</details>


### [38] [A Multi-View Multi-Timescale Hypergraph-Empowered Spatiotemporal Framework for EV Charging Forecasting](https://arxiv.org/abs/2511.22072)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: HyperCast：基于超图建模高阶时空依赖的电动汽车充电需求预测框架，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的方法只能建模充电站之间的成对关系，无法捕捉城市充电网络中复杂的群体动态和集体充电行为模式

Method: 开发HyperCast框架，利用超图建模EV充电模式中的高阶时空依赖；集成多视图超图（静态地理邻近性和动态需求相似性）和多时间尺度输入；采用专门的超时空块和定制的交叉注意力机制融合不同视图和时间尺度的信息

Result: 在四个公共数据集上的广泛实验表明，HyperCast显著优于多种最先进的基线方法，证明了显式建模集体充电行为对提高预测准确性的有效性

Conclusion: 通过超图建模高阶时空依赖关系，能够更准确地预测电动汽车充电需求，为电网稳定运行和电动汽车参与电力市场提供支持

Abstract: Accurate electric vehicle (EV) charging demand forecasting is essential for stable grid operation and proactive EV participation in electricity market. Existing forecasting methods, particularly those based on graph neural networks, are often limited to modeling pairwise relationships between stations, failing to capture the complex, group-wise dynamics inherent in urban charging networks. To address this gap, we develop a novel forecasting framework namely HyperCast, leveraging the expressive power of hypergraphs to model the higher-order spatiotemporal dependencies hidden in EV charging patterns. HyperCast integrates multi-view hypergraphs, which capture both static geographical proximity and dynamic demand-based functional similarities, along with multi-timescale inputs to differentiate between recent trends and weekly periodicities. The framework employs specialized hyper-spatiotemporal blocks and tailored cross-attention mechanisms to effectively fuse information from these diverse sources: views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms a wide array of state-of-the-art baselines, demonstrating the effectiveness of explicitly modeling collective charging behaviors for more accurate forecasting.

</details>


### [39] [ARES: Anomaly Recognition Model For Edge Streams](https://arxiv.org/abs/2511.22078)
*Simone Mungari,Albert Bifet,Giuseppe Manco,Bernhard Pfahringer*

Main category: cs.LG

TL;DR: ARES是一个用于时序图边流异常检测的无监督框架，结合图神经网络和半空间树，无需数据标注，通过监督阈值机制优化检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的流式信息常表示为时序图，边随时间动态变化。实时检测边异常对风险缓解至关重要，但传统方法面临概念漂移、数据量大和实时响应需求等挑战。

Method: ARES结合图神经网络(GNN)进行特征提取和半空间树(HST)进行异常评分。GNN在潜在空间中嵌入节点和边属性以捕获尖峰和突发异常行为，HST高效划分空间隔离异常。还加入基于统计离散度的监督阈值机制，使用少量标注数据确定最优阈值。

Result: 在多个真实网络攻击场景中进行了广泛评估，与现有方法相比表现出优越性能，同时分析了其空间和时间复杂度。

Conclusion: ARES是一个有效的无监督时序图边流异常检测框架，能够应对概念漂移、大数据量和实时需求等挑战，在不同领域具有良好适应性。

Abstract: Many real-world scenarios involving streaming information can be represented as temporal graphs, where data flows through dynamic changes in edges over time. Anomaly detection in this context has the objective of identifying unusual temporal connections within the graph structure. Detecting edge anomalies in real time is crucial for mitigating potential risks. Unlike traditional anomaly detection, this task is particularly challenging due to concept drifts, large data volumes, and the need for real-time response. To face these challenges, we introduce ARES, an unsupervised anomaly detection framework for edge streams. ARES combines Graph Neural Networks (GNNs) for feature extraction with Half-Space Trees (HST) for anomaly scoring. GNNs capture both spike and burst anomalous behaviors within streams by embedding node and edge properties in a latent space, while HST partitions this space to isolate anomalies efficiently. ARES operates in an unsupervised way without the need for prior data labeling. To further validate its detection capabilities, we additionally incorporate a simple yet effective supervised thresholding mechanism. This approach leverages statistical dispersion among anomaly scores to determine the optimal threshold using a minimal set of labeled data, ensuring adaptability across different domains. We validate ARES through extensive evaluations across several real-world cyber-attack scenarios, comparing its performance against existing methods while analyzing its space and time complexity.

</details>


### [40] [Quantum Bayesian Optimization for Quality Improvement in Fuselage Assembly](https://arxiv.org/abs/2511.22090)
*Jiayu Liu,Chong Liu,Trevor Rhone,Yinan Wang*

Main category: cs.LG

TL;DR: 提出量子贝叶斯优化框架，用于航空航天机身装配中的形状控制，相比经典方法显著提高样本效率并降低尺寸误差


<details>
  <summary>Details</summary>
Motivation: 现有航空航天机身装配中的形状调整方法面临样本效率低的问题，源于经典蒙特卡洛方法在从分布中揭示平均响应时的局限性。量子算法能以更少样本达到相同估计精度，这为改进制造实践中的样本效率提供了机会

Method: 提出量子贝叶斯优化框架，利用基于有限元分析模型或代理模型的量子预言机，以更少查询获得更准确的环境响应估计。采用上置信界作为采集函数，战略性地选择最可能最大化目标函数的输入值

Result: 实验结果表明，相比经典方法，QBO在相同仿真查询下实现了显著更低的尺寸误差和不确定性，理论上证明能以更少样本保持相当的优化结果

Conclusion: 量子贝叶斯优化框架能有效提高航空航天机身装配中的样本效率，实现更精确的形状控制，为智能制造中的优化问题提供了量子优势

Abstract: Recent efforts in smart manufacturing have enhanced aerospace fuselage assembly processes, particularly by innovating shape adjustment techniques to minimize dimensional gaps between assembled sections. Existing approaches have shown promising results but face the issue of low sample efficiency from the manufacturing systems. It arises from the limitation of the classical Monte Carlo method when uncovering the mean response from a distribution. In contrast, recent work has shown that quantum algorithms can achieve the same level of estimation accuracy with significantly fewer samples than the classical Monte Carlo method from distributions. Therefore, we can adopt the estimation of the quantum algorithm to obtain the estimation from real physical systems (distributions). Motivated by this advantage, we propose a Quantum Bayesian Optimization (QBO) framework for precise shape control during assembly to improve the sample efficiency in manufacturing practice. Specifically, this approach utilizes a quantum oracle, based on finite element analysis (FEA)-based models or surrogate models, to acquire a more accurate estimation of the environment response with fewer queries for a certain input. QBO employs an Upper Confidence Bound (UCB) as the acquisition function to strategically select input values that are most likely to maximize the objective function. It has been theoretically proven to require much fewer samples while maintaining comparable optimization results. In the case study, force-controlled actuators are applied to one fuselage section to adjust its shape and reduce the gap to the adjoining section. Experimental results demonstrate that QBO achieves significantly lower dimensional error and uncertainty compared to classical methods, particularly using the same queries from the simulation.

</details>


### [41] [Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs](https://arxiv.org/abs/2511.22099)
*Daniel Agyei Asante,Md Mokarram Chowdhury,Yang Li*

Main category: cs.LG

TL;DR: 低秩压缩在减少LLM规模的同时，对隐私、对抗鲁棒性、公平性和伦理对齐等可信度指标产生复杂影响：隐私保护在训练数据方面保持或改善，但在对话中PII保护减弱；对抗鲁棒性保持或增强；伦理推理在零样本设置下下降但可通过少样本提示部分恢复；公平性在压缩后下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限环境中的部署受到其巨大规模的限制。虽然低秩分解等模型压缩方法能有效减少模型大小、内存和计算需求，但这些压缩模型的可信度影响尚未得到充分研究。本研究旨在全面探索低秩压缩对LLM可信度的影响。

Method: 使用多种低秩算法压缩不同规模和变体的LLM，评估压缩对隐私、对抗鲁棒性、公平性和伦理对齐的影响。采用梯度归因分析识别对对抗鲁棒性贡献最大的层，并研究模型规模和微调对可信度的影响。

Result: 低秩压缩对可信度产生多方面影响：训练数据隐私保持或改善，但对话中PII保护减弱；对抗鲁棒性普遍保持且常增强；伦理推理在零样本设置下下降，但少样本提示可部分恢复；公平性在压缩后下降。模型规模和微调对可信度有重要影响。

Conclusion: 低秩压缩在减少LLM规模的同时，对可信度产生复杂影响。研究为可信压缩策略提供指导，强调需要在压缩效率与可信度保护之间取得平衡，并建议针对不同可信度维度采用特定优化方法。

Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.

</details>


### [42] [Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba](https://arxiv.org/abs/2511.22101)
*Zhaofeng Zhang*

Main category: cs.LG

TL;DR: 复制并改进Uniswap V3自适应流动性供给的深度强化学习研究，提出结合Mamba与DDQN的新结构


<details>
  <summary>Details</summary>
Motivation: 原论文提出使用深度强化学习优化Uniswap V3流动性供给，但存在改进空间。本研究旨在复制原方法并设计更优的模型结构

Method: 1. 复制原方法：从Uniswap Subgraph获取数据，实现原模型；2. 改进方法：提出结合Mamba与DDQN的新结构，使用新的奖励函数，重新清洗数据，引入两个新基线进行对比

Result: 虽然新模型尚未应用于所有数据集，但相比原模型具有更强的理论支持，并在部分测试中表现更好

Conclusion: 提出的Mamba+DDQN新结构在理论上更优，实验初步验证了其有效性，为Uniswap V3流动性供给优化提供了改进方案

Abstract: The report goes through the main steps of replicating and improving the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.

</details>


### [43] [Representative Action Selection for Large Action Space: From Bandits to MDPs](https://arxiv.org/abs/2511.22104)
*Quan Zhou,Shie Mannor*

Main category: cs.LG

TL;DR: 该研究提出从大型动作空间中选取代表性子集的方法，使强化学习能在组合决策问题中高效运行，无需评估所有动作。


<details>
  <summary>Details</summary>
Motivation: 在库存管理和推荐系统等应用中，动作空间极大，直接在整个空间上学习不可行。需要找到能在所有环境中都包含近似最优动作的固定子集，以实现高效学习。

Method: 将元多臂赌博机（meta-bandits）的方法扩展到马尔可夫决策过程（MDPs），使用现有算法在放松的非中心化次高斯过程模型下工作，适应更大的环境异质性。

Result: 证明现有算法能达到与使用完整动作空间相当的性能，为大规模组合决策提供计算和样本高效的解决方案。

Conclusion: 该方法为不确定性下的大规模组合决策问题提供了理论保证和实用解决方案，能够处理环境异质性并保持学习效率。

Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.

</details>


### [44] [Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.22105)
*Saad Masrur,Ismail Guvenc,David Lopez Perez*

Main category: cs.LG

TL;DR: 提出基于多智能体深度强化学习（MARL-DDQN）的毫米波网络动态睡眠模式优化方法，在3D城市环境中实现能效最大化并满足服务质量约束


<details>
  <summary>Details</summary>
Motivation: 现有毫米波网络睡眠模式优化方法依赖静态基站流量模型，无法捕捉非平稳流量动态，且状态-动作空间过大，限制了实际部署。需要解决动态环境下的能效优化问题。

Method: 提出MARL-DDQN框架，采用多智能体深度强化学习和双深度Q网络，在3D城市环境中结合时变社区化用户设备移动模型。集成实际基站功耗模型和波束赋形，以分布式决策最小化信令开销。

Result: MARL-DDQN在动态场景下优于现有策略（All On、IT-QoS-LB、MARL-DDPG、MARL-PPO），达到0.60 Mbit/Joule能效、8.5 Mbps第10百分位吞吐量，95%时间满足QoS约束。

Conclusion: MARL-DDQN框架能有效解决毫米波网络动态睡眠模式优化问题，在保证服务质量的同时显著提升能效，为实际部署提供了可行方案。

Abstract: Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.

</details>


### [45] [An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface](https://arxiv.org/abs/2511.22108)
*Zhou Biyan,Arindam Basu*

Main category: cs.LG

TL;DR: 本文提出适用于深度脉冲神经网络的连续学习方法，采用Banditron和AGREL两种强化学习算法，在植入式脑机接口中实现高效、低功耗的神经解码器，显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 随着植入式脑机接口记录神经元数量呈指数增长，需要在植入设备中集成神经解码器以实现数据压缩。然而，系统的非平稳性使解码器性能不可靠，频繁重新训练不现实，因此需要连续学习方法来确保iBMI用户的安全和舒适。

Method: 提出适用于深度脉冲神经网络的连续学习方法，采用Banditron和AGREL两种强化学习算法。这些算法计算资源需求有限，能有效处理非平稳性问题，适合植入式设备的能量限制。通过开环和闭环实验评估方法效果。

Result: 开环实验中，DSNN Banditron和DSNN AGREL的准确率在长时间内保持稳定。闭环扰动实验中，DSNN Banditron性能与DSNN AGREL相当，同时训练期间内存访问使用减少98%，乘累加操作需求减少99%。相比先前连续学习SNN解码器，DSNN Banditron计算需求减少98%。

Conclusion: DSNN Banditron是未来无线iBMI系统的理想候选方案，在保持性能的同时显著降低了计算资源需求，适合植入式设备的能量限制。

Abstract: The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.

</details>


### [46] [Toward Data-Driven Surrogates of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.22112)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 开发基于球形傅里叶神经算子（SFNO）的太阳风稳态建模替代模型，相比传统数值替代模型HUX在多个指标上表现相当或更好，支持高效实时空间天气预报。


<details>
  <summary>Details</summary>
Motivation: 太阳风变化（如高速流和日冕物质抛射）会破坏卫星、电网和通信系统，需要准确建模进行空间天气预报。传统的3D磁流体动力学模型计算成本高，限制了边界条件不确定性研究。

Method: 使用球形傅里叶神经算子（SFNO）开发太阳风稳态建模的替代模型，并与先前开发的数值替代模型HUX进行比较。

Result: SFNO在多个指标上达到与HUX相当或更好的性能。虽然HUX在物理平滑性方面仍有优势，但这凸显了需要改进评估标准而非SFNO的缺陷。

Conclusion: SFNO作为一种灵活可训练的替代模型，能够实现高效实时预报，并能随着更多数据而改进，为太阳风建模提供了有前景的解决方案。

Abstract: The solar wind, a continuous stream of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Variations such as high-speed streams and coronal mass ejections can disrupt satellites, power grids, and communications, making accurate modeling essential for space weather forecasting. While 3D magnetohydrodynamic (MHD) models are used to simulate and investigate these variations in the solar wind, they tend to be computationally expensive, limiting their usefulness in investigating the impacts of boundary condition uncertainty. In this work, we develop a surrogate for steady state solar wind modeling, using a Spherical Fourier Neural Operator (SFNO). We compare our model to a previously developed numerical surrogate for this task called HUX, and we show that the SFNO achieves comparable or better performance across several metrics. Though HUX retains advantages in physical smoothness, this underscores the need for improved evaluation criteria rather than a flaw in SFNO. As a flexible and trainable approach, SFNO enables efficient real-time forecasting and can improve with more data. The source code and more visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity.

</details>


### [47] [IVGAE: Handling Incomplete Heterogeneous Data with a Variational Graph Autoencoder](https://arxiv.org/abs/2511.22116)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal%*

Main category: cs.LG

TL;DR: IVGAE是一个基于变分图自编码器的框架，用于处理异构数据（数值和分类特征）的缺失值填补，通过构建样本-特征二分图和使用双解码器架构来建模结构依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的表格数据集经常包含缺失值，特别是当数据同时包含数值和分类特征时。现有的填补方法往往难以捕捉复杂的结构依赖关系，也无法有效处理异构数据。

Method: IVGAE构建样本-特征二分图来表示样本与特征之间的关系，应用图表示学习来建模结构依赖。采用双解码器架构：一个解码器重建特征嵌入，另一个建模缺失模式。对于分类变量，引入了基于Transformer的异构嵌入模块来避免高维one-hot编码。

Result: 在16个真实世界数据集上的实验表明，IVGAE在MCAR、MAR和MNAR三种缺失机制下，在30%缺失率的情况下，在RMSE和下游F1分数上都取得了持续的改进。

Conclusion: IVGAE通过图表示学习和双解码器架构，为异构数据的缺失值填补提供了一个有效的解决方案，能够更好地捕捉数据中的结构依赖关系。

Abstract: Handling missing data remains a fundamental challenge in real-world tabular datasets, especially when data are heterogeneous with both numerical and categorical features. Existing imputation methods often fail to capture complex structural dependencies and handle heterogeneous data effectively. We present \textbf{IVGAE}, a Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data. IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning to model structural dependencies. A key innovation is its \textit{dual-decoder architecture}, where one decoder reconstructs feature embeddings and the other models missingness patterns, providing structural priors aware of missing mechanisms. To better encode categorical variables, we introduce a Transformer-based heterogeneous embedding module that avoids high-dimensional one-hot encoding. Extensive experiments on 16 real-world datasets show that IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30\% missing rates. Code and data are available at: https://github.com/echoid/IVGAE.

</details>


### [48] [A Variational Manifold Embedding Framework for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2511.22128)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 提出一个变分框架，将降维算法视为最优流形嵌入问题的解，比PCA更灵活且具有可解释性


<details>
  <summary>Details</summary>
Motivation: 现有降维方法各有局限：PCA变体简单可解释但无法捕捉非线性流形结构；自编码器难以解释；基于图嵌入的方法可能导致流形几何的病态扭曲

Method: 提出变分框架，将降维算法形式化为最优流形嵌入问题的解，通过构造允许非线性嵌入，满足偏微分方程并反映嵌入目标的对称性

Result: 该框架比PCA更灵活，具有更好的可解释性，在某些情况下可以解析表征，特殊情况下能精确恢复PCA

Conclusion: 提出的变分框架为降维提供了统一视角，既保持了PCA的可解释性，又克服了其非线性建模的局限性

Abstract: Dimensionality reduction algorithms like principal component analysis (PCA) are workhorses of machine learning and neuroscience, but each has well-known limitations. Variants of PCA are simple and interpretable, but not flexible enough to capture nonlinear data manifold structure. More flexible approaches have other problems: autoencoders are generally difficult to interpret, and graph-embedding-based methods can produce pathological distortions in manifold geometry. Motivated by these shortcomings, we propose a variational framework that casts dimensionality reduction algorithms as solutions to an optimal manifold embedding problem. By construction, this framework permits nonlinear embeddings, allowing its solutions to be more flexible than PCA. Moreover, the variational nature of the framework has useful consequences for interpretability: each solution satisfies a set of partial differential equations, and can be shown to reflect symmetries of the embedding objective. We discuss these features in detail and show that solutions can be analytically characterized in some cases. Interestingly, one special case exactly recovers PCA.

</details>


### [49] [Benchmarking In-context Experiential Learning Through Repeated Product Recommendations](https://arxiv.org/abs/2511.22130)
*Gilbert Yang,Yaqin Chen,Thomson Yen,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 论文提出了BELA基准测试，用于评估智能体在动态真实环境中的经验学习和主动探索能力，特别是在产品推荐场景中。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要关注确定性任务，无法衡量智能体通过积累经验进行自适应学习和推理的能力。真实世界环境具有不完整知识和动态变化的特点，需要智能体具备情境经验学习能力。

Method: 构建BELA基准测试，包含：(1) 来自亚马逊的真实产品数据；(2) 代表异质潜在偏好的多样化用户画像；(3) 基于用户画像的LLM用户模拟器，用于生成丰富的交互轨迹。

Result: 当前前沿模型在跨情景学习中难以显著改进，突显了需要具备强大情境学习能力的智能体系统。

Conclusion: 需要开发能够在动态环境中通过经验积累进行自适应学习和推理的智能体系统，BELA基准为此提供了评估框架。

Abstract: To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.

</details>


### [50] [Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks](https://arxiv.org/abs/2511.22133)
*Sahil Kashyap,Rajdip Nayek*

Main category: cs.LG

TL;DR: 提出一个概率数字孪生框架，用于处理物理模型错误指定的动力学系统响应预测，结合高斯过程潜在力模型和贝叶斯神经网络实现端到端不确定性感知推理


<details>
  <summary>Details</summary>
Motivation: 解决动力学系统中物理模型错误指定问题，开发能够系统传播不确定性、可信赖的数字孪生框架，实现从诊断到预测的完整不确定性量化

Method: 1. 诊断阶段：将模型形式误差视为线性动力系统的潜在输入力，使用高斯过程潜在力模型从传感器测量中联合估计系统状态和模型误差；2. 训练贝叶斯神经网络学习从系统状态到模型误差的概率非线性映射；3. 预测阶段：使用该映射生成伪测量，通过卡尔曼滤波进行状态预测

Result: 框架在四个非线性示例中验证：单自由度振荡器、多自由度系统、Bouc-Wen迟滞系统和Silverbox实验数据集，展示了预测准确性和对模型错误指定的鲁棒性

Conclusion: 该概率数字孪生框架能够系统传播从诊断到预测的不确定性，为处理物理模型错误指定的动力学系统提供了可信赖的响应预测解决方案

Abstract: This work presents a probabilistic digital twin framework for response prediction in dynamical systems governed by misspecified physics. The approach integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to enable end-to-end uncertainty-aware inference and prediction. In the diagnosis phase, model-form errors (MFEs) are treated as latent input forces to a nominal linear dynamical system and jointly estimated with system states using GPLFM from sensor measurements. A BNN is then trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, while capturing diagnostic uncertainty. For prognosis, this mapping is used to generate pseudo-measurements, enabling state prediction via Kalman filtering. The framework allows for systematic propagation of uncertainty from diagnosis to prediction, a key capability for trustworthy digital twins. The framework is demonstrated using four nonlinear examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, and two established benchmarks -- the Bouc-Wen hysteretic system and the Silverbox experimental dataset -- highlighting its predictive accuracy and robustness to model misspecification.

</details>


### [51] [TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices](https://arxiv.org/abs/2511.22138)
*Mohd Ariful Haque,Fahad Rahman,Kishor Datta Gupta,Khalil Shujaee,Roy George*

Main category: cs.LG

TL;DR: 研究评估小型语言模型在边缘设备上执行代理任务的能力，通过多种优化策略在BFCL基准测试中达到65.74%的总体准确率。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型在边缘设备上执行代理任务（函数/工具/API调用）的可行性，实现不依赖云基础设施的隐私保护、低延迟自主代理。

Method: 使用Berkeley Function Calling Leaderboard框架评估SLMs，采用参数驱动优化策略：监督微调、参数高效微调、强化学习优化、直接偏好优化以及混合方法，并在AgentBank数据上构建DPO训练管道。

Result: 中等规模模型（1-3B参数）显著优于超紧凑模型（<1B参数），混合优化达到65.74%总体准确率和55.62%多轮对话准确率。

Conclusion: 混合优化策略使小型语言模型能够在边缘设备上提供准确、高效、稳定的代理AI，实现超越云端的隐私保护、低延迟自主代理应用。

Abstract: This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.

</details>


### [52] [From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures](https://arxiv.org/abs/2511.22150)
*Florian Rottach,William Rudman,Bastain Rieck,Harrisen Scells,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 该论文提出统一拓扑签名(UTS)框架，用于全面分析文本嵌入空间的组织结构，发现拓扑几何特征与下游任务性能的关联。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入在空间中的组织方式不仅能增强模型可解释性，还能揭示驱动下游任务性能的因素。当前缺乏对文本嵌入模型拓扑几何特征的全面分析框架。

Method: 首先对多种文本嵌入模型和数据集进行拓扑几何度量的综合分析，发现现有指标的冗余性和局限性。基于此提出统一拓扑签名(UTS)框架，通过多属性视角全面表征嵌入空间。

Result: UTS能够预测模型特定属性、揭示由模型架构驱动的相似性，并将拓扑结构与排名效果关联，准确预测文档可检索性。证明多属性视角对理解嵌入空间几何至关重要。

Conclusion: 需要采用整体、多属性的视角来理解和利用文本嵌入的几何结构。UTS框架为分析嵌入空间提供了有效的工具，能够揭示模型特性与下游任务性能的关联。

Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.

</details>


### [53] [Designing Instance-Level Sampling Schedules via REINFORCE with James-Stein Shrinkage](https://arxiv.org/abs/2511.22177)
*Peiyu Yu,Suraj Kothawade,Sirui Xie,Ying Nian Wu,Hongliang Fei*

Main category: cs.LG

TL;DR: 提出一种新的后处理方法：通过学习实例级采样时间表来改进冻结的文本到图像采样器，而不是调整模型权重


<details>
  <summary>Details</summary>
Motivation: 大多数后处理方法专注于模型权重调整（微调对齐或蒸馏加速），本文探索不同的路线：重新调度冻结采样器的采样时间表，以释放预训练采样器的额外生成潜力

Method: 1) 学习实例级（提示和噪声条件）采样时间表，而非固定全局时间表；2) 使用单次Dirichlet策略；3) 引入基于James-Stein估计器的新奖励基线，降低高维策略学习中的梯度估计误差

Result: 1) 在Stable Diffusion和Flux模型家族中一致改进文本-图像对齐，包括文本渲染和组合控制；2) 5步Flux-Dev采样器使用该方法可达到与专门蒸馏的Flux-Schnell采样器相当的生成质量

Conclusion: 该方法作为一种新兴的模型无关后处理杠杆，能够解锁预训练采样器的额外生成潜力，为文本到图像生成提供了新的优化维度

Abstract: Most post-training methods for text-to-image samplers focus on model weights: either fine-tuning the backbone for alignment or distilling it for few-step efficiency. We take a different route: rescheduling the sampling timeline of a frozen sampler. Instead of a fixed, global schedule, we learn instance-level (prompt- and noise-conditioned) schedules through a single-pass Dirichlet policy. To ensure accurate gradient estimates in high-dimensional policy learning, we introduce a novel reward baseline based on a principled James-Stein estimator; it provably achieves lower estimation errors than commonly used variants and leads to superior performance. Our rescheduled samplers consistently improve text-image alignment including text rendering and compositional control across modern Stable Diffusion and Flux model families. Additionally, a 5-step Flux-Dev sampler with our schedules can attain generation quality comparable to deliberately distilled samplers like Flux-Schnell. We thus position our scheduling framework as an emerging model-agnostic post-training lever that unlocks additional generative potential in pretrained samplers.

</details>


### [54] [PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units](https://arxiv.org/abs/2511.22199)
*Sejeong Jang,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: PULSE-ICU是一个自监督基础模型，从大规模EHR序列中学习事件级ICU表示，无需重采样或手动特征工程，在18个预测任务上表现优异，并在外部验证中显示出对领域转移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ICU数据高度不规则、异质且时间碎片化，这对可泛化的临床预测提出了挑战。现有方法需要重采样或手动特征工程，限制了模型的通用性和适应性。

Method: 提出PULSE-ICU自监督基础模型：1）统一嵌入模块编码事件身份、连续值、单位和时间属性；2）基于Longformer的编码器高效建模长轨迹；3）在18个预测任务上进行微调，包括死亡率、干预预测和表型识别。

Result: 在18个预测任务上取得强劲性能，在eICU、HiRID和P12的外部验证中显示出显著改进，仅需最小微调，证明了对领域转移和变量约束的鲁棒性。

Conclusion: 基础式建模可以提高数据效率和适应性，为不同临床环境中的ICU决策支持提供可扩展框架。

Abstract: Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.

</details>


### [55] [BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning](https://arxiv.org/abs/2511.22210)
*Junsung Park*

Main category: cs.LG

TL;DR: BiCQL-ML是一种免策略的离线逆强化学习算法，通过双层框架联合优化奖励函数和保守Q函数，避免显式策略学习，在标准离线RL基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 离线逆强化学习（IRL）旨在仅使用固定的演示数据恢复解释专家行为的奖励函数，而无需额外的在线交互。现有方法通常需要显式策略学习，这可能导致次优性能。

Method: 提出BiCQL-ML算法，采用双层框架交替优化：(1) 在当前奖励下通过保守Q学习（CQL）学习保守Q函数；(2) 更新奖励参数以最大化专家动作的期望Q值，同时抑制对分布外动作的过度泛化。该方法可视为基于软值匹配原则的最大似然估计。

Result: 理论保证BiCQL-ML收敛到使专家策略达到软最优的奖励函数。在标准离线RL基准测试中，BiCQL-ML在奖励恢复和下游策略性能方面均优于现有离线IRL基线方法。

Conclusion: BiCQL-ML通过免策略的双层优化框架，有效解决了离线IRL问题，在理论和实验上都表现出优越性能，为离线逆强化学习提供了新的有效方法。

Abstract: Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

</details>


### [56] [FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2511.22265)
*Yuan Yao,Lixu Wang,Jiaqi Wu,Jin Song,Simin Chen,Zehua Wang,Zijian Tian,Wei Chen,Huixia Li,Xiaoxiao Li*

Main category: cs.LG

TL;DR: FedRE提出了一种基于纠缠表示的联邦学习框架，通过随机权重聚合本地表示和标签编码，在保护隐私的同时实现异构模型的高效协作训练。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习假设客户端使用同构模型架构，但实际应用中客户端在数据和计算资源上存在异质性，这使得同构假设不切实际，因此需要研究模型异构的联邦学习方法。

Method: FedRE使用纠缠表示作为客户端知识的新形式：每个客户端使用归一化随机权重将本地表示聚合成单个纠缠表示，并用相同权重将对应的one-hot标签编码聚合成纠缠标签编码，然后上传到服务器训练全局分类器。每轮训练重新采样随机权重以增加多样性。

Result: 实验表明FedRE在模型性能、隐私保护和通信开销之间实现了有效权衡。纠缠表示方法减轻了全局分类器的过度自信，促进了更平滑的决策边界，同时降低了表示反转攻击的风险。

Conclusion: FedRE通过纠缠表示框架成功解决了模型异构联邦学习中的关键挑战，在保持隐私保护的同时实现了高效的协作训练，为实际部署提供了可行的解决方案。

Abstract: Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.

</details>


### [57] [TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation](https://arxiv.org/abs/2511.22277)
*Henrijs Princis,Arindam Sharma,Cristina David*

Main category: cs.LG

TL;DR: TreeCoder是一个用于代码生成的解码框架，通过树搜索和约束函数确保代码正确性，无需依赖提示工程


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面表现出色，但仅通过自然语言提示生成的代码经常违反语法或语义约束，需要更可靠的解码方法

Method: 将解码表示为候选程序的树搜索，将解码策略和约束函数（如风格、语法、执行）作为可优化的组件，支持系统化探索和自动调优

Result: 在MBPP（Python）和SQL-Spider基准测试中，TreeCoder显著提高了CodeLlama、Mistral和DeepSeek等开源模型的准确率，通常大幅优于无约束基线

Conclusion: TreeCoder提供了一个通用灵活的框架，通过在解码过程中强制执行正确性和结构约束，显著提升LLM代码生成的质量和可靠性

Abstract: Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.

</details>


### [58] [The Hidden Cost of Approximation in Online Mirror Descent](https://arxiv.org/abs/2511.22283)
*Ofir Schlisselberg,Uri Sherman,Tomer Koren,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文系统研究了在线镜像下降(OMD)算法在近似计算误差下的性能，揭示了正则化器光滑性与误差鲁棒性之间的复杂关系，并针对不同正则化器在单纯形及其子集上的误差容忍度进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 现有的OMD分析通常假设理想的无误差设置，而实际应用中优化子问题往往只能近似求解。这限制了我们对实际性能保证的理解，因此需要系统研究近似OMD算法。

Method: 系统研究近似在线镜像下降算法，分析正则化器光滑性与近似误差鲁棒性的关系。特别关注单纯形及其子集上的不同正则化器（负熵、log-barrier、Tsallis正则化器）的误差容忍度。

Result: 1. 当正则化器一致光滑时，建立了误差导致的超额遗憾的紧界；2. 在单纯形上，负熵需要指数级小误差以避免线性遗憾，而log-barrier和Tsallis正则化器对多项式级误差保持鲁棒；3. 在随机损失和单纯形域下，负熵重新获得鲁棒性，但这一性质不适用于所有子集。

Conclusion: 正则化器的选择对近似OMD算法的误差鲁棒性至关重要。不同正则化器对近似误差的容忍度存在显著差异，这为实际应用中算法设计提供了重要指导，特别是在需要考虑计算效率与性能权衡的场景下。

Abstract: Online mirror descent (OMD) is a fundamental algorithmic paradigm that underlies many algorithms in optimization, machine learning and sequential decision-making. The OMD iterates are defined as solutions to optimization subproblems which, oftentimes, can be solved only approximately, leading to an inexact version of the algorithm. Nonetheless, existing OMD analyses typically assume an idealized error free setting, thereby limiting our understanding of performance guarantees that should be expected in practice. In this work we initiate a systematic study into inexact OMD, and uncover an intricate relation between regularizer smoothness and robustness to approximation errors. When the regularizer is uniformly smooth, we establish a tight bound on the excess regret due to errors. Then, for barrier regularizers over the simplex and its subsets, we identify a sharp separation: negative entropy requires exponentially small errors to avoid linear regret, whereas log-barrier and Tsallis regularizers remain robust even when the errors are only polynomial. Finally, we show that when the losses are stochastic and the domain is the simplex, negative entropy regains robustness-but this property does not extend to all subsets, where exponentially small errors are again necessary to avoid suboptimal regret.

</details>


### [59] [Online Dynamic Pricing of Complementary Products](https://arxiv.org/abs/2511.22291)
*Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出一种针对互补产品的动态定价算法，通过考虑产品间的需求互动关系来最大化整体收入


<details>
  <summary>Details</summary>
Motivation: 传统定价方法多为静态模型或基于规则的启发式方法，现有动态定价算法通常独立优化每个产品价格，忽略了产品间的需求互动关系，特别是互补产品的协同效应，导致无法充分利用协调定价策略的潜力

Method: 提出在线学习算法，考虑产品需求间的正负互动关系；利用交易数据通过整数规划问题识别有利的互补关系；采用基于异方差高斯过程的数据驱动、计算高效的多臂老虎机解决方案来优化定价策略

Result: 在模拟环境中验证了解决方案的有效性，证明相比忽略产品互动的可比学习算法，该算法能够提高收入

Conclusion: 通过明确考虑互补产品的联合需求结构，设计动态定价机制能够有效利用产品间的互动关系，实现更高的整体收入，为复杂产品组合的定价优化提供了新方法

Abstract: Traditional pricing paradigms, once dominated by static models and rule-based heuristics, are increasingly being replaced by dynamic, data-driven approaches powered by machine learning algorithms. Despite their growing sophistication, most dynamic pricing algorithms focus on optimizing the price of each product independently, disregarding potential interactions among items. By neglecting these interdependencies in consumer demand across related goods, sellers may fail to capture the full potential of coordinated pricing strategies. In this paper, we address this problem by exploring dynamic pricing mechanisms designed explicitly for complementary products, aiming to exploit their joint demand structure to maximize overall revenue. We present an online learning algorithm considering both positive and negative interactions between products' demands. The algorithm utilizes transaction data to identify advantageous complementary relationships through an integer programming problem between different items, and then optimizes pricing strategies using data-driven and computationally efficient multi-armed bandit solutions based on heteroscedastic Gaussian processes. We validate our solution in a simulated environment, and we demonstrate that our solution improves the revenue w.r.t. a comparable learning algorithm ignoring such interactions.

</details>


### [60] [Adaptive tumor growth forecasting via neural & universal ODEs](https://arxiv.org/abs/2511.22292)
*Kavya Subramanian,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 使用神经ODE和通用微分方程构建自适应肿瘤生长模型，改进传统模型的个性化适应性


<details>
  <summary>Details</summary>
Motivation: 传统肿瘤生长模型（如Gompertz和Bertalanffy方程）虽然能捕捉一般肿瘤动态，但难以适应患者特异性变异，特别是在数据有限的情况下。需要更灵活的模型来提高预测准确性，指导个性化治疗策略。

Method: 利用科学机器学习的两大支柱：神经ODE和通用微分方程，以Gompertz模型为基础，将刚性项替换为自适应神经网络，在Julia编程语言中构建模型。进行数据约束下的预测和符号恢复，将学习到的动态转化为显式数学表达式。

Result: 该方法能够从实验数据中学习，捕捉隐藏动态，提高预测准确性，并可将学习到的模型转化为可解释的数学表达式。

Conclusion: 基于神经ODE和UDE的自适应肿瘤生长模型有潜力改善预测准确性，指导动态有效的治疗策略，提高临床结果。

Abstract: Forecasting tumor growth is critical for optimizing treatment. Classical growth models such as the Gompertz and Bertalanffy equations capture general tumor dynamics but may fail to adapt to patient-specific variability, particularly with limited data available. In this study, we leverage Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs), two pillars of Scientific Machine Learning (SciML), to construct adaptive tumor growth models capable of learning from experimental data. Using the Gompertz model as a baseline, we replace rigid terms with adaptive neural networks to capture hidden dynamics through robust modeling in the Julia programming language. We use our models to perform forecasting under data constraints and symbolic recovery to transform the learned dynamics into explicit mathematical expressions. Our approach has the potential to improve predictive accuracy, guiding dynamic and effective treatment strategies for improved clinical outcomes.

</details>


### [61] [FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts](https://arxiv.org/abs/2511.22305)
*Dario Fenoglio,Mohan Li,Pietro Barbiero,Nicholas D. Lane,Marc Langheinrich,Martin Gjoreski*

Main category: cs.LG

TL;DR: FLUX是一个基于聚类的联邦学习框架，通过隐私保护的客户端描述符提取和无监督聚类，解决了训练和测试时的四种常见分布偏移问题，无需先验知识且支持测试时适应。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法假设客户端数据独立同分布(IID)，但在现实场景中这一假设往往不成立，导致全局模型性能显著下降，限制了联邦学习的实际应用。

Method: 提出FLUX框架：1) 使用隐私保护的客户端描述符提取技术；2) 进行无监督聚类；3) 无需分布偏移类型或聚类数量的先验知识；4) 支持测试时适应，使未见过的未标记客户端能受益于最合适的集群特定模型。

Result: 在四个标准基准、两个真实世界数据集和十个最先进基线上进行实验，FLUX在不同分布偏移下提高了性能和稳定性，相比最佳基线平均准确率提升高达23个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: FLUX是一个有效的聚类联邦学习框架，能够处理训练和测试时的多种分布偏移问题，无需先验知识且支持测试时适应，在保持效率的同时显著提升了非IID场景下的性能。

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients while preserving data privacy. Traditional FL methods often use a global model to fit all clients, assuming that clients' data are independent and identically distributed (IID). However, when this assumption does not hold, the global model accuracy may drop significantly, limiting FL applicability in real-world scenarios. To address this gap, we propose FLUX, a novel clustering-based FL (CFL) framework that addresses the four most common types of distribution shifts during both training and test time. To this end, FLUX leverages privacy-preserving client-side descriptor extraction and unsupervised clustering to ensure robust performance and scalability across varying levels and types of distribution shifts. Unlike existing CFL methods addressing non-IID client distribution shifts, FLUX i) does not require any prior knowledge of the types of distribution shifts or the number of client clusters, and ii) supports test-time adaptation, enabling unseen and unlabeled clients to benefit from the most suitable cluster-specific models. Extensive experiments across four standard benchmarks, two real-world datasets and ten state-of-the-art baselines show that FLUX improves performance and stability under diverse distribution shifts, achieving an average accuracy gain of up to 23 percentage points over the best-performing baselines, while maintaining computational and communication overhead comparable to FedAvg.

</details>


### [62] [DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks](https://arxiv.org/abs/2511.22314)
*Wenbin Wu,Kejiang Qian,Alexis Lui,Christopher Jack,Yue Wu,Peter McBurney,Fengxiang He,Bryan Zhang*

Main category: cs.LG

TL;DR: DeXposure是首个大规模去中心化金融网络跨协议信用风险暴露数据集，包含4370万条目，涵盖4300个协议、602条区块链和2.43万种代币，时间跨度2020-2025年。论文定义了基于TVL变化的协议间价值关联信用风险暴露度量，并开发了三个机器学习基准任务。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模、系统性的去中心化金融网络跨协议信用风险暴露数据集，这限制了金融风险监控、政策分析和DeFi市场建模的研究。需要构建一个全面的数据集来支持机器学习在金融领域的应用研究。

Method: 1) 利用DefiLlama元数据构建代币到协议的模型，从代币存量动态推断跨协议信用风险暴露；2) 定义基于TVL变化的协议间价值关联信用风险暴露度量；3) 开发三个机器学习基准：图聚类用于网络结构演化跟踪，向量自回归用于冲击期间部门级风险动态分析，时序图神经网络用于动态链接预测。

Result: 1) 网络规模快速增长；2) 向关键协议集中趋势明显；3) 网络密度下降（实际连接与可能连接比率降低）；4) 不同部门（借贷平台、交易交易所、资产管理协议）的冲击传播模式存在显著差异。数据集和代码已公开。

Conclusion: DeXposure数据集填补了去中心化金融网络跨协议信用风险暴露数据空白，为机器学习研究和金融风险监控提供了重要资源。该数据集支持图聚类、向量自回归和时序图分析等基准任务，有助于推动机器学习在金融领域的应用。

Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.

</details>


### [63] [SingleQuant: Efficient Quantization of Large Language Models in a Single Pass](https://arxiv.org/abs/2511.22316)
*Jinying Xiao,Bin Ji,Shasha Li,Xiaodong Liu,Ma Jun,Ye Zhong,Wei Li,Xuan Xie,Qingbo Wu,Jie Yu*

Main category: cs.LG

TL;DR: SingleQuant是一种单次量化的LLM量化框架，通过解耦量化截断来消除梯度噪声和非平滑问题，实现快速高保真量化


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法结合不兼容的梯度优化和量化截断，导致严重的收敛病理问题，延长量化时间并降低任务性能。STE在Stiefel流形上引入非平滑性和梯度噪声，阻碍优化收敛

Method: 提出SingleQuant单次量化框架，构造对齐旋转变换(ART)和均匀性旋转变换(URT)处理不同激活异常值。ART通过闭式最优旋转平滑异常值，URT通过几何映射重塑分布。两者都使用严格公式化的Givens旋转

Result: 在7B-70B LLMs上优于基线方法，量化LLaMA-2-13B时实现1400倍加速，平均任务性能提升+0.57%

Conclusion: SingleQuant通过消除非平滑性和梯度噪声因素，能够在短时间内实现高性能的LLM量化，显著提升量化效率和模型性能

Abstract: Large Language Models (LLMs) quantization facilitates deploying LLMs in resource-limited settings, but existing methods that combine incompatible gradient optimization and quantization truncation lead to serious convergence pathology. This prolongs quantization time and degrades LLMs' task performance. Our studies confirm that Straight-Through Estimator (STE) on Stiefel manifolds introduce non-smoothness and gradient noise, obstructing optimization convergence and blocking high-fidelity quantized LLM development despite extensive training. To tackle the above limitations, we propose SingleQuant, a single-pass quantization framework that decouples from quantization truncation, thereby eliminating the above non-smoothness and gradient noise factors. Specifically, SingleQuant constructs Alignment Rotation Transformation (ART) and Uniformity Rotation Transformation (URT) targeting distinct activation outliers, where ART achieves smoothing of outlier values via closed-form optimal rotations, and URT reshapes distributions through geometric mapping. Both matrices comprise strictly formulated Givens rotations with predetermined dimensions and rotation angles, enabling promising LLMs task performance within a short time. Experimental results demonstrate SingleQuant's superiority over the selected baselines across diverse tasks on 7B-70B LLMs. To be more precise, SingleQuant enables quantized LLMs to achieve higher task performance while necessitating less time for quantization. For example, when quantizing LLaMA-2-13B, SingleQuant achieves 1,400$\times$ quantization speedup and increases +0.57\% average task performance compared to the selected best baseline.

</details>


### [64] [Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement](https://arxiv.org/abs/2511.22343)
*Panteleimon Dogoulis,Mohammad Iman Alizadeh,Sylvain Kubler,Maxime Cordy*

Main category: cs.LG

TL;DR: 提出PI-TTT框架，通过测试时训练增强机器学习潮流计算的物理一致性，在推理时强制执行交流潮流方程和运行约束


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的潮流计算相比传统数值方法具有计算优势，但难以保持完全的物理一致性，需要提高精度和可行性

Method: 提出物理信息测试时训练框架，通过少量梯度更新对代理模型输出进行轻量级自监督精炼，在推理时强制执行交流潮流方程和运行约束

Result: 在IEEE 14、118、300总线系统和PEGASE 1354总线网络上，PI-TTT将潮流残差和运行约束违反降低1-2个数量级，同时保持计算优势

Conclusion: PI-TTT提供快速、准确且物理可靠的预测，为电力系统分析中可扩展且物理一致的学习提供了有前景的方向

Abstract: Power Flow (PF) calculation based on machine learning (ML) techniques offer significant computational advantages over traditional numerical methods but often struggle to maintain full physical consistency. This paper introduces a physics-informed test-time training (PI-TTT) framework that enhances the accuracy and feasibility of ML-based PF surrogates by enforcing AC power flow equalities and operational constraints directly at inference time. The proposed method performs a lightweight self-supervised refinement of the surrogate outputs through few gradient-based updates, enabling local adaptation to unseen operating conditions without requiring labeled data. Extensive experiments on the IEEE 14-, 118-, and 300-bus systems and the PEGASE 1354-bus network show that PI-TTT reduces power flow residuals and operational constraint violations by one to two orders of magnitude compared with purely ML-based models, while preserving their computational advantage. The results demonstrate that PI-TTT provides fast, accurate, and physically reliable predictions, representing a promising direction for scalable and physics-consistent learning in power system analysis.

</details>


### [65] [Cleaning the Pool: Progressive Filtering of Unlabeled Pools in Deep Active Learning](https://arxiv.org/abs/2511.22344)
*Denis Huseljic,Marek Herde,Lukas Rauch,Paul Hahn,Bernhard Sick*

Main category: cs.LG

TL;DR: REFINE是一种集成主动学习方法，通过两阶段策略（渐进过滤和覆盖选择）结合多种AL策略，无需预先知道哪种策略最优，在各种数据集和模型上表现优于单一策略和现有集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习策略（如不确定性、代表性）在不同数据集、模型和AL周期中效果差异很大，单一策略可能无法在整个AL过程中保持最优性能，需要一种能自动组合多种策略的方法。

Method: REFINE采用两阶段方法：1）渐进过滤阶段通过集成多种AL策略迭代精炼未标注池，保留捕捉不同数据价值概念的候选样本；2）覆盖选择阶段从精炼池中选择最终批次，确保覆盖所有已识别的价值概念。

Result: 在6个分类数据集和3个基础模型上的实验表明，REFINE始终优于单一策略和现有集成方法。渐进过滤作为预处理步骤能提升任何AL策略在精炼池上的性能，且该方法易于扩展集成新的AL策略。

Conclusion: REFINE提供了一种有效的集成主动学习方法，能够自适应地组合多种AL策略，无需预先知道哪种策略最优，在各种场景下都能获得稳定且优越的性能。

Abstract: Existing active learning (AL) strategies capture fundamentally different notions of data value, e.g., uncertainty or representativeness. Consequently, the effectiveness of strategies can vary substantially across datasets, models, and even AL cycles. Committing to a single strategy risks suboptimal performance, as no single strategy dominates throughout the entire AL process. We introduce REFINE, an ensemble AL method that combines multiple strategies without knowing in advance which will perform best. In each AL cycle, REFINE operates in two stages: (1) Progressive filtering iteratively refines the unlabeled pool by considering an ensemble of AL strategies, retaining promising candidates capturing different notions of value. (2) Coverage-based selection then chooses a final batch from this refined pool, ensuring all previously identified notions of value are accounted for. Extensive experiments across 6 classification datasets and 3 foundation models show that REFINE consistently outperforms individual strategies and existing ensemble methods. Notably, progressive filtering serves as a powerful preprocessing step that improves the performance of any individual AL strategy applied to the refined pool, which we demonstrate on an audio spectrogram classification use case. Finally, the ensemble of REFINE can be easily extended with upcoming state-of-the-art AL strategies.

</details>


### [66] [AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices](https://arxiv.org/abs/2511.22355)
*Mengyang Liu,Chenyu Lu,Haodong Tian,Fang Dong,Ruiting Zhou,Wei Wang,Dian Shen,Guangtong Li,Ye Wan,Li Li*

Main category: cs.LG

TL;DR: AutoTailor是一个自动化端到端框架，用于在边缘设备上实现基于SuperNet的自适应模型部署，显著减少了代码量和硬件分析成本，同时提升了准确性和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于SuperNet的自适应模型部署方法需要繁琐的模型感知开发和耗时的硬件感知分析，这限制了其在实际应用中的采用。需要一种自动化解决方案来简化这一过程。

Method: AutoTailor采用计算图引导的编译方法，自动将用户提供的ML模型转换为SuperNets。它集成了无需学习的延迟和准确性预测器，支持高效的专业化。

Result: AutoTailor将SuperNet构建的代码行数减少了11-27倍，硬件感知分析成本降低了至少11倍，相比最先进方法实现了最高15.60%的绝对准确性提升和60.03%的延迟降低。

Conclusion: AutoTailor是首个实现自动化端到端SuperNet自适应模型部署的框架，通过自动化和高效性能预测，显著降低了开发成本并提升了部署效率。

Abstract: On-device machine learning (ML) has become a fundamental component of emerging mobile applications. Adaptive model deployment delivers efficient inference for heterogeneous device capabilities and performance requirements through customizing neural architectures. SuperNet-based approaches offer a promising solution by generating a large number of model variants from a pre-trained ML model. However, applying SuperNet in existing frameworks suffers from tedious model-aware development and time-consuming hardware-aware profiling, which limits their practical adoption.
  We present AutoTailor, the first framework to enable automated, end-to-end SuperNet-based adaptive model deployment for edge devices. Unlike manual SuperNet construction, AutoTailor employs a computation graph-guided compilation approach to automatically transform user-provided ML models into SuperNets. To support efficient specialization, AutoTailor incorporates learning-free latency and accuracy predictors, enabling low-cost yet accurate performance prediction. Our extended evaluations demonstrate that AutoTailor reduces the lines of code for SuperNet construction by 11--27$\times$, decreases hardware-aware profiling costs by at least 11$\times$, and achieves up to 15.60\% absolute accuracy improvement and 60.03\% latency reduction compared to state-of-the-art approaches across diverse models and devices.

</details>


### [67] [Efficient-Husformer: Efficient Multimodal Transformer Hyperparameter Optimization for Stress and Cognitive Loads](https://arxiv.org/abs/2511.22362)
*Merey Orazaly,Fariza Temirkhanova,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 提出Efficient-Husformer，一种通过超参数优化设计的Transformer架构，用于多类别压力检测，在WESAD和CogLoad数据集上实现显著性能提升，同时保持模型紧凑（约30k参数）。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在生理信号分析中表现出色，但存在计算强度和内存需求高的问题。需要设计更高效的Transformer架构来平衡性能和计算效率。

Method: 提出Efficient-Husformer，通过结构化搜索空间进行超参数优化，针对多模态生理数据集（WESAD和CogLoad）设计紧凑的Transformer架构，评估不同架构决策的影响。

Result: 最佳配置在WESAD和CogLoad数据集上分别达到88.41%和92.61%的准确率，相比原始Husformer提升13.83%和6.98%。模型仅使用单层、3个注意力头、模型维度18/30、FFN维度120/30，约30k参数。

Conclusion: 通过超参数优化设计的紧凑Transformer架构能够在保持高性能的同时显著降低计算需求，为生理信号分析中的高效模型设计提供了有效方法。

Abstract: Transformer-based models have gained considerable attention in the field of physiological signal analysis. They leverage long-range dependencies and complex patterns in temporal signals, allowing them to achieve performance superior to traditional RNN and CNN models. However, they require high computational intensity and memory demands. In this work, we present Efficient-Husformer, a novel Transformer-based architecture developed with hyperparameter optimization (HPO) for multi-class stress detection across two multimodal physiological datasets (WESAD and CogLoad). The main contributions of this work are: (1) the design of a structured search space, targeting effective hyperparameter optimization; (2) a comprehensive ablation study evaluating the impact of architectural decisions; (3) consistent performance improvements over the original Husformer, with the best configuration achieving an accuracy of 88.41 and 92.61 (improvements of 13.83% and 6.98%) on WESAD and CogLoad datasets, respectively. The best-performing configuration is achieved with the (L + dm) or (L + FFN) modality combinations, using a single layer, 3 attention heads, a model dimension of 18/30, and FFN dimension of 120/30, resulting in a compact model with only about 30k parameters.

</details>


### [68] [SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning](https://arxiv.org/abs/2511.22367)
*Hugo Hazard,Zafeirios Fountas,Martin A. Benfeghoul,Adnan Oomerjee,Jun Wang,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 提出SuRe（基于惊喜的优先级回放）和双学习器设计，用于LLM持续学习，在大量任务场景下取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前持续学习方法（正则化和回放）在视觉领域表现良好，但在大规模语言模型的多任务学习中落后于多任务学习，特别是在大量任务场景下。需要解决回放的两个失败模式：选择（回放什么）和整合（如何巩固新知识）。

Method: 1. SuRe（基于惊喜的优先级回放）：通过负对数似然（NLL）评估序列的"惊喜"程度，优先回放最令人惊讶的序列；2. 双学习器设计：包含快速和慢速LoRA适配器，通过指数移动平均（EMA）合并，实现快速适应同时稳定长期知识。

Result: 在大量任务（LNT）设置中达到SOTA性能，在标准CL和LNT基准测试中平均表现最佳。SuRe与双学习器结合带来额外提升，在LNT上比先前SOTA提高最多+5个准确率点。消融研究证实该方法在减少回放频率和小缓冲区下仍保持鲁棒。

Conclusion: 回放可以作为LLM持续微调的强基线方法，基于惊喜的选择和慢权重整合是缓解灾难性遗忘的互补组件，共同提升了LLM在持续学习场景下的性能。

Abstract: Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.

</details>


### [69] [Predicting and Interpolating Spatiotemporal Environmental Data: A Case Study of Groundwater Storage in Bangladesh](https://arxiv.org/abs/2511.22378)
*Anna Pazola,Mohammad Shamsudduha,Richard G. Taylor,Allan Tucker*

Main category: cs.LG

TL;DR: 比较两种深度学习策略（网格到网格 vs 网格到点）用于时空预测和插值，发现空间插值比时间预测更困难，地质不确定性对点时间行为有重要影响。


<details>
  <summary>Details</summary>
Motivation: 地理空间观测数据通常仅限于点测量，因此需要时间预测和空间插值来构建连续场。本研究旨在评估两种深度学习策略来解决这一挑战。

Method: 使用孟加拉国地下水储量数据作为案例研究，比较两种方法：1）网格到网格方法（建模前聚合），使用网格化预测因子建模栅格化目标；2）网格到点方法（建模后聚合），使用网格化预测因子建模点目标，然后通过克里金插值填充域。

Result: 空间插值比时间预测困难得多。最近邻点不一定最相似，地质不确定性强烈影响点的时间行为。这些发现为未来基于时间序列动态聚类位置的高级插值方法提供了动机。

Conclusion: 虽然以地下水储量为例，但结论适用于其他由间接可观测因素控制的环境变量。未来工作应开发基于时间序列动态聚类位置的高级插值方法。

Abstract: Geospatial observational datasets are often limited to point measurements, making temporal prediction and spatial interpolation essential for constructing continuous fields. This study evaluates two deep learning strategies for addressing this challenge: (1) a grid-to-grid approach, where gridded predictors are used to model rasterised targets (aggregation before modelling), and (2) a grid-to-point approach, where gridded predictors model point targets, followed by kriging interpolation to fill the domain (aggregation after modelling). Using groundwater storage data from Bangladesh as a case study, we compare the effcacy of these approaches. Our findings indicate that spatial interpolation is substantially more difficult than temporal prediction. In particular, nearest neighbours are not always the most similar, and uncertainties in geology strongly influence point temporal behaviour. These insights motivate future work on advanced interpolation methods informed by clustering locations based on time series dynamics. Demonstrated on groundwater storage, the conclusions are applicable to other environmental variables governed by indirectly observable factors. Code is available at https://github.com/pazolka/interpolation-prediction-gwsa.

</details>


### [70] [TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting](https://arxiv.org/abs/2511.22395)
*Ganeshan Niroshan,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: TS2Vec-Ensemble：一个结合自监督学习表示与显式时间特征的混合框架，通过双模型集成架构提升时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 现有的自监督对比学习方法（如TS2Vec）在预测任务中表现不佳，因为它们的目标函数侧重于实例区分而非捕捉对预测至关重要的确定性模式（如季节性和趋势）。需要一种能结合学习表示与显式时间先验的方法来提升长期预测性能。

Method: 提出TS2Vec-Ensemble混合框架：1）使用预训练的TS2Vec编码器获取隐式学习动态；2）融合显式工程化时间特征编码周期性模式；3）采用双模型集成架构，两个回归头分别关注学习动态和季节模式；4）通过自适应加权方案结合两者，权重针对每个预测范围独立优化。

Result: 在ETT基准数据集上进行广泛实验（单变量和多变量预测），TS2Vec-Ensemble始终显著优于标准TS2Vec基线和其他最先进模型，验证了混合策略对长期时间序列预测的优越性。

Conclusion: 结合学习表示与显式时间先验的混合策略是提升长期时间序列预测性能的有效方法。TS2Vec-Ensemble通过动态调整短期动态与长期季节性的优先级，为时间序列预测提供了新的解决方案。

Abstract: Self-supervised representation learning, particularly through contrastive methods like TS2Vec, has advanced the analysis of time series data. However, these models often falter in forecasting tasks because their objective functions prioritize instance discrimination over capturing the deterministic patterns, such as seasonality and trend, that are critical for accurate prediction. This paper introduces TS2Vec-Ensemble, a novel hybrid framework designed to bridge this gap. Our approach enhances the powerful, implicitly learned dynamics from a pretrained TS2Vec encoder by fusing them with explicit, engineered time features that encode periodic cycles. This fusion is achieved through a dual-model ensemble architecture, where two distinct regression heads -- one focused on learned dynamics and the other on seasonal patterns -- are combined using an adaptive weighting scheme. The ensemble weights are optimized independently for each forecast horizon, allowing the model to dynamically prioritize short-term dynamics or long-term seasonality as needed. We conduct extensive experiments on the ETT benchmark datasets for both univariate and multivariate forecasting. The results demonstrate that TS2Vec-Ensemble consistently and significantly outperforms the standard TS2Vec baseline and other state-of-the-art models, validating our hypothesis that a hybrid of learned representations and explicit temporal priors is a superior strategy for long-horizon time series forecasting.

</details>


### [71] [Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions](https://arxiv.org/abs/2511.22406)
*Roland Stolz,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出高效数值近似方法解决动作约束RL中截断正态分布熵、对数概率等关键特征计算难题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有动作约束RL方法在处理截断正态分布时，关键特征（熵、对数概率及其梯度）计算困难，现有近似方法严重降低性能

Method: 提出高效数值近似方法计算截断正态分布的关键特征，并提供截断策略分布的高效采样策略

Result: 在三个基准环境中验证，使用准确估计的方法相比近似方法获得显著性能提升

Conclusion: 动作约束RL中准确估计截断分布的关键特征至关重要，提出的高效数值近似方法能有效解决计算难题并提升性能

Abstract: In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.

</details>


### [72] [PISA: Prioritized Invariant Subgraph Aggregation](https://arxiv.org/abs/2511.22435)
*Ali Ghasemi,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: PISA提出动态MLP聚合机制，有效整合多个不变子图表示，在15个数据集上比现有方法提升最多5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图数据OOD泛化方法存在局限：CIGA仅提取单一不变子图可能遗漏多个因果模式，SuGAr虽然学习多个不变子图但聚合方式简单（均匀或贪婪），限制了性能提升。

Method: 提出PISA框架，采用动态MLP聚合机制，能够优先考虑并更有效地组合多个不变子图的表示，提升模型对复杂分布变化的鲁棒性。

Result: 在包括DrugOOD在内的15个数据集上进行实验，PISA相比现有方法（CIGA、SuGAr等）实现了最高5%的分类准确率提升。

Conclusion: 动态聚合机制对于有效整合多个不变子图表示至关重要，PISA框架在复杂图数据OOD泛化任务中展现出优越性能。

Abstract: Recent work has extended the invariance principle for out-of-distribution (OOD) generalization from Euclidean to graph data, where challenges arise due to complex structures and diverse distribution shifts in node attributes and topology. To handle these, Chen et al. proposed CIGA (Chen et al., 2022b), which uses causal modeling and an information-theoretic objective to extract a single invariant subgraph capturing causal features. However, this single-subgraph focus can miss multiple causal patterns. Liu et al. (2025) addressed this with SuGAr, which learns and aggregates diverse invariant subgraphs via a sampler and diversity regularizer, improving robustness but still relying on simple uniform or greedy aggregation. To overcome this, the proposed PISA framework introduces a dynamic MLP-based aggregation that prioritizes and combines subgraph representations more effectively. Experiments on 15 datasets, including DrugOOD (Ji et al., 2023), show that PISA achieves up to 5% higher classification accuracy than prior methods.

</details>


### [73] [An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction](https://arxiv.org/abs/2511.22460)
*Yifan Lei,Jiahua Luo,Tingyu Jiang,Bo Zhang,Lifeng Wang,Dapeng Liu,Zhaoren Wu,Haijie Gu,Huan Yu,Jie Jiang*

Main category: cs.LG

TL;DR: 提出基于GPU加速的双塔网络特征交互方法，显著提升广告检索精度并降低计算成本，首次在工业界检索系统中成功实现Wide & Deep模型。


<details>
  <summary>Details</summary>
Motivation: 现有双塔模型在广告检索中存在特征交互不足的问题，用户和广告的嵌入仅在最后内积计算时交互，而允许早期特征交互的DNN模型在检索阶段计算成本过高不可行。

Method: 提出基于GPU加速的特征交互双塔网络，引入新颖的压缩倒排列表设计，支持大规模高效特征交互计算，首次在工业界检索系统中实现Wide & Deep架构。

Result: 在腾讯广告真实业务场景中，该方法在离线评估中优于现有方法，并已成功部署到腾讯广告推荐系统，带来显著的在线性能提升。

Conclusion: 该方法不仅验证了所提方法的有效性，还为优化大规模广告检索系统提供了新的实践指导，解决了双塔模型特征交互不足与计算效率之间的平衡问题。

Abstract: In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.

</details>


### [74] [Adversarial Flow Models](https://arxiv.org/abs/2511.22475)
*Shanchuan Lin,Ceyuan Yang,Zhijie Lin,Hao Chen,Haoqi Fan*

Main category: cs.LG

TL;DR: 提出对抗流模型，统一对抗模型和流模型，支持单步或多步生成，通过对抗目标训练，比传统GAN更稳定，比一致性方法更高效


<details>
  <summary>Details</summary>
Motivation: 传统GAN学习任意传输计划，训练不稳定；一致性方法需要学习中间时间步，模型容量大、训练迭代多、误差累积。需要一种既能稳定训练又能高效生成的方法

Method: 对抗流模型：结合对抗模型和流模型，生成器学习确定性噪声到数据的映射（与流匹配模型相同的优化传输），支持原生单步或多步生成，使用对抗目标训练

Result: 在ImageNet-256px上，B/2模型接近一致性XL/2模型性能；XL/2模型创下最佳FID 2.38；通过深度重复端到端训练56层和112层模型，单步前向传播分别达到FID 2.08和1.94，超越2NFE和4NFE对应模型

Conclusion: 对抗流模型成功统一对抗模型和流模型，提供稳定训练和高效生成，在图像生成任务上取得优异性能，为生成模型提供新方向

Abstract: We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.

</details>


### [75] [Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges](https://arxiv.org/abs/2511.22483)
*Guanxi Lu,Hao Mark Chen,Zhiqiang Que,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: 量化压缩会影响LLM的可信度指标，作者提出精度集成投票方法提升可信度性能


<details>
  <summary>Details</summary>
Motivation: 现有量化框架主要关注困惑度或分类准确率，忽略了可信度指标，这在金融、医疗等高风险领域应用时存在风险

Method: 系统研究量化对四种可信度指标的影响，并提出精度集成投票方法，利用同一模型的不同精度变体的预测进行集成

Result: 量化在不同压缩比和量化方法下对可信度指标表现不稳定，精度集成投票方法可将可信度指标性能提升高达5.8%

Conclusion: 开发模型压缩技术时需要考虑可信度因素，压缩与可信度的交叉研究对安全关键应用具有重要意义

Abstract: Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.

</details>


### [76] [Space Explanations of Neural Network Classification](https://arxiv.org/abs/2511.22498)
*Faezeh Labbaf,Tomáš Kolárik,Martin Blicha,Grigory Fedyukovich,Michael Wand,Natasha Sharygina*

Main category: cs.LG

TL;DR: 提出一种基于逻辑的"空间解释"概念，为神经网络在输入特征空间的连续区域提供可证明的行为保证，通过Craig插值和不可满足核生成自动创建解释


<details>
  <summary>Details</summary>
Motivation: 现有神经网络解释方法缺乏对网络行为在连续输入区域的可证明保证，需要一种能提供形式化验证保证的解释方法

Method: 使用Craig插值算法和不可满足核生成技术自动生成空间解释，为神经网络在连续输入区域的行为提供逻辑证明

Result: 在从小型到大型的真实案例研究中，生成的空间解释比现有最先进方法计算出的解释更有意义

Conclusion: 空间解释为神经网络分类提供了具有可证明保证的解释框架，在真实应用中表现出优越性

Abstract: We present a novel logic-based concept called Space Explanations for classifying neural networks that gives provable guarantees of the behavior of the network in continuous areas of the input feature space. To automatically generate space explanations, we leverage a range of flexible Craig interpolation algorithms and unsatisfiable core generation. Based on real-life case studies, ranging from small to medium to large size, we demonstrate that the generated explanations are more meaningful than those computed by state-of-the-art.

</details>


### [77] [Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems](https://arxiv.org/abs/2511.22515)
*Shiva Parsarad,Isabel Wagner*

Main category: cs.LG

TL;DR: 该研究对推荐系统中两种差分隐私机制(DPSGD和LDP)在四种推荐模型上的效果进行了全面评估，发现隐私保护会降低推荐效用，但不同模型受影响程度不同，且两种隐私机制在准确性和公平性方面存在不同权衡。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统越来越多地采用差分隐私来保护用户数据，需要了解隐私机制如何影响推荐准确性和公平性。目前缺乏对不同隐私机制和推荐模型组合效果的全面评估。

Method: 使用两种差分隐私机制(DPSGD和LDP)应用于四种推荐系统(NCF、BPR、SVD、VAE)，在MovieLens-1M和Yelp数据集上进行跨模型评估，分析隐私保护对推荐准确性和公平性的影响。

Result: 更强的隐私保护会降低推荐效用，但影响不均匀：NCF在DPSGD下准确率损失最小(ε≈1时低于10%)，SVD和BPR损失较大，VAE对隐私最敏感。DPSGD通常减少热门与冷门物品的推荐差距，而LDP更接近保留原有模式。

Conclusion: 没有单一的差分隐私机制在所有情况下都最优，每种机制在不同隐私级别和数据条件下提供不同的权衡。选择隐私机制需要考虑具体推荐模型、数据特性和隐私要求。

Abstract: Recommender systems (RSs) output ranked lists of items, such as movies or restaurants, that users may find interesting, based on the user's past ratings and ratings from other users. RSs increasingly incorporate differential privacy (DP) to protect user data, raising questions about how privacy mechanisms affect both recommendation accuracy and fairness. We conduct a comprehensive, cross-model evaluation of two DP mechanisms, differentially private stochastic gradient descent (DPSGD) and local differential privacy (LDP), applied to four recommender systems (Neural Collaborative Filtering (NCF), Bayesian Personalized Ranking (BPR), Singular Value Decomposition (SVD), and Variational Autoencoder (VAE)) on the MovieLens-1M and Yelp datasets. We find that stronger privacy consistently reduces utility, but not uniformly. NCF under DPSGD shows the smallest accuracy loss (under 10 percent at epsilon approximately 1), whereas SVD and BPR experience larger drops, especially for users with niche preferences. VAE is the most sensitive to privacy, with sharp declines for sparsely represented groups. The impact on bias metrics is similarly heterogeneous. DPSGD generally reduces the gap between recommendations of popular and less popular items, whereas LDP preserves existing patterns more closely. These results highlight that no single DP mechanism is uniformly superior; instead, each provides trade-offs under different privacy regimes and data conditions.

</details>


### [78] [List-Decodable Regression via Expander Sketching](https://arxiv.org/abs/2511.22524)
*Herbod Pourali,Sajjad Hashemian,Ebrahim Ardeshir-Larijani*

Main category: cs.LG

TL;DR: 提出基于扩展图的草图框架，用于列表可解码线性回归，实现近乎最优的样本复杂度、列表大小和计算效率


<details>
  <summary>Details</summary>
Motivation: 解决列表可解码线性回归中样本复杂度、列表大小和计算效率之间的权衡问题，避免使用复杂的SoS方法或显式批次结构

Method: 使用无损扩展图合成轻度污染的批次，通过鲁棒聚合和短谱滤波阶段实现高效算法

Result: 达到样本复杂度$\tilde{O}((d+\log(1/δ))/α)$，列表大小$O(1/α)$，近输入稀疏运行时间$\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$

Conclusion: 扩展图草图框架为列表可解码线性回归提供了高效且理论保证强的解决方案，避免了复杂数学工具的需求

Abstract: We introduce an expander-sketching framework for list-decodable linear regression that achieves sample complexity $\tilde{O}((d+\log(1/δ))/α)$, list size $O(1/α)$, and near input-sparsity running time $\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$ under standard sub-Gaussian assumptions. Our method uses lossless expanders to synthesize lightly contaminated batches, enabling robust aggregation and a short spectral filtering stage that matches the best known efficient guarantees while avoiding SoS machinery and explicit batch structure.

</details>


### [79] [Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs](https://arxiv.org/abs/2511.22567)
*Feyza Eksen,Stefan Oehmcke,Stefan Lüdtke*

Main category: cs.LG

TL;DR: 提出基于认知不确定性的传感器布局新方法，使用ConvCNPs与MDNs估计认知不确定性，相比总不确定性方法能更有效降低模型误差


<details>
  <summary>Details</summary>
Motivation: 现有传感器布局方法依赖总预测不确定性（包含认知和偶然不确定性），在模糊区域可能导致次优传感器选择，需要区分认知不确定性来改进布局效果

Method: 扩展ConvCNPs，加入混合密度网络（MDNs）输出头来估计认知不确定性，提出基于认知不确定性减少期望的新获取函数进行传感器布局

Result: 初步结果表明，基于认知不确定性的传感器布局比基于总体不确定性的方法能更有效地减少模型误差

Conclusion: 区分认知不确定性对于传感器布局至关重要，提出的方法为数据驱动的传感器优化提供了更有效的解决方案

Abstract: Accurate sensor placement is critical for modeling spatio-temporal systems such as environmental and climate processes. Neural Processes (NPs), particularly Convolutional Conditional Neural Processes (ConvCNPs), provide scalable probabilistic models with uncertainty estimates, making them well-suited for data-driven sensor placement. However, existing approaches rely on total predictive uncertainty, which conflates epistemic and aleatoric components, that may lead to suboptimal sensor selection in ambiguous regions. To address this, we propose expected reduction in epistemic uncertainty as a new acquisition function for sensor placement. To enable this, we extend ConvCNPs with a Mixture Density Networks (MDNs) output head for epistemic uncertainty estimation. Preliminary results suggest that epistemic uncertainty driven sensor placement more effectively reduces model error than approaches based on overall uncertainty.

</details>


### [80] [Entropy is all you need for Inter-Seed Cross-Play in Hanabi](https://arxiv.org/abs/2511.22581)
*Johannes Forkel,Jakob Foerster*

Main category: cs.LG

TL;DR: 在Hanabi游戏中，标准独立PPO算法通过提高熵系数到0.05（而非通常的0.01），在跨种子交叉游戏中达到新的最优性能，超越了所有专门设计的算法。


<details>
  <summary>Details</summary>
Motivation: 研究零样本协调和临时团队协作中最复杂、最流行的基准之一Hanabi游戏，探索标准算法在跨种子交叉游戏中的潜力，挑战专门设计的算法。

Method: 使用独立PPO算法，将熵正则化系数从0.01提高到0.05，同时采用高GAE系数λ=0.9，并在actor-critic架构中使用RNN而非前馈网络。

Result: 该方法在Hanabi的跨种子交叉游戏中实现了新的最优性能，显著超越了所有先前专门设计的算法。高熵正则化确保不同随机种子产生的联合策略相互兼容。

Conclusion: 虽然超参数对自玩和交叉游戏分数都有显著影响，但在某些简单Dec-POMDP中，增加熵正则化的标准策略梯度方法仍无法实现完美的跨种子交叉游戏，表明仍需开发新的零样本协调算法。

Abstract: We find that in Hanabi, one of the most complex and popular benchmarks for zero-shot coordination and ad-hoc teamplay, a standard implementation of independent PPO with a slightly higher entropy coefficient 0.05 instead of the typically used 0.01, achieves a new state-of-the-art in cross-play between different seeds, beating by a significant margin all previous specialized algorithms, which were specifically designed for this setting. We provide an intuition for why sufficiently high entropy regularization ensures that different random seed produce joint policies which are mutually compatible. We also empirically find that a high $λ_{\text{GAE}}$ around 0.9, and using RNNs instead of just feed-forward layers in the actor-critic architecture, strongly increase inter-seed cross-play. While these results demonstrate the dramatic effect that hyperparameters can have not just on self-play scores but also on cross-play scores, we show that there are simple Dec-POMDPs though, in which standard policy gradient methods with increased entropy regularization are not able to achieve perfect inter-seed cross-play, thus demonstrating the continuing necessity for new algorithms for zero-shot coordination.

</details>


### [81] [The Multiclass Score-Oriented Loss (MultiSOL) on the Simplex](https://arxiv.org/abs/2511.22587)
*Francesco Marchetti,Edoardo Legnaro,Sabrina Guastavino*

Main category: cs.LG

TL;DR: 将二元分类中的score-oriented损失函数扩展到多分类，提出MultiSOL函数，直接优化目标指标，避免后验阈值调整，对类别不平衡具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在监督二元分类中，score-oriented损失函数可以直接在训练阶段优化特定性能指标，避免后验阈值调整。但现有方法主要针对二元分类，需要将其扩展到多分类场景。

Method: 使用最近提出的多维阈值分类框架，将score-oriented损失函数扩展到多分类，定义MultiSOL函数。在构建过程中将决策阈值视为具有先验分布的随机变量。

Result: MultiSOL保持了二元设置中的主要优势：直接优化目标指标、对类别不平衡具有鲁棒性。在多个分类实验中表现与最先进损失函数相当，并提供了单纯形几何与score-oriented学习交互的新见解。

Conclusion: 成功将score-oriented损失函数扩展到多分类，提出的MultiSOL函数在保持二元优势的同时，为多分类问题提供了有效的直接优化方法，揭示了单纯形几何在score-oriented学习中的重要作用。

Abstract: In the supervised binary classification setting, score-oriented losses have been introduced with the aim of optimizing a chosen performance metric directly during the training phase, thus avoiding \textit{a posteriori} threshold tuning. To do this, in their construction, the decision threshold is treated as a random variable provided with a certain \textit{a priori} distribution. In this paper, we use a recently introduced multidimensional threshold-based classification framework to extend such score-oriented losses to multiclass classification, defining the Multiclass Score-Oriented Loss (MultiSOL) functions. As also demonstrated by several classification experiments, this proposed family of losses is designed to preserve the main advantages observed in the binary setting, such as the direct optimization of the target metric and the robustness to class imbalance, achieving performance comparable to other state-of-the-art loss functions and providing new insights into the interaction between simplex geometry and score-oriented learning.

</details>


### [82] [LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://arxiv.org/abs/2511.22598)
*Huanyu Li,Zongyuan Li,Wei Huang,Xian Guo*

Main category: cs.LG

TL;DR: LLM-Cave：一个轻量级基准测试环境，用于评估大语言模型的序列推理和决策能力，相比现有复杂环境更高效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要限于单步交互，而现有的序列决策环境（如TextStarCraftII和LLM-PySC2）过于复杂，需要数小时才能完成一局游戏。需要开发一个轻量级但能有效评估LLM序列推理和决策能力的基准环境。

Method: 引入LLM-Cave基准测试环境，这是一个符号主义时代的经典实例。人工智能代理能够探索环境，利用部分可观测状态信息推理附近危险来避免潜在损失。该环境设计为轻量级，便于高效评估。

Result: 实验评估了GPT-4o-mini、o1-mini和DeepSeek-R1等主流大语言模型的序列推理能力、决策性能和计算效率。DeepSeek-R1在复杂推理任务上取得了最高成功率，而4o-mini等较小模型通过采用思维链推测和规划者-批评者策略，在挑战任务上显著缩小了性能差距，但牺牲了计算效率。

Conclusion: 结构化、多步推理结合基于LLM的反馈机制可以显著增强LLM的决策能力，为改进较弱模型的推理能力提供了有前景的方向，并提出了一个以推理为中心的LLM评估新基准。

Abstract: Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.

</details>


### [83] [Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning](https://arxiv.org/abs/2511.22640)
*Riccardo De Santi,Marin Vlastelica,Ya-Ping Hsieh,Zebang Shen,Niao He,Andreas Krause*

Main category: cs.LG

TL;DR: 提出Flow Density Control (FDC)算法，用于优化预训练生成模型在更广泛目标函数下的微调，超越传统平均奖励最大化，支持风险规避、多样性等目标，并采用更通用的先验信息保留方式。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法主要关注最大化生成样本的期望奖励，并通过KL散度正则化保留预训练模型知识。但在实际应用中，需要优化更通用的目标函数（如风险规避、多样性等），并考虑更通用的先验信息保留方式。

Method: 提出Flow Density Control (FDC)算法，将复杂优化问题分解为一系列更简单的微调任务序列，每个子任务可通过现有可扩展方法求解。算法基于镜像流的最新理论理解，推导收敛保证。

Result: 在文本到图像生成和分子设计任务上验证了FDC的有效性，表明该方法能够引导预训练生成模型优化目标函数，解决当前微调方案无法处理的实用相关任务。

Conclusion: FDC为优化预训练生成模型提供了通用框架，支持更广泛的目标函数和先验信息保留方式，扩展了生成模型在实际应用中的能力。

Abstract: Adapting large-scale foundation flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Renyi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.

</details>


### [84] [Spatially Aware Dictionary-Free Eigenfunction Identification for Modeling and Control of Nonlinear Dynamical Systems](https://arxiv.org/abs/2511.22648)
*David Grasev*

Main category: cs.LG

TL;DR: 提出一种无需预定义基函数的数据驱动Koopman特征函数发现新方法，通过参考轨迹识别Koopman模态振幅，转换到包含特征值和时间基本函数的新基，使用正则化最小二乘拟合和全局优化器优化特征值，成功应用于多个非线性动力系统。


<details>
  <summary>Details</summary>
Motivation: 传统Koopman算子方法需要预定义基函数，限制了其应用范围。本文旨在开发一种无需预定义基函数的数据驱动方法，直接从数据中发现Koopman特征函数，提高方法的通用性和准确性。

Method: 基于参考轨迹识别Koopman模态振幅，将Koopman模态分解转换到包含特征值和时间基本函数的新基。通过正则化最小二乘拟合将轨迹投影到该基上获得特征函数初值，使用全局优化器优化特征值。通过映射初始状态值到特征函数值揭示其空间结构，计算数值梯度，惩罚偏离Koopman偏微分方程的解。

Result: 方法成功应用于多个基准非线性动力系统：带输入的FitzHugh-Nagumo系统、van der Pol和Duffing振荡器、带控制的2轴涡轮喷气发动机。结果表明，结合主特征值和空间结构完整性促进显著提高了Koopman预测器的准确性。即使在稀疏状态空间采样下也能有效发现Koopman谱分量，揭示状态空间的几何特征如不变分区。

Conclusion: 该方法为各种动力系统提供了一种实用的Koopman特征函数发现方法，无需预定义基函数。特征函数梯度的数值近似可用于输入动力学建模和控制设计，支持该方法在实际应用中的实用性。

Abstract: A new approach to data-driven discovery of Koopman eigenfunctions without a pre-defined set of basis functions is proposed. The approach is based on a reference trajectory, for which the Koopman mode amplitudes are first identified, and the Koopman mode decomposition is transformed to a new basis, which contains fundamental functions of eigenvalues and time. The initial values of the eigenfunctions are obtained by projecting trajectories onto this basis via a regularized least-squares fit. A global optimizer was employed to optimize the eigenvalues. Mapping initial-state values to eigenfunction values reveals their spatial structure, enabling the numerical computation of their gradients. Thus, deviations from the Koopman partial differential equation are penalized, leading to more robust solutions. The approach was successfully tested on several benchmark nonlinear dynamical systems, including the FitzHugh-Nagumo system with inputs, van der Pol and Duffing oscillators, and a 2-spool turbojet engine with control. The study demonstrates that incorporating principal eigenvalues and spatial structure integrity promotion significantly improves the accuracy of Koopman predictors. The approach effectively discovers Koopman spectral components even with sparse state-space sampling and reveals geometric features of the state space, such as invariant partitions. Finally, the numerical approximation of the eigenfunction gradient can be used for input dynamics modeling and control design. The results support the practicality of the approach for use with various dynamical systems.

</details>


### [85] [Automated Design Optimization via Strategic Search with Large Language Models](https://arxiv.org/abs/2511.22651)
*Anthony Carreon,Vansh Sharma,Venkat Raman*

Main category: cs.LG

TL;DR: AUTO是一个基于大语言模型的智能体框架，将设计优化视为无梯度搜索问题，通过策略推理实现GPU代码优化，性能接近专家实现，搜索效率达贝叶斯优化的50-70%，成本显著低于人工开发。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在定义良好的搜索空间中表现优异，但在设计问题中，当变换和设计参数难以定义时表现不佳。大语言模型提供了有前景的替代方案，能够动态解释设计空间并利用编码的领域知识。

Method: 提出AUTO框架，将设计优化视为由LLM策略推理引导的无梯度搜索问题。框架采用两个协作智能体：策略师（选择探索与利用策略）和执行者（执行详细设计）。应用于GPU代码优化领域。

Result: 在化学动力学积分和稠密矩阵乘法等关键领域，AUTO生成的解决方案与专家实现相当。框架达到贝叶斯优化方法50-70%的搜索效率，优化约需8小时，估计成本最高159美元，而中等薪资软件开发者估计成本最高480美元。

Conclusion: 这些发现为在定义不明确、先验信息有限的搜索空间中自动化设计优化打开了大门。

Abstract: Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \$159 per run, compared to an estimated cost of up to \$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.

</details>


### [86] [Structure-aware Hybrid-order Similarity Learning for Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.22656)
*Lin Xu,Ke Li,Dongjie Wang,Fengmao Lv,Tianrui Li,Yanyong Huang*

Main category: cs.LG

TL;DR: 提出SHINE-FS方法，通过联合学习一阶和二阶相似性图构建混合阶相似性图，解决多视图无监督特征选择中局部和全局结构捕获不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法主要使用一阶相似性图来保留局部结构，但往往忽略了二阶相似性可以捕获的全局结构。少数使用预定义二阶相似性图的方法容易受到噪声和异常值的影响，导致特征选择性能不理想。

Method: SHINE-FS首先学习共识锚点和相应的锚点图来捕获锚点与样本之间的跨视图关系。基于获得的跨视图共识信息，生成样本的低维表示，通过识别判别性特征来重建多视图数据。然后利用锚点-样本关系学习二阶相似性图。最后，通过联合学习一阶和二阶相似性图，构建混合阶相似性图来同时捕获局部和全局结构。

Result: 在真实多视图数据集上的综合实验结果表明，SHINE-FS优于现有的最先进方法。

Conclusion: SHINE-FS通过构建混合阶相似性图有效捕获数据的局部和全局结构，提高了多视图无监督特征选择的性能，能够更好地揭示数据的内在结构。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently emerged as an effective dimensionality reduction method for unlabeled multi-view data. However, most existing methods mainly use first-order similarity graphs to preserve local structure, often overlooking the global structure that can be captured by second-order similarity. In addition, a few MUFS methods leverage predefined second-order similarity graphs, making them vulnerable to noise and outliers and resulting in suboptimal feature selection performance. In this paper, we propose a novel MUFS method, termed Structure-aware Hybrid-order sImilarity learNing for multi-viEw unsupervised Feature Selection (SHINE-FS), to address the aforementioned problem. SHINE-FS first learns consensus anchors and the corresponding anchor graph to capture the cross-view relationships between the anchors and the samples. Based on the acquired cross-view consensus information, it generates low-dimensional representations of the samples, which facilitate the reconstruction of multi-view data by identifying discriminative features. Subsequently, it employs the anchor-sample relationships to learn a second-order similarity graph. Furthermore, by jointly learning first-order and second-order similarity graphs, SHINE-FS constructs a hybrid-order similarity graph that captures both local and global structures, thereby revealing the intrinsic data structure to enhance feature selection. Comprehensive experimental results on real multi-view datasets show that SHINE-FS outperforms the state-of-the-art methods.

</details>


### [87] [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/abs/2511.22662)
*Lewis Smith,Bilal Chughtai,Neel Nanda*

Main category: cs.LG

TL;DR: 论文指出当前缺乏可靠的AI欺骗检测评估数据，并分析了收集此类数据的障碍


<details>
  <summary>Details</summary>
Motivation: 构建可靠的AI欺骗检测器对于缓解高级AI系统风险至关重要，但当前缺乏可用于评估的明确标记的欺骗/诚实示例

Method: 通过概念论证、现有实证研究分析以及新颖的案例研究分析来识别收集欺骗检测数据的障碍

Result: 识别出多个具体的障碍，表明当前缺乏必要的评估数据，并发现现有实证解决方案虽然有价值但可能不足

Conclusion: 欺骗检测的进展需要进一步考虑数据收集问题，当前障碍限制了可靠检测器的开发与评估

Abstract: Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.

</details>


### [88] [Modèles de Fondation et Ajustement : Vers une Nouvelle Génération de Modèles pour la Prévision des Séries Temporelles](https://arxiv.org/abs/2511.22674)
*Morad Laglil,Emilie Devijver,Eric Gaussier,Bertrand Pracca*

Main category: cs.LG

TL;DR: 该论文综述了用于零样本时间序列预测的基础模型架构、预训练策略和优化方法，并研究了微调对特定数据集性能的影响，发现微调能提升零样本预测能力，尤其对长期预测效果更佳。


<details>
  <summary>Details</summary>
Motivation: 受大型语言模型启发，研究者开发了用于零样本时间序列预测的基础模型，这些模型在大量时间序列数据上预训练，学习可泛化的表示，减少了对任务特定架构和手动调参的需求。

Method: 论文回顾了主要的基础模型架构、预训练策略和优化方法，并研究了预训练后微调对特定数据集性能的影响，通过实证分析评估微调效果。

Result: 实证结果表明，微调通常能提升零样本预测能力，特别是对长期预测范围（long-term horizons）效果更为显著。

Conclusion: 基础模型通过大规模预训练学习可泛化表示，而微调能进一步优化其在特定数据集上的性能，特别是在长期预测任务中，这为时间序列预测提供了一种有效的零样本学习框架。

Abstract: Inspired by recent advances in large language models, foundation models have been developed for zero-shot time series forecasting, enabling prediction on datasets unseen during pretraining. These large-scale models, trained on vast collections of time series, learn generalizable representations for both point and probabilistic forecasting, reducing the need for task-specific architectures and manual tuning.
  In this work, we review the main architectures, pretraining strategies, and optimization methods used in such models, and study the effect of fine-tuning after pretraining to enhance their performance on specific datasets. Our empirical results show that fine-tuning generally improves zero-shot forecasting capabilities, especially for long-term horizons.

</details>


### [89] [Test-time scaling of diffusions with flow maps](https://arxiv.org/abs/2511.22688)
*Amirmojtaba Sabour,Michael S. Albergo,Carles Domingo-Enrich,Nicholas M. Boffi,Sanja Fidler,Karsten Kreis,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出FMTT算法，通过直接操作流映射而非梯度奖励来改进扩散模型在测试时的采样，实现更好的奖励优化和图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时使用奖励梯度改进扩散模型存在根本问题，因为用户指定的奖励通常只在生成结束时定义在数据分布上。现有解决方案使用去噪器估计样本在生成结束时的状态，但这种方法不够理想。

Method: 提出Flow Map Trajectory Tilting (FMTT)算法，通过利用流映射与控制瞬时传输的速度场之间的关系，直接操作流映射而非奖励梯度。该方法可用于通过重要性加权进行精确采样，或通过原则性搜索识别奖励倾斜分布的局部最大化器。

Result: FMTT在理论上证明比标准测试时方法（涉及奖励梯度）能实现更好的奖励上升。实验表明该方法优于其他前瞻技术，并且流映射能够处理复杂的奖励函数，实现新的图像编辑形式（如与视觉语言模型交互）。

Conclusion: 通过直接操作流映射而非依赖奖励梯度，FMTT提供了一种更稳健的方法来改进扩散模型在测试时的采样，能够处理复杂的奖励函数并实现新的图像编辑能力。

Abstract: A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.

</details>


### [90] [Generative Anchored Fields: Controlled Data Generation via Emergent Velocity Fields and Transport Algebra](https://arxiv.org/abs/2511.22693)
*Deressa Wodajo Deressa,Hannes Mareen,Peter Lambert,Glenn Van Wallendael*

Main category: cs.LG

TL;DR: GAF是一种生成模型，通过学习独立的端点预测器J（噪声）和K（数据）而非轨迹预测器，实现可组合控制的传输代数操作，支持多模态间的可控插值、混合生成和语义变形。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型通常学习轨迹预测器，限制了可控性和组合性。GAF旨在通过分解端点预测器实现更灵活的代数操作，支持丰富的可组合生成能力。

Method: 提出生成锚定场（GAF），学习独立的噪声端点预测器J和数据端点预测器K，速度场v=K-J从它们的时间条件分歧中产生。通过传输代数对学习到的{(J_n,K_n)}头进行代数操作，实现组合控制。

Result: 在CelebA-HQ 64×64上达到FID 7.5的强样本质量，同时提供无损失循环传输（LPIPS=0.0），支持可控插值、混合生成和语义变形等组合生成功能。

Conclusion: GAF通过端点预测器分解实现了传输代数，将组合生成作为架构原语，在保持高质量生成的同时提供了丰富的可控性和组合性操作能力。

Abstract: We present Generative Anchored Fields (GAF), a generative model that learns independent endpoint predictors $J$ (noise) and $K$ (data) rather than a trajectory predictor. The velocity field $v=K-J$ emerges from their time-conditioned disagreement. This factorization enables \textit{Transport Algebra}: algebraic operation on learned $\{(J_n,K_n)\}_{n=1}^N$ heads for compositional control. With class-specific $K_n$ heads, GAF supports a rich family of directed transport maps between a shared base distribution and multiple modalities, enabling controllable interpolation, hybrid generation, and semantic morphing through vector arithmetic. We achieve strong sample quality (FID 7.5 on CelebA-HQ $64\times 64$) while uniquely providing compositional generation as an architectural primitive. We further demonstrate, GAF has lossless cyclic transport between its initial and final state with LPIPS=$0.0$. Code available at https://github.com/IDLabMedia/GAF

</details>


### [91] [Integrated Transcriptomic-proteomic Biomarker Identification for Radiation Response Prediction in Non-small Cell Lung Cancer Cell Lines](https://arxiv.org/abs/2511.22735)
*Yajun Yu,Guoping Xu,Steve Jiang,Robert Timmerman,John Minna,Yuanyuan Zhang,Hao Peng*

Main category: cs.LG

TL;DR: 开发了一个整合转录组-蛋白质组的框架，用于识别预测非小细胞肺癌辐射反应（以2Gy存活分数SF2衡量）的并发生物标志物，通过结合RNA测序和蛋白质组学数据提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 开发一个整合转录组和蛋白质组数据的框架，以识别能够预测非小细胞肺癌细胞系辐射反应（SF2）的并发生物标志物，弥补单组学数据的局限性。

Method: 收集73个NSCLC细胞系的RNA-seq数据和46个细胞系的DIA-MS蛋白质组数据，预处理后保留1,605个共享基因。使用Lasso回归进行特征选择，通过五折交叉验证重复十次。构建支持向量回归模型，分别使用转录组、蛋白质组和整合数据集。评估模型性能（R2和RMSE），分析RNA-蛋白质表达相关性。

Result: RNA-蛋白质表达呈显著正相关（中位数Pearson r=0.363）。从三组数据中识别出20个优先基因特征。单组学模型跨组学泛化能力有限，而整合模型在两个数据集中均表现出平衡的预测准确性（转录组：R2=0.461，RMSE=0.120；蛋白质组：R2=0.604，RMSE=0.111）。

Conclusion: 本研究首次提出了用于NSCLC SF2预测的蛋白质转录组学框架，证明了整合转录组和蛋白质组数据的互补价值。识别的并发生物标志物捕捉了转录调控和功能性蛋白质活性，提供了机制见解和转化潜力。

Abstract: To develop an integrated transcriptome-proteome framework for identifying concurrent biomarkers predictive of radiation response, as measured by survival fraction at 2 Gy (SF2), in non-small cell lung cancer (NSCLC) cell lines. RNA sequencing (RNA-seq) and data-independent acquisition mass spectrometry (DIA-MS) proteomic data were collected from 73 and 46 NSCLC cell lines, respectively. Following preprocessing, 1,605 shared genes were retained for analysis. Feature selection was performed using least absolute shrinkage and selection operator (Lasso) regression with a frequency-based ranking criterion under five-fold cross-validation repeated ten times. Support vector regression (SVR) models were constructed using transcriptome-only, proteome-only, and combined transcriptome-proteome feature sets. Model performance was assessed by the coefficient of determination (R2) and root mean square error (RMSE). Correlation analyses evaluated concordance between RNA and protein expression and the relationships of selected biomarkers with SF2. RNA-protein expression exhibited significant positive correlations (median Pearson's r = 0.363). Independent pipelines identified 20 prioritized gene signatures from transcriptomic, proteomic, and combined datasets. Models trained on single-omic features achieved limited cross-omic generalizability, while the combined model demonstrated balanced predictive accuracy in both datasets (R2=0.461, RMSE=0.120 for transcriptome; R2=0.604, RMSE=0.111 for proteome). This study presents the first proteotranscriptomic framework for SF2 prediction in NSCLC, highlighting the complementary value of integrating transcriptomic and proteomic data. The identified concurrent biomarkers capture both transcriptional regulation and functional protein activity, offering mechanistic insights and translational potential.

</details>


### [92] [VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization](https://arxiv.org/abs/2511.22749)
*Zeng Wang,Weihua Xiao,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Ramesh Karri*

Main category: cs.LG

TL;DR: VeriDispatcher是一个多LLM RTL生成框架，通过预推理难度预测将任务分派给合适的LLM，在保持准确性的同时显著降低商业API调用成本。


<details>
  <summary>Details</summary>
Motivation: 不同LLM在RTL生成任务上各有优势，但现有工作主要关注单个模型的提示或微调。如何协调多个LLM共同提高RTL质量同时降低成本，而不是运行所有模型选择最佳输出，这个问题尚未得到充分研究。

Method: 提出VeriDispatcher框架：1) 为每个LLM训练紧凑分类器，基于任务描述的语义嵌入和难度评分；2) 难度评分来自结合语法、结构相似性和功能正确性的基准变体；3) 推理时使用预测器将任务路由到选定的LLM子集。

Result: 在RTLLM和VerilogEval上测试10个不同LLM：1) RTLLM上准确率提升18%，仅使用40%商业调用；2) VerilogEval上保持准确性，商业使用减少25%；3) 实现硬件设计自动化中成本效益高、高质量的LLM部署。

Conclusion: VeriDispatcher通过智能任务分派有效协调多个LLM，在显著降低商业API成本的同时提高或保持RTL生成质量，为硬件设计自动化中的LLM部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.

</details>


### [93] [Exact Learning of Arithmetic with Differentiable Agents](https://arxiv.org/abs/2511.22751)
*Hristo Papazov,Francesco D'Angelo,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 提出可微分有限状态转换器（DFSTs），一种图灵完备的模型家族，通过端到端对数并行可微分训练，在算术任务上实现强长度泛化。


<details>
  <summary>Details</summary>
Motivation: 探索基于梯度的方法进行精确算法学习的可能性，解决现有架构在长度泛化方面的局限性，实现常数精度、常数时间生成。

Method: 使用可微分有限状态转换器（DFSTs），利用专家智能体的策略轨迹观察进行训练，支持二进制和十进制加法与乘法任务的端到端对数并行可微分训练。

Result: 在极小数据集上训练的模型能够无错误地泛化到比训练样本长数千倍的输入，展示了在算术任务上的强长度泛化能力。

Conclusion: 在结构化中间监督下训练可微分智能体可能为基于梯度的精确算法技能学习开辟新途径。

Abstract: We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.

</details>


### [94] [GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels](https://arxiv.org/abs/2511.22793)
*Bhavya Sai Nukapotula,Rishabh Tripathi,Seth Pregler,Dileep Kalathil,Srinivas Shakkottai,Theodore S. Rappaport*

Main category: cs.LG

TL;DR: GSpaRC使用3D高斯基元表示RF环境，通过定制CUDA管道实现亚毫秒级延迟的实时信道状态信息重建，显著降低5G网络中的导频开销。


<details>
  <summary>Details</summary>
Motivation: 在5G网络中，获取信道状态信息需要频繁发送导频信号，消耗高达25%的频谱资源。现有方法虽然能减少开销，但推理延迟在5-100毫秒范围，无法满足实时系统需求。

Method: 使用紧凑的3D高斯基元表示RF环境，每个基元由轻量级神经网络参数化，并加入基于距离衰减等物理特征。采用针对RF接收定制的等距柱状投影到半球表面，反映全向天线特性。通过定制CUDA管道实现全并行化的方向排序、splatting和渲染。

Result: 在多个RF数据集上评估，GSpaRC实现了与最新方法相似的CSI重建精度，同时将训练和推理时间降低了一个数量级以上，突破了1毫秒延迟障碍。

Conclusion: GSpaRC通过适度的GPU计算换取显著的导频开销减少，实现了可扩展、低延迟的信道估计，适合部署在5G和未来无线系统中。

Abstract: Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.

</details>


### [95] [Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?](https://arxiv.org/abs/2511.22794)
*Fitria Wulandari Ramlan,Colm O'Riordan,Gabriel Kronberger,James McDermott*

Main category: cs.LG

TL;DR: 使用知识蒸馏生成合成数据来提升符号回归模型的外推性能


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在训练数据范围内表现良好，但在外推区域（超出训练数据范围）表现不佳。符号回归虽然能生成灵活模型，但在外推时行为不可靠，需要改进外推性能。

Method: 使用核密度估计识别输入空间中训练数据稀疏的区域，通过知识蒸馏方法生成合成数据：教师模型在新输入点上生成预测，然后用这些数据训练学生模型。评估了神经网络、随机森林和遗传编程作为教师和学生模型的效果。

Result: 在六个基准数据集上的实验表明，遗传编程模型在训练合成数据后外推性能得到改善，特别是在外推区域。改进程度取决于数据集和教师模型，最显著的改进是使用GPe生成的合成数据训练GPp模型。内插区域只有轻微变化，模型性能在不同输入空间区域存在异质性误差。

Conclusion: 通过知识蒸馏生成合成数据的方法为改善机器学习模型的外推性能提供了实用解决方案，特别是在符号回归任务中。该方法能有效提升模型在数据稀疏区域的表现。

Abstract: Many machine learning models perform well when making predictions within the training data range, but often struggle when required to extrapolate beyond it. Symbolic regression (SR) using genetic programming (GP) can generate flexible models but is prone to unreliable behaviour in extrapolation. This paper investigates whether adding synthetic data can help improve performance in such cases. We apply Kernel Density Estimation (KDE) to identify regions in the input space where the training data is sparse. Synthetic data is then generated in those regions using a knowledge distillation approach: a teacher model generates predictions on new input points, which are then used to train a student model. We evaluate this method across six benchmark datasets, using neural networks (NN), random forests (RF), and GP both as teacher models (to generate synthetic data) and as student models (trained on the augmented data). Results show that GP models can often improve when trained on synthetic data, especially in extrapolation areas. However, the improvement depends on the dataset and teacher model used. The most important improvements are observed when synthetic data from GPe is used to train GPp in extrapolation regions. Changes in interpolation areas show only slight changes. We also observe heterogeneous errors, where model performance varies across different regions of the input space. Overall, this approach offers a practical solution for better extrapolation. Note: An earlier version of this work appeared in the GECCO 2025 Workshop on Symbolic Regression. This arXiv version corrects several parts of the original submission.

</details>


### [96] [Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence](https://arxiv.org/abs/2511.22813)
*Antoine Salomon*

Main category: cs.LG

TL;DR: INN提出智能神经网络范式，将神经元作为具有内部记忆和学习通信模式的一等实体，通过图结构而非层级组织实现涌现计算，在Text8基准上超越Transformer并匹配LSTM性能。


<details>
  <summary>Details</summary>
Motivation: 受生物神经元智能特性启发：维持内部状态、选择性通信、自组织成复杂图而非刚性层级。探索是否能让AI从类似智能计算单元中涌现。

Method: 提出智能神经网络(INN)，每个智能神经元结合选择性状态空间动态（知道何时激活）和基于注意力的路由（知道向谁发送信号），在完全图中组织而非顺序层。

Result: 在Text8字符建模基准上达到1.705 BPC，显著优于可比Transformer(2.055 BPC)，匹配高度优化的LSTM基线。参数匹配的堆叠Mamba块无法收敛(>3.4 BPC)，证明图拓扑提供训练稳定性。

Conclusion: 神经元中心设计结合图组织不仅是生物启发，更是计算有效的，为模块化、可解释、可扩展的神经架构开辟新方向。

Abstract: Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers.
  Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing.
  This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures.

</details>


### [97] [A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees](https://arxiv.org/abs/2511.22823)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出一个统一的弱监督学习框架，通过直接构建基于弱监督数据结构的稳定代理风险，避免后处理调整，涵盖多种弱监督设置。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督学习方法通常针对特定监督模式（如PU、UU、CLL、PLL等），依赖后处理校正来缓解间接监督引起的不稳定性，缺乏统一框架。

Method: 提出基于弱监督数据结构直接构建稳定代理风险的统一框架，涵盖PU、UU、CLL、PLL、多类未标记和基于元组的学习等多种设置，建立基于Rademacher复杂度的非渐近泛化界。

Result: 实验显示该方法在不同类别先验、数据集规模和类别数量下均取得一致性能提升，无需启发式稳定化，且对过拟合具有鲁棒性。

Conclusion: 该统一框架为弱监督学习提供了理论基础和实践方法，通过直接构建稳定代理风险避免了后处理调整，在多种弱监督设置下表现出优越性能和鲁棒性。

Abstract: Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.

</details>


### [98] [CausalProfiler: Generating Synthetic Benchmarks for Rigorous and Transparent Evaluation of Causal Machine Learning](https://arxiv.org/abs/2511.22842)
*Panayiotis Panayiotou,Audrey Poinsot,Alessandro Leite,Nicolas Chesneau,Marc Schoenauer,Özgür Şimşek*

Main category: cs.LG

TL;DR: CausalProfiler是一个用于因果机器学习的随机合成基准测试生成器，通过随机采样因果模型、数据、查询和真实值来创建多样化的评估环境，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 当前因果机器学习的实证评估实践有限，现有基准测试通常依赖少量手工制作或半合成数据集，导致结论脆弱且难以泛化。需要更系统、透明的评估框架。

Method: 提出CausalProfiler，基于对因果模型类别、查询类型和数据特征的明确设计选择，随机采样生成合成因果基准测试。该方法覆盖因果推理的三个层次：观察、干预和反事实。

Result: CausalProfiler是首个具有覆盖保证和透明假设的随机合成因果基准测试生成器。通过评估多种最先进方法在不同条件和假设下的表现，展示了其分析能力和洞察力。

Conclusion: CausalProfiler为因果机器学习方法提供了严格、透明的评估框架，能够生成多样化的测试条件，支持在识别机制内外的全面评估，推动了因果机器学习评估实践的发展。

Abstract: Causal machine learning (Causal ML) aims to answer "what if" questions using machine learning algorithms, making it a promising tool for high-stakes decision-making. Yet, empirical evaluation practices in Causal ML remain limited. Existing benchmarks often rely on a handful of hand-crafted or semi-synthetic datasets, leading to brittle, non-generalizable conclusions. To bridge this gap, we introduce CausalProfiler, a synthetic benchmark generator for Causal ML methods. Based on a set of explicit design choices about the class of causal models, queries, and data considered, the CausalProfiler randomly samples causal models, data, queries, and ground truths constituting the synthetic causal benchmarks. In this way, Causal ML methods can be rigorously and transparently evaluated under a variety of conditions. This work offers the first random generator of synthetic causal benchmarks with coverage guarantees and transparent assumptions operating on the three levels of causal reasoning: observation, intervention, and counterfactual. We demonstrate its utility by evaluating several state-of-the-art methods under diverse conditions and assumptions, both in and out of the identification regime, illustrating the types of analyses and insights the CausalProfiler enables.

</details>


### [99] [PerfMamba: Performance Analysis and Pruning of Selective State Space Models](https://arxiv.org/abs/2511.22849)
*Abdullah Al Asif,Mobina Kashaniyan,Sixing Yu,Juan Pablo Muñoz,Ali Jannesari*

Main category: cs.LG

TL;DR: 对Mamba-1和Mamba-2选择性状态空间模型进行实证性能分析，发现SSM组件计算资源消耗大，提出状态剪枝技术实现1.14倍加速和11.50%内存减少。


<details>
  <summary>Details</summary>
Motivation: 选择性SSM作为Transformer的替代方案具有理论计算效率优势，但其运行时行为、资源利用模式和扩展特性尚未得到充分理解，阻碍了最优部署和架构改进。

Method: 对Mamba-1和Mamba-2进行系统性能分析，研究计算模式、内存访问、I/O特性和扩展特性（序列长度64-16384），并提出基于SSM组件低活动状态剪枝的技术。

Result: SSM组件在Mamba块中消耗大量计算资源；提出的剪枝技术在保持精度的同时，实现1.14倍加速和11.50%内存减少，性能提升在不同序列长度下均有效。

Conclusion: 研究为设计更高效的SSM架构提供了有价值的指导，提出的剪枝方法可在保持精度的同时显著提升性能，适用于广泛的现实应用场景。

Abstract: Recent advances in sequence modeling have introduced selective SSMs as promising alternatives to Transformer architectures, offering theoretical computational efficiency and sequence processing advantages. A comprehensive understanding of selective SSMs in runtime behavior, resource utilization patterns, and scaling characteristics still remains unexplored, thus obstructing their optimal deployment and further architectural improvements. This paper presents a thorough empirical study of Mamba-1 and Mamba-2, systematically profiled for performance to assess the design principles that contribute to their efficiency in state-space modeling. A detailed analysis of computation patterns, memory access, I/O characteristics, and scaling properties was performed for sequence lengths ranging from 64 to 16384 tokens. Our findings show that the SSM component, a central part of the selective SSM architecture, demands a significant portion of computational resources compared to other components in the Mamba block. Based on these insights, we propose a pruning technique that selectively removes low-activity states within the SSM component, achieving measurable throughput and memory gains while maintaining accuracy within a moderate pruning regime. This approach results in performance improvements across varying sequence lengths, achieving a 1.14x speedup and reducing memory usage by 11.50\%. These results offer valuable guidance for designing more efficient SSM architectures that can be applied to a wide range of real-world applications.

</details>


### [100] [TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE](https://arxiv.org/abs/2511.22853)
*Jiawen Wei,Lan Jiang,Pengbo Wei,Ziwen Ye,Teng Song,Chen Chen,Guangrui Ma*

Main category: cs.LG

TL;DR: TARFVAE：结合Transformer自回归流和变分自编码器的高效一步生成式时间序列预测框架


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法大多采用循环生成操作或重复去噪步骤，预测过程繁琐，尤其不适合长期预测。这些方法通常只在短期预测上进行实验，与确定性方法在长期预测上的比较有限，实际优势不明确。

Method: 提出TARFVAE框架，结合Transformer自回归流（TARFLOW）和变分自编码器（VAE）。TARFLOW增强VAE的后验估计，打破高斯假设，实现更信息丰富的潜在空间。仅使用TARFLOW的前向过程，避免自回归逆操作，实现快速生成。从先验潜在空间采样，通过VAE解码器直接生成全时域预测。

Result: 在基准数据集上，TARFVAE在不同预测时域上均优于最先进的确定性和生成式模型，同时保持高效的预测速度。

Conclusion: TARFVAE为生成式时间序列预测提供了一个高效且强大的解决方案，通过简单MLP模块实现了卓越性能。

Abstract: Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting.

</details>


### [101] [CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate](https://arxiv.org/abs/2511.22854)
*Finn G. Vamosi,Nils D. Forkert*

Main category: cs.LG

TL;DR: 提出双智能体辩论框架，通过对话式因果推理提升语言模型在因果推断任务上的准确性，在CLadder数据集上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 人类进行因果推理时会考虑多种"假设"场景，类似地，语言模型也应能通过内部对话在多种干预和反事实中判断因果主张的有效性。当前推理语言模型在因果推断和对抗辩论方面的潜力尚未充分探索

Method: 采用双智能体辩论框架：一个模型提供结构化因果推断，另一个模型批判性地检查推理中的逻辑缺陷。当出现分歧时，智能体试图说服对方，挑战彼此的逻辑并修正结论，直到达成共识。使用推理语言模型作为辩论智能体

Result: 在CLadder数据集上，DeepSeek-R1的整体准确率从78.03%提升至87.45%，反事实类别从67.94%提升至80.04%；Qwen3的整体准确率从84.16%提升至89.41%，反事实问题从71.53%提升至80.35%。强模型与弱智能体辩论仍能显著受益

Conclusion: 多智能体辩论能有效提升因果推理准确性，推理模型可作为因果推断多智能体系统的构建模块，多样视角对因果问题解决至关重要

Abstract: When people reason about cause and effect, they often consider many competing "what if" scenarios before deciding which explanation fits best. Analogously, advanced language models capable of causal inference can consider multiple interventions and counterfactuals to judge the validity of causal claims. Crucially, this type of reasoning is less like a single calculation and more like an internal dialogue between alternative hypotheses. In this paper, we make this dialogue explicit through a dual-agent debate framework where one model provides a structured causal inference, and the other critically examines this reasoning for logical flaws. When disagreements arise, agents attempt to persuade each other, challenging each other's logic and revising their conclusions until they converge on a mutually agreed answer. To take advantage of this deliberative process, we specifically use reasoning language models, whose strengths in both causal inference and adversarial debate remain under-explored relative to standard large language models. We evaluate our approach on the CLadder dataset, a benchmark linking natural language questions to formally defined causal graphs across all three rungs of Pearl's ladder of causation. With Qwen3 and DeepSeek-R1 as debater agents, we demonstrate that multi-agent debate improves DeepSeek-R1's overall accuracy in causal inference from 78.03% to 87.45%, with the counterfactual category specifically improving from 67.94% to 80.04% accuracy. Similarly, Qwen3's overall accuracy improves from 84.16% to 89.41%, and counterfactual questions from 71.53% to 80.35%, showing that strong models can still benefit greatly from debate with weaker agents. Our results highlight the potential of reasoning models as building blocks for multi-agent systems in causal inference, and demonstrate the importance of diverse perspectives in causal problem-solving.

</details>


### [102] [Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation](https://arxiv.org/abs/2511.22862)
*Jiacheng Li,Songhe Feng*

Main category: cs.LG

TL;DR: 本文提出BriMPR框架，通过渐进式重对齐解决多模态测试时适应的耦合效应问题，包含单模态特征对齐和跨模态对比学习两个模块。


<details>
  <summary>Details</summary>
Motivation: 在多模态场景中，不同模态间的分布偏移程度不同，导致单模态浅层特征偏移和跨模态高层语义错位的复杂耦合效应，这阻碍了现有TTA方法向多模态领域的扩展。

Method: 提出BriMPR框架，采用分而治之策略：1) 将MMTTA分解为多个单模态特征对齐子问题，利用提示调优校准单模态全局特征分布；2) 为掩码和完整模态组合分配可信伪标签，引入跨模态实例级对比学习增强信息交互。

Result: 在包括基于损坏和真实世界域偏移基准的MMTTA任务上进行广泛实验，证明了该方法的优越性。

Conclusion: BriMPR框架通过渐进式重对齐有效解决了多模态测试时适应中的耦合效应问题，为多模态TTA提供了有效解决方案。

Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at [this URL](https://github.com/Luchicken/BriMPR).

</details>


### [103] [ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining](https://arxiv.org/abs/2511.22866)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.LG

TL;DR: ARM-Explainer：基于关联规则挖掘的图神经网络解释器，用于最大团问题，能识别关键节点特征并提升GNN性能


<details>
  <summary>Details</summary>
Motivation: 图神经网络在组合优化问题中应用广泛，但其预测解释方法仍不成熟，需要开发有效的解释工具来理解GNN的决策过程

Method: 提出ARM-Explainer，一种基于关联规则挖掘的后置模型级解释器，应用于混合几何散射GNN在最大团问题上的预测分析

Result: 在TWITTER和BHOSLIB-DIMACS数据集上，前8条关联规则的中位提升度和置信度分别达到2.42和0.49；通过增强信息性节点特征，GNN在BHOSLIB-DIMACS大图上的最大团大小中位数提升22%（从29.5到36）

Conclusion: ARM-Explainer能有效解释GNN预测，识别关键节点特征及其取值范围，并且这些解释性特征能显著提升GNN在组合优化问题上的性能

Abstract: Numerous graph neural network (GNN)-based algorithms have been proposed to solve graph-based combinatorial optimization problems (COPs), but methods to explain their predictions remain largely undeveloped. We introduce ARM-Explainer, a post-hoc, model-level explainer based on association rule mining, and demonstrate it on the predictions of the hybrid geometric scattering (HGS) GNN for the maximum clique problem (MCP), a canonical NP-hard graph-based COP. The eight most explanatory association rules discovered by ARM-Explainer achieve high median lift and confidence values of 2.42 and 0.49, respectively, on test instances from the TWITTER and BHOSLIB-DIMACS benchmark datasets. ARM-Explainer identifies the most important node features, together with their value ranges, that influence the GNN's predictions on these datasets. Furthermore, augmenting the GNN with informative node features substantially improves its performance on the MCP, increasing the median largest-found clique size by 22% (from 29.5 to 36) on large graphs from the BHOSLIB-DIMACS dataset.

</details>


### [104] [Covering-Space Normalizing Flows: Approximating Pushforwards on Lens Spaces](https://arxiv.org/abs/2511.22882)
*William Ghanem*

Main category: cs.LG

TL;DR: 通过通用覆盖映射构造透镜空间上的推前分布，并利用流进行近似，特别处理对称S^3分布中的冗余问题


<details>
  <summary>Details</summary>
Motivation: 研究如何通过通用覆盖映射将S^3上的分布推前到透镜空间L(p;q)上，并寻求有效的近似方法，特别关注对称分布中的冗余问题

Method: 使用通用覆盖映射ρ: S^3 → L(p;q)构造推前分布，利用流在L(p;q)上进行近似，特别处理对称S^3分布中的冗余删除

Result: 成功近似了von Mises-Fisher诱导的目标密度推前分布，以及为苯分子建模构建的Z_12对称玻尔兹曼分布在S^3上的推前分布

Conclusion: 该方法能有效构造和近似透镜空间上的推前分布，特别在处理对称分布冗余方面表现良好，为分子建模等应用提供了有效工具

Abstract: We construct pushforward distributions via the universal covering map rho: S^3 -> L(p;q) with the goal of approximating these distributions using flows on L(p;q). We highlight that our method deletes redundancies in the case of a symmetric S^3 distribution. Using our model, we approximate the pushforwards of von Mises-Fisher-induced target densities as well as that of a Z_12-symmetric Boltzmann distribution on S^3 constructed to model benzene.

</details>


### [105] [Modeling Chaotic Pedestrian Behavior Using Chaos Indicators and Supervised Learning](https://arxiv.org/abs/2511.22887)
*Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.LG

TL;DR: 该研究提出一个数据驱动框架，使用监督学习建模行人混沌运动，通过计算机视觉提取轨迹，用混沌指标量化行为，PCA整合为统一混沌分数，CatBoost模型预测效果优异。


<details>
  <summary>Details</summary>
Motivation: 随着全球城市致力于提升步行性和安全性，理解行人行为的不规则和不可预测性变得日益重要。需要量化行人混沌行为以支持城市规划、基础设施改进和自动驾驶系统风险评估。

Method: 1) 在白天和夜间条件下录制视频捕捉行人动态；2) 计算机视觉技术提取行人轨迹；3) 使用近似熵和李雅普诺夫指数（分别计算速度和方向变化）四个混沌指标量化行为混沌；4) PCA整合为统一混沌分数；5) 构建个体、群体和交通上下文特征；6) 训练Random Forest和CatBoost回归模型；7) SHAP分析特征重要性。

Result: CatBoost模型表现最佳：白天PCA-based CatBoost模型R²达0.8319，夜间模型R²达0.8574。SHAP分析显示旅行距离、运动持续时间和速度变异性是混沌行为的主要贡献因素。

Conclusion: 该框架能够量化现实环境中的行为不稳定性，帮助规划者和工程师识别高风险行人区域、改进基础设施、校准微观仿真模型，并为自动驾驶系统提供基于可观测、可解释特征的短期运动不可预测性评估。

Abstract: As cities around the world aim to improve walkability and safety, understanding the irregular and unpredictable nature of pedestrian behavior has become increasingly important. This study introduces a data-driven framework for modeling chaotic pedestrian movement using empirically observed trajectory data and supervised learning. Videos were recorded during both daytime and nighttime conditions to capture pedestrian dynamics under varying ambient and traffic contexts. Pedestrian trajectories were extracted through computer vision techniques, and behavioral chaos was quantified using four chaos metrics: Approximate Entropy and Lyapunov Exponent, each computed for both velocity and direction change. A Principal Component Analysis (PCA) was then applied to consolidate these indicators into a unified chaos score. A comprehensive set of individual, group-level, and contextual traffic features was engineered and used to train Random Forest and CatBoost regression models. CatBoost models consistently achieved superior performance. The best daytime PCA-based CatBoost model reached an R^2 of 0.8319, while the nighttime PCA-based CatBoost model attained an R^2 of 0.8574. SHAP analysis highlighted that features such as distance travel, movement duration, and speed variability were robust contributors to chaotic behavior. The proposed framework enables practitioners to quantify and anticipate behavioral instability in real-world settings. Planners and engineers can use chaos scores to identify high-risk pedestrian zones, apprise infrastructure improvements, and calibrate realistic microsimulation models. The approach also supports adaptive risk assessment in automated vehicle systems by capturing short-term motion unpredictability grounded in observable, interpretable features.

</details>


### [106] [Adversarial Training for Process Reward Models](https://arxiv.org/abs/2511.22888)
*Gurusha Juneja,Deepak Nathani,William Yang Wang*

Main category: cs.LG

TL;DR: APRM通过对抗训练提升PRM性能，无需人工标注即可生成渐进式困难负例，在数学推理任务上显著提升模型准确率


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型(PRMs)面临两个主要限制：1) 人工步骤级标注成本高昂；2) 静态训练数据难以泛化到新错误类型。这限制了PRMs在提升LLM推理能力方面的广泛应用。

Method: 提出对抗训练的过程奖励模型(APRM)：包含生成器(G)和判别器(R)两个组件。生成器学习产生欺骗判别器的推理错误，判别器同时学习检测这些错误。这种对抗机制自动生成渐进式困难负例，无需人工步骤级标注。

Result: 在多个数学推理基准测试中，APRM平均比最强的PRM基线提升3.4个百分点。在分布外任务上提升更显著，达到5.3个百分点。

Conclusion: 对抗训练为过程奖励模型提供了一种有效的自我监督学习范式，能够自动生成高质量训练数据，显著提升模型对推理错误的检测能力和泛化性能，解决了传统PRMs面临的人工标注成本和泛化能力限制。

Abstract: Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.

</details>


### [107] [EnECG: Efficient Ensemble Learning for Electrocardiogram Multi-task Foundation Model](https://arxiv.org/abs/2511.22935)
*Yuhao Xu,Xiaoda Wang,Jiaying Lu,Sirui Ding,Defu Cao,Huaxiu Yao,Yan Liu,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: EnECG是一个基于专家混合的集成学习框架，专门用于ECG多任务分析，通过轻量级适配策略结合多个专业基础模型，降低计算成本同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有ECG分析模型未能充分利用各种心脏异常之间的相互关系，而开发一个能提取所有相关特征的多任务模型又很困难。大规模基础模型虽然强大，但通常未在ECG数据上预训练，完全重新训练或微调计算成本高昂。

Method: 提出EnECG框架：1) 集成多个专业基础模型，每个擅长不同ECG解释方面；2) 采用轻量级适配策略：为每个基础模型附加专用输出层，仅对新参数应用低秩适配(LoRA)；3) 使用专家混合(MoE)机制学习集成权重，有效结合各模型的互补专业知识。

Result: 实验结果表明，通过最小化微调范围，EnECG能够帮助降低计算和内存成本，同时保持基础模型的强大表示能力。该框架不仅增强了特征提取和预测性能，还确保了实际临床应用的效率。

Conclusion: EnECG通过集成多个专业基础模型并结合轻量级适配策略，为ECG多任务分析提供了一个高效实用的解决方案，在降低计算成本的同时提升了性能，适合实际临床应用。

Abstract: Electrocardiogram (ECG) analysis plays a vital role in the early detection, monitoring, and management of various cardiovascular conditions. While existing models have achieved notable success in ECG interpretation, they fail to leverage the interrelated nature of various cardiac abnormalities. Conversely, developing a specific model capable of extracting all relevant features for multiple ECG tasks remains a significant challenge. Large-scale foundation models, though powerful, are not typically pretrained on ECG data, making full re-training or fine-tuning computationally expensive. To address these challenges, we propose EnECG(Mixture of Experts-based Ensemble Learning for ECG Multi-tasks), an ensemble-based framework that integrates multiple specialized foundation models, each excelling in different aspects of ECG interpretation. Instead of relying on a single model or single task, EnECG leverages the strengths of multiple specialized models to tackle a variety of ECG-based tasks. To mitigate the high computational cost of full re-training or fine-tuning, we introduce a lightweight adaptation strategy: attaching dedicated output layers to each foundation model and applying Low-Rank Adaptation (LoRA) only to these newly added parameters. We then adopt a Mixture of Experts (MoE) mechanism to learn ensemble weights, effectively combining the complementary expertise of individual models. Our experimental results demonstrate that by minimizing the scope of fine-tuning, EnECG can help reduce computational and memory costs while maintaining the strong representational power of foundation models. This framework not only enhances feature extraction and predictive performance but also ensures practical efficiency for real-world clinical applications. The code is available at https://github.com/yuhaoxu99/EnECG.git.

</details>


### [108] [CORGI: GNNs with Convolutional Residual Global Interactions for Lagrangian Simulation](https://arxiv.org/abs/2511.22938)
*Ethan Ji,Yuanzhou Chen,Arush Ramteke,Fang Sun,Tianrun Yu,Jai Parera,Wei Wang,Yizhou Sun*

Main category: cs.LG

TL;DR: CORGI：一种混合架构，通过将粒子特征投影到网格、应用卷积更新并映射回粒子域，增强GNN求解器以捕获流体流动中的全局相互作用，显著提升精度而计算开销很小。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解器在处理非线性流体动力学问题时计算成本高，而现有的拉格朗日神经网络代理（如GNS和SEGNN）由于感受野有限，难以捕捉流体流动中固有的全局相互作用。

Method: 提出CORGI混合架构，在GNN求解器基础上增加轻量级欧拉组件进行全局上下文聚合：1）将粒子特征投影到网格；2）应用卷积更新；3）将更新后的特征映射回粒子域。

Result: 在GNS骨干网络上，CORGI将推演精度提升57%，仅增加13%推理时间和31%训练时间；相比SEGNN，精度提升49%，同时减少48%推理时间和30%训练时间；在相同运行时约束下，平均比GNS提升47%精度。

Conclusion: CORGI通过结合拉格朗日和欧拉方法的优势，有效解决了GNN求解器感受野有限的问题，能够在不同计算预算下显著提升流体动力学模拟的精度和效率。

Abstract: Partial differential equations (PDEs) are central to dynamical systems modeling, particularly in hydrodynamics, where traditional solvers often struggle with nonlinearity and computational cost. Lagrangian neural surrogates such as GNS and SEGNN have emerged as strong alternatives by learning from particle-based simulations. However, these models typically operate with limited receptive fields, making them inaccurate for capturing the inherently global interactions in fluid flows. Motivated by this observation, we introduce Convolutional Residual Global Interactions (CORGI), a hybrid architecture that augments any GNN-based solver with a lightweight Eulerian component for global context aggregation. By projecting particle features onto a grid, applying convolutional updates, and mapping them back to the particle domain, CORGI captures long-range dependencies without significant overhead. When applied to a GNS backbone, CORGI achieves a 57% improvement in rollout accuracy with only 13% more inference time and 31% more training time. Compared to SEGNN, CORGI improves accuracy by 49% while reducing inference time by 48% and training time by 30%. Even under identical runtime constraints, CORGI outperforms GNS by 47% on average, highlighting its versatility and performance on varied compute budgets.

</details>


### [109] [Bandit Guided Submodular Curriculum for Adaptive Subset Selection](https://arxiv.org/abs/2511.22944)
*Prateek Chanda,Prayas Agrawal,Saral Sureka,Lokesh Reddy Polu,Atharv Kshirsagar,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出ONLINESUBMOD方法，将自适应子集选择重新定义为多臂老虎机问题，每个臂对应一个指导样本选择的子模函数，通过在线贪心策略优化效用驱动的奖励，在多种采样机制下实现无遗憾性能。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习从简单到困难样本进行，但定义可靠的难度概念仍然困难。先前工作使用子模函数在课程学习中诱导难度分数，本研究重新解释自适应子集选择问题。

Method: 将自适应子集选择重新表述为多臂老虎机问题，每个臂对应一个指导样本选择的子模函数。提出ONLINESUBMOD在线贪心策略，优化效用驱动的奖励函数，在各种采样机制下实现理论上的无遗憾性能。

Result: ONLINESUBMOD在视觉和语言数据集上优于传统课程学习和双层优化方法，显示出更优的准确率-效率权衡。验证驱动的奖励指标为课程调度提供了原则性指导。

Conclusion: 通过将自适应子集选择重新定义为多臂老虎机问题，并提出ONLINESUBMOD在线贪心策略，本研究为课程学习提供了新的理论框架和实用方法，验证驱动的奖励指标能够有效指导课程调度。

Abstract: Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce ONLINESUBMOD, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, ONLINESUBMOD outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validationdriven reward metrics offer a principled way to guide the curriculum schedule.

</details>


### [110] [Experts are all you need: A Composable Framework for Large Language Model Inference](https://arxiv.org/abs/2511.22955)
*Shrihari Sridharan,Sourjya Roy,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: Comp-LLM提出了一种可组合的推理框架，通过子查询依赖图实现专家间的协作，在保持准确性的同时减少模型大小和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在模型规模大、计算负担重的问题；MoE模型需要联合预训练且不支持多步推理；多智能体框架依赖顺序循环导致延迟高。需要一种既能减少模型大小又能支持高效多步推理的解决方案。

Method: Comp-LLM包含三个组件：1) 子查询生成器：分解输入查询，通过嵌入相似度为每个子查询分配专家，构建依赖图；2) 查询执行器：处理图中的节点，基于依赖关系和资源约束识别并行机会；3) 响应聚合器：将专家中间响应合成连贯的最终答案。

Result: 在多个基准测试中，Comp-LLM相比相似大小的单体LLM准确率提升高达11.01%，模型大小减少1.67-3.56倍，同时相对于其家族中最大模型没有显著性能下降。相比顺序子查询处理，延迟改善1.1-1.7倍。

Conclusion: Comp-LLM通过可组合推理框架有效解决了大语言模型的计算负担和多步推理问题，实现了准确性、模型效率和推理延迟的平衡改进。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

</details>


### [111] [A Trainable Centrality Framework for Modern Data](https://arxiv.org/abs/2511.22959)
*Minh Duc Vu,Mingshuo Liu,Doudou Zhou*

Main category: cs.LG

TL;DR: FUSE是一个神经中心性框架，通过全局头（基于距离比较学习无锚点中心性分数）和局部头（通过去噪分数匹配近似平滑对数密度势）的组合，在任意数据表示上计算中心性分数，单个参数可在两种信号间插值。


<details>
  <summary>Details</summary>
Motivation: 传统深度概念在高维空间中计算昂贵且不稳定，难以扩展到非欧几里得数据，需要一种能在任意表示上高效计算中心性的方法。

Method: FUSE包含两个组件：全局头通过成对距离比较学习无锚点中心性分数；局部头通过去噪分数匹配近似平滑对数密度势。通过0-1之间的单个参数在两种校准信号间插值，实现单次前向传播计算多视角深度中心性。

Result: 在合成分布、真实图像、时间序列、文本数据和标准异常检测基准测试中，FUSE能恢复有意义的经典排序，揭示多尺度几何结构，与强经典基线相比具有竞争力，同时保持简单高效。

Conclusion: FUSE提供了一个统一的神经框架，能在任意数据表示上高效计算中心性分数，解决了传统深度方法在高维和非欧几里得数据中的局限性，为稳健估计、排序和异常检测提供了新工具。

Abstract: Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.

</details>


### [112] [A Modular Framework for Rapidly Building Intrusion Predictors](https://arxiv.org/abs/2511.23000)
*Xiaoxuan Wang,Rolf Stadler*

Main category: cs.LG

TL;DR: 提出模块化框架用于快速组装在线攻击预测器，解决传统单体预测器无法应对数百种攻击类型的问题


<details>
  <summary>Details</summary>
Motivation: 现有攻击预测器多为针对特定攻击类型的单体设计，而MITRE框架包含数百种攻击类型，为每种类型训练单独预测器不可行

Method: 提出模块化框架，通过可复用组件快速组装在线攻击预测器，支持动态调整预测及时性和准确性之间的权衡

Result: 使用公开数据集训练和评估，展示了多个模块化预测器示例，证明在训练过程中可以从模块化组件网络中动态组装有效预测器

Conclusion: 模块化框架能够解决大规模攻击类型预测的可行性问题，同时提供对预测及时性和准确性的灵活控制

Abstract: We study automated intrusion prediction in an IT system using statistical learning methods. The focus is on developing online attack predictors that detect attacks in real time and identify the current stage of the attack. While such predictors have been proposed in the recent literature, these works typically rely on constructing a monolithic predictor tailored to a specific attack type and scenario. Given that hundreds of attack types are cataloged in the MITRE framework, training a separate monolithic predictor for each of them is infeasible. In this paper, we propose a modular framework for rapidly assembling online attack predictors from reusable components. The modular nature of a predictor facilitates controlling key metrics like timeliness and accuracy of prediction, as well as tuning the trade-off between them. Using public datasets for training and evaluation, we provide many examples of modular predictors and show how an effective predictor can be dynamically assembled during training from a network of modular components.

</details>


### [113] [Masked Diffusion for Generative Recommendation](https://arxiv.org/abs/2511.23021)
*Kulin Shah,Bhuvesh Kumar,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 该论文提出使用掩码扩散模型替代自回归模型进行生成式推荐，解决了自回归方法推理成本高、数据利用效率低和短上下文偏置的问题，在数据受限和粗粒度召回方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义ID的生成式推荐使用自回归建模，存在推理成本高（需要顺序解码）、训练数据利用效率低、以及偏向学习短上下文关系的局限性。受NLP领域突破启发，作者希望改进这些缺点。

Method: 提出使用掩码扩散模型来建模用户语义ID序列的概率分布。该方法使用离散掩码噪声来学习序列分布，将掩码token的概率建模为在给定未掩码token条件下的独立分布，从而实现掩码token的并行解码。

Result: 实验表明，该方法在性能上持续优于自回归建模，特别是在数据受限设置和粗粒度召回方面表现突出。同时，该方法在推理时能够并行预测多个语义ID，同时保持优于自回归模型的性能。

Conclusion: 掩码扩散模型为基于语义ID的生成式推荐提供了更有效的替代方案，解决了自回归方法的局限性，在性能、效率和灵活性方面都有显著优势。

Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

</details>


### [114] [Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring](https://arxiv.org/abs/2511.23036)
*Changhun Kim,Yechan Mun,Hyeongwon Jang,Eunseo Lee,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: Delta-XAI：一个用于在线时间序列监控模型解释的框架，通过包装现有XAI方法并引入SWING新方法，系统性解决时间依赖性问题


<details>
  <summary>Details</summary>
Motivation: 当前时间序列XAI方法主要独立分析每个时间步，忽略了时间依赖性，导致难以解释预测变化、无法利用在线动态、评估困难等问题

Method: 1) 提出Delta-XAI框架，通过包装函数适配14种现有XAI方法；2) 引入系统性评估套件评估忠实度、充分性、一致性等；3) 基于实验结果提出SWING方法，通过整合过去观测来捕捉时间依赖性

Result: 实验显示经典梯度方法（如IG）在时间分析适配后优于最新方法；SWING在多种设置和指标下表现一致有效，能缓解分布外效应

Conclusion: Delta-XAI为在线时间序列解释提供了系统性框架，SWING方法通过整合时间依赖性显著提升了解释质量，在敏感领域具有重要应用价值

Abstract: Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.

</details>


### [115] [Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory](https://arxiv.org/abs/2511.23083)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 论文发现高容量核Hopfield网络的"优化脊"对应统计流形上的"稳定性边缘"，是Fisher信息矩阵奇异时的临界边界，将学习动力学与容量通过最小描述长度原理统一起来。


<details>
  <summary>Details</summary>
Motivation: 高容量核Hopfield网络表现出具有极端稳定性的"优化脊"，虽然之前与"谱集中"相关，但其起源仍然不清楚。需要从几何角度理解这一现象的本质。

Method: 在统计流形上分析网络动力学，将优化脊识别为Fisher信息矩阵变得奇异时的"稳定性边缘"，并揭示欧几里得空间中的力对抗是黎曼空间中"对偶平衡"的表现。

Result: 优化脊对应统计流形上的临界边界，此时Fisher信息矩阵奇异。这为理解网络稳定性和容量提供了几何框架，将学习动力学与容量通过最小描述长度原理统一。

Conclusion: 通过几何理论揭示了高容量核Hopfield网络中优化脊的本质，将其理解为统计流形上的稳定性边缘，为自组织临界性提供了统一的几何理论框架。

Abstract: High-capacity kernel Hopfield networks exhibit a "Ridge of Optimization" characterized by extreme stability. While previously linked to "Spectral Concentration," its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the "Edge of Stability," a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.

</details>


### [116] [Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer Embeddings for Antimicrobial Peptide Design](https://arxiv.org/abs/2511.23120)
*Pankhil Gawade,Adam Izdebski,Myriam Lizotte,Kevin R. Moon,Jake S. Rhodes,Guy Wolf,Ewa Szczurek*

Main category: cs.LG

TL;DR: FDD是一种基于扩散的框架，通过冻结、扩散和解码过程，在适应下游任务的同时保持预训练嵌入的几何结构


<details>
  <summary>Details</summary>
Motivation: 当前迁移策略（微调和探测）要么扭曲预训练嵌入的几何结构，要么缺乏足够的表达能力捕捉任务相关信号，特别是在监督数据稀缺时问题更突出

Method: FDD（Freeze, Diffuse, Decode）框架：冻结预训练嵌入，在嵌入的固有流形上传播监督信号，实现几何感知的嵌入空间适应

Result: 应用于抗菌肽设计时，FDD产生低维、可预测且可解释的表征，支持属性预测、检索和潜在空间插值

Conclusion: FDD提供了一种新颖的扩散框架，能够在适应下游任务的同时保持预训练嵌入的底层几何结构，解决了当前迁移策略的局限性

Abstract: Pretrained transformers provide rich, general-purpose embeddings, which are transferred to downstream tasks. However, current transfer strategies: fine-tuning and probing, either distort the pretrained geometric structure of the embeddings or lack sufficient expressivity to capture task-relevant signals. These issues become even more pronounced when supervised data are scarce. Here, we introduce Freeze, Diffuse, Decode (FDD), a novel diffusion-based framework that adapts pre-trained embeddings to downstream tasks while preserving their underlying geometric structure. FDD propagates supervised signal along the intrinsic manifold of frozen embeddings, enabling a geometry-aware adaptation of the embedding space. Applied to antimicrobial peptide design, FDD yields low-dimensional, predictive, and interpretable representations that support property prediction, retrieval, and latent-space interpolation.

</details>


### [117] [Automated Discovery of Laser Dicing Processes with Bayesian Optimization for Semiconductor Manufacturing](https://arxiv.org/abs/2511.23141)
*David Leeftink,Roman Doll,Heleen Visserman,Marco Post,Faysal Boughorbel,Max Hinne,Marcel van Gerven*

Main category: cs.LG

TL;DR: 该论文提出了首个自动化激光切割工艺发现系统，使用贝叶斯优化和两级保真度策略，在工业设备上自动找到生产就绪的激光切割参数，匹配或超越专家水平。


<details>
  <summary>Details</summary>
Motivation: 半导体晶圆激光切割是微电子制造的关键步骤，传统上需要数周专家时间来为新材料调整工艺参数，平衡切割速度、分离质量和材料完整性。这个过程既耗时又依赖专家经验。

Method: 将问题建模为高维约束多目标贝叶斯优化任务，引入顺序两级保真度策略来最小化昂贵的破坏性芯片强度评估。在工业LASER1205切割系统上实现自动化工艺发现。

Result: 在裸硅和产品晶圆上，该方法自主交付了可行的配置，在生产速度、芯片强度和结构完整性方面匹配或超越专家基准，仅需技术人员级别操作。验证显示可以从最终代理模型中获得具有不同权衡的多个可行解。

Conclusion: 专家对发现工艺的进一步细化可以在保持芯片强度和结构完整性的同时进一步提高生产速度，超越纯手动或纯自动方法。该方法实现了半导体激光切割工艺的自动化发现。

Abstract: Laser dicing of semiconductor wafers is a critical step in microelectronic manufacturing, where multiple sequential laser passes precisely separate individual dies from the wafer. Adapting this complex sequential process to new wafer materials typically requires weeks of expert effort to balance process speed, separation quality, and material integrity. We present the first automated discovery of production-ready laser dicing processes on an industrial LASER1205 dicing system. We formulate the problem as a high-dimensional, constrained multi-objective Bayesian optimization task, and introduce a sequential two-level fidelity strategy to minimize expensive destructive die-strength evaluations. On bare silicon and product wafers, our method autonomously delivers feasible configurations that match or exceed expert baselines in production speed, die strength, and structural integrity, using only technician-level operation. Post-hoc validation of different weight configurations of the utility functions reveals that multiple feasible solutions with qualitatively different trade-offs can be obtained from the final surrogate model. Expert-refinement of the discovered process can further improve production speed while preserving die strength and structural integrity, surpassing purely manual or automated methods.

</details>


### [118] [Adapting Neural Audio Codecs to EEG](https://arxiv.org/abs/2511.23142)
*Ard Kastrati,Luca Lanzendörfer,Riccardo Rigoni,John Staib Matilla,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 预训练的神经音频编解码器（如DAC）经过适当预处理后可直接用于EEG压缩，无需修改即可获得稳定重建，微调后效果更佳。论文还提出了多通道扩展DAC-MC来捕捉电极间空间依赖关系。


<details>
  <summary>Details</summary>
Motivation: EEG和音频在采样率、通道结构和尺度上存在本质差异，但探索是否可以利用预训练的神经音频编解码器作为EEG压缩的有效起点，从而避免从头训练并利用音频领域的预训练优势。

Method: 1. 使用最先进的神经音频编解码器DAC作为基础模型；2. 将原始EEG数据映射到编解码器的基于步长的帧结构中；3. 直接重用音频预训练的编码器-解码器；4. 提出DAC-MC多通道扩展，包含基于注意力的跨通道聚合和通道特定解码；5. 系统探索压缩质量权衡：残差码本深度、码本大小和输入采样率。

Result: 1. 即使不修改，该设置也能产生稳定的EEG重建；2. 在EEG数据上微调进一步提高了保真度和泛化能力，优于从头训练；3. DAC-MC能够有效捕捉跨电极的空间依赖关系；4. 在TUH异常和癫痫数据集上的评估表明，适应后的编解码器保留了临床相关信息，体现在基于频谱图的重建损失和下游分类准确性上。

Conclusion: 预训练的神经音频编解码器可以作为EEG压缩的有效起点，通过适当的数据预处理和架构适应（如多通道扩展），能够实现高质量的EEG重建并保留临床相关信息，为跨模态迁移学习提供了有前景的方向。

Abstract: EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.

</details>


### [119] [A Theoretical Framework for Discovering Groups and Unitary Representations via Tensor Factorization](https://arxiv.org/abs/2511.23152)
*Dongsung Huh,Halyun Jeong*

Main category: cs.LG

TL;DR: HyperCube模型通过算子值张量分解发现群结构及其酉表示，理论分析揭示了其倾向于学习群结构的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 理解HyperCube模型为何能够自动发现群结构及其酉表示，从理论上解释这种归纳偏置的来源。

Method: 将目标函数分解为尺度调节项(𝒷)和方向对齐项(ℛ≥0)，分离出共线流形(ℛ=0)，证明该流形仅对群同位素存在可行解，且𝒷项施加变分压力趋向酉性。

Result: 在共线主导猜想下证明：(1)全局最小值由群的酉正则表示实现；(2)非群操作具有严格更高的目标值，量化了模型对群结合结构的归纳偏置。

Conclusion: HyperCube模型具有发现群结构的固有倾向，其理论框架为理解算子值张量分解的归纳偏置提供了严格基础，揭示了模型对群同位素结构的偏好。

Abstract: We analyze the HyperCube model, an \textit{operator-valued} tensor factorization architecture that discovers group structures and their unitary representations. We provide a rigorous theoretical explanation for this inductive bias by decomposing its objective into a term regulating factor scales ($\mathcal{B}$) and a term enforcing directional alignment ($\mathcal{R} \geq 0$). This decomposition isolates the \textit{collinear manifold} ($\mathcal{R}=0$), to which numerical optimization consistently converges for group isotopes. We prove that this manifold admits feasible solutions exclusively for group isotopes, and that within it, $\mathcal{B}$ exerts a variational pressure toward unitarity. To bridge the gap to the global landscape, we formulate a \textit{Collinearity Dominance Conjecture}, supported by empirical observations. Conditional on this dominance, we prove two key results: (1) the global minimum is achieved by the unitary regular representation for groups, and (2) non-group operations incur a strictly higher objective value, formally quantifying the model's inductive bias toward the associative structure of groups (up to isotopy).

</details>


### [120] [Estimating the Event-Related Potential from Few EEG Trials](https://arxiv.org/abs/2511.23162)
*Anders Vestergaard Nørskov,Kasper Jørgensen,Alexander Neergaard Zahid,Morten Mørup*

Main category: cs.LG

TL;DR: EEG2ERP：一种新颖的不确定性感知自编码器方法，可将任意数量的EEG试次映射到相关的ERP，显著减少ERP研究所需的试次数


<details>
  <summary>Details</summary>
Motivation: 传统ERP估计需要大量EEG试次的平均来降低噪声和信号变异性，这限制了ERP研究的效率和应用。需要开发能够减少必要试次数量的方法。

Method: 提出EEG2ERP，一种不确定性感知自编码器方法，使用自举训练目标和单独的方差解码器来建模ERP估计的不确定性，支持零样本泛化到新受试者。

Result: 在三个公开数据集（ERP CORE、P300 Speller BCI、面部感知神经影像）上评估，在少量试次情况下，EEG2ERP比传统和稳健平均方法提供显著更好的ERP估计。

Conclusion: EEG2ERP是首个将EEG信号映射到相关ERP的深度学习方法，朝着减少ERP研究所需试次数的方向迈出了重要一步。

Abstract: Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP

</details>


### [121] [Energy-Efficient Vision Transformer Inference for Edge-AI Deployment](https://arxiv.org/abs/2511.23166)
*Nursultan Amanzhol,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 提出两阶段评估流程，结合设备无关模型选择与设备相关测量，评估ViT模型在边缘设备上的能效表现


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformers在能耗受限设备上的部署增加，需要超越准确率的评估方法来全面评估模型能效

Method: 两阶段评估流程：第一阶段使用NetScore指标进行设备无关模型筛选；第二阶段使用可持续准确率指标(SAM)结合设备测量进行模型排名

Result: 在13个ViT模型上测试发现：混合模型LeViT_Conv_192在TX2上比ViT基准节能53%；蒸馏模型TinyViT-11M_Distilled在RTX 3050上表现优异

Conclusion: 提出的两阶段评估方法能有效识别不同设备上的高效ViT模型，为实际部署提供能效优化指导

Abstract: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

</details>


### [122] [SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data](https://arxiv.org/abs/2511.23238)
*Yuting Fang,Qouc Le Gia,Flora Salim*

Main category: cs.LG

TL;DR: SDE-Attention：一种用于不规则采样时间序列的SDE-RNN模型，通过通道级注意力机制（包括通道重校准、时变特征注意力和金字塔多尺度自注意力）处理高缺失率数据，在单变量和多变量基准测试中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 医疗和传感器网络中常见具有大量缺失观测的不规则采样时间序列，现有方法在处理高缺失率数据时效果有限，需要开发能够有效处理这种数据特征的模型。

Method: 提出SDE-Attention系列模型，在SDE-RNN的潜在预RNN状态上引入通道级注意力机制，包括三种注意力类型：通道重校准、时变特征注意力和金字塔多尺度自注意力。

Result: 在合成周期数据集和真实世界基准测试中，潜在空间注意力机制始终优于普通SDE-RNN。在单变量UCR数据集上，SDE-TVF-L模型（基于LSTM的时变特征模型）表现最佳，在30%、60%和90%缺失率下平均准确率分别提高约4%、6%和10%。在多变量UEA基准测试中，注意力增强模型同样优于基线，SDE-TVF-L在高缺失率下平均准确率最高提升7%。

Conclusion: 时变特征注意力在单变量数据集上最稳健，而在多变量数据集上，不同注意力类型在不同任务中表现优异，表明SDE-Attention可以根据问题结构灵活调整。注意力机制显著提升了模型处理高缺失率时间序列数据的能力。

Abstract: Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.

</details>


### [123] [Towards Understanding Transformers in Learning Random Walks](https://arxiv.org/abs/2511.23239)
*Wei Shi,Yuan Cao*

Main category: cs.LG

TL;DR: 本文从理论上分析单层Transformer在随机游走任务上的学习能力和可解释性，证明梯度下降训练后能达到最优精度，并揭示注意力机制作为token选择器、值矩阵执行概率转移的机制。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在实际应用中表现出色，但缺乏理论理解和可解释性。本文旨在通过研究Transformer学习经典统计模型（圆上随机游走）的能力，从理论上理解其成功机制。

Method: 采用理论分析方法，研究单层Transformer模型在圆上随机游走预测任务上的表现。通过梯度下降训练，分析模型收敛后的结构和功能，特别关注注意力机制和值矩阵的作用。

Result: 理论证明：梯度下降训练后，单层Transformer能达到随机游走预测的最优精度。分析显示训练后的模型具有可解释性：softmax注意力作为token选择器聚焦于直接父状态，值矩阵基于该父状态执行一步概率转移来预测下一个状态位置。

Conclusion: 本文为Transformer的成功提供了理论解释，揭示了其在简单统计任务中的工作机制。同时发现小初始化梯度下降在某些简单任务中可能失败或难以收敛，表明理论条件是紧的。实验支持了理论发现。

Abstract: Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.

</details>


### [124] [Heteroscedastic Neural Networks for Path Loss Prediction with Link-Specific Uncertainty](https://arxiv.org/abs/2511.23243)
*Jonathan Ethier*

Main category: cs.LG

TL;DR: 提出一种神经网络模型，联合预测路径损耗的均值和链路特定方差，通过最小化高斯负对数似然实现异方差不确定性估计，在RF测试数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统和现代基于机器学习的路径损耗模型通常假设恒定的预测方差，这限制了不确定性估计的准确性。需要能够提供链路特定不确定性估计的模型，以支持更精确的RF规划和干扰分析。

Method: 提出一种神经网络架构，通过最小化高斯负对数似然联合预测路径损耗的均值和链路特定方差。比较了共享参数、部分共享参数和独立参数三种架构，使用准确性、校准度和锐度等指标在大型公共RF驾驶测试数据集上进行评估。

Result: 共享参数架构表现最佳，达到7.4 dB的RMSE，95%预测区间的覆盖率为95.1%，平均区间宽度为29.6 dB。这些不确定性估计支持链路特定覆盖裕度，改进RF规划和干扰分析，并提供有效的模型弱点自诊断。

Conclusion: 提出的异方差不确定性估计方法显著优于传统恒定方差假设，为无线通信系统设计提供了更可靠的不确定性量化工具，支持更精确的网络规划和性能分析。

Abstract: Traditional and modern machine learning-based path loss models typically assume a constant prediction variance. We propose a neural network that jointly predicts the mean and link-specific variance by minimizing a Gaussian negative log-likelihood, enabling heteroscedastic uncertainty estimates. We compare shared, partially shared, and independent-parameter architectures using accuracy, calibration, and sharpness metrics on blind test sets from large public RF drive-test datasets. The shared-parameter architecture performs best, achieving an RMSE of 7.4 dB, 95.1 percent coverage for 95 percent prediction intervals, and a mean interval width of 29.6 dB. These uncertainty estimates further support link-specific coverage margins, improve RF planning and interference analyses, and provide effective self-diagnostics of model weaknesses.

</details>


### [125] [Time Series Forecasting via Direct Per-Step Probability Distribution Modeling](https://arxiv.org/abs/2511.23260)
*Linghao Kong,Xiaopeng Hong*

Main category: cs.LG

TL;DR: 提出interPDN模型，通过双分支架构直接构建离散概率分布而非标量值，解决时间序列预测中的不确定性量化问题


<details>
  <summary>Details</summary>
Motivation: 深度神经网络时间序列预测模型难以量化预测不确定性，因为它们直接输出标量值，无法捕捉预测的置信度

Method: interPDN模型：1) 直接构建每步的离散概率分布而非标量；2) 使用预定义支持集计算期望作为回归输出；3) 引入双分支架构，支持集交错排列；4) 添加粗时间尺度分支进行长期趋势预测；5) 使用另一分支输出作为辅助信号，对当前分支预测施加自监督一致性约束

Result: 在多个真实世界数据集上的广泛实验表明interPDN具有优越性能

Conclusion: interPDN通过直接建模概率分布和双分支自监督架构，有效解决了时间序列预测中的不确定性量化问题

Abstract: Deep neural network-based time series prediction models have recently demonstrated superior capabilities in capturing complex temporal dependencies. However, it is challenging for these models to account for uncertainty associated with their predictions, because they directly output scalar values at each time step. To address such a challenge, we propose a novel model named interleaved dual-branch Probability Distribution Network (interPDN), which directly constructs discrete probability distributions per step instead of a scalar. The regression output at each time step is derived by computing the expectation of the predictive distribution on a predefined support set. To mitigate prediction anomalies, a dual-branch architecture is introduced with interleaved support sets, augmented by coarse temporal-scale branches for long-term trend forecasting. Outputs from another branch are treated as auxiliary signals to impose self-supervised consistency constraints on the current branch's prediction. Extensive experiments on multiple real-world datasets demonstrate the superior performance of interPDN.

</details>


### [126] [An Improved and Generalised Analysis for Spectral Clustering](https://arxiv.org/abs/2511.23261)
*George Tyler,Luca Zanetti*

Main category: cs.LG

TL;DR: 本文重新审视谱聚类的理论性能，证明只要最小特征值分组良好且与谱的其余部分分离，谱聚类就能有效工作，这适用于多尺度层次聚类和有向图等传统分析未覆盖的场景。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类分析未能充分解释其在多尺度层次聚类和有向图等场景中的有效性。本文旨在提供更通用的理论框架，解释谱聚类在更广泛条件下的良好性能，特别是在特征值分组分离的情况下。

Method: 采用理论分析方法，研究谱聚类算法在特征值分组分离条件下的性能。将分析扩展到传统图拉普拉斯矩阵之外，包括有向图的厄米特表示。通过特征值分离条件建立谱聚类的理论保证。

Result: 证明谱聚类在最小特征值分组良好且与谱的其余部分分离时表现良好，这适用于多尺度层次聚类。对有向图的分析表明，谱聚类能够恢复边缘方向基本一致的簇划分。在合成和真实数据集上的实验验证了理论预测的准确性。

Conclusion: 谱聚类的有效性依赖于特征值的分组分离特性，这一理论框架比传统分析更通用，能够解释谱聚类在多尺度层次聚类和有向图等场景中的良好性能，为生态网络分析等应用提供了理论基础。

Abstract: We revisit the theoretical performances of Spectral Clustering, a classical algorithm for graph partitioning that relies on the eigenvectors of a matrix representation of the graph. Informally, we show that Spectral Clustering works well as long as the smallest eigenvalues appear in groups well separated from the rest of the matrix representation's spectrum. This arises, for example, whenever there exists a hierarchy of clusters at different scales, a regime not captured by previous analyses. Our results are very general and can be applied beyond the traditional graph Laplacian. In particular, we study Hermitian representations of digraphs and show Spectral Clustering can recover partitions where edges between clusters are oriented mostly in the same direction. This has applications in, for example, the analysis of trophic levels in ecological networks. We demonstrate that our results accurately predict the performances of Spectral Clustering on synthetic and real-world data sets.

</details>


### [127] [BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning](https://arxiv.org/abs/2511.23264)
*Ariful Islam,Md Rifat Hossen,Tanvir Mahmud*

Main category: cs.LG

TL;DR: BanglaSentNet：一个可解释的混合深度学习框架，集成LSTM、BiLSTM、GRU和BanglaBERT，用于孟加拉语电商评论的多方面情感分析，在准确率、可解释性和跨域泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语多方面情感分析面临标注数据有限、形态复杂性、代码混合现象和领域转移问题，现有方法缺乏可解释性和跨域泛化能力，影响3亿孟加拉语用户的实际应用。

Method: 提出BanglaSentNet框架，集成LSTM、BiLSTM、GRU和BanglaBERT，采用动态加权集成学习；引入包含8,755条手动标注的孟加拉语产品评论数据集；结合SHAP特征归因和注意力可视化实现可解释性。

Result: 达到85%准确率和0.88 F1分数，优于独立深度学习模型3-7%；可解释性套件获得9.4/10可解释性评分，人类一致性达87.6%；跨域迁移学习在零样本下保持67-76%效果，少样本学习仅需500-1000样本即可达到90-95%微调性能。

Conclusion: 为孟加拉语情感分析建立了新的SOTA基准，推进了低资源语言的集成学习方法，为商业应用提供了可行的解决方案，具有实际部署价值。

Abstract: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.

</details>


### [128] [Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting](https://arxiv.org/abs/2511.23276)
*Joongwon Chae,Runming Wang,Chen Xiong,Gong Yunhan,Lian Zhang,Ji Jiansong,Dongmei Yu,Peiwu Qin*

Main category: cs.LG

TL;DR: 提出一个两智能体框架，将上下文解释与概率预测解耦，用于手足口病预测，结合LLM处理异构信号和神经符号核心进行校准预测


<details>
  <summary>Details</summary>
Motivation: 传统模型和基础模型虽然能纳入协变量，但缺乏语义推理能力来解释冲突驱动因素之间的因果相互作用，无法有效整合领域知识和上下文信息

Method: 采用两智能体框架：1) LLM"事件解释器"处理学校日程、气象摘要和报告等异构信号，生成标量传播影响信号；2) 神经符号核心将此信号与历史病例数结合，产生校准的概率预测

Result: 在香港(2023-2024)和丽水(2024)的真实HFMD数据集上评估，相比传统和基础模型基线，在保持竞争性点预测准确性的同时，提供稳健的90%预测区间(覆盖度0.85-1.00)和人类可解释的推理

Conclusion: 通过LLM结构化整合领域知识可以达到最先进性能，同时产生符合公共卫生工作流程的上下文感知预测，为疾病监测提供更可靠的决策支持

Abstract: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .

</details>


### [129] [Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla](https://arxiv.org/abs/2511.23287)
*Ariful Islam,Tanvir Mahmud,Md Rifat Hossen*

Main category: cs.LG

TL;DR: 提出BangACMM框架，通过中间融合策略结合文本和视觉特征，在孟加拉语社交媒体作者意图分类任务上取得84.11%的宏F1分数，比先前方法提升8.4个百分点。


<details>
  <summary>Details</summary>
Motivation: 社交媒体用户生成内容爆炸式增长，作者意图理解对内容解读至关重要。先前单模态方法存在局限性，需要利用多模态信息提升孟加拉语社交媒体意图分类性能。

Method: 使用Uddessho数据集（3,048个帖子，6个意图类别），系统评估基于Transformer的语言模型（mBERT、DistilBERT、XLM-RoBERTa）和视觉架构（ViT、Swin、SwiftFormer、ResNet、DenseNet、MobileNet）。提出新颖的中间融合策略，优于早期和晚期融合方法。

Result: 中间融合策略（特别是mBERT和Swin Transformer组合）达到84.11%的宏F1分数，比先前孟加拉语多模态方法提升8.4个百分点，建立了新的最先进水平。视觉上下文显著增强了意图分类性能。

Conclusion: 中间层跨模态特征集成在模态特定表示和跨模态学习之间提供了最佳平衡。该研究为孟加拉语和其他低资源语言建立了新的基准和方法标准，提出的框架称为BangACMM。

Abstract: The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).

</details>


### [130] [Machine Learning for Scientific Visualization: Ensemble Data Analysis](https://arxiv.org/abs/2511.23290)
*Hamid Gadirov*

Main category: cs.LG

TL;DR: 该论文提出深度学习方法来改进时空科学集合数据的分析和可视化，包括基于自动编码器的降维、FLINT模型用于流场估计与时间插值，以及HyperFLINT超网络方法实现参数感知的自适应处理。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和实验测量产生大量时空数据，但高维度、复杂结构和信息缺失使得提取有意义的洞察具有挑战性。传统分析方法难以处理这些问题，需要更鲁棒的数据驱动方法。

Method: 1. 基于自动编码器的降维方法，评估部分标注下的投影稳定性，采用帕累托最优选择策略识别最佳自动编码器变体；2. FLINT深度学习模型，在流场监督和无监督设置下进行高质量流场估计和时间插值；3. HyperFLINT超网络方法，基于模拟参数进行条件化，实现参数感知的自适应流场估计和标量数据插值。

Result: 开发了可扩展、自适应的高质量解决方案，能够在2D+时间和3D+时间集合中重建缺失速度场并生成高保真时间插值，无需领域特定假设或大量微调，在稀疏或不完整数据下也能获得更准确的重建。

Conclusion: 该论文推进了科学可视化的深度学习技术，为解释复杂时空集合数据提供了可扩展、自适应和高质量的解决方案，显著改进了科学数据的分析和可视化能力。

Abstract: Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.

</details>


### [131] [Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems](https://arxiv.org/abs/2511.23307)
*Enzo Nicolás Spotorno,Josafat Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 提出了一个物理信息学习框架，用于处理具有未知动力学和代数不变量的微分方程约束的复杂信息物理系统，包含HRPINN和PHRPINN两种架构。


<details>
  <summary>Details</summary>
Motivation: 解决复杂信息物理系统中同时存在未知动力学和代数不变量的微分方程学习问题，传统方法难以同时保证物理一致性和数据效率。

Method: 1. 提出HRPINN架构，将已知物理作为硬结构约束嵌入循环积分器中学习残差动力学；2. 提出PHRPINN扩展，通过预测-投影机制严格强制执行代数不变量。

Result: 在真实世界电池预测DAE上验证HRPINN，在标准约束基准上评估PHRPINN，展示了高精度和数据效率，同时揭示了物理一致性、计算成本和数值稳定性之间的权衡。

Conclusion: 该框架为复杂信息物理系统的物理信息学习提供了有效解决方案，通过理论分析和实验验证展示了其潜力，并为实际部署提供了实用指导。

Abstract: This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.

</details>


### [132] [Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.23315)
*Azusa Yamaguchi*

Main category: cs.LG

TL;DR: 论文通过大规模实验发现去中心化多智能体强化学习存在三种相态：协调稳定相、脆弱过渡区和阻塞无序相，由双不稳定性脊分隔，核漂移是主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 为了更好理解去中心化多智能体强化学习中协调何时出现、波动或崩溃的动态特性，需要更清晰地描述多智能体学习系统的相结构。

Method: 以完全独立Q学习为最小测试平台，在环境大小L和智能体密度ρ参数空间进行大规模实验，构建基于合作成功率(CSR)和TD误差方差稳定性指数的相图。

Result: 发现三种相态：协调稳定相、脆弱过渡区、阻塞无序相，由双不稳定性脊分隔；核漂移是主要驱动因素；同步分析显示时间对齐是持续合作的必要条件；移除智能体标识符会消除漂移并破坏三相结构。

Conclusion: 去中心化MARL展现出由规模、密度和核漂移相互作用控制的相结构，表明涌现的协调行为是一种分布交互驱动的相现象，智能体间微小不对称性是漂移的必要驱动因素。

Abstract: A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.

</details>


### [133] [ParaGate: Parasitic-Driven Domain Adaptation Transfer Learning for Netlist Performance Prediction](https://arxiv.org/abs/2511.23340)
*Bin Sun,Jingyi Zhou,Jianan Mu,Zhiteng Chao,Tianmeng Yang,Ziyue Xu,Jing Ye,Huawei Li*

Main category: cs.LG

TL;DR: ParaGate是一个三步跨阶段预测框架，直接从网表推断布局级时序和功耗，解决了传统EDA流程中布局级性能指标只能在布局布线后获取的问题。


<details>
  <summary>Details</summary>
Motivation: 传统EDA流程中，布局级性能指标只能在布局布线后获取，阻碍了早期阶段的全局优化。现有的神经网络解决方案由于商业布局布线工具的黑盒启发式方法导致数据差异大，面临泛化挑战。

Method: ParaGate采用三步框架：1）两阶段迁移学习方法预测寄生参数，先在中等规模电路上预训练，再在更大电路上微调以捕捉极端条件；2）依赖EDA工具进行时序分析，卸载长路径数值推理；3）使用子图特征进行全局校准。

Result: ParaGate在openE906上实现了强泛化能力，到达时间R2从0.119提升到0.897，仅需少量微调数据。这证明ParaGate可以为综合和布局阶段的全局优化提供指导。

Conclusion: ParaGate通过创新的跨阶段预测框架，成功解决了从网表直接预测布局级性能的泛化问题，为EDA流程的早期全局优化提供了有效指导。

Abstract: In traditional EDA flows, layout-level performance metrics are only obtainable after placement and routing, hindering global optimization at earlier stages. Although some neural-network-based solutions predict layout-level performance directly from netlists, they often face generalization challenges due to the black-box heuristics of commercial placement-and-routing tools, which create disparate data across designs. To this end, we propose ParaGate, a three-step cross-stage prediction framework that infers layout-level timing and power from netlists. First, we propose a two-phase transfer-learning approach to predict parasitic parameters, pre-training on mid-scale circuits and fine-tuning on larger ones to capture extreme conditions. Next, we rely on EDA tools for timing analysis, offloading the long-path numerical reasoning. Finally, ParaGate performs global calibration using subgraph features. Experiments show that ParaGate achieves strong generalization with minimal fine-tuning data: on openE906, its arrival-time R2 from 0.119 to 0.897. These results demonstrate that ParaGate could provide guidance for global optimization in the synthesis and placement stages.

</details>


### [134] [Distributed Dynamic Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2511.23347)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出分布式动态关联记忆(DDAM)框架，将经典关联记忆扩展到多智能体和时变数据流场景，并设计了基于树的分布式在线梯度下降算法DDAM-TOGD，在静态和动态环境中都提供了理论性能保证。


<details>
  <summary>Details</summary>
Motivation: 关联记忆是现代神经网络架构（如Transformer）的关键机制，但现有方法主要针对单智能体和静态数据。需要将其扩展到分布式动态环境中，以支持多智能体协作和时变数据流处理。

Method: 提出DDAM框架，每个智能体维护本地关联记忆，通过兴趣矩阵选择性记忆其他智能体的信息。设计了DDAM-TOGD算法，基于树结构的分布式在线梯度下降，支持实时记忆更新和智能体间通信。

Result: 理论分析证明DDAM-TOGD在静态环境中具有次线性静态遗憾，在非静态环境中具有路径长度依赖的动态遗憾界。数值实验显示该方法在准确性和鲁棒性上优于共识分布式优化等基线方法。

Conclusion: DDAM框架成功将关联记忆扩展到分布式动态环境，DDAM-TOGD算法提供了理论保证和实际性能优势，通过优化路由树设计可进一步减少通信延迟，为多智能体协作学习提供了有效解决方案。

Abstract: An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings with multiple agents and time-varying data streams. In DDAM, each agent maintains a local AM that must not only store its own associations but also selectively memorize information from other agents based on a specified interest matrix. To address this problem, we propose a novel tree-based distributed online gradient descent algorithm, termed DDAM-TOGD, which enables each agent to update its memory on the fly via inter-agent communication over designated routing trees. We derive rigorous performance guarantees for DDAM-TOGD, proving sublinear static regret in stationary environments and a path-length dependent dynamic regret bound in non-stationary environments. These theoretical results provide insights into how communication delays and network structure impact performance. Building on the regret analysis, we further introduce a combinatorial tree design strategy that optimizes the routing trees to minimize communication delays, thereby improving regret bounds. Numerical experiments demonstrate that the proposed DDAM-TOGD framework achieves superior accuracy and robustness compared to representative online learning baselines such as consensus-based distributed optimization, confirming the benefits of the proposed approach in dynamic, distributed environments.

</details>


### [135] [Learning-Augmented Online Bipartite Matching in the Random Arrival Order Model](https://arxiv.org/abs/2511.23388)
*Kunanon Burathep,Thomas Erlebach,William K. Moses*

Main category: cs.LG

TL;DR: 本文研究了随机到达顺序模型下的在线无权二分图匹配问题，在增强学习设置中利用不可信的在线顶点类型预测，改进了先前工作对最优匹配大小的限制，提出了具有(1-o(1))一致性和(β-o(1))鲁棒性的算法。


<details>
  <summary>Details</summary>
Motivation: 先前工作（Choo等人，ICML 2024）在增强学习设置中研究了在线二分图匹配问题，但他们的分析仅限于最优匹配大小为n的情况（即每个在线顶点都能匹配）。本文旨在消除这一限制，使算法适用于更一般的场景。

Method: 使用到达序列的前缀作为样本来判断预测是否接近真实到达序列，然后根据判断结果选择跟随预测或使用忽略预测的基线算法。与先前工作不同，本文只要求预测匹配的大小至少为αn（0<α≤1），而不要求最优匹配大小为n。

Result: 提出的学习增强算法实现了(1-o(1))的一致性和(β-o(1))的鲁棒性。此外，研究还表明竞争比随着预测误差的增加在一致性和鲁棒性之间平滑退化。

Conclusion: 本文成功扩展了先前关于学习增强在线二分图匹配的工作，消除了对最优匹配大小的限制，同时保持了良好的理论保证。算法在预测准确时接近最优，在预测不准确时仍能保持基线算法的竞争比。

Abstract: We study the online unweighted bipartite matching problem in the random arrival order model, with $n$ offline and $n$ online vertices, in the learning-augmented setting: The algorithm is provided with untrusted predictions of the types (neighborhoods) of the online vertices. We build upon the work of Choo et al. (ICML 2024, pp. 8762-8781) who proposed an approach that uses a prefix of the arrival sequence as a sample to determine whether the predictions are close to the true arrival sequence and then either follows the predictions or uses a known baseline algorithm that ignores the predictions and is $β$-competitive. Their analysis is limited to the case that the optimal matching has size $n$, i.e., every online vertex can be matched. We generalize their approach and analysis by removing any assumptions on the size of the optimal matching while only requiring that the size of the predicted matching is at least $αn$ for any constant $0 < α\le 1$. Our learning-augmented algorithm achieves $(1-o(1))$-consistency and $(β-o(1))$-robustness. Additionally, we show that the competitive ratio degrades smoothly between consistency and robustness with increasing prediction error.

</details>


### [136] [Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning](https://arxiv.org/abs/2511.23402)
*Jiajun Guo,Xin Luo,Jie Liu*

Main category: cs.LG

TL;DR: 提出一种结合学习型数据压缩的多模态模型结构，将模型嵌入压缩为低比特整数，基于熵编码理论确定最优离散表示级别，大幅降低分布式学习中的传输成本


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能解决数据隐私问题，但高网络通信成本（特别是大型基础模型需要传输大量高维数据）一直是主要障碍

Method: 提出新的多模态模型结构，结合学习型数据压缩方法，将模型嵌入压缩为低比特整数，同时基于熵编码理论确定最优离散表示级别

Result: 在保持模型性能的同时，显著降低了分割学习中分区间的传输成本

Conclusion: 该方法有效解决了分割学习中的通信瓶颈问题，为分布式隐私保护学习提供了高效的解决方案

Abstract: Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation models that require transmitting large amounts of high-dimensional data. To resolve this issue, we present a new multimodal model structure that incorporates a learning-based data compression method, which compresses model embeddings into low-bit integers while preserving the model's performance, greatly reducing the transmission costs between partitions. We then determine the optimal number of discrete representation levels based on a solid theoretical foundation from entropy coding.

</details>


### [137] [LFM2 Technical Report](https://arxiv.org/abs/2511.23404)
*Alexander Amini,Anna Banaszak,Harold Benoit,Arthur Böök,Tarek Dakhran,Song Duong,Alfred Eng,Fernando Fernandes,Marc Härkönen,Anne Harrington,Ramin Hasani,Saniya Karwa,Yuri Khrustalev,Maxime Labonne,Mathias Lechner,Valentine Lechner,Simon Lee,Zetian Li,Noel Loo,Jacob Marks,Edoardo Mosca,Samuel J. Paech,Paul Pak,Rom N. Parnichkun,Alex Quach,Ryan Rogers,Daniela Rus,Nayan Saxena,Bettina Schlager,Tim Seyde,Jimmy T. H. Smith,Aditya Tadimeti,Neehal Tumma*

Main category: cs.LG

TL;DR: LFM2是一个面向边缘设备部署的液体基础模型家族，通过硬件感知架构搜索实现高效推理，支持多模态和检索任务，提供350M-8.3B参数版本，具有32K上下文长度和开源部署方案。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上部署大型语言模型时的效率问题，包括内存限制、推理延迟和计算资源约束，同时保持强大的任务能力，为实际边缘应用提供实用的基础模型。

Method: 采用硬件在环架构搜索设计混合骨干网络（门控短卷积+分组查询注意力）；使用温和解耦Top-K知识蒸馏避免支持不匹配；课程学习按难度排序数据；三阶段后训练（监督微调、长度归一化偏好优化、模型合并）；开发多模态变体（视觉、音频、检索）。

Result: LFM2模型在10-12T token预训练后表现优异：LFM2-2.6B在IFEval达到79.56%，GSM8K达到82.41%；CPU上预填充和解码速度比同类模型快2倍；多模态变体在视觉、音频、检索任务上表现竞争力，音频模型性能可与3倍大模型媲美。

Conclusion: LFM2系列为边缘应用提供了高效、实用的基础模型解决方案，通过硬件感知设计、创新训练方法和多模态扩展，实现了在资源受限设备上的快速推理和强大任务能力，并开源了完整的部署工具链。

Abstract: We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.

</details>


### [138] [ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts](https://arxiv.org/abs/2511.23442)
*Hang Yu,Di Zhang,Qiwei Du,Yanping Zhao,Hai Zhang,Guang Chen,Eduardo E. Veas,Junqiao Zhao*

Main category: cs.LG

TL;DR: ASTRO是一个用于离线强化学习的轨迹缝合数据增强框架，通过生成分布新颖且符合动态的轨迹来提升策略学习效果


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集的数据集中学习，但包含次优和碎片化轨迹的数据集给奖励传播带来挑战，导致价值估计不准确和策略性能下降。现有的生成模型轨迹缝合方法要么局限于行为策略的支持范围，要么违反底层动态，限制了策略改进效果。

Method: ASTRO首先学习时间距离表示来识别不同且可达的缝合目标，然后采用动态引导的缝合规划器，通过Rollout Deviation Feedback（目标状态序列与实际到达状态序列之间的差距）自适应生成连接动作序列，提高轨迹缝合的可行性和可达性。

Result: ASTRO在OGBench套件上取得了显著的性能提升，在D4RL等标准离线RL基准测试中也表现出一致的改进，优于先前的离线RL增强方法。

Conclusion: ASTRO通过生成分布新颖且动态一致的轨迹，有效解决了离线强化学习中轨迹缝合的挑战，显著提升了策略学习性能。

Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.

</details>


### [139] [Provable Benefits of Sinusoidal Activation for Modular Addition](https://arxiv.org/abs/2511.23443)
*Tianlong Huang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文研究激活函数在学习模加法中的作用，发现正弦激活的MLP仅需宽度2即可精确实现模加法，而ReLU网络需要宽度随模数线性增长，且正弦网络具有更好的泛化能力和长度外推性。


<details>
  <summary>Details</summary>
Motivation: 研究不同激活函数（特别是正弦和ReLU）在模加法学习任务中的表达能力差异，以及它们对泛化性能的影响，旨在理解激活函数在神经网络学习结构性问题中的作用。

Method: 首先理论分析正弦和ReLU网络在模加法任务中的表达能力，建立宽度需求的理论界限；然后为正弦网络提出新颖的Natarajan维度泛化界，推导过参数化情况下的宽度无关边际泛化理论；最后通过实验验证理论结果。

Result: 正弦MLP仅需宽度2即可精确实现任意固定长度m的模加法（带偏置时甚至能统一处理所有长度），而ReLU网络宽度需随m线性增长且不能同时拟合不同模余数的长度。正弦网络具有近乎最优的样本复杂度Õ(p)，在过参数化情况下表现出宽度无关的泛化能力，实验显示正弦网络在各种情况下都比ReLU网络泛化更好，且具有更强的长度外推能力。

Conclusion: 正弦激活函数在模加法学习中具有显著优势：表达能力更强（宽度需求更低），泛化性能更好（样本复杂度更优），长度外推能力更强。这为理解激活函数在神经网络学习结构性问题中的作用提供了重要见解。

Abstract: This paper studies the role of activation functions in learning modular addition with two-layer neural networks. We first establish a sharp expressivity gap: sine MLPs admit width-$2$ exact realizations for any fixed length $m$ and, with bias, width-$2$ exact realizations uniformly over all lengths. In contrast, the width of ReLU networks must scale linearly with $m$ to interpolate, and they cannot simultaneously fit two lengths with different residues modulo $p$. We then provide a novel Natarajan-dimension generalization bound for sine networks, yielding nearly optimal sample complexity $\widetilde{\mathcal{O}}(p)$ for ERM over constant-width sine networks. We also derive width-independent, margin-based generalization for sine networks in the overparametrized regime and validate it. Empirically, sine networks generalize consistently better than ReLU networks across regimes and exhibit strong length extrapolation.

</details>


### [140] [Physics-Informed Neural Networks for Thermophysical Property Retrieval](https://arxiv.org/abs/2511.23449)
*Ali Waseem,Malcolm Mielle*

Main category: cs.LG

TL;DR: 提出基于物理信息神经网络(PINN)的迭代框架，从热成像图估计墙体热导率k，无需侵入式测量或长时间观测


<details>
  <summary>Details</summary>
Motivation: 现有热导率测量方法存在侵入性、观测时间长或对环境条件敏感的问题，需要开发非侵入式、快速且鲁棒的现场测量方法

Method: 提出PINN迭代框架：交替进行固定k时的正向热问题PINN求解，以及通过比较热成像图与PINN预测表面温度来优化k，直至收敛

Result: 在不同环境条件和数据采集时间下准确预测k，即使违反稳态假设，最大MAE仅为4.0851，展示了现场可靠估计材料特性的潜力

Conclusion: PINN方法为现场材料特性估计提供了可靠途径，无需长时间测量活动，可作为机器学习解决现场逆问题的研究起点

Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.

</details>


### [141] [The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference](https://arxiv.org/abs/2511.23455)
*Hans Gundlach,Jayson Lynch,Matthias Mertens,Neil Thompson*

Main category: cs.LG

TL;DR: 该研究分析了AI模型在基准测试上的成本变化，发现前沿模型在知识、推理、数学和软件工程等任务上的性能成本每年下降5-10倍，其中算法效率提升约3倍/年。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在基准测试上的进步往往依赖于更昂贵的模型，这扭曲了实际能力与成本之间的关系。研究旨在通过价格数据来更准确地衡量AI在实际应用中的进展。

Method: 使用Artificial Analysis和Epoch AI的数据构建了迄今为止最大的当前和历史价格数据集，用于运行基准测试。通过分析价格变化趋势，分离出经济因素、硬件效率改进和算法效率改进的影响。

Result: 研究发现，前沿模型在知识、推理、数学和软件工程基准测试上达到特定性能水平的成本每年下降约5-10倍。控制竞争效应和硬件价格下降后，算法效率进步约为每年3倍。

Conclusion: 评估者应将基准测试的成本作为衡量AI实际影响的重要组成部分，并公开这些成本信息，以更准确地反映AI技术的真实进展。

Abstract: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.

</details>


### [142] [SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments](https://arxiv.org/abs/2511.23465)
*Xinyi Li,Zaishuo Xia,Weyl Lu,Chenjie Hao,Yubei Chen*

Main category: cs.LG

TL;DR: SmallWorld Benchmark：一个用于系统评估世界模型能力的测试平台，通过隔离和精确控制的环境动态来评估模型是否真正捕捉到环境底层规则。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型缺乏统一和受控的系统评估设置，难以判断它们是否真正捕捉到环境动态的底层规则。

Method: 引入SmallWorld Benchmark测试平台，在完全可观测状态空间中对代表性架构（包括循环状态空间模型、Transformer、扩散模型和神经ODE）进行综合实验，涵盖六个不同领域。

Result: 实验结果揭示了这些模型捕捉环境结构的有效性，以及它们在长时间推演中预测性能的退化情况，突出了当前建模范式的优势和局限性。

Conclusion: 该研究为世界模型评估提供了系统框架，揭示了当前模型的局限性，并为表示学习和动态建模的未来改进方向提供了见解。

Abstract: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In this work, we address this open challenge by introducing the SmallWorld Benchmark, a testbed designed to assess world model capability under isolated and precisely controlled dynamics without relying on handcrafted reward signals. Using this benchmark, we conduct comprehensive experiments in the fully observable state space on representative architectures including Recurrent State Space Model, Transformer, Diffusion model, and Neural ODE, examining their behavior across six distinct domains. The experimental results reveal how effectively these models capture environment structure and how their predictions deteriorate over extended rollouts, highlighting both the strengths and limitations of current modeling paradigms and offering insights into future improvement directions in representation learning and dynamics modeling.

</details>


### [143] [ThetaEvolve: Test-time Learning on Open Problems](https://arxiv.org/abs/2511.23473)
*Yiping Wang,Shao-Rong Su,Zhiyuan Zeng,Eva Xu,Liliang Ren,Xinyu Yang,Zeyi Huang,Xuehai He,Luyao Ma,Baolin Peng,Hao Cheng,Pengcheng He,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: ThetaEvolve是一个开源框架，简化并扩展了AlphaEvolve，通过结合上下文学习和强化学习，使小型开源模型能在开放优化问题上取得新的最佳边界。


<details>
  <summary>Details</summary>
Motivation: AlphaEvolve虽然取得了数学发现突破，但它是闭源系统，依赖前沿LLM集合，且是纯推理系统无法内化演化策略。需要开源框架让小型模型也能在开放问题上取得突破。

Method: ThetaEvolve采用单个LLM、大型程序数据库增强探索、批量采样提高吞吐量、懒惰惩罚避免停滞输出、可选奖励塑形提供稳定训练信号，支持测试时的上下文学习和强化学习。

Result: ThetaEvolve首次让小型开源模型（如DeepSeek-R1-0528-Qwen3-8B）在AlphaEvolve提到的开放问题（圆包装和自相关不等式）上取得新的最佳边界。在两个模型和四个开放任务中，测试时强化学习始终优于纯推理基线。

Conclusion: ThetaEvolve是一个有效的开源演化框架，使小型模型能够学习演化能力，在训练任务和未见任务上都表现出更快的进展和更好的最终性能，推动了数学发现的民主化。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [144] [Platinum: Path-Adaptable LUT-Based Accelerator Tailored for Low-Bit Weight Matrix Multiplication](https://arxiv.org/abs/2511.21910)
*Haoxuan Shan,Cong Guo,Chiyue Wei,Feng Cheng,Junyao Zhang,Hai,Li,Yiran Chen*

Main category: cs.AR

TL;DR: Platinum：基于查找表的轻量级ASIC加速器，用于整数权重混合精度矩阵乘法，通过离线生成构造路径降低开销，支持比特串行和三值权重自适应切换，在BitNet b1.58-3B上实现显著加速和能耗降低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速扩展需要更高效的硬件。量化在效率和性能之间提供了有希望的权衡。超低比特量化存在大量结果重用机会，可以通过查找表加速，但现有方法存在查找表构建的计算和硬件开销，且仅依赖比特串行计算，对三值权重网络不够优化。

Method: 提出Platinum轻量级ASIC加速器，采用离线生成的构造路径降低查找表构建开销，通过自适应路径切换同时支持通用比特串行和优化的三值权重执行，实现整数权重混合精度矩阵乘法。

Result: 在BitNet b1.58-3B上，Platinum相比SpikingEyeriss、Prosperity和16线程T-MAC（CPU）分别实现73.6倍、4.09倍和2.15倍加速，能耗降低32.4倍、3.23倍和20.9倍，芯片面积仅0.96mm²。

Conclusion: Platinum展示了基于查找表的ASIC作为边缘平台上超低比特神经网络的高效、可扩展解决方案的潜力，通过创新的路径优化和自适应执行策略显著提升性能并降低能耗。

Abstract: The rapid scaling of large language models demands more efficient hardware. Quantization offers a promising trade-off between efficiency and performance. With ultra-low-bit quantization, there are abundant opportunities for results reuse, and thus it can be boosted with lookup tables (LUTs) based acceleration. However, existing LUT-based methods suffer from computation and hardware overheads for LUT construction, and rely solely on bit-serial computation, which is suboptimal for ternary-weight networks. We propose Platinum, a lightweight ASIC accelerator for integer weight mixed-precision matrix multiplication (mpGEMM) using LUTs. Platinum reduces LUT construction overhead via offline-generated construction paths and supports both general bit-serial and optimized ternary-weight execution through adaptive path switching. On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), respectively, along with energy reductions of 32.4x, 3.23x, and 20.9x, all within a 0.96mm2 chip area. This demonstrates the potential of LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms.

</details>


### [145] [CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing](https://arxiv.org/abs/2511.22166)
*Shuai Dong,Junyi Yang,Ye Ke,Hongyang Shang,Arindam Basu*

Main category: cs.AR

TL;DR: 提出CADC方法，通过在交叉阵列计算中嵌入非线性树突函数来增加部分和的稀疏性，显著减少系统开销并提升能效


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在交叉阵列内存计算架构中加速时，大型卷积层需要跨多个交叉阵列分区，产生大量部分和，导致额外的缓冲、传输和累加开销。需要减少这些系统级开销。

Method: 提出交叉阵列感知的树突卷积（CADC），受神经科学树突计算原理启发，在交叉阵列计算中直接嵌入非线性树突函数（将负值归零），大幅增加部分和的稀疏性。

Result: CADC显著减少部分和数量：LeNet-5减少80%，ResNet-18减少54%，VGG-16减少66%，SNN最多减少88%。稀疏性带来两个关键优势：1）零压缩和零跳过减少29.3%的缓冲和传输开销，减少47.9%的累加开销；2）最小化ADC量化噪声累积，精度下降很小。SRAM-based IMC实现达到2.15 TOPS和40.8 TOPS/W，相比现有IMC加速器实现11x-18x加速和1.9x-22.9x能效提升。

Conclusion: CADC通过引入非线性树突函数有效增加部分和稀疏性，显著降低交叉阵列内存计算架构中的系统开销，在保持高精度的同时大幅提升计算性能和能效，为CNN加速提供了创新解决方案。

Abstract: Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.

</details>


### [146] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas是一个基于MLIR的硬件-软件协同设计框架，通过DMA引擎和HLS优化提升RISC-V ASIP性能，结合基于e-graph的编译器匹配引擎，在点云处理和LLM推理等实际工作负载上实现最高9.27倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有开源RISC-V生态系统中的ASIP框架存在性能受限的问题，主要原因是硬件合成能力有限和编译器支持僵化，无法充分发挥ASIP在各种应用中的专业化潜力。

Method: 提出Aquas框架：1）硬件方面，通过突发DMA引擎增强快速内存访问能力，并采用高级HLS优化；2）编译器方面，提出基于e-graph的可重定向方法，配备新颖的指令匹配引擎，实现高效的指令匹配。

Result: 在实际工作负载评估中，包括点云处理和LLM推理，Aquas框架实现了最高9.27倍的性能加速，显著优于现有框架。

Conclusion: Aquas通过硬件-软件协同设计方法有效解决了RISC-V ASIP框架的性能瓶颈问题，为应用特定处理器设计提供了高效的端到端解决方案。

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [147] [FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators](https://arxiv.org/abs/2511.22348)
*Shuao Jia,Zichao Ling,Chen Bai,Kang Zhao,Jianwang Zhai*

Main category: cs.AR

TL;DR: FADiff是一个基于梯度的优化框架，用于自动寻找DNN在张量加速器上的最优层内映射和层间融合策略，以提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 在张量加速器上高效部署DNN（如LLM）对最大化AI系统计算效率至关重要，但由于层内映射和层间融合交互产生的巨大复杂设计空间，实现这一目标具有挑战性。

Method: 构建统一可微的分析成本模型，准确预测单层映射和各种层融合策略的能耗和延迟；通过将离散约束编码到损失函数中，采用基于梯度的方法高效探索设计空间，确定映射和融合的最优联合策略。

Result: 实验结果表明FADiff优于现有方法，在能耗和延迟方面实现了更好的优化效果。

Conclusion: FADiff框架能够有效解决DNN在张量加速器上的部署优化问题，通过联合优化层内映射和层间融合策略，显著提升推理效率。

Abstract: Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.

</details>


### [148] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 提出3RSeT方案，通过选择性标签比较减少STT-MRAM缓存中的读取干扰错误率，降低71.8%错误率，提升3.6倍MTTF，减少62.1%能耗，性能无损失且面积开销小于0.4%


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为片上缓存最有前景的SRAM替代技术，虽然具有低漏电、高密度、抗辐射和非易失性等优点，但在读取操作中存在的无意位翻转（读取干扰错误）是严重的可靠性挑战。特别是在缓存集合中进行并行比较操作时对所有标签的同时访问，这是读取干扰错误的主要来源，而先前工作未解决此问题。

Method: 提出3RSeT（Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison）方案，通过选择性标签比较来减少标签读取。该方法主动禁用那些没有命中机会的标签，利用每个访问请求中标签的低有效位来预测哪些标签不需要参与比较。

Result: 使用gem5全系统周期精确模拟器评估显示：3RSeT将标签阵列的读取干扰错误率降低71.8%，使平均故障时间（MTTF）提高3.6倍。能耗降低62.1%，性能不受影响，面积开销小于0.4%。

Conclusion: 3RSeT是一种低成本有效的解决方案，能够显著降低STT-MRAM缓存中的读取干扰错误率，同时减少能耗且几乎不影响性能和面积，为STT-MRAM缓存的可靠性问题提供了实用解决方案。

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [149] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: ITA架构将LLM权重编码到ASIC物理电路中，消除内存层次结构，解决边缘设备部署中的"内存墙"问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费边缘设备上部署受到"内存墙"限制——每个生成的token都需要从DRAM获取数十亿模型权重，带来巨大的带宽和能耗成本。当前架构将模型权重视为可变软件数据，为保持通用可编程性付出了巨大能耗代价。

Method: 提出不可变张量架构(ITA)，将模型权重视为物理电路拓扑而非数据。通过将参数直接编码到成熟节点ASIC(28nm/40nm)的金属互连和逻辑中，完全消除内存层次结构。采用"分脑"系统设计：主机CPU管理动态KV缓存操作，ITA ASIC作为无状态的ROM嵌入式数据流引擎。

Result: 该方法从根本上解决了内存带宽瓶颈，显著降低了边缘设备上LLM推理的能耗，使在资源受限设备上部署大型语言模型成为可能。

Conclusion: ITA架构代表了从将模型权重视为软件数据到将其视为物理电路拓扑的范式转变，为边缘AI部署提供了突破性的硬件解决方案，消除了传统架构中的内存层次结构瓶颈。

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [150] [Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation](https://arxiv.org/abs/2511.23011)
*Yanjing Wang,Lizhou Wu,Sunfeng Gao,Yibo Tang,Junhui Luo,Zicong Wang,Yang Ou,Dezun Dong,Nong Xiao,Mingche Lai*

Main category: cs.AR

TL;DR: Cohet：首个基于CXL的缓存一致性异构计算框架，通过解耦计算与内存资源，形成CPU和XPU计算池共享统一内存池，显著提升异构计算性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于PCIe的异构计算系统存在细粒度主机-设备交互效率低下和编程模型复杂的问题。虽然CXL缓存一致性异构计算有潜力从根本上改变CPU和XPU的协作计算模式，但相关研究因CXL平台稀缺、软硬件生态不成熟和应用前景不明而受阻。

Method: 提出Cohet框架，将计算和内存资源解耦，形成无偏见的CPU和XPU计算池，共享单一统一的内存池。通过标准malloc/mmap接口向CPU和XPU计算线程暴露内存，由操作系统智能管理异构资源。同时开发了全系统周期级模拟器SimCXL，能够建模所有CXL子协议和设备类型。

Result: CXL.cache相比DMA传输在缓存行粒度下降低延迟68%，提升带宽14.4倍。基于CXL的NIC相比PCIe-NIC在远程原子操作卸载上实现5.5到40.2倍加速，在RPC序列化/反序列化卸载上平均加速1.86倍。SimCXL模拟器经过真实CXL测试平台校准，平均模拟误差为3%。

Conclusion: Cohet作为首个CXL驱动的缓存一致性异构计算框架，通过统一内存池和智能资源管理，显著提升了异构计算性能，为CXL在异构计算领域的应用提供了重要参考和工具支持。

Abstract: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

</details>


### [151] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: 提出GAV技术结合欠压和位串行计算，实现灵活近似计算，GAVINA架构支持任意混合精度和欠压，能效达89 TOP/sW


<details>
  <summary>Details</summary>
Motivation: 电压过缩放（欠压）作为近似计算技术具有吸引力（功率与电压平方关系），但高错误率阻碍了其广泛应用，且现有欠压加速器依赖8位算术，无法与先进低精度架构竞争

Method: 提出GAV技术，结合欠压和位串行计算，在选定的最低有效位组合上激进降低供电电压；实现GAVINA架构，支持任意混合精度和灵活欠压

Result: GAVINA架构在最激进配置下能效达89 TOP/sW；通过开发GAVINA误差模型，显示GAV可通过欠压提升20%能效，在ResNet-18上精度损失可忽略

Conclusion: GAV技术成功解决了欠压技术的高错误率问题，结合位串行计算实现了灵活近似方法，GAVINA架构在保持精度的同时显著提升了能效

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [152] [Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends](https://arxiv.org/abs/2511.22334)
*Pablo Prieto,Pablo Abad*

Main category: cs.PF

TL;DR: 该研究评估了不同硬件平台（CPU、GPU、NPU）在边缘设备上运行小型语言模型（SLM）的性能和能效，发现专用NPU在性能和能效方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 边缘设备通常受限于处理能力、内存和能耗，不适合运行大型语言模型。小型语言模型（SLM）为资源受限环境提供了轻量级AI推理方案，但需要选择合适的硬件平台来平衡性能与效率。

Method: 使用统一的执行框架和一系列先进的SLM，评估商业CPU（Intel和ARM）、GPU（NVIDIA）和NPU（RaiderChip）的推理性能和能效，包括最大性能、处理效率和能耗效率。

Result: 专用后端（NPU）在性能上远超通用CPU，NPU在性能和能效综合指标（如EDP）上表现最佳。带宽归一化对跨架构公平比较至关重要，低功耗ARM处理器在考虑能耗时也有竞争力。

Conclusion: 针对效率和性能优化的硬件设计在边缘工作负载中具有明显优势，NPU是边缘设备运行SLM的最佳选择，能同时满足高性能和低能耗需求。

Abstract: Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.

</details>


### [153] [What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$](https://arxiv.org/abs/2511.22442)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 该论文分析了Fβ分数在分类模型排名中的有效性，发现Fβ诱导的排名是有意义的，但F1分数并非最优折衷，并提供了寻找最优β值的理论工具。


<details>
  <summary>Details</summary>
Motivation: 分类模型性能评估中，精确率和召回率都是重要的多维指标，但两者往往存在矛盾。Fβ分数作为加权调和平均数被广泛使用，但其是否能产生有意义的排名和良好的折衷效果缺乏理论保证。

Method: 1) 证明Fβ诱导的排名是有意义的，定义了精确率和召回率排名之间的最短路径；2) 将两个分数间的折衷问题表达为基于Kendall秩相关的优化问题；3) 提供理论工具和闭式表达式来寻找任意性能分布下的最优β值。

Result: 研究发现Fβ诱导的排名确实有意义，但F1分数及其偏斜不敏感版本在折衷效果上远非最优。通过六个案例研究展示了如何应用理论工具找到最优β值。

Conclusion: Fβ分数能产生有意义的模型排名，但常用的F1分数并非最优折衷选择。论文提供了系统化的理论框架和工具来寻找精确率和召回率之间的最优平衡点。

Abstract: Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_β$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_β$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_β$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $β$ for any distribution or set of performances, and we illustrate their use on six case studies.

</details>


### [154] [Motion-to-Motion Latency Measurement Framework for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2511.22467)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.PF

TL;DR: 本文提出了一个用于测量远程操作CAV时运动到运动（M2M）延迟的框架，该框架使用霍尔效应传感器和同步的Raspberry Pi设备，能够独立于底层架构测量转向操作的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 延迟是连接自动驾驶车辆（CAV）远程操作的关键性能因素。现有研究主要关注视频管道的端到端（G2G）延迟，但缺乏测量物理转向操作到车辆转向运动之间延迟（M2M）的标准方法。

Method: 开发了一个M2M延迟测量框架，使用霍尔效应传感器和两个同步的Raspberry Pi 5设备。系统在两侧记录基于中断的时间戳来估计M2M延迟，独立于底层远程操作架构。

Result: 精度测试显示测量准确度为10-15毫秒。现场结果表明执行器延迟主导M2M延迟，中位数值超过750毫秒。

Conclusion: 提出的M2M延迟测量框架能够准确测量远程操作CAV时的端到端延迟，揭示了执行器延迟是主要瓶颈，为系统优化提供了重要依据。

Abstract: Latency is a key performance factor for the teleoperation of Connected and Autonomous Vehicles (CAVs). It affects how quickly an operator can perceive changes in the driving environment and apply corrective actions. Most existing work focuses on Glass-to-Glass (G2G) latency, which captures delays only in the video pipeline. However, there is no standard method for measuring Motion-to-Motion (M2M) latency, defined as the delay between the physical steering movement of the remote operator and the corresponding steering motion in the vehicle. This paper presents an M2M latency measurement framework that uses Hall-effect sensors and two synchronized Raspberry Pi~5 devices. The system records interrupt-based timestamps on both sides to estimate M2M latency, independently of the underlying teleoperation architecture. Precision tests show an accuracy of 10--15~ms, while field results indicate that actuator delays dominate M2M latency, with median values above 750~ms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [155] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 提出一个结合高性能集群计算、智能算法和区块链的新框架，通过改进的PoW共识、动态信任评级和统计抽签系统，实现高效、环保、包容的分布式计算


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法需要大量计算资源，导致高能耗，并且排除了计算能力较弱的系统。需要一种更全面、可扩展且环保的智能算法开发和实施方法。

Method: 提出一个将高性能集群计算与智能算法在区块链基础设施中无缝融合的新框架。核心包括：1) 改进的PoW共识机制，将计算工作与区块奖励直接挂钩；2) 基于准确区块验证记录演变的动态"信任评级"系统；3) 统计"抽签"系统，让计算能力较弱的节点也有机会参与区块创建。

Result: 该框架实现了高效资源利用和广泛参与，通过信任评级创建了基于贡献质量的奖励系统，通过统计抽签系统确保了不同计算能力节点的公平参与机会。

Conclusion: 提出的框架为解决高性能计算和智能算法的能耗和包容性问题提供了有吸引力的解决方案，通过区块链技术实现了高效、环保且广泛参与的分布式计算生态系统。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [156] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 该论文研究了异步消息传递模型(AMP_f)与Heard-Of模型(HO_f)在分布式计算中的等价关系，发现对于无色任务在n>2f时等价，对于有色任务仅在f=1时等价，分离源于HO_f中的静默进程问题。


<details>
  <summary>Details</summary>
Motivation: 重新审视两个基本分布式计算模型之间的关系：具有最多f个崩溃故障的异步消息传递模型(AMP_f)和具有最多f个消息遗漏的Heard-Of模型(HO_f)，旨在精确界定基于轮次的抽象在何处能够捕获异步计算。

Method: 通过双向模拟方法，在AMP_f和HO_f之间建立等价关系证明，使用一个中间模型来捕捉静默概念，并将结果扩展到针对非自适应对手的随机协议。

Result: 对于无色任务，当n>2f时两个模型等价；对于有色任务，仅在f=1(且n>2)时等价。较大f时的分离源于HO_f中静默进程可能导致决策冲突。结果也适用于随机协议，表明规范轮次的表达能力限制是结构性的而非概率性的。

Conclusion: 研究结果精确界定了基于轮次的抽象在何处能够捕获异步计算，揭示了HO_f模型中静默进程对有色任务可解性的影响，为分布式计算模型间的等价关系提供了理论边界。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [157] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出了一种基于延迟约束的解耦架构，将集群资源分为延迟严格和延迟宽松池，通过瓶颈调度器和快速抢占机制，在保证在线服务SLO的同时，将离线吞吐量提升高达3倍。


<details>
  <summary>Details</summary>
Motivation: LLM部署中同时存在延迟敏感的在线服务和成本敏感的离线工作负载。在Prefill/Decode解耦系统中直接共置这些工作负载会导致严重的负载不平衡，因为请求组合的波动会改变固有的P/D比例，而现有的动态调整技术无法跟上在线服务的突发流量模式。

Method: 1. 延迟约束解耦架构：基于任务延迟要求将集群资源分为延迟严格和延迟宽松池，实现离线解码任务的灵活放置；2. 基于瓶颈的调度器：采用基于Roofline性能模型的性能瓶颈调度；3. 快速抢占机制：严格强制执行在线请求的服务级别目标(SLOs)。

Result: 在真实世界轨迹上的实验表明，与现有的离线系统方法相比，该方法在保持在线请求SLO的同时，将离线吞吐量提升高达3倍。

Conclusion: 提出的延迟约束解耦架构通过资源池分离、瓶颈调度和快速抢占机制，有效解决了P/D解耦系统中的负载不平衡问题，在保证在线服务质量的同时显著提升了离线工作负载的吞吐量。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [158] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+是针对元数据缓存设计的缓存替换算法，通过在小FIFO队列中引入关联窗口来避免误判热块，在元数据跟踪中比S3-FIFO降低高达28.5%的缺失率。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存存在固有的关联引用特性，即使对应的数据访问不包含关联引用。这些关联引用会降低缓存替换算法的效果，因为它们经常被错误地归类为热块。

Method: Clock2Q+使用三个队列（类似S3-FIFO），但在小FIFO队列中引入了关联窗口，该窗口中的块不设置引用位。这种简单增强使其能够有效处理元数据缓存中的关联引用问题。

Result: 在元数据跟踪中，Clock2Q+比第二佳算法S3-FIFO降低高达28.5%的缺失率。该算法在VMware的vSAN和VDFS存储产品中已实现，具有低CPU开销、低内存开销、多CPU可扩展性，且易于调优和实现。

Conclusion: Clock2Q+是针对元数据缓存特性设计的有效缓存替换算法，通过简单但巧妙的关联窗口机制显著提升了性能，同时满足大规模存储系统的关键要求。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [159] [ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast](https://arxiv.org/abs/2511.21969)
*Matteo Bjornsson,Taylor Hardin,Taylor Heinecke,Marcin Furtak,David L. Millman,Mike P. Wittie*

Main category: cs.DC

TL;DR: ZipperChain是一种无需分布式共识的分布式账本技术，通过专用服务流水线在少数节点上构建区块，实现接近网络线速的交易吞吐量和500毫秒的最终确定性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式共识机制因节点间网络通信存在性能限制，ZipperChain旨在解决这一问题，提供无需分布式共识的高性能区块链解决方案。

Method: 采用专用服务流水线架构，在少量节点上部署，利用快速数据中心网络构建区块，将信任从广泛使用的第三方服务转移到ZipperChain的正确性保证上。

Result: 交易吞吐量接近网络线速，区块最终确定性达到约500毫秒，且无需原生代币激励验证者社区。

Conclusion: ZipperChain通过创新的架构设计，在保证交易数据不可篡改、一致性和可用性的同时，避免了传统分布式共识的性能瓶颈，实现了高性能的区块链系统。

Abstract: Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.

</details>


### [160] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 研究比较了在跨语言复制数据系统中集成复制数据库(RDL)的两种策略：外部函数接口(FFI)和通用数据格式(CDF)，发现CDF在软件质量、延迟、内存消耗和吞吐量方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要在多个执行站点复制数据，业务需求和资源限制常导致不同站点使用不同编程语言。复制数据库(RDL)通常只支持单一语言，在跨语言环境中集成现有RDL需要专门代码，但这些代码的软件质量和性能特征缺乏深入理解。

Method: 通过实证研究比较两种RDL集成策略：外部函数接口(FFI)和通用数据格式(CDF)，测量并比较它们的软件指标和性能，评估它们在不同语言环境中的适用性。

Result: 研究发现采用CDF进行跨语言交互在软件质量、延迟、内存消耗和吞吐量方面具有优势。通过创建支持编译、解释和托管语言的CDF-based RDL，并增强其插件扩展性，验证了这些发现。

Conclusion: 随着现代分布式系统使用多种语言，本研究为设计跨语言复制数据系统中的RDL提供了新的见解，CDF策略显示出更好的综合性能。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [161] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT提出了一种前缀感知的注意力核实现，通过打包共享前缀查询、多瓦片内核和KV分割等技术，显著减少解码注意力中的内存访问和资源空闲，在vLLM中实现平均67.4%的注意力延迟降低。


<details>
  <summary>Details</summary>
Motivation: LLM服务中解码注意力成为主要瓶颈，这是内存受限的操作，需要从全局内存加载大量KV缓存。实际工作负载中存在大量跨请求的层次化共享前缀（如系统提示、工具/模板、RAG），现有注意力实现无法充分利用前缀共享，导致重复加载共享前缀KV缓存和资源空闲，加剧了内存带宽压力。

Method: PAT采用打包-前向-合并范式：1) 按共享前缀打包查询以减少重复内存访问；2) 运行定制化的多瓦片内核实现高资源效率；3) 应用多流前向和KV分割减少资源空闲；4) 最终合并执行在线softmax，开销可忽略。

Result: 在真实世界和合成工作负载上的评估显示，PAT在相同配置下相比最先进的注意力核，平均减少注意力延迟67.4%，TPOT降低13.6-83.4%。

Conclusion: PAT通过前缀感知的注意力核实现，有效解决了LLM解码中内存受限的问题，显著提升了注意力计算效率，可作为vLLM的即插即用插件使用。

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [162] [Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)](https://arxiv.org/abs/2511.22380)
*Kaya Alpturer,Ron van der Meyden,Sushmita Ruj,Godfrey Wong*

Main category: cs.DC

TL;DR: 该论文研究在崩溃故障模型中，针对多种有限信息交换的同步共识问题，提出新的信息交换机制，实现比最优协议稍晚但计算成本更低的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识逻辑的最优容错共识协议主要采用"完全信息"交换方式，消息开销大。Alpturer等人提出了有限信息交换下的最优性概念，本文在此基础上研究崩溃故障模型中的同步共识问题。

Method: 论文考虑文献中的多种有限信息交换机制：FloodSet协议、带故障计数的变体、以及关联代理值的变体。同时引入新的信息交换机制，通过知识基程序实现，为每种信息交换推导出最优协议。

Result: 提出的新信息交换机制使决策最多比Dwork和Moses的最优协议晚一轮，但具有更低的计算成本和空间需求。为每种考虑的信息交换机制推导出了最优协议。

Conclusion: 在崩溃故障模型中，通过有限信息交换可以实现接近最优的同步共识协议，在延迟和计算成本之间取得良好平衡，为实际分布式系统提供了更实用的解决方案。

Abstract: Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the "full information" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.

</details>


### [163] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer是一个统一的系统级加速框架，通过专家放置优化、缓存压缩和调度来最大化大语言模型服务的端到端效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现代AI应用中广泛使用，但在大规模服务系统中面临计算密集、延迟约束严格和吞吐量瓶颈等挑战，需要系统级优化来提高服务效率。

Method: OmniInfer包含三个互补组件：OmniPlacement用于负载感知的混合专家调度，OmniAttn用于稀疏注意力加速，OmniProxy用于解耦感知的请求调度。框架基于vLLM构建，通过自适应资源解耦、高效稀疏性利用以及预填充和解码阶段的全局协调来实现优化。

Result: 在10节点Ascend 910C集群上评估DeepSeek-R1模型，OmniInfer达到616 QPM，统一框架将TPOT降低36%，OmniProxy的叠加进一步将TTFT降低38%。

Conclusion: OmniInfer通过系统级优化显著提升大语言模型服务效率，已开源供社区使用。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [164] [DisCEdge: Distributed Context Management for Large Language Models at the Edge](https://arxiv.org/abs/2511.22599)
*Mohammadreza Malekabbasi,Minghe Wang,David Bermbach*

Main category: cs.DC

TL;DR: DisCEdge是一个分布式上下文管理系统，通过在边缘节点间以token化形式存储和复制用户上下文，解决了LLM在边缘部署中的上下文管理挑战，显著提升了响应时间和减少了同步开销。


<details>
  <summary>Details</summary>
Motivation: 在边缘部署LLM服务有利于延迟敏感和隐私保护应用，但LLM的无状态特性使得在分布式边缘节点间管理用户上下文（如会话、偏好）变得困难。现有解决方案（如客户端上下文存储）会引入网络延迟和带宽开销，削弱了边缘部署的优势。

Method: 提出DisCEdge系统，将用户上下文以token序列而非原始文本形式存储和复制在边缘节点间。这种方法避免了冗余计算，实现了高效的数据复制。在真实边缘环境中使用商用硬件实现并评估了开源原型。

Result: DisCEdge相比基于原始文本的系统，将中位响应时间提升了14.46%，中位节点间同步开销降低了15%。相比客户端上下文管理，将客户端请求大小中位数减少了90%，同时保证了数据一致性。

Conclusion: DisCEdge通过token化的分布式上下文管理，有效解决了边缘LLM部署中的上下文管理挑战，显著提升了性能并减少了开销，为边缘AI应用提供了实用的解决方案。

Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.

</details>


### [165] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC：利用GPU光线追踪核心硬件加速的蒙特卡洛算法，显著提升网格蒙特卡洛模拟速度


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛（MMC）方法虽然精度高，但频繁的光线-边界相交测试计算成本高，限制了性能提升

Method: 基于NVIDIA OptiX平台实现，利用现代GPU的光线追踪核心硬件加速能力，将图形光线追踪管道扩展到浑浊介质中的体积光线追踪

Result: 与传统软件光线追踪MMC算法结果高度一致，在不同GPU架构上实现1.5-4.5倍加速，无需复杂四面体网格生成

Conclusion: 从软件转向硬件光线追踪不仅简化了MMC模拟工作流程，还带来显著性能提升，有望随着光线追踪硬件普及而进一步增强

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [166] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe：一个针对LoRA适配器服务中秩多样性问题的动态放置和路由框架，通过智能调度和远程访问优化，显著提升吞吐量并减少GPU需求。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA服务系统在处理多租户环境中不同秩（大小）的适配器时存在性能倾斜问题，导致GPU资源利用不足，需要更多GPU来满足SLO要求。

Method: LoRAServe采用工作负载感知的动态适配器放置和路由框架，通过动态重新平衡GPU间的适配器分配，并利用GPU Direct RDMA进行远程访问。

Result: 在生产环境测试中，LoRAServe相比现有系统实现了2倍吞吐量提升、9倍TTFT降低，并在SLO约束下减少了50%的GPU使用。

Conclusion: LoRAServe通过有效管理LoRA服务中的秩多样性，显著提升了多租户环境下的资源利用效率和服务性能。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


### [167] [Areon: Latency-Friendly and Resilient Multi-Proposer Consensus](https://arxiv.org/abs/2511.23025)
*Álvaro Castro-Castilla,Marcin Pawlowski,Hong-Sheng Zhou*

Main category: cs.DC

TL;DR: Areon是一个基于DAG的多提议者PoS共识协议，通过滑动窗口内的块引用形成并行投票，使用CCA-local分叉选择规则解决冲突，实现低延迟最终性。


<details>
  <summary>Details</summary>
Motivation: 现有链式PoS协议在部分同步网络下可能存在重组频率高、延迟大的问题，需要设计更鲁棒、低延迟的共识机制。

Method: 1) 允许每个时隙多个提议者，块组织成DAG结构；2) 块在滑动窗口内相互引用，形成最大反链作为并行投票；3) 使用CCA-local、窗口过滤的分叉选择规则，比较子DAG权重；4) 引入Tip-Boundedness结构不变量确保有限宽度边界。

Result: 1) 形式化理想协议(Areon-Ideal)和实际协议(Areon-Base)；2) 证明(k,ε)-最终性定理；3) 模拟显示相比Ouroboros Praos，Areon-Base在广泛对抗条件下实现有界延迟最终性，重组频率和深度更低。

Conclusion: Areon通过DAG结构和多提议者设计，在部分同步网络下实现了鲁棒、低延迟的PoS共识，相比传统链式协议在重组性能上有显著改进。

Abstract: We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.
  We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.
  Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.

</details>


### [168] [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)
*Chenyu Liu,Zhaoyang Zhang,Zirui Chen,Zhaohui Yang*

Main category: cs.DC

TL;DR: 提出C²P²SL方法，通过流水线并行化通信与计算过程，显著减少分割学习在无线网络中的训练时间


<details>
  <summary>Details</summary>
Motivation: 传统分割学习虽然保护了本地数据隐私，但其通信和计算过程是顺序执行的，导致系统效率有限。需要克服这一限制，提高无线网络中分割学习的训练效率。

Method: 将分布式训练中的流水线并行技术应用于分割学习，提出C²P²SL方法。将用户设备和基站的通信与计算过程视为整体流水线，在不同微批次之间实现流水线并行化。通过联合优化任务分割和资源分配问题，采用交替优化方法求解。

Result: 实验结果表明，C²P²SL在不同通信条件下能够将系统训练时间减少超过38%，同时保持收敛精度。

Conclusion: C²P²SL通过流水线并行化有效解决了分割学习在无线网络中的效率瓶颈问题，显著提升了训练效率，为资源受限环境下的分布式学习提供了有效解决方案。

Abstract: Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\% while maintaining convergence accuracy under different communication conditions.

</details>


### [169] [Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election](https://arxiv.org/abs/2511.23297)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 在内容无关通信模型下，利用网络拓扑知识可以在多种图中实现领导者选举，但需要精确的拓扑信息。


<details>
  <summary>Details</summary>
Motivation: 先前研究显示在内容无关通信模型中，两个节点无法计算任何非常数函数，这似乎排除了许多图问题的可能性。本研究探索在已知网络拓扑的情况下，领导者选举是否可行。

Method: 1. 证明关于边对称的图不可能实现随机终止的领导者选举；2. 为非对称树设计基于拓扑知识的静默终止算法；3. 分析拓扑知识对算法可行性的必要性。

Result: 1. 边对称图无法实现领导者选举；2. 非对称树可实现领导者选举，消息复杂度为O(n²)；3. 偶直径树仅需知道直径即可实现选举，消息复杂度O(nr)；4. 精确拓扑知识对算法可行性至关重要。

Conclusion: 在内容无关通信模型中，网络拓扑知识是实现领导者选举的关键因素，但需要精确的拓扑信息，而不仅仅是拓扑类别知识。

Abstract: The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.
  In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.
  Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.
  Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.
  Necessity of topology knowledge: In the family of graphs $\mathcal{G} = \{P_3, P_5\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\mathcal{G}$, then terminating leader election is impossible.

</details>
