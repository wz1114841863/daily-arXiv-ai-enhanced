<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.LG](#cs.LG) [Total: 89]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [PIM or CXL-PIM? Understanding Architectural Trade-offs Through Large-Scale Benchmarking](https://arxiv.org/abs/2511.14400)
*I-Ting Lee,Bao-Kai Wang,Liang-Chi Chen,Wen Sheng Lim,Da-Wei Chang,Yu-Ming Chang,Chieng-Chung Ho*

Main category: cs.ET

TL;DR: 本文通过大规模对比分析PIM和CXL-PIM两种近内存计算架构，揭示了它们在统一地址空间和分离地址空间之间的权衡，识别了不同工作负载下哪种架构更优的具体条件。


<details>
  <summary>Details</summary>
Motivation: 现有PIM硬件由于主机和设备地址空间分离，导致显式数据传输成为性能瓶颈，而CXL-PIM提供统一地址空间但访问延迟更高。这两种对立接口模型的工作负载依赖性权衡尚未被小规模研究充分揭示。

Method: 使用真实PIM硬件测量和基于跟踪的CXL建模，进行大规模并排比较分析，识别统一地址访问何时能够摊销链路延迟以克服传输瓶颈。

Result: 研究发现存在阶段性和数据集大小相关的机制，其中两种架构的相对排名会发生逆转，揭示了统一地址访问在特定条件下能够克服传输瓶颈，而紧密耦合的PIM在其他条件下仍更优。

Conclusion: 研究结果为未来近内存系统设计提供了实用指导，明确了在不同工作负载特征下选择PIM或CXL-PIM架构的具体条件。

Abstract: Processing-in-memory (PIM) reduces data movement by executing near memory, but our large-scale characterization on real PIM hardware shows that end-to-end performance is often limited by disjoint host and device address spaces that force explicit staging transfers. In contrast, CXL-PIM provides a unified address space and cache-coherent access at the cost of higher access latency. These opposing interface models create workload-dependent tradeoffs that are not captured by small-scale studies. This work presents a side-by-side, large-scale comparison of PIM and CXL-PIM using measurements from real PIM hardware and trace-driven CXL modeling. We identify when unified-address access amortizes link latency enough to overcome transfer bottlenecks, and when tightly coupled PIM remains preferable. Our results reveal phase- and dataset-size regimes in which the relative ranking between the two architectures reverses, offering practical guidance for future near-memory system design.

</details>


### [2] [Empirical Quantum Advantage in Constrained Optimization from Encoded Unitary Designs](https://arxiv.org/abs/2511.14296)
*Chinonso Onah,Roman Firt,Kristel Michielsen*

Main category: cs.ET

TL;DR: 提出了约束增强量子近似优化算法(CE-QAOA)，这是一种浅层、约束感知的ansatz，在[n]^m的单热积空间中操作，通过无辅助量子比特、深度最优的编码器准备W_n态，结合经典检查器构建多项式时间混合量子经典求解器。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统QAOA在处理约束优化问题时需要深度电路和大量辅助量子比特的问题，开发一种浅层、约束感知的算法，能够在多项式时间内找到可行解。

Method: 使用CE-QAOA算法，在单热积空间中操作，每个块用n-1个双量子比特旋转准备W_n态，使用限制在同一块内的双局域XY混合器，结合经典检查器验证解的有效性。

Result: 在无噪声电路模拟中，对4到10个位置的TSP实例，在深度p=1时使用多项式采样预算和粗粒度参数网格就能恢复全局最优解。当固定r>=1个位置不同于起始城市时，实现了Theta(n^r)的采样复杂度降低。

Conclusion: CE-QAOA在浅层深度下就能有效解决约束优化问题，相比经典采样方法在最小最大意义上实现了指数级分离，展示了量子算法在组合优化中的优势。

Abstract: We introduce the Constraint-Enhanced Quantum Approximate Optimization Algorithm (CE-QAOA), a shallow, constraint-aware ansatz that operates inside the one-hot product space of size [n]^m, where m is the number of blocks and each block is initialized with an n-qubit W_n state. We give an ancilla-free, depth-optimal encoder that prepares a W_n state using n-1 two-qubit rotations per block, and a two-local XY mixer restricted to the same block of n qubits with a constant spectral gap. Algorithmically, we wrap constant-depth sampling with a deterministic classical checker to obtain a polynomial-time hybrid quantum-classical solver (PHQC) that returns the best observed feasible solution in O(S n^2) time, where S is the number of shots. We obtain two advantages. First, when CE-QAOA fixes r >= 1 locations different from the start city, we achieve a Theta(n^r) reduction in shot complexity even against a classical sampler that draws uniformly from the feasible set. Second, against a classical baseline restricted to raw bitstring sampling, we show an exp(Theta(n^2)) separation in the minimax sense. In noiseless circuit simulations of TSP instances ranging from 4 to 10 locations from the QOPTLib benchmark library, we recover the global optimum at depth p = 1 using polynomial shot budgets and coarse parameter grids defined by the problem sizes.

</details>


### [3] [Graph Neural Networks for Vehicular Social Networks: Trends, Challenges, and Opportunities](https://arxiv.org/abs/2511.14720)
*Elham Binshaflout,Aymen Hamrouni,Hakim Ghazzai*

Main category: cs.ET

TL;DR: 该调查首次全面回顾了图神经网络在车载社交网络中的应用，系统分类分析了交通预测、信号控制等主要任务，指出GNNs在VSNs中具有巨大潜力但缺乏完整系统建模研究。


<details>
  <summary>Details</summary>
Motivation: 图神经网络因其处理复杂互联数据的强大能力，特别适合智能交通系统应用。本文旨在填补GNNs在车载社交网络中应用研究的空白，提供系统性的综述分析。

Method: 通过系统文献回顾方法，分类分析现有研究，涵盖交通流预测、轨迹预测、信号控制、驾驶辅助、路由问题和连接管理等主要VSN任务，并提供定量分析和关键见解。

Result: 研究发现GNNs在提高特定任务或子VSN图的准确性、鲁棒性和实时性能方面表现出强大潜力，但缺乏对整个独立VSN系统完整建模的研究。

Conclusion: 随着数据可用性增加和图学习技术进步，GNNs有望在未来大规模、完全集成的VSN应用中发挥核心作用，需要更多完整系统建模研究。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling complex, interconnected data, making them particularly well suited for a wide range of Intelligent Transportation System (ITS) applications. This survey presents the first comprehensive review dedicated specifically to the use of GNNs within Vehicular Social Networks (VSNs). By leveraging both Euclidean and non-Euclidean transportation-related data, including traffic patterns, road users, and weather conditions, GNNs offer promising solutions for analyzing and enhancing VSN applications. The survey systematically categorizes and analyzes existing studies according to major VSN-related tasks, including traffic flow and trajectory prediction, traffic forecasting, signal control, driving assistance, routing problem, and connectivity management. It further provides quantitative insights and synthesizes key takeaways derived from the literature review. Additionally, the survey examines the available datasets and outlines open research directions needed to advance GNN-based VSN applications. The findings indicate that, although GNNs demonstrate strong potential for improving the accuracy, robustness, and real-time performances of on task-specific or sub-VSN graphs, there remains a notable absence of studies that model a complete, standalone VSN encompassing all functional components. With the increasing availability of data and continued progress in graph learning, GNNs are expected to play a central role in enabling future large-scale and fully integrated VSN applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 该论文将GCS算法的形式模型扩展到实现近似的假设下，通过用双向测量范式替代单向测量范式，移除了许多先前为证明性能而施加的限制，显著降低了算法估计误差。


<details>
  <summary>Details</summary>
Motivation: 扩展GCS算法的形式模型，使其在实现近似的假设下运行，通过改变测量范式来移除先前工作中的限制，创造更现实的部署模型。

Method: 用双向测量范式替代单向测量范式，解除对统一链路长度的要求，提供频率源的形式模型，对算法估计误差的不同组成部分进行细粒度区分。

Result: 将不确定性对算法估计误差的贡献显著降低到每链路延迟的10%到0.1%范围内（之前约为延迟量级），并给出了GCS局部和全局偏差的匹配上界。

Conclusion: 通过改变测量范式，在保持GCS核心行为的同时，显著提升了算法的实际适用性和性能，为GCS的灵活部署提供了更现实的模型。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [5] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia是一个GPU即服务模型和架构，通过动态执行模式识别和运行时评估，为异构环境中的无服务器AI工作负载提供SLO感知、成本高效的硬件加速。


<details>
  <summary>Details</summary>
Motivation: 当前平台在管理硬件加速方面存在困难，静态用户设备分配无法在变化负载或位置下保证SLO合规性，一次性动态选择往往导致次优或成本低效的配置。

Method: Gaia结合了轻量级执行模式标识器（在部署时检查函数代码并发出四种执行模式之一）和动态函数运行时（持续重新评估用户定义的SLO以在CPU和GPU后端之间进行升级或降级）。

Result: 评估显示Gaia能够无缝选择最适合工作负载的硬件加速，将端到端延迟降低高达95%。

Conclusion: Gaia能够在异构环境中为无服务器AI实现SLO感知、成本高效的加速。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [6] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: TT-Edge是一个软硬件协同设计框架，通过在边缘AI处理器上专门优化张量训练分解(TTD)的计算流程，实现了1.7倍加速和40.2%能耗降低。


<details>
  <summary>Details</summary>
Motivation: 分布式学习在资源受限的边缘设备上需求增长，TTD虽然提供高压缩比但重复的SVD和矩阵乘法在低功耗处理器上带来显著延迟和能耗问题。

Method: 将SVD分解为双对角化和对角化两个阶段，将计算密集型任务卸载到专门的TTD引擎，该引擎与现有GEMM加速器紧密集成以减少矩阵向量传输。

Result: 在RISC-V边缘AI处理器上压缩ResNet-32模型时，相比仅使用GEMM的基线，速度提升1.7倍，总能耗降低40.2%，总功耗仅增加4%。

Conclusion: TT-Edge通过轻量级设计有效解决了边缘环境中基于TTD压缩的延迟和能耗瓶颈问题。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [7] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: VOLT是一个轻量级编译器工具链，旨在解决开源GPU架构中SIMT功能执行和性能优化的技术挑战，通过分层设计支持多抽象级别的代码生成和优化。


<details>
  <summary>Details</summary>
Motivation: 开源GPU架构需要复杂的编译器框架来执行现有GPU程序和优化性能，这在开源硬件开发中往往被低估。VOLT旨在解决这一技术挑战。

Method: 采用分层设计，在中间端集中处理SIMT相关分析和优化，支持多种前端语言和开源GPU硬件，确保可扩展性以适应GPU架构演进。

Result: 通过ISA扩展和主机运行时API的案例研究，证明了VOLT能够有效支持扩展。

Conclusion: VOLT提供了一个可扩展的编译器框架，能够支持开源GPU架构的SIMT执行和优化需求，适应不断发展的GPU技术。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [8] [What happens when nanochat meets DiLoCo?](https://arxiv.org/abs/2511.13761)
*Alexander Acker,Soeren Becker,Sasho Nedelkoski,Dominik Scheinert,Odej Kao,Philipp Wiesner*

Main category: cs.DC

TL;DR: 论文研究了分布式训练中通信受限环境下的模型权衡，使用nanochat项目作为基线，比较了DiLoCo算法与标准数据并行训练的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探索在通信受限的分布式环境中训练LLM时引入的模型权衡，这些权衡在现有研究中尚未充分探索。

Method: 使用nanochat作为可控基线，实现DiLoCo算法作为轻量级包装器，在多个本地步骤后进行同步，与标准数据并行训练进行比较。

Result: DiLoCo在预训练中实现稳定收敛和竞争性损失，但在中期训练和SFT后产生较差的MMLU、GSM8K和HumanEval分数。使用DiLoCo预训练权重后运行DDP无法恢复性能。

Conclusion: 异步更新导致的表示漂移是不可逆的，会损害下游任务的对齐能力。

Abstract: Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.
  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.
  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.

</details>


### [9] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: ADP是一个完全在GPU上运行的框架，通过自动动态精度技术，使用低精度单元（如FP4）来模拟双精度浮点运算，在保持FP64精度的同时实现显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代GPU硬件转向低精度格式（FP16、FP8、FP4），传统FP64流水线吞吐量较低。需要开发能够在低精度硬件上实现双精度精度的算法。

Method: 基于指数跨度容量（ESC）硬件无关估计器，自动确定分解参数；集成异常处理、运行时启发式算法和无缝回退到原生FP64；采用无符号整数切片方案提高表示效率。

Result: 在55位尾数设置下，相比原生FP64 GEMM，在NVIDIA Blackwell GB200上实现2.3倍加速，在RTX Pro 6000 Blackwell服务器版上实现13.2倍加速，运行时间开销小于10%。

Conclusion: 低精度加速器可以作为高保真、高性能科学计算工作负载的实用、生产就绪基础。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [10] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出了语义多路复用新概念，将多任务压缩表示合并为单一语义表示，可在不增加天线或带宽的情况下同时处理更多任务，显著降低延迟、能耗和通信负载。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统仅支持比特级并行传输，限制了可并发处理的任务数量，需要突破这一瓶颈。

Method: 将流多路复用从比特级转移到任务级，通过将多个任务相关的压缩表示合并为单一语义表示来实现语义多路复用。

Result: 实验表明语义多路复用可在保持足够任务精度的同时联合处理多个任务，图像分类精度在4×4信道上从2个任务增加到8个任务时下降不到4%，相比基线延迟降低8倍、能耗降低25倍、通信负载降低54倍。

Conclusion: 语义多路复用通过扩展语义层的有效自由度，在不违背香农容量规则的前提下实现了更多任务的并发处理，显著提升了系统性能。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [11] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: MPI派生数据类型(DDTs)的性能表现因MPI实现而异，没有统一的性能优势。在单节点测试中，DDTs在某些情况下最快，但在其他情况下最慢，性能可移植性无法保证。


<details>
  <summary>Details</summary>
Motivation: 评估MPI派生数据类型在实际应用中的性能表现，比较不同MPI实现下DDTs与手动打包方法的性能差异。

Method: 使用三个2D应用程序（Jacobi CFD求解器、康威生命游戏、基于格点的图像重建），每个应用分别实现BASIC版本（手动打包）和DDT版本（使用MPI_Type_vector等）。在1-4个进程上测试强扩展和弱扩展，比较四种MPI实现（MPICH、Open MPI、Intel MPI、MVAPICH2）。

Result: 结果混合：DDTs在某些MPI实现上最快（如Intel MPI和MPICH上的图像重建代码），但在其他实现上最慢（如Open MPI和MVAPICH2）。CFD求解器中BASIC版本普遍优于DDTs，而生命游戏的性能排名因MPI库而异。

Conclusion: 没有一种策略在所有程序、通信语义和MPI实现中都占优势，DDTs的性能可移植性无法保证。建议在实际MPI实现和通信模式下同时分析DDT和手动打包设计的性能。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [12] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个简化的 CUDA 框架，通过八个核心原语和统一编程模板，显著简化了重叠多 GPU 内核的开发，在 Hopper 和 Blackwell 架构上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型规模扩大和硬件计算吞吐量提升超过互连带宽改进，GPU 间通信已成为现代 AI 工作负载的主要瓶颈。现有系统通过计算-通信重叠来缓解，但在异构工作负载和新加速器上往往无法达到理论峰值性能。

Method: PK 扩展了 ThunderKittens 框架，通过八个核心原语和统一编程模板体现多 GPU 内核设计原则。该方法基于对多 GPU 性能影响因素（数据传输机制、资源调度和设计开销）的全面分析。

Result: 在 Hopper 和 Blackwell 架构上验证，仅用不到 50 行设备代码，PK 实现了：数据并行和 tensor 并行工作负载最高 2.33 倍加速，序列并行工作负载 4.08 倍加速，专家并行工作负载 1.22 倍加速。

Conclusion: PK 证明了通过一组简单、可重用的原则可以系统性地指导最优多 GPU 内核设计，显著简化了重叠多 GPU 内核的开发过程。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [13] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe是一个容错的张量并行(TP)服务系统，通过循环KVCache放置、混合注意力和细粒度负载感知路由等技术，在GPU故障时维持高性能推理，相比标准方法提升2倍吞吐量并降低两个数量级的恢复延迟。


<details>
  <summary>Details</summary>
Motivation: 传统张量并行(TP)的紧耦合特性使系统脆弱：单个GPU故障会中断执行、触发昂贵的KVCache重计算，并导致长期的计算和内存不平衡。

Method: 1) 循环KVCache放置实现均匀内存利用；2) 混合注意力结合张量和数据并行注意力消除慢节点；3) 细粒度负载感知路由动态平衡请求；4) 主动KVCache备份和按需权重恢复避免重计算和冗余数据传输。

Result: 在8xH100 DGX系统上测试，相比标准故障处理方法，FailSafe实现高达2倍的吞吐量提升和两个数量级的恢复延迟降低。即使最多三个GPU故障，仍能维持高吞吐量和均衡利用。

Conclusion: FailSafe展示了在动态不可靠硬件条件下，实现稳健高效LLM服务的能力，为大规模语言模型推理提供了可靠的容错解决方案。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [14] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache是一个资源感知的张量缓存和迁移系统，通过智能协调GPU、CPU和NVMe之间的内存使用来加速大语言模型训练，解决云环境中GPU内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 云环境中训练大语言模型面临GPU内存容量有限和成本高的瓶颈，现有GPU内存卸载方法存在张量迁移延迟高和设备内存利用率低的问题，导致训练时间延长和云成本增加。

Method: 10Cache通过分析张量执行顺序构建预取策略，基于张量大小分布在固定内存中分配内存缓冲区，并重用内存缓冲区以减少分配开销，实现跨GPU、CPU和NVMe层级的智能内存协调。

Result: 在多样化LLM工作负载中，10Cache相比最先进的卸载方法，训练时间最高加速2倍，GPU缓存命中率提高86.6倍，CPU和GPU内存利用率分别提高2.15倍和1.33倍。

Conclusion: 10Cache是优化云环境中LLM训练吞吐量和资源效率的实用且可扩展的解决方案，能够显著提升内存效率并减少对高端GPU的依赖。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [15] [Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks](https://arxiv.org/abs/2511.14450)
*Mulei Ma,Minrui Xu,Zihan Chen,Yang Yang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: Hyperion是一个分层两阶段框架，联合优化LLM在多层网络中的分区和调度，以最小化端到端延迟，在Phi-3-medium模型上比GPipe和HEFT基线分别降低延迟达52.1%和31.2%。


<details>
  <summary>Details</summary>
Motivation: LLM在边缘、雾和云层部署时，受限于GPU内存、异构计算和可变层间带宽，需要模型分区和请求调度。分区和调度问题紧密耦合，次优调度会抵消良好分区的优势。

Method: 两阶段框架：阶段1使用二分搜索与动态规划进行离线层间分区，在层容量和内存约束下产生平衡的阶段时间；阶段2使用轻量级自适应实时任务调度算法进行在线层内调度，利用实时队列长度和有效容量估计将请求映射到最佳可用节点。

Result: 在多层推理任务中，Hyperion显著降低端到端延迟，在Phi-3-medium模型上比GPipe和HEFT基线分别降低52.1%和31.2%。在长序列生成中保持比GPipe低44.5%的延迟，并实现更高的GPU利用率。

Conclusion: Hyperion通过联合优化分区和调度，在多层网络中有效最小化LLM流水线推理的端到端延迟，引入可忽略的运行时开销且无需模型重训练，展示了优越的可扩展性。

Abstract: Large Language Models (LLMs) are increasingly executed across edge, fog, and cloud tiers where limited GPU memory, heterogeneous compute, and variable inter-tier bandwidth jointly constrain deployment and motivate model partitioning and request scheduling. In this setting, achieving low end-to-end latency is governed not only by where a model is deployed (inter-tier model partitioning) but also by how incoming requests are scheduled (intra-tier task scheduling) across heterogeneous nodes. These two problems are tightly coupled, as a suboptimal scheduler can negate the benefits of a good partition, and vice versa. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling to minimize end-to-end latency for pipelined LLM inference in multi-tier networks, balancing compute and memory across tiers while introducing negligible runtime overhead and requiring no model retraining. Motivated by the observation that partition choices evolve on slower timescales than request arrivals, Stage 1 performs offline, inter-tier partitioning via a Binary Search with Dynamic Programming (BSDP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Adaptive Real-time Task Scheduling (ARTS) algorithm that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experimental results on multi-tier inference tasks demonstrate that Hyperion significantly reduces end-to-end latency by up to 52.1\% and 31.2\%, with the Phi-3-medium model, compared to the GPipe and HEFT baselines, respectively. Furthermore, Hyperion shows superior scalability in long-sequence generation, maintaining a 44.5\% lower latency than GPipe and achieving higher GPU utilization.

</details>


### [16] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 分析联邦学习中参与者故障对模型质量的影响，特别是在跨组织场景下，研究故障时机、数据分布和评估方法的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在跨组织协作中需要可靠系统，但参与者可能因各种原因故障。虽然跨设备FL已有相关研究，但跨组织FL中参与者故障影响的研究相对较少。

Method: 在参与者较少的跨组织联邦学习环境中，进行广泛研究分析参与者故障对模型质量的影响，重点关注故障时机、数据分布和评估方法等因素。

Result: 研究表明，在高数据偏斜情况下，评估结果过于乐观，掩盖了真实影响；故障时机显著影响训练模型的质量。

Conclusion: 研究结果为构建鲁棒联邦学习系统的研究者和软件架构师提供了重要见解，强调了考虑参与者故障影响的重要性。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [17] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks是一种新颖的锁定算法，具有简单性、恒定时间到达和解锁路径、FIFO准入顺序、空间效率高、在争用情况下产生较少一致性流量等优点。


<details>
  <summary>Details</summary>
Motivation: 开发一种性能与最先进锁算法相当，但对运行时环境约束更少、更容易集成到现有系统中的锁定算法。

Method: 设计了一种新颖的锁定算法，其中指针不会在线程间转移或逃逸所有权，提供恒定时间的到达和解锁操作。

Result: Hapax Locks在延迟和可扩展性方面与最先进的锁算法性能相当，同时减少了环境依赖，便于系统集成。

Conclusion: Hapax Locks是一种简单高效的锁定算法，特别适合集成到现有系统或现有API下，具有良好的性能和易用性。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [18] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer是一个创新的在线上下文学习系统，通过利用共享相同提示的请求在输出长度和生成模式上的相似性，解决了同步强化学习系统中的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的同步强化学习系统在rollout阶段面临严重的性能瓶颈，存在显著的长尾延迟和资源利用率低的问题，主要由于工作负载不平衡导致。

Method: Seer引入了三项关键技术：动态负载平衡的分割rollout、上下文感知调度和自适应分组推测解码，这些机制共同减少了长尾延迟并提高了资源效率。

Result: 在生产级强化学习工作负载上的评估显示，与最先进的同步强化学习系统相比，Seer将端到端rollout吞吐量提高了74%到97%，长尾延迟降低了75%到93%。

Conclusion: Seer通过利用请求间的相似性，显著加速了强化学习训练迭代，有效解决了同步强化学习系统中的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


### [19] [Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance](https://arxiv.org/abs/2511.14664)
*W. Michael Brown,Anurag Ramesh,Thomas Lubinski,Thien Nguyen,David E. Bernal Neira*

Main category: cs.DC

TL;DR: 该论文介绍了在QED-C基准测试中引入MPI以支持HPC系统上的多GPU量子模拟，展示了GPU架构和互连技术对性能的影响，其中互连性能改进比GPU架构改进带来更大的性能提升。


<details>
  <summary>Details</summary>
Motivation: 量子计算的经典模拟资源需求巨大，但模拟对算法开发和硬件设计至关重要。多GPU模拟需要处理系统间通信瓶颈问题。

Method: 在QED-C基准测试中引入MPI，使用多种互连路径进行基准测试，包括最新的NVIDIA Grace Blackwell NVL72架构。

Result: GPU架构改进带来了4.5倍以上的加速，但互连性能改进对多GPU模拟的时间解决方案带来了超过16倍的性能提升。

Conclusion: 互连技术进步对多GPU量子模拟性能的影响比GPU架构改进更为显著，强调了高性能互连在量子计算模拟中的重要性。

Abstract: As is intrinsic to the fundamental goal of quantum computing, classical simulation of quantum algorithms is notoriously demanding in resource requirements. Nonetheless, simulation is critical to the success of the field and a requirement for algorithm development and validation, as well as hardware design. GPU-acceleration has become standard practice for simulation, and due to the exponential scaling inherent in classical methods, multi-GPU simulation can be required to achieve representative system sizes. In this case, inter-GPU communications can bottleneck performance. In this work, we present the introduction of MPI into the QED-C Application-Oriented Benchmarks to facilitate benchmarking on HPC systems. We review the advances in interconnect technology and the APIs for multi-GPU communication. We benchmark using a variety of interconnect paths, including the recent NVIDIA Grace Blackwell NVL72 architecture that represents the first product to expand high-bandwidth GPU-specialized interconnects across multiple nodes. We show that while improvements to GPU architecture have led to speedups of over 4.5X across the last few generations of GPUs, advances in interconnect performance have had a larger impact with over 16X performance improvements in time to solution for multi-GPU simulations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels](https://arxiv.org/abs/2511.13764)
*Arun Thangamani,Md Asghar Ahmad Shahid,Adam Siemieniuk,Rolf Morel,Renato Golin,Alexander Heinecke*

Main category: cs.LG

TL;DR: 提出了一种基于MLIR的编译方案，自动生成可扩展的高性能微内核，消除对底层库的依赖，直接生成接近最优的代码。


<details>
  <summary>Details</summary>
Motivation: AI和机器学习工作负载快速发展，但实现接近峰值性能仍需要深厚的硬件专业知识，这增加了复杂性并限制了大多数ML从业者的可扩展性。

Method: 利用MLIR方言桥接领域级操作和处理器能力，通过组合低层IR构造的纳米内核形成针对每个目标的高效微内核。

Result: 实验表明生成的纳米内核具有生产质量，与最先进的微内核库相竞争。

Conclusion: 该编译方案能够自动生成高性能微内核，减少对专家知识和专用库的依赖，提高ML工作负载的可扩展性和易用性。

Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.

</details>


### [21] [Extended Physics Informed Neural Network for Hyperbolic Two-Phase Flow in Porous Media](https://arxiv.org/abs/2511.13734)
*Saif Ur Rehman,Wajid Yousuf*

Main category: cs.LG

TL;DR: 使用扩展物理信息神经网络(XPINN)框架求解具有非凸通量函数的非线性Buckley-Leverett方程，通过动态域分解和Rankine-Hugoniot跳跃条件准确捕捉间断饱和前沿和复合波相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统离散化求解器在求解非线性双曲偏微分方程时计算成本高，标准PINNs难以捕捉陡峭梯度、间断和复杂非线性波相互作用。

Method: 采用XPINN框架，在空间和时间上动态分解计算域为前激波和后激波区域，使用局部子网络学习不同流动行为，通过Rankine-Hugoniot跳跃条件实现子网络耦合。

Result: XPINN方法准确捕捉了间断饱和前沿和复合波相互作用，无需人工扩散或熵修正，相比标准PINNs具有更好的稳定性、更快的收敛速度和更强的非线性波动力学分辨率。

Conclusion: XPINN框架是解决多相流问题中挑战性双曲PDE的有效且可扩展工具，使用更小的领域特定模型和更少的可训练参数实现优越性能。

Abstract: The accurate solution of nonlinear hyperbolic partial differential equations (PDEs) remains a central challenge in computational science due to the presence of steep gradients, discontinuities, and multiscale structures that make conventional discretization-based solvers computationally demanding. Physics-Informed Neural Networks (PINNs) embed the governing equations into the learning process, enabling mesh-free solution of PDEs, yet they often struggle to capture steep gradients, discontinuities, and complex nonlinear wave interactions. To address these limitations, this study employs the Extended Physics-Informed Neural Network (XPINN) framework to solve the nonlinear Buckley-Leverett equation with a nonconvex flux function, which models immiscible two-phase flow in porous media. The computational domain is dynamically decomposed in space and time into evolving pre-shock and post-shock regions, allowing localized subnetworks to efficiently learn distinct flow behaviors. Coupling between subnetworks is achieved through the Rankine-Hugoniot jump condition, which enforces physically consistent flux continuity across the moving shock interface. Numerical experiments demonstrate that the proposed XPINN approach accurately captures discontinuous saturation fronts and compound wave interactions without requiring artificial diffusion or entropy corrections. Compared to standard PINNs, the XPINN framework achieves superior stability, faster convergence, and enhanced resolution of nonlinear wave dynamics using smaller, domain-specific models with fewer trainable parameters, establishing it as an effective and scalable tool for solving challenging hyperbolic PDEs in multiphase flow problems. The code of this work is available on github.com/saifkhanengr/XPINN-for-Buckley-Leverett.

</details>


### [22] [Blurred Encoding for Trajectory Representation Learning](https://arxiv.org/abs/2511.13741)
*Silin Zhou,Yao Chen,Shuo Shang,Lisi Chen,Bingsheng He,Ryosuke Shibasaki*

Main category: cs.LG

TL;DR: BLUE是一种轨迹表示学习方法，通过逐渐降低GPS坐标精度创建多级分层补丁，在保持细粒度时空细节的同时捕获整体旅行模式，在多个下游任务中平均比最佳基线方法提升30.90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA TRL方法将原始GPS轨迹转换为网格或道路轨迹以捕获高级旅行语义，但会丢失细粒度时空细节，因为多个GPS点被分组到单个网格单元或道路段中。

Method: 提出BLUrred Encoding方法，通过逐渐降低GPS坐标精度创建分层补丁；采用编码器-解码器金字塔结构，每个补丁级别使用Transformer学习轨迹嵌入，池化准备更高层输入，上采样为更低层提供指导；使用轨迹重建任务和MSE损失进行训练。

Result: 与8个SOTA TRL方法在3个下游任务上比较，BLUE始终比所有基线获得更高准确率，平均比最佳基线方法提升30.90%。

Conclusion: BLUE通过分层补丁设计有效平衡了细粒度时空细节和整体旅行模式的捕获，在轨迹表示学习任务中表现出色。

Abstract: Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLUrred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90%. Our code is available at https://github.com/slzhou-xy/BLUE.

</details>


### [23] [DeepDefense: Layer-Wise Gradient-Feature Alignment for Building Robust Neural Networks](https://arxiv.org/abs/2511.13749)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出了DeepDefense防御框架，通过梯度特征对齐(GFA)正则化来抑制对抗性漏洞，在多个攻击类型下显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易受到对抗性扰动的攻击，这些微小但精心设计的输入会导致错误预测。

Method: 应用梯度特征对齐(GFA)正则化，将输入梯度与内部特征表示对齐，促进损失景观在切向方向上的平滑性。

Result: 在CIFAR-10上，DeepDefense训练的CNN模型在APGD攻击下比标准对抗训练提升15.2%，在FGSM攻击下提升24.7%。对DeepFool和EADEN等优化攻击，需要20-30倍更高的扰动幅度才能导致误分类。

Conclusion: DeepDefense提供了一种架构无关、易于实现且高效的防御方法，为提升深度学习模型对抗鲁棒性提供了有前景的方向。

Abstract: Deep neural networks are known to be vulnerable to adversarial perturbations, which are small and carefully crafted inputs that lead to incorrect predictions. In this paper, we propose DeepDefense, a novel defense framework that applies Gradient-Feature Alignment (GFA) regularization across multiple layers to suppress adversarial vulnerability. By aligning input gradients with internal feature representations, DeepDefense promotes a smoother loss landscape in tangential directions, thereby reducing the model's sensitivity to adversarial noise.
  We provide theoretical insights into how adversarial perturbation can be decomposed into radial and tangential components and demonstrate that alignment suppresses loss variation in tangential directions, where most attacks are effective. Empirically, our method achieves significant improvements in robustness across both gradient-based and optimization-based attacks. For example, on CIFAR-10, CNN models trained with DeepDefense outperform standard adversarial training by up to 15.2% under APGD attacks and 24.7% under FGSM attacks. Against optimization-based attacks such as DeepFool and EADEN, DeepDefense requires 20 to 30 times higher perturbation magnitudes to cause misclassification, indicating stronger decision boundaries and a flatter loss landscape. Our approach is architecture-agnostic, simple to implement, and highly effective, offering a promising direction for improving the adversarial robustness of deep learning models.

</details>


### [24] [Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk](https://arxiv.org/abs/2511.14682)
*Vaskar Chakma,MD Jaheid Hasan Nerab,Abdur Rouf,Abu Sayed,Hossem MD Saim,Md. Nournabi Khan*

Main category: cs.LG

TL;DR: 本研究系统比较了机器学习方法在吸烟相关健康风险评估中的应用，发现随机森林模型表现最佳（AUC=0.926），关键预测指标包括血压、甘油三酯、肝酶和肾功能指标。


<details>
  <summary>Details</summary>
Motivation: 吸烟是全球主要可预防死因，但现有医疗筛查方法常错过早期预警信号，导致晚期诊断。需要开发能早期识别高风险个体的方法。

Method: 基于55,691人的健康筛查数据，比较随机森林、XGBoost和LightGBM三种算法，使用SHAP分析解释模型预测依据。

Result: 随机森林模型表现最佳（AUC=0.926），SHAP分析显示血压、甘油三酯、肝酶和血清肌酐是最重要的预测指标。

Conclusion: 机器学习模型能有效识别吸烟相关健康风险，关键生物标志物可为早期干预提供临床指导，但本研究为横断面设计，不能预测未来疾病发展。

Abstract: Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.

</details>


### [25] [SCALEX: Scalable Concept and Latent Exploration for Diffusion Models](https://arxiv.org/abs/2511.13750)
*E. Zhixuan Zeng,Yuhao Chen,Alexander Wong*

Main category: cs.LG

TL;DR: SCALEX是一个可扩展的自动化框架，用于探索扩散模型的潜在空间，通过自然语言提示提取语义方向，实现无需重新训练或标注的零样本解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析扩散模型中的社会偏见时存在局限性：要么局限于预定义类别，要么依赖潜在方向的手动解释，这限制了可扩展性并阻碍了发现细微或意外模式。

Method: SCALEX从H空间提取语义有意义的潜在方向，仅使用自然语言提示，实现零样本解释，无需重新训练或标注。

Result: SCALEX能够检测职业提示中的性别偏见，对身份描述符的语义对齐进行排序，并在无监督情况下揭示聚类的概念结构。

Conclusion: 通过直接将提示与潜在方向关联，SCALEX使扩散模型中的偏见分析比先前方法更具可扩展性、可解释性和可扩展性。

Abstract: Image generation models frequently encode social biases, including stereotypes tied to gender, race, and profession. Existing methods for analyzing these biases in diffusion models either focus narrowly on predefined categories or depend on manual interpretation of latent directions. These constraints limit scalability and hinder the discovery of subtle or unanticipated patterns.
  We introduce SCALEX, a framework for scalable and automated exploration of diffusion model latent spaces. SCALEX extracts semantically meaningful directions from H-space using only natural language prompts, enabling zero-shot interpretation without retraining or labelling. This allows systematic comparison across arbitrary concepts and large-scale discovery of internal model associations. We show that SCALEX detects gender bias in profession prompts, ranks semantic alignment across identity descriptors, and reveals clustered conceptual structure without supervision. By linking prompts to latent directions directly, SCALEX makes bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.

</details>


### [26] [Motor Imagery Classification Using Feature Fusion of Spatially Weighted Electroencephalography](https://arxiv.org/abs/2511.13752)
*Abdullah Al Shiam,Md. Khademul Islam Molla,Abu Saleh Musa Miah,Md. Abdus Samad Kamal*

Main category: cs.LG

TL;DR: 提出了一种基于脑区特定通道选择和多域特征融合的脑机接口分类方法，通过脑区通道分组和三种特征提取技术（CSP、模糊C均值、TSM）结合SVM分类器，显著提高了运动想象任务的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 脑电图信号的多通道特性导致计算复杂度高，需要显式信息处理来减少计算负担，同时提高分类准确性。

Method: 基于脑区功能相关性的通道选择，将EEG通道按脑区分组，应用CSP、模糊C均值聚类和TSM三种方法提取空间、聚类和非线性特征，使用SVM进行分类。

Result: 在BCI竞赛数据集IVA和I上分别达到90.77%和84.50%的分类准确率，优于现有方法。

Conclusion: 脑区特定通道选择和多域特征融合方法有效提高了运动想象任务的分类性能，减少了数据维度并提升了计算效率。

Abstract: A Brain Computer Interface (BCI) connects the human brain to the outside world, providing a direct communication channel. Electroencephalography (EEG) signals are commonly used in BCIs to reflect cognitive patterns related to motor function activities. However, due to the multichannel nature of EEG signals, explicit information processing is crucial to lessen computational complexity in BCI systems. This study proposes an innovative method based on brain region-specific channel selection and multi-domain feature fusion to improve classification accuracy. The novelty of the proposed approach lies in region-based channel selection, where EEG channels are grouped according to their functional relevance to distinct brain regions. By selecting channels based on specific regions involved in motor imagery (MI) tasks, this technique eliminates irrelevant channels, reducing data dimensionality and improving computational efficiency. This also ensures that the extracted features are more reflective of the brain actual activity related to motor tasks. Three distinct feature extraction methods Common Spatial Pattern (CSP), Fuzzy C-means clustering, and Tangent Space Mapping (TSM), are applied to each group of channels based on their brain region. Each method targets different characteristics of the EEG signal: CSP focuses on spatial patterns, Fuzzy C means identifies clusters within the data, and TSM captures non-linear patterns in the signal. The combined feature vector is used to classify motor imagery tasks (left hand, right hand, and right foot) using Support Vector Machine (SVM). The proposed method was validated on publicly available benchmark EEG datasets (IVA and I) from the BCI competition III and IV. The results show that the approach outperforms existing methods, achieving classification accuracies of 90.77% and 84.50% for datasets IVA and I, respectively.

</details>


### [27] [Robustness of LLM-enabled vehicle trajectory prediction under data security threats](https://arxiv.org/abs/2511.13753)
*Feilong Wang,Fuqiang Liu*

Main category: cs.LG

TL;DR: 该研究首次系统分析了LLM驱动的车辆轨迹预测模型的对抗脆弱性，发现即使微小的物理合理扰动也能显著破坏模型输出，揭示了LLM在自动驾驶系统中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动驾驶系统中展现出推理和决策能力，但其在安全关键系统中的鲁棒性尚未被探索。本研究旨在填补这一空白，分析LLM车辆轨迹预测模型的对抗脆弱性。

Method: 提出一种单特征差分进化攻击方法，在黑盒设置下扰动周围车辆的单个运动学特征，在highD数据集上进行实验验证。

Result: 实验表明，即使微小且物理合理的扰动也能显著破坏模型输出，揭示了LLM预测器对对抗性操纵的敏感性。进一步分析揭示了准确性与鲁棒性之间的权衡。

Conclusion: 这是首次揭示LLM驱动自动驾驶模型在车辆交互背景下的对抗脆弱性，强调了未来LLM智能交通系统需要以鲁棒性为导向的设计。

Abstract: The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.

</details>


### [28] [Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement](https://arxiv.org/abs/2511.13755)
*Zhe Yang,Wenrui Li,Hongtao Chen,Penghong Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.LG

TL;DR: 提出了RedReg方法，通过自适应冗余调节来解决多模态学习中的模态偏差问题，仅在冗余高时触发干预，并基于跨模态语义估计主导模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中，优势模态往往主导反向传播导致优化不平衡，现有方法存在两个问题：1）主导模态长期支配削弱了表示-输出耦合，导致冗余信息积累；2）直接统一调整优势模态梯度，忽略模态间的语义和方向性。

Method: 基于信息瓶颈原理，构建冗余阶段监控器，使用有效增益增长率和冗余的联合标准触发干预；设计共信息门控机制估计主导模态贡献；将主导模态梯度投影到联合多模态梯度子空间的正交补空间，并根据冗余进行梯度抑制。

Result: 实验表明该方法在大多数场景下优于当前主要方法，消融实验验证了方法的有效性。

Conclusion: RedReg方法能有效平衡多模态优化，自适应调节冗余信息，在保持模态特定信息的同时提高性能。

Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git

</details>


### [29] [Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks](https://arxiv.org/abs/2511.13756)
*Niklas Erdmann,Lars Bentsen,Roy Stenbro,Heine Nygard Riise,Narada Dilp Warakagoda,Paal E. Engelstad*

Main category: cs.LG

TL;DR: 提出了一种基于深度格网络(DLN)的单调约束同时/隐式分位数回归方法，用于时间序列的概率预测，能够生成完整、非参数的累积分布函数(CDF)而无需分位数交叉问题。


<details>
  <summary>Details</summary>
Motivation: 概率预测不仅能提供更多未来预测信息，还能弥补点预测的弱点。传统CDF建模局限于参数化方法，本文旨在连接概率预测和单调网络领域，实现隐式、完整且非参数CDF的预测。

Method: 使用LSTM作为嵌入层，将分位数输入扩展到DLN的所有子格中，利用DLN的单调约束能力防止分位数交叉，实现多时间范围的隐式CDF预测。

Result: 在太阳能辐照度观测的日间小时预测应用中，该方法性能与无约束方法相当或更好，与可扩展单调神经网络相比表现更优。

Conclusion: 通过DLN的适配，旨在促进单调神经网络和概率预测技术之间的交叉研究兴趣。

Abstract: Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.

</details>


### [30] [VitalBench: A Rigorous Multi-Center Benchmark for Long-Term Vital Sign Prediction in Intraoperative Care](https://arxiv.org/abs/2511.13757)
*Xiuding Cai,Xueyao Wang,Sen Wang,Yaoyao Zhu,Jiao Chen,Yu Yao*

Main category: cs.LG

TL;DR: VitalBench是一个专门用于术中生命体征预测的基准测试，包含来自两个独立医疗中心的4000多例手术数据，提供完整数据、不完整数据和跨中心泛化三个评估轨道。


<details>
  <summary>Details</summary>
Motivation: 解决术中生命体征预测领域缺乏标准化基准、数据不完整和跨中心验证有限的问题，以提升患者安全和手术效果。

Method: 构建包含4000多例手术数据的基准测试框架，采用最小化预处理依赖和掩码损失技术，提供三个评估轨道来反映临床实践的真实复杂性。

Result: 开发了VitalBench基准测试平台，为模型开发和比较提供标准化统一平台，确保数据处理的连贯性。

Conclusion: VitalBench为推进术中生命体征预测模型奠定了基础，确保模型不仅准确，而且在多样临床环境中具有鲁棒性和适应性。

Abstract: Intraoperative monitoring and prediction of vital signs are critical for ensuring patient safety and improving surgical outcomes. Despite recent advances in deep learning models for medical time-series forecasting, several challenges persist, including the lack of standardized benchmarks, incomplete data, and limited cross-center validation. To address these challenges, we introduce VitalBench, a novel benchmark specifically designed for intraoperative vital sign prediction. VitalBench includes data from over 4,000 surgeries across two independent medical centers, offering three evaluation tracks: complete data, incomplete data, and cross-center generalization. This framework reflects the real-world complexities of clinical practice, minimizing reliance on extensive preprocessing and incorporating masked loss techniques for robust and unbiased model evaluation. By providing a standardized and unified platform for model development and comparison, VitalBench enables researchers to focus on architectural innovation while ensuring consistency in data handling. This work lays the foundation for advancing predictive models for intraoperative vital sign forecasting, ensuring that these models are not only accurate but also robust and adaptable across diverse clinical environments. Our code and data are available at https://github.com/XiudingCai/VitalBench.

</details>


### [31] [ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space](https://arxiv.org/abs/2511.13758)
*Jun-Hyoung Park,Ho-Jun Song,Seong-Whan Lee*

Main category: cs.LG

TL;DR: ChemFixer是一个基于Transformer的框架，用于将深度学习生成的无效分子修正为有效分子，提高分子有效性同时保持化学和生物学分布特性。


<details>
  <summary>Details</summary>
Motivation: 深度学习分子生成模型经常产生化学无效分子，限制了学习到的化学空间的可使用范围，给实际应用带来挑战。

Method: 基于Transformer架构，使用掩码技术进行预训练，并在构建的大规模有效/无效分子对数据集上进行微调。

Result: ChemFixer提高了分子有效性，有效保持了原始输出的化学和生物分布特性，在数据有限的药物-靶点相互作用预测任务中也表现出色。

Conclusion: ChemFixer有望成为深度学习药物发现各阶段的实用工具，增强分子有效性并扩展可访问的化学空间。

Abstract: Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.

</details>


### [32] [Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection](https://arxiv.org/abs/2511.13759)
*Han Wang,Deyi Ji,Junyu Lu,Lanyun Zhu,Hailong Zhang,Haiyang Wu,Liqun Liu,Peng Shu,Roy Ka-Wei Lee*

Main category: cs.LG

TL;DR: 提出了一种自训练框架，通过协作伪标注利用未标记数据来解决社交媒体冒犯内容检测中的低资源挑战。


<details>
  <summary>Details</summary>
Motivation: 社交媒体冒犯内容检测需要高质量标注数据，但此类数据稀缺，因为冒犯实例出现频率低且人工标注成本高。

Method: 使用轻量级分类器和多智能体视觉语言模型(MA-VLMs)进行迭代伪标注，将未标记数据分为一致未知集和分歧未知集，MA-VLMs模拟主持人和用户双重视角，采用PNU损失优化分类器。

Result: 在基准数据集上的实验表明，该框架在有限监督下显著优于基线方法，并接近大规模模型的性能。

Conclusion: 所提出的自训练框架能有效利用未标记数据，在低资源场景下提升冒犯内容检测性能。

Abstract: Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models

</details>


### [33] [MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm](https://arxiv.org/abs/2511.13760)
*Xiao Fan,Jingyan Jiang,Zhaoru Chen,Fanding Huang,Xiao Chen,Qinting Jiang,Bowen Zhang,Xing Tang,Zhi Wang*

Main category: cs.LG

TL;DR: MoETTA是一个基于熵的测试时自适应框架，采用混合专家架构来处理现实世界中的混合分布偏移问题，通过解耦的专家模型实现多样化的梯度方向适应。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在统一适应路径下无法处理混合分布偏移中不同域间可能冲突的最优梯度方向，且当前基准测试仅关注合成或同质偏移，无法反映现实世界异构混合分布偏移的复杂性。

Method: 提出MoETTA框架，集成混合专家架构，引入结构解耦的专家集合，使模型能够沿不同梯度方向进行适应，实现灵活且解耦的参数更新。

Result: 在三个混合分布偏移设置上的广泛实验表明，MoETTA始终优于强基线方法，建立了最先进的性能，并展示了通过专家级多样性建模多个适应方向的好处。

Conclusion: MoETTA通过混合专家架构有效解决了现实世界混合分布偏移的挑战，证明了多样化适应方向建模的重要性，并为更真实的部署条件提供了新的基准测试。

Abstract: Test-Time adaptation (TTA) has proven effective in mitigating performance drops under single-domain distribution shifts by updating model parameters during inference. However, real-world deployments often involve mixed distribution shifts, where test samples are affected by diverse and potentially conflicting domain factors, posing significant challenges even for SOTA TTA methods. A key limitation in existing approaches is their reliance on a unified adaptation path, which fails to account for the fact that optimal gradient directions can vary significantly across different domains. Moreover, current benchmarks focus only on synthetic or homogeneous shifts, failing to capture the complexity of real-world heterogeneous mixed distribution shifts. To address this, we propose MoETTA, a novel entropy-based TTA framework that integrates the Mixture-of-Experts (MoE) architecture. Rather than enforcing a single parameter update rule for all test samples, MoETTA introduces a set of structurally decoupled experts, enabling adaptation along diverse gradient directions. This design allows the model to better accommodate heterogeneous shifts through flexible and disentangled parameter updates. To simulate realistic deployment conditions, we introduce two new benchmarks: potpourri and potpourri+. While classical settings focus solely on synthetic corruptions, potpourri encompasses a broader range of domain shifts--including natural, artistic, and adversarial distortions--capturing more realistic deployment challenges. Additionally, potpourri+ further includes source-domain samples to evaluate robustness against catastrophic forgetting. Extensive experiments across three mixed distribution shifts settings show that MoETTA consistently outperforms strong baselines, establishing SOTA performance and highlighting the benefit of modeling multiple adaptation directions via expert-level diversity.

</details>


### [34] [Gene Incremental Learning for Single-Cell Transcriptomics](https://arxiv.org/abs/2511.13762)
*Jiaxin Qi,Yan Cui,Jianqiang Huang,Gaogang Xie*

Main category: cs.LG

TL;DR: 该论文首次在单细胞转录组学中提出了基因增量学习框架，将类别增量学习方法应用于基因标记的学习，解决了基因遗忘问题并建立了完整的基准测试。


<details>
  <summary>Details</summary>
Motivation: 虽然类别在计算机视觉的增量学习中已被广泛研究，但标记（如基因）的增量学习研究仍然稀缺。基因在生物学数据中具有整体性特征，这给设计增量学习框架带来了挑战。

Method: 将现有的类别增量学习方法适配到基因增量学习中，以缓解基因遗忘问题，并建立了基因增量学习的流程和评估体系。

Result: 通过大量实验验证了框架设计和评估方法的合理性，以及方法适配的有效性。

Conclusion: 为单细胞转录组学中的基因增量学习提供了一个完整的基准，证明了基因增量学习的可行性和现有方法的有效性。

Abstract: Classes, as fundamental elements of Computer Vision, have been extensively studied within incremental learning frameworks. In contrast, tokens, which play essential roles in many research fields, exhibit similar characteristics of growth, yet investigations into their incremental learning remain significantly scarce. This research gap primarily stems from the holistic nature of tokens in language, which imposes significant challenges on the design of incremental learning frameworks for them. To overcome this obstacle, in this work, we turn to a type of token, gene, for a large-scale biological dataset--single-cell transcriptomics--to formulate a pipeline for gene incremental learning and establish corresponding evaluations. We found that the forgetting problem also exists in gene incremental learning, thus we adapted existing class incremental learning methods to mitigate the forgetting of genes. Through extensive experiments, we demonstrated the soundness of our framework design and evaluations, as well as the effectiveness of our method adaptations. Finally, we provide a complete benchmark for gene incremental learning in single-cell transcriptomics.

</details>


### [35] [PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning](https://arxiv.org/abs/2511.13765)
*Shengjie Sun,Jiafei Lyu,Runze Liu,Mengbei Yan,Bo Liu,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: PROF是一个利用大语言模型从自然语言描述和单个专家轨迹生成可执行奖励函数代码的离线模仿学习框架，通过奖励偏好排名策略自动选择和优化奖励函数。


<details>
  <summary>Details</summary>
Motivation: 现有离线模仿学习方法假设轨迹与专家演示的相似性与奖励正相关，这过于简化了奖励结构。需要更准确地估计未标记数据集的奖励。

Method: 提出PROF框架：1) 使用LLM从自然语言描述和专家轨迹生成奖励函数代码；2) 提出奖励偏好排名(RPR)策略评估奖励函数质量；3) 通过RPR和基于文本的梯度优化交替进行奖励函数选择和优化。

Result: 在D4RL基准测试中，PROF在多个数据集和领域上超越或匹配了最近的强基线方法。

Conclusion: PROF框架有效实现了奖励函数的自动生成和优化，为下游策略学习提供了高质量的奖励函数，在离线模仿学习中表现出色。

Abstract: Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.

</details>


### [36] [Credal Ensemble Distillation for Uncertainty Quantification](https://arxiv.org/abs/2511.13766)
*Kaizheng Wang,Fabio Cuzzolin,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: 提出了一种称为可信集成蒸馏（CED）的新框架，将深度集成压缩为单个模型CREDIT，用于分类任务中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 深度集成在量化预测不确定性和区分其偶然性和认知性成分方面表现出色，但其高计算和内存成本限制了实际部署。

Method: 通过可信集成蒸馏框架，将深度集成压缩为单个模型CREDIT，预测类别概率区间而非单一softmax分布，定义可信集进行不确定性量化。

Result: 在分布外检测基准测试中，CED实现了优于或与现有基线相当的不确定性估计，同时显著降低了与深度集成相比的推理开销。

Conclusion: CED框架有效解决了深度集成的计算和内存成本问题，在保持良好不确定性估计性能的同时大幅提升推理效率。

Abstract: Deep ensembles (DE) have emerged as a powerful approach for quantifying predictive uncertainty and distinguishing its aleatoric and epistemic components, thereby enhancing model robustness and reliability. However, their high computational and memory costs during inference pose significant challenges for wide practical deployment. To overcome this issue, we propose credal ensemble distillation (CED), a novel framework that compresses a DE into a single model, CREDIT, for classification tasks. Instead of a single softmax probability distribution, CREDIT predicts class-wise probability intervals that define a credal set, a convex set of probability distributions, for uncertainty quantification. Empirical results on out-of-distribution detection benchmarks demonstrate that CED achieves superior or comparable uncertainty estimation compared to several existing baselines, while substantially reducing inference overhead compared to DE.

</details>


### [37] [MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts](https://arxiv.org/abs/2511.14102)
*Wenfeng Wang,Jiacheng Liu,Xiaofeng Hou,Xinfeng Xia,Peng Tang,Mingxuan Zhang,Chao Li,Minyi Guo*

Main category: cs.LG

TL;DR: MoE-SpeQ是一个新的推理系统，通过推测执行和专家卸载的协同设计，解决了MoE模型推理中的I/O瓶颈问题，在内存受限设备上实现了最高2.34倍的加速。


<details>
  <summary>Details</summary>
Motivation: 解决最先进MoE模型推理时因内存需求过大而需要将专家卸载到主机内存，但由此产生的PCIe总线I/O瓶颈严重影响了性能的问题。

Method: 使用小型设备端草稿模型预测未来token所需的专家序列，通过运行时编排器预取这些专家，将昂贵的I/O与有用计算重叠，并从关键路径中隐藏延迟。采用自适应调节器根据摊销屋顶模型动态调整推测策略。

Result: 在内存受限设备上，对于Phi-MoE模型，MoE-SpeQ相比最先进的卸载框架实现了最高2.34倍的加速。

Conclusion: 为在资源受限环境中管理数据相关内存访问建立了一种新的原则性方法，使MoE推理在普通硬件上更加可行。

Abstract: The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.
  This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.

</details>


### [38] [Dynamic Temperature Scheduler for Knowledge Distillation](https://arxiv.org/abs/2511.13767)
*Sibgat Ul Islam,Jawad Ibn Ahad,Fuad Rahman,Mohammad Ruhul Amin,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 提出动态温度调度器(DTS)，根据师生模型之间的交叉熵损失差距动态调整知识蒸馏中的温度参数，解决了传统固定温度方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏使用固定温度参数，无法适应训练不同阶段的需求，且师生模型架构差异导致logit幅度不匹配。研究发现学生在训练早期需要更soft的概率，后期需要更sharp的概率。

Method: 引入动态温度调度器(DTS)，基于师生模型分布之间的交叉熵损失差距来动态调整温度参数，无需额外超参数调整。

Result: 在视觉任务(CIFAR-100、Tiny-ImageNet)和NLP任务(GLUE、Dolly等)上验证，DTS在多种知识蒸馏策略中均优于静态温度基线。

Conclusion: DTS是首个基于师生分布差异的自适应温度调度方法，能够无缝集成到现有知识蒸馏框架中，显著提升性能。

Abstract: Knowledge Distillation (KD) trains a smaller student model using a large, pre-trained teacher model, with temperature as a key hyperparameter controlling the softness of output probabilities. Traditional methods use a fixed temperature throughout training, which is suboptimal. Moreover, architectural differences between teacher and student often result in mismatched logit magnitudes. We demonstrate that students benefit from softer probabilities early in training but require sharper probabilities in later stages. We introduce Dynamic Temperature Scheduler (DTS), which adjusts temperature dynamically based on the cross-entropy loss gap between teacher and student. To our knowledge, this is the first temperature scheduling method that adapts based on the divergence between teacher and student distributions. Our method integrates seamlessly with existing KD frameworks. We validate DTS across multiple KD strategies on vision (CIFAR-100, Tiny-ImageNet) and NLP tasks (GLUE, Dolly, SelfIns, UnNI, S-NI), consistently outperforming static-temperature baselines. Code is available at https://github.com/Sibgat-Ul/DTS.

</details>


### [39] [\textit{FLARE}: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning](https://arxiv.org/abs/2511.14715)
*Abolfazl Younesi,Leon Kiss,Zahra Najafabadi Samani,Juan Aznar Poveda,Thomas Fahringer*

Main category: cs.LG

TL;DR: FLARE是一个自适应信誉联邦学习框架，通过多维信誉评分、自适应阈值和信誉加权聚合来防御恶意客户端攻击，在多种攻击场景下保持高模型精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习防御机制依赖静态阈值和二元分类，无法适应现实部署中客户端行为的动态变化，容易受到拜占庭攻击、数据投毒和自适应对抗行为的威胁。

Method: FLARE框架包含：多维信誉评分系统、自校准自适应阈值机制、信誉加权聚合与软排除策略，以及基于本地差分隐私的信誉评分机制。

Result: 在MNIST、CIFAR-10和SVHN数据集上的实验表明，FLARE在多种攻击类型下比现有方法鲁棒性提升高达16%，模型收敛速度更快，检测性能强且计算开销小。

Conclusion: FLARE通过连续多维信任评估有效防御联邦学习中的恶意客户端攻击，在保持模型性能的同时显著提升系统安全性。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE

</details>


### [40] [Compiling to linear neurons](https://arxiv.org/abs/2511.13769)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.LG

TL;DR: Cajal是一个类型化、高阶线性编程语言，旨在直接编程神经网络，通过编译为线性神经元使离散算法与基于梯度的学习兼容。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络编程依赖间接学习算法（如梯度下降），缺乏离散结构，无法将离散算法编译到神经网络中，限制了网络的学习能力。

Method: 开发Cajal编程语言，证明其程序可编译为线性神经元，实现离散算法的可微分表达，并与神经网络链接。

Result: 实验表明，通过Cajal链接线性神经元可使网络学习更快、数据效率更高、更易调试。

Conclusion: 线性编程语言为直接编程神经网络提供了路径，实现了学习与普通编程离散结构的丰富交互。

Abstract: We don't program neural networks directly. Instead, we rely on an indirect style where learning algorithms, like gradient descent, determine a neural network's function by learning from data. This indirect style is often a virtue; it empowers us to solve problems that were previously impossible. But it lacks discrete structure. We can't compile most algorithms into a neural network -- even if these algorithms could help the network learn. This limitation occurs because discrete algorithms are not obviously differentiable, making them incompatible with the gradient-based learning algorithms that determine a neural network's function. To address this, we introduce $\textsf{Cajal}$: a typed, higher-order and linear programming language intended to be a minimal vehicle for exploring a direct style of programming neural networks. We prove $\textsf{Cajal}$ programs compile to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation of $\textsf{Cajal}$, we conduct several experiments where we link these linear neurons against other neural networks to determine part of their function prior to learning. Linking with these neurons allows networks to learn faster, with greater data-efficiency, and in a way that's easier to debug. A key lesson is that linear programming languages provide a path towards directly programming neural networks, enabling a rich interplay between learning and the discrete structures of ordinary programming.

</details>


### [41] [Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture](https://arxiv.org/abs/2511.13780)
*Nihal Mehta*

Main category: cs.LG

TL;DR: 本文从分布语义学角度为自注意力机制提供了数学解释，证明其源于将语料库级共现统计投影到序列上下文中，Transformer架构的代数形式是该投影原则的自然结果而非任意设计选择。


<details>
  <summary>Details</summary>
Motivation: 为自注意力机制提供理论基础，证明Transformer架构的设计选择源于分布语义学原理而非随意设计，从而建立更深刻的理论理解。

Method: 从GloVe嵌入的共现矩阵出发，展示投影如何自然捕获上下文影响，查询-键-值机制作为建模方向关系的自然非对称扩展，位置编码和多头注意力作为同一投影原则的结构化改进。

Result: 证明了自注意力机制从语料库级共现统计投影到序列上下文中自然产生，Transformer架构的特定代数形式遵循这些投影原则。

Conclusion: 自注意力机制和Transformer架构的设计选择具有坚实的数学基础，源于分布语义学的投影原则，这为理解神经网络架构提供了新的理论视角。

Abstract: This paper presents a mathematical interpretation of self-attention by connecting it to distributional semantics principles. We show that self-attention emerges from projecting corpus-level co-occurrence statistics into sequence context. Starting from the co-occurrence matrix underlying GloVe embeddings, we demonstrate how the projection naturally captures contextual influence, with the query-key-value mechanism arising as the natural asymmetric extension for modeling directional relationships. Positional encodings and multi-head attention then follow as structured refinements of this same projection principle. Our analysis demonstrates that the Transformer architecture's particular algebraic form follows from these projection principles rather than being an arbitrary design choice.

</details>


### [42] [Exploring Transferability of Self-Supervised Learning by Task Conflict Calibration](https://arxiv.org/abs/2511.13787)
*Huijie Guo,Jingyao Wang,Peizheng Guo,Xingchen Shen,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 本文提出TC²方法，通过任务冲突校准来提升自监督学习的表示可迁移性，采用两阶段双层优化框架解决任务冲突问题。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习的表示可迁移性，解决任务冲突对可迁移性的阻碍问题。

Method: 提出任务冲突校准(TC²)方法：1) 在批次内构建多个SSL任务注入任务级信息；2) 使用因子提取网络生成因果生成因子，权重提取网络为每个样本分配专用权重；3) 通过数据重构、正交性和稀疏性确保有效性；4) 采用两阶段双层优化框架进行训练。

Result: 在多个下游任务上的实验结果表明，该方法能持续提升SSL模型的可迁移性。

Conclusion: TC²方法通过显式建模任务冲突并校准样本表示，有效提升了自监督学习的表示可迁移性。

Abstract: In this paper, we explore the transferability of SSL by addressing two central questions: (i) what is the representation transferability of SSL, and (ii) how can we effectively model this transferability? Transferability is defined as the ability of a representation learned from one task to support the objective of another.
  Inspired by the meta-learning paradigm, we construct multiple SSL tasks within each training batch to support explicitly modeling transferability. Based on empirical evidence and causal analysis, we find that although introducing task-level information improves transferability, it is still hindered by task conflict. To address this issue, we propose a Task Conflict Calibration (TC$^2$) method to alleviate the impact of task conflict. Specifically, it first splits batches to create multiple SSL tasks, infusing task-level information. Next, it uses a factor extraction network to produce causal generative factors for all tasks and a weight extraction network to assign dedicated weights to each sample, employing data reconstruction, orthogonality, and sparsity to ensure effectiveness. Finally, TC$^2$ calibrates sample representations during SSL training and integrates into the pipeline via a two-stage bi-level optimization framework to boost the transferability of learned representations. Experimental results on multiple downstream tasks demonstrate that our method consistently improves the transferability of SSL models.

</details>


### [43] [Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments](https://arxiv.org/abs/2511.13788)
*Samuel Nathanson,Rebecca Williams,Cynthia Matuszek*

Main category: cs.LG

TL;DR: 研究发现大型语言模型在对抗性交互中，攻击者与目标模型的大小比例与有害行为发生率呈正相关，模型规模不对称影响对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体和安全关键环境中应用增多，需要了解模型在对抗性交互中的脆弱性如何随规模变化，特别是大型模型是否能够系统性地绕过小型模型的安全防护。

Method: 使用JailbreakBench标准化对抗任务，模拟超过6000次多轮攻击者-目标模型交互，涵盖0.6B-120B参数的不同规模LLM，通过三个独立LLM法官评估危害分数和拒绝行为。

Result: 攻击者与目标模型大小比例的对数与平均危害分数呈强正相关（Pearson r=0.51），攻击者行为多样性对对抗结果影响大于目标易感性，攻击者拒绝频率与危害呈强负相关（rho=-0.93）。

Conclusion: 模型规模不对称影响鲁棒性，攻击者侧的对齐机制能有效减轻有害响应，这为研究模型间对齐和安全性提供了探索性证据。

Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.

</details>


### [44] [ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design](https://arxiv.org/abs/2511.13809)
*Emanuel Covaci,Fabian Galis,Radu Balan,Daniela Zaharie,Darian Onchis*

Main category: cs.LG

TL;DR: 提出一种可微分的全局可解释性方法，通过将特征重要性估计直接集成到模型训练中，使用ScoresActivation函数实现特征排序，在保持高预测性能的同时提供忠实、稳定的特征重要性排名。


<details>
  <summary>Details</summary>
Motivation: 当前的事后解释方法与模型训练过程脱节，限制了其忠实性和实用性，需要一种将可解释性直接融入训练过程的方法。

Method: 提出ScoresActivation函数作为特征排序机制嵌入学习流程，使模型能够以可微分和端到端可训练的方式根据特征对预测性能的贡献进行优先级排序。

Result: 在基准数据集上的评估显示，该方法产生与SHAP值和真实特征重要性一致的全局忠实、稳定的特征排名，特征评分速度比SHAP快150倍，同时提高了分类准确率，对无关输入具有鲁棒性。

Conclusion: 这项工作弥合了模型准确性和可解释性之间的差距，为固有可解释机器学习提供了一个可扩展的框架。

Abstract: Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.

</details>


### [45] [Beat the long tail: Distribution-Aware Speculative Decoding for RL Training](https://arxiv.org/abs/2511.13841)
*Zelei Shao,Vikranth Srivatsa,Sanjana Srivastava,Qingyang Wu,Alpay Ariyak,Xiaoxia Wu,Ameen Patel,Jue Wang,Percy Liang,Tri Dao,Ce Zhang,Yiying Zhang,Ben Athiwaratkun,Chenfeng Xu,Junxiong Wang*

Main category: cs.LG

TL;DR: DAS是一个分布感知的推测解码框架，通过利用历史rollout数据构建自适应草稿器，加速RL后训练中的rollout阶段，可减少50%的rollout时间而不改变模型输出。


<details>
  <summary>Details</summary>
Motivation: RL后训练中rollout阶段的效率受到长轨迹生成的限制，特别是长尾分布中少数长生成主导了计算时间。同时历史rollout数据揭示了稳定的提示级别模式。

Method: DAS整合了两个关键思想：基于最近rollout构建的自适应非参数草稿器（使用增量维护的后缀树），以及长度感知的推测策略，为主导计算时间的长轨迹分配更激进的草稿预算。

Result: 在数学和代码推理任务上的实验表明，DAS将rollout时间减少高达50%，同时保持相同的训练曲线。

Conclusion: 分布感知的推测解码可以显著加速RL后训练，而不会影响学习质量。

Abstract: Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.

</details>


### [46] [AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection](https://arxiv.org/abs/2511.13880)
*Saleh Momeni,Changnan Xiao,Bing Liu*

Main category: cs.LG

TL;DR: 提出AnaCP方法解决类增量学习中的特征适应问题，在保持分析分类器效率的同时实现增量特征适应，无需基于梯度的训练，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习方法存在灾难性遗忘问题，而基于预训练模型的方法虽然高效但无法持续适应特征表示，导致性能次优。

Method: AnaCP（分析对比投影）方法，通过对比投影实现增量特征适应，同时保持分析分类器的效率，无需梯度更新。

Result: 实验表明AnaCP不仅优于现有基线方法，而且达到了联合训练的准确率水平，这是类增量学习的理论上限。

Conclusion: AnaCP成功解决了类增量学习中的特征适应问题，在避免灾难性遗忘的同时实现了接近理论上限的性能。

Abstract: This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.

</details>


### [47] [Tractable Probabilistic Models for Investment Planning](https://arxiv.org/abs/2511.13888)
*Nicolas M. Cuadrado A.,Mohannad Takrouri,Jiří Němeček,Martin Takáč,Jakub Mareček*

Main category: cs.LG

TL;DR: 提出使用可处理概率模型（TPMs）特别是和积网络（SPNs）来改进电力系统长期投资规划中的不确定性建模，替代传统的有限场景方法。


<details>
  <summary>Details</summary>
Motivation: 电力系统投资规划需要数十年预测，面临深度不确定性。传统有限场景方法限制了洞察场景特定波动性并阻碍稳健决策。

Method: 使用可处理概率模型（TPMs）特别是和积网络（SPNs），支持关键量的精确可扩展推理，包括场景似然、边际和条件概率。

Result: 通过代表性电力系统规划案例研究证明该方法在计算和可靠性方面优于传统基于场景的模型。

Conclusion: TPMs框架支持将机会约束优化直接嵌入投资规划，以规定置信水平强制执行安全性或可靠性要求，实现场景分析和波动性量化。

Abstract: Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.
  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the approach's effectiveness through a representative power system planning case study, illustrating computational and reliability advantages over traditional scenario-based models.

</details>


### [48] [Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis](https://arxiv.org/abs/2511.13893)
*Kai Chen,Chen Gong,Tianhao Wang*

Main category: cs.LG

TL;DR: MargNet是一个用于差分隐私表格数据合成的神经网络方法，通过将统计模型的边际选择策略融入神经网络，在密集相关数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为统计模型优于神经网络方法，但这种结论忽略了密集相关数据集的挑战。神经网络具有拟合复杂分布的能力，但现有方法仍有局限，需要结合统计模型的成功设计。

Method: 提出MargNet方法，将统计模型的边际选择策略融入神经网络训练。采用自适应边际选择策略，训练神经网络生成符合选定边际的数据。

Result: 在稀疏相关数据集上，性能接近最佳统计方法，速度提升7倍；在密集相关数据集上，保真度误差比之前最佳方法降低26%，达到新的最先进水平。

Conclusion: MargNet成功结合了统计模型和神经网络的优势，在复杂数据集上显著提升了差分隐私数据合成的性能，证明了神经网络在密集相关场景下的优越性。

Abstract: In differentially private (DP) tabular data synthesis, the consensus is that statistical models are better than neural network (NN)-based methods. However, we argue that this conclusion is incomplete and overlooks the challenge of densely correlated datasets, where intricate dependencies can overwhelm statistical models. In such complex scenarios, neural networks are more suitable due to their capacity to fit complex distributions by learning directly from samples. Despite this potential, existing NN-based algorithms still suffer from significant limitations. We therefore propose MargNet, incorporating successful algorithmic designs of statistical models into neural networks. MargNet applies an adaptive marginal selection strategy and trains the neural networks to generate data that conforms to the selected marginals. On sparsely correlated datasets, our approach achieves utility close to the best statistical method while offering an average 7$\times$ speedup over it. More importantly, on densely correlated datasets, MargNet establishes a new state-of-the-art, reducing fidelity error by up to 26\% compared to the previous best. We release our code on GitHub.\footnote{https://github.com/KaiChen9909/margnet}

</details>


### [49] [Weather Maps as Tokens: Transformers for Renewable Energy Forecasting](https://arxiv.org/abs/2511.13935)
*Federico Battini*

Main category: cs.LG

TL;DR: 提出了一种将天气图作为transformer序列中的token来预测可再生能源的新方法，通过卷积神经网络编码空间信息，transformer捕捉时间动态，相比现有方法显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能有效整合天气模式的空间背景与时间演化，而准确的可再生能源预测对于减少化石燃料依赖和实现电网脱碳至关重要。

Method: 使用轻量级卷积神经网络将每小时天气图编码为空间token，然后通过transformer处理这些token来捕捉45小时预测范围内的时序动态。

Result: 与ENTSO-E运营预测相比，风能和太阳能的RMSE分别降低了约60%和20%。

Conclusion: 该方法通过将天气图作为token处理，成功整合了空间和时间信息，显著提高了可再生能源预测的准确性。

Abstract: Accurate renewable energy forecasting is essential to reduce dependence on fossil fuels and enabling grid decarbonization. However, current approaches fail to effectively integrate the rich spatial context of weather patterns with their temporal evolution. This work introduces a novel approach that treats weather maps as tokens in transformer sequences to predict renewable energy. Hourly weather maps are encoded as spatial tokens using a lightweight convolutional neural network, and then processed by a transformer to capture temporal dynamics across a 45-hour forecast horizon. Despite disadvantages in input initialization, evaluation against ENTSO-E operational forecasts shows a reduction in RMSE of about 60\% and 20\% for wind and solar respectively. A live dashboard showing daily forecasts is available at: https://www.sardiniaforecast.ifabfoundation.it.

</details>


### [50] [Complex-Weighted Convolutional Networks: Provable Expressiveness via Complex Diffusion](https://arxiv.org/abs/2511.13937)
*Cristina López Amado,Tassilo Schwarz,Yu Tian,Renaud Lambiotte*

Main category: cs.LG

TL;DR: 提出了一种基于复数权重扩散的图神经网络框架，通过为边分配复数权重来增强模型表达能力，解决了GNN的过平滑问题并在异配图上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统GNN存在过平滑问题，在异配图上表现不佳，需要一种更强大的扩散机制来增强模型表达能力。

Method: 引入复数权重结构，将随机游走扩展到复数域，提出Complex-Weighted Convolutional Network (CWCN)，直接从数据中学习复数权重结构。

Result: CWCN在基准数据集上取得了有竞争力的性能，无需额外超参数，实现简单。

Conclusion: 复数权重扩散为增强GNN表达能力提供了理论基础和实用机制，开辟了理论严谨且实际有效的新模型方向。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications, yet they remain limited by oversmoothing and poor performance on heterophilic graphs. To address these challenges, we introduce a novel framework that equips graphs with a complex-weighted structure, assigning each edge a complex number to drive a diffusion process that extends random walks into the complex domain. We prove that this diffusion is highly expressive: with appropriately chosen complex weights, any node-classification task can be solved in the steady state of a complex random walk. Building on this insight, we propose the Complex-Weighted Convolutional Network (CWCN), which learns suitable complex-weighted structures directly from data while enriching diffusion with learnable matrices and nonlinear activations. CWCN is simple to implement, requires no additional hyperparameters beyond those of standard GNNs, and achieves competitive performance on benchmark datasets. Our results demonstrate that complex-weighted diffusion provides a principled and general mechanism for enhancing GNN expressiveness, opening new avenues for models that are both theoretically grounded and practically effective.

</details>


### [51] [The Impact of Bootstrap Sampling Rate on Random Forest Performance in Regression Tasks](https://arxiv.org/abs/2511.13952)
*Michał Iwaniuk,Mateusz Jarosz,Bartłomiej Borycki,Bartosz Jezierski,Jan Cwalina,Stanisław Kaźmierczak,Jacek Mańdziuk*

Main category: cs.LG

TL;DR: 研究表明，随机森林中调整bootstrap rate(BR)超参数可以显著提升模型性能，最佳BR值取决于数据集特性：全局特征-目标关系强的数据集适合高BR，局部目标方差高的数据集适合低BR。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林默认使用BR=1.0，但缺乏对BR超参数影响的系统性研究，需要探索不同BR值对模型性能的影响及其与数据集特性的关系。

Method: 在39个异质回归数据集和16种RF配置上，系统性地测试BR从0.2到5.0的变化，使用重复二折交叉验证和均方误差评估性能，并在合成数据集上验证噪声水平与BR的关系。

Result: 调优BR能显著改善性能：24个数据集最佳BR≤1.0，15个数据集最佳BR>1.0，仅4个数据集BR=1.0最优。数据集特性与BR偏好相关：全局关系强的偏好高BR，局部方差高的偏好低BR。

Conclusion: BR是一个重要的超参数，应根据数据集特性进行调优以优化随机森林回归模型，体现了偏差-方差权衡：低噪声场景高BR减少偏差，高噪声场景低BR减少方差。

Abstract: Random Forests (RFs) typically train each tree on a bootstrap sample of the same size as the training set, i.e., bootstrap rate (BR) equals 1.0. We systematically examine how varying BR from 0.2 to 5.0 affects RF performance across 39 heterogeneous regression datasets and 16 RF configurations, evaluating with repeated two-fold cross-validation and mean squared error. Our results demonstrate that tuning the BR can yield significant improvements over the default: the best setup relied on BR \leq 1.0 for 24 datasets, BR > 1.0 for 15, and BR = 1.0 was optimal in 4 cases only. We establish a link between dataset characteristics and the preferred BR: datasets with strong global feature-target relationships favor higher BRs, while those with higher local target variance benefit from lower BRs. To further investigate this relationship, we conducted experiments on synthetic datasets with controlled noise levels. These experiments reproduce the observed bias-variance trade-off: in low-noise scenarios, higher BRs effectively reduce model bias, whereas in high-noise settings, lower BRs help reduce model variance. Overall, BR is an influential hyperparameter that should be tuned to optimize RF regression models.

</details>


### [52] [Efficient reconstruction of multidimensional random field models with heterogeneous data using stochastic neural networks](https://arxiv.org/abs/2511.13977)
*Mingtao Xia,Qijing Shen*

Main category: cs.LG

TL;DR: 本文分析了基于Wasserstein距离训练随机神经网络重建多维随机场模型的可扩展性，证明了在有限训练数据下的泛化误差界，部分缓解了维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 研究多维随机场模型重建中的维度灾难问题，探索在噪声异质分布情况下如何提高随机神经网络训练的效率和鲁棒性。

Method: 改进基于Wasserstein距离的随机神经网络训练方法，证明在有限训练数据下的泛化误差界，并进行多维不确定性量化任务的数值实验验证。

Result: 当噪声在不同维度间异质分布时，泛化误差的收敛率可能不显式依赖于模型维度，部分缓解了维度灾难问题。数值实验表明该方法能成功训练随机神经网络学习多维不确定性模型。

Conclusion: 基于Wasserstein距离的方法能够有效训练随机神经网络重建多维随机场模型，在异质噪声情况下部分克服维度灾难，展示了该方法的鲁棒性和实用性。

Abstract: In this paper, we analyze the scalability of a recent Wasserstein-distance approach for training stochastic neural networks (SNNs) to reconstruct multidimensional random field models. We prove a generalization error bound for reconstructing multidimensional random field models on training stochastic neural networks with a limited number of training data. Our results indicate that when noise is heterogeneous across dimensions, the convergence rate of the generalization error may not depend explicitly on the model's dimensionality, partially alleviating the "curse of dimensionality" for learning multidimensional random field models from a finite number of data points. Additionally, we improve the previous Wasserstein-distance SNN training approach and showcase the robustness of the SNN. Through numerical experiments on different multidimensional uncertainty quantification tasks, we show that our Wasserstein-distance approach can successfully train stochastic neural networks to learn multidimensional uncertainty models.

</details>


### [53] [Data Whitening Improves Sparse Autoencoder Learning](https://arxiv.org/abs/2511.13981)
*Ashwin Saraswatula,David Klindt*

Main category: cs.LG

TL;DR: PCA Whitening预处理技术能改善稀疏自编码器(SAE)的性能，通过优化训练过程提升特征可解释性，尽管重建质量略有下降。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器训练中的输入数据相关性导致优化困难，需要改进训练方法来提升特征可解释性。

Method: 应用PCA Whitening预处理输入激活，通过理论分析和模拟验证其对优化地形的改善作用，并在SAEBench基准上评估ReLU和Top-K SAE。

Result: 白化处理一致改善了可解释性指标（稀疏探测精度和特征解缠），尽管重建质量有轻微下降。

Conclusion: 白化应作为SAE训练的默认预处理步骤，特别是在优先考虑可解释性而非完美重建时。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.

</details>


### [54] [Node-Level Uncertainty Estimation in LLM-Generated SQL](https://arxiv.org/abs/2511.13984)
*Hilaf Hasson,Ruocheng Guo*

Main category: cs.LG

TL;DR: 提出了一个检测LLM生成SQL错误的实用框架，通过在查询抽象语法树(AST)的节点级别估计不确定性来实现。该方法包含语义感知标签算法和基于丰富特征的监督分类器，能精确定位查询中的错误位置。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常使用token对数概率作为不确定性度量，但这种方法在检测SQL错误时效果有限，无法提供细粒度的错误定位信息。需要一种能精确识别SQL查询中具体错误位置的更有效方法。

Method: 采用两阶段方法：1) 语义感知标签算法，为AST节点分配正确性标签；2) 使用模式感知和词汇特征的监督分类器预测每个节点的错误概率。特征包括标识符有效性、别名解析、类型兼容性、范围模糊性和拼写错误信号。

Result: 在多个数据库和数据集上的实验表明，该方法显著优于token对数概率方法：平均AUC提高了27.44%，在跨数据库评估中保持鲁棒性。

Conclusion: 节点中心、语义基础的不确定性估计是聚合序列级别置信度度量的强大且可解释的替代方案，支持目标修复、人机协同审查和下游选择性执行。

Abstract: We present a practical framework for detecting errors in LLM-generated SQL by estimating uncertainty at the level of individual nodes in the query's abstract syntax tree (AST). Our approach proceeds in two stages. First, we introduce a semantically aware labeling algorithm that, given a generated SQL and a gold reference, assigns node-level correctness without over-penalizing structural containers or alias variation. Second, we represent each node with a rich set of schema-aware and lexical features - capturing identifier validity, alias resolution, type compatibility, ambiguity in scope, and typo signals - and train a supervised classifier to predict per-node error probabilities. We interpret these probabilities as calibrated uncertainty, enabling fine-grained diagnostics that pinpoint exactly where a query is likely to be wrong. Across multiple databases and datasets, our method substantially outperforms token log-probabilities: average AUC improves by +27.44% while maintaining robustness under cross-database evaluation. Beyond serving as an accuracy signal, node-level uncertainty supports targeted repair, human-in-the-loop review, and downstream selective execution. Together, these results establish node-centric, semantically grounded uncertainty estimation as a strong and interpretable alternative to aggregate sequence level confidence measures.

</details>


### [55] [On the Gradient Complexity of Private Optimization with Private Oracles](https://arxiv.org/abs/2511.13999)
*Michael Menart,Aleksandar Nikolov*

Main category: cs.LG

TL;DR: 本文研究了差分隐私下Lipschitz凸损失函数经验/总体风险最小化的运行时间下界，揭示了隐私优化器相比非隐私方法需要支付维度相关的运行时代价。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私优化算法的运行时间下界，理解隐私保护对优化效率的影响，特别是维度依赖的惩罚。

Method: 通过理论分析建立下界：对于非光滑损失，考虑私有代理预言机；对于光滑损失，仅假设优化器是私有的；还研究了信息受限预言机下的限制。

Result: 证明了多个紧下界：非光滑情况下Ω(min{√d/α², d/log(1/α)})，光滑情况下Ω̃(√d/α + min{1/α², n})，信息受限情况下Ω(min{d/(α²Γ), d/log(1/α)})。

Conclusion: 差分隐私优化器需要支付维度相关的运行时惩罚，梯度量化技术在优化中存在根本性限制。

Abstract: We study the running time, in terms of first order oracle queries, of differentially private empirical/population risk minimization of Lipschitz convex losses. We first consider the setting where the loss is non-smooth and the optimizer interacts with a private proxy oracle, which sends only private messages about a minibatch of gradients. In this setting, we show that expected running time $Ω(\min\{\frac{\sqrt{d}}{α^2}, \frac{d}{\log(1/α)}\})$ is necessary to achieve $α$ excess risk on problems of dimension $d$ when $d \geq 1/α^2$. Upper bounds via DP-SGD show these results are tight when $d>\tildeΩ(1/α^4)$. We further show our lower bound can be strengthened to $Ω(\min\{\frac{d}{\bar{m}α^2}, \frac{d}{\log(1/α)} \})$ for algorithms which use minibatches of size at most $\bar{m} < \sqrt{d}$. We next consider smooth losses, where we relax the private oracle assumption and give lower bounds under only the condition that the optimizer is private. Here, we lower bound the expected number of first order oracle calls by $\tildeΩ\big(\frac{\sqrt{d}}α + \min\{\frac{1}{α^2}, n\}\big)$, where $n$ is the size of the dataset. Modifications to existing algorithms show this bound is nearly tight. Compared to non-private lower bounds, our results show that differentially private optimizers pay a dimension dependent runtime penalty. Finally, as a natural extension of our proof technique, we show lower bounds in the non-smooth setting for optimizers interacting with information limited oracles. Specifically, if the proxy oracle transmits at most $Γ$-bits of information about the gradients in the minibatch, then $Ω\big(\min\{\frac{d}{α^2Γ}, \frac{d}{\log(1/α)}\}\big)$ oracle calls are needed. This result shows fundamental limitations of gradient quantization techniques in optimization.

</details>


### [56] [How to Marginalize in Causal Structure Learning?](https://arxiv.org/abs/2511.14001)
*William Zhao,Guy Van den Broeck,Benjie Wang*

Main category: cs.LG

TL;DR: 提出了一种使用可处理概率电路的新方法，用于贝叶斯网络结构学习中的边缘化计算，相比传统动态规划方法能提高性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络结构学习需要推断可能的DAG后验分布，传统方法使用动态规划限制节点可能的父节点集，这限制了学习能力。

Method: 利用可处理概率电路来绕过传统限制，开发了新的学习例程，在原始分布和边缘查询上训练这些电路，利用电路架构实现快速精确的边缘化。

Result: 经验证明，使用该方法回答边缘查询能让贝叶斯结构学习器相比当前方法提升性能。

Conclusion: 概率电路为贝叶斯网络结构学习提供了一种有效的边缘化计算方法，能够改善学习性能。

Abstract: Bayesian networks (BNs) are a widely used class of probabilistic graphical models employed in numerous application domains. However, inferring the network's graphical structure from data remains challenging. Bayesian structure learners approach this problem by inferring a posterior distribution over the possible directed acyclic graphs underlying the BN. The inference process often requires marginalizing over probability distributions, which is typically done using dynamic programming methods that restrict the set of possible parents for each node. Instead, we present a novel method that utilizes tractable probabilistic circuits to circumvent this restriction. This method utilizes a new learning routine that trains these circuits on both the original distribution and marginal queries. The architecture of probabilistic circuits then inherently allows for fast and exact marginalization on the learned distribution. We then show empirically that utilizing our method to answer marginals allows Bayesian structure learners to improve their performance compared to current methods.

</details>


### [57] [Certified but Fooled! Breaking Certified Defences with Ghost Certificates](https://arxiv.org/abs/2511.14003)
*Quoc Viet Vo,Tashreque M. Haq,Paul Montague,Tamas Abraham,Ehsan Abbasnejad,Damith C. Ranasinghe*

Main category: cs.LG

TL;DR: 该论文研究如何利用概率认证框架的漏洞，通过微小且难以察觉的扰动来欺骗认证防御系统，使其为对抗性输入生成虚假的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 研究认证防御系统的局限性，探索即使需要误导分类器并操纵认证过程生成虚假鲁棒性保证时，所需扰动是否仍可保持微小和难以察觉。

Method: 采用区域聚焦对抗样本方法，精心设计微小扰动来欺骗认证模型，使其为目标类别生成虚假的大鲁棒性半径证书。

Result: 在ImageNet数据集上的广泛评估表明，该方法能有效绕过Densepure等最先进的认证防御系统，实现比源类别幽灵证书更大的认证半径。

Conclusion: 这项工作强调了需要更好地理解鲁棒性认证方法的局限性，现有认证框架存在被恶意利用的风险。

Abstract: Certified defenses promise provable robustness guarantees. We study the malicious exploitation of probabilistic certification frameworks to better understand the limits of guarantee provisions. Now, the objective is to not only mislead a classifier, but also manipulate the certification process to generate a robustness guarantee for an adversarial input certificate spoofing. A recent study in ICLR demonstrated that crafting large perturbations can shift inputs far into regions capable of generating a certificate for an incorrect class. Our study investigates if perturbations needed to cause a misclassification and yet coax a certified model into issuing a deceptive, large robustness radius for a target class can still be made small and imperceptible. We explore the idea of region-focused adversarial examples to craft imperceptible perturbations, spoof certificates and achieve certification radii larger than the source class ghost certificates. Extensive evaluations with the ImageNet demonstrate the ability to effectively bypass state-of-the-art certified defenses such as Densepure. Our work underscores the need to better understand the limits of robustness certification methods.

</details>


### [58] [From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs](https://arxiv.org/abs/2511.14017)
*Erum Mushtaq,Anil Ramakrishna,Satyapriya Krishna,Sattvik Sahai,Prasoon Goyal,Kai-Wei Chang,Tao Zhang,Rahul Gupta*

Main category: cs.LG

TL;DR: 研究表明，在特定领域进行拒绝遗忘可能引发跨领域的突发性错位现象，即使目标领域与干预领域无关。通过概念向量分析发现，表示层相似度高的概念更容易在干预后出现错位。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现对不安全代码数据进行微调会触发突发性错位现象，模型即使在无关提示下也会生成恶意响应。需要深入理解引发突发性错位的算法、任务和数据集。

Method: 在网络安全和安全概念上进行拒绝遗忘，评估七个负责任AI领域的拒绝分数。使用交叉熵损失函数在受影响领域的小规模保留数据上进行增强，分析表示层的概念纠缠。

Result: 窄领域遗忘能产生目标概念的合规响应，但可能将突发性错位传播到无关领域。安全概念对其他领域（如偏见）的突发性错位影响更大。在Mistral-7b-0.3v和Qwen-7b-2.5模型家族中一致观察到该效应。

Conclusion: 通过概念向量分析发现，早期层表示相似度高的概念在目标拒绝遗忘干预后更容易出现突发性错位。增强的拒绝遗忘方法可以在很大程度上恢复受影响领域的对齐性。

Abstract: Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.

</details>


### [59] [SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics](https://arxiv.org/abs/2511.14049)
*Semen Leontev*

Main category: cs.LG

TL;DR: SmallML是一个贝叶斯迁移学习框架，使用仅50-200个观测数据就能实现企业级预测精度，解决了中小企业因数据量小而被排除在AI应用之外的问题。


<details>
  <summary>Details</summary>
Motivation: 中小企业占美国企业的99.9%，但由于运营规模与现代机器学习的数据需求不匹配，它们被系统性地排除在AI应用之外。

Method: 开发了三层架构：第一层使用SHAP方法从22,673条公共记录中提取信息先验；第二层在5-50个中小企业间实现分层池化；第三层提供具有有限样本覆盖保证的保形预测。

Result: 在客户流失数据验证中，每个企业使用100个观测数据达到96.7%±4.2%的AUC，比独立逻辑回归提高24.2个百分点，训练时间仅33分钟。

Conclusion: SmallML通过使3300万家美国中小企业能够获得企业级预测，解决了AI民主化中的关键差距。

Abstract: Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.
  We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.
  Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.
  Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning

</details>


### [60] [Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds](https://arxiv.org/abs/2511.14056)
*Marios Papamichals,Regina Ruane*

Main category: cs.LG

TL;DR: 提出了径向补偿(RC)方法，通过选择切空间中的基础密度，使似然仅依赖于测地距离，将参数语义与曲率解耦，解决了流形上生成模型中曲率与参数纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有流形生成模型使用指数映射或体积保持图，但两者都会将曲率与模型参数纠缠，导致梯度方差增大。在高维潜在归一化流中，包裹指数先验会使半径拉伸超出曲率尺度，导致测试似然差和求解器僵硬。

Method: 引入径向补偿(RC)这一信息几何方法，选择切空间中的基础密度，使似然仅依赖于测地距离。推导了平衡指数(bExp)图族，平衡体积失真和测地误差。RC让径向参数保持其在测地单位中的通常含义，而图可以作为数值预调节器进行调优。

Result: RC在所有bExp设置下保持相同的流形密度和Fisher信息，较小的拨号值减少梯度方差和流成本。经验上，RC在图像、图、蛋白质模型等各种密度、VAE、流上产生稳定的生成模型。

Conclusion: RC改善了似然，恢复了干净的测地半径，防止了高维流中的半径爆炸，使RC-bExp成为流形上似然训练生成模型的稳健默认选择。

Abstract: Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.

</details>


### [61] [A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation](https://arxiv.org/abs/2511.14057)
*Xianghe Liu,Jiajia Liu,Chuxian Xu,Minghan Wang,Hongbo Peng,Tao Sun,Jiaqi Xu*

Main category: cs.LG

TL;DR: 提出基于机器学习的多模态框架，整合可穿戴传感器数据，同时进行射箭动作识别和压力估计，为精准运动提供实时反馈系统。


<details>
  <summary>Details</summary>
Motivation: 传统运动分析系统昂贵且侵入性强，限制了在自然训练环境中的应用。需要开发非侵入式、成本效益高的方法来同时评估运动员的生物力学稳定性和心理韧性。

Method: 使用自主研发的腕戴设备（含加速度计和PPG传感器）收集同步运动生理数据。动作识别采用平滑差分加速度特征和LSTM模型；压力估计通过HRV特征和MLP分类器实现。

Result: 动作识别准确率达96.8%，F1分数95.9%；压力估计准确率达80%，能有效区分高低压力水平。

Conclusion: 整合运动和生理传感可为运动员技术和心理状态提供有意义的洞察，为射箭等精准运动开发智能实时反馈系统奠定基础。

Abstract: In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.

</details>


### [62] [CafeMed: Causal Attention Fusion Enhanced Medication Recommendation](https://arxiv.org/abs/2511.14064)
*Kelin Ren,Chan-Yang Ju,Dong-Ho Lee*

Main category: cs.LG

TL;DR: CafeMed是一个集成动态因果推理和跨模态注意力的药物推荐框架，通过动态因果权重生成器和通道协调注意力细化模块，解决了现有方法忽视医疗实体协同效应和静态因果关系的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐系统存在两个基本限制：(i) 将医疗实体视为独立特征，未建模其对药物选择的协同效应；(ii) 使用静态因果关系，无法适应患者特定情境和健康状态。

Method: CafeMed包含两个关键组件：因果权重生成器(CWG)将静态因果效应转换为基于个体患者状态的动态调制权重；通道协调注意力细化模块(CHARM)捕捉诊断和程序之间的复杂相互依赖关系。

Result: 在MIMIC-III和MIMIC-IV数据集上的大量实验表明，CafeMed显著优于最先进的基线方法，在药物预测准确性方面表现优异，同时保持较低的药物-药物相互作用率。

Conclusion: 整合动态因果关系和跨模态协同作用能够产生更符合临床实践和个性化的药物推荐。

Abstract: Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.

</details>


### [63] [CFG-EC: Error Correction Classifier-Free Guidance](https://arxiv.org/abs/2511.14075)
*Nakkyu Yang,Yechan Lee,SooJean Han*

Main category: cs.LG

TL;DR: 提出了CFG-EC方法，通过修正无条件噪声预测来减少CFG在训练和采样过程中的不一致性，提高图像生成质量和提示对齐度。


<details>
  <summary>Details</summary>
Motivation: CFG在训练时随机切换条件和空提示，但在采样时同时输出两者，导致训练和采样过程中的噪声估计不一致，产生误差。

Method: CFG-EC通过主动调整无条件噪声误差分量，使其与条件误差分量正交，防止两个引导分量之间的干扰，从而约束采样误差的上界。

Result: 数值实验表明，CFG-EC在低引导采样机制下表现显著优于CFG和CFG++，在所有情况下都实现了更高的提示对齐度。

Conclusion: CFG-EC是一种可增强任何基于CFG方法的通用校正方案，能够提供更可靠的引导轨迹，实现高保真度的图像生成。

Abstract: Classifier-Free Guidance (CFG) has become a mainstream approach for simultaneously improving prompt fidelity and generation quality in conditional generative models. During training, CFG stochastically alternates between conditional and null prompts to enable both conditional and unconditional generation. However, during sampling, CFG outputs both null and conditional prompts simultaneously, leading to inconsistent noise estimates between the training and sampling processes. To reduce this error, we propose CFG-EC, a versatile correction scheme augmentable to any CFG-based method by refining the unconditional noise predictions. CFG-EC actively realigns the unconditional noise error component to be orthogonal to the conditional error component. This corrective maneuver prevents interference between the two guidance components, thereby constraining the sampling error's upper bound and establishing more reliable guidance trajectories for high-fidelity image generation. Our numerical experiments show that CFG-EC handles the unconditional component more effectively than CFG and CFG++, delivering a marked performance increase in the low guidance sampling regime and consistently higher prompt alignment across the board.

</details>


### [64] [Meta-SimGNN: Adaptive and Robust WiFi Localization Across Dynamic Configurations and Diverse Scenarios](https://arxiv.org/abs/2511.14076)
*Qiqi Xiao,Ziqi Ye,Yinghui He,Jianwei Liu,Guanding Yu*

Main category: cs.LG

TL;DR: Meta-SimGNN是一个基于元学习的WiFi定位系统，通过图神经网络解决设备配置变化（如带宽、AP数量、天线数量）对CSI数据维度的影响，提高定位的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于元学习的定位研究主要关注环境布局变化，但忽视了设备配置变化对CSI数据维度的影响，这会破坏神经网络的可使用性。

Method: 1) 细粒度CSI图构建方案，将每个AP作为图节点；2) 幅度-相位融合方法和特征提取方法处理节点特征；3) 相似性引导的元学习策略，通过比较新场景与历史场景的相似度确定微调参数。

Result: 在不同场景下的商用WiFi设备实验表明，Meta-SimGNN在定位泛化性和准确性方面优于基线方法。

Conclusion: Meta-SimGNN通过图神经网络和元学习的结合，有效解决了设备配置变化带来的定位挑战，提高了系统的实用性和适应性。

Abstract: To promote the practicality of deep learning-based localization, existing studies aim to address the issue of scenario dependence through meta-learning. However, these studies primarily focus on variations in environmental layouts while overlooking the impact of changes in device configurations, such as bandwidth, the number of access points (APs), and the number of antennas used. Unlike environmental changes, variations in device configurations affect the dimensionality of channel state information (CSI), thereby compromising neural network usability. To address this issue, we propose Meta-SimGNN, a novel WiFi localization system that integrates graph neural networks with meta-learning to improve localization generalization and robustness. First, we introduce a fine-grained CSI graph construction scheme, where each AP is treated as a graph node, allowing for adaptability to changes in the number of APs. To structure the features of each node, we propose an amplitude-phase fusion method and a feature extraction method. The former utilizes both amplitude and phase to construct CSI images, enhancing data reliability, while the latter extracts dimension-consistent features to address variations in bandwidth and the number of antennas. Second, a similarity-guided meta-learning strategy is developed to enhance adaptability in diverse scenarios. The initial model parameters for the fine-tuning stage are determined by comparing the similarity between the new scenario and historical scenarios, facilitating rapid adaptation of the model to the new localization scenario. Extensive experimental results over commodity WiFi devices in different scenarios show that Meta-SimGNN outperforms the baseline methods in terms of localization generalization and accuracy.

</details>


### [65] [Observational Auditing of Label Privacy](https://arxiv.org/abs/2511.14084)
*Iden Kalemaj,Luca Melis,Maxime Boucher,Ilya Mironov,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 提出了一种新的观测式差分隐私审计框架，无需修改训练数据集即可评估隐私保证，解决了传统方法在大规模系统中的工程挑战。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计方法需要修改训练数据集（如注入异常样本或移除样本），在大规模系统中资源消耗大、工程开销高，因此需要一种无需干预原始数据集的审计方法。

Method: 利用数据分布的固有随机性，开发观测式审计框架，将隐私审计从传统的成员推理扩展到保护属性（标签作为特例），无需改变原始数据集。

Result: 在Criteo和CIFAR-10数据集上的实验证明，该方法能够有效审计标签隐私保证。

Conclusion: 这项工作为大规模生产环境中的实用隐私审计开辟了新途径。

Abstract: Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.

</details>


### [66] [Soft-Label Training Preserves Epistemic Uncertainty](https://arxiv.org/abs/2511.14117)
*Agamdeep Singh,Ashish Tiwari,Hosein Hasanbeig,Priyanshu Gupta*

Main category: cs.LG

TL;DR: 论文主张将标注分布视为真实标签，而非聚合为单一标签，通过软标签训练保留认知不确定性，在保持准确性的同时更好地反映人类标注的多样性。


<details>
  <summary>Details</summary>
Motivation: 标准实践将多样的人类标注聚合为单一标签，这在模糊数据上存在认知偏差，迫使模型在本质上模糊的情况下表达虚假的置信度。

Method: 采用软标签训练方法，将标注分布视为真实标签，而非聚合为单一标签。

Result: 在视觉和NLP任务中，软标签训练相比硬标签训练实现了32%更低的KL散度和61%更强的模型与标注熵相关性，同时保持相同的准确性。

Conclusion: 标注分布应被视为认知不确定性的忠实表示，而非需要聚合的噪声信号，模型应学习重现这种不确定性。

Abstract: Many machine learning tasks involve inherent subjectivity, where annotators naturally provide varied labels. Standard practice collapses these label distributions into single labels, aggregating diverse human judgments into point estimates. We argue that this approach is epistemically misaligned for ambiguous data--the annotation distribution itself should be regarded as the ground truth. Training on collapsed single labels forces models to express false confidence on fundamentally ambiguous cases, creating a misalignment between model certainty and the diversity of human perception. We demonstrate empirically that soft-label training, which treats annotation distributions as ground truth, preserves epistemic uncertainty. Across both vision and NLP tasks, soft-label training achieves 32% lower KL divergence from human annotations and 61% stronger correlation between model and annotation entropy, while matching the accuracy of hard-label training. Our work repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.

</details>


### [67] [Synthetic Survival Control: Extending Synthetic Controls for "When-If" Decision](https://arxiv.org/abs/2511.14133)
*Jessy Xinyi Han,Devavrat Shah*

Main category: cs.LG

TL;DR: 提出了Synthetic Survival Control (SSC)方法，用于在面板数据设置中估计反事实危险轨迹，解决观察性数据中时间到事件结果的因果推断挑战。


<details>
  <summary>Details</summary>
Motivation: 观察性数据中估计时间到事件结果的因果效应面临审查、样本量有限和非随机治疗分配的挑战，需要回答"如果-何时"问题。

Method: SSC方法将目标单位的反事实危险轨迹估计为其他单位观察轨迹的加权组合，基于低秩结构的面板框架进行因果生存分析。

Result: 在癌症治疗结果的多国临床数据集上验证，发现新疗法与改善生存相关，干预后危险轨迹相对于合成对应物更低。

Conclusion: 该框架为使用观察性数据进行反事实生存推断提供了一个通用且可解释的工具，在医学、经济学和公共政策领域具有广泛应用价值。

Abstract: Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such "when-if" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.

</details>


### [68] [Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation](https://arxiv.org/abs/2511.14135)
*Promise Ekpo,Saesha Agarwal,Felix Grimm,Lekan Molu,Angelique Taylor*

Main category: cs.LG

TL;DR: Fair-GNE：一种在多智能体强化学习环境中通过约束广义纳什均衡寻求公平工作负载分配的方法，在医疗救援模拟中显著改善了工作负载平衡。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法通过事后协调来塑造奖励以引导公平性，缺乏在运行时由个体智能体不可改变的可证明自执行公平性。在智能体共享资源的设置中，需要解决这一缺陷。

Method: 将MARL建模为约束广义纳什均衡寻求（GNE）博弈，通过自适应约束执行引导群体策略达到安全且局部有效的均衡状态，使任何智能体都无法通过单方面改变决策来提高效用函数。

Result: 在自定义高保真复苏模拟器中，Fair-GNE相比固定惩罚基线显著改善了工作负载平衡（0.89 vs. 0.33 JFI，p < 0.01），同时保持了86%的任务成功率。

Conclusion: Fair-GNE通过自适应约束执行实现了统计显著的公平性提升，为大型多智能体学习型医疗系统提供了清晰的公式化、评估指标和均衡寻求创新。

Abstract: Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.

</details>


### [69] [A Comprehensive Study of Implicit and Explicit Biases in Large Language Models](https://arxiv.org/abs/2511.14153)
*Fatima Kazi,Alex Young,Yash Inani,Setareh Rafatirad*

Main category: cs.LG

TL;DR: 该研究提出了一个自动偏见识别框架，用于检测LLMs中的显性和隐性社会偏见，并通过微调和数据增强策略将性能提升高达20%。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，识别和缓解LLMs中的偏见至关重要，因为它们可能延续有害的刻板印象和错误信息。

Method: 使用StereoSet和CrowSPairs等偏见基准评估BERT、GPT-3.5等模型，采用双管齐下的方法检测显性和隐性偏见，应用词袋分析和微调增强策略。

Result: 微调模型在性别偏见方面表现不佳，但在识别种族偏见方面表现优异；增强策略使隐式偏见基准性能提升高达20%。

Conclusion: 尽管LLMs在偏见检测方面取得了一定成功，但它们往往过度依赖关键词，需要持续改进以确保公平输出。

Abstract: Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [70] [Certified Signed Graph Unlearning](https://arxiv.org/abs/2511.14168)
*Junpeng Zhao,Lin Li,Kaixi Hu,Kaize Shi,Jingling Yuan*

Main category: cs.LG

TL;DR: 提出了CSGU方法，为符号图神经网络提供可证明的隐私保证，通过三阶段方法在保持模型效用的同时实现选择性数据删除。


<details>
  <summary>Details</summary>
Motivation: 现有图遗忘方法针对传统GNN设计，忽略了符号图的异质性特征，应用于SGNN时会丢失关键符号信息，降低模型效用和遗忘效果。

Method: 三阶段方法：1）通过三角结构高效识别最小影响邻域；2）应用社会学理论量化节点重要性以优化隐私预算分配；3）执行重要性加权的参数更新实现认证修改。

Result: 大量实验表明CSGU优于现有方法，在SGNN上实现了效用保持和遗忘效果的卓越性能。

Conclusion: CSGU方法成功解决了符号图神经网络的选择性遗忘问题，提供了可证明的隐私保证，同时保持了社会学原则和模型效用。

Abstract: Signed graphs model complex relationships through positive and negative edges, with widespread real-world applications. Given the sensitive nature of such data, selective removal mechanisms have become essential for privacy protection. While graph unlearning enables the removal of specific data influences from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and overlook the unique heterogeneous properties of signed graphs. When applied to Signed Graph Neural Networks (SGNNs), these methods lose critical sign information, degrading both model utility and unlearning effectiveness. To address these challenges, we propose Certified Signed Graph Unlearning (CSGU), which provides provable privacy guarantees while preserving the sociological principles underlying SGNNs. CSGU employs a three-stage method: (1) efficiently identifying minimal influenced neighborhoods via triangular structures, (2) applying sociological theories to quantify node importance for optimal privacy budget allocation, and (3) performing importance-weighted parameter updates to achieve certified modifications with minimal utility degradation. Extensive experiments demonstrate that CSGU outperforms existing methods, achieving superior performance in both utility preservation and unlearning effectiveness on SGNNs.

</details>


### [71] [N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator](https://arxiv.org/abs/2511.14195)
*Zheyu Lin,Jirui Yang,Hengqi Guo,Yubing Bao,Yao Guan*

Main category: cs.LG

TL;DR: N-GLARE是一种基于潜在表征的非生成式LLM安全评估器，通过分析潜在表征的角概率轨迹和JSS度量，以极低的成本复现红队测试的安全排名结果。


<details>
  <summary>Details</summary>
Motivation: 主流红队测试方法依赖在线生成和黑盒输出分析，成本高昂且存在反馈延迟，不适合新模型训练后的敏捷诊断。

Method: 完全基于模型的潜在表征操作，分析潜在表征的角概率轨迹(APT)，引入Jensen-Shannon可分离性(JSS)度量。

Result: 在40多个模型和20种红队策略上的实验表明，JSS度量与红队测试的安全排名高度一致，仅用不到1%的token成本和运行时成本复现了大规模红队测试的判别趋势。

Conclusion: N-GLARE为实时诊断提供了一种高效的无输出评估代理，显著降低了LLM安全评估的成本和延迟。

Abstract: Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.

</details>


### [72] [Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts](https://arxiv.org/abs/2511.14218)
*Xinlei Xiong,Wenbo Hu,Shuxun Zhou,Kaifeng Bi,Lingxi Xie,Ying Liu,Richang Hong,Qi Tian*

Main category: cs.LG

TL;DR: 提出了一种统一的混合贝叶斯深度学习框架，将集合预报与贝叶斯深度学习相结合，显式分解预测不确定性为认知不确定性和偶然不确定性。


<details>
  <summary>Details</summary>
Motivation: 天气预测面临大气混沌性挑战，传统集合预报计算成本高，而贝叶斯深度学习提供了有前景但往往分离的替代方案，需要桥接这两种范式。

Method: 通过变分推理学习认知不确定性，通过物理信息随机扰动方案建模流依赖大气动力学来学习偶然不确定性，建立统一理论框架连接BDL和EPS。

Result: 在ERA5再分析数据集上的实验表明，该方法不仅提高了预报准确性，提供了更好校准的不确定性量化，而且相比最先进的概率扩散模型具有更优的计算效率。

Conclusion: 提出的混合贝叶斯深度学习框架成功桥接了传统集合预报和贝叶斯深度学习，为天气预测提供了统一的不确定性量化方法，并承诺开源代码。

Abstract: Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25° spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.

</details>


### [73] [Parallelizing Tree Search with Twice Sequential Monte Carlo](https://arxiv.org/abs/2511.14220)
*Yaniv Oren,Joery A. de Vries,Pascal R. van der Vaart,Matthijs T. J. Spaan,Wendelin Böhmer*

Main category: cs.LG

TL;DR: 提出了TSMCTS算法，通过方差减少和路径退化缓解，在保持SMC并行化优势的同时，解决了SMC在深度搜索时面临的问题。


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习方法中，SMC作为MCTS的替代方案更易于并行化和GPU加速，但存在方差大和路径退化问题，限制了其在深度搜索中的扩展性。

Method: 引入TSMCTS算法，通过两次顺序蒙特卡洛树搜索来减少方差和缓解路径退化问题。

Result: 在离散和连续环境中，TSMCTS优于SMC基线和现代MCTS版本，能够更好地扩展序列计算能力。

Conclusion: TSMCTS通过改进SMC的局限性，在保持并行化优势的同时，实现了在深度搜索中的良好扩展性。

Abstract: Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.

</details>


### [74] [EBind: a practical approach to space binding](https://arxiv.org/abs/2511.14229)
*Jim Broadbent,Felix Cohen,Frederik Hvilshøj,Eric Landau,Eren Sasoglu*

Main category: cs.LG

TL;DR: EBind是一个简单、以数据为中心且参数高效的方法，用于绑定多模态对比模型的嵌入空间。通过精心策划的数据集，仅用1.8B参数的图像-文本-视频-音频-3D模型就能超越4-17倍大小的模型，并在单GPU上几小时内完成训练。


<details>
  <summary>Details</summary>
Motivation: 简化空间绑定过程，通过专注于单一编码器和高质量数据，实现在单GPU上快速训练最先进的多模态模型，解决传统方法需要多天训练和大量计算资源的问题。

Method: 使用三个互补数据源：670万全自动多模态五元组、100万人工标注的三元组（负匹配、部分匹配、正匹配）和340万现有标注数据项。采用参数高效的方法绑定图像、文本、视频、音频和3D的嵌入空间。

Result: 1.8B参数的EBind模型在13个不同评估中表现优异，超越了4-17倍大小的模型。同时创建了首个高质量、共识标注的音频与点云零样本分类基准。

Conclusion: EBind证明了通过精心策划的数据集和参数高效的方法，可以在有限计算资源下实现多模态空间绑定的最先进性能，并将开源代码、模型权重和数据集。

Abstract: We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.

</details>


### [75] [Object-Centric World Models for Causality-Aware Reinforcement Learning](https://arxiv.org/abs/2511.14262)
*Yosuke Nishimoto,Takashi Matsubara*

Main category: cs.LG

TL;DR: STICA是一个统一框架，使用对象为中心的Transformer作为世界模型和因果感知的策略价值网络，在对象丰富的基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型难以准确复制高维、非平稳、多对象交互的复杂环境，而人类通过分解环境为离散对象来高效决策，这启发了对象中心化的方法。

Method: 将观察表示为对象中心化token集合，包含动作和奖励token，世界模型预测token级动态和交互，策略价值网络估计token级因果关系并在注意力层中使用。

Result: 在对象丰富的基准测试中，STICA在样本效率和最终性能方面均优于最先进的智能体。

Conclusion: 对象中心化的世界模型结合因果感知决策能够有效处理复杂环境，为强化学习提供了新的方向。

Abstract: World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.

</details>


### [76] [Algebraformer: A Neural Approach to Linear Systems](https://arxiv.org/abs/2511.14263)
*Pietro Sittoni,Francesco Tudisco*

Main category: cs.LG

TL;DR: 提出了Algebraformer，一种基于Transformer的架构，用于端到端求解线性系统，特别针对病态系统，具有O(n²)内存复杂度，在测试时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法为经典算法任务提供了新的可能性，但现有数值方法求解病态线性系统需要仔细的参数调整、预处理或领域专业知识。

Method: 使用基于Transformer的架构，采用新颖的编码方案高效表示矩阵和向量输入，支持可扩展推理。

Result: 在谱方法边界值问题的插值任务和牛顿法加速等应用驱动的线性问题上表现出色，达到竞争性精度。

Conclusion: 通用神经架构能有效降低传统科学计算流程的复杂度。

Abstract: Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.

</details>


### [77] [Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention](https://arxiv.org/abs/2511.14265)
*Rui Zhang,Chao Li,Kezhong Liu,Chen Wang,Bolong Zheng,Hongbo Jiang*

Main category: cs.LG

TL;DR: 提出一个统一的船舶多模态轨迹预测框架，通过可解释的导航意图（持续意图和瞬时意图）来提高预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有船舶多模态轨迹预测方法存在场景适用性有限和可解释性不足的问题，特别是在复杂海事环境中预测快速行为变化时。

Method: 构建持续意图树从历史轨迹中提取，使用条件变分自编码器建模动态瞬时意图，并采用非局部注意力机制保持全局场景一致性。

Result: 在真实AIS数据集上的实验表明，该方法在多种场景下具有广泛适用性，在ADE和FDE指标上取得显著提升。

Conclusion: 该方法通过显式揭示每个预测轨迹背后的导航意图，提高了船舶轨迹预测的可解释性，为智能海事系统提供了更可靠的预测框架。

Abstract: Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.

</details>


### [78] [Comparing Task-Agnostic Embedding Models for Tabular Data](https://arxiv.org/abs/2511.14276)
*Frederik Hoppe,Lars Kleinemeier,Astrid Franz,Udo Göbel*

Main category: cs.LG

TL;DR: 本文评估了表格基础模型（TabPFN和TabICL）与经典特征工程（TableVectorizer）在异常检测和监督学习任务中的表现，发现TableVectorizer在性能相当或更优的同时速度可快三个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前表格基础模型将表示学习和任务特定推理封装在单一网络中，资源消耗大。本文专注于可迁移、任务无关的表示学习。

Method: 系统评估TabPFN、TabICL和TableVectorizer在异常检测（ADBench）和监督学习（TabArena Lite）任务中的表现。

Result: TableVectorizer特征在性能相当或更优的同时，比表格基础模型快三个数量级。

Conclusion: 简单的TableVectorizer特征在表格数据表示学习中具有显著优势，特别是在计算效率方面。

Abstract: Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations from tabular foundation models (TabPFN and TabICL) alongside with classical feature engineering (TableVectorizer) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple TableVectorizer features achieve comparable or superior performance while being up to three orders of magnitude faster than tabular foundation models. The code is available at https://github.com/ContactSoftwareAI/TabEmbedBench.

</details>


### [79] [Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning](https://arxiv.org/abs/2511.14282)
*Vincent-Daniel Yun,Junhyuk Jo,Sunwoo Lee*

Main category: cs.LG

TL;DR: 提出一种方差放大正则化器(VAR)，通过在训练中增加模型参数的方差来提高剪枝鲁棒性，相比现有方法无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络参数量大，不利于实际应用。现有剪枝方法在激进剪枝后准确率显著下降，而SAM、CrAM等剪枝鲁棒优化器虽然能缓解这一问题，但会产生不可忽视的额外计算开销。

Method: 提出方差放大正则化器(VAR)，在训练过程中故意增加模型参数的方差。研究发现方差较高的参数表现出更好的剪枝鲁棒性，VAR利用这一特性通过促进权重分布的方差来减轻剪枝的不利影响。

Result: 通过理论分析证明了VAR的收敛行为，并通过大量实证结果展示了VAR在剪枝鲁棒性方面的优越表现。

Conclusion: VAR是一种有效的剪枝鲁棒训练方法，能够在无需额外计算开销的情况下显著提高模型对剪枝的鲁棒性。

Abstract: Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.

</details>


### [80] [H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata](https://arxiv.org/abs/2511.14312)
*Chenyang Xu,Siming Li,Hao Wang*

Main category: cs.LG

TL;DR: H-LDM是一个分层潜在扩散模型，用于从结构化元数据生成临床准确且可控的PCG信号，解决了心血管疾病诊断中标记病理数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 心音图分析对心血管疾病诊断至关重要，但标记病理数据的稀缺限制了AI系统的能力。

Method: 采用多尺度VAE学习生理解耦的潜在空间，分离节律、心音和杂音；构建分层文本到生物信号管道，利用丰富的临床元数据对17种不同条件进行细粒度控制；使用由新型医学注意力模块引导的可解释扩散过程。

Result: 在PhysioNet CirCor数据集上实现了最先进的性能：Fréchet音频距离为9.7，属性解耦得分为92%，临床有效性达87.1%。使用合成数据增强诊断模型可将罕见疾病分类准确率提高11.3%。

Conclusion: H-LDM为心脏诊断中的数据增强开辟了新方向，通过可解释的临床洞察弥合了数据稀缺问题。

Abstract: Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.

</details>


### [81] [Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect](https://arxiv.org/abs/2511.14317)
*Yuwen Zhang,Viet Tran,Paul Weng*

Main category: cs.LG

TL;DR: 提出两种工具（干预效率和扰动验证框架）来解决临床机器学习中的Rashomon效应问题，帮助在性能相近的模型中选出更稳健且符合临床实用性的模型。


<details>
  <summary>Details</summary>
Motivation: 临床机器学习中存在多个性能相近的模型（Rashomon效应），加上小样本、不平衡、噪声数据等问题，使得传统验证方法不可靠，模型选择变得不确定。

Method: 提出干预效率（IE）和扰动验证框架（PVF）。IE是容量感知指标，量化模型在有限干预条件下的效率；PVF评估模型在数据扰动下的稳定性。

Result: 在合成和真实医疗数据集上的实验表明，使用这些工具能够选出泛化能力更强且符合容量约束的模型。

Conclusion: 这些工具为解决临床环境中的Rashomon效应提供了新方向，将预测性能与临床实用性联系起来。

Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.

</details>


### [82] [Learning with Statistical Equality Constraints](https://arxiv.org/abs/2511.14320)
*Aneesh Barthakur,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 本文提出了针对包含等式约束的统计学习问题的泛化理论，并开发了一种实用的序列无约束优化算法，在公平学习、插值分类器和边界值问题中展示了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用日益复杂，除了准确性外还需要满足其他要求。现有方法需要仔细调整超参数权重，这在处理等式约束（如公平性和边界值问题）时变得低效。现有约束优化技术缺乏对等式约束的泛化保证。

Method: 推导了等式约束统计学习问题的泛化理论，提出基于求解一系列无约束经验学习问题的实用算法。

Result: 证明了等式约束学习问题的解可以通过样本和丰富参数化来近似，提出的算法在公平学习、插值分类器和边界值问题中表现有效。

Conclusion: 该工作为处理等式约束的机器学习问题提供了理论基础和实用算法，扩展了约束优化在公平性和边界值问题等领域的应用能力。

Abstract: As machine learning applications grow increasingly ubiquitous and complex, they face an increasing set of requirements beyond accuracy. The prevalent approach to handle this challenge is to aggregate a weighted combination of requirement violation penalties into the training objective. To be effective, this approach requires careful tuning of these hyperparameters (weights), involving trial-and-error and cross-validation, which becomes ineffective even for a moderate number of requirements. These issues are exacerbated when the requirements involve parities or equalities, as is the case in fairness and boundary value problems. An alternative technique uses constrained optimization to formulate these learning problems. Yet, existing approximation and generalization guarantees do not apply to problems involving equality constraints. In this work, we derive a generalization theory for equality-constrained statistical learning problems, showing that their solutions can be approximated using samples and rich parametrizations. Using these results, we propose a practical algorithm based on solving a sequence of unconstrained, empirical learning problems. We showcase its effectiveness and the new formulations enabled by equality constraints in fair learning, interpolating classifiers, and boundary value problems.

</details>


### [83] [Enforcing hidden physics in physics-informed neural networks](https://arxiv.org/abs/2511.14348)
*Nanxi Chen,Sifan Wang,Rujin Ma,Airong Chen,Chuanjie Cui*

Main category: cs.LG

TL;DR: 提出了一个简单通用的不可逆性正则化策略，在PINN训练中强制执行隐藏的物理定律作为软约束，确保解尊重不可逆物理过程的本质单向性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在训练过程中经常忽略热力学第二定律所隐含的隐藏不可逆性，导致非物理解或训练失败。

Method: 引入不可逆性正则化策略，将隐藏物理定律作为软约束集成到PINN训练过程中。

Result: 在多种基准测试中，该正则化方案将预测误差降低了一个数量级以上，且只需对现有PINN框架进行最小修改。

Conclusion: 该框架适用于广泛的PDE控制物理系统，将在科学机器学习社区产生重要影响。

Abstract: Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.

</details>


### [84] [Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation](https://arxiv.org/abs/2511.14406)
*Bastien Vuillod,Pierre-Alain Moellic,Jean-Max Dutertre*

Main category: cs.LG

TL;DR: 本文分析了联邦学习中LoRA参数高效微调技术对后门攻击的影响，发现LoRA秩越低，后门持久性越长，并指出了FL中后门攻击评估存在的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的大模型适配面临完整性威胁，特别是后门攻击。需要研究参数高效微调技术（如LoRA）如何影响后门攻击的有效性和持久性。

Method: 分析LoRA在不同秩设置下对最先进后门攻击的影响，重点关注后门寿命这一关键特征，通过实验研究攻击场景和攻击者能力对后门注入效果的影响。

Result: 实验发现，对于最优注入的后门，当LoRA的秩较低时，攻击后的后门持久性更长。

Conclusion: 研究揭示了FL中后门攻击评估的问题，有助于开发更鲁棒和公平的后门攻击评估方法，提高关键FL系统风险评估的可靠性。

Abstract: Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.

</details>


### [85] [Toward Robust and Harmonious Adaptation for Cross-modal Retrieval](https://arxiv.org/abs/2511.14416)
*Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 本文提出REST方法解决跨模态检索中的查询偏移问题，包括在线偏移和多样偏移，通过查询预测精炼和梯度解耦模块实现鲁棒适应。


<details>
  <summary>Details</summary>
Motivation: 现有通用到定制化跨模态检索方法假设目标域数据完全可用，但实际场景中查询以在线方式到达且具有多样性，导致查询偏移问题，破坏公共空间结构并遗忘通用知识。

Method: REST方法：1）针对在线偏移，通过精炼检索结果制定查询预测，设计QS鲁棒目标函数在线保护公共空间；2）针对多样偏移，使用梯度解耦模块在适应过程中巧妙操作梯度，防止遗忘通用知识。

Result: 在三个跨模态检索任务的20个基准测试上进行广泛实验，验证了该方法对抗查询偏移的有效性。

Conclusion: REST方法能够有效应对跨模态检索中的查询偏移问题，实现在线和和谐适应，保持公共空间结构和通用知识。

Abstract: Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.

</details>


### [86] [FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis](https://arxiv.org/abs/2511.14419)
*Xiaowei Xu,Justin Sonneck,Hongxiao Wang,Roman Burkard,Hendrik Wohrle,Anton Grabmasier,Matthias Gunzer,Jianxu Chen*

Main category: cs.LG

TL;DR: FlowRoI是一个基于光流的感兴趣区域提取框架，用于高通量免疫细胞迁移研究中的图像压缩，能够显著提高压缩率同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 高通量成像平台如ComplexEye产生海量数据，给存储和传输带来巨大负担，需要高效的压缩方法。

Method: 使用光流估计连续帧间的运动，提取覆盖迁移细胞的RoI掩码，然后结合JPEG2000进行RoI感知压缩。

Result: FlowRoI计算效率高，在Intel i7-1255U CPU上达到约30帧/秒的处理速度；在图像质量方面，细胞区域PSNR更高，压缩率比标准JPEG2000提高2.0-2.2倍。

Conclusion: FlowRoI为高通量细胞迁移研究提供了一种高效的数据压缩解决方案，平衡了存储需求和图像质量。

Abstract: Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.

</details>


### [87] [MiAD: Mirage Atom Diffusion for De Novo Crystal Generation](https://arxiv.org/abs/2511.14426)
*Andrey Okhotin,Maksim Nakhodnov,Nikita Kazeev,Andrey E Ustyuzhanin,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 提出了Mirage Atom Diffusion (MiAD)模型，通过mirage infusion技术让扩散模型能在生成过程中改变原子数量，显著提升了S.U.N.晶体材料的生成质量


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成晶体材料时无法改变原子数量，这限制了模型采样轨迹的多样性，影响了生成质量

Method: 引入mirage infusion技术，允许原子状态在存在和不存在之间转换，构建了MiAD模型——一个等变联合扩散模型

Result: 模型质量提升2.5倍，在MP-20数据集上达到8.2%的S.U.N.率，大幅超越现有最先进方法

Conclusion: MiAD通过允许改变原子数量，显著提升了扩散模型在晶体材料生成方面的性能

Abstract: In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \href{https://github.com/andrey-okhotin/miad.git}{\texttt{github.com/andrey-okhotin/miad}}.

</details>


### [88] [Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters](https://arxiv.org/abs/2511.14452)
*Emanuele Palumbo,Sorawit Saengkyongam,Maria R. Cervera,Jens Behrmann,Andrew C. Miller,Guillermo Sapiro,Christina Heinze-Deml,Antoine Wehenkel*

Main category: cs.LG

TL;DR: 提出了一种混合方法，使用血流动力学模拟和未标记的临床数据直接从PPG信号估计心血管生物标志物，解决了PPG测量预测关键心脏生物标志物的挑战。


<details>
  <summary>Details</summary>
Motivation: 连续心血管监测在精准健康中起关键作用，但关键心脏生物标志物如每搏输出量和心输出量需要侵入性测量。PPG作为非侵入性替代方案，但预测关键心脏生物标志物仍面临挑战，且标注PPG测量数据稀缺。

Method: 混合模型结合了在配对PPG-APW数据上训练的条件变分自编码器，以及在标记的模拟APW段上训练的心脏生物标志物条件密度估计器。

Result: 实验证明该方法能够检测心输出量和每搏输出量的波动，并在监测这些生物标志物的时间变化方面优于监督基线。

Conclusion: 所提出的混合方法能够直接从PPG信号估计心血管生物标志物，为连续心血管监测提供了有效的非侵入性解决方案。

Abstract: Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.

</details>


### [89] [Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks](https://arxiv.org/abs/2511.14455)
*Nicola Rares Franco,Lorenzo Tedesco*

Main category: cs.LG

TL;DR: CPFN是一种条件分布估计的生成框架，通过随机映射学习条件分布，支持高效条件采样和条件统计量估计，无需可逆性或对抗训练。


<details>
  <summary>Details</summary>
Motivation: 现有的条件分布估计方法在效率和训练复杂度方面存在局限，需要一种轻量级且易于训练的方法来实现高效的条件采样和统计量估计。

Method: 学习一个随机映射φ(x,u)，使得φ(x,U)与Y|X=x的分布近似相同，通过KL散度推导的目标函数进行训练，无需可逆性或对抗训练。

Result: 实验表明CPFN在性能上可与或优于最先进方法（包括核估计器、树基算法和深度学习技术），同时保持轻量级和易训练特性。

Conclusion: CPFN提供了一种有效的条件分布估计框架，在保持竞争力的同时简化了训练过程，适用于各种条件采样和统计估计任务。

Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.

</details>


### [90] [nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers](https://arxiv.org/abs/2511.14465)
*Clément Dumas*

Main category: cs.LG

TL;DR: nnterp是一个轻量级的NNsight包装器，为transformer分析提供统一接口，同时保留原始HuggingFace实现，解决了当前工具在一致性和准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前transformer可解释性研究工具面临根本权衡：自定义实现（如TransformerLens）确保接口一致但需要手动适配每个架构并引入数值不匹配，而直接使用HuggingFace（通过NNsight）保留精确行为但缺乏跨模型标准化。

Method: 开发nnterp作为NNsight的轻量级包装器，通过自动模块重命名和全面验证测试，提供transformer分析的统一接口，同时保留原始HuggingFace实现。

Result: nnterp使研究人员能够编写一次干预代码，并在跨越16个架构家族的50多个模型变体上部署，包含常见可解释性方法的内置实现，并提供对支持模型的注意力概率的直接访问。

Conclusion: nnterp通过打包验证测试，在机制可解释性工具中弥合了正确性和可用性之间的差距。

Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.

</details>


### [91] [Notes on Kernel Methods in Machine Learning](https://arxiv.org/abs/2511.14485)
*Diego Armando Pérez-Rosero,Danna Valentina Salazar-Dubois,Juan Camilo Lugo-Rojas,Andrés Marino Álvarez-Meza,Germán Castellanos-Dominguez*

Main category: cs.LG

TL;DR: 本文提供了核方法及其在机器学习中几何基础的自包含介绍，涵盖希尔伯特空间、正定核、再生核希尔伯特空间等核心概念


<details>
  <summary>Details</summary>
Motivation: 为机器学习中的核方法提供系统性的理论基础，强调希尔伯特空间几何在统计估计和概率度量表示中的作用

Method: 从希尔伯特空间构造出发，发展正定核、RKHS和希尔伯特-施密特算子理论，重新审视协方差、回归和信息度量的几何视角

Result: 建立了核密度估计、分布核嵌入和最大均值差异的理论框架，为高斯过程、核贝叶斯推断等高级主题奠定基础

Conclusion: 该导论为现代机器学习的功能分析方法提供了坚实的数学基础，连接了经典统计概念与先进的核方法

Abstract: These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.

</details>


### [92] [Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching](https://arxiv.org/abs/2511.14488)
*Jintao Zhang,Mingyue Cheng,Zirui Liu,Xianquan Wang,Yitong Zhou,Qi Liu*

Main category: cs.LG

TL;DR: PAFM是一个扰动感知的流匹配框架，通过建模扰动轨迹来生成结构一致的时间序列，解决了传统流匹配方法在捕捉突变转换方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 时间序列生成在分析决策任务中很重要，但局部扰动引起的时间异质性给生成结构一致的时间序列带来了挑战。传统流匹配方法由于使用全局共享参数，无法充分捕捉扰动时间序列中的突变转换。

Method: 提出PAFM框架：1）采用扰动引导训练模拟局部扰动；2）使用双路径速度场捕捉扰动下的轨迹偏差；3）引入具有流路由的专家混合解码器，动态分配建模能力以适应不同的轨迹动态。

Result: 在无条件和有条件生成任务上的大量实验表明，PAFM始终优于强基线方法。

Conclusion: PAFM通过扰动感知的流匹配方法，有效提升了时间序列生成的结构一致性和对扰动的敏感性。

Abstract: Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \textbf{PAFM}, a \textbf{P}erturbation-\textbf{A}ware \textbf{F}low \textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.

</details>


### [93] [CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design](https://arxiv.org/abs/2511.14510)
*Jiawei Yi,Ping Gong,Youhui Bai,Jiaqi Ruan,Shengnan Wang,Pengcheng Wang,Haibo Wang,Weiguang Wang,Xia Zhu,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: CLO是一个通过算法-系统协同设计实现的CPU轻量级KVCache卸载系统，解决了现有系统忽视的CPU瓶颈问题，显著提升了LLM推理的解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着百万token LLM的发展，推理系统面临可扩展性限制，其中KVCache主导内存使用和数据传输开销。现有卸载系统虽然将KVCache迁移到CPU内存并使用top-k注意力减少数据传输，但忽视了CPU瓶颈的三个关键方面：细粒度动态缓存管理开销、PCIe带宽利用率差以及GPU运行时气泡。

Method: CLO采用算法-系统协同设计，包括：(1)粗粒度的head-wise近似GPU缓存策略，(2)数据预取与GPU持久缓存的无缝结合，(3)零拷贝传输引擎充分利用PCIe带宽，以及GPU中心同步方法消除GPU停顿。

Result: 在两个广泛使用的LLM上的评估表明，CLO在保持与最先进系统相当精度的同时，显著最小化CPU开销，充分利用PCIe带宽，将解码吞吐量提高了9.3%-66.6%。

Conclusion: 算法-系统协同设计对于现代GPU平台上内存受限的LLM推理至关重要。CLO已开源。

Abstract: The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.

</details>


### [94] [Full Atom Peptide Design via Riemannian Euclidean Bayesian Flow Networks](https://arxiv.org/abs/2511.14516)
*Hao Qian,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: PepBFN是首个用于全原子肽设计的贝叶斯流网络，通过连续参数分布建模解决扩散和流匹配模型在肽配体设计中的两个主要挑战：离散残基类型采样与连续变量更新的不匹配，以及侧链扭转角单峰分布假设与多峰旋转异构态本质的冲突。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流匹配模型在肽配体设计中面临两个关键问题：离散残基类型采样与连续结构参数更新的动态不匹配，以及侧链扭转角单峰分布假设无法准确反映多峰旋转异构态，这限制了模型性能。

Method: PepBFN在完全连续空间中直接建模参数分布：1）通过学习连续参数分布建模离散残基类型，实现与其他连续结构参数的联合平滑贝叶斯更新；2）使用高斯混合贝叶斯流捕捉多峰侧链旋转异构态；3）采用基于Matrix Fisher的黎曼流直接在SO(3)流形上建模残基取向。

Result: 在侧链堆积、反向折叠和配体设计任务上的实验表明，PepBFN在计算肽设计中具有强大潜力。

Conclusion: PepBFN通过贝叶斯流网络框架，在完全连续空间中建模肽结构参数分布，解决了现有方法的动态不匹配和多峰建模问题，为计算肽设计提供了有前景的新方法。

Abstract: Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.

</details>


### [95] [MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation](https://arxiv.org/abs/2511.14543)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 提出了一种混合确定性扩散框架，用于处理混合类型表格数据的不完整性问题，通过分离连续和离散特征到不同的生成通道来提高插补准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据中数值、分类和离散属性共存，现有基于扩散的插补模型假设同质特征空间，难以保持条件一致性，导致分类变量信息崩溃或数值变量不稳定，单一扩散过程不足以处理混合类型表格插补。

Method: 将异质特征分离到两个互补的生成通道：连续DDIM通道为数值变量提供高效稳定的确定性去噪，离散潜路径扩散通道基于loopholing离散扩散建模分类和离散特征而不离开有效样本流形，两个通道在统一条件插补目标下训练。

Result: 在多个真实数据集上的实验表明，该框架相比现有扩散方法和经典方法，在MCAR、MAR和MNAR设置下实现了更高的插补准确性、更稳定的采样轨迹和更好的鲁棒性。

Conclusion: 结构感知的扩散过程对于推进深度学习处理不完整表格数据方法具有重要意义，混合确定性扩散框架有效解决了混合类型表格数据的插补挑战。

Abstract: Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.
  We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.
  Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.

</details>


### [96] [Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction](https://arxiv.org/abs/2511.14544)
*Jaume Ros,Alessio Arleo,Fernando Paulovich*

Main category: cs.LG

TL;DR: 提出了Warping Index (WI)这一新的维度缩减投影质量度量指标，专注于测量投影中空区域的正确保持，以避免视觉分析中的误导。


<details>
  <summary>Details</summary>
Motivation: 现有的投影质量度量主要关注全局或局部结构的保持，但忽略了视觉扭曲、异常值和伪影等问题，这些因素可能导致用户在视觉分析中得出错误结论。

Method: 基于空区域正确保持对数据忠实视觉表示至关重要的假设，开发了Warping Index (WI)度量指标来量化维度缩减投影的质量。

Result: 引入了Warping Index这一新的投影质量度量方法，专注于评估投影中空区域的保持程度。

Conclusion: Warping Index为维度缩减投影提供了一个新的质量评估视角，特别关注视觉表示中的空区域保持，有助于避免视觉分析中的误导。

Abstract: Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.

</details>


### [97] [Task Addition and Weight Disentanglement in Closed-Vocabulary Models](https://arxiv.org/abs/2511.14569)
*Adam Hazimeh,Alessandro Favero,Pascal Frossard*

Main category: cs.LG

TL;DR: 本文研究了任务算术在闭词汇图像分类模型中的应用，发现权重解纠缠是预训练模型的普遍特性，使得闭词汇视觉变换器也能通过任务算术进行高效编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管任务算术在开放词汇模型中表现出色，但在没有语言监督预训练的闭词汇模型中的应用尚未探索。本文旨在填补这一空白，扩展任务算术的适用范围。

Method: 在闭词汇图像分类模型中部署和研究任务加法，考虑不同的预训练方案，分析权重解纠缠特性，并与线性探测基线进行比较。

Result: 发现权重解纠缠是预训练模型的普遍特性，闭词汇视觉变换器可以通过任务算术实现高效编辑，达到高任务加法性能，且线性探测是任务加法的竞争性基线。

Conclusion: 研究结果将任务算术的适用性扩展到更广泛的预训练模型类别，为在不同设置中更高效地使用预训练模型开辟了新途径。

Abstract: Task arithmetic has recently emerged as a promising method for editing pre-trained \textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.

</details>


### [98] [ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents](https://arxiv.org/abs/2511.14584)
*Ankush Kadu,Ashwanth Krishnan*

Main category: cs.LG

TL;DR: ReflexGrad是一个结合分层任务分解、因果反思和梯度优化的新架构，在ALFWorld基准上实现了67%的零样本成功率，无需任务特定训练或演示。


<details>
  <summary>Details</summary>
Motivation: 解决智能体从经验中学习并在多样化任务中实现零样本泛化的根本挑战，探索互补学习机制的协同整合潜力。

Method: 结合三种互补机制：LLM驱动的分层TODO分解、历史感知因果反思分析失败原因、梯度优化系统改进，实现纯LLM语义推理的零样本泛化。

Result: 在ALFWorld基准任务上，首次试验(Trial 0)达到67%的零样本成功率，无需任何先验任务经验或演示，有效性能在首次接触时建立。

Conclusion: 互补学习机制的协同整合能够实现接近少样本基线的稳健零样本泛化，证明了该架构在稳定收敛和跨任务迁移方面的有效性。

Abstract: Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.

</details>


### [99] [Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare](https://arxiv.org/abs/2511.14619)
*Marco Locatelli,Arjen Hommersom,Roberto Clemens Cerioli,Daniela Besozzi,Fabio Stella*

Main category: cs.LG

TL;DR: 提出Fuzzy MAP EM算法，将专家知识通过模糊伪计数整合到EM框架中，解决有限数据下POMDP参数学习问题，在医疗模拟中表现优于标准EM算法。


<details>
  <summary>Details</summary>
Motivation: 在有限数据条件下学习部分可观测马尔可夫决策过程(POMDP)参数具有挑战性，需要有效利用专家知识来指导学习过程。

Method: 将专家定义的模糊模型产生的模糊伪计数整合到期望最大化(EM)框架中，自然地将问题重新表述为最大后验概率(MAP)估计。

Result: 在合成医疗模拟中，该方法在低数据和高噪声条件下始终优于标准EM算法；重症肌无力案例研究显示能恢复临床一致的POMDP。

Conclusion: Fuzzy MAP EM算法是医疗保健中数据高效建模的实用工具，具有实际应用潜力。

Abstract: Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.

</details>


### [100] [Failure to Mix: Large language models struggle to answer according to desired probability distributions](https://arxiv.org/abs/2511.14630)
*Ivy Yuqian Yang,David Yu Zhang*

Main category: cs.LG

TL;DR: 现代大语言模型无法遵循概率分布生成输出，即使要求以49%概率输出"1"，模型仍会几乎100%输出概率稍高的"0"，表现出类似阶跃函数的行为。


<details>
  <summary>Details</summary>
Motivation: 科学创意生成和选择需要遵循目标概率分布进行探索，而当前AI基准测试有客观正确答案，通过强化学习训练LLM会抑制概率探索行为。

Method: 进行系统实验，要求LLM按照简单概率分布生成输出，测试模型遵循分布的能力。

Result: 所有测试的现代LLM都严重无法遵循概率分布，即使要求以49%概率输出"1"，模型仍会几乎100%输出概率稍高的"0"。

Conclusion: LLM表现出类似阶跃函数的行为，几乎只生成概率略高的输出，这种倾向甚至压倒了模型内置的强偏见。

Abstract: Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.

</details>


### [101] [Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting](https://arxiv.org/abs/2511.14632)
*Yuchen Luo,Xinyu Li,Liuhua Peng,Mingming Gong*

Main category: cs.LG

TL;DR: Adapformer是一个基于Transformer的多变量时间序列预测框架，通过自适应通道管理结合了通道独立和通道依赖方法的优点，在预测准确性和计算效率方面都达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统多变量时间序列预测方法存在局限性：通道独立方法无法利用通道间交互信息，通道依赖方法则容易引入过多无关信息导致过拟合。需要一种能够平衡这两种策略的方法。

Method: Adapformer采用双阶段编码器-解码器架构，包含自适应通道增强器(ACE)用于丰富嵌入过程，以及自适应通道预测器(ACF)用于优化预测。ACE选择性整合关键依赖关系，ACF专注于最相关的协变量以减少噪声和冗余。

Result: 在多个数据集上的严格测试表明，Adapformer在预测准确性和计算效率方面都优于现有模型，达到了最先进的性能水平。

Conclusion: Adapformer通过自适应通道管理成功解决了多变量时间序列预测中的关键挑战，为这一领域提供了新的有效解决方案。

Abstract: In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.

</details>


### [102] [AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training](https://arxiv.org/abs/2511.14721)
*Fu-Ming Guo,Yingfang Fan*

Main category: cs.LG

TL;DR: 提出AdamHuberDecay作为AdamW的替代方案，使用解耦的Huber正则化器替代L2惩罚，在保持计算效率的同时实现更快的收敛、更好的性能和更强的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统的AdamW优化器中L2惩罚的二次性质使得所有参数以相同速率向原点衰减，容易受到极端梯度方向的影响，且对条件良好的坐标过度惩罚。

Method: 将L2惩罚替换为解耦的平滑Huber正则化器，当参数幅度低于阈值δ时进行二次衰减，超过δ时进行线性衰减，保持O(1)额外计算成本。

Result: 在GPT-2和GPT-3预训练中，收敛速度快10-15%，验证困惑度降低最多4点，下游任务性能提升2.5-4.7%，通过幅度剪枝实现20-30%内存节省。

Conclusion: AdamHuberDecay为下一代基础生成变换器的训练提供了一种简单、有原则的路径，实现了更高效和鲁棒的训练。

Abstract: Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.
  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.

</details>


### [103] [LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data](https://arxiv.org/abs/2511.14738)
*Tzu-Hsuan Chou,Chun-Nan Chou*

Main category: cs.LG

TL;DR: 提出了LAUD框架，将大语言模型与主动学习结合，解决无标签数据场景下的模型训练问题，通过零样本学习缓解冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中缺乏标注数据导致无法获得高性能模型，迫使从业者依赖繁琐低效的基于提示的方法。

Method: LAUD框架整合大语言模型与主动学习，使用零样本学习构建初始标签集来缓解冷启动问题。

Result: 实验结果显示，LAUD框架下的大语言模型在商品名称分类任务上优于零样本或少样本学习方法。

Conclusion: LAUD框架有效解决了无标签数据场景下的模型训练问题，展示了其实际应用价值。

Abstract: Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.

</details>


### [104] [Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction](https://arxiv.org/abs/2511.14743)
*Christof Naumzik,Abdurahman Maarouf,Stefan Feuerriegel,Markus Weinmann*

Main category: cs.LG

TL;DR: 使用高斯过程模型聚合在线评分，考虑时间动态和评论异质性，相比样本均值减少10.2%的预测误差。


<details>
  <summary>Details</summary>
Motivation: 标准评分聚合方法（如样本均值）无法适应质量随时间变化，且忽略评论异质性（如情感、有用性）。

Method: 开发定制高斯过程模型，捕捉评分时间动态并考虑评论异质性。

Result: 基于121,123条Yelp评分数据，高斯过程模型预测未来评分的准确性显著提高，平均绝对误差比样本均值减少10.2%。

Conclusion: 在线声誉系统应采用更智能的评分聚合方法，提供更准确、自适应的客户满意度信号。

Abstract: Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.

</details>


### [105] [Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge](https://arxiv.org/abs/2511.14744)
*Antonia Ebner,Christoph Bartmann,Sonja Topf,Sohvi Luukkonen,Johannes Schimunek,Günter Klambauer*

Main category: cs.LG

TL;DR: 论文回顾了深度学习在药物发现领域的应用，特别是2015年Tox21数据挑战赛作为转折点。研究发现由于数据集在后续整合中被修改，导致研究结果难以比较。为此作者创建了可复现的排行榜，结果显示过去十年毒性预测方法进步有限。


<details>
  <summary>Details</summary>
Motivation: 由于Tox21数据集在后续基准测试中被修改和标签被插补，导致不同研究之间的结果无法直接比较，难以评估过去十年生物活性和毒性预测方法的真实进展。

Method: 创建了一个基于原始Tox21挑战赛数据集的可复现排行榜，托管在Hugging Face上，并提供了基准方法和代表性方法。

Result: 排行榜显示，原始的Tox21获胜者DeepTox方法和2017年提出的基于描述符的自归一化神经网络仍然具有竞争力，位列毒性预测方法的前列。

Conclusion: 过去十年毒性预测方法是否取得实质性进展仍不明确，原始方法依然保持竞争力。作者公开了所有基准和评估模型，可通过Hugging Face Spaces的标准API调用进行推理。

Abstract: Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's "ImageNet moment" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.

</details>


### [106] [Look-Ahead Reasoning on Learning Platforms](https://arxiv.org/abs/2511.14745)
*Haiqing Zhu,Tijana Zrnic,Celestine Mendler-Dünner*

Main category: cs.LG

TL;DR: 论文研究了学习平台中用户的战略行为，特别是前瞻性推理对模型预测的影响，对比了自私行为和集体行为的差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注用户对已部署模型的战略响应，而忽略了用户行为之间的相互影响及其对未来预测的长期影响。

Method: 首先形式化了行为经济学中的k级思维概念，然后研究了集体推理，即用户通过协调行动来共同影响模型。

Result: 研究发现k级思维虽然加速了均衡收敛，但长期来看对个体没有额外收益；集体协调行为则揭示了学习者效用与用户效用之间的对齐关系。

Conclusion: 用户协调行为与自私行为的对比揭示了协调的益处和限制，学习者与用户效用之间的对齐成为关键概念，与战略分类、表演性预测和算法集体行动等框架相关。

Abstract: On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.

</details>


### [107] [SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction](https://arxiv.org/abs/2511.14753)
*Junfeng Wu,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu,Yinan Wang*

Main category: cs.LG

TL;DR: 提出了SparseST框架，通过利用数据稀疏性来开发高效时空模型，并在模型性能和计算效率之间探索帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: ConvLSTM及其变体在时空数据挖掘中表现出色但计算成本高，不适用于边缘设备。现有高效AI方法主要减少模型冗余，但时空数据挖掘需要大模型容量，而数据和特征冗余却带来了不必要的计算负担。

Method: 开发了SparseST框架，利用数据稀疏性设计高效时空模型，并设计了多目标复合损失函数来平衡模型性能和计算效率。

Result: SparseST框架能够在保持模型性能的同时显著降低计算成本，为资源受限的边缘设备提供了可行的解决方案。

Conclusion: 通过利用数据稀疏性而非仅仅减少模型冗余，可以更有效地实现时空数据挖掘的高效计算，为边缘计算环境下的时空分析提供了实用指导。

Abstract: Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.

</details>


### [108] [$π^{*}_{0.6}$: a VLA That Learns From Experience](https://arxiv.org/abs/2511.14759)
*Ali Amin,Raichelle Aniceto,Ashwin Balakrishna,Kevin Black,Ken Conley,Grace Connors,James Darpinian,Karan Dhabalia,Jared DiCarlo,Danny Driess,Michael Equi,Adnan Esmail,Yunhao Fang,Chelsea Finn,Catherine Glossop,Thomas Godden,Ivan Goryachev,Lachy Groom,Hunter Hancock,Karol Hausman,Gashon Hussein,Brian Ichter,Szymon Jakubczak,Rowan Jen,Tim Jones,Ben Katz,Liyiming Ke,Chandra Kuchi,Marinda Lamb,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Yao Lu,Vishnu Mano,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Charvi Sharma,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,Will Stoeckle,Alex Swerdlow,James Tanner,Marcel Torne,Quan Vuong,Anna Walling,Haohuan Wang,Blake Williams,Sukwon Yoo,Lili Yu,Ury Zhilinsky,Zhiyuan Zhou*

Main category: cs.LG

TL;DR: RECAP方法通过优势条件策略整合异构数据进行VLA模型的强化学习训练，在真实家庭环境中实现了衣物折叠、箱子组装和意式咖啡制作等复杂任务，显著提升了任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过强化学习在真实世界部署中改进视觉-语言-动作模型，解决复杂机器人任务的实际应用问题。

Method: 提出RECAP方法，通过优势条件策略整合演示数据、在线收集数据和专家远程干预数据，先预训练通用VLA模型，再通过机器人数据收集进行下游任务专门化。

Result: RECAP训练出的模型能在真实家庭环境中可靠地折叠衣物、组装箱子和制作意式咖啡，在最困难任务上任务吞吐量提高一倍以上，失败率减半。

Conclusion: RECAP方法通过整合异构数据和优势条件策略，有效提升了VLA模型在真实世界复杂任务中的性能表现。

Abstract: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [109] [NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference](https://arxiv.org/abs/2511.13950)
*Lei Zhao,Luca Buonanno,Archit Gajjar,John Moon,Aishwarya Natarajan,Sergey Serebryakov,Ron M. Roth,Xia Sheng,Youtao Zhang,Paolo Faraboschi,Jim Ignowski,Giacomo Pedretti*

Main category: cs.AR

TL;DR: NL-DPE是一种非线性点积引擎，通过结合RRAM和模拟内容寻址存储器(ACAM)在模拟域执行任意非线性函数和数据依赖矩阵乘法，完全消除ADC需求，显著提升能效和速度。


<details>
  <summary>Details</summary>
Motivation: 传统RRAM内存计算加速器存在三个主要限制：仅支持静态点积运算、需要大功耗ADC电路、设备非理想性引入误差，这些限制了现代LLM的可扩展和准确加速。

Method: 在交叉点阵列中增加基于RRAM的ACAM，将非线性函数和数据依赖矩阵乘法转换为决策树在模拟域执行；使用软件噪声感知微调(NAF)处理设备噪声，无需设备内校准。

Result: 相比GPU基线实现28倍能效提升和249倍加速；相比现有IMC加速器实现22倍能效提升和245倍加速，同时保持高精度。

Conclusion: NL-DPE成功克服了传统IMC加速器的关键限制，为大规模模型提供了可扩展且准确的加速解决方案。

Abstract: Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.
  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.

</details>


### [110] [A Bit Level Weight Reordering Strategy Based on Column Similarity to Explore Weight Sparsity in RRAM-based NN Accelerator](https://arxiv.org/abs/2511.14202)
*Weiping Yang,Shilin Zhou,Hui Xu,Yujiao Nie,Qimin Zhou,Zhiwei Li,Changlin Chen*

Main category: cs.AR

TL;DR: 提出了一种位级权重重排序策略，可在RRAM加速器上实现稀疏神经网络权重的紧凑映射，解决了CIM与权重稀疏性难以同时使用的问题。


<details>
  <summary>Details</summary>
Motivation: CIM和权重稀疏都是减少神经网络推理中数据移动的有效技术，但由于稀疏神经网络会破坏CIM所需的结构化计算模式，两者难以在同一加速器中同时使用。

Method: 采用位级权重重排序策略，将权重以二进制补码形式映射到RRAM交叉阵列，利用位级稀疏性和相似性，保留具有相同比特值的列对中仅一列，然后将压缩后的权重矩阵映射到操作单元。

Result: 在典型神经网络上的仿真结果显示，平均性能提升61.24%，在不同稀疏率下实现1.51x-2.52x的能耗节省，相比最先进设计仅有轻微开销。

Conclusion: 该方法成功解决了CIM与权重稀疏性难以同时使用的问题，通过位级重排序实现了稀疏神经网络在RRAM加速器上的高效映射。

Abstract: Compute-in-Memory (CIM) and weight sparsity are two effective techniques to reduce data movement during Neural Network (NN) inference. However, they can hardly be employed in the same accelerator simultaneously because CIM requires structural compute patterns which are disrupted in sparse NNs. In this paper, we partially solve this issue by proposing a bit level weight reordering strategy which can realize compact mapping of sparse NN weight matrices onto Resistive Random Access Memory (RRAM) based NN Accelerators (RRAM-Acc). In specific, when weights are mapped to RRAM crossbars in a binary complement manner, we can observe that, which can also be mathematically proven, bit-level sparsity and similarity commonly exist in the crossbars. The bit reordering method treats bit sparsity as a special case of bit similarity, reserve only one column in a pair of columns that have identical bit values, and then map the compressed weight matrices into Operation Units (OU). The performance of our design is evaluated with typical NNs. Simulation results show a 61.24% average performance improvement and 1.51x-2.52x energy savings under different sparsity ratios, with only slight overhead compared to the state-of-the-art design.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [111] [Enabling Heterogeneous Performance Analysis for Scientific Workloads](https://arxiv.org/abs/2511.13928)
*Maksymilian Graczyk,Vincent Desbiolles,Stefan Roiser,Andrea Guerrieri*

Main category: cs.PF

TL;DR: Adaptyst是CERN正在开发的开源、架构无关的性能分析工具，本文研究eBPF-based方法（Uprobes和USDT）在异构计算性能分析中的表现和实现复杂度。


<details>
  <summary>Details</summary>
Motivation: 异构计算整合了CPU、GPU、FPGA等不同处理单元，需要有效的性能分析来确定最适合的任务调度平台，优化性能和能耗。

Method: 研究两种基于eBPF的内置方法（Uprobes和USDT）的性能和实现复杂度。

Result: 为Adaptyst工具的未来集成和异构性能分析能力发展制定了路线图。

Conclusion: eBPF-based方法在异构计算性能分析中具有潜力，需要进一步集成到Adaptyst工具中。

Abstract: Heterogeneous computing integrates diverse processing elements, such as CPUs, GPUs, and FPGAs, within a single system, aiming to leverage the strengths of each architecture to optimize performance and energy consumption. In this context, efficient performance analysis plays a critical role in determining the most suitable platform for dispatching tasks, ensuring that workloads are allocated to the processing units where they can execute most effectively. Adaptyst is a novel ongoing effort at CERN, with the aim to develop an open-source, architecture-agnostic performance analysis for scientific workloads. This study explores the performance and implementation complexity of two built-in eBPF-based methods such as Uprobes and USDT, with the aim of outlining a roadmap for future integration into Adaptyst and advancing toward heterogeneous performance analysis capabilities.

</details>
