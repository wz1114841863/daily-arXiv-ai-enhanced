<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 82]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 4]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset](https://arxiv.org/abs/2512.13696)
*Md Shahabub Alam,Md Asifuzzaman Jishan,Ayan Kumar Ghosh*

Main category: cs.LG

TL;DR: 提出一种物理引导的深度神经网络方法，用于热泵系统应力分类，在When2Heat数据集上达到78.1%测试准确率，相比基线方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 热泵系统在现代节能建筑中至关重要，但其运行应力检测面临挑战：复杂的热力学相互作用和有限的真实世界数据，需要更有效的检测方法。

Method: 采用物理引导的深度神经网络方法，结合物理引导的特征选择和类别定义，使用5层隐藏层和双重正则化策略的神经网络架构。

Result: 在When2Heat数据集上达到78.1%测试准确率和78.5%验证准确率，相比浅层网络提升5.0%，相比有限特征集提升4.0%，相比单一正则化策略提升2.0%。

Conclusion: 该方法通过物理引导的特征选择、可变阈值实现真实类别分布，以及跨国能源模式分析，为热泵应力检测提供了生产就绪的解决方案。

Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.

</details>


### [2] [Scaling and Transferability of Annealing Strategies in Large Language Model Training](https://arxiv.org/abs/2512.13705)
*Siqi Wang,Zhengyu Chen,Teng Xiao,Zheqi Lv,Jinluan Yang,Xunliang Cai,Jingang Wang,Xiaomeng Li*

Main category: cs.LG

TL;DR: 该研究提出了一个改进的预测框架，用于优化大语言模型训练中的学习率退火策略，证明小模型可以作为大模型训练动态的可靠代理，减少超参数搜索成本。


<details>
  <summary>Details</summary>
Motivation: 学习率调度对训练大语言模型至关重要，但不同模型配置下的最优退火策略仍难以理解，需要减少昂贵的超参数搜索成本。

Method: 研究大语言模型训练中退火动态的可迁移性，改进Warmup-Steady-Decay调度器的预测框架，整合训练步数、最大学习率和退火行为，实现更高效的学习率调度优化。

Result: 验证了最优退火比率遵循一致模式，可以在不同训练配置间迁移，小模型可以作为大模型训练动态的可靠代理，在Dense和MoE模型上均得到验证。

Conclusion: 该工作为选择最优退火策略提供了实用指导，无需进行详尽的超参数搜索，证明了训练动态在不同规模模型间的可迁移性。

Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.

</details>


### [3] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 研究微调大语言模型时出现的灾难性遗忘问题，通过混合训练策略在保持数学推理能力的同时完全避免了NLI能力的遗忘


<details>
  <summary>Details</summary>
Motivation: 当微调大语言模型进行数学推理等专门任务时，模型会出现灾难性遗忘，丧失之前学习的一般能力。研究者希望探索如何在保持专业性能的同时避免这种遗忘。

Method: 使用Flan-T5-Base模型，在DeepMind Mathematics数据集上进行微调，测量在MultiNLI上的遗忘情况。提出混合训练策略，在训练过程中交错数学和NLI示例，系统探索从1:1到15:1的不同混合比例。

Result: 纯数学训练将数学准确率从3.1%提升到12.0%，但导致NLI准确率从81.0%暴跌至16.5%。混合训练完全消除了灾难性遗忘：1:1比例在保持12.0%数学准确率的同时，保留了86.2%的NLI准确率。即使只有6.2%的NLI示例也能提供有效的正则化效果。

Conclusion: 专业化训练不一定需要遗忘一般能力，混合训练策略可以完全避免灾难性遗忘。这对扩展到更大模型具有重要意义，混合训练可能带来超越遗忘预防的额外好处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [4] [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)
*Kaiming Luo*

Main category: cs.LG

TL;DR: 提出VPIA方法，从异构稳态数据中推断复杂系统的相互作用结构，无需时间轨迹或导数估计


<details>
  <summary>Details</summary>
Motivation: 现有重构方法难以处理非线性、异构和高阶耦合的系统，特别是在只能观测到稳态的情况下

Method: 变分物理信息基函数(VPIA)，将稳态约束嵌入可微变分表示，通过最小化物理导出的稳态残差来重构耦合结构

Result: 在多种非线性系统中，VPIA能准确恢复有向、加权和多体结构，即使存在显著噪声

Conclusion: VPIA为仅能获得快照观测的复杂相互作用网络提供统一且鲁棒的物理约束推断框架

Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.

</details>


### [5] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve是一个用于Any-to-Any多模态模型的高效在线服务系统，通过自动规划模型部署和分布式执行，显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: Any-to-Any多模态模型能够接受和生成文本与多模态数据（图像、视频、音频）的组合，但带来了请求类型、计算路径和计算规模的高度异构性，这给模型在线服务带来了巨大挑战。

Method: 1) 允许开发者描述通用Any-to-Any模型的计算图；2) 规划器根据模型和工作负载特征自动优化部署计划，包括是否以及如何将模型分解为更小组件；3) 分布式运行时按照计划执行模型，高效处理异构性。

Result: Cornserve能够高效服务多样化的Any-to-Any模型和工作负载，相比现有解决方案，吞吐量提升最高达3.81倍，尾部延迟降低最高达5.79倍。

Conclusion: Cornserve通过自动化的部署规划和高效的分布式执行，成功解决了Any-to-Any多模态模型在线服务的异构性挑战，为这类新兴模型提供了高效的服务系统。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [6] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 该研究结合SAR影像与环境水文数据，使用机器学习方法对肯尼亚Nyando河流域进行洪水易发性建模，发现随机森林模型表现最佳，识别出维多利亚湖附近低洼平原为高风险区。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统、基础设施和人类生计构成严重威胁。本研究旨在开发一种结合遥感数据和机器学习的方法，为数据有限地区提供准确的洪水易发性评估，支持灾害风险管理和土地利用规划。

Method: 研究使用Sentinel-1双极化SAR数据（2024年5月洪水事件）生成洪水清单作为训练数据，结合六个环境因子（坡度、高程、坡向、土地利用/覆盖、土壤类型、距河流距离），训练四种监督分类器：逻辑回归、分类回归树、支持向量机和随机森林。通过精度、Cohen's Kappa和ROC分析评估模型性能。

Result: 随机森林模型表现最佳（精度=0.762；Kappa=0.480），优于其他三种模型。基于RF的易发性地图显示维多利亚湖附近的Kano平原低洼地区洪水易发性最高，这与历史洪水记录和2024年5月洪水事件的影响一致。

Conclusion: 研究表明结合SAR数据和集成机器学习方法在数据有限地区进行洪水易发性制图具有重要价值。生成的易发性地图为灾害风险减少、土地利用规划和早期预警系统开发提供了重要见解。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [7] [Improving Slow Transfer Predictions: Generative Methods Compared](https://arxiv.org/abs/2512.14522)
*Jacob Taegon Kim,Alex Sim,Kesheng Wu,Jinoh Kim*

Main category: cs.LG

TL;DR: 该研究探讨了在科学计算网络中通过数据增强技术解决类别不平衡问题，以提升机器学习模型对数据传输性能的预测能力，发现即使使用先进的CTGAN等生成技术，相比简单的分层采样也没有显著改进。


<details>
  <summary>Details</summary>
Motivation: 在科学计算网络中，早期预测数据传输性能对于识别潜在缓慢传输和优化网络使用至关重要。然而，机器学习模型在此场景下的预测能力受到类别不平衡问题的严重制约，需要解决这一瓶颈来提升预测准确性。

Method: 研究分析和比较了多种数据增强策略，包括传统的过采样方法和生成技术。同时调整训练数据集中的类别不平衡比例，评估这些方法对模型性能的影响。

Result: 虽然数据增强可能改善性能，但随着不平衡比例的增加，性能提升并不显著。研究发现，即使是最先进的技术如CTGAN，相比简单的分层采样也没有带来显著改进。

Conclusion: 在科学计算网络性能预测的类别不平衡问题中，复杂的数据增强技术并未显著优于简单的分层采样方法，这表明在该特定应用场景下，简单的平衡策略可能已足够有效。

Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.

</details>


### [8] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 提出Hessian Reassignment方法，通过两步实现文档分类器的类别级遗忘：先进行影响式更新减去目标类别贡献，再通过Top-1分类保证决策空间保证，在保持准确率的同时大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在高效移除特定训练数据对模型的影响，避免完全重新训练。虽然LLM的遗忘研究已有进展，但文档分类模型的遗忘相对较少研究，特别是类别级遗忘问题。

Method: 提出Hessian Reassignment方法：1）通过求解Hessian-向量系统进行单次影响式更新，减去目标类别所有训练点的影响；2）使用Top-1分类强制执行决策空间保证，而非随机重新分类删除类别的样本。

Result: 在标准文本基准测试中，Hessian Reassignment在保持接近完整重新训练准确率的同时，运行速度提升数个数量级。同时，在移除类别上持续降低成员推断优势。

Conclusion: Hessian Reassignment为文档分类中的高效类别遗忘提供了一条实用且原则性的路径，实现了准确率与效率的良好平衡。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [9] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 开发机器学习框架整合废水监测、气象和空气质量数据预测美国RSV相关住院率，发现废水RSV水平是最强预测因子，并揭示原住民和阿拉斯加原住民住院率显著更高。


<details>
  <summary>Details</summary>
Motivation: 呼吸道合胞病毒（RSV）是导致幼儿住院的主要原因，其暴发受环境条件强烈影响。需要整合多种数据源来预测RSV相关住院率，以便及时进行公共卫生干预和资源分配。

Method: 整合每周住院率、废水RSV水平、每日气象测量和空气污染物浓度数据。使用CART、随机森林和Boosting等分类模型，预测RSV相关住院率分为低风险、警报和流行三个等级。

Result: 废水RSV水平被确定为最强预测因子，其次是温度、臭氧水平和比湿等气象和空气质量变量。发现原住民和阿拉斯加原住民的RSV相关住院率显著更高，高海拔州（表面压力较低）住院率也持续较高。

Conclusion: 结合环境和社区监测数据预测RSV暴发具有重要价值。开发了交互式R Shiny仪表板，使用户能够探索不同州的RSV风险水平，可视化关键预测因子的影响，并交互生成RSV暴发预测。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [10] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 提出两阶段联邦少样本学习框架，用于个性化EEG癫痫检测，解决医疗数据分散、稀缺和隐私问题


<details>
  <summary>Details</summary>
Motivation: 临床实践中EEG数据稀缺、分散在不同机构、受隐私法规限制，难以构建集中式AI癫痫检测模型

Method: 两阶段框架：第一阶段通过联邦学习在非IID模拟医院站点微调BIOT模型；第二阶段使用仅5个标记EEG片段进行联邦少样本个性化适应

Result: 联邦微调平衡准确率0.43（集中式0.52）；FFSL阶段客户端特定模型平均平衡准确率0.77，Cohen's kappa 0.62，加权F1 0.73

Conclusion: FFSL能在现实数据可用性和隐私约束下支持有效的患者自适应癫痫检测

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [11] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 该论文提出在有限时间预算约束下的推荐系统问题，使用强化学习同时学习用户偏好和时间预算模式，通过模拟框架验证在电商场景下能比传统上下文赌博机方法获得更好的参与度。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务忽略了用户时间预算这一关键资源约束。在移动购物等场景中，用户需要花费时间评估推荐商品，高相关性但高评估成本的商品可能超出用户时间预算，影响用户参与度。需要平衡商品相关性和评估成本。

Method: 1. 将时间约束的slate推荐统一建模为具有预算感知效用的马尔可夫决策过程；2. 开发模拟框架研究在重排序数据上的策略行为；3. 使用强化学习算法（包括on-policy和off-policy控制）同时学习用户偏好和时间预算模式。

Result: 实验使用阿里巴巴个性化重排序数据集，在电商场景下验证了强化学习方法在严格时间预算约束下比传统上下文赌博机方法表现更好，能提高用户参与度潜力。

Conclusion: 有限时间预算下的推荐需要平衡相关性和评估成本，强化学习方法能有效学习用户偏好和时间预算模式，在资源约束下提供更好的推荐效果，为时间敏感型推荐系统提供了新思路。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [12] [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)
*Yuhan Tang,Kangxin Cui,Jung Ho Park,Yibo Zhao,Xuan Jiang,Haoze He,Dingyi Zhuang,Shenhao Wang,Jiangbo Yu,Haris Koutsopoulos,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出RAST-MoE框架，使用自注意力混合专家模型解决网约车平台的自适应延迟匹配问题，在真实Uber数据上提升总奖励13%，减少匹配和接驾延迟10%和15%


<details>
  <summary>Details</summary>
Motivation: 网约车平台需要在高度不确定的供需条件下平衡乘客等待时间和系统效率。自适应延迟匹配需要在立即分配司机和批量处理请求之间做出权衡，现有方法往往过度简化交通动态或使用浅层编码器，无法捕捉复杂的时空模式。

Method: 提出Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE)框架，将自适应延迟匹配形式化为具有自注意力混合专家编码器的机制感知MDP。专家自动专业化，提高表示能力同时保持计算效率。使用物理信息拥堵代理保持真实的密度-速度反馈，支持数百万次高效推演，自适应奖励方案防止病态策略。

Result: 仅用1200万参数，在真实Uber轨迹数据（旧金山）上，总奖励提升超过13%，平均匹配延迟减少10%，接驾延迟减少15%。在未见需求机制下表现出鲁棒性，训练稳定。

Conclusion: 研究结果表明混合专家增强的强化学习在处理复杂时空动态的大规模决策问题中具有潜力，为网约车平台的自适应延迟匹配提供了有效解决方案。

Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.

</details>


### [13] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: CurvaDion通过动量变化检测高曲率区域，只在需要时同步梯度，减少99%通信，同时保持收敛性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型分布式训练中，梯度同步成为关键瓶颈。现有方法如Dion虽然通过低秩更新减少通信，但每步都同步，忽略了优化过程中不同区域对同步需求的差异。

Method: 提出CurvaDion方法，使用相对最大动量变化(RMMC)检测需要同步的高曲率区域。RMMC利用优化过程中已计算的动量动态作为方向曲率的计算可行代理，每层仅增加O(d)操作。

Result: 在160M到1.3B参数模型中，CurvaDion实现了99%的通信减少，同时匹配基线收敛性能。

Conclusion: CurvaDion通过智能检测需要同步的优化区域，显著减少分布式训练通信开销，为大规模模型训练提供高效解决方案。

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [14] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 论文提出了一种用于风数据超分辨率的新型复合分类器自由引导（CCFG）方法，并将其应用于WindDM扩散模型，实现了高保真度且成本效益显著的风数据重建。


<details>
  <summary>Details</summary>
Motivation: 高分辨率、高精度的风数据对于天气建模、风力涡轮机优化等应用至关重要，但获取成本高昂且困难。传统方法无法同时兼顾成本效益和准确性，而现有的深度学习方法在处理多通道风数据时存在局限性。

Method: 提出了复合分类器自由引导（CCFG）方法，这是对标准分类器自由引导（CFG）的泛化，能够更好地处理多条件输入（风数据通常有10个以上输入通道）。该方法可无缝集成到任何预训练的扩散模型中，并应用于WindDM扩散模型进行风数据超分辨率重建。

Result: CCFG在风数据超分辨率任务中比标准CFG产生更高保真度的输出。WindDM在工业规模风动力学重建中达到了深度学习模型中的最先进重建质量，同时成本比传统方法降低了高达1000倍。

Conclusion: 提出的CCFG方法有效解决了风数据超分辨率中多条件输入处理的挑战，WindDM模型在保持高精度的同时大幅降低了成本，为工业应用提供了实用的解决方案。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [15] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: PIS：基于集合条件扩散的物理反演框架，能在任意稀疏观测下稳定求解PDE约束参数反问题，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: PDE约束的物理参数反演在稀疏、不规则观测下本质上是病态的，现有深度学习和算子学习方法在极端稀疏条件下失效，缺乏鲁棒性和不确定性量化

Method: 提出物理反演求解器(PIS)：基于集合条件扩散框架，使用Set Transformer编码器处理任意几何的观测数据，采用余弦退火稀疏课程学习增强鲁棒性，并附带信息论分析

Result: 在Darcy流、Helmholtz波场反演、结构健康监测三个PDE反问题上，PIS在极端稀疏条件下（观测率仅0.29%）仍保持稳定准确，反演误差降低12.28%-88.73%，能生成校准的后验样本

Conclusion: PIS是一个强大、通用且对稀疏性具有独特鲁棒性的物理反演解决方案，能在任意严重欠采样观测下可靠工作

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [16] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: LLRC是一种无需微调的梯度掩码学习方法，通过训练掩码权重选择奇异值，优化大语言模型低秩压缩的秩选择问题，在保持激活相似性的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法在秩选择上存在局限：启发式方法搜索空间有限导致次优结果，梯度方法在无微调时性能不如启发式方法。需要一种无需后压缩微调就能优化秩选择的梯度方法。

Method: 提出LLRC方法，使用校准数据集仅训练掩码权重来选择奇异值，通过最小化中间激活与原始模型的差异来逐步减少奇异值数量，实现无微调的秩优化。

Result: 在Llama-2-13B模型上，20%压缩率下，LLRC在MMLU、BoolQ和OpenbookQA任务上分别比STRS方法提升12%、3.5%和4.4%。在无微调方法中表现最佳，且与微调版LLM-Pruner性能相当。

Conclusion: LLRC通过梯度学习掩码权重，有效解决了低秩压缩中的秩选择问题，在无需后压缩微调的情况下实现了压缩率与下游任务性能的更好平衡。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [17] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出基于参数高效微调(PEFT)的联邦推荐框架，通过减少嵌入参数传输量来提升通信效率，同时保持或提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着云边协同的发展，联邦推荐在保护隐私的同时面临嵌入参数过多导致的通信效率低下问题，现有研究主要关注模型效率而忽视了嵌入参数开销。

Method: 提出基于PEFT的联邦推荐训练框架，采用轻量级插件式设计，集成LoRA、哈希编码等PEFT技术，并探索使用残差量化变分自编码器(RQ-VAE)作为新的PEFT策略。

Result: 在多种联邦推荐模型和数据集上的实验表明，该框架能显著降低通信开销，同时提高推荐准确性。

Conclusion: 提出的PEFT嵌入框架为联邦推荐提供了一种有效降低通信成本并提升性能的解决方案，具有实际应用价值。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [18] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: DARTs是一个用于高维多变量时间序列异常检测的鲁棒性长短时双路径框架，通过窗口感知的时空软融合机制有效处理噪声并捕获长程时空依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低维场景下能有效识别异常模式，但在处理高维噪声时间序列时，难以鲁棒地捕获长程时空依赖关系，这限制了在工业控制系统中的实际应用效果。

Method: 提出长短时双路径框架：短时路径包含多视图稀疏图学习器和扩散多关系图单元，用于捕获高噪声下的层次化短时时空模式；长时路径包含多尺度时空图构造器，用于建模高维表示空间中的长时动态；最后通过窗口感知时空软融合机制过滤残留噪声并整合异常模式。

Result: 在主流数据集上的大量定性和定量实验结果表明，DARTs方法在异常检测方面表现出优越性和鲁棒性，消融研究也验证了各组件设计的关键作用。

Conclusion: DARTs框架通过创新的长短时双路径设计和软融合机制，有效解决了高维噪声时间序列中长程时空依赖关系的建模问题，为工业控制系统中的多变量时间序列异常检测提供了鲁棒的解决方案。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [19] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出TF-MCL模型，通过时间-频率融合和多域交叉损失改进抑郁症脑电检测，在公开数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症脑电检测方法过度依赖标签，而对比学习方法未能充分考虑脑电信号的时频分布特性，获取低语义数据表示的能力不足。

Method: 提出TF-MCL模型：1) 使用融合映射头(FMH)生成时频混合表示；2) 通过多域交叉损失函数优化，重构时频域和融合域的表示分布。

Result: 在MODMA和PRED+CT数据集上，准确率分别比现有最佳方法提升5.87%和9.96%。

Conclusion: TF-MCL模型能有效融合脑电信号的时频信息，提升抑郁症检测性能，为自监督学习在脑电分析中的应用提供了新思路。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [20] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 论文提出层流假说，认为良性输入在LLM潜在空间中产生平滑过渡，而对抗性提示会引发语义湍流，可通过层间余弦速度方差检测，实现轻量级越狱检测和安全架构诊断。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御策略依赖计算昂贵的外部分类器或脆弱的词汇过滤器，忽视了模型推理过程的内在动态。需要一种基于模型内部动态的轻量级检测方法。

Method: 提出层流假说，认为良性输入产生平滑潜在空间过渡，对抗性提示引发语义湍流。引入零样本度量：层间余弦速度方差，通过分析不同层激活向量的变化来检测内部冲突。

Result: 实验显示，RLHF对齐的Qwen2-1.5B在攻击下湍流增加75.4%，验证了内部冲突假说；而Gemma-2B湍流减少22.0%，表现出不同的"反射式"拒绝机制。该方法能有效检测越狱并诊断安全架构。

Conclusion: 语义湍流不仅可作为轻量级实时越狱检测器，还能作为非侵入式诊断工具，用于分类黑盒模型的安全架构类型，为LLM安全提供新视角。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [21] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 该研究比较了在数据稀缺环境下基于嵌入的金融新闻情感分析方法，发现预训练嵌入在数据不足时效果有限，验证集过小会导致过拟合，建议采用少样本学习等替代方法。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析有助于市场理解，但标准NLP方法在小数据集上效果不佳。本研究旨在评估在资源受限环境下，基于嵌入的方法在金融新闻情感分类中的表现。

Method: 使用Word2Vec、GloVe和句子变换器表示，结合梯度提升算法，在手动标注的新闻标题上进行情感分类实验比较。

Result: 实验发现验证集和测试集性能存在显著差距，模型表现甚至不如简单基线方法。预训练嵌入在数据不足时收益递减，小验证集导致模型选择时过拟合。

Conclusion: 嵌入质量本身无法解决数据稀缺的根本问题。对于资源有限的实践者，当标注样本稀缺时，应考虑少样本学习、数据增强或词典增强的混合方法等替代方案。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [22] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: MIDUS提出了一种新的深度扩展方法，用头级记忆层替代传统的FFN复制，通过为每个注意力头分配独立记忆库，实现高效参数扩展和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统深度扩展方法通过复制层并进行持续预训练，但主要依赖前馈网络，导致效率受限且性能提升有限。需要一种既能增加模型容量又不会过度增加参数和推理成本的方法。

Method: MIDUS用头级记忆层替换复制块中的FFN，为每个注意力头分配独立记忆库，支持头级检索并将信息注入后续层，同时保持头级功能结构。结合稀疏记忆访问和头级表示，并包含高效的每头值分解模块。

Result: 在持续预训练实验中，MIDUS相比强大的深度扩展基线表现出稳健的性能提升，同时保持高效的参数占用。在效率和性能之间实现了更好的平衡。

Conclusion: MIDUS通过其头级记忆设计，成为传统FFN复制进行深度扩展的有力且资源高效的替代方案，缓解了效率与性能之间的权衡问题。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [23] [Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention](https://arxiv.org/abs/2512.13758)
*Léo Hein,Giovanni de Nunzio,Giovanni Chierchia,Aurélie Pirayre,Laurent Najman*

Main category: cs.LG

TL;DR: 提出HDA-STGNN模型，利用车速数据、道路静态属性和路网拓扑，无需实时流量数据即可预测全网交通流量分布


<details>
  <summary>Details</summary>
Motivation: 现有交通流量估计方法存在局限性：预测模型忽略无监测道路，空间插值方法依赖实时流量数据，在传感器稀缺城市难以应用。而车速数据和道路静态属性更易获取，可覆盖所有路段。

Method: 提出混合定向注意力时空图神经网络(HDA-STGNN)，这是一个归纳式深度学习框架，利用车速数据、道路静态属性和路网拓扑结构，预测全网所有路段的日交通流量分布。

Result: 通过大量消融实验验证模型有效性，证明模型能够捕捉复杂的时空依赖关系，并突显拓扑信息对于无需实时流量数据即可准确估计全网交通流量的重要性。

Conclusion: HDA-STGNN模型能够在不依赖实时流量数据的情况下，利用更易获取的车速数据和道路属性，实现全网交通流量的准确估计，解决了传感器稀缺城市的交通监测难题。

Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.

</details>


### [24] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN：一种结合交叉熵损失与监督对比损失、融合KNN与半监督图构建、集成对比学习与伪标签的半监督GCN多视图学习框架，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的多视图学习方法未能充分利用视图间的互补信息，导致特征表示次优且性能受限。需要开发能更好整合多视图结构信息、提高模型泛化能力的框架。

Method: 1) 设计结合交叉熵损失与监督对比损失的联合损失函数，减小类内方差并增大类间可分性；2) 融合KNN与半监督图构建方法增强图结构鲁棒性；3) 提出集成对比学习与伪标签的统一框架，增强多视图语义对齐并利用未标记数据。

Result: 在多个基准测试中，MV-SupGCN consistently surpasses state-of-the-art methods，验证了集成方法的有效性。

Conclusion: MV-SupGCN通过集成互补组件（联合损失、鲁棒图构建、对比学习与伪标签），有效提升了多视图学习的性能，为复杂多视图数据建模提供了强大框架。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [25] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: SCPO是一种基于采样的权重空间投影方法，用于约束策略学习，通过轨迹采样和平滑性边界构建局部安全区域，使用凸SOCP进行投影更新，确保从安全初始化开始的所有中间策略都保持安全。


<details>
  <summary>Details</summary>
Motivation: 安全关键学习需要在不离开安全操作区域的情况下改进性能。研究模型参数必须满足未知、基于滚动的安全约束的约束策略学习问题。

Method: 提出SCPO方法：结合轨迹采样和平滑性边界构建局部安全区域，将梯度更新通过凸二阶锥规划（SOCP）进行投影，产生安全的一阶更新步长，无需约束函数的梯度信息。

Result: 在有害监督的回归任务和带有恶意专家的约束双积分器任务中，该方法能持续拒绝不安全更新，在整个训练过程中保持可行性，并实现有意义的原始目标改进。

Conclusion: SCPO提供了安全归纳保证：从任何安全初始化开始，所有中间策略在可行投影下都保持安全。在有稳定备份策略的约束控制设置中，进一步确保闭环稳定性，并实现超越保守备份的安全适应。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [26] [EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models](https://arxiv.org/abs/2512.13806)
*Siegfried Ludwig,Stylianos Bakas,Konstantinos Barmpas,Georgios Zoumpourlis,Dimitrios A. Adamos,Nikolaos Laskaris,Yannis Panagakis,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 提出D3方法，通过弱监督训练分离EEG信号中的潜在脑活动成分，解决深度学习模型在脑机接口中隐藏过拟合问题，提高模型泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前EEG深度学习方法在基准测试中表现良好，但在实际应用中泛化能力差，存在隐藏过拟合问题，需要一种能分离脑活动成分、提高模型可解释性和泛化能力的方法。

Method: 提出解耦解码分解(D3)方法，通过预测输入窗口在试验序列中的位置来分离EEG信号的潜在成分，使用完全独立的子网络架构确保严格可解释性，并建立特征解释范式分析成分激活模式。

Result: D3能可靠分离运动想象数据中的脑活动成分，基于适当成分子集训练的分类器能避免任务相关伪影导致的隐藏过拟合，在线性可分离的潜在空间中实现有效的少样本睡眠分期学习。

Conclusion: D3方法能区分脑活动的真实成分和虚假特征，解决隐藏过拟合问题，提高模型在实际应用中的泛化能力，同时为神经科学研究提供分离个体脑过程、发现未知动态的工具。

Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.

</details>


### [27] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出CTVP框架，通过语义轨道分析验证不可信代码生成模型，检测后门注入，无需直接执行潜在恶意代码。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地生成代码且缺乏人工监督，后门注入和恶意行为成为关键问题，需要可靠的控制框架来验证代码生成模型的安全性。

Method: 提出跨轨迹验证协议（CTVP），通过分析模型对语义等价程序变换的执行轨迹预测的一致性模式来检测行为异常。引入对抗鲁棒性商数（ARQ）量化验证计算成本。

Result: 理论分析建立了信息论边界，证明该方法不可博弈——由于基本空间复杂度约束，对手无法通过训练改进。ARQ显示验证成本随轨道大小呈指数增长。

Conclusion: 语义轨道分析为代码生成任务提供了可扩展、理论基础的AI控制方法，能够有效检测后门行为。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [28] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出一种通过修正原因来改进RLHF的方法：首先识别导致不满意响应的训练数据，然后通过反学习这些数据来改进模型


<details>
  <summary>Details</summary>
Motivation: 人类改善不满意结果的常见策略是找到原因并修正。本文探索将这一策略应用于改进语言模型的RLHF对齐，因为RLHF调优后的模型仍可能产生不满意响应

Method: 方法分为两部分：1) 事后解释方法，通过约束组合优化问题识别导致不满意响应的训练数据；2) 反学习方法，通过反学习这些训练数据来改进不满意响应，同时不显著影响其他满意响应

Result: 实验结果表明，该方法能够有效改进RLHF的性能

Conclusion: 将人类"找到原因并修正"的策略应用于RLHF改进是可行的，提出的方法能有效识别和修正导致不满意响应的训练数据，从而提升语言模型的对齐效果

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [29] [Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains](https://arxiv.org/abs/2512.13852)
*Jelena Losic*

Main category: cs.LG

TL;DR: 提出结合持续同调特征与稳定性正则化的图神经网络框架，增强对结构扰动的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 图神经网络已成为图表示学习的标准方法，但对结构扰动仍然脆弱。需要一种理论上有基础且经验上验证的方法来提高图学习的鲁棒性。

Method: 集成持续同调特征与稳定性正则化，基于持续同调的稳定性定理，结合GIN架构与从持续图像中提取的多尺度拓扑特征，并通过Hiraoka-Kusano启发的稳定性约束进行强化。

Result: 在六个涵盖生化、社交和协作网络的多样化数据集上，该方法表现出对边扰动的卓越鲁棒性，同时保持有竞争力的准确性。在扰动下性能下降最小（大多数数据集0-4%），显著优于基线稳定性方法。

Conclusion: 这项工作提供了一个理论上有基础且经验上验证的鲁棒图学习方法，与拓扑正则化的最新进展相一致。

Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization

</details>


### [30] [Dropout Neural Network Training Viewed from a Percolation Perspective](https://arxiv.org/abs/2512.13853)
*Finley Devlin,Jaron Sanders*

Main category: cs.LG

TL;DR: 本文研究深度神经网络dropout训练中的渗流效应，发现当dropout随机移除连接时可能破坏输入到输出的路径，导致网络无法基于数据做出预测，这种渗流效应在无偏置网络中会导致训练崩溃。


<details>
  <summary>Details</summary>
Motivation: 研究dropout训练中是否存在类似统计物理中的渗流现象，以及这种渗流效应如何影响深度神经网络的训练效果和预测能力。

Method: 建立模拟dropout过程的渗流模型，分析网络拓扑结构与路径问题的关系，从理论上证明dropout中存在渗流效应，并通过实验展示该效应对无偏置网络训练的影响。

Result: 理论证明dropout训练中存在渗流效应，当移除足够多连接时会导致输入到输出路径中断；实验显示这种渗流效应会导致无偏置神经网络的训练崩溃，并启发式地论证该问题也存在于有偏置网络中。

Conclusion: dropout训练中存在显著的渗流效应，这种效应会影响神经网络的训练稳定性，特别是在无偏置网络中可能导致训练崩溃，为理解dropout正则化机制提供了新的物理视角。

Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.

</details>


### [31] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 提出两种估计二元分类器L1校准误差的方法：1）为有界变差校准函数提供上界；2）通过修改分类器实现高效上界估计，不影响性能且无需严格假设


<details>
  <summary>Details</summary>
Motivation: 解决从有限数据集估计二元分类器L1校准误差的问题，提供实用且分布无关的估计方法

Method: 1）为有界变差校准函数提供上界；2）提出修改分类器的方法，使其校准误差能被高效上界估计，同时保持分类器性能

Result: 所有结果都是非渐近且分布无关的，提供了实用的校准误差测量建议，方法在真实数据集上运行开销适中

Conclusion: 提出的方法为实际测量校准误差提供了实用指导，能够在真实世界数据集上以适度开销运行

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [32] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出一个结合去噪自编码器、卷积分词器和Transformer编码器的婴儿哭声分类系统，采用联邦学习训练，实现隐私保护、噪声鲁棒和通信高效的边缘部署。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类有助于早期评估婴儿需求，但现有解决方案面临隐私问题（音频数据）、对背景噪声敏感以及跨录音环境的领域偏移等部署限制。

Method: 端到端婴儿哭声分析管道，包含去噪自编码器、卷积分词器和Transformer编码器，采用通信高效的联邦学习训练。系统执行设备端去噪、自适应分割、事后校准和基于能量的分布外弃权。联邦训练采用带8位适配器增量的正则化控制变量更新和安全聚合。

Result: 在Baby Chillanto和Donate-a-Cry数据集（带ESC-50噪声叠加）上，模型达到宏观F1分数0.938、AUC 0.962和预期校准误差0.032，同时将每轮客户端上传从约36-42 MB减少到3.3 MB。在NVIDIA Jetson Nano上的实时边缘推理达到每1秒频谱图帧96毫秒。

Conclusion: 该研究展示了一条实用的隐私保护、噪声鲁棒和通信高效的婴儿哭声分类路径，适合联邦部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [33] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA是一种实用的后训练剪枝方法，通过将层权重重构转化为独立的行级二次规划问题，在保持精度的同时实现大规模剪枝，无需微调。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法存在权衡：简单启发式方法速度快但精度下降，而联合优化方法精度高但计算不可行。需要一种既能保持精度又具有可扩展性的实用方法。

Method: 将掩码选择后的层权重重构转化为独立的行级二次规划问题，这些QP共享层Hessian矩阵。实现加速器友好的QP求解器，每层累积一个Hessian并并行求解多个小QP，实现单加速器上的大规模剪枝。

Result: 在多个LLM家族和稀疏度下，OPTIMA将零样本性能提升高达3.97%绝对精度。在NVIDIA H100上，40小时内完成80亿参数transformer的端到端剪枝，峰值内存60GB。

Conclusion: OPTIMA为一次性后训练剪枝设定了新的精度-效率权衡基准，实现了实用的大规模模型剪枝而无需微调。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [34] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 论文发现长上下文LLM存在"分数稀释"问题，现有推理时策略（如生成思考标记）在长上下文任务中效果有限，提出通过针对上下文的梯度更新来显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能够处理数百万标记的长上下文，但经验表明它们无法可靠地利用这些长文本。同时，推理时计算（如生成思考标记）虽然能提升多步推理任务性能，但在长上下文任务中效果迅速减弱。

Method: 提出一种简单方法：通过对给定上下文进行针对性的梯度更新，理论上克服静态自注意力的限制。这种方法将推理时计算从生成更多思考标记转向上下文特定的训练。

Result: 该方法在各种模型和长上下文基准测试中带来一致的大幅性能提升。在Qwen3-4B模型上，LongBench-v2和ZeroScrolls基准测试子集平均分别提升12.6和14.1个百分点。

Conclusion: 对于长上下文任务，少量上下文特定训练比当前推理时扩展策略（如生成更多思考标记）是更好的推理计算使用方式。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [35] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习、深度学习与传统动态模型在南美洲降水预报中的表现，发现LSTM模型在强降水预报中表现最佳，证实了深度学习在气候预报中的可行性。


<details>
  <summary>Details</summary>
Motivation: 降水预报对社会至关重要，但传统动态模型复杂且计算量大。虽然AI技术已被用作替代或补充，但缺乏对纯数据驱动方法在降水预报中可行性的广泛研究。本研究旨在填补这一空白。

Method: 研究比较了传统机器学习方法（随机森林、XGBoost）和深度学习方法（1D CNN、LSTM、GRU），同时使用巴西全球大气模型（BAM）作为传统动态建模方法的代表。研究考虑了2019年所有季节的南美洲降水数据，并采用可解释AI来分析模型行为。

Result: LSTM模型表现出最强的预测性能，而传统动态模型BAM表现最差。LSTM在强降水预报中最为准确，尽管延迟较高。如果考虑成本，XGBoost提供较低延迟但精度略有损失。

Conclusion: 研究结果证实了深度学习模型在气候预报中的可行性，巩固了全球主要气象和气候预报中心的趋势。LSTM在降水预报中表现优异，特别是对强降水事件。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [36] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 使用神经ODE学习2RDM动力学，发现只有在二粒子和三粒子关联度高的区域才能准确预测，否则需要记忆依赖的闭包方案。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统的非平衡动力学需要高效描述方法。传统方法要么计算成本指数增长（精确波函数），要么忽略重要关联（平均场）。TD2RDM方法通过传播二粒子约化密度矩阵提供中间方案，但其时间局域重构函数在记忆效应方面的有效性不明确。

Method: 使用神经ODE模型在精确2RDM数据上训练（无降维），学习二粒子约化密度矩阵的动力学，而不需要显式的三粒子信息。通过分析二粒子和三粒子累积量之间的Pearson相关性来评估模型性能。

Result: 神经ODE仅在二粒子和三粒子累积量Pearson相关性大的参数区域能准确再现动力学。在反相关或非相关区域失败，表明没有简单的时间局域函数能捕获演化。时间平均的三粒子关联积累幅度是成功的主要预测因子：中等关联积累时，神经ODE和现有TD2RDM重构都准确；强关联时系统崩溃。

Conclusion: 在强关联区域需要记忆依赖的核函数进行三粒子累积量重构。神经ODE可作为模型无关的诊断工具，映射累积量展开方法的适用范围，并指导非局域闭包方案的开发。从有限数据学习高维RDM动力学为快速、数据驱动的相关量子物质模拟开辟了新途径。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [37] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: 提出自适应数字孪生框架，通过贝叶斯更新在线学习状态转移概率，增强土木工程中数字孪生的价值实现


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生在土木工程中的价值实现有限，需要增强自适应能力来提升个性化、鲁棒性和成本效益

Method: 使用概率图模型表示数字孪生，通过动态贝叶斯网络建模物理与虚拟域的双向交互，将状态转移概率作为随机变量进行层次化在线学习，通过强化学习求解参数化马尔可夫决策过程

Result: 提出的自适应数字孪生框架在铁路桥梁结构健康监测和维护规划案例中表现出增强的个性化、鲁棒性和成本效益

Conclusion: 自适应机制能显著提升数字孪生在土木工程中的价值实现，为结构健康监测和维护规划提供了更有效的解决方案

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [38] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出Phalanx层作为窗口注意力或线性递归的即插即用替代方案，在10亿参数多混合模型中，相比优化Transformer在4K到32K上下文长度上实现10-40%加速，同时保持相同困惑度


<details>
  <summary>Details</summary>
Motivation: 多混合架构因更好的质量和性能有望主导语言建模，但需要解决GPU内存层次对齐和跨warp通信成本高的问题

Method: 引入线性递归的层次分解框架，开发与GPU内存层次对齐的算法，提出滑动窗口递归(SWR)，专注于将递归截断为硬件对齐的自然锯齿状窗口，减少跨warp通信

Result: 在10亿参数多混合模型中，Phalanx层在4K到32K上下文长度上相比优化Transformer实现10-40%速度提升，同时保持相同困惑度

Conclusion: 滑动窗口递归和Phalanx层为多混合语言模型提供了高效的内存层次对齐解决方案，显著提升性能同时保持模型质量

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [39] [A Complete Guide to Spherical Equivariant Graph Transformers](https://arxiv.org/abs/2512.13927)
*Sophia Tang*

Main category: cs.LG

TL;DR: 该指南系统介绍了球形等变图神经网络的理论基础与实现方法，为处理三维分子系统提供旋转不变的建模框架。


<details>
  <summary>Details</summary>
Motivation: 三维分子和生物分子系统具有固有的旋转对称性，传统模型无法保证预测结果在物理旋转下的正确变换。需要开发能够尊重SO(3)旋转对称性的等变神经网络，使预测在输入旋转时以物理意义明确的方式变化。

Method: 构建完整的球形等变建模理论基础，包括群表示、球谐函数、张量积、Clebsch-Gordan分解和SO(3)-等变核构造。基于此构建Tensor Field Network和SE(3)-Transformer架构，实现几何图上的等变消息传递和注意力机制。

Result: 提供了自包含的球形EGNNs介绍，包含清晰的数学推导和带注释的代码示例，使研究人员能够理解和实现这些模型。

Conclusion: 球形等变图神经网络为处理三维分子系统提供了原则性框架，确保预测尊重物理旋转对称性，适用于化学、分子性质预测、蛋白质结构建模和生成建模等领域。

Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.

</details>


### [40] [Informing Acquisition Functions via Foundation Models for Molecular Discovery](https://arxiv.org/abs/2512.13935)
*Qi Chen,Fabio Ramos,Alán Aspuru-Guzik,Florian Shkurti*

Main category: cs.LG

TL;DR: 提出一种免似然贝叶斯优化方法，直接利用LLM和化学基础模型的先验知识，通过树状空间划分和蒙特卡洛树搜索提高分子发现的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在低数据、大搜索空间下性能受限，而LLM和化学基础模型虽提供丰富先验，但高维特征、上下文学习成本高以及深度贝叶斯代理模型的计算负担阻碍了其充分利用。

Method: 1. 免似然BO方法，绕过显式代理建模，直接利用LLM和化学基础模型的先验知识指导采集函数；2. 学习分子搜索空间的树状划分，配合局部采集函数，通过蒙特卡洛树搜索高效选择候选分子；3. 引入粗粒度LLM聚类，将采集函数评估限制在具有统计上更高属性值的簇中，大幅提升可扩展性。

Result: 通过大量实验和消融研究，该方法显著提高了LLM引导的分子发现贝叶斯优化的可扩展性、鲁棒性和样本效率。

Conclusion: 所提出的免似然BO方法有效解决了传统BO在低数据大空间下的局限性，充分利用了LLM和化学基础模型的先验知识，通过树状划分和聚类策略实现了高效可扩展的分子发现。

Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.

</details>


### [41] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: PGDM利用时间数据中的固有模式进行预测，通过原型分析提取模式并估计最可能的下一个模式，结合不确定性量化动态调整引导强度，在视觉场测量和动作捕捉预测中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多元时间序列预测中很少考虑数据中的重复结构或模式，这限制了预测的准确性和现实性。

Method: PGDM首先使用原型分析提取时间数据中的固有模式，估计序列中最可能的下一个模式，并用该模式估计引导预测。同时引入基于原型分析的不确定性量化技术，并根据模式估计的不确定性动态调整引导强度。

Result: 在两个应用场景中，模式引导使PGDM性能显著提升：视觉场测量预测的MAE/CRPS提升达40.67%/56.26%，动作捕捉预测提升达14.12%/14.10%。PGDM在基线对比中表现更优，分别提升达65.58%/84.83%和93.64%/92.55%。

Conclusion: PGDM通过利用时间数据中的固有模式进行引导预测，显著提升了扩散模型在时间序列预测中的性能，同时提供了一种有效的不确定性量化方法。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [42] [A Single Architecture for Representing Invariance Under Any Space Group](https://arxiv.org/abs/2512.13989)
*Cindy Y. Zhang,Elif Ertekin,Peter Orbanz,Ryan P. Adams*

Main category: cs.LG

TL;DR: 提出单一机器学习架构，可自动适应任意空间群对称性，通过对称性适应的傅里叶基实现权重共享和零样本学习


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每个对称群设计专门架构，限制了可扩展性和知识迁移。对于材料科学中的230个三维空间群，这一问题尤为突出。

Method: 通过显式表征群操作对傅里叶系数的约束，构建对称性适应的傅里叶基，并将这些约束编码到神经网络层中，实现跨空间群的权重共享。

Result: 在材料性质预测任务中达到竞争性性能，并能通过零样本学习推广到未见过的空间群。

Conclusion: 该方法克服了传统对称性适应架构的局限性，实现了单一架构适应多种空间群，促进了知识迁移并解决了数据稀疏问题。

Abstract: Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.

</details>


### [43] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该论文提出了一个经过精心整理的MHC-II抗原表位数据集，并建立了三个机器学习任务来推进计算免疫治疗研究。


<details>
  <summary>Details</summary>
Motivation: MHC-II抗原表位在免疫治疗中至关重要，但由于其复杂的结合特异性和模糊的基序模式，相比MHC-I研究更具挑战性。现有MHC-II数据集较小且标准化程度低，限制了计算免疫治疗的发展。

Method: 从免疫表位数据库(IEDB)和其他公共来源整理了一个精心策划的数据集，扩展并标准化了现有的肽-MHC-II数据集，并引入了具有更丰富生物背景的新型抗原-MHC-II数据集。基于此数据集制定了肽结合、肽呈递和抗原呈递三个机器学习任务，并采用多尺度评估框架对现有模型进行基准测试。

Result: 创建了一个有价值的MHC-II抗原表位资源，为未来机器学习指导的表位发现和免疫反应预测建模研究提供了基础。通过多尺度评估框架对现有模型进行了基准测试，并对各种建模设计进行了全面分析。

Conclusion: 这项工作为推进计算免疫治疗提供了宝贵资源，通过精心策划的数据集和系统化的机器学习任务框架，为MHC-II抗原表位预测的未来研究奠定了坚实基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [44] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5是一个病理学基础模型，通过联合建模组织学、基因组学、表观遗传学和转录组学等多模态数据，创建更全面的肿瘤生物学患者表征，在临床和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 癌症进展涉及多个生物层面的相互作用，特别是超越形态学的分子层面，这些对仅基于图像的模型是不可见的。为了捕捉更广泛的生物学景观，需要整合多模态数据来更全面地反映肿瘤生物学。

Method: 1. 多模态SigLIP损失实现异构模态间的全配对对比学习；2. 片段感知旋转位置编码(F-RoPE)模块保留WSI中的空间结构和组织片段拓扑；3. 针对WSI和RNA-seq的领域专业化内部基础模型，提供生物学基础嵌入以实现稳健的多模态对齐。

Result: 在两个互补基准测试中评估：内部真实世界临床数据集和包含80个任务的Patho-Bench基准。模型表现出高数据和参数效率，在Patho-Bench上达到与最先进基础模型相当的性能，同时在内部临床设置中展现出最高的适应性。

Conclusion: 结果强调了生物学信息多模态设计的价值，并突显了整合基因型到表型建模在下一代精准肿瘤学中的潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [45] [Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks](https://arxiv.org/abs/2512.14023)
*Yong Fang,Na Li,Hangguan Shan,Eryun Liu,Xinyu Li,Wei Ni,Er-Ping Li*

Main category: cs.LG

TL;DR: HSMGNN：首个用于多元时间序列预测的混合欧几里得-黎曼几何图神经网络，通过双空间表示和自适应距离机制提升预测精度


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在欧几里得或黎曼空间中建模多元时间序列数据，难以捕捉真实数据中多样的几何结构和复杂的时空依赖关系

Method: 提出HSMGNN模型：1）子流形交叉段嵌入将输入投影到欧几里得和黎曼空间；2）自适应距离库层降低黎曼距离计算成本；3）融合图卷积网络通过可学习融合算子整合双空间特征

Result: 在三个基准数据集上的实验表明，HSMGNN相比最先进基线方法在预测精度上提升高达13.8%

Conclusion: HSMGNN通过混合几何表示框架，首次实现了对多元时间序列几何特性的表达性和全面性建模，显著提升了预测性能

Abstract: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.

</details>


### [46] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD是一个统一的时间序列分析框架，通过自适应时频融合和去噪机制，在分类、预测和异常检测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析面临三大挑战：1）缺乏高效、多任务兼容的统一框架；2）现有方法通常针对单一任务或特定数据类型；3）现实数据常受噪声、复杂频率成分和多尺度动态模式影响，难以进行稳健的特征提取。

Method: FusAD采用自适应时频融合机制，结合傅里叶和小波变换捕捉全局-局部和多尺度动态特征；包含自适应去噪机制自动感知和过滤各类噪声；集成通用信息融合和解码结构，结合掩码预训练促进多粒度表示的高效学习和迁移。

Result: 在主流时间序列基准测试中，FusAD在分类、预测和异常检测任务上始终优于最先进的模型，同时保持高效率和可扩展性。

Conclusion: FusAD为解决时间序列分析中的统一建模挑战提供了一个有效的解决方案，通过创新的时频融合和去噪机制实现了多任务兼容性和鲁棒性。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [47] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: SonicMoE：通过内存高效算法、GPU内核优化和令牌舍入技术，解决细粒度MoE模型激活内存开销大和稀疏MoE计算浪费问题，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型趋向细粒度专家（小专家维度）和高稀疏性（激活专家数恒定但总专家数增加），这虽然提高了每FLOP的模型质量，但带来了两个主要问题：1）细粒度MoE激活内存开销大且硬件效率低（IO成本高）；2）稀疏MoE在分组GEMM内核中因填充导致计算浪费。

Method: 提出三方面创新：1）内存高效算法，最小化MoE前向/反向传播的激活缓存；2）GPU内核设计，实现内存IO与计算重叠；3）"令牌舍入"方法，减少分组GEMM内核中的填充浪费。

Result: SonicMoE在Hopper GPU上相比ScatterMoE的BF16 MoE内核：激活内存减少45%，计算吞吐提升1.86倍。在64个H100上达到2130亿令牌/天的训练吞吐，与ScatterMoE在96个H100上的2250亿令牌/天相当。高稀疏设置下，令牌舍入算法相比传统top-K路由带来额外1.16倍加速，同时保持相似下游性能。

Conclusion: SonicMoE通过系统级优化解决了MoE模型训练中的内存和计算效率瓶颈，显著提升了训练速度，同时开源所有内核以促进MoE模型训练加速。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [48] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: DIFNOs（导数信息傅里叶神经算子）是一种能同时准确模拟高保真算子响应及其导数的FNO变体，在PDE约束优化中作为代理模型优于传统FNO，具有更高的样本效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统FNO在PDE约束优化中作为代理模型时，需要准确的导数信息才能保证优化质量。然而，传统FNO训练只关注算子输出，不保证导数准确性，这限制了其在优化问题中的应用效果。

Method: 提出DIFNOs：通过同时最小化输出和Fréchet导数误差来训练FNO；建立FNO及其导数的通用逼近理论；开发基于降维和多分辨率技术的高效训练方案，降低内存和计算成本。

Result: 理论证明了FNO及其导数在紧集上的同时通用逼近能力；数值实验显示DIFNOs在非线性扩散-反应、Helmholtz和Navier-Stokes方程中，相比传统FNO具有更高的样本效率和准确性，特别适合无限维PDE约束逆问题求解。

Conclusion: DIFNOs作为导数增强的FNO变体，能有效解决PDE约束优化问题，在理论保证和计算效率方面均优于传统FNO，为算子学习和逆问题求解提供了高效工具。

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [49] [Arithmetic-Intensity-Aware Quantization](https://arxiv.org/abs/2512.14090)
*Taig Singh,Shreshth Rajan,Nikhil Iyer*

Main category: cs.LG

TL;DR: AIQ是一种混合精度量化框架，通过逐层选择比特宽度来最大化算术强度同时最小化精度损失，提升内存受限神经网络推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 现代神经网络日益受内存限制，推理吞吐量受DRAM带宽而非计算能力限制，需要优化内存访问效率

Method: 提出算术强度感知量化(AIQ)，这是一种后训练量化方法，使用搜索算法在逐层量化方案上最小化算术强度和精度的加权损失

Result: 在ResNet-20/CIFAR-10上，AIQ将算术强度提高约50%，测试精度保持在1个百分点内；在MobileNetV2上，吞吐量比FP32基线提高1.66倍

Conclusion: AIQ能有效提升内存受限神经网络的推理吞吐量，且自然地更激进地量化较大层，优于全局均匀量化方案

Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.

</details>


### [50] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出S-GRPO方法，使用逻辑相似性奖励机制替代传统奖励建模，通过形式逻辑一致性引导模型对齐人类偏好，避免模型崩溃


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法依赖奖励模型的质量和稳定性，而传统奖励建模存在启发式估计的局限性。真实世界问题可从多角度解读，需要确保基于逻辑的强化学习不会导致模型崩溃

Method: 提出逻辑相似性奖励机制，使用形式逻辑一致性替代传统奖励建模。引入S-GRPO（GRPO的监督变体），结合监督组件，联合优化生成项、KL散度正则化和基于标签的目标

Result: S-GRPO在性能和鲁棒性上均优于标准监督微调（SFT），并能扩展现有偏好学习框架（如GRPO和DPO），提供更灵活、任务自适应的对齐训练方法

Conclusion: 逻辑相似性奖励机制是传统奖励建模的有效替代方案，S-GRPO框架为语言模型对齐提供了更稳定和灵活的方法，能够处理多视角解释的现实问题

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [51] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder提出了一种主动建模建筑物和发射器的无线电路径损耗预测架构，通过解耦特征编码和掩码引导低秩注意力机制，解决了现有方法在环境建模、多发射器场景和分布偏移方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线电路径损耗预测方法存在三个主要问题：1）被动环境建模，忽略了发射器和关键环境特征；2）过度关注单发射器场景，而现实世界多为多发射器场景；3）过度关注分布内性能，忽略了分布偏移挑战，特别是在训练/测试环境建筑物密度或发射器配置不同的情况下。

Method: 提出了PathFinder架构，通过解耦特征编码主动建模建筑物和发射器，集成掩码引导低秩注意力机制独立关注接收器和建筑物区域，并引入发射器导向的Mixup策略进行鲁棒训练。还创建了新的单到多发射器RPP基准来评估外推性能。

Result: 实验结果表明，PathFinder显著优于现有最先进方法，特别是在具有挑战性的多发射器场景中表现出色。

Conclusion: PathFinder通过主动环境建模和专门针对多发射器场景的设计，有效解决了现有无线电路径损耗预测方法的局限性，为5G网络优化和物联网应用提供了更可靠的预测工具。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [52] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出在深度主动学习中使用形式化验证生成的对抗样本来增强训练数据，相比传统梯度攻击方法能更有效提升模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度主动学习旨在减少神经网络训练中的标注成本，但现有方法主要关注选择哪些样本进行标注。本文探索通过添加违反鲁棒性约束的对抗样本来进一步增强训练数据效率，特别是研究形式化验证生成的对抗样本相比传统梯度攻击方法的优势。

Method: 1) 使用形式化验证方法生成对抗样本，而非传统的基于梯度的攻击方法；2) 将这种对抗样本增强策略应用于多种现代深度主动学习技术；3) 提出一种新的深度主动学习技术，并同样应用对抗样本增强。

Result: 实验表明：1) 形式化验证生成的对抗样本相比标准梯度攻击方法能更显著提升深度主动学习性能；2) 这种增强策略应用于多种现代深度主动学习技术以及作者提出的新技术时，在标准基准测试中都显著改善了模型泛化能力。

Conclusion: 通过形式化验证生成对抗样本来增强训练数据是提升深度主动学习性能的有效方法，相比传统梯度攻击方法能带来更显著的泛化性能提升，为深度主动学习的数据效率优化提供了新方向。

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [53] [Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix](https://arxiv.org/abs/2512.14188)
*Wei Tao,Sheng Long,Xin Liu,Wei Li,Qing Tao*

Main category: cs.LG

TL;DR: 论文提出AdaMI攻击方法，使用基于动量的自适应矩阵优化对抗样本生成，解决MI-FGSM不收敛问题，提升对抗样本的迁移性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的对抗攻击方法（如PGD、MI-FGSM）都使用sign函数缩放扰动，从优化理论角度看存在理论问题。作者发现PGD实际上是使用当前梯度确定步长的投影梯度法的特定重构，而使用传统自适应矩阵和累积梯度缩放扰动时，PGD就变成了AdaGrad。这启发了新的攻击方法设计。

Method: 提出AdaMI攻击方法，使用基于动量的自适应矩阵优化扰动。该方法被证明在凸问题上能达到最优收敛，解决了MI-FGSM的不收敛问题，确保优化过程的稳定性。

Result: 实验表明，提出的基于动量的自适应矩阵可以作为通用有效技术，在不同网络上提升对抗样本的迁移性，优于最先进方法，同时保持更好的稳定性和不可感知性。

Conclusion: AdaMI通过引入基于动量的自适应矩阵，解决了现有对抗攻击方法的理论缺陷，在保证稳定性和不可感知性的同时，显著提升了对抗样本的迁移能力。

Abstract: Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.

</details>


### [54] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 论文提出使用随机桥（random-bridges）作为生成模型中的随机传输方法，相比传统方法能以更少步骤生成高质量样本，计算成本低且适合高速生成任务。


<details>
  <summary>Details</summary>
Motivation: 在生成建模领域，需要高效的随机传输方法来连接两个概率分布。传统方法通常需要较多步骤，计算成本高。随机桥作为条件随机过程，能够提供灵活的时间演化模式（马尔可夫/非马尔可夫、连续/不连续/混合），为生成建模提供新的高效框架。

Method: 基于随机桥理论，将生成过程建模为在固定时间点取目标分布的随机过程。通过适当初始化，随机桥可以作为两个概率分布之间的随机传输。从一般概率陈述出发，推导出具体的学习和模拟算法表示。实证研究基于高斯随机桥实现。

Result: 基于高斯随机桥的实验结果表明，相比传统方法，能以显著更少的步骤生成高质量样本，同时获得有竞争力的Fréchet Inception Distance分数。分析证明该框架计算成本低，适合高速生成任务。

Conclusion: 随机桥为生成建模提供了有效的随机传输框架，具有计算效率高、生成速度快、灵活性强的优势，能够以更少步骤实现高质量样本生成，适合实际应用中的高速生成需求。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [55] [Understanding and Improving Hyperbolic Deep Reinforcement Learning](https://arxiv.org/abs/2512.14202)
*Timo Klein,Thomas Lang,Andrii Shkabrii,Alexander Sturm,Kevin Sidak,Lukas Miklautz,Claudia Plant,Yllka Velaj,Sebastian Tschiatschek*

Main category: cs.LG

TL;DR: Hyper++：一种稳定的双曲深度强化学习代理，通过改进双曲特征空间中的梯度优化，解决了PPO训练中的不稳定性问题，在ProcGen和Atari-5上表现优异。


<details>
  <summary>Details</summary>
Motivation: 双曲特征空间能自然捕捉复杂RL环境中的层次和关系结构，但现有方法面临优化挑战，特别是由于RL的非平稳性导致的大范数嵌入梯度不稳定问题。

Method: 通过分析Poincaré Ball和Hyperboloid模型中的梯度，识别训练失败原因，提出Hyper++：1) 使用分类值损失稳定critic训练；2) 特征正则化保证有界范数；3) 采用更优化友好的双曲网络层公式。

Result: 在ProcGen上保证稳定学习，超越先前双曲代理，减少约30%的墙钟时间；在Atari-5上使用Double DQN时，显著优于欧几里得和双曲基线方法。

Conclusion: Hyper++通过解决双曲特征空间中的梯度优化问题，实现了稳定高效的深度强化学习，为利用双曲几何进行RL表示学习提供了实用解决方案。

Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

</details>


### [56] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出LLM compare方法，通过大语言模型进行成对难度比较并计算Bradley-Terry分数，用于评估超出人类和现有模型能力范围的分布外问题的难度。


<details>
  <summary>Details</summary>
Motivation: 现有难度评估方法（如人工校准或基于性能的评分）无法扩展到分布外问题，因为这些方法不可扩展、耗时且依赖真实答案。需要一种能够评估当前人类和LLM都无法解决的难题的难度测量方法。

Method: 使用大语言模型进行成对难度比较，然后基于比较结果计算Bradley-Terry分数。该方法具有连续性、动态性、模型无关性且不依赖真实答案信息。

Result: 1) 概念框架验证显示LLM compare占据所有理想象限；2) 与人工标注高度一致（Pearson r≥0.80，n=1876）；3) 对幻觉具有鲁棒性（10%噪声注入下Pearson相关性下降小于6%）。

Conclusion: LLM compare是首个连续、动态、模型无关且不依赖真实答案的难度测量方法，能够有效替代耗时的人工标注和合成数据生成，对课程设计、模型评估和AI辅助研究构思有重要推动作用。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [57] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: 论文通过理论分析证明，在多模态表示学习中，使用预训练模型进行数据过滤（教师过滤）能显著提升对比学习性能，特别是在数据质量较低时效果更明显。


<details>
  <summary>Details</summary>
Motivation: 现代多模态表示学习依赖互联网规模数据集，但原始网络数据质量参差不齐，数据筛选成为关键步骤。教师过滤方法（使用预训练模型计算质量分数）在实践中表现良好，但缺乏理论解释。

Method: 采用标准双模态数据生成模型，在线性对比学习框架下分析数据过滤的效果。通过理论推导，对比有无过滤情况下的误差上界和下界。

Result: 理论证明：1）无过滤时误差上下界为1/(η√n)；2）使用教师过滤后，在大η（高质量数据）情况下误差上界为1/√(ηn)，在小η（低质量数据）情况下误差上界为1/√n，显示出过滤的明显优势。

Conclusion: 教师过滤在多模态表示学习中具有理论保证的优越性，特别是在数据质量较差时能显著提升学习效果，为实践中广泛使用的数据筛选方法提供了理论依据。

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [58] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 本文提出了一种从数据中学习反应扩散系统的方法，确保学习模型的物理一致性和适定性，通过修改参数化反应项来保证质量守恒和准正性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动模型学习反应扩散系统时，往往无法保证物理一致性（如质量守恒、非负性）和适定性，这限制了模型的可靠性和可解释性。

Method: 基于正则化框架，提出系统修改参数化反应项的技术，使其固有满足质量守恒和准正性；扩展理论结果，证明学习问题解收敛到极限系统的唯一正则化最小化解。

Result: 开发了能保证物理一致性的反应扩散系统学习方法，提供了准正性函数的逼近结果，证明了学习问题解在强制守恒律和准正性条件下的收敛性。

Conclusion: 该方法推进了可解释、可靠的数据驱动反应扩散模型的发展，确保模型符合基本物理定律，为物理一致性机器学习提供了理论保证。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [59] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 该论文提出了一种名为RGM的新方法，用于评估图生成模型，克服了传统MMD指标的局限性，并对GRAN和EDGE两种先进模型进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 图生成在许多领域至关重要，但现有评估方法主要依赖最大均值差异(MMD)来衡量生成图的性质分布，这种方法存在局限性，需要更有效的评估方法。

Method: 提出了RGM（Representation-aware Graph-generation Model evaluation）评估方法，使用几何深度学习模型在专门设计的合成和真实图数据集上进行图分类任务，以此评估图生成模型的性能。

Result: 对GRAN和EDGE两种先进图生成模型的评估显示，虽然它们能生成具有某些拓扑性质的图，但在保留区分不同图域的结构特征方面存在显著局限性。同时证实了MMD作为评估指标的不足。

Conclusion: 需要超越MMD的替代评估方法，图生成模型在保留结构特征方面仍有改进空间，RGM方法为未来研究提供了更有效的评估框架。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [60] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME是一个轻量级时间序列基础模型家族，支持确定性和概率性预测，通过生成式概率建模确保效率和鲁棒性，在零样本预测任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测需要既高效又鲁棒的模型，能够同时处理确定性和概率性预测任务，并在零样本场景下表现良好。

Method: 使用Legendre Memory增强泛化能力，采用LegT和LegS变体在编码和解码阶段捕获数据内在归纳偏置；采用Normalization Flow预测头建模复杂分布。

Result: 在TSFM-Bench和ProbTS等基准测试中，FLAME在确定性和概率性预测任务上均展现出一致的SOTA零样本性能。

Conclusion: FLAME是一个高效且强大的时间序列基础模型，通过创新的架构设计在零样本预测任务中表现出色，为时间序列分析提供了新的解决方案。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [61] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 提出基于决策树的解释性贝叶斯优化替代模型，解决传统高斯过程模型解释性差、处理分类数据困难、计算复杂的问题，在尖峰函数上表现更优


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯过程的偏好贝叶斯优化方法存在三个主要问题：模型难以解释、处理分类数据困难、计算复杂度高，限制了在实际场景中的应用

Method: 引入基于决策树的解释性替代模型，能够同时处理分类和连续数据，并扩展到大型数据集，使用历史偏好数据加速新用户的优化过程

Result: 在八个尖峰优化函数上的实验表明，该模型在尖峰函数上优于基于高斯过程的替代方法，在非尖峰函数上性能仅略低；在真实Sushi数据集上成功学习个体寿司偏好

Conclusion: 决策树基替代模型为偏好贝叶斯优化提供了更实用、可解释且可扩展的解决方案，特别适用于尖峰函数和包含分类数据的实际应用场景

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [62] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: Hopfield网络通过最小能量流训练时，会隐式地偏向范数效率高的解，从而在群结构数据中自动学习图同构类，样本复杂度为多项式级别。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在群结构数据训练中如何隐式地学习对称性，特别是Hopfield网络如何从少量随机样本中推断图同构类。

Method: 使用经典Hopfield网络，通过最小能量流（MEF）的梯度下降训练，分析网络在群结构数据上的学习行为，研究参数如何收敛到不变子空间。

Result: 发现图同构类可以在三维不变子空间中表示；MEF训练具有隐式偏向范数效率高的解，这支撑了学习同构类的多项式样本复杂度；多种学习规则下参数都收敛到不变子空间。

Conclusion: Hopfield网络泛化的统一机制是：学习过程中对范数效率的偏向驱动了在群结构数据下近似不变性的涌现。

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [63] [Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis](https://arxiv.org/abs/2512.14361)
*Nicholas Tagliapietra,Katharina Ensinger,Christoph Zimmer,Osman Mian*

Main category: cs.LG

TL;DR: CaDyT是一种用于动态系统因果发现的新方法，利用基于差异的因果模型和高斯过程推理来处理连续时间动态，在规则和不规则采样数据上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统根据其潜在因果关系在连续时间中演化，但现有方法要么离散化时间（在非规则采样数据上表现差），要么忽略底层因果关系，需要一种能同时解决这两个挑战的方法。

Method: CaDyT基于差异因果模型，使用精确高斯过程推理建模连续时间动态，通过算法马尔可夫条件和最小描述长度原则指导的贪婪搜索来识别因果结构。

Result: 实验表明CaDyT在规则和不规则采样数据上都优于现有最先进方法，发现的因果网络更接近真实底层动态。

Conclusion: CaDyT通过结合连续时间建模和因果发现，为动态系统提供了更准确的因果结构识别方法，特别是在处理非规则采样数据时表现出色。

Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.

</details>


### [64] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 首个基于提升量子差分隐私的黑盒隐私审计框架，通过量子金丝雀检测QML模型记忆化并量化隐私泄露


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在敏感数据上训练时存在记忆化个体记录的风险，而现有的量子差分隐私机制缺乏对部署模型的实证验证工具

Method: 基于提升量子差分隐私，利用量子金丝雀（策略性偏移编码的量子态）来检测记忆化，并通过金丝雀偏移与迹距离界限的数学连接来量化隐私泄露

Result: 建立了金丝雀偏移与迹距离界限的严格数学连接，推导出隐私预算消耗的经验下界，在模拟和物理量子硬件上验证了框架有效性

Conclusion: 该框架填补了理论保证与实际隐私验证之间的关键空白，为QML系统提供了强大的隐私验证能力

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [65] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: RePo是一种新颖的上下文重定位机制，通过可微分模块动态分配token位置来减少额外认知负荷，提升LLM在噪声上下文、结构化数据和长上下文任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM架构采用固定线性或常数位置索引，这种无信息结构增加了额外认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。

Method: 提出RePo机制，使用可微分模块f_φ动态分配token位置以捕捉上下文依赖关系，而非依赖预定义的整数范围。在OLMo-2 1B骨干网络上进行持续预训练。

Result: RePo显著提升了在噪声上下文、结构化数据和长上下文任务上的性能，同时在一般短上下文任务上保持竞争力。分析显示RePo能更好地关注远距离相关信息，在密集非线性空间中分配位置，并捕捉输入上下文的内在结构。

Conclusion: RePo通过减少额外认知负荷，为LLM提供了更有效的上下文表示机制，在多种复杂任务上展现出优越性能，同时保持通用能力。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [66] [SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design](https://arxiv.org/abs/2512.14397)
*Yunjia Yang,Weishao Tang,Mengxin Liu,Nils Thuerey,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: SuperWing是一个包含4,239个参数化机翼几何形状和28,856个RANS流场解的开源数据集，用于训练可推广的三维机翼气动预测机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 现有三维机翼数据集稀缺且多样性有限，限制了机器学习代理模型在气动设计中的通用性和推广能力。

Method: 使用简化的几何参数化方法生成机翼形状，包含翼展方向的翼型变化、扭转和上反角，在典型飞行包线内进行广泛的马赫数和攻角模拟。

Result: 数据集包含4,239个机翼几何形状和28,856个RANS流场解；基于该数据集训练的Transformer模型在表面流动预测上达到2.5阻力计数误差，并在DLR-F6和NASA CRM等复杂基准机翼上表现出强大的零样本泛化能力。

Conclusion: SuperWing数据集具有足够的多样性和实用性，能够支持开发可推广的三维机翼气动预测机器学习模型，为实际工程应用提供潜力。

Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.

</details>


### [67] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: GRAFT模型通过文本引导的跨注意力机制，将多源文本信息与电力负荷数据对齐，实现电网感知的负荷预测，并在澳大利亚五州数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 电力负荷同时受到天气、日历节奏、突发事件和政策等多时间尺度外生因素的影响，需要开发能够有效整合多源文本信息的电网感知预测方法。

Method: 改进STanHOP模型，严格对齐日聚合新闻、社交媒体和政策文本与半小时负荷数据，通过跨注意力机制实现文本引导的融合，并提供即插即用的外部记忆接口。

Result: 在澳大利亚五州2019-2021年数据集上，GRAFT在小时、日、月三个时间尺度上显著超越基线方法，达到或超过SOTA性能，在事件驱动场景中表现稳健。

Conclusion: GRAFT通过文本引导的融合机制有效整合多源信息，为电力负荷预测提供了标准化评估框架，并支持文本到负荷效应的时空定位和源级解释。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [68] [Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space](https://arxiv.org/abs/2512.14418)
*Dejun Hu,Zhiming Li,Jia-Rui Shen,Jia-Ning Tu,Zi-Hao Ye,Junliang Zhang*

Main category: cs.LG

TL;DR: 提出双轴表示-完全收敛学习(RCCL)策略，通过FD25数据集实现有机分子化学空间的近乎完全覆盖，使图神经网络达到表示完全收敛学习和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习正在重塑分子和材料建模，但化学空间极其庞大(10^30-10^60)，模型能否在整个化学空间实现收敛学习仍是一个开放的科学问题。

Method: 提出双轴表示-完全收敛学习(RCCL)策略，结合图卷积网络(GCN)编码局部价键环境和无桥图(NBG)编码环/笼拓扑结构，构建FD25数据集覆盖13,302个局部价键单元和165,726个环/笼拓扑。

Result: 在FD25上训练的图神经网络表现出表示完全收敛学习和强分布外泛化能力，在外部基准测试中整体预测误差约为1.0 kcal/mol MAE。

Conclusion: 建立了分子表示、结构完整性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能奠定了基础。

Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.

</details>


### [69] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: ClimaX-LETKF是首个纯数据驱动的机器学习集合天气预报系统，通过同化观测数据实现多年稳定运行，研究发现RTPP比RTPS更适合MLWP模型


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习天气预报取得显著进展，但在MLWP模型中同化真实观测或集合预报的研究仍然有限，需要开发独立于数值天气预报模型的纯数据驱动系统

Method: 提出ClimaX-LETKF系统，基于局部集合变换卡尔曼滤波，同化NCEP ADP全球高空和地面天气观测数据，比较RTPP和RTPS两种扰动松弛方法

Result: 系统能稳定运行多年，RTPP比RTPS提供更好的稳定性和准确性，而NWP模型通常更适合RTPS；MLWP模型恢复大气场到吸引子的能力弱于NWP模型

Conclusion: 这项工作为增强MLWP集合预报系统提供了宝贵见解，代表了向实际应用迈出的重要一步，揭示了MLWP与NWP模型在扰动处理上的不同特性

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [70] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: AnySleep是一个深度神经网络模型，能够使用任意EEG或EOG数据在可调时间分辨率下进行睡眠分期，在21个数据集上训练，性能优于现有基线，并支持亚30秒时间尺度分析。


<details>
  <summary>Details</summary>
Motivation: 睡眠研究需要人工睡眠分期，这是劳动密集型的步骤。传统PSG记录以30秒为周期评分是基于实用而非生理原因，且不同中心在电极数量、配置和受试者特征上差异很大，这给多中心睡眠研究和发现短时间尺度生物标志物带来了挑战。

Method: 开发了AnySleep深度神经网络模型，使用任意EEG或EOG数据进行睡眠分期。模型在21个数据集的超过19,000个夜间记录上训练和验证，涵盖近200,000小时的EEG和EOG数据，以促进跨站点的鲁棒泛化。

Result: 模型达到最先进性能，在30秒周期上优于或等于现有基线。提供更多通道时性能提高，但在EOG缺失或仅有EOG或单个EEG导联（额叶、中央或枕叶）时仍保持良好性能。在亚30秒时间尺度上，模型能捕捉与觉醒一致的短暂觉醒，并改善对生理特征（年龄、性别）和病理生理状况（睡眠呼吸暂停）的预测。

Conclusion: AnySleep模型能够处理异构电极设置，促进大规模睡眠研究，并加速发现新的睡眠生物标志物。模型已公开可用，以支持不同电极配置的研究。

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [71] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: Kinetic-Mamba：基于Mamba的神经算子框架，用于化学动力学建模，通过三种模型变体预测热化学状态演化，在合成气和GRI-Mech 3.0反应机制上表现出高精度。


<details>
  <summary>Details</summary>
Motivation: 准确的化学动力学建模对燃烧模拟至关重要，传统方法难以处理复杂反应路径和热化学状态演化。需要结合神经算子的表达能力与Mamba架构的高效时序建模能力。

Method: 提出Kinetic-Mamba框架，包含三种模型：(1)独立Mamba模型预测热化学状态变量时间演化；(2)约束Mamba模型在保持质量守恒的同时学习状态动力学；(3)基于温度区间的双Mamba模型架构。还开发了潜在空间变体，在降维潜在空间中演化动力学并在物理流形上重构完整状态。

Result: 在合成气和GRI-Mech 3.0反应机制上的计算实验表明，该框架仅使用状态变量的初始条件就能高保真地预测复杂动力学行为。通过时间分解和递归预测策略评估了准确性和鲁棒性，并在分布外数据集上测试了外推能力。

Conclusion: Kinetic-Mamba框架成功整合了神经算子的表达能力与Mamba架构的高效时序建模能力，为化学动力学建模提供了高精度解决方案，仅需初始条件即可预测复杂反应行为，在燃烧模拟中具有应用潜力。

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [72] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 使用变分自编码器生成合成心房电图，解决心脏电生理成像中配对数据稀缺问题，并通过数据增强提升下游深度学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 心房颤动是最常见的心律失常，需要准确表征心房电活动。非侵入性心电图成像结合深度学习估计心内电图有前景，但受限于配对体表电位-心内电图数据集的稀缺。

Method: 提出两种变分自编码器模型：窦性心律专用VAE（VAE-S）和类别条件VAE（VAE-C），后者可同时处理窦性心律和房颤信号。使用形态学、频谱和分布相似性指标评估生成的心电图。

Result: VAE-S在模拟心电图上实现更高保真度，VAE-C支持节律特异性生成但窦性重建质量降低。生成的心电图用于下游非侵入性心电图重建任务的数据增强，适度增强可提升估计性能。

Conclusion: 基于VAE的生成模型有潜力缓解数据稀缺问题，增强基于深度学习的心电图成像流程，为心脏电生理研究提供合成数据支持。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [73] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 该论文批评当前时间序列反事实解释方法在临床推荐场景中的不足，主张转向考虑因果合理性和时间一致性的可持续、目标导向干预方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类的反事实解释方法主要基于静态数据假设，仅关注最小化输入扰动来改变模型预测，这在临床推荐场景中不够充分，因为干预措施随时间展开，需要因果合理性和时间一致性。

Method: 论文通过分析现有方法的局限性，特别是时间盲点和缺乏用户中心考虑，并对几种最先进的时间序列方法进行鲁棒性分析，展示生成的反事实对随机噪声高度敏感。

Result: 分析显示现有方法生成的反事实对随机噪声高度敏感，在真实临床环境中可靠性有限，因为微小测量变异不可避免。

Conclusion: 呼吁开发超越仅改变预测而不考虑可行性或可操作性的方法和评估框架，强调需要在实际场景中对用户可行的、目的驱动的可操作干预措施。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [74] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出Residual GRU with Multi-Head Self-Attention模型，在UCI心脏病数据集上取得最佳性能，准确率0.861，优于传统方法和现代深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要可靠高效的预测工具。传统方法依赖手工特征和临床经验，机器学习方法可提高可重复性但难以在嘈杂异构的临床数据中泛化。

Method: 提出紧凑的深度学习架构，整合残差双向门控循环单元用于特征列的序列建模、通道重加权块、以及带可学习分类token的多头自注意力池化来捕获全局上下文。

Result: 在UCI心脏病数据集上，模型准确率0.861，macro-F1 0.860，ROC-AUC 0.908，PR-AUC 0.904，优于所有基线。消融研究确认各组件贡献，t-SNE可视化显示学习嵌入有更清晰的类别分离。

Conclusion: 轻量级混合循环和注意力架构在临床风险预测中实现了准确性和效率的良好平衡，支持在资源受限的医疗环境中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [75] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: Geo-DeepONet：一种几何感知的深度算子网络，结合有限元离散化的域信息，可在任意非结构化网格上实现精确的算子学习而无需重新训练。基于此开发了几何感知混合预条件迭代求解器，提高了参数PDE求解的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统迭代求解器对参数偏微分方程的收敛行为高度依赖于域和离散化方式。先前提出的混合求解器（结合传统求解器和神经算子）在特定几何上表现良好，但在未训练过的几何上表现不佳。需要一种能适应任意几何形状的鲁棒求解方法。

Method: 提出Geo-DeepONet，一种几何感知的深度算子网络，从有限元离散化中提取域信息。基于此开发几何感知混合预条件迭代求解器，将Geo-DeepONet与传统方法（如松弛方案和Krylov子空间算法）耦合。

Result: 通过在不同非结构化域上的参数PDE数值实验，证明了所提出的混合求解器在多个实际应用中具有增强的鲁棒性和效率。

Conclusion: Geo-DeepONet能够实现跨任意非结构化网格的精确算子学习而无需重新训练，基于此开发的几何感知混合预条件迭代求解器显著提高了参数PDE求解的鲁棒性和效率。

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


### [76] [Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets](https://arxiv.org/abs/2512.14615)
*Omid Khormali*

Main category: cs.LG

TL;DR: 提出OW-HNPV方法，通过测量持久性特征出现和消失的速度来检测时变网络异常，在以太坊交易网络中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要测量累积拓扑存在性，缺乏对拓扑变化速度的考量。需要一种能够自动降噪、数学稳定且适用于不同特征类型网络的方法来检测动态网络中的结构异常。

Method: 提出重叠加权分层归一化持久性速度(OW-HNPV)，这是首个基于速度的持久性图分析方法，通过重叠加权自动降噪，并证明了其数学稳定性。

Result: 在以太坊交易网络(2017年5月-2018年5月)中，OW-HNPV在加密货币异常检测上表现优异，7天价格预测的AUC比基线模型提升10.4%，在中长期预测(4-7天)中表现最稳定。

Conclusion: 建模拓扑速度对于检测动态网络中的结构异常至关重要，OW-HNPV为时变网络分析提供了新的速度视角，在异常检测任务中展现出优越性能。

Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.

</details>


### [77] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: QR-MAX：首个基于模型的RL算法，用于离散非马尔可夫奖励决策过程，通过奖励机实现马尔可夫转移学习与非马尔可夫奖励处理的分解，获得PAC收敛保证和多项式样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 许多实际决策问题涉及依赖整个系统历史的任务，而不仅仅是达到具有期望属性的状态。马尔可夫强化学习方法不适合此类任务，而非马尔可夫奖励决策过程（NMRDPs）使智能体能够处理时间依赖性任务，但该方法长期以来缺乏（近）最优性和样本效率的形式化保证。

Method: 提出QR-MAX算法，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理分解。这是首个利用这种分解实现PAC收敛到ε-最优策略的基于模型RL算法。然后扩展到连续状态空间，提出Bucket-QR-MAX，使用SimHash-based离散化器保持相同的分解结构。

Result: 在复杂度递增的环境上与最先进的基于模型RL方法进行实验比较，显示出样本效率的显著提升和寻找最优策略的鲁棒性增强。

Conclusion: QR-MAX解决了NMRDPs中长期缺乏的形式化保证问题，通过分解方法实现了PAC收敛和多项式样本复杂度，并在连续状态空间中保持了相同的优势结构。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [78] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: 提出PageRank Transformer (ParaFormer)解决图Transformer中的过度平滑问题，通过PageRank增强的注意力模块模拟深层Transformer行为，在节点分类和图分类任务上取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管图Transformer通过全局注意力捕捉全局信息，但研究发现全局注意力本身存在严重的过度平滑问题，导致节点表示变得不可区分，这种效应甚至比GNNs中的过度平滑更严重。

Method: 提出PageRank Transformer (ParaFormer)，采用PageRank增强的注意力模块，模拟深层Transformer的行为，理论上和实证上都证明其作为自适应滤波器能够缓解过度平滑。

Result: 在11个数据集（从数千到数百万节点）的节点分类和图分类任务中，ParaFormer都取得了持续的性能改进，验证了其有效性。

Conclusion: ParaFormer通过PageRank增强的注意力机制有效缓解了图Transformer中的过度平滑问题，在各种规模的图学习任务中表现出优越性能。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [79] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: gridfm-datakit-v1是一个用于生成真实多样电力潮流(PF)和最优潮流(OPF)数据集的Python库，解决了现有数据集在随机性、多样性、超出运行限制场景和可变成本函数方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有电力系统数据集面临三个主要挑战：1)缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；2)PF数据集仅限于OPF可行点，阻碍了机器学习求解器对违反运行限制情况的泛化；3)OPF数据集使用固定发电机成本函数，限制了在不同成本情况下的泛化能力。

Method: 通过结合真实世界负荷曲线的全局负荷缩放与局部噪声，支持任意N-k拓扑扰动来创建多样且真实的数据集；生成超出运行限制的PF样本；生成具有可变发电机成本的OPF数据；并能高效扩展到大型电网（最多10,000个节点）。

Result: 开发了gridfm-datakit-v1库，提供了与OPFData、OPF-Learn、PGLearn和PFΔ的比较，可在GitHub上获取并支持pip安装。

Conclusion: gridfm-datakit解决了现有电力系统数据集的局限性，能够生成更真实、多样且具有挑战性的PF和OPF数据集，有助于训练更鲁棒的机器学习求解器，特别是在处理违反运行限制和可变成本场景时。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [80] [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)
*Rae Chipera,Jenny Du,Irene Tsapara*

Main category: cs.LG

TL;DR: 非光滑激活函数（混沌、随机、分形）在回声状态网络中不仅保持回声状态特性，还比传统光滑函数收敛更快、谱半径容忍度更高，其中康托函数表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算依赖光滑的全局Lipschitz连续激活函数，限制了在国防、灾害响应和药物建模等极端条件下的应用。需要研究非光滑激活函数在回声状态网络中的表现。

Method: 系统研究非光滑激活函数（混沌、随机、分形变体），通过36,610个储层配置的全面参数扫描，引入量化激活函数的理论框架，定义退化回声状态特性(d-ESP)。

Result: 多个非光滑函数不仅保持ESP，在收敛速度和谱半径容忍度上优于传统光滑函数。康托函数ESP一致性行为可达谱半径ρ~10，比tanh和ReLU收敛快2.6倍。发现临界拥挤比Q=N/k预测离散激活函数失效阈值。

Conclusion: 预处理拓扑而非连续性本身决定稳定性：单调压缩预处理保持ESP，而分散或不连续预处理触发急剧失效。某些分形函数的优异性能机制仍待解释，表明对激活函数几何特性如何影响储层动态的理解存在根本性差距。

Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.

</details>


### [81] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 开发了一个多模态机器学习框架EWI，通过整合临床和运营数据预测ICU入院、急救团队派遣和死亡风险，在18,633名患者数据上达到C-statistic 0.796，已部署为医院分诊工具。


<details>
  <summary>Details</summary>
Motivation: 医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者对患者护理质量监测和医生护理管理都至关重要，但将不同数据流转化为准确可解释的风险评估面临数据格式不一致的挑战。

Method: 开发了多模态机器学习框架EWI（早期预警指数），采用人机协作流程：临床医生帮助确定警报阈值和解释模型输出，使用SHAP（Shapley Additive exPlanations）增强可解释性，突出显示驱动每个患者风险的临床和运营因素（如预定手术、病房普查）。从结构化和非结构化电子健康记录数据中自动提取特征。

Result: 在美国一家大型医院的18,633名独特患者数据集上，EWI实现了C-statistic 0.796的性能。已部署在医院仪表板中，将患者分为三个风险层级，目前用作主动管理风险患者的分诊工具。

Conclusion: 该方法通过自动对不同风险水平的患者进行分类，为医生节省了宝贵时间，使他们能够专注于患者护理而非筛选复杂的EHR数据。通过进一步确定具体的风险驱动因素，该模型为护理人员调度和关键资源分配提供了数据驱动的调整依据，从而帮助临床医生和管理人员避免下游并发症，改善整体患者流程。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


### [82] [Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean](https://arxiv.org/abs/2512.14686)
*Chuan He*

Main category: cs.LG

TL;DR: 该论文研究了在重尾噪声（尾指数α∈(0,2]）下随机一阶方法的复杂度，通过分析梯度裁剪的偏差-方差权衡，为全范围尾指数建立了统一的复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有随机一阶方法复杂度研究主要针对α∈(1,2]的有限均值噪声，当α接近1时复杂度趋于无穷大。实际应用中存在更广泛的重尾噪声（包括无限均值情况），需要更全面的理论分析。

Method: 通过分析梯度裁剪中的偏差-方差权衡，提出新分析方法，控制噪声尾部的对称性度量，为全范围尾指数α∈(0,2]建立统一的复杂度保证。

Result: 当噪声尾部对称性得到控制时，裁剪的随机一阶方法在重尾噪声下对所有尾指数α∈(0,2]都能获得改进的复杂度保证，包括有界方差到无限均值的各种情况。

Conclusion: 该研究填补了重尾噪声下随机优化理论的重要空白，为全范围尾指数提供了统一的复杂度分析框架，并通过数值实验验证了理论发现。

Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $α$ of the noise. Nonetheless, existing complexity results often cover only the case $α\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $α$ approaches $1$. This paper tackles the general case of noise with tail index $α\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $α\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [83] [Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement](https://arxiv.org/abs/2512.14151)
*Songze Liu,Hongkun Du,Shaowen Wang*

Main category: cs.AR

TL;DR: 提出ACPC机制，结合TCN预测和优先级感知替换策略，优化LLM推理中的缓存污染问题，提升缓存命中率和系统性能。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的token序列查找和嵌入向量检索产生高度不规则、突发性的访问模式，导致传统预取和替换策略误判，引发严重缓存污染，降低系统性能。

Method: 提出自适应缓存污染控制机制，集成基于时间卷积网络的访问预测和优先级感知替换策略。TCN模块学习token访问序列的时间依赖关系识别高重用缓存行，替换策略根据预测重用可能性和缓存占用动态调整驱逐优先级。

Result: 相比最先进的机器学习替换基线，ACPC减少缓存污染41.7%，提高缓存命中率8.9%，降低L2缺失惩罚60.0%。TCN-ACPC框架提高token生成吞吐量15.9%，达到最低最终损失0.21。

Conclusion: ACPC能有效识别有用缓存行，减少动态LLM访问行为下的冗余预取，为大规模LLM服务系统提供可扩展的学习驱动解决方案，优化内存效率和延迟。

Abstract: Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.

</details>


### [84] [ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework](https://arxiv.org/abs/2512.14172)
*Qijun Zhang,Shang Liu,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: ReadyPower是一个新的分析型功耗建模框架，通过引入架构级、实现级和技术级参数来改进McPAT模型，解决了传统分析模型不准确和机器学习模型不可靠、难解释、难使用的问题。


<details>
  <summary>Details</summary>
Motivation: 现代处理器设计中功耗是主要目标，需要准确高效的功耗建模技术。传统分析模型（如McPAT）存在显著不准确性，而基于机器学习的方法虽然准确但存在不可靠性、可解释性差和使用困难等问题，限制了工业界应用。

Method: 提出ReadyPower框架，通过向广泛采用的McPAT分析模型引入三个层次的参数：架构级、实现级和技术级参数。这些参数通过不同方式确定，以弥合实际处理器实现与分析模型之间的差异。

Result: 在不同训练场景下，ReadyPower相比基于机器学习的基线方法，平均实现了>20%更低的平均绝对百分比误差（MAPE）和>0.2更高的相关系数R，在BOOM和XiangShan两种CPU架构上均表现优异。

Conclusion: ReadyPower提供了一个可靠、可解释且易于使用的功耗建模框架，解决了传统分析模型准确性不足和机器学习模型实际应用困难的问题，为早期功耗优化和设计空间探索提供了实用工具。

Abstract: Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.

</details>


### [85] [TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips](https://arxiv.org/abs/2512.14256)
*Huizheng Wang,Taiquan Wei,Zichuan Wang,Dingcheng Jiang,Qize Yang,Jiaxin Liu,Jingxiang Hou,Chao Li,Jinyi Deng,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: TEMP框架通过拓扑感知的张量流分区、流量感知映射和双层晶圆求解，优化晶圆级芯片上的LLM训练，实现1.7倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 晶圆级芯片(WSCs)虽然提供高计算能力和芯片间带宽，但面临片上内存与计算资源的权衡限制。现有张量并行策略无法充分利用WSCs的通信优势同时保持内存效率。

Method: 提出张量流分区范式(TSPP)，利用WSCs丰富的通信带宽缓解片上内存约束。针对WSCs的2D网格拓扑缺乏长距离灵活互连的问题，开发TEMP框架，包含：1)拓扑感知的张量流分区，2)流量感知映射，3)双层晶圆求解。

Result: TEMP在各种模型上相比最先进的LLM训练系统实现了1.7倍的平均吞吐量提升，有效克服了WSCs的硬件限制和并行化挑战。

Conclusion: TEMP框架通过集成拓扑感知、流量优化和高效求解方法，成功解锁了TSPP在晶圆级芯片上的全部潜力，显著提升了LLM训练性能。

Abstract: Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.
  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.
  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.

</details>


### [86] [PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion](https://arxiv.org/abs/2512.14322)
*Huizheng Wang,Hongbin Wang,Zichuan Wang,Zhiheng Yue,Yang Wang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: PADE：一种无需预测器的算法-硬件协同设计，用于动态稀疏注意力加速，通过位级不确定性区间保护过滤、双向稀疏性乱序执行和交错稀疏分块注意力等技术，显著提升硬件效率和能耗比。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的模型虽然革命性，但自注意力的二次计算成本带来严重的计算和内存开销。现有稀疏注意力方法因需要额外的稀疏性预测器而缺乏实用性，硬件效率严重下降。

Method: 提出PADE算法-硬件协同设计，包含三个关键技术：1) 位级不确定性区间保护过滤策略，在每个位轮次准确识别无关紧要的token；2) 双向稀疏性乱序执行，提高硬件利用率；3) 交错稀疏分块注意力，降低I/O和计算复杂度。

Result: 在22个基准测试上，PADE相比Nvidia H100 GPU实现7.43倍加速和31.1倍能效提升；相比SOTA加速器Sanger、DOTA和SOFA，分别实现5.1倍、4.3倍和3.4倍的能耗节省。

Conclusion: PADE通过创新的算法-硬件协同设计，实现了无需额外稀疏性预测器的实用稀疏注意力加速，显著提升了硬件效率和能耗比，为动态稀疏注意力加速提供了有效的解决方案。

Abstract: Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.
  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.
  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.

</details>


### [87] [Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models](https://arxiv.org/abs/2512.14661)
*Chiyue Wei,Cong Guo,Junyao Zhang,Haoxuan Shan,Yifan Xu,Ziyue Zhang,Yudong Liu,Qinsi Wang,Changchun Zhou,Hai "Helen" Li,Yiran Chen*

Main category: cs.AR

TL;DR: Focus提出了一种流式聚焦架构，通过渐进式细粒度冗余消除来高效加速视觉语言模型推理，实现2.4倍加速和3.3倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视频理解任务中表现出色，但模型规模增大和视频级输入导致计算和内存开销巨大，给硬件加速器实时部署带来挑战。现有方法通常采用粗粒度令牌剪枝或合并，存在运行时开销高的问题。

Method: 提出Focus流式聚焦架构，采用多层次聚焦范式：1) 基于文本提示的语义引导令牌剪枝；2) 使用局部比较的空间-时间块级聚焦；3) 通过运动感知匹配的向量级冗余消除。所有步骤与架构协同设计，支持流式友好、片上执行。

Result: 在脉动阵列加速器中实现Focus模块化单元，相比最先进加速器实现2.4倍加速和3.3倍能耗降低，在性能和能效方面均显著超越。

Conclusion: Focus通过渐进式细粒度冗余消除有效解决了VLM推理的计算和内存瓶颈，为硬件加速器上的实时部署提供了高效解决方案，已开源完整实现。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [88] [Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing](https://arxiv.org/abs/2512.14002)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: 提出SARound算法解决VEC中带截止时间约束的任务卸载与资源分配问题，将近似比从1/6提升到1/4，并设计在线服务订阅框架，通过VecSim仿真验证性能优越性


<details>
  <summary>Details</summary>
Motivation: 车载边缘计算(VEC)中，由于网络带宽和计算资源受限、任务截止时间严格、网络条件快速变化，高效的任务卸载和资源分配对时间关键型应用仍具挑战性

Method: 1. 将问题建模为带截止时间约束的任务卸载与资源分配问题(DOAP)；2. 提出SARound近似算法，基于线性规划舍入和局部比率技术；3. 设计在线服务订阅和卸载控制框架；4. 开发VEC仿真器VecSim，集成所提框架管理实时车载任务全生命周期

Result: SARound算法将DOAP问题的最佳已知近似比从1/6提升到1/4，在基于真实出租车轨迹数据和目标检测应用的实验中，SARound在不同网络条件下始终优于现有基线方法，同时保持运行效率

Conclusion: 提出的SARound算法和在线框架有效解决了VEC中带截止时间约束的任务卸载与资源分配问题，显著提升了系统性能，为智能交通系统中的实时应用提供了可行的解决方案

Abstract: Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading and resource allocation for time-critical applications in VEC remain challenging due to constrained network bandwidth and computational resources, stringent task deadlines, and rapidly changing network conditions. To address these challenges, we formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), denoted as $\mathbf{P}$, in VEC with both bandwidth and computational resource constraints, aiming to maximize the total vehicle utility. To solve $\mathbf{P}$, we propose $\mathtt{SARound}$, an approximation algorithm based on Linear Program rounding and local-ratio techniques, that improves the best-known approximation ratio for DOAP from $\frac{1}{6}$ to $\frac{1}{4}$. Additionally, we design an online service subscription and offloading control framework to address the challenges of short task deadlines and rapidly changing wireless network conditions. To validate our approach, we develop a comprehensive VEC simulator, VecSim, using the open-source simulation libraries OMNeT++ and Simu5G. VecSim integrates our designed framework to manage the full life-cycle of real-time vehicular tasks. Experimental results, based on profiled object detection applications and real-world taxi trace data, show that $\mathtt{SARound}$ consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency.

</details>


### [89] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出一种混合主动-反应式自动扩缩算法，结合机器学习预测和实时调整，在Kubernetes中实现，将SLA违规率从23%降低到6%


<details>
  <summary>Details</summary>
Motivation: 边缘计算需要低延迟以满足SLA要求，但现有自动扩缩算法存在性能问题和配置复杂性，无法保证SLA合规性

Method: 提出混合自动扩缩算法：1) 基于机器学习的主动扩缩器预测资源需求；2) 反应式扩缩器基于当前资源利用率和SLA约束进行即时调整；集成到Kubernetes作为扩展

Result: 在边缘环境中进行广泛实验，现有解决方案SLA违规率高达23%，而提出的混合解决方案SLA违规率仅为6%，在各种应用中确保稳定的SLA合规性

Conclusion: 提出的混合主动-反应式自动扩缩算法能有效降低SLA违规率，在边缘计算环境中提供更好的资源管理和SLA保证

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [90] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: 分析屏障同步对并行计算系统稳定性和性能的影响，特别针对Apache Spark的屏障执行模式，研究屏障导致的空闲时间和性能开销。


<details>
  <summary>Details</summary>
Motivation: 许多并行化机器学习工作负载需要任务同步（屏障），Apache Spark新增的屏障执行模式允许用户添加屏障，但屏障会导致部分工作节点空闲，降低系统稳定性和性能。

Method: 1) 分析(s,k,l)屏障系统的稳定性，允许任务在完成l/k后离开；2) 推导混合屏障系统的性能界限，服务有无屏障的混合作业；3) 针对纯1-屏障情况，将界限和模拟结果与Spark系统基准数据对比；4) 研究真实系统中的开销分布，归因于双重事件和轮询驱动调度机制；5) 为此类开销建立模型并通过模拟验证。

Result: 建立了屏障系统的稳定性分析框架，推导了混合屏障系统的性能界限，识别了真实Spark系统中屏障开销的来源（双重事件和轮询驱动调度），并开发了相应的开销模型。

Conclusion: 屏障同步确实会降低并行计算系统的稳定性和性能，通过建立数学模型可以量化这种影响，为优化屏障执行模式提供了理论基础。

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [91] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Björkqvist*

Main category: cs.DC

TL;DR: PruneX是一个分布式数据并行训练系统，通过协同设计剪枝算法与集群层次结构来减少多节点GPU集群中的节点间通信开销，实现约60%的通信量减少和6.75倍的强扩展加速。


<details>
  <summary>Details</summary>
Motivation: 在多节点GPU集群上进行大规模分布式训练时，节点间通信带宽日益成为瓶颈。传统的剪枝感知分布式训练系统通常无法有效减少通信开销，因为非结构化稀疏性无法被高度优化的密集集体原语有效利用。

Method: PruneX提出了层次化结构化ADMM（H-SADMM）算法，在节点间同步前强制执行节点级结构化稀疏性，实现动态缓冲区压缩，消除零值传输和索引开销。系统采用领导者-跟随者执行模型，分离节点内和节点间进程组，在带宽受限的链路上对压缩张量执行密集集体操作，同时将完全同步限制在高带宽的节点内互连上。

Result: 在64个GPU上对ResNet架构的评估表明，PruneX减少了约60%的节点间通信量，实现了6.75倍的强扩展加速，优于密集基线（5.81倍）和Top-K梯度压缩（3.71倍）。

Conclusion: PruneX通过协同设计剪枝算法与集群层次结构，有效解决了分布式训练中的通信瓶颈问题，显著提升了大规模训练的效率。

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>
