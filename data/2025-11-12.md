<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.LG](#cs.LG) [Total: 95]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory](https://arxiv.org/abs/2511.08568)
*Jie Ren,Bin Ma,Shuangyan Yang,Benjamin Francis,Ehsan K. Ardestani,Min Si,Dong Li*

Main category: cs.PF

TL;DR: RecMG是一个机器学习引导的系统，用于在分层内存上进行向量缓存和预取，专门针对深度学习推荐模型的内存访问挑战，显著减少了按需获取次数并提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型的内存需求达到TB级别，分层内存架构提供了成本效益高的解决方案，但复杂的嵌入访问模式带来了嵌入向量放置的挑战。

Method: RecMG使用独立的ML模型进行缓存和预取，采用新颖的可微分损失函数来缩小预取搜索空间并最小化按需获取。

Result: 与最先进的时间、空间和基于ML的预取器相比，RecMG分别减少了2.2倍、2.8倍和1.5倍的按需获取。在工业级DLRM推理场景中，RecMG将端到端推理时间减少了高达43%。

Conclusion: RecMG通过机器学习指导的缓存和预取策略，有效解决了分层内存架构中嵌入向量放置的挑战，显著提升了DLRM推理性能。

Abstract: Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing](https://arxiv.org/abs/2511.07665)
*Yuzhe Fu,Changchun Zhou,Hancheng Ye,Bowen Duan,Qiyu Huang,Chiyue Wei,Cong Guo,Hai "Helen'' Li,Yiran Chen*

Main category: cs.AR

TL;DR: FractalCloud是一种基于分形思想的硬件架构，用于高效处理大规模3D点云，通过形状感知的分区方法和块并行操作，在28nm工艺下实现21.7倍加速和27倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 随着点云神经网络处理大规模点云（数十万点），全对全计算和全局内存访问带来O(n²)计算复杂度和内存流量，现有加速器因低效分区和非并行架构而扩展性差。

Method: 提出FractalCloud架构，包含：(1) 分形方法实现形状感知和硬件友好的分区；(2) 块并行点操作分解并并行化所有点操作；(3) 专用硬件设计支持片上分形和灵活并行。

Result: 在28nm工艺下实现1.5mm²核心面积，相比最先进加速器实现21.7倍速度提升和27倍能耗降低，同时保持网络精度。

Conclusion: FractalCloud证明了其在大规模点云神经网络推理中的可扩展性和高效性。

Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.

</details>


### [3] [PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization](https://arxiv.org/abs/2511.07985)
*Simei Yang,Xinyu Shi,Lu Zhao,Yunyu Ling,Quanjun Wang,Francky Catthoor*

Main category: cs.AR

TL;DR: PIMfused是一种硬件-软件协同设计，通过在近内存处理架构中采用融合层数据流来优化CNN执行，减少跨bank数据传输，提升性能、功耗和面积效率。


<details>
  <summary>Details</summary>
Motivation: 传统逐层数据流在DRAM-PIM架构中会导致跨bank数据传输，成为CNN加速的性能瓶颈。需要打破层间bank依赖关系来优化数据传输。

Method: 提出PIMfused硬件-软件协同设计，采用融合层数据流，提高数据重用率并消除跨bank数据依赖，同时保持bank级并行性。

Result: 在4-bank PIMcores配置下，PIMfused相比GDDR6-AiM基线将内存周期减少到30.6%，能耗降低到83.4%，面积减少到76.5%。

Conclusion: 融合层数据流能有效优化DRAM-PIM架构中的CNN执行，显著减少跨bank数据传输，实现全面的PPA收益。

Abstract: Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.

</details>


### [4] [Re$^{\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating](https://arxiv.org/abs/2511.08054)
*Yunqi Shi,Xi Lin,Zhiang Wang,Siyuan Xu,Shixiong Kai,Yao Lai,Chengrui Gao,Ke Xue,Mingxuan Yuan,Chao Qian,Zhi-Hua Zhou*

Main category: cs.AR

TL;DR: Re$^{\text{2}}$MaP方法通过递归原型设计和基于树的重新定位，生成专家级质量的宏布局。该方法通过多级宏分组、PPA感知单元聚类、椭圆优化和基于树的重新定位，显著提升了时序性能。


<details>
  <summary>Details</summary>
Motivation: 现有宏布局方法在时序优化方面存在不足，需要一种能够综合考虑线长、数据流和设计约束的方法来生成高质量的宏布局。

Method: 1. 多级宏分组和PPA感知单元聚类生成统一连接矩阵；2. 使用DREAMPlace构建混合尺寸布局原型；3. ABPlace椭圆优化方法均匀分布宏；4. 基于树的重新定位优化专家启发式成本函数；5. 递归迭代过程逐步定位宏组。

Result: 相比最先进的学术布局器Hier-RTLMP，WNS提升最高22.22%(平均10.26%)，TNS提升最高97.91%(平均33.97%)；相比会议版本ReMaP，在WNS、TNS、功耗、DRC违规和运行时间方面均有提升。

Conclusion: Re$^{\text{2}}$MaP通过递归原型设计和基于树的重新定位，能够生成高质量的宏布局，显著改善时序性能，并在多个指标上优于现有方法。

Abstract: This work introduces the Re$^{\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.

</details>


### [5] [BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning](https://arxiv.org/abs/2511.08315)
*Mingkai Miao,Jianheng Tang,Guangyu Hu,Hongce Zhang*

Main category: cs.AR

TL;DR: BDD2Seq是一个图到序列框架，使用图神经网络编码器和指针网络解码器来预测BDD变量排序，显著降低量子成本和加速可逆电路综合。


<details>
  <summary>Details</summary>
Motivation: 在基于BDD的可逆电路综合中，变量排序影响BDD节点数量和量子成本等关键指标。由于寻找最优变量排序是NP完全问题，现有启发式方法在电路复杂度增加时性能下降。

Method: 提出BDD2Seq框架，将电路网表视为图，使用图神经网络编码器学习结构依赖关系，指针网络解码器预测变量排序，并采用多样化束搜索提高结果质量。

Result: 在三个公共基准测试上，BDD2Seq实现了约1.4倍更低的量子成本和3.7倍更快的综合速度，优于现代启发式算法。

Conclusion: 这是首个使用基于图的生成模型和多样性促进解码来解决BDD可逆电路综合中变量排序问题的工作，通过学习结构依赖关系实现了显著性能提升。

Abstract: Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.

</details>


### [6] [DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator](https://arxiv.org/abs/2511.08395)
*Xingyu Liu,Jiawei Liang,Yipu Zhang,Linfeng Du,Chaofang Ma,Hui Yu,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 提出基于FPGA的硬件高效RBD加速器，包含三个关键创新：精度感知量化框架、除法延迟优化和模块间DSP复用方法，实现了8倍吞吐量提升和7.4倍延迟降低。


<details>
  <summary>Details</summary>
Motivation: 为高自由度机器人系统开发高效的硬件加速器，解决现有RBD加速器在性能和资源利用方面的不足。

Method: 1. 精度感知量化框架减少DSP需求；2. 质量矩阵求逆算法中的除法延迟优化；3. 模块间DSP复用方法提高DSP利用率。

Result: 相比最先进的RBD加速器，在各种机器人类型上实现了8倍吞吐量提升和7.4倍延迟降低。

Conclusion: 该工作证明了所提方法在硬件效率和可扩展性方面的有效性，特别适用于高自由度机器人系统。

Abstract: We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.

</details>


### [7] [CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices](https://arxiv.org/abs/2511.08575)
*Zhenxiao Fu,Chen Fan,Lei Jiang*

Main category: cs.AR

TL;DR: CO2-Meter是一个用于估计LLM边缘推理中运营碳和隐含碳的统一框架，通过考虑外围能耗、不同推理阶段特性和SoC设计复杂性，提供比现有方法更准确的碳足迹评估。


<details>
  <summary>Details</summary>
Motivation: LLMs在边缘设备上的部署面临碳挑战，现有估计方法不完整，忽略了外围能耗、预填充/解码阶段的不同行为以及SoC设计复杂性。

Method: 开发了方程式外围能耗模型和数据集；基于GNN的预测器，包含阶段特定的LLM能耗数据；SoC瓶颈分析的单元级隐含碳模型；并通过验证证明优于现有方法。

Result: 验证显示CO2-Meter在准确性上优于现有方法，案例研究证明其在识别碳热点和指导可持续LLM边缘平台设计方面的有效性。

Conclusion: CO2-Meter为LLM边缘推理提供了全面的碳足迹评估框架，能够有效识别碳热点并指导可持续设计决策。

Abstract: LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: A3GNN是一个在异构CPU-GPU平台上实现经济、自适应、自动GNN训练的框架，通过局部感知采样和细粒度并行调度提升资源利用率，并使用强化学习优化吞吐量、内存占用和精度的权衡。


<details>
  <summary>Details</summary>
Motivation: GNN训练通常依赖昂贵的高性能计算平台，限制了在许多任务中的可访问性。分析表明，通过在资源受限设备上充分利用可用资源，可以实现显著的效率提升。

Method: 提出A3GNN框架，采用局部感知采样和细粒度并行调度来提高资源利用率，并利用强化学习探索设计空间，实现吞吐量、内存占用和精度之间的帕累托最优权衡。

Result: 实验显示A3GNN能够弥合性能差距，使7个Nvidia 2080Ti GPU在吞吐量上比2个A100 GPU高出1.8倍，且精度损失最小。

Conclusion: A3GNN框架成功实现了在资源受限设备上高效训练GNN的目标，通过智能资源管理和优化策略，在保持精度的同时显著提升了训练效率。

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [9] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: 论文探讨了从传统单体GPU集群向分解式推理架构的转变，通过将计算密集的前填充阶段与内存密集的解码阶段解耦，实现独立扩展和优化，以解决大语言模型推理中的多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模达到万亿级别，实时推理已成为主要瓶颈，部署这些模型面临内存带宽、计算吞吐量和延迟要求的分布式系统挑战。

Method: 采用分解式推理架构，应用服务分解、资源解耦和工作负载分区等分布式系统原则，将预填充阶段和解码阶段分离为独立可扩展组件。

Result: 该范式缓解了资源争用，能够独立优化关键指标如首令牌时间和令牌间延迟。

Conclusion: 分解式推理架构是解决大语言模型推理瓶颈的有效方法，通过分布式系统原则实现延迟最小化、吞吐量最大化和成本降低的多目标优化。

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [10] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: Synera是一个设备-云协同的LLM服务系统，通过高效的SLM-LLM协同机制解决移动端LLM部署中的性能挑战，在保持低延迟的同时显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 移动端部署LLM面临生成质量下降和延迟延长的性能挑战，现有云卸载方案受通信瓶颈限制，而设备端SLM方案因资源限制牺牲生成质量。

Method: 提出Synera系统，采用通信高效的选卸载、无停顿并行推理和可扩展云批处理等定制设计，实现设备-云协同LLM推理。

Result: 在真实测试平台上，Synera相比竞争基线实现1.20-5.47倍更好的生成质量，延迟性能相当；相比现有云服务，在各种基准测试中降低8.2-16.5%的云服务成本。

Conclusion: Synera通过设备-云协同机制有效解决了移动端LLM部署的性能挑战，在保持低成本的同时显著提升了生成质量。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [11] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: 本文提出了首个基于实践的LLM推理事故分析框架，通过156个高严重性事故验证，识别出主要故障模式（60%推理引擎故障）和缓解策略（74%自动检测），并提供了可复用的分类法和实践指南。


<details>
  <summary>Details</summary>
Motivation: 超大规模LLM推理对云系统提出极高要求，即使短暂故障也会造成重大业务影响。需要系统化分析事故模式来提升服务可靠性。

Method: 基于一年运维经验开发分类法和方法论，在156个高严重性事故上验证，并对2025年4-6月数据进行定量研究以确保时效性。

Result: 实现高标注一致性（Cohen's K ~0.89），识别主导故障模式（推理引擎故障占60%，其中超时占40%），74%事故自动检测，28%需要热修复。

Conclusion: 系统化、基于实证的推理操作分析能够推动更可靠、成本效益更高的规模化LLM服务，分类法指导了针对性策略如连接活性检测、GPU容量感知路由等。

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [12] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: 本文首次在单板计算机（SBC）上对25个量化开源大语言模型进行基准测试，评估了生成吞吐量、内存使用和功耗，发现SBC可支持1.5B参数模型，Llamafile比Ollama性能更好。


<details>
  <summary>Details</summary>
Motivation: 随着设备端LLM推理需求增长，需要在边缘硬件上部署轻量级AI解决方案。单板计算机如树莓派和香橙派提供了本地化、保护隐私的推理平台，但在LLM工作负载方面研究不足。

Method: 在树莓派4、树莓派5和香橙派5 Pro三款SBC上，使用Ollama和Llamafile两种推理运行时，对25个量化开源LLM进行基准测试，评估不同CPU配置下的生成吞吐量、内存使用和功耗。

Result: SBC可可靠支持高达1.5B参数的模型，Llamafile比Ollama实现高达4倍的吞吐量提升和30-40%的功耗降低，识别了架构特定瓶颈和运行时权衡。

Conclusion: 这项研究首次在SBC上对LLM推理进行了广泛评估，弥合了高性能语言模型与廉价边缘计算之间的差距，为实际部署提供了实用建议。

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [13] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对MCP增强的LLM交互进行了测量分析，揭示了能力、性能和成本之间的权衡关系，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然增强了LLM与外部工具的交互能力，但包含大量上下文信息会显著增加token使用量，导致成本上升和计算负载增加。

Method: 采用基于测量的分析方法，研究不同LLM模型和MCP配置对token效率、成本、任务完成时间和成功率等关键指标的影响。

Result: 发现了能力、性能和成本之间的权衡关系，并识别出并行工具调用和任务中止机制等优化潜力。

Conclusion: 这些发现为开发更高效、鲁棒且成本效益更高的MCP增强工作流程提供了有用见解。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [14] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: DynaKV是一个针对智能手机上长序列LLM解码的自适应KVCache管理方法，通过动态集群调整、连续性优化的闪存管理和内存高效缓存设计，解决了KVCache分布漂移问题，提升了检索准确性和解码效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM对长序列解码需求的增长，智能手机上由于DRAM容量限制，KVCache的内存占用随序列长度线性增长成为瓶颈。现有的检索方法在解码过程中面临KVCache分布漂移问题，导致检索准确性下降和效率降低。

Method: DynaKV采用三种关键技术：(1)无迁移集群自适应，在检索过程中动态分割集群；(2)连续性中心闪存管理，将相关条目和集群共置并使用双头布局；(3)内存高效缓存设计，在DRAM和闪存间虚拟化缓存空间并扩展替换策略。

Result: 评估显示DynaKV相比现有解决方案，在检索准确性上平均提升1.38倍，端到端延迟平均加速1.47倍。

Conclusion: DynaKV有效解决了智能手机上长序列LLM解码的KVCache管理问题，其洞察可扩展到其他长上下文工作负载和多层内存层次结构，具有广泛适用性。

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [15] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: UniFormer是一个统一的Transformer架构，旨在同时优化通用计算平台（如GPU）和定制计算平台（如FPGA）的性能，解决模型在不同计算平台间迁移时的效率损失问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型在通用计算和定制计算平台间迁移时，由于并行计算范式的根本差异，往往需要在复杂性、效率或准确性方面做出妥协，且跨平台优化原则研究不足。

Method: 通过提高并行性和计算-存储融合，设计统一的Transformer架构UniFormer，使其在保持高精度的同时，在不同计算平台上都能实现高效部署。

Result: UniFormer在GPU上实现了SOTA精度和延迟，同时在FPGA上表现出强大的适应性，是首个同时考虑通用和定制计算架构的高效Transformer工作。

Conclusion: UniFormer证明了统一架构设计可以有效解决跨平台部署的挑战，为未来模型在异构计算环境中的高效部署提供了重要参考。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [16] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: HyProv是一个混合式溯源管理系统，结合集中式和联邦式范式，为工作流溯源轨迹提供可扩展、在线且工作流感知的查询功能。


<details>
  <summary>Details</summary>
Motivation: 现有溯源系统难以平衡可扩展性、实时处理、在线溯源分析和跨组件集成，且大多数解决方案不具备工作流感知能力，无法利用工作流规范进行优化。

Method: HyProv使用集中式组件管理小型稳定的工作流规范特定溯源数据，并通过联邦查询不同可扩展监控和溯源数据库来补充大规模执行日志，实现低延迟访问当前执行数据。

Result: 实验表明HyProv可扩展到大型工作流，以亚秒级延迟回答溯源查询，并且仅对集群增加适度的CPU和内存开销。

Conclusion: HyProv系统成功解决了工作流溯源管理的挑战，实现了可扩展、低延迟的溯源查询能力。

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [17] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 本地小语言模型（≤20B参数）现在能在许多任务上达到与前沿模型相竞争的性能，结合本地加速器（如Apple M4 Max）的交互式延迟，使得本地推理能够从集中式基础设施中重新分配需求。本文提出智能每瓦特（IPW）作为评估本地推理能力和效率的指标，并通过大规模实证研究验证了本地推理的可行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型查询主要由集中式云基础设施处理，快速增长的需求给这种模式带来压力。云提供商难以按需扩展基础设施。两个进展使得重新思考这一模式成为可能：小语言模型在许多任务上达到与前沿模型相竞争的性能，本地加速器能以交互式延迟运行这些模型。

Method: 提出智能每瓦特（IPW）作为评估本地推理能力和效率的指标。进行了大规模实证研究，涵盖20+最先进的本地语言模型、8种加速器，以及100万真实世界单轮聊天和推理查询。对每个查询测量准确性、能耗、延迟和功耗。

Result: 1. 本地语言模型能准确回答88.7%的单轮聊天和推理查询，准确性因领域而异；2. 2023-2025年间，IPW提升了5.3倍，本地查询覆盖率从23.2%上升到71.3%；3. 本地加速器比运行相同模型的云加速器至少实现1.4倍更低的IPW。

Conclusion: 本地推理能够有意义地从集中式基础设施中重新分配需求，IPW是跟踪这一转变的关键指标。研究证明了本地推理的可行性，并发布了IPW分析工具用于系统性的智能每瓦特基准测试。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [18] [Generic Algorithm for Universal TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.08034)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Ilija Basicevic*

Main category: cs.DC

TL;DR: 提出了一个通用的TDM通信算法，克服了原有框架中节点只能与单个对等节点通信的限制，支持节点与任意数量的对等节点进行通信。


<details>
  <summary>Details</summary>
Motivation: 原有联邦学习框架中的TDM通信算法只支持节点间的成对通信，这限制了实际应用场景，特别是在卫星间链路等需要多节点同时通信的场景中。

Method: 开发了新的通用TDM通信算法，包括理论基础、系统设计和系统验证三个部分，支持节点与多个对等节点进行通信。

Result: 新算法能够支持真实的TDM通信场景，特别是在卫星间链路等需要多节点通信的应用中。

Conclusion: 提出的通用TDM通信算法有效解决了原有框架的通信限制，为联邦学习在更复杂网络环境中的应用提供了支持。

Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.

</details>


### [19] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: ProbSelect是一种用于GPU加速设备上联邦学习客户端选择的新方法，通过分析建模和概率预测，无需历史数据或持续监控，在3D连续体中提高SLO合规性并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 在边缘、云和空间设备组成的3D连续体中，传统客户端选择方法依赖持续监控和历史数据收集，这在动态环境中不实用，且现有方法主要考虑CPU计算，无法捕捉GPU加速训练的复杂特性。

Method: 使用分析建模和概率预测进行客户端选择，在用户定义的SLO内建模，无需历史数据或持续监控。

Result: 在不同GPU架构和工作负载上的广泛评估显示，ProbSelect平均提高SLO合规性13.77%，相比基线方法减少72.5%的计算浪费。

Conclusion: ProbSelect在动态3D连续体环境中有效解决了GPU加速设备上联邦学习客户端选择的问题，显著提高了性能效率。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [20] [\uline{LO}w-c\uline{O}st yet High-\uline{P}erformant \uline{S}parse Matrix-Matrix Multiplication on Arm SME Architectures](https://arxiv.org/abs/2511.08158)
*Kelun Lei,Hailong Yang,Kaige Zhang,Kejie Ma,Yiqing Wang,Xin You,Yufan Xu,Enrique S. Quintana-Orti,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: LOOPS是一个混合执行框架，通过结合行向CSR和向量向BCSR布局，协同利用NEON向量指令和SME矩阵扩展资源，在Armv9架构上实现高性能稀疏矩阵-稠密矩阵乘法。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵-稠密矩阵乘法(SpMM)在科学计算和图学习中至关重要，但如何在Armv9架构上有效利用SME和传统SIMD资源处理非结构化稀疏工作负载仍是一个挑战。

Method: 提出LOOPS混合执行框架，采用行向CSR部分和向量向BCSR部分布局，通过自适应两级并行化方案支持FP64、FP32和FP16多精度SpMM，并由轻量级性能模型指导。

Result: 在Apple M4Pro CPU上测试SuiteSparse数据集，LOOPS相比CPU基准TACO实现FP32 9.93倍、FP64 14.4倍加速，相比Armadillo实现FP32 71.3倍、FP64 54.8倍加速。与NVIDIA A100 GPU上的cuSPARSE和Magicube相比，LOOPS实现19.8-33.5倍加速，且能效显著优于GPU。

Conclusion: LOOPS框架成功展示了在Armv9架构上协同利用SME和SIMD资源的有效性，为稀疏矩阵计算提供了高性能和高能效的解决方案。

Abstract: Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\times$ (FP32)/14.4$\times$ (FP64) against the CPU baseline TACO and 71.3$\times$ (FP32)/54.8$\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\times$ and 33.5$\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.

</details>


### [21] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 该论文研究了图结构上机器人聚集问题，在存在多重性且无法检测多重性的约束条件下，针对顶点和边传递图设计了两种时间最优的聚集算法。


<details>
  <summary>Details</summary>
Motivation: 研究在受限环境下（机器人无法检测多重性、初始配置可能包含多重性）的图结构上机器人聚集问题，特别关注顶点和边传递图这一特定图类。

Method: 使用循环调度器，针对无限网格和超立方体两种特定拓扑结构设计了两种不同的聚集算法，这些算法充分利用了底层拓扑的特性。

Result: 提出了两种时间最优的聚集算法，分别适用于无限网格和超立方体，并提供了基本的不可能性结果。

Conclusion: 由于算法严重依赖于特定拓扑的特性，推测不存在适用于所有可解情况的通用算法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [22] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 使用约束编程优化Kubernetes调度器，在默认调度器无法分配pod时作为备用机制，在1秒调度窗口内能在44%的情况下分配更多高优先级pod，在10秒窗口内提升到73%


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用轻量级启发式算法，可能导致次优的pod分配和资源碎片化，阻止了本可部署的pod分配

Method: 提出使用约束编程来找到满足所有优先级和资源请求的最优pod分配方案，作为默认调度器的插件，使用OR-Tools约束求解器

Result: 在小到中型集群实验中，1秒调度窗口内能在44%的情况下比默认调度器分配更多高优先级pod，10秒窗口内提升到73%，同时能在19%的情况下证明默认调度器分配已是最优

Conclusion: 约束编程方法能有效改进Kubernetes调度性能，特别是在默认调度器失败的情况下，同时能验证调度最优性

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [23] [CNN-Based Automated Parameter Extraction Framework for Modeling Memristive Devices](https://arxiv.org/abs/2511.07926)
*Akif Hamid,Orchi Hassan*

Main category: cs.ET

TL;DR: 提出了一个自动化框架，用于从RRAM器件的I-V特性中提取斯坦福RRAM模型的拟合参数，使用CNN生成初始参数估计，并通过启发式优化块进行精炼。


<details>
  <summary>Details</summary>
Motivation: 现有RRAM紧凑模型依赖多个拟合参数来重现器件I-V特性，这些参数与可测量量无直接关系，提取过程需要大量手动调整，耗时且难以适应不同器件。

Method: 使用在合成数据集上训练的卷积神经网络生成初始参数估计，然后通过三个启发式优化块在参数空间中进行自适应二分搜索以最小化误差。

Result: 使用四个关键NVM指标评估框架：置位电压、复位电压、磁滞回线面积和低阻态斜率。与先前报告的斯坦福模型拟合、其他分析模型和实验数据相比，该框架在不同器件特性上实现了低误差。

Conclusion: 该框架为RRAM建模提供了快速、可靠且稳健的解决方案。

Abstract: Resistive random access memory (RRAM) is a promising candidate for next-generation nonvolatile memory (NVM) and in-memory computing applications. Compact models are essential for analyzing the circuit and system-level performance of experimental RRAM devices. However, most existing RRAM compact models rely on multiple fitting parameters to reproduce the device I-V characteristics, and in most cases, as the parameters are not directly related to measurable quantities, their extraction requires extensive manual tuning, making the process time-consuming and limiting adaptability across different devices. This work presents an automated framework for extracting the fitting parameters of the widely used Stanford RRAM model directly from the device I-V characteristics. The framework employs a convolutional neural network (CNN) trained on a synthetic dataset to generate initial parameter estimates, which are then refined through three heuristic optimization blocks that minimize errors via adaptive binary search in the parameter space. We evaluated the framework using four key NVM metrics: set voltage, reset voltage, hysteresis loop area, and low resistance state (LRS) slope. Benchmarking against RRAM device characteristics derived from previously reported Stanford model fits, other analytical models, and experimental data shows that the framework achieves low error across diverse device characteristics, offering a fast, reliable, and robust solution for RRAM modeling.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种结合概率特征采样和模拟退火超参数调优的随机森林增强框架，显著提升了预测准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决传统随机森林在捕捉数据相关信号方面的局限性，通过自适应超参数配置来增强分类性能

Method: 集成概率特征采样和模拟退火超参数调优，强调捕捉数据中最相关的信号，实现动态参数调整

Result: 在多个领域（信用风险评估、物联网异常检测、早期医疗诊断、高维生物数据分析）中显示出准确性的持续提升和特征相关性的有意义洞察

Conclusion: 结合重要性感知采样和元启发式优化能有效提升随机森林分类器的性能，为复杂分类问题提供强大解决方案

Abstract: This paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.

</details>


### [25] [Clustering Guided Residual Neural Networks for Multi-Tx Localization in Molecular Communications](https://arxiv.org/abs/2511.08513)
*Ali Sonmez,Erencem Ozbey,Efe Feyzi Mantaroglu,H. Birkan Yilmaz*

Main category: cs.LG

TL;DR: 提出基于聚类的质心校正方法和两种聚类引导的残差神经网络，用于分子通信中的多发射器定位，显著降低了定位误差。


<details>
  <summary>Details</summary>
Motivation: 分子通信中多发射器定位具有挑战性，主要由于扩散的随机性和接收器表面分子分布的重叠问题。

Method: 1. 基于聚类的质心校正方法增强对密度变化和异常值的鲁棒性；2. 两种聚类引导的残差神经网络：AngleNN用于方向细化，SizeNN用于聚类大小估计。

Result: 实验结果显示，与K-means相比，定位误差显著降低：2发射器情况下降低69%，4发射器情况下降低43%。

Conclusion: 所提出的方法在分子通信多发射器定位中表现出显著性能提升，有效解决了扩散随机性和分子分布重叠带来的挑战。

Abstract: Transmitter localization in Molecular Communication via Diffusion is a critical topic with many applications. However, accurate localization of multiple transmitters is a challenging problem due to the stochastic nature of diffusion and overlapping molecule distributions at the receiver surface. To address these issues, we introduce clustering-based centroid correction methods that enhance robustness against density variations, and outliers. In addition, we propose two clusteringguided Residual Neural Networks, namely AngleNN for direction refinement and SizeNN for cluster size estimation. Experimental results show that both approaches provide significant improvements with reducing localization error between 69% (2-Tx) and 43% (4-Tx) compared to the K-means.

</details>


### [26] [Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution](https://arxiv.org/abs/2511.07459)
*Ashutosh Agarwal*

Main category: cs.LG

TL;DR: LEVER提出了一种新颖的Siamese架构解决方案，通过知识迁移减少标签不一致性，显著提升了极端分类任务中低频类别的性能表现。


<details>
  <summary>Details</summary>
Motivation: 极端分类任务中的低频类别由于样本稀疏而存在高标签不一致性问题，这严重影响了分类性能，需要专门的方法来解决。

Method: 采用鲁棒的Siamese风格架构，利用知识迁移来减少标签不一致性，并增强One-vs-All分类器的性能。

Result: 在多个极端分类数据集上的综合测试显示，该方法在处理低频类别方面取得了显著改进，为该领域设立了新的基准。

Conclusion: LEVER通过知识迁移有效解决了极端分类中低频类别的标签不一致问题，同时论文还贡献了两个新的多意图数据集，为未来研究提供了重要资源。

Abstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.

</details>


### [27] [Slimmable NAM: Neural Amp Models with adjustable runtime computational cost](https://arxiv.org/abs/2511.07470)
*Steven Atkinson*

Main category: cs.LG

TL;DR: 提出了可调整大小的神经放大器模型，无需额外训练即可改变模型大小和计算成本，让音乐家能够在模型精度和计算量之间轻松权衡。


<details>
  <summary>Details</summary>
Motivation: 让音乐家能够在使用神经放大器模型时，根据实际需求在模型精度和计算成本之间进行灵活权衡，而无需重新训练模型。

Method: 开发了可调整大小的神经放大器模型架构，支持在运行时动态改变模型大小和计算复杂度，且计算开销可忽略不计。

Result: 与常用基线方法进行了性能量化比较，并开发了实时音频效果插件来演示该模型的实际应用。

Conclusion: 可调整大小的神经放大器模型为音乐应用提供了灵活的计算精度权衡方案，无需额外训练即可适应不同的计算资源需求。

Abstract: This work demonstrates "slimmable Neural Amp Models", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.

</details>


### [28] [Towards Personalized Quantum Federated Learning for Anomaly Detection](https://arxiv.org/abs/2511.07471)
*Ratun Rahman,Sina Shaham,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 提出了一种用于异常检测的个性化量子联邦学习(PQFL)框架，通过参数化量子电路和经典优化器增强本地模型训练，并引入量子中心个性化策略来适应每个客户端的硬件特性和数据表示。


<details>
  <summary>Details</summary>
Motivation: 现实量子网络中客户端在硬件能力、电路设计、噪声水平和数据编码方面存在差异，导致固有的异构性，使得训练单一全局模型在客户端处理不平衡或非IID数据时效果不佳。

Method: 使用参数化量子电路和经典优化器增强本地模型训练，引入量子中心个性化策略，使每个客户端的模型适应其自身硬件特性和数据表示。

Result: PQFL显著提高了不同现实条件下的异常检测准确率，相比最先进方法，将错误率降低高达23%，在AUROC和AUPR上分别获得24.2%和20.5%的提升。

Conclusion: PQFL在实用量子联邦设置中表现出有效性和可扩展性，能够有效解决量子客户端异构性问题。

Abstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.

</details>


### [29] [Multivariate Variational Autoencoder](https://arxiv.org/abs/2511.07472)
*Mehmet Can Yavuz*

Main category: cs.LG

TL;DR: MVAE是一种变分自编码器变体，通过全局耦合矩阵和逐样本对角尺度来建模全协方差后验，在保持高斯可处理性的同时提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统VAE中后验协方差限制为对角矩阵的问题，希望构建一个能够建模潜在变量间相关性的全协方差后验分布。

Method: 将后验协方差分解为全局耦合矩阵C和逐样本对角尺度，通过L=Cdiag(σ)实现高效重参数化，保持KL散度的解析计算。

Result: 在多个数据集上，MVAE在重建质量、校准性能和无监督结构发现方面均优于对角协方差VAE，特别是在中等潜在维度下表现更佳。

Conclusion: MVAE提供了一种既保持高斯可处理性又能建模潜在相关性的有效方法，在多个评估指标上展现出稳健改进。

Abstract: We present the Multivariate Variational Autoencoder (MVAE), a VAE variant that preserves Gaussian tractability while lifting the diagonal posterior restriction. MVAE factorizes each posterior covariance, where a \emph{global} coupling matrix $\mathbf{C}$ induces dataset-wide latent correlations and \emph{per-sample} diagonal scales modulate local uncertainty. This yields a full-covariance family with analytic KL and an efficient reparameterization via $\mathbf{L}=\mathbf{C}\mathrm{diag}(\boldsymbolσ)$. Across Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently matches or improves reconstruction (MSE~$\downarrow$) and delivers robust gains in calibration (NLL/Brier/ECE~$\downarrow$) and unsupervised structure (NMI/ARI~$\uparrow$) relative to diagonal-covariance VAEs with matched capacity, especially at mid-range latent sizes. Latent-plane visualizations further indicate smoother, more coherent factor traversals and sharper local detail. We release a fully reproducible implementation with training/evaluation scripts and sweep utilities to facilitate fair comparison and reuse.

</details>


### [30] [RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records](https://arxiv.org/abs/2511.07473)
*Yang Yang,Kathryn Pollak,Bibhas Chakraborty,Molei Liu,Doudou Zhou,Chuan Hong*

Main category: cs.LG

TL;DR: RELEAP是一个基于强化学习的主动学习框架，通过下游预测性能作为反馈来指导表型校正和样本选择，在有限标注预算下显著提升风险预测性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录表型分析通常依赖噪声代理标签，这削弱了下游风险预测的可靠性。传统主动学习方法依赖固定启发式规则，无法确保表型改进能提升预测性能。

Method: 提出RELEAP框架，自适应整合多种查询策略，基于下游模型反馈更新策略。在Duke大学健康系统队列上评估，使用逻辑回归和惩罚Cox生存模型进行肺癌风险预测。

Result: RELEAP在所有基准测试中表现最佳：逻辑回归AUC从0.774提升至0.805，生存C指数从0.718提升至0.752。在相同标注预算下比启发式方法获得更平滑稳定的性能提升。

Conclusion: RELEAP通过下游反馈优化表型校正，提供了一个可扩展、标注高效的范式，减少了人工图表审查，增强了基于EHR的风险预测可靠性。

Abstract: Objective: Electronic health record (EHR) phenotyping often relies on noisy proxy labels, which undermine the reliability of downstream risk prediction. Active learning can reduce annotation costs, but most rely on fixed heuristics and do not ensure that phenotype refinement improves prediction performance. Our goal was to develop a framework that directly uses downstream prediction performance as feedback to guide phenotype correction and sample selection under constrained labeling budgets.
  Materials and Methods: We propose Reinforcement-Enhanced Label-Efficient Active Phenotyping (RELEAP), a reinforcement learning-based active learning framework. RELEAP adaptively integrates multiple querying strategies and, unlike prior methods, updates its policy based on feedback from downstream models. We evaluated RELEAP on a de-identified Duke University Health System (DUHS) cohort (2014-2024) for incident lung cancer risk prediction, using logistic regression and penalized Cox survival models. Performance was benchmarked against noisy-label baselines and single-strategy active learning.
  Results: RELEAP consistently outperformed all baselines. Logistic AUC increased from 0.774 to 0.805 and survival C-index from 0.718 to 0.752. Using downstream performance as feedback, RELEAP produced smoother and more stable gains than heuristic methods under the same labeling budget.
  Discussion: By linking phenotype refinement to prediction outcomes, RELEAP learns which samples most improve downstream discrimination and calibration, offering a more principled alternative to fixed active learning rules.
  Conclusion: RELEAP optimizes phenotype correction through downstream feedback, offering a scalable, label-efficient paradigm that reduces manual chart review and enhances the reliability of EHR-based risk prediction.

</details>


### [31] [Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data](https://arxiv.org/abs/2511.07481)
*Reem Al-Saidi,Erman Ayday,Ziad Kobti*

Main category: cs.LG

TL;DR: 该研究探讨了在基因组序列上应用的大型语言模型中的嵌入重建攻击，重点关注微调如何影响对这些攻击的脆弱性。研究发现微调增强了模型对重建攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 基于Pan等人的工作，研究旨在确定任务特定优化是增强还是削弱隐私保护，填补了原研究中未指定嵌入类型的空白。

Method: 使用HS3D基因组数据集，应用重建攻击管道到预训练和微调模型嵌入，实现专门针对DNA序列的标记化机制，并进行详细的比较分析。

Result: 微调在多个架构中增强了对抗重建攻击的抵抗力：XLNet (+19.8%)、GPT-2 (+9.8%) 和 BERT (+7.8%)。

Conclusion: 任务特定优化可能是一种隐私增强机制，需要为处理敏感基因组数据的语言模型开发先进的保护机制，同时微调值得进一步探索作为隐私增强技术。

Abstract: This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\%), GPT-2 (+9.8\%), and BERT (+7.8\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.

</details>


### [32] [ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings](https://arxiv.org/abs/2511.07658)
*Xiaomeng Yang,Jian Gao,Yanzhi Wang,Xuan Zhang*

Main category: cs.LG

TL;DR: ZeroSim是一个基于Transformer的性能建模框架，旨在实现电路设计的零样本泛化，无需微调即可适应未见过的拓扑结构，显著加速模拟电路设计自动化。


<details>
  <summary>Details</summary>
Motivation: 传统SPICE仿真耗时，现有机器学习方法需要拓扑特定重训练或手动子结构分割，限制了可扩展性和适应性。

Method: 采用三种关键策略：1) 包含360万实例的多样化训练语料库；2) 利用全局感知令牌和分层注意力的统一拓扑嵌入；3) 拓扑条件参数映射方法。

Result: ZeroSim显著优于多层感知机、图神经网络和Transformer等基线模型，在强化学习参数优化中实现13倍加速。

Conclusion: ZeroSim为模拟电路设计自动化任务提供了实用价值，实现了强大的零样本泛化能力。

Abstract: Although recent advancements in learning-based analog circuit design automation have tackled tasks such as topology generation, device sizing, and layout synthesis, efficient performance evaluation remains a major bottleneck. Traditional SPICE simulations are time-consuming, while existing machine learning methods often require topology-specific retraining or manual substructure segmentation for fine-tuning, hindering scalability and adaptability. In this work, we propose ZeroSim, a transformer-based performance modeling framework designed to achieve robust in-distribution generalization across trained topologies under novel parameter configurations and zero-shot generalization to unseen topologies without any fine-tuning. We apply three key enabling strategies: (1) a diverse training corpus of 3.6 million instances covering over 60 amplifier topologies, (2) unified topology embeddings leveraging global-aware tokens and hierarchical attention to robustly generalize to novel circuits, and (3) a topology-conditioned parameter mapping approach that maintains consistent structural representations independent of parameter variations. Our experimental results demonstrate that ZeroSim significantly outperforms baseline models such as multilayer perceptrons, graph neural networks and transformers, delivering accurate zero-shot predictions across different amplifier topologies. Additionally, when integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a remarkable speedup (13x) compared to conventional SPICE simulations, underscoring its practical value for a wide range of analog circuit design automation tasks.

</details>


### [33] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: AAPP是一种动态结构化剪枝方法，通过自适应保留对齐相关电路，在保持计算效率的同时显著提升LLM的安全拒绝率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理需要大量计算资源，动态剪枝虽然比静态方法更高效，但会加剧对齐退化问题，因为只保留输入相关的安全关键电路

Method: 基于Probe Pruning，提出Alignment-Aware Probe Pruning (AAPP)方法，在推理过程中自适应保留对齐相关电路

Result: 在LLaMA 2-7B、Qwen2.5-14B-Instruct和Gemma-3-12B-IT上的实验显示，在相同计算量下，AAPP将拒绝率提高了50%

Conclusion: AAPP能够实现高效且保持安全性的LLM部署，解决了动态剪枝带来的对齐脆弱性问题

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [34] [Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs](https://arxiv.org/abs/2511.07484)
*Dharmateja Priyadarshi Uddandarao,Ravi Kiran Vadlamani*

Main category: cs.LG

TL;DR: 提出结合结构因果模型与transformer生成式AI的反事实用户行为预测框架，通过因果图建模用户行为关系，在多种数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了在产品部署前有效模拟和评估潜在干预措施，需要能够预测用户在反事实条件下的行为轨迹。

Method: 构建因果图映射用户交互、采用指标和产品特征之间的关系，使用基于因果变量的生成模型生成反事实行为轨迹。

Result: 在网页交互、移动应用和电商数据集上测试，该方法优于传统预测和提升建模技术。

Conclusion: 该框架通过因果路径可视化提高了可解释性，使产品团队能够在部署前有效模拟和评估潜在干预措施。

Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.

</details>


### [35] [When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift](https://arxiv.org/abs/2511.07485)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，将机器学习中的不同偏差机制（如虚假相关性、子群体偏移、类别不平衡和公平性违规）形式化为条件独立性违反，并证明了它们之间的定量等价关系。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统存在多种失败模式（不公平性、对虚假相关性的脆弱性、少数子群体性能差等），这些通常被不同研究社区孤立研究。作者旨在建立一个统一框架来理解这些偏差机制之间的内在联系。

Method: 通过信息论度量将偏差形式化为条件独立性违反，推导出虚假相关性、子群体偏移、类别不平衡和公平性违规之间的正式等价条件。理论预测了不同偏差机制间的定量等价关系。

Result: 在六个数据集和三种架构上的实证验证表明，预测的等价关系在最差群体准确度3%的误差范围内成立，使得去偏差方法能够在不同问题领域间进行有原则的迁移。

Conclusion: 这项工作在公平性、鲁棒性和分布偏移的文献之间建立了桥梁，提供了一个共同的视角来理解机器学习中的各种偏差机制。

Abstract: Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $α$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \approx (1+α)/(1-α)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.

</details>


### [36] [Provably Efficient Sample Complexity for Robust CMDP](https://arxiv.org/abs/2511.07486)
*Sourav Ganguly,Arnob Ghosh*

Main category: cs.LG

TL;DR: 本文研究了在安全约束下学习策略的问题，提出了首个具有样本复杂度保证的鲁棒约束MDP算法RCVI，解决了马尔可夫策略在约束条件下的次优性问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然建立了RCMDP的策略优化有限时间迭代复杂度保证，但样本复杂度保证仍未被充分探索。需要解决马尔可夫策略在矩形不确定性集下的次优性问题。

Method: 引入包含剩余效用预算的增强状态空间表示，提出鲁棒约束值迭代算法RCVI，使用生成模型实现样本复杂度为O(|S||A|H^5/ε^2)。

Result: RCVI算法实现了最多ε的约束违反，这是RCMDP领域的首个样本复杂度保证。实证结果验证了方法的有效性。

Conclusion: 提出的RCVI算法成功解决了RCMDP的样本复杂度问题，通过增强状态空间表示克服了马尔可夫策略的局限性，为鲁棒约束强化学习提供了理论保证。

Abstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/ε^2)$ achieving at most $ε$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.

</details>


### [37] [Methodological Precedence in Health Tech: Why ML/Big Data Analysis Must Follow Basic Epidemiological Consistency. A Case Study](https://arxiv.org/abs/2511.07500)
*Marco Roccetti*

Main category: cs.LG

TL;DR: 该研究通过一个疫苗结果与精神事件的队列研究案例，展示了高级分析方法（如机器学习和大数据）必须建立在基本方法学一致性验证的基础上。研究发现，复杂分析会放大而非纠正基本设计缺陷，导致误导性结论。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和海量数据处理在健康研究中的应用日益广泛，研究者需要认识到这些复杂方法的严谨性依赖于基础数据集的质量和统计设计的有效性。本文旨在强调在应用高级分析前必须验证基本方法学一致性的重要性。

Method: 研究通过对一个已发表的疫苗结果与精神事件队列研究应用简单的描述性统计方法和国家流行病学基准，暴露了多个统计上不可调和的悖论，包括高风险群体中慢性疾病风险降低的不可信结果和矛盾的发病率比较。

Result: 研究发现报告的危害比（HRs）因队列构建中未校正的选择偏倚而无效，观察到的效应是数学伪影。这些悖论明确否定了原始研究的结论。

Conclusion: 即使是复杂的健康研究也必须首先通过基本流行病学一致性测试，然后才能认为从后续高级机器学习或统计建模得出的结论有效或可发表。在缺乏随机化的情况下，倾向评分匹配等稳健方法对于从管理数据中获得有效因果推断至关重要。

Abstract: The integration of advanced analytical tools, including Machine Learning (ML) and massive data processing, has revolutionized health research, promising unprecedented accuracy in diagnosis and risk prediction. However, the rigor of these complex methods is fundamentally dependent on the quality and integrity of the underlying datasets and the validity of their statistical design. We propose an emblematic case where advanced analysis (ML/Big Data) must necessarily be subsequent to the verification of basic methodological coherence. This study highlights a crucial cautionary principle: sophisticated analyses amplify, rather than correct, severe methodological flaws rooted in basic design choices, leading to misleading or contradictory findings. By applying simple, standard descriptive statistical methods and established national epidemiological benchmarks to a recently published cohort study on vaccine outcomes and psychiatric events, we expose multiple, statistically irreconcilable paradoxes. These paradoxes, including an implausible risk reduction for a chronic disorder in a high-risk group and contradictory incidence rate comparisons, definitively invalidate the reported hazard ratios (HRs). We demonstrate that the observed effects are mathematical artifacts stemming from an uncorrected selection bias in the cohort construction. This analysis serves as a robust reminder that even the most complex health studies must first pass the test of basic epidemiological consistency before any conclusion drawn from subsequent advanced ML or statistical modeling can be considered valid or publishable. We conclude that robust methods, such as Propensity Score Matching, are essential for achieving valid causal inference from administrative data in the absence of randomization

</details>


### [38] [N-ReLU: Zero-Mean Stochastic Extension of ReLU](https://arxiv.org/abs/2511.07559)
*Md Motaleb Hossen Manik,Md Zabirul Islam,Ge Wang*

Main category: cs.LG

TL;DR: N-ReLU是一种基于ReLU的随机激活函数，通过用高斯噪声替换负激活值来解决死神经元问题，同时保持期望输出不变。在MNIST数据集上，N-ReLU在中等噪声水平下取得了与ReLU及其他变体相当或略优的性能，且无死神经元出现。


<details>
  <summary>Details</summary>
Motivation: 标准ReLU激活函数存在死神经元问题，即负输入导致神经元永久失活。为了解决这个问题，需要一种既能保持ReLU优势又能避免死神经元的改进方法。

Method: 提出N-ReLU（Noise-ReLU），这是ReLU的零均值随机扩展，用高斯噪声替换负激活值，同时保持期望输出与ReLU相同。该方法在非活跃区域保持梯度流动，并在训练过程中起到退火式正则化作用。

Result: 在MNIST数据集上使用MLP和CNN架构的实验表明，N-ReLU在中等噪声水平（σ=0.05-0.10）下达到了与ReLU、LeakyReLU、PReLU、GELU和RReLU相当或略优的准确率，且收敛稳定，未观察到死神经元。

Conclusion: 轻量级高斯噪声注入提供了一种简单有效的机制，可以在不修改网络结构或引入额外参数的情况下增强优化鲁棒性。

Abstract: Activation functions are fundamental for enabling nonlinear representations in deep neural networks. However, the standard rectified linear unit (ReLU) often suffers from inactive or "dead" neurons caused by its hard zero cutoff. To address this issue, we introduce N-ReLU (Noise-ReLU), a zero-mean stochastic extension of ReLU that replaces negative activations with Gaussian noise while preserving the same expected output. This expectation-aligned formulation maintains gradient flow in inactive regions and acts as an annealing-style regularizer during training. Experiments on the MNIST dataset using both multilayer perceptron (MLP) and convolutional neural network (CNN) architectures show that N-ReLU achieves accuracy comparable to or slightly exceeding that of ReLU, LeakyReLU, PReLU, GELU, and RReLU at moderate noise levels (sigma = 0.05-0.10), with stable convergence and no dead neurons observed. These results demonstrate that lightweight Gaussian noise injection offers a simple yet effective mechanism to enhance optimization robustness without modifying network structures or introducing additional parameters.

</details>


### [39] [SCALAR: Benchmarking SAE Interaction Sparsity in Toy LLMs](https://arxiv.org/abs/2511.07572)
*Sean P. Fillingham,Andrew Gordon,Peter Lai,Xavier Poncini,David Quarel,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 提出了SCALAR基准来评估稀疏自编码器(SAE)特征间的交互稀疏性，并引入Staircase SAEs通过权重共享限制上游特征重复，显著提高了交互稀疏性。


<details>
  <summary>Details</summary>
Motivation: 当前SAE在孤立训练时无法鼓励跨层稀疏连接，导致提取的电路中出现上游特征不必要地影响多个下游特征的问题，而现有评估方法只关注单个SAE性能，忽略了交互稀疏性。

Method: 引入SCALAR基准测量SAE特征间的交互稀疏性；提出Staircase SAEs使用权重共享限制上游特征在多个下游特征中的重复；比较TopK SAEs、Jacobian SAEs和Staircase SAEs。

Result: Staircase SAEs相比TopK SAEs在交互稀疏性上提高了59.67%±1.83%(前馈层)和63.15%±1.35%(Transformer块)；JSAEs在前馈层有8.54%±0.38%改进但无法在Transformer块有效训练；在216K参数玩具模型和GPT-2 Small上验证了Staircase SAEs保持交互稀疏性改进同时维持特征可解释性。

Conclusion: 通过基准测试和架构比较，强调了交互稀疏性在SAE中的重要性，Staircase SAEs是实现这一目标的有效方法。

Abstract: Mechanistic interpretability aims to decompose neural networks into interpretable features and map their connecting circuits. The standard approach trains sparse autoencoders (SAEs) on each layer's activations. However, SAEs trained in isolation don't encourage sparse cross-layer connections, inflating extracted circuits where upstream features needlessly affect multiple downstream features. Current evaluations focus on individual SAE performance, leaving interaction sparsity unexamined. We introduce SCALAR (Sparse Connectivity Assessment of Latent Activation Relationships), a benchmark measuring interaction sparsity between SAE features. We also propose "Staircase SAEs", using weight-sharing to limit upstream feature duplication across downstream features. Using SCALAR, we compare TopK SAEs, Jacobian SAEs (JSAEs), and Staircase SAEs. Staircase SAEs improve relative sparsity over TopK SAEs by $59.67\% \pm 1.83\%$ (feedforward) and $63.15\% \pm 1.35\%$ (transformer blocks). JSAEs provide $8.54\% \pm 0.38\%$ improvement over TopK for feedforward layers but cannot train effectively across transformer blocks, unlike Staircase and TopK SAEs which work anywhere in the residual stream. We validate on a $216$K-parameter toy model and GPT-$2$ Small ($124$M), where Staircase SAEs maintain interaction sparsity improvements while preserving feature interpretability. Our work highlights the importance of interaction sparsity in SAEs through benchmarking and comparing promising architectures.

</details>


### [40] [LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows](https://arxiv.org/abs/2511.07585)
*Raffi Khatchadourian,Rolando Franco*

Main category: cs.LG

TL;DR: 研究发现大型语言模型在金融应用中存在输出漂移问题，小型模型在确定性设置下表现更稳定，而大型模型输出一致性显著降低，这对金融领域AI部署提出了新的考量。


<details>
  <summary>Details</summary>
Motivation: 金融领域部署LLM时，非确定性输出会破坏审计性和可信度，需要量化不同规模模型的输出稳定性差异。

Method: 开发了金融校准的确定性测试框架，包括贪婪解码、固定种子和SEC结构感知检索排序，使用财务重要性阈值和SEC引用验证进行不变性检查。

Result: 小型模型在T=0.0时达到100%输出一致性，而GPT-OSS-120B只有12.5%一致性，结构化任务比RAG任务更稳定。

Conclusion: 模型规模与输出稳定性呈反比关系，挑战了大型模型在金融生产部署中普遍优越的假设，需要基于风险的部署决策框架。

Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.

</details>


### [41] [One Router to Route Them All: Homogeneous Expert Routing for Heterogeneous Graph Transformers](https://arxiv.org/abs/2511.07603)
*Georgiy Shakirov,Albert Arakelov*

Main category: cs.LG

TL;DR: 提出Homogeneous Expert Routing (HER)方法，在异质图神经网络中集成MoE，通过随机掩码类型嵌入实现类型无关的专家专业化，在链接预测任务上优于标准HGT和类型分离的MoE基线。


<details>
  <summary>Details</summary>
Motivation: 传统HGNNs依赖节点/边类型参数化，可能导致对表面标签的过度依赖并阻碍跨类型知识迁移。探索在异质图中集成MoE，质疑类型特定专家的必要性。

Method: 提出HER方法，在HGT中集成MoE层，在路由过程中随机掩码类型嵌入，鼓励专家进行类型无关的专业化。

Result: 在IMDB、ACM和DBLP数据集上的链接预测任务中，HER持续优于标准HGT和类型分离的MoE基线。IMDB上的分析显示HER专家按语义模式（如电影类型）而非节点类型进行专业化。

Conclusion: 在专家路由中正则化类型依赖性可以产生更可泛化、高效和可解释的表示，为异质图学习提供了新的设计原则。

Abstract: A common practice in heterogeneous graph neural networks (HGNNs) is to condition parameters on node/edge types, assuming types reflect semantic roles. However, this can cause overreliance on surface-level labels and impede cross-type knowledge transfer. We explore integrating Mixture-of-Experts (MoE) into HGNNs--a direction underexplored despite MoE's success in homogeneous settings. Crucially, we question the need for type-specific experts. We propose Homogeneous Expert Routing (HER), an MoE layer for Heterogeneous Graph Transformers (HGT) that stochastically masks type embeddings during routing to encourage type-agnostic specialization. Evaluated on IMDB, ACM, and DBLP for link prediction, HER consistently outperforms standard HGT and a type-separated MoE baseline. Analysis on IMDB shows HER experts specialize by semantic patterns (e.g., movie genres) rather than node types, confirming routing is driven by latent semantics. Our work demonstrates that regularizing type dependence in expert routing yields more generalizable, efficient, and interpretable representations--a new design principle for heterogeneous graph learning.

</details>


### [42] [BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services](https://arxiv.org/abs/2511.08142)
*Anna Lackinger,Andrea Morichetta,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.LG

TL;DR: BIPPO是一种基于多智能体强化学习的能量高效客户端选择方法，用于解决联邦学习在资源受限IoT环境中的基础设施效率问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保证负载分布和隐私，但未原生考虑基础设施效率，特别是在资源受限的IoT环境中。现有基于强化学习的客户端选择方法未充分考虑资源限制、设备流失等基础设施挑战，且缺乏通用性和能效优化。

Method: 提出BIPPO（预算感知独立近端策略优化），这是一种能量高效的多智能体强化学习解决方案，通过改进的采样器来优化客户端选择。

Result: 在高度预算受限环境下对两个图像分类任务进行评估，BIPPO相比非RL机制、传统PPO和IPPO提高了平均准确率，且仅消耗可忽略的预算比例，即使客户端数量增加也能保持稳定。

Conclusion: BIPPO为IoT-FL中的客户端选择提供了高性能、稳定、可扩展和可持续的解决方案。

Abstract: Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.

</details>


### [43] [Partial Action Replacement: Tackling Distribution Shift in Offline MARL](https://arxiv.org/abs/2511.07629)
*Yue Jin,Giovanni Montana*

Main category: cs.LG

TL;DR: 提出了SPaCQL算法，通过部分动作替换策略缓解离线多智能体强化学习中的分布外问题，在因子化行为策略下显著降低分布偏移。


<details>
  <summary>Details</summary>
Motivation: 离线多智能体强化学习面临分布外联合动作评估的挑战，特别是在行为策略因子化的情况下。

Method: 开发了Soft-Partial Conservative Q-Learning (SPaCQL)，使用部分动作替换策略，并基于价值估计不确定性动态加权不同替换策略。

Result: 理论证明在因子化行为策略下，分布偏移随偏离智能体数量线性增长而非指数增长；实验显示SPaCQL在具有独立性结构的离线数据集上优于基线算法。

Conclusion: 部分动作替换策略能有效缓解离线MARL中的分布外问题，SPaCQL算法在因子化行为策略场景下表现出显著优势。

Abstract: Offline multi-agent reinforcement learning (MARL) is severely hampered by the challenge of evaluating out-of-distribution (OOD) joint actions. Our core finding is that when the behavior policy is factorized - a common scenario where agents act fully or partially independently during data collection - a strategy of partial action replacement (PAR) can significantly mitigate this challenge. PAR updates a single or part of agents' actions while the others remain fixed to the behavioral data, reducing distribution shift compared to full joint-action updates. Based on this insight, we develop Soft-Partial Conservative Q-Learning (SPaCQL), using PAR to mitigate OOD issue and dynamically weighting different PAR strategies based on the uncertainty of value estimation. We provide a rigorous theoretical foundation for this approach, proving that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents rather than exponentially with the joint-action space. This yields a provably tighter value error bound for this important class of offline MARL problems. Our theoretical results also indicate that SPaCQL adaptively addresses distribution shift using uncertainty-informed weights. Our empirical results demonstrate SPaCQL enables more effective policy learning, and manifest its remarkable superiority over baseline algorithms when the offline dataset exhibits the independence structure.

</details>


### [44] [FlowTIE: Flow-based Transport of Intensity Equation for Phase Gradient Estimation from 4D-STEM Data](https://arxiv.org/abs/2511.07633)
*Arya Bangun,Maximilian Töllner,Xuan Zhao,Christian Kübel,Hanno Scharr*

Main category: cs.LG

TL;DR: FlowTIE是一个基于神经网络的框架，用于从4D-STEM数据重建相位，将传输强度方程与基于流的相位梯度表示相结合，在厚样品动态散射条件下提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决在厚样品动态散射条件下传统相位重建方法的局限性，将数据驱动学习与物理先验相结合以提高重建精度和鲁棒性。

Method: 将传输强度方程与基于流的相位梯度表示集成，结合神经网络框架，允许与多层切片方法等厚样品模型集成。

Result: 在晶体材料模拟数据集上的验证表明，FlowTIE相比经典TIE和基于梯度的优化方法提高了相位重建精度，且速度更快。

Conclusion: FlowTIE框架能够有效改善相位重建，在厚样品条件下表现出更好的性能，并可集成到更复杂的样品模型中。

Abstract: We introduce FlowTIE, a neural-network-based framework for phase reconstruction from 4D-Scanning Transmission Electron Microscopy (STEM) data, which integrates the Transport of Intensity Equation (TIE) with a flow-based representation of the phase gradient. This formulation allows the model to bridge data-driven learning with physics-based priors, improving robustness under dynamical scattering conditions for thick specimen. The validation on simulated datasets of crystalline materials, benchmarking to classical TIE and gradient-based optimization methods are presented. The results demonstrate that FlowTIE improves phase reconstruction accuracy, fast, and can be integrated with a thick specimen model, namely multislice method.

</details>


### [45] [Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private](https://arxiv.org/abs/2511.07637)
*Ruihan Wu,Erchi Wang,Zhiyuan Zhang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 本文提出了两种差分隐私保护的检索增强生成算法（MURAG和MURAG-ADA），用于多查询场景下的隐私保护RAG系统，能够在实际隐私预算内扩展到数百个查询。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私RAG方法仅适用于单查询场景，无法满足实际使用需求。当外部语料库包含敏感信息时，未受保护的RAG系统存在隐私泄露风险。

Method: 提出了两种算法：MURAG利用个体隐私过滤器，使累积隐私损失仅取决于文档被检索的频率而非查询总数；MURAG-ADA通过私有发布查询特定阈值，进一步提高效用，实现更精确的相关文档选择。

Result: 在多个LLM和数据集上的实验表明，所提方法能在实际差分隐私预算（ε≈10）内扩展到数百个查询，同时保持有意义的效用。

Conclusion: 本文提出的多查询差分隐私RAG算法为实际应用场景提供了可行的隐私保护解决方案，在保护敏感信息的同时维持了系统实用性。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\varepsilon\approx10$), while preserving meaningful utility.

</details>


### [46] [Adaptive Graph Learning with Transformer for Multi-Reservoir Inflow Prediction](https://arxiv.org/abs/2511.07649)
*Pengfei Hu,Ming Fan,Xiaoxue Han,Chang Lu,Wei Zhang,Hyun Kang,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: AdaTrip是一个自适应时变图学习框架，用于多水库入流预测，通过动态图结构和注意力机制捕捉水库间的时空依赖关系，在科罗拉多河上游流域30个水库上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有水库入流预测方法主要关注单一水库模型，忽略了相互连接水库之间的空间依赖性，需要开发能够捕捉水库间水文连接关系的多水库预测框架。

Method: 构建动态图结构，将水库作为节点，有向边反映水文连接关系，采用注意力机制自动识别关键的时空依赖关系，并通过参数共享提升数据有限水库的预测性能。

Result: 在科罗拉多河上游流域30个水库上的评估表明，AdaTrip优于现有基线方法，特别是对记录有限的水库通过参数共享实现了性能提升，并提供可解释的注意力图谱。

Conclusion: AdaTrip通过自适应图学习框架有效解决了多水库入流预测问题，不仅提升了预测精度，还提供了对水文控制机制的可解释性洞察，支持运营决策制定。

Abstract: Reservoir inflow prediction is crucial for water resource management, yet existing approaches mainly focus on single-reservoir models that ignore spatial dependencies among interconnected reservoirs. We introduce AdaTrip as an adaptive, time-varying graph learning framework for multi-reservoir inflow forecasting. AdaTrip constructs dynamic graphs where reservoirs are nodes with directed edges reflecting hydrological connections, employing attention mechanisms to automatically identify crucial spatial and temporal dependencies. Evaluation on thirty reservoirs in the Upper Colorado River Basin demonstrates superiority over existing baselines, with improved performance for reservoirs with limited records through parameter sharing. Additionally, AdaTrip provides interpretable attention maps at edge and time-step levels, offering insights into hydrological controls to support operational decision-making. Our code is available at https://github.com/humphreyhuu/AdaTrip.

</details>


### [47] [Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network](https://arxiv.org/abs/2511.07651)
*Yicheng Zhan,Fahim Ahmed,Amy Burrell,Matthew J. Tonkin,Sarah Galambos,Jessica Woodhams,Dalal Alrajeh*

Main category: cs.LG

TL;DR: 提出了一种Siamese自编码器框架，用于改进犯罪关联分析，通过处理高维稀疏数据并整合地理时间特征，在ViCLAS数据集上实现了比传统方法高达9%的AUC提升。


<details>
  <summary>Details</summary>
Motivation: 传统犯罪关联方法在处理高维、稀疏和异构数据时存在局限性，需要更有效的方法来识别连环犯罪者并提高公共安全。

Method: 使用Siamese自编码器框架学习有意义的潜在表示，在解码器阶段整合地理时间特征以减轻稀疏特征空间中的信号稀释问题，并分析不同领域知识驱动的数据降维策略。

Result: 该方法在多个评估指标上实现了一致的改进，AUC比传统方法提高了高达9%，并为调查决策提供了可解释的见解。

Conclusion: 先进的机器学习方法可以显著提高犯罪关联分析的准确性，同时提供可解释的见解来支持调查决策。

Abstract: Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.

</details>


### [48] [CAE: Character-Level Autoencoder for Non-Semantic Relational Data Grouping](https://arxiv.org/abs/2511.07657)
*Veera V S Bhargav Nunna,Shinae Kang,Zheyuan Zhou,Virginia Wang,Sucharitha Boinapally,Michael Foley*

Main category: cs.LG

TL;DR: 提出了一种字符级自编码器方法，用于自动识别和分组非语义关系数据库中的相同列，通过检测数据模式和结构来发现列相似性。


<details>
  <summary>Details</summary>
Motivation: 企业关系数据库包含大量非语义数据（如IP地址、产品标识符、编码键和时间戳），这些数据对传统语义分析构成挑战，需要新的方法来处理非语义关系数据集。

Method: 使用字符级自编码器架构，在字符级别操作并保持固定字典约束，对非语义关系表列的文本表示进行编码，提取高维特征嵌入用于数据分组。

Result: 实验评估显示，在关系数据集的top 5列匹配任务中达到80.95%的准确率，显著优于传统NLP方法如词袋模型（47.62%）。

Conclusion: 该工作弥合了字符级神经架构的理论进展与企业数据管理实践之间的差距，为大规模非语义工业数据集的模式理解和数据剖析提供了自动化解决方案。

Abstract: Enterprise relational databases increasingly contain vast amounts of non-semantic data - IP addresses, product identifiers, encoded keys, and timestamps - that challenge traditional semantic analysis. This paper introduces a novel Character-Level Autoencoder (CAE) approach that automatically identifies and groups semantically identical columns in non-semantic relational datasets by detecting column similarities based on data patterns and structures. Unlike conventional Natural Language Processing (NLP) models that struggle with limitations in semantic interpretability and out-of-vocabulary tokens, our approach operates at the character level with fixed dictionary constraints, enabling scalable processing of large-scale data lakes and warehouses. The CAE architecture encodes text representations of non-semantic relational table columns and extracts high-dimensional feature embeddings for data grouping. By maintaining a fixed dictionary size, our method significantly reduces both memory requirements and training time, enabling efficient processing of large-scale industrial data environments. Experimental evaluation demonstrates substantial performance gains: our CAE approach achieved 80.95% accuracy in top 5 column matching tasks across relational datasets, substantially outperforming traditional NLP approaches such as Bag of Words (47.62%). These results demonstrate its effectiveness for identifying and clustering identical columns in relational datasets. This work bridges the gap between theoretical advances in character-level neural architectures and practical enterprise data management challenges, providing an automated solution for schema understanding and data profiling of non-semantic industrial datasets at scale.

</details>


### [49] [Probabilities Are All You Need: A Probability-Only Approach to Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2511.07694)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 提出一种无需训练的高效不确定性估计方法，通过使用回答的前K个概率来近似预测熵，并采用自适应机制确定K值以提高灵活性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理任务中表现强劲，但容易产生幻觉，生成事实错误或误导性输出。现有不确定性估计方法通常需要多次采样或额外计算来评估语义熵。

Method: 使用回答的前K个概率来近似预测熵，并采用自适应机制动态确定K值，过滤低置信度概率。

Result: 在三个自由形式问答数据集和多个LLM上的实验结果表明，该方法优于昂贵的现有最优基线方法。

Conclusion: 该方法有助于提升LLM的可信度，为更广泛的目标做出贡献。

Abstract: Large Language Models (LLMs) exhibit strong performance across various natural language processing (NLP) tasks but remain vulnerable to hallucinations, generating factually incorrect or misleading outputs. Uncertainty estimation, often using predictive entropy estimation, is key to addressing this issue. However, existing methods often require multiple samples or extra computation to assess semantic entropy. This paper proposes an efficient, training-free uncertainty estimation method that approximates predictive entropy using the responses' top-$K$ probabilities. Moreover, we employ an adaptive mechanism to determine $K$ to enhance flexibility and filter out low-confidence probabilities. Experimental results on three free-form question-answering datasets across several LLMs demonstrate that our method outperforms expensive state-of-the-art baselines, contributing to the broader goal of enhancing LLM trustworthiness.

</details>


### [50] [On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection](https://arxiv.org/abs/2511.07700)
*Brandon Dominique,Prudence Lam,Nicholas Kurtansky,Jochen Weber,Kivanc Kose,Veronica Rotemberg,Jennifer Dy*

Main category: cs.LG

TL;DR: AI模型在黑色素瘤检测中表现出专家级性能，但在不同人口统计子组中存在性能差异。本文通过引入校准作为AUROC公平性指标的补充基准指标，评估了领先皮肤癌检测算法在不同子组中的性能，发现现有模型在新数据集上存在过度诊断风险和校准问题。


<details>
  <summary>Details</summary>
Motivation: AI模型在临床应用中面临性能在不同人口统计子组（性别、种族、年龄）中表现不一致的问题，现有基准主要依赖AUROC指标，无法评估模型提供准确概率估计的能力。

Method: 将校准作为AUROC公平性指标的补充基准指标，评估ISIC 2020挑战赛领先皮肤癌检测算法在ISIC 2020和PROVE-AI数据集上的性能，重点关注性别、种族（菲茨帕特里克皮肤类型）和年龄定义的子组。

Result: 现有模型虽然提高了判别准确性，但在应用于新数据集时往往过度诊断风险并表现出校准问题。

Conclusion: 研究强调需要全面的模型审计策略和广泛的元数据收集，以实现公平的AI驱动医疗解决方案。

Abstract: Artificial Intelligence (AI) models have demonstrated expert-level performance in melanoma detection, yet their clinical adoption is hindered by performance disparities across demographic subgroups such as gender, race, and age. Previous efforts to benchmark the performance of AI models have primarily focused on assessing model performance using group fairness metrics that rely on the Area Under the Receiver Operating Characteristic curve (AUROC), which does not provide insights into a model's ability to provide accurate estimates. In line with clinical assessments, this paper addresses this gap by incorporating calibration as a complementary benchmarking metric to AUROC-based fairness metrics. Calibration evaluates the alignment between predicted probabilities and observed event rates, offering deeper insights into subgroup biases. We assess the performance of the leading skin cancer detection algorithm of the ISIC 2020 Challenge on the ISIC 2020 Challenge dataset and the PROVE-AI dataset, and compare it with the second and third place models, focusing on subgroups defined by sex, race (Fitzpatrick Skin Tone), and age. Our findings reveal that while existing models enhance discriminative accuracy, they often over-diagnose risk and exhibit calibration issues when applied to new datasets. This study underscores the necessity for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven healthcare solutions. All code is publicly available at https://github.com/bdominique/testing_strong_calibration.

</details>


### [51] [Diffusion Guided Adversarial State Perturbations in Reinforcement Learning](https://arxiv.org/abs/2511.07701)
*Xiaolin Sun,Feidi Liu,Zhengming Ding,ZiZhan Zheng*

Main category: cs.LG

TL;DR: SHIFT是一种基于扩散模型的策略无关状态扰动攻击，能够生成语义不同的扰动状态，有效突破现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有RL系统防御方法的有效性主要源于传统l_p范数约束攻击的局限性，这些攻击难以改变图像输入的语义，即使在大扰动预算下也是如此。

Method: 提出SHIFT攻击方法，利用扩散模型生成语义不同但保持真实性和历史对齐的扰动状态，避免被检测。

Result: 评估显示SHIFT攻击能有效突破现有最先进的防御方法，显著优于现有攻击，同时具有更好的感知隐蔽性。

Conclusion: RL智能体对语义感知的对抗扰动存在严重脆弱性，表明需要开发更鲁棒的政策。

Abstract: Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.

</details>


### [52] [Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework](https://arxiv.org/abs/2511.07702)
*Meraj Hassanzadeh,Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani*

Main category: cs.LG

TL;DR: 提出了一种基于科学机器学习的新型多维优化框架，能够瞬时解决复杂优化问题，相比传统方法显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统仿真优化方法存在只能单问题优化、计算时间长等局限性，需要开发更高效的多维优化方法。

Method: 采用深度强化学习架构作为优化器，结合参数化物理信息神经网络作为环境，快速探索问题参数间的关系。

Result: 在微混合器案例中，该方法在整个施密特数范围内均优于基线，最大效率提升约32%（施密特数为13.3时）。

Conclusion: 该方法相比遗传算法具有显著优势，为多维工程优化问题提供了高效的解决方案。

Abstract: Multidimensional optimization has consistently been a critical challenge in engineering. However, traditional simulation-based optimization methods have long been plagued by significant limitations: they are typically capable of optimizing only a single problem at a time and require substantial computational time for meshing and numerical simulation. This paper introduces a novel framework leveraging cutting-edge Scientific Machine Learning (Sci-ML) methodologies to overcome these inherent drawbacks of conventional approaches. The proposed method provides instantaneous solutions to a spectrum of complex, multidimensional optimization problems. A micromixer case study is employed to demonstrate this methodology. An agent, operating on a Deep Reinforcement Learning (DRL) architecture, serves as the optimizer to explore the relationships between key problem parameters. This optimizer interacts with an environment constituted by a parametric Physics-Informed Neural Network (PINN), which responds to the agent's actions at a significantly higher speed than traditional numerical methods. The agent's objective, conditioned on the Schmidt number is to discover the optimal geometric and physical parameters that maximize the micromixer's efficiency. After training the agent across a wide range of Schmidt numbers, we analyzed the resulting optimal designs. Across this entire spectrum, the achieved efficiency was consistently greater than the baseline, normalized value. The maximum efficiency occurred at a Schmidt number of 13.3, demonstrating an improvement of approximately 32%. Finally, a comparative analysis with a Genetic Algorithm was conducted under equivalent conditions to underscore the advantages of the proposed method.

</details>


### [53] [A Ranking-Based Optimization Algorithm for the Vehicle Relocation Problem in Car Sharing Services](https://arxiv.org/abs/2511.07724)
*Piotr Szwed,Paweł Skrzynski,Jarosław Wąs*

Main category: cs.LG

TL;DR: 提出了一种基于区域划分和快速排序算法的车辆重新定位方法，用于解决自由流动汽车共享服务中的车辆重新分配问题，通过使用滑板车转移人员来重新定位车辆。


<details>
  <summary>Details</summary>
Motivation: 解决自由流动汽车共享服务中的车辆重新定位问题，优化车辆分布以满足服务需求，提高服务效率。

Method: 将服务区域划分为具有相似时间模式的区域，应用离散优化方法；提出基于车辆可用性、需求概率密度和行程时长的快速排序算法进行决策。

Result: 在真实数据上的实验显示，相比无优化基线，该算法平均改进8.44%，MIP求解器改进19.6%；根据人员规模，性能指标可提升3%-10%。

Conclusion: 提出的解决方案能有效改善车辆重新定位性能，且算法效率高，适用于实际业务场景。

Abstract: The paper addresses the Vehicle Relocation Problem in free-floating car-sharing services by presenting a solution focused on strategies for repositioning vehicles and transferring personnel with the use of scooters. Our method begins by dividing the service area into zones that group regions with similar temporal patterns of vehicle presence and service demand, allowing the application of discrete optimization methods. In the next stage, we propose a fast ranking-based algorithm that makes its decisions on the basis of the number of cars available in each zone, the projected probability density of demand, and estimated trip durations. The experiments were carried out on the basis of real-world data originating from a major car-sharing service operator in Poland. The results of this algorithm are evaluated against scenarios without optimization that constitute a baseline and compared with the results of an exact algorithm to solve the Mixed Integer Programming (MIP) model. As performance metrics, the total travel time was used. Under identical conditions (number of vehicles, staff, and demand distribution), the average improvements with respect to the baseline of our algorithm and MIP solver were equal to 8.44\% and 19.6\% correspondingly. However, it should be noted that the MIP model also mimicked decisions on trip selection, which are excluded by current services business rules. The analysis of results suggests that, depending on the size of the workforce, the application of the proposed solution allows for improving performance metrics by roughly 3%-10%.

</details>


### [54] [Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning](https://arxiv.org/abs/2511.07730)
*Bill Chunyuan Zheng,Vivek Myers,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 该论文提出了一种结合局部更新和全局更新的目标条件强化学习方法，通过多步蒙特卡洛回报拟合准度量距离，在长视野任务中表现出色，并在真实机器人操作领域实现了多步缝合。


<details>
  <summary>Details</summary>
Motivation: 解决AI中长期视野推理的挑战，特别是如何估计观测对之间的时间距离。传统方法中，时间差分方法具有最优性保证但性能较差，而蒙特卡洛方法性能更好但缺乏保证。

Method: 提出一种实用的GCRL方法，使用多步蒙特卡洛回报拟合准度量距离，将局部更新和全局更新方法相结合。

Result: 在长达4000步的长视野模拟任务中表现优于现有GCRL方法，即使使用视觉观测。在真实机器人操作领域（Bridge设置）实现了多步缝合，是首个在该领域从无标签离线视觉观测数据实现端到端多步缝合的GCRL方法。

Conclusion: 该方法成功整合了局部和全局更新的优势，在长视野任务中取得了显著改进，并在真实世界机器人操作中验证了有效性。

Abstract: Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.

</details>


### [55] [Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations](https://arxiv.org/abs/2511.07734)
*Shu Hong,Yongsheng Mei,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 提出了一个可扩展的图结构全局优化框架，使用低秩谱表示从稀疏结构观测构建高斯过程代理模型，解决了贝叶斯优化在图结构域中的挑战。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在优化昂贵的黑盒目标方面很强大，但扩展到图结构领域仍然具有挑战性，因为图的离散和组合性质。现有方法要么依赖完整的图拓扑（对于大型或部分观测图不实用），要么依赖增量探索（收敛缓慢）。

Method: 使用低秩谱表示从稀疏结构观测构建高斯过程代理模型，通过可学习嵌入联合推断图结构和节点表示，实现高效的全局搜索和原则性不确定性估计。

Result: 在合成和真实数据集上的实验表明，与先前方法相比，该方法实现了更快的收敛和更好的优化性能。

Conclusion: 该框架为图结构优化提供了一个可扩展的解决方案，即使在数据有限的情况下也能实现高效的全局搜索和准确的不确定性估计。

Abstract: Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for large or partially observed graphs-or incremental exploration, which can lead to slow convergence. We introduce a scalable framework for global optimization over graphs that employs low-rank spectral representations to build Gaussian process (GP) surrogates from sparse structural observations. The method jointly infers graph structure and node representations through learnable embeddings, enabling efficient global search and principled uncertainty estimation even with limited data. We also provide theoretical analysis establishing conditions for accurate recovery of underlying graph structure under different sampling regimes. Experiments on synthetic and real-world datasets demonstrate that our approach achieves faster convergence and improved optimization performance compared to prior methods.

</details>


### [56] [From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training](https://arxiv.org/abs/2511.07738)
*Donglai Xu,Hongzheng Yang,Yuzhi Zhao,Pingping Zhang,Jinpeng Chen,Wenao Ma,Zhijian Hou,Mengyang Wu,Xiaolei Li,Senkang Hu,Ziyi Guan,Jason Chun Lok Li,Lai Man Po*

Main category: cs.LG

TL;DR: 提出了一种用于多模态大语言模型强化学习的两阶段令牌级熵优化方法，通过动态调节从探索到利用的训练过程，增强对标注噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中高质量标注数据稀缺且存在显著标注噪声，现有无监督RLVR方法容易过拟合到错误标签，限制了GRPO中关键的奖励排序信号。

Method: 两阶段令牌级熵优化方法：探索阶段通过令牌级熵最大化促进多样化输出生成，防止过早收敛到噪声标签；利用阶段通过令牌级熵最小化鼓励模型产生自信的确定性输出。

Result: 在三个MLLM骨干网络（Qwen2-VL-2B、Qwen2-VL-7B、Qwen2.5-VL-3B）上，该方法在多种噪声设置和任务中始终优于先前方法，实现了鲁棒且优越的性能。

Conclusion: 该分阶段策略通过统一和增强外部、内部和基于熵的方法，在噪声容忍度和性能方面都表现出色，为MLLM的强化学习提供了有效的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.

</details>


### [57] [Schedulers for Schedule-free: Theoretically inspired hyperparameters](https://arxiv.org/abs/2511.07767)
*Yuen-Man Pun,Matthew Buchholz,Robert M. Gower*

Main category: cs.LG

TL;DR: 本文扩展了无调度方法的理论，支持任意学习率调度器，并提出了新的自适应Polyak学习率调度方法，在深度神经网络实验中验证了理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的无调度方法理论仅支持恒定学习率，而实际实现使用了预热调度，需要扩展理论以支持任意调度器并更新平均参数。

Method: 扩展无调度方法的最后迭代收敛理论，允许任意调度器使用，并基于凸性假设设计了新的自适应Polyak学习率调度方法。

Result: 理论证明了在wsd调度下达到最优收敛率O(1/√T)，新Polyak调度在模型蒸馏任务中表现优于多个基线方法。

Conclusion: 扩展的无调度理论具有实际预测能力，新设计的自适应Polyak调度实现了最优的任意时间最后迭代收敛性能。

Abstract: The recently proposed schedule-free method has been shown to achieve strong performance when hyperparameter tuning is limited. The current theory for schedule-free only supports a constant learning rate, where-as the implementation used in practice uses a warm-up schedule. We show how to extend the last-iterate convergence theory of schedule-free to allow for any scheduler, and how the averaging parameter has to be updated as a function of the learning rate. We then perform experiments showing how our convergence theory has some predictive power with regards to practical executions on deep neural networks, despite that this theory relies on assuming convexity. When applied to the warmup-stable-decay (wsd) schedule, our theory shows the optimal convergence rate of $\mathcal{O}(1/\sqrt{T})$. We then use convexity to design a new adaptive Polyak learning rate schedule for schedule-free. We prove an optimal anytime last-iterate convergence for our new Polyak schedule, and show that it performs well compared to a number of baselines on a black-box model distillation task.

</details>


### [58] [Physical Consistency of Aurora's Encoder: A Quantitative Study](https://arxiv.org/abs/2511.07787)
*Benjamin Richards,Pushpa Kumar Balan*

Main category: cs.LG

TL;DR: 本文通过线性分类器探测Aurora天气模型的潜在表示，验证其是否学习到物理一致的特征，发现模型能识别陆地-海洋边界、极端温度事件和大气不稳定等概念，但难以捕捉最罕见事件。


<details>
  <summary>Details</summary>
Motivation: 大型天气预报模型如Aurora虽然准确但缺乏透明度，这种"黑箱"特性阻碍了其在高风险操作环境中的应用，需要验证其内部表示是否与已知物理和气象概念一致。

Method: 使用大规模嵌入数据集，训练线性分类器来识别三个不同概念：基本陆地-海洋边界、高影响极端温度事件和大气不稳定。

Result: 定量证据表明Aurora学习了物理一致的特征，但也突显了其在捕捉最罕见事件方面的局限性。

Conclusion: 这项工作强调了可解释性方法对于验证和建立对下一代AI驱动天气模型信任的关键需求。

Abstract: The high accuracy of large-scale weather forecasting models like Aurora is often accompanied by a lack of transparency, as their internal representations remain largely opaque. This "black box" nature hinders their adoption in high-stakes operational settings. In this work, we probe the physical consistency of Aurora's encoder by investigating whether its latent representations align with known physical and meteorological concepts. Using a large-scale dataset of embeddings, we train linear classifiers to identify three distinct concepts: the fundamental land-sea boundary, high-impact extreme temperature events, and atmospheric instability. Our findings provide quantitative evidence that Aurora learns physically consistent features, while also highlighting its limitations in capturing the rarest events. This work underscores the critical need for interpretability methods to validate and build trust in the next generation of Al-driven weather models.

</details>


### [59] [Analyzing Political Text at Scale with Online Tensor LDA](https://arxiv.org/abs/2511.07809)
*Sara Kangaslahti,Danny Ebanks,Jean Kossaifi,Anqi Liu,R. Michael Alvarez,Animashree Anandkumar*

Main category: cs.LG

TL;DR: 提出一种可扩展到数十亿文档的主题建模方法TLDA，具有参数可识别性和样本复杂度保证，计算效率比现有并行LDA方法快3-4倍，并提供GPU实现。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法难以处理超大规模文本数据，限制了社会科学研究者对海量语料库的实时分析能力。

Method: 提出张量潜在狄利克雷分配(TLDA)方法，利用张量分解技术确保参数可识别性和恢复性，并实现GPU加速。

Result: 方法在数十亿文档规模上仍保持线性扩展性，成功应用于#MeToo运动演变分析和2020大选舞弊社交媒体对话研究。

Conclusion: 该方法为社会科学研究者提供了分析超大规模语料库的能力，能够近乎实时地研究重要理论相关问题。

Abstract: This paper proposes a topic modeling method that scales linearly to billions of documents. We make three core contributions: i) we present a topic modeling method, Tensor Latent Dirichlet Allocation (TLDA), that has identifiable and recoverable parameter guarantees and sample complexity guarantees for large data; ii) we show that this method is computationally and memory efficient (achieving speeds over 3-4x those of prior parallelized Latent Dirichlet Allocation (LDA) methods), and that it scales linearly to text datasets with over a billion documents; iii) we provide an open-source, GPU-based implementation, of this method. This scaling enables previously prohibitive analyses, and we perform two real-world, large-scale new studies of interest to political scientists: we provide the first thorough analysis of the evolution of the #MeToo movement through the lens of over two years of Twitter conversation and a detailed study of social media conversations about election fraud in the 2020 presidential election. Thus this method provides social scientists with the ability to study very large corpora at scale and to answer important theoretically-relevant questions about salient issues in near real-time.

</details>


### [60] [Multi-Objective Bilevel Learning](https://arxiv.org/abs/2511.07824)
*Zhiyao Zhang,Zhuqing Liu,Xin Zhang,Wen-Yen Chen,Jiyan Yang,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的加权切比雪夫多超梯度下降(WC-MHGD)算法框架，用于解决多目标双层学习问题，在确定性和随机设置下都具有有限时间帕累托平稳性收敛保证。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用日益复杂，现代ML框架需要处理不同层间耦合决策变量的多个潜在冲突目标，这产生了对多目标双层学习(MOBL)的需求。目前该领域仍处于起步阶段，许多重要问题尚未充分探索。

Method: 提出加权切比雪夫多超梯度下降(WC-MHGD)算法框架，考虑上层子问题偏好引导的多冲突目标MOBL问题，其中部分输入依赖于下层子问题的最优解。

Result: 该框架在确定性和随机设置下都具有有限时间帕累托平稳性收敛率保证，不仅意味着低oracle复杂度，还能实现系统的帕累托前沿探索。

Conclusion: 通过广泛实验验证了理论结果，为MOBL问题提供了有效的优化算法，能够识别偏好引导的帕累托平稳解并实现系统的帕累托前沿探索。

Abstract: As machine learning (ML) applications grow increasingly complex in recent years, modern ML frameworks often need to address multiple potentially conflicting objectives with coupled decision variables across different layers. This creates a compelling need for multi-objective bilevel learning (MOBL). So far, however, the field of MOBL remains in its infancy and many important problems remain under-explored. This motivates us to fill this gap and systematically investigate the theoretical and algorithmic foundation of MOBL. Specifically, we consider MOBL problems with multiple conflicting objectives guided by preferences at the upper-level subproblem, where part of the inputs depend on the optimal solution of the lower-level subproblem. Our goal is to develop efficient MOBL optimization algorithms to (1) identify a preference-guided Pareto-stationary solution with low oracle complexity; and (2) enable systematic Pareto front exploration. To this end, we propose a unifying algorithmic framework called weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) for both deterministic and stochastic settings with finite-time Pareto-stationarity convergence rate guarantees, which not only implies low oracle complexity but also induces systematic Pareto front exploration. We further conduct extensive experiments to confirm our theoretical results.

</details>


### [61] [MURPHY: Multi-Turn GRPO for Self Correcting Code Generation](https://arxiv.org/abs/2511.07833)
*Chanakya Ekbote,Vijay Lingam,Behrooz Omidvar-Tehrani,Jun Huan,Sujay Sanghavi,Anoop Deoras,Stefano Soatto*

Main category: cs.LG

TL;DR: Murphy是一个多轮反思优化框架，通过结合迭代自我校正来增强语言模型的推理能力，在代码生成任务上相比GRPO方法获得8%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法如GRPO在推理基准上有效，但在需要迭代决策的代理任务上表现不佳，需要开发能够进行多轮自我校正的框架。

Method: Murphy扩展了GRPO，在训练过程中加入迭代自我校正，利用定量和定性执行反馈，让模型在多轮中逐步优化推理过程。

Result: 在Qwen和OLMo等模型家族上的代码生成基准测试显示，Murphy在相似计算预算下相比GRPO获得高达8%的pass@1相对增益。

Conclusion: Murphy框架通过多轮反思优化有效提升了语言模型在代理任务中的推理能力，证明了迭代自我校正的重要性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.

</details>


### [62] [DP-AdamW: Investigating Decoupled Weight Decay and Bias Correction in Private Deep Learning](https://arxiv.org/abs/2511.07843)
*Jay Chooi,Kevin Cong,Russell Li,Lillian Sun*

Main category: cs.LG

TL;DR: 提出了DP-AdamW和DP-AdamW-BC两种差分隐私优化器，其中DP-AdamW在文本分类、图像分类和图节点分类任务上均优于现有差分隐私优化器，而DP-AdamW-BC的偏置校正反而降低了性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习广泛使用敏感数据，需要差分隐私保护来防止训练过程中的信息泄露，但现有DP优化器在保持性能方面存在挑战。AdamW因其强大的经验性能而流行，但缺乏专门的DP版本。

Method: 开发了DP-AdamW及其偏置校正版本DP-AdamW-BC，提供了隐私和收敛的理论保证，并在多个隐私预算下进行实证分析。

Result: DP-AdamW在文本分类上比现有DP优化器高出15%以上，图像分类上高出5%，图节点分类上稳定高出1%。但DP-AdamW-BC的偏置校正反而降低了准确率。

Conclusion: DP-AdamW是当前最有效的差分隐私优化器，但偏置校正策略需要重新评估，因为与DP-AdamBC不同，它在DP-AdamW中未能带来性能提升。

Abstract: As deep learning methods increasingly utilize sensitive data on a widespread scale, differential privacy (DP) offers formal guarantees to protect against information leakage during model training. A significant challenge remains in implementing DP optimizers that retain strong performance while preserving privacy. Recent advances introduced ever more efficient optimizers, with AdamW being a popular choice for training deep learning models because of strong empirical performance. We study \emph{DP-AdamW} and introduce \emph{DP-AdamW-BC}, a differentially private variant of the AdamW optimizer with DP bias correction for the second moment estimator. We start by showing theoretical results for privacy and convergence guarantees of DP-AdamW and DP-AdamW-BC. Then, we empirically analyze the behavior of both optimizers across multiple privacy budgets ($ε= 1, 3, 7$). We find that DP-AdamW outperforms existing state-of-the-art differentially private optimizers like DP-SGD, DP-Adam, and DP-AdamBC, scoring over 15\% higher on text classification, up to 5\% higher on image classification, and consistently 1\% higher on graph node classification. Moreover, we empirically show that incorporating bias correction in DP-AdamW (DP-AdamW-BC) consistently decreases accuracy, in contrast to the improvement of DP-AdamBC improvement over DP-Adam.

</details>


### [63] [A General Method for Proving Networks Universal Approximation Property](https://arxiv.org/abs/2511.07857)
*Wei Wang*

Main category: cs.LG

TL;DR: 提出一个通用模块化框架来证明神经网络的通用逼近性质，通过定义具有通用逼近能力的模块(UAM)，证明由这些模块组成的网络自动保持通用逼近性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每种新架构单独构造证明，存在冗余且缺乏统一理论基础，阻碍了对不同网络家族的统一理论理解。

Method: 定义通用逼近模块(UAM)作为基本构建块，证明由UAM组成的深度网络自动保持通用逼近性质，并将逼近过程解释为模块间的渐进精炼。

Result: 建立了一个统一的分析框架，能够同时处理多种网络架构的通用逼近证明，避免了重复工作。

Conclusion: 该框架不仅统一了对不同架构的分析，还使人们能够逐步理解表达能力在网络中的演化过程。

Abstract: Deep learning architectures are highly diverse. To prove their universal approximation properties, existing works typically rely on model-specific proofs. Generally, they construct a dedicated mathematical formulation for each architecture (e.g., fully connected networks, CNNs, or Transformers) and then prove their universal approximability. However, this approach suffers from two major limitations: first, every newly proposed architecture often requires a completely new proof from scratch; second, these proofs are largely isolated from one another, lacking a common analytical foundation. This not only incurs significant redundancy but also hinders unified theoretical understanding across different network families. To address these issues, this paper proposes a general and modular framework for proving universal approximation. We define a basic building block (comprising one or multiple layers) that possesses the universal approximation property as a Universal Approximation Module (UAM). Under this condition, we show that any deep network composed of such modules inherently retains the universal approximation property. Moreover, the overall approximation process can be interpreted as a progressive refinement across modules. This perspective not only unifies the analysis of diverse architectures but also enables a step-by-step understanding of how expressive power evolves through the network.

</details>


### [64] [Algorithm-Relative Trajectory Valuation in Policy Gradient Control](https://arxiv.org/abs/2511.07878)
*Shihao Li,Jiachen Li,Jiamin Xu,Christopher Martin,Wei Li,Dongmei Chen*

Main category: cs.LG

TL;DR: 该论文研究了策略梯度控制中轨迹价值如何依赖于学习算法。在不确定LQR中使用轨迹Shapley分析，发现持续激励(PE)与REINFORCE算法的边际价值呈负相关，但经过稳定化处理后相关性变为正。


<details>
  <summary>Details</summary>
Motivation: 探究轨迹价值对学习算法的依赖性，理解不同算法下轨迹贡献的机制差异。

Method: 在不确定LQR环境中使用轨迹Shapley分析，比较REINFORCE算法及其稳定化版本（状态白化或Fisher预条件处理）下的轨迹价值。

Result: 发现PE与REINFORCE边际价值负相关(r≈-0.38)，稳定化后转为正相关(r≈+0.29)；方差机制解释了这一现象。

Conclusion: 轨迹价值是算法相关的，决策对齐的评分方法（如Leave-One-Out）与Shapley方法在轨迹剪枝中具有互补作用。

Abstract: We study how trajectory value depends on the learning algorithm in policy-gradient control. Using Trajectory Shapley in an uncertain LQR, we find a negative correlation between Persistence of Excitation (PE) and marginal value under vanilla REINFORCE ($r\approx-0.38$). We prove a variance-mediated mechanism: (i) for fixed energy, higher PE yields lower gradient variance; (ii) near saddles, higher variance increases escape probability, raising marginal contribution. When stabilized (state whitening or Fisher preconditioning), this variance channel is neutralized and information content dominates, flipping the correlation positive ($r\approx+0.29$). Hence, trajectory value is algorithm-relative. Experiments validate the mechanism and show decision-aligned scores (Leave-One-Out) complement Shapley for pruning, while Shapley identifies toxic subsets.

</details>


### [65] [Meta-cognitive Multi-scale Hierarchical Reasoning for Motor Imagery Decoding](https://arxiv.org/abs/2511.07884)
*Si-Hyun Kim,Heon-Gyu Kwak,Byoung-Hee Kwon,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出了一种用于四类运动想象分类的分层和元认知解码框架，通过多尺度分层信号处理模块和自省不确定性估计模块，提高了EEG信号解码的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑机接口在实际部署中受到运动想象EEG信号的噪声和变异性的限制，需要提高解码的可靠性和鲁棒性。

Method: 使用多尺度分层信号处理模块重新组织骨干特征为时间多尺度表示，并结合自省不确定性估计模块分配每周期可靠性分数并指导迭代优化。在三个标准EEG骨干网络(EEGNet、ShallowConvNet和DeepConvNet)上实例化该框架。

Result: 在所有骨干网络上，所提出的组件相比基线提高了平均分类准确率并减少了受试者间方差，表明对受试者异质性和噪声试验的鲁棒性增强。

Conclusion: 将分层多尺度处理与自省置信度估计相结合可以增强基于运动想象的脑机接口系统的可靠性。

Abstract: Brain-computer interface (BCI) aims to decode motor intent from noninvasive neural signals to enable control of external devices, but practical deployment remains limited by noise and variability in motor imagery (MI)-based electroencephalogram (EEG) signals. This work investigates a hierarchical and meta-cognitive decoding framework for four-class MI classification. We introduce a multi-scale hierarchical signal processing module that reorganizes backbone features into temporal multi-scale representations, together with an introspective uncertainty estimation module that assigns per-cycle reliability scores and guides iterative refinement. We instantiate this framework on three standard EEG backbones (EEGNet, ShallowConvNet, and DeepConvNet) and evaluate four-class MI decoding using the BCI Competition IV-2a dataset under a subject-independent setting. Across all backbones, the proposed components improve average classification accuracy and reduce inter-subject variance compared to the corresponding baselines, indicating increased robustness to subject heterogeneity and noisy trials. These results suggest that combining hierarchical multi-scale processing with introspective confidence estimation can enhance the reliability of MI-based BCI systems.

</details>


### [66] [A Generalized Spectral Framework to Expain Neural Scaling and Compression Dynamics](https://arxiv.org/abs/2511.07892)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个广义谱框架，统一了学习动态和压缩现象，通过将谱演化函数从线性核形式推广到渐近多项式函数，并引入了有效谱-时间弹性参数。


<details>
  <summary>Details</summary>
Motivation: 尽管经验缩放定律在特定体系内是一致的，但在模型压缩等相关设置中报告了明显不同的缩放行为。受神经网络表示谱分析最新进展的启发，需要开发一个统一框架来理解这些现象。

Method: 开发了一个广义谱框架，将谱演化函数从线性核形式g(λt)=λt推广到渐近多项式函数g(λ,t;β)，该函数由有效谱-时间弹性ρ(β)表征。

Result: 该框架恢复了现有的惰性学习和特征学习理论作为特例，并得出了学习与压缩之间的不变关系。

Conclusion: 提出的广义谱框架为理解不同缩放行为提供了统一的理论基础，将学习动态和压缩现象纳入共同的功能框架中。

Abstract: Empirical scaling laws describe how test loss and other performance metrics depend on model size, dataset size, and compute. While such laws are consistent within specific regimes, apparently distinct scaling behaviors have been reported for related settings such as model compression. Motivated by recent progress in spectral analyses of neural representations, this paper develops a \emph{generalized spectral framework} that unifies learning dynamics and compression phenomena under a common functional ansatz. We generalize the spectral evolution function from the linear kernel form $g(λt)=λt$ to an asymptotically polynomial function $g(λ,t;β)$, characterized by an effective spectral--temporal elasticity $ρ(β)$. This framework recovers existing lazy and feature-learning theories as special cases and yields an invariant relation between learning and compression

</details>


### [67] [Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction](https://arxiv.org/abs/2511.07899)
*Ihab Tabbara,Yuxuan Yang,Hussein Sibai*

Main category: cs.LG

TL;DR: 提出基于共形预测的框架，为学习型Hamilton-Jacobi可达性分析提供概率安全保证，通过校准不安全名义控制器与学习安全策略之间的切换来防止系统到达故障状态。


<details>
  <summary>Details</summary>
Motivation: 学习型HJ可达性分析虽然能近似值函数，但无法保证学习结果正确性，需要提供安全保证。

Method: 使用共形预测框架校准名义控制器与学习安全策略的切换，并研究使用独立训练HJ值函数集合作为安全过滤器。

Result: 通过CP框架为学习型HJ值函数和策略提供概率安全保证，防止控制系统到达故障状态。

Conclusion: 共形预测方法能够有效为学习型HJ可达性分析提供可量化的安全保证，提升自主系统的安全部署可靠性。

Abstract: Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.

</details>


### [68] [Test-driven Reinforcement Learning](https://arxiv.org/abs/2511.07904)
*Zhao Yu,Xiuping Wu,Liangjun Ke*

Main category: cs.LG

TL;DR: 提出测试驱动强化学习(TdRL)框架，使用多个测试函数替代单一奖励函数来定义任务目标，解决了强化学习中奖励函数设计困难的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励函数既要定义最优目标又要指导学习过程，这种双重目的使得手动设计奖励函数很困难，往往导致任务表示不理想。

Method: 使用通过-失败测试和指示性测试两类测试函数分别定义最优目标和指导学习过程；提出基于最大熵策略优化的轨迹回报函数学习方法；开发了lexicographic启发式方法比较轨迹与最优轨迹集的相对距离关系。

Result: 在DeepMind Control Suite基准测试中，TdRL在策略训练方面匹配或优于手工设计的奖励方法，具有更高的设计简单性和对多目标优化的内在支持。

Conclusion: TdRL为表示任务目标提供了新的视角，有助于解决强化学习应用中的奖励设计挑战。

Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.

</details>


### [69] [CellARC: Measuring Intelligence with Cellular Automata](https://arxiv.org/abs/2511.07908)
*Miroslav Lžičař*

Main category: cs.LG

TL;DR: CellARC是一个基于多色一维元胞自动机的抽象推理合成基准，包含95k训练集和2k测试集，支持快速迭代和小模型评估。


<details>
  <summary>Details</summary>
Motivation: 构建一个可控的任务空间，解耦泛化能力与人类先验知识，支持难度可控的无限采样，研究模型在有限预算下快速推断新规则的能力。

Method: 使用多色一维元胞自动机构建基准，包含字母表大小、半径、规则族、Langton's lambda等可控参数，支持符号、循环、卷积、Transformer、递归和LLM等基线模型评估。

Result: 10M参数的普通Transformer在插值/外推测试集上达到58.0%/32.4%的准确率，GPT-5 High达到62.3%/48.1%，Transformer与最佳符号基线的集成模型达到65.4%/35.5%。

Conclusion: CellARC提供了一个可重现的研究平台，展示了神经符号互补性，并为快速规则推断研究提供了可控的基准环境。

Abstract: We introduce CellARC, a synthetic benchmark for abstraction and reasoning built from multicolor 1D cellular automata (CA). Each episode has five support pairs and one query serialized in 256 tokens, enabling rapid iteration with small models while exposing a controllable task space with explicit knobs for alphabet size k, radius r, rule family, Langton's lambda, query coverage, and cell entropy. We release 95k training episodes plus two 1k test splits (interpolation/extrapolation) and evaluate symbolic, recurrent, convolutional, transformer, recursive, and LLM baselines. CellARC decouples generalization from anthropomorphic priors, supports unlimited difficulty-controlled sampling, and enables reproducible studies of how quickly models infer new rules under tight budgets. Our strongest small-model baseline (a 10M-parameter vanilla transformer) outperforms recent recursive models (TRM, HRM), reaching 58.0%/32.4% per-token accuracy on the interpolation/extrapolation splits, while a large closed model (GPT-5 High) attains 62.3%/48.1% on subsets of 100 test tasks. An ensemble that chooses per episode between the Transformer and the best symbolic baseline reaches 65.4%/35.5%, highlighting neuro-symbolic complementarity. Leaderboard: https://cellarc.mireklzicar.com

</details>


### [70] [Rectified Noise: A Generative Model Using Positive-incentive Noise](https://arxiv.org/abs/2511.07911)
*Zhenyu Gu,Yanchen Xu,Sida Huang,Yubin Guo,Hongyuan Zhang*

Main category: cs.LG

TL;DR: 提出Rectified Noise (ΔRN)方法，通过在预训练的Rectified Flow模型的速度场中注入π-noise，显著提升生成性能，仅需0.39%额外参数。


<details>
  <summary>Details</summary>
Motivation: 尽管Rectified Flow主要基于概率流ODE，但研究发现通过反向时间SDE注入噪声可以提升生成性能。受Positive-incentive Noise (π-noise)启发，开发新的生成算法。

Method: 提出Rectified Noise (ΔRN)算法，训练π-noise生成器，将π-noise注入预训练RF模型的速度场中，实现从预训练RF模型到π-noise生成器的高效转换。

Result: 在ImageNet-1k上，使用Rectified Noise的RF模型将FID从10.16降至9.05；π-noise生成器仅需0.39%额外训练参数即可实现性能提升。

Conclusion: Rectified Noise是一种有效的生成算法，能够显著提升预训练RF模型的生成性能，且参数效率极高。

Abstract: Rectified Flow (RF) has been widely used as an effective generative model. Although RF is primarily based on probability flow Ordinary Differential Equations (ODE), recent studies have shown that injecting noise through reverse-time Stochastic Differential Equations (SDE) for sampling can achieve superior generative performance. Inspired by Positive-incentive Noise ($π$-noise), we propose an innovative generative algorithm to train $π$-noise generators, namely Rectified Noise ($Δ$RN), which improves the generative performance by injecting $π$-noise into the velocity field of pre-trained RF models. After introducing the Rectified Noise pipeline, pre-trained RF models can be efficiently transformed into $π$-noise generators. We validate Rectified Noise by conducting extensive experiments across various model architectures on different datasets. Notably, we find that: (1) RF models using Rectified Noise reduce FID from \textbf{10.16 to 9.05} on ImageNet-1k. (2) The models of $π$-noise generators achieve improved performance with only \textbf{0.39\%} additional training parameters.

</details>


### [71] [Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison](https://arxiv.org/abs/2511.07919)
*Yoonho Lee,Joseph Boen,Chelsea Finn*

Main category: cs.LG

TL;DR: Feedback Descent是一个通过结构化文本反馈而非标量奖励来优化文本制品（提示、代码、分子）的框架，在推理时无需修改模型权重即可实现定向优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法将详细评判压缩为二进制偏好，造成信息瓶颈。Feedback Descent通过保留结构化文本反馈来拓宽偏好学习中的信息瓶颈，实现在文本空间而非权重空间的定向优化。

Method: 使用上下文学习将结构化反馈转化为类似梯度的方向信息，实现针对性编辑。评估器为每个比较配对文本反馈，作为高带宽监督。整个迭代循环在推理时完成，无需修改模型权重。

Result: 在三个不同领域评估中，Feedback Descent优于最先进的提示优化方法（GEPA）、强化学习方法（GRPO、REINVENT）以及专门的基于图的分子优化器。在DOCKSTRING分子发现基准测试中，识别出超过260,000个化合物数据库中99.9%百分位的新颖类药物分子。

Conclusion: Feedback Descent通过结构化文本反馈实现了有效的文本制品优化，在多个领域超越了现有方法，证明了保留详细评判而非压缩为二进制偏好的优势。

Abstract: We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.

</details>


### [72] [SERL: Self-Examining Reinforcement Learning on Open-Domain](https://arxiv.org/abs/2511.07922)
*Weixuan Ou,Yanzhao Zheng,Shuoshuo Sun,Wei Zhang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Pengwei Yan,Yifan Qiao*

Main category: cs.LG

TL;DR: 提出了SERL（自我检查强化学习）框架，让LLM同时扮演Actor和Judge角色，通过两种内部奖励机制实现自我改进，无需外部信号。


<details>
  <summary>Details</summary>
Motivation: 解决开放域任务中RL面临的两个关键挑战：主观性任务无法提供可验证奖励，以及RLHF依赖外部奖励机制。

Method: SERL框架包含两个协同的奖励机制：基于Copeland风格成对比较的Actor改进奖励，和鼓励一致判断的Judge可靠性奖励。

Result: 在AlpacaEval 2上，Qwen3-8B的LC胜率从52.37%提升到59.90%，达到自我改进方法中的最先进水平，性能可与更大的Qwen3-32B模型相媲美。

Conclusion: SERL在开放域任务中展现出卓越的有效性和鲁棒性，实现了无需外部信号的自我改进强化学习。

Abstract: Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.

</details>


### [73] [IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data](https://arxiv.org/abs/2511.07930)
*Dang Nha Nguyen,Hai Dang Nguyen,Khoa Tho Anh Nguyen*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列数据增强方法IBMA，将插补增强数据与Mixup增强相结合，在多个预测模型和数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中的数据增强策略相对较少，特别是像Mixup这样的高级技术很少被使用，需要开发更有效的数据增强方法来提升模型泛化能力。

Method: IBMA方法结合了插补增强数据和Mixup增强，通过生成具有时间模式保持的增强样本来提升模型性能。

Result: 在4个数据集上对3种预测模型进行测试，与8种其他增强技术相比，IBMA在24个实例中22个表现更好，其中10个达到最佳性能，特别是在iTrainformer插补下效果最显著。

Conclusion: IBMA是一种有效的时间序列数据增强方法，能够显著提升预测模型的性能，特别是在结合iTrainformer插补时表现最佳。

Abstract: Data augmentation in time series forecasting plays a crucial role in enhancing model performance by introducing variability while maintaining the underlying temporal patterns. However, time series data offers fewer augmentation strategies compared to fields such as image or text, with advanced techniques like Mixup rarely being used. In this work, we propose a novel approach, Imputation-Based Mixup Augmentation (IBMA), which combines Imputation-Augmented data with Mixup augmentation to bolster model generalization and improve forecasting performance. We evaluate the effectiveness of this method across several forecasting models, including DLinear (MLP), TimesNet (CNN), and iTrainformer (Transformer), these models represent some of the most recent advances in time series forecasting. Our experiments, conducted on four datasets (ETTh1, ETTh2, ETTm1, ETTm2) and compared against eight other augmentation techniques, demonstrate that IBMA consistently enhances performance, achieving 22 improvements out of 24 instances, with 10 of those being the best performances, particularly with iTrainformer imputation.

</details>


### [74] [Predict-then-Optimize Method for Seaport Power-Logistics Scheduling: Generalization across Varying Tasks Stream](https://arxiv.org/abs/2511.07938)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Yan Xu,Wentao Huang,Honglin Wen*

Main category: cs.LG

TL;DR: 提出一个决策聚焦的持续学习框架，用于适应不断变化的海港船舶到达模式，通过Fisher信息正则化提升跨任务泛化能力，并开发可微凸代理来稳定梯度反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统预测-优化管道假设下游优化任务配置固定，难以适应由变化的海港船舶到达引起的任务结构演变，导致泛化能力差。

Method: 采用决策聚焦的持续学习框架，引入基于Fisher信息的正则化来保护对先前任务关键的参数，并开发可微凸代理以稳定梯度反向传播。

Result: 在裕廊港校准的实验表明，该方法在决策性能和泛化能力上优于现有方法，同时降低了计算成本。

Conclusion: 所提出的方法能够为新的调度任务学习决策对齐的预测模型，同时保持对早期任务的泛化能力。

Abstract: Power-logistics scheduling in modern seaports typically follow a predict-then-optimize pipeline. To enhance decision quality, decision-focused learning has been proposed to align forecasting and optimization via end-to-end training. However, most formulations assume a fixed task configuration in downstream optimization, and thus generalize poorly to evolving task structures induced by varying seaport vessel arrivals. We address this gap with a decision-focused continual learning framework that adapts online to a stream of scheduling tasks. Specifically, we introduce Fisher information based regularization to enhance cross-task generalization by preserving parameters critical to prior tasks. A differentiable convex surrogate is also developed to stabilize gradient backpropagation. The proposed approach enables learning a decision-aligned forecasting model for new scheduling tasks while retaining generalization on earlier tasks. Experiments calibrated to the Jurong Port demonstrate superior decision performance and generalization over existing methods with reduced computational cost.

</details>


### [75] [Balance Equation-based Distributionally Robust Offline Imitation Learning](https://arxiv.org/abs/2511.07942)
*Rishabh Agrawal,Yusuf Alvi,Rahul Jain,Ashutosh Nayyar*

Main category: cs.LG

TL;DR: 提出一种基于平衡方程的分布鲁棒离线模仿学习框架，仅使用名义动态下的专家演示数据学习鲁棒策略，无需额外环境交互，在动态变化环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习方法假设训练和部署时的环境动态保持不变，但实际中建模误差、参数变化和对抗扰动会导致动态偏移，造成性能严重下降。

Method: 将问题表述为在转移模型不确定性集上的分布鲁棒优化，寻求在最坏情况转移分布下最小化模仿损失的策略，并证明该鲁棒目标可以完全用名义数据分布重新表述。

Result: 在连续控制基准测试中，该方法在扰动或偏移环境下相比最先进的离线模仿学习基线实现了更好的鲁棒性和泛化能力。

Conclusion: 提出的框架能够仅从名义动态下的专家演示中学习鲁棒策略，有效应对环境动态变化带来的挑战。

Abstract: Imitation Learning (IL) has proven highly effective for robotic and control tasks where manually designing reward functions or explicit controllers is infeasible. However, standard IL methods implicitly assume that the environment dynamics remain fixed between training and deployment. In practice, this assumption rarely holds where modeling inaccuracies, real-world parameter variations, and adversarial perturbations can all induce shifts in transition dynamics, leading to severe performance degradation. We address this challenge through Balance Equation-based Distributionally Robust Offline Imitation Learning, a framework that learns robust policies solely from expert demonstrations collected under nominal dynamics, without requiring further environment interaction. We formulate the problem as a distributionally robust optimization over an uncertainty set of transition models, seeking a policy that minimizes the imitation loss under the worst-case transition distribution. Importantly, we show that this robust objective can be reformulated entirely in terms of the nominal data distribution, enabling tractable offline learning. Empirical evaluations on continuous-control benchmarks demonstrate that our approach achieves superior robustness and generalization compared to state-of-the-art offline IL baselines, particularly under perturbed or shifted environments.

</details>


### [76] [Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective](https://arxiv.org/abs/2511.07970)
*Justin Lee,Zheda Mai,Jinsu Yoo,Chongyu Fan,Cheng Zhang,Wei-Lun Chao*

Main category: cs.LG

TL;DR: 本文首次系统研究了文本到图像扩散模型中的持续遗忘问题，发现现有遗忘方法在连续请求下会导致模型效用快速崩溃，并提出基于正则化的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法假设遗忘请求一次性到达，但实践中请求往往是连续的。需要研究持续遗忘场景下模型性能保持的问题。

Method: 研究了一套附加正则化器来缓解参数漂移，并提出基于梯度投影的方法来约束与目标子空间正交的参数漂移。

Result: 提出的方法显著改善了持续遗忘性能，与其他正则化器互补可获得进一步增益。

Conclusion: 持续遗忘是文本到图像生成中的基本挑战，本研究为推进安全可靠的生成AI提供了见解、基准和开放方向。

Abstract: Machine unlearning--the ability to remove designated concepts from a pre-trained model--has advanced rapidly, particularly for text-to-image diffusion models. However, existing methods typically assume that unlearning requests arrive all at once, whereas in practice they often arrive sequentially. We present the first systematic study of continual unlearning in text-to-image diffusion models and show that popular unlearning methods suffer from rapid utility collapse: after only a few requests, models forget retained knowledge and generate degraded images. We trace this failure to cumulative parameter drift from the pre-training weights and argue that regularization is crucial to addressing it. To this end, we study a suite of add-on regularizers that (1) mitigate drift and (2) remain compatible with existing unlearning methods. Beyond generic regularizers, we show that semantic awareness is essential for preserving concepts close to the unlearning target, and propose a gradient-projection method that constrains parameter drift orthogonal to their subspace. This substantially improves continual unlearning performance and is complementary to other regularizers for further gains. Taken together, our study establishes continual unlearning as a fundamental challenge in text-to-image generation and provides insights, baselines, and open directions for advancing safe and accountable generative AI.

</details>


### [77] [Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning](https://arxiv.org/abs/2511.07971)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: LOREN是一种曲率感知的零阶优化方法，用于微调大语言模型，通过自适应估计各向异性扰动分布、低秩块对角预处理器和REINFORCE留一梯度估计器，提高精度和收敛速度，同时减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的零阶方法通过随机扰动的有限差分估计梯度，存在高方差和次优搜索方向的问题，需要改进梯度估计质量。

Method: 将梯度预条件问题重新表述为自适应估计各向异性扰动分布；使用自然进化策略框架通过低秩块对角预处理器捕获曲率；应用REINFORCE留一梯度估计器减少方差。

Result: 在标准LLM基准测试中，LOREN优于最先进的零阶方法，实现更高精度和更快收敛，相比MeZO-Adam峰值内存使用减少高达27.3%。

Conclusion: LOREN通过曲率感知的零阶优化方法有效解决了现有方法的局限性，在精度、收敛速度和内存效率方面均有显著提升。

Abstract: We introduce LOREN, a curvature-aware zeroth-order (ZO) optimization method for fine-tuning large language models (LLMs). Existing ZO methods, which estimate gradients via finite differences using random perturbations, often suffer from high variance and suboptimal search directions. Our approach addresses these challenges by: (i) reformulating the problem of gradient preconditioning as that of adaptively estimating an anisotropic perturbation distribution for gradient estimation, (ii) capturing curvature through a low-rank block diagonal preconditioner using the framework of natural evolution strategies, and (iii) applying a REINFORCE leave-one-out (RLOO) gradient estimator to reduce variance. Experiments on standard LLM benchmarks show that our method outperforms state-of-the-art ZO methods by achieving higher accuracy and faster convergence, while cutting peak memory usage by up to 27.3% compared with MeZO-Adam.

</details>


### [78] [Generalizable Insights for Graph Transformers in Theory and Practice](https://arxiv.org/abs/2511.08028)
*Timo Stoll,Luis Müller,Christopher Morris*

Main category: cs.LG

TL;DR: 提出了广义距离Transformer(GDT)架构，系统分析了图Transformer中注意力机制和位置嵌入的表达能力，并通过大规模实验验证了跨领域应用中的有效设计选择。


<details>
  <summary>Details</summary>
Motivation: 当前图Transformer架构在注意力机制、位置嵌入和表达能力方面差异很大，现有理论结果往往与特定设计选择绑定，缺乏在大规模数据上的全面实证验证，理论与实践之间存在差距。

Method: 提出广义距离Transformer(GDT)架构，使用标准注意力机制并整合近年来的多项进展，通过超过800万张图、约2.7亿个token的大规模实验，涵盖图像目标检测、分子性质预测、代码摘要和分布外算法推理等多个领域。

Result: 识别出在各种应用、任务和模型规模中一致表现良好的设计选择，在少样本迁移设置中无需微调即可展现强大性能。

Conclusion: 将理论和实践发现提炼为关于有效图Transformer设计、训练和推理的多个可泛化见解。

Abstract: Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference.

</details>


### [79] [From Sequential to Recursive: Enhancing Decision-Focused Learning with Bidirectional Feedback](https://arxiv.org/abs/2511.08035)
*Xinyu Wang,Jinxiao Du,Yiyang Peng,Wei Ma*

Main category: cs.LG

TL;DR: 提出了递归决策聚焦学习（R-DFL）框架，通过在下游优化和上游预测之间引入双向反馈，解决了传统顺序决策聚焦学习（S-DFL）的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统顺序决策聚焦学习（S-DFL）无法捕捉复杂交互场景中预测与优化之间的双向反馈，限制了决策质量。

Method: 提出了R-DFL框架，采用两种微分方法：基于自动微分的显式展开和基于定点方法的隐式微分，以实现高效的梯度传播。

Result: 在报童问题和二分匹配问题上的实验表明，R-DFL相比顺序基线方法显著提升了最终决策质量，并在闭环决策问题中展现出强大的适应性。

Conclusion: R-DFL通过引入预测与优化之间的双向反馈，在决策质量上优于传统方法，且计算效率更高，适用于复杂交互场景。

Abstract: Decision-focused learning (DFL) has emerged as a powerful end-to-end alternative to conventional predict-then-optimize (PTO) pipelines by directly optimizing predictive models through downstream decision losses. Existing DFL frameworks are limited by their strictly sequential structure, referred to as sequential DFL (S-DFL). However, S-DFL fails to capture the bidirectional feedback between prediction and optimization in complex interaction scenarios. In view of this, we first time propose recursive decision-focused learning (R-DFL), a novel framework that introduces bidirectional feedback between downstream optimization and upstream prediction. We further extend two distinct differentiation methods: explicit unrolling via automatic differentiation and implicit differentiation based on fixed-point methods, to facilitate efficient gradient propagation in R-DFL. We rigorously prove that both methods achieve comparable gradient accuracy, with the implicit method offering superior computational efficiency. Extensive experiments on both synthetic and real-world datasets, including the newsvendor problem and the bipartite matching problem, demonstrate that R-DFL not only substantially enhances the final decision quality over sequential baselines but also exhibits robust adaptability across diverse scenarios in closed-loop decision-making problems.

</details>


### [80] [DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://arxiv.org/abs/2511.08043)
*Xueliang Zhao,Wei Wu,Jian Guan,Qintong Li,Lingpeng Kong*

Main category: cs.LG

TL;DR: 提出DynaAct框架，自动构建紧凑的动作空间来增强复杂问题解决中的顺序推理，通过提取通用草图和使用子模函数选择最优候选集


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖缺乏可扩展性的手动定义动作空间，要么使用计算上不可行的非结构化空间，需要自动构建紧凑动作空间的方法

Method: 首先通过LLM从多样化复杂推理问题语料中提取通用草图来估计完整动作空间，然后基于效用和多样性制定子模函数，使用贪心算法选择最优候选集

Result: 在六个不同标准基准测试上的广泛实验表明，该方法显著提高了整体性能，同时保持了高效推理且不引入显著延迟

Conclusion: DynaAct框架能够有效构建紧凑动作空间，提升顺序推理性能，同时保持计算效率

Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.

</details>


### [81] [Online Linear Regression with Paid Stochastic Features](https://arxiv.org/abs/2511.08073)
*Nadav Merlis,Kyoungseok Jang,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 研究在线线性回归中特征向量被噪声污染的问题，学习者可以付费降低噪声水平。当噪声协方差已知时，最优遗憾率为√T；当未知时，最优遗憾率变为T^{2/3}。


<details>
  <summary>Details</summary>
Motivation: 研究实际中特征测量精度与成本之间的权衡问题，例如使用更昂贵设备提高测量精度，或通过激励数据提供者发布隐私保护较少的特征。

Method: 使用矩阵鞅集中性分析，证明经验损失对所有支付和线性预测器一致收敛于期望损失。

Result: 当噪声协方差已知时，忽略对数项的最优遗憾率为√T；当噪声协方差未知时，最优遗憾率为T^{2/3}（忽略对数项）。

Conclusion: 在线线性回归中，特征噪声协方差的已知与否显著影响学习性能，未知协方差会导致更高的遗憾率。

Abstract: We study an online linear regression setting in which the observed feature vectors are corrupted by noise and the learner can pay to reduce the noise level. In practice, this may happen for several reasons: for example, because features can be measured more accurately using more expensive equipment, or because data providers can be incentivized to release less private features. Assuming feature vectors are drawn i.i.d. from a fixed but unknown distribution, we measure the learner's regret against the linear predictor minimizing a notion of loss that combines the prediction error and payment. When the mapping between payments and noise covariance is known, we prove that the rate $\sqrt{T}$ is optimal for regret if logarithmic factors are ignored. When the noise covariance is unknown, we show that the optimal regret rate becomes of order $T^{2/3}$ (ignoring log factors). Our analysis leverages matrix martingale concentration, showing that the empirical loss uniformly converges to the expected one for all payments and linear predictors.

</details>


### [82] [An Integrated Fusion Framework for Ensemble Learning Leveraging Gradient Boosting and Fuzzy Rule-Based Models](https://arxiv.org/abs/2511.08077)
*Jinbo Li,Peng Liu,Long Chen,Witold Pedrycz,Weiping Ding*

Main category: cs.LG

TL;DR: 提出一个集成融合框架，将梯度提升与模糊规则模型相结合，通过动态控制因子优化模型贡献，防止过拟合并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 模糊规则模型具有良好可解释性但面临复杂设计和大数据集扩展性问题，而梯度提升能提供强大性能但缺乏可解释性。融合两者可克服各自局限。

Method: 集成融合框架，在每次迭代中构建模糊规则模型，通过动态控制因子调节模型贡献，防止模型主导、鼓励多样性、作为正则化参数，并包含基于样本的校正机制。

Result: 实验证明该框架能有效提升性能，特别是在减轻过拟合和规则复杂性方面，同时保持模型的可解释性。

Conclusion: 通过优化因子控制每个模型的贡献，该框架在提升性能的同时保持了可解释性，简化了模型的维护和更新。

Abstract: The integration of different learning paradigms has long been a focus of machine learning research, aimed at overcoming the inherent limitations of individual methods. Fuzzy rule-based models excel in interpretability and have seen widespread application across diverse fields. However, they face challenges such as complex design specifications and scalability issues with large datasets. The fusion of different techniques and strategies, particularly Gradient Boosting, with Fuzzy Rule-Based Models offers a robust solution to these challenges. This paper proposes an Integrated Fusion Framework that merges the strengths of both paradigms to enhance model performance and interpretability. At each iteration, a Fuzzy Rule-Based Model is constructed and controlled by a dynamic factor to optimize its contribution to the overall ensemble. This control factor serves multiple purposes: it prevents model dominance, encourages diversity, acts as a regularization parameter, and provides a mechanism for dynamic tuning based on model performance, thus mitigating the risk of overfitting. Additionally, the framework incorporates a sample-based correction mechanism that allows for adaptive adjustments based on feedback from a validation set. Experimental results substantiate the efficacy of the presented gradient boosting framework for fuzzy rule-based models, demonstrating performance enhancement, especially in terms of mitigating overfitting and complexity typically associated with many rules. By leveraging an optimal factor to govern the contribution of each model, the framework improves performance, maintains interpretability, and simplifies the maintenance and update of the models.

</details>


### [83] [Hierarchical Structure-Property Alignment for Data-Efficient Molecular Generation and Editing](https://arxiv.org/abs/2511.08080)
*Ziyu Fan,Zhijian Huang,Yahan Li,Xiaowen Hu,Siyuan Shen,Yunliang Wang,Zeyu Zhong,Shuhong Liu,Shuning Yang,Shangqian Wu,Min Wu,Lei Deng*

Main category: cs.LG

TL;DR: HSPAG是一个数据高效的分层结构-性质对齐框架，通过将SMILES和分子性质视为互补模态，在原子、子结构和整个分子层面学习它们的关系，支持多性质约束的可控生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 解决分子生成和编辑中的两个关键问题：(i) 捕获分子结构与多个性质之间的复杂关系具有挑战性；(ii) 分子性质的窄覆盖和不完整标注削弱了基于性质的模型效果。

Method: 采用分层结构-性质对齐方法，通过支架聚类选择代表性样本，通过辅助VAE选择困难样本，减少预训练数据需求；结合性质相关性感知掩码机制和多样化扰动策略提升稀疏标注下的生成质量。

Result: 实验证明HSPAG能够捕获细粒度的结构-性质关系，支持多性质约束下的可控生成。两个真实案例研究进一步验证了其编辑能力。

Conclusion: HSPAG框架有效解决了分子性质约束生成和编辑中的关键挑战，在数据效率和多性质控制方面表现出色。

Abstract: Property-constrained molecular generation and editing are crucial in AI-driven drug discovery but remain hindered by two factors: (i) capturing the complex relationships between molecular structures and multiple properties remains challenging, and (ii) the narrow coverage and incomplete annotations of molecular properties weaken the effectiveness of property-based models. To tackle these limitations, we propose HSPAG, a data-efficient framework featuring hierarchical structure-property alignment. By treating SMILES and molecular properties as complementary modalities, the model learns their relationships at atom, substructure, and whole-molecule levels. Moreover, we select representative samples through scaffold clustering and hard samples via an auxiliary variational auto-encoder (VAE), substantially reducing the required pre-training data. In addition, we incorporate a property relevance-aware masking mechanism and diversified perturbation strategies to enhance generation quality under sparse annotations. Experiments demonstrate that HSPAG captures fine-grained structure-property relationships and supports controllable generation under multiple property constraints. Two real-world case studies further validate the editing capabilities of HSPAG.

</details>


### [84] [HipKittens: Fast and Furious AMD Kernels](https://arxiv.org/abs/2511.08083)
*William Hu,Drew Wadsworth,Sean Siddens,Stanley Winata,Daniel Y. Fu,Ryann Swann,Muhammad Osama,Christopher Ré,Simran Arora*

Main category: cs.LG

TL;DR: HipKittens (HK) 是一个为 AMD GPU 设计的编程框架，基于 tile-based 抽象，在 CDNA3 和 CDNA4 平台上与手写汇编性能相当，并在某些 AI 工作负载中超越现有内核 1.2-2.4 倍。


<details>
  <summary>Details</summary>
Motivation: AMD GPU 提供顶尖的计算和内存带宽，但高性能内核需要手写汇编。现有 DSL（如 ThunderKittens）主要针对 NVIDIA 硬件，需要探索这些编程原语在 AMD 上的通用性。

Method: 开发 HipKittens 框架，采用显式的 tile-based 编程模型，优化内存访问和细粒度异步执行，重新设计适用于 AMD 的算法实现。

Result: HK 内核在 GEMM 和注意力计算中与 AMD 手写汇编内核竞争，并始终优于编译器基线。在某些场景下（如 d=64 注意力、GQA 反向传播、内存受限内核）性能提升 1.2-2.4 倍。

Conclusion: Tile-based 抽象可推广到 AMD GPU，但需要重新设计算法实现。HK 为跨 GPU 厂商的高性能 AI 内核提供了统一的软件层基础。

Abstract: AMD GPUs offer state-of-the-art compute and memory bandwidth; however, peak performance AMD kernels are written in raw assembly. To address the difficulty of mapping AI algorithms to hardware, recent work proposes C++ embedded and PyTorch-inspired domain-specific languages like ThunderKittens (TK) to simplify high performance AI kernel development on NVIDIA hardware. We explore the extent to which such primitives -- for explicit tile-based programming with optimized memory accesses and fine-grained asynchronous execution across workers -- are NVIDIA-specific or general. We provide the first detailed study of the programming primitives that lead to performant AMD AI kernels, and we encapsulate these insights in the HipKittens (HK) programming framework. We find that tile-based abstractions used in prior DSLs generalize to AMD GPUs, however we need to rethink the algorithms that instantiate these abstractions for AMD. We validate the HK primitives across CDNA3 and CDNA4 AMD platforms. In evaluations, HK kernels compete with AMD's hand-optimized assembly kernels for GEMMs and attention, and consistently outperform compiler baselines. Moreover, assembly is difficult to scale to the breadth of AI workloads; reflecting this, in some settings HK outperforms all available kernel baselines by $1.2-2.4\times$ (e.g., $d=64$ attention, GQA backwards, memory-bound kernels). These findings help pave the way for a single, tile-based software layer for high-performance AI kernels that translates across GPU vendors. HipKittens is released at: https://github.com/HazyResearch/HipKittens.

</details>


### [85] [Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks](https://arxiv.org/abs/2511.08086)
*Muthukumar Pandaram,Jakob Hollenstein,David Drexel,Samuele Tosatto,Antonio Rodríguez-Sánchez,Justus Piater*

Main category: cs.LG

TL;DR: 本文分析了机器人强化学习环境中真实动态的稀疏性假设，发现全局稀疏性罕见，但存在局部、状态依赖的稀疏性，且呈现特定结构模式。


<details>
  <summary>Details</summary>
Motivation: 检验动态模型中因果图稀疏性和时间稀疏性假设在实际强化学习任务中的有效性。

Method: 通过分析MuJoCo Playground基准套件中机器人强化学习环境的真实动态，研究因果图稀疏性、状态依赖稀疏性和局部动态变化稀疏性。

Result: 全局稀疏性罕见，但存在局部、状态依赖的稀疏性，这种稀疏性呈现时间局部化簇（如接触事件）和影响特定状态维度的结构模式。

Conclusion: 挑战了动态学习中常见的稀疏性先验假设，强调需要基于真实世界动态状态依赖稀疏性结构的接地归纳偏置。

Abstract: The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.
  In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.
  We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.
  Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.

</details>


### [86] [Stuart-Landau Oscillatory Graph Neural Network](https://arxiv.org/abs/2511.08094)
*Kaicheng Zhang,David N. Reynolds,Piero Deidda,Francesco Tudisco*

Main category: cs.LG

TL;DR: 提出了基于Stuart-Landau振荡器动力学的复值图神经网络SLGNN，通过保留振幅和相位动力学来缓解深度GNN的过平滑和梯度消失问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度图神经网络的过平滑和梯度消失问题，现有振荡图神经网络主要基于相位动力学，缺乏振幅调节能力。

Method: 将Stuart-Landau振荡器动力学引入图神经网络，节点特征振幅和相位都动态演化，通过可调超参数控制特征振幅与网络结构的相互作用。

Result: 在节点分类、图分类和图回归任务上的实验表明，SLGNN优于现有振荡图神经网络。

Conclusion: SLGNN为深度振荡图架构提供了一个新颖、表达力强且理论基础扎实的框架。

Abstract: Oscillatory Graph Neural Networks (OGNNs) are an emerging class of physics-inspired architectures designed to mitigate oversmoothing and vanishing gradient problems in deep GNNs. In this work, we introduce the Complex-Valued Stuart-Landau Graph Neural Network (SLGNN), a novel architecture grounded in Stuart-Landau oscillator dynamics. Stuart-Landau oscillators are canonical models of limit-cycle behavior near Hopf bifurcations, which are fundamental to synchronization theory and are widely used in e.g. neuroscience for mesoscopic brain modeling. Unlike harmonic oscillators and phase-only Kuramoto models, Stuart-Landau oscillators retain both amplitude and phase dynamics, enabling rich phenomena such as amplitude regulation and multistable synchronization. The proposed SLGNN generalizes existing phase-centric Kuramoto-based OGNNs by allowing node feature amplitudes to evolve dynamically according to Stuart-Landau dynamics, with explicit tunable hyperparameters (such as the Hopf-parameter and the coupling strength) providing additional control over the interplay between feature amplitudes and network structure. We conduct extensive experiments across node classification, graph classification, and graph regression tasks, demonstrating that SLGNN outperforms existing OGNNs and establishes a novel, expressive, and theoretically grounded framework for deep oscillatory architectures on graphs.

</details>


### [87] [A robust methodology for long-term sustainability evaluation of Machine Learning models](https://arxiv.org/abs/2511.08120)
*Jorge Paz-Ruza,João Gama,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas*

Main category: cs.LG

TL;DR: 提出一个评估机器学习模型长期可持续性的综合协议，适用于批处理和流式学习场景，证明传统静态评估无法可靠反映数据演变和模型更新下的可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统可持续性评估缺乏标准化、模型无关的评估协议，主要关注短期实验资源使用和批处理学习，无法反映真实世界长期AI生命周期。

Method: 提出综合评估协议，在多样化分类任务上使用多种模型类型进行实验，比较批处理和流式学习场景下的可持续性表现。

Result: 实验表明长期可持续性在不同模型间差异显著，且在许多情况下，更高的环境成本带来的性能收益很小。

Conclusion: 传统静态训练-测试评估无法可靠捕捉数据演变和重复模型更新下的可持续性，需要更全面的长期评估框架。

Abstract: Sustainability and efficiency have become essential considerations in the development and deployment of Artificial Intelligence systems, yet existing regulatory and reporting practices lack standardized, model-agnostic evaluation protocols. Current assessments often measure only short-term experimental resource usage and disproportionately emphasize batch learning settings, failing to reflect real-world, long-term AI lifecycles. In this work, we propose a comprehensive evaluation protocol for assessing the long-term sustainability of ML models, applicable to both batch and streaming learning scenarios. Through experiments on diverse classification tasks using a range of model types, we demonstrate that traditional static train-test evaluations do not reliably capture sustainability under evolving data and repeated model updates. Our results show that long-term sustainability varies significantly across models, and in many cases, higher environmental cost yields little performance benefit.

</details>


### [88] [SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories](https://arxiv.org/abs/2511.08136)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 本文提出SafeMIL方法，通过多示例学习从非偏好轨迹中学习参数化成本函数，用于离线安全模仿学习，使智能体在避免风险行为的同时保持奖励性能。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，在线交互可能存在风险，且难以准确指定每个时间步的奖励和安全成本信息。但收集反映不良或风险行为的轨迹（非偏好轨迹）通常是可行的，这些轨迹隐含地传达了智能体应避免的行为。

Method: 提出SafeMIL方法，通过多示例学习学习参数化成本函数，预测状态-动作对是否具有风险。学习到的成本函数用于避免非偏好行为，从而产生优先考虑安全性的策略。

Result: 实验证明该方法能够学习满足成本约束的更安全策略，且不会降低奖励性能，优于多个基线方法。

Conclusion: SafeMIL方法通过利用非偏好轨迹学习成本函数，在离线安全模仿学习中实现了安全性和性能的良好平衡。

Abstract: In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.

</details>


### [89] [Deep (Predictive) Discounted Counterfactual Regret Minimization](https://arxiv.org/abs/2511.08174)
*Hang Xu,Kai Li,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种高效的模型无关神经CFR算法，能够有效近似高级CFR变体，在非完美信息游戏中表现出更快的收敛速度和更强的对抗性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于基础CFR，难以有效整合更先进的CFR变体，限制了CFR在大规模游戏中的适用性。

Method: 使用价值网络收集方差减少的采样优势，通过自举法拟合累积优势，并应用折扣和裁剪操作来模拟高级CFR变体的更新机制。

Result: 与模型无关神经算法相比，在典型非完美信息游戏中收敛更快，在大型扑克游戏中表现出更强的对抗性能。

Conclusion: 所提出的神经CFR算法成功克服了现有方法在近似高级CFR变体方面的局限性，为大规模非完美信息游戏提供了有效的解决方案。

Abstract: Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games. To enhance CFR's applicability in large games, researchers use neural networks to approximate its behavior. However, existing methods are mainly based on vanilla CFR and struggle to effectively integrate more advanced CFR variants. In this work, we propose an efficient model-free neural CFR algorithm, overcoming the limitations of existing methods in approximating advanced CFR variants. At each iteration, it collects variance-reduced sampled advantages based on a value network, fits cumulative advantages by bootstrapping, and applies discounting and clipping operations to simulate the update mechanisms of advanced CFR variants. Experimental results show that, compared with model-free neural algorithms, it exhibits faster convergence in typical imperfect-information games and demonstrates stronger adversarial performance in a large poker game.

</details>


### [90] [Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics](https://arxiv.org/abs/2511.08185)
*Tai Hoang,Alessandro Trenta,Alessio Gravina,Niklas Freymuth,Philipp Becker,Davide Bacciu,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出了信息保持图神经网络模拟器（IGNS），基于哈密顿动力学原理，能够更好地捕捉长程相互作用并减少自回归展开中的误差积累，在复杂物理系统模拟中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，现有图神经网络模拟器在捕捉长程相互作用和自回归展开时存在误差积累问题，需要改进。

Method: 基于哈密顿动力学原理构建图神经网络模拟器，引入预热阶段初始化全局上下文，使用几何编码处理不规则网格，采用多步训练目标减少展开误差。

Result: 在所有任务中，IGNS始终优于最先进的图神经网络模拟器，在挑战性和复杂动力系统中实现了更高的准确性和稳定性。

Conclusion: IGNS通过信息保持机制和哈密顿动力学框架，有效解决了现有图神经网络模拟器的局限性，为复杂物理系统模拟提供了更准确和稳定的解决方案。

Abstract: Learning to simulate complex physical systems from data has emerged as a promising way to overcome the limitations of traditional numerical solvers, which often require prohibitive computational costs for high-fidelity solutions. Recent Graph Neural Simulators (GNSs) accelerate simulations by learning dynamics on graph-structured data, yet often struggle to capture long-range interactions and suffer from error accumulation under autoregressive rollouts. To address these challenges, we propose Information-preserving Graph Neural Simulators (IGNS), a graph-based neural simulator built on the principles of Hamiltonian dynamics. This structure guarantees preservation of information across the graph, while extending to port-Hamiltonian systems allows the model to capture a broader class of dynamics, including non-conservative effects. IGNS further incorporates a warmup phase to initialize global context, geometric encoding to handle irregular meshes, and a multi-step training objective to reduce rollout error. To evaluate these properties systematically, we introduce new benchmarks that target long-range dependencies and challenging external forcing scenarios. Across all tasks, IGNS consistently outperforms state-of-the-art GNSs, achieving higher accuracy and stability under challenging and complex dynamical systems.

</details>


### [91] [The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression](https://arxiv.org/abs/2511.08226)
*Raphaël Bayle,Martial Mermillod,Robert M. French*

Main category: cs.LG

TL;DR: 本文提出OPRE在线数据集压缩算法，通过最小化对数据的先验假设，在CIFAR-10和CIFAR-100上实现了优于现有在线持续学习方法的性能，并认为在线数据集压缩是实现完全无偏持续学习的关键。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法大多引入对数据的先验信息（如预训练特征提取器），无法被认为是无偏的，这限制了方法的泛化能力。需要开发更少假设的持续学习方法。

Method: 提出OPRE（Online Patch Redundancy Eliminator）在线数据集压缩算法，结合测试时的分类器训练，仅需对数据做出最小且可解释的假设。

Result: 在CIFAR-10和CIFAR-100数据集上，OPRE的性能优于多个最先进的在线持续学习方法。

Conclusion: 在线数据集压缩可能是实现完全无偏持续学习的必要条件，OPRE方法展示了在最小化先验假设下仍能取得优异性能的可能性。

Abstract: In order to achieve Continual Learning (CL), the problem of catastrophic forgetting, one that has plagued neural networks since their inception, must be overcome. The evaluation of continual learning methods relies on splitting a known homogeneous dataset and learning the associated tasks one after the other. We argue that most CL methods introduce a priori information about the data to come and cannot be considered agnostic. We exemplify this point with the case of methods relying on pretrained feature extractors, which are still used in CL. After showing that pretrained feature extractors imply a loss of generality with respect to the data that can be learned by the model, we then discuss other kinds of a priori information introduced in other CL methods. We then present the Online Patch Redundancy Eliminator (OPRE), an online dataset compression algorithm, which, along with the training of a classifier at test time, yields performance on CIFAR-10 and CIFAR-100 superior to a number of other state-of-the-art online continual learning methods. Additionally, OPRE requires only minimal and interpretable hypothesis on the data to come. We suggest that online dataset compression could well be necessary to achieve fully agnostic CL.

</details>


### [92] [Towards Non-Stationary Time Series Forecasting with Temporal Stabilization and Frequency Differencing](https://arxiv.org/abs/2511.08229)
*Junkai Lu,Peng Chen,Chenjuan Guo,Yang Shu,Meng Wang,Bin Yang*

Main category: cs.LG

TL;DR: 提出了DTAF双分支框架，通过时间稳定融合和频率波建模分别处理时间和频域的非平稳性，在非平稳条件下显著提升长期时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列常表现出非平稳性，包括时间分布偏移和频谱变异性，这对长期时间序列预测构成重大挑战。

Method: DTAF采用双分支框架：时间稳定融合模块使用非平稳专家混合滤波器解耦和抑制时间非平稳模式；频率波建模模块应用频率差分动态突出显著频谱偏移的组件。

Result: 在真实世界基准测试上的广泛实验表明，DTAF优于最先进的基线方法，在非平稳条件下显著提高了预测精度。

Conclusion: DTAF通过融合时间和频域的互补输出，生成能够适应时间和频域非平稳性的稳健预测。

Abstract: Time series forecasting is critical for decision-making across dynamic domains such as energy, finance, transportation, and cloud computing. However, real-world time series often exhibit non-stationarity, including temporal distribution shifts and spectral variability, which pose significant challenges for long-term time series forecasting. In this paper, we propose DTAF, a dual-branch framework that addresses non-stationarity in both the temporal and frequency domains. For the temporal domain, the Temporal Stabilizing Fusion (TFS) module employs a non-stationary mix of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while preserving long-term dependencies. For the frequency domain, the Frequency Wave Modeling (FWM) module applies frequency differencing to dynamically highlight components with significant spectral shifts. By fusing the complementary outputs of TFS and FWM, DTAF generates robust forecasts that adapt to both temporal and frequency domain non-stationarity. Extensive experiments on real-world benchmarks demonstrate that DTAF outperforms state-of-the-art baselines, yielding significant improvements in forecasting accuracy under non-stationary conditions. All codes are available at https://github.com/PandaJunk/DTAF.

</details>


### [93] [PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore](https://arxiv.org/abs/2511.08241)
*Zhihao Lin,Lin Wu,Zhen Tian,Jianglin Lan*

Main category: cs.LG

TL;DR: PrefPoE是一个基于偏好-专家乘积框架的强化学习探索方法，通过优势引导的智能探索解决传统熵最大化方法的高方差问题，在多个控制任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中的探索策略（如熵最大化）存在高方差和策略更新效率低的问题，需要一种更智能的探索方法来平衡探索与利用。

Method: 提出PrefPoE框架，训练偏好网络聚焦高优势动作，通过专家乘积（PoE）与主策略融合，创建软信任区域来稳定策略更新并保持定向探索。

Result: 在连续和离散动作空间的多种控制任务中表现优异：HalfCheetah-v4提升321%，Ant-v4提升69%，LunarLander-v2提升276%，训练稳定性和样本效率均有提升。

Conclusion: 通过优势引导的偏好学习"在哪里探索"与学习"如何行动"同等重要，为增强策略梯度方法提供了一个通用框架。

Abstract: Exploration in reinforcement learning remains a critical challenge, as naive entropy maximization often results in high variance and inefficient policy updates. We introduce \textbf{PrefPoE}, a novel \textit{Preference-Product-of-Experts} framework that performs intelligent, advantage-guided exploration via the first principled application of product-of-experts (PoE) fusion for single-task exploration-exploitation balancing. By training a preference network to concentrate probability mass on high-advantage actions and fusing it with the main policy through PoE, PrefPoE creates a \textbf{soft trust region} that stabilizes policy updates while maintaining targeted exploration. Across diverse control tasks spanning both continuous and discrete action spaces, PrefPoE demonstrates consistent improvements: +321\% on HalfCheetah-v4 (1276~$\rightarrow$~5375), +69\% on Ant-v4, +276\% on LunarLander-v2, with consistently enhanced training stability and sample efficiency. Unlike standard PPO, which suffers from entropy collapse, PrefPoE sustains adaptive exploration through its unique dynamics, thereby preventing premature convergence and enabling superior performance. Our results establish that learning \textit{where to explore} through advantage-guided preferences is as crucial as learning how to act, offering a general framework for enhancing policy gradient methods across the full spectrum of reinforcement learning domains. Code and pretrained models are available in supplementary materials.

</details>


### [94] [A Unified Geometric Field Theory Framework for Transformers: From Manifold Embeddings to Kernel Modulation](https://arxiv.org/abs/2511.08243)
*Xianshuai Shi,Jianfeng Zhu,Leibo Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，将Transformer中的位置编码、核积分算子和注意力机制整合起来进行理论研究。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然在各领域取得了巨大成功，但其核心组件（位置编码和注意力机制）缺乏统一的物理或数学解释。

Method: 将离散位置映射到连续流形上的空间函数，将Transformer层解释为在嵌入流形上作用的核调制算子。

Result: 建立了一个结构化的理论框架，为Transformer提供了场论解释。

Conclusion: 该框架为Transformer的核心组件提供了统一的数学和物理解释，有助于深入理解其工作机制。

Abstract: The Transformer architecture has achieved tremendous success in natural language processing, computer vision, and scientific computing through its self-attention mechanism. However, its core components-positional encoding and attention mechanisms-have lacked a unified physical or mathematical interpretation. This paper proposes a structural theoretical framework that integrates positional encoding, kernel integral operators, and attention mechanisms for in-depth theoretical investigation. We map discrete positions (such as text token indices and image pixel coordinates) to spatial functions on continuous manifolds, enabling a field-theoretic interpretation of Transformer layers as kernel-modulated operators acting over embedded manifolds.

</details>


### [95] [Data-Driven Discovery of Feature Groups in Clinical Time Series](https://arxiv.org/abs/2511.08260)
*Fedor Sergeev,Manuel Burger,Polina Leshetkina,Vincent Fortuin,Gunnar Rätsch,Rita Kuznetsova*

Main category: cs.LG

TL;DR: 提出一种通过聚类特征嵌入层权重来自动学习特征分组的新方法，该方法可无缝集成到标准监督训练中，并在临床相关任务中提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 临床时间序列数据通常包含数百个异构特征，基于相似性和预测任务相关性对特征进行分组已被证明能提升深度学习架构性能，但仅凭语义知识先验定义这些分组对领域专家来说具有挑战性。

Method: 通过聚类特征嵌入层权重来自动学习特征分组，该方法可无缝集成到标准监督训练流程中。

Result: 在合成数据上优于静态聚类方法，在真实医疗数据上达到与专家定义分组相当的性能，且学习到的特征组具有临床可解释性。

Conclusion: 该方法能够数据驱动地发现任务相关的变量关系，为临床时间序列分析提供了一种有效的特征分组学习方案。

Abstract: Clinical time series data are critical for patient monitoring and predictive modeling. These time series are typically multivariate and often comprise hundreds of heterogeneous features from different data sources. The grouping of features based on similarity and relevance to the prediction task has been shown to enhance the performance of deep learning architectures. However, defining these groups a priori using only semantic knowledge is challenging, even for domain experts. To address this, we propose a novel method that learns feature groups by clustering weights of feature-wise embedding layers. This approach seamlessly integrates into standard supervised training and discovers the groups that directly improve downstream performance on clinically relevant tasks. We demonstrate that our method outperforms static clustering approaches on synthetic data and achieves performance comparable to expert-defined groups on real-world medical data. Moreover, the learned feature groups are clinically interpretable, enabling data-driven discovery of task-relevant relationships between variables.

</details>


### [96] [Rethinking Explanation Evaluation under the Retraining Scheme](https://arxiv.org/abs/2511.08281)
*Yi Cai,Thibaud Ardoin,Mayank Gulati,Gerhard Wunder*

Main category: cs.LG

TL;DR: 本文分析了基于重训练的归因评估方法ROAR中理论与实证结果不一致的问题，识别出符号问题是导致评估失真的关键因素，并提出改进方案来提升解释器评估的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于推理的归因评估方法存在分布偏移问题，而基于重训练的ROAR方法虽然解决了这个问题，但其评估结果与理论预期存在矛盾，需要深入分析这种不一致的原因。

Method: 识别了ROAR评估中的符号问题，提出通过重新构建评估过程来解决该问题，并基于现有框架设计了新的变体方法，以提高评估效率。

Result: 改进后的评估方案有效解决了符号问题，显著提升了评估效率，并在多个数据规模上对精选解释器进行了深入性能分析。

Conclusion: 研究揭示了归因评估中的开放挑战，提出的改进方案为解释器选择和基准测试提供了更实用的评估框架，推动了可解释性研究的未来发展。

Abstract: Feature attribution has gained prominence as a tool for explaining model decisions, yet evaluating explanation quality remains challenging due to the absence of ground-truth explanations. To circumvent this, explanation-guided input manipulation has emerged as an indirect evaluation strategy, measuring explanation effectiveness through the impact of input modifications on model outcomes during inference. Despite the widespread use, a major concern with inference-based schemes is the distribution shift caused by such manipulations, which undermines the reliability of their assessments. The retraining-based scheme ROAR overcomes this issue by adapting the model to the altered data distribution. However, its evaluation results often contradict the theoretical foundations of widely accepted explainers. This work investigates this misalignment between empirical observations and theoretical expectations. In particular, we identify the sign issue as a key factor responsible for residual information that ultimately distorts retraining-based evaluation. Based on the analysis, we show that a straightforward reframing of the evaluation process can effectively resolve the identified issue. Building on the existing framework, we further propose novel variants that jointly structure a comprehensive perspective on explanation evaluation. These variants largely improve evaluation efficiency over the standard retraining protocol, thereby enhancing practical applicability for explainer selection and benchmarking. Following our proposed schemes, empirical results across various data scales provide deeper insights into the performance of carefully selected explainers, revealing open challenges and future directions in explainability research.

</details>


### [97] [Dual-Kernel Graph Community Contrastive Learning](https://arxiv.org/abs/2511.08287)
*Xiang Chen,Kun Yue,Wenjie Liu,Zhenyu Zhang,Liang Duan*

Main category: cs.LG

TL;DR: 提出了一种高效的图对比学习框架，通过将输入图转换为紧凑的节点集网络来降低计算复杂度，并结合知识蒸馏技术加速推理。


<details>
  <summary>Details</summary>
Motivation: 解决图对比学习在大规模图上的可扩展性问题，包括GNN消息传递机制的计算密集性和对比损失对正负节点对的二次计算复杂度。

Method: 1. 将输入图转换为紧凑的节点集网络，保留跨社区的结构信息；2. 引入线性复杂度的核化图社区对比损失；3. 在解耦的GNN架构中融入知识蒸馏技术。

Result: 在16个不同规模的真实世界数据集上的实验表明，该方法在效果和可扩展性方面均优于最先进的图对比学习基线方法。

Conclusion: 所提出的高效图对比学习框架成功解决了大规模图上的可扩展性问题，在保持强泛化性能的同时显著提升了训练和推理效率。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful paradigm for training Graph Neural Networks (GNNs) in the absence of task-specific labels. However, its scalability on large-scale graphs is hindered by the intensive message passing mechanism of GNN and the quadratic computational complexity of contrastive loss over positive and negative node pairs. To address these issues, we propose an efficient GCL framework that transforms the input graph into a compact network of interconnected node sets while preserving structural information across communities. We firstly introduce a kernelized graph community contrastive loss with linear complexity, enabling effective information transfer among node sets to capture hierarchical structural information of the graph. We then incorporate a knowledge distillation technique into the decoupled GNN architecture to accelerate inference while maintaining strong generalization performance. Extensive experiments on sixteen real-world datasets of varying scales demonstrate that our method outperforms state-of-the-art GCL baselines in both effectiveness and scalability.

</details>


### [98] [Test-time Diverse Reasoning by Riemannian Activation Steering](https://arxiv.org/abs/2511.08305)
*Ly Tran Ho Khanh,Dongxuan Zhu,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出了一种基于黎曼优化的测试时激活引导策略，通过最大化推理路径的多样性来解决Best-of-N推理中的输出多样性限制问题。


<details>
  <summary>Details</summary>
Motivation: Best-of-N推理在解决复杂任务时，由于模型生成相似输出而重复相同错误，存在输出多样性限制的瓶颈。

Method: 在测试时同步锚点处，通过黎曼优化在球面乘积上求解对数行列式目标函数，找到最大化干预激活子集总体积的引导向量，并使用黎曼块坐标下降算法获得问题的驻点。

Result: 在流行的数学基准测试中，该策略在生成多样性和解决方案准确性方面优于普通采样技术。

Conclusion: 测试时黎曼激活引导策略能有效提升推理路径的多样性，从而提高Best-of-N推理的性能。

Abstract: Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.

</details>


### [99] [Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework](https://arxiv.org/abs/2511.08314)
*Xiaoyu Fan,Lin Guo,Ruizhen Jia,Yang Tian,Zhihao Yang,Boxue Tian*

Main category: cs.LG

TL;DR: MolRuleLoss是一个基于子结构替换规则的框架，通过将SSR的偏导数约束整合到分子性质回归模型的损失函数中，显著提升了多种分子性质预测任务的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助药物发现中的分子性质预测模型在回归任务中准确性较差，特别是对于分布外分子表现灾难性差，需要提高模型的准确性和泛化能力。

Method: 提出MolRuleLoss框架，将子结构替换规则的偏导数约束整合到分子性质回归模型的损失函数中，利用SSR的数量和质量来提升预测性能。

Result: 在脂溶性、水溶性和溶剂化自由能预测任务中，RMSE分别从0.660降至0.587、0.798降至0.777、1.877降至1.252，性能提升2.6-33.3%。对于分布外分子，分子量预测的RMSE从29.507降至0.007。

Conclusion: MolRuleLoss作为附加组件显著提升了多种分子性质回归模型的预测准确性和泛化能力，支持化学信息学和AI辅助药物发现等领域的应用。

Abstract: Artificial Intelligence (AI)-aided drug discovery is an active research field, yet AI models often exhibit poor accuracy in regression tasks for molecular property prediction, and perform catastrophically poorly for out-of-distribution (OOD) molecules. Here, we present MolRuleLoss, a substructure-substitution-rule-informed framework that improves the accuracy and generalizability of multiple molecular property regression models (MPRMs) such as GEM and UniMol for diverse molecular property prediction tasks. MolRuleLoss incorporates partial derivative constraints for substructure substitution rules (SSRs) into an MPRM's loss function. When using GEM models for predicting lipophilicity, water solubility, and solvation-free energy (using lipophilicity, ESOL, and freeSolv datasets from MoleculeNet), the root mean squared error (RMSE) values with and without MolRuleLoss were 0.587 vs. 0.660, 0.777 vs. 0.798, and 1.252 vs. 1.877, respectively, representing 2.6-33.3% performance improvements. We show that both the number and the quality of SSRs contribute to the magnitude of prediction accuracy gains obtained upon adding MolRuleLoss to an MPRM. MolRuleLoss improved the generalizability of MPRMs for "activity cliff" molecules in a lipophilicity prediction task and improved the generalizability of MPRMs for OOD molecules in a melting point prediction task. In a molecular weight prediction task for OOD molecules, MolRuleLoss reduced the RMSE value of a GEM model from 29.507 to 0.007. We also provide a formal demonstration that the upper bound of the variation for property change of SSRs is positively correlated with an MPRM's error. Together, we show that using the MolRuleLoss framework as a bolt-on boosts the prediction accuracy and generalizability of multiple MPRMs, supporting diverse applications in areas like cheminformatics and AI-aided drug discovery.

</details>


### [100] [Adversarial Bias: Data Poisoning Attacks on Fairness](https://arxiv.org/abs/2511.08331)
*Eunice Chan,Hanghang Tong*

Main category: cs.LG

TL;DR: 本文提出了一种针对朴素贝叶斯分类器的对抗性投毒攻击方法，通过向训练集注入少量精心设计的对抗数据点，能够有效破坏模型公平性，同时保持整体性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在现实应用中的广泛部署，确保其公平性变得至关重要。现有研究主要关注评估和改进机器学习系统的公平性，而关于公平性脆弱性（即AI系统公平性如何被故意破坏）的研究相对较少。

Method: 采用理论分析证明简单的对抗性投毒策略足以在朴素贝叶斯分类器中诱导最大程度的不公平行为。核心思想是战略性地向训练集注入少量精心设计的对抗数据点，偏置模型的决策边界以不成比例地影响受保护群体，同时保持泛化性能。

Result: 在多个基准数据集和模型上的实验表明，该方法在降低公平性指标方面显著优于现有方法，通常以相当或仅略微更差的准确性影响实现显著更高的不公平水平。该方法对多种模型都有效，而先前的工作则不具备这种广泛适用性。

Conclusion: 该方法展示了一种强大而有效的破坏机器学习系统公平性的方法，揭示了AI系统在公平性方面的潜在脆弱性，为未来开发更鲁棒的公平机器学习系统提供了重要启示。

Abstract: With the growing adoption of AI and machine learning systems in real-world applications, ensuring their fairness has become increasingly critical. The majority of the work in algorithmic fairness focus on assessing and improving the fairness of machine learning systems. There is relatively little research on fairness vulnerability, i.e., how an AI system's fairness can be intentionally compromised. In this work, we first provide a theoretical analysis demonstrating that a simple adversarial poisoning strategy is sufficient to induce maximally unfair behavior in naive Bayes classifiers. Our key idea is to strategically inject a small fraction of carefully crafted adversarial data points into the training set, biasing the model's decision boundary to disproportionately affect a protected group while preserving generalizable performance. To illustrate the practical effectiveness of our method, we conduct experiments across several benchmark datasets and models. We find that our attack significantly outperforms existing methods in degrading fairness metrics across multiple models and datasets, often achieving substantially higher levels of unfairness with a comparable or only slightly worse impact on accuracy. Notably, our method proves effective on a wide range of models, in contrast to prior work, demonstrating a robust and potent approach to compromising the fairness of machine learning systems.

</details>


### [101] [LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration](https://arxiv.org/abs/2511.08339)
*Ruiyu Qiu,Rui Wang,Guanghui Yang,Xiang Li,Zhijiang Shao*

Main category: cs.LG

TL;DR: 提出了LPPG-RL框架，通过序列梯度投影解决词典序多目标强化学习问题，在连续空间中兼容所有策略梯度算法，并引入子问题探索来防止梯度消失。


<details>
  <summary>Details</summary>
Motivation: 传统安全RL和多目标RL方法难以有效强制执行优先级排序，现有LMORL方法要么依赖启发式阈值调整，要么仅限于离散域。

Method: 使用序列梯度投影识别可行策略更新方向，将投影步骤重新表述为优化问题，利用Dykstra投影而非通用求解器加速计算，并引入子问题探索机制。

Result: 在2D导航环境中实验表明，LPPG-RL优于现有最先进的连续LMORL方法。

Conclusion: LPPG-RL是一个有效的LMORL框架，具有理论收敛保证和政策改进下界，在连续空间中表现出色。

Abstract: Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.

</details>


### [102] [HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting](https://arxiv.org/abs/2511.08340)
*Andrey Savchenko,Oleg Kachan*

Main category: cs.LG

TL;DR: 提出HN-MVTS架构，通过超网络生成目标预测网络最后一层的权重，作为数据自适应正则化器，提升多变量时间序列预测的泛化能力和长程预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列预测中复杂通道依赖模型性能下降的问题，同时保持通道独立模型的高鲁棒性优势。

Method: 将基于超网络的生成先验与任意神经网络预测模型集成，超网络输入为时间序列组件的可学习嵌入矩阵，仅生成目标网络最后一层权重作为正则化器。

Result: 在8个基准数据集上的广泛实验表明，将HN-MVTS应用于最先进模型（DLinear、PatchTST、TSMixer等）通常能提升其性能。

Conclusion: 超网络驱动的参数化为在复杂场景中增强现有预测技术提供了有前景的方向。

Abstract: Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.

</details>


### [103] [From Confusion to Clarity: ProtoScore - A Framework for Evaluating Prototype-Based XAI](https://arxiv.org/abs/2511.08361)
*Helena Monke,Benjamin Sae-Chew,Benjamin Fresz,Marco F. Huber*

Main category: cs.LG

TL;DR: ProtoScore框架为基于原型的可解释AI方法提供标准化基准评估，特别关注时间序列数据，通过整合Co-12属性实现公平比较。


<details>
  <summary>Details</summary>
Motivation: 神经网络在医疗、金融等高风险领域的复杂性和不透明性需要可解释AI方法，但缺乏标准化基准来客观比较基于原型的XAI方法，特别是针对时间序列数据。

Method: 开发ProtoScore评估框架，整合Nauta等人的Co-12属性，为基于原型的XAI方法建立标准化评估标准。

Result: ProtoScore框架能够有效比较不同原型方法之间以及与其他XAI方法的性能，帮助从业者选择合适的解释方法。

Conclusion: ProtoScore为基于原型的XAI方法提供了可靠的评估框架，促进了该领域的发展，减少了用户研究的成本。

Abstract: The complexity and opacity of neural networks (NNs) pose significant challenges, particularly in high-stakes fields such as healthcare, finance, and law, where understanding decision-making processes is crucial. To address these issues, the field of explainable artificial intelligence (XAI) has developed various methods aimed at clarifying AI decision-making, thereby facilitating appropriate trust and validating the fairness of outcomes. Among these methods, prototype-based explanations offer a promising approach that uses representative examples to elucidate model behavior. However, a critical gap exists regarding standardized benchmarks to objectively compare prototype-based XAI methods, especially in the context of time series data. This lack of reliable benchmarks results in subjective evaluations, hindering progress in the field. We aim to establish a robust framework, ProtoScore, for assessing prototype-based XAI methods across different data types with a focus on time series data, facilitating fair and comprehensive evaluations. By integrating the Co-12 properties of Nauta et al., this framework allows for effectively comparing prototype methods against each other and against other XAI methods, ultimately assisting practitioners in selecting appropriate explanation methods while minimizing the costs associated with user studies. All code is publicly available at https://github.com/HelenaM23/ProtoScore .

</details>


### [104] [Multi-objective Hyperparameter Optimization in the Age of Deep Learning](https://arxiv.org/abs/2511.08371)
*Soham Basu,Frank Hutter,Danny Stoll*

Main category: cs.LG

TL;DR: PriMO是首个能够整合多目标用户信念的超参数优化算法，在多个深度学习基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 深度学习专家通常有关于哪些超参数设置能获得良好性能的先验知识，但现有超参数优化算法很少能利用这种知识，且没有算法能整合多目标先验知识

Method: 提出PriMO算法，能够整合多目标用户信念

Result: 在8个深度学习基准测试中，PriMO在多目标和单目标设置下都达到了最先进性能

Conclusion: PriMO已成为深度学习从业者的新首选超参数优化算法

Abstract: While Deep Learning (DL) experts often have prior knowledge about which hyperparameter settings yield strong performance, only few Hyperparameter Optimization (HPO) algorithms can leverage such prior knowledge and none incorporate priors over multiple objectives. As DL practitioners often need to optimize not just one but many objectives, this is a blind spot in the algorithmic landscape of HPO. To address this shortcoming, we introduce PriMO, the first HPO algorithm that can integrate multi-objective user beliefs. We show PriMO achieves state-of-the-art performance across 8 DL benchmarks in the multi-objective and single-objective setting, clearly positioning itself as the new go-to HPO algorithm for DL practitioners.

</details>


### [105] [EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting](https://arxiv.org/abs/2511.08396)
*Zhiwei Zhang,Xinyi Du,Xuanchi Guo,Weihao Wang,Wenjuan Han*

Main category: cs.LG

TL;DR: EMAformer通过引入三个关键归纳偏置（全局稳定性、相位敏感性和跨轴特异性）来增强Transformer架构，在多元时间序列预测任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer架构在多元时间序列预测中仍落后于最新的MLP模型，作者认为这主要是由于不稳定的通道间关系导致的。

Method: 提出EMAformer模型，通过辅助嵌入套件增强Transformer，引入全局稳定性、相位敏感性和跨轴特异性三个关键归纳偏置。

Result: 在12个真实世界基准测试中达到最先进性能，平均将预测误差降低了2.73%（MSE）和5.15%（MAE）。

Conclusion: EMAformer显著提升了基于Transformer的方法在多元时间序列预测中的实际适用性。

Abstract: Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on https://github.com/PlanckChang/EMAformer.

</details>


### [106] [Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment](https://arxiv.org/abs/2511.08399)
*Hua Ye,Hang Ding,Siyuan Chen,Yiyang Jiang,Changyuan Zhang,Xuan Zhang*

Main category: cs.LG

TL;DR: BACL是一种轻量级多模态模型增强方法，通过边界感知课程学习和局部注意力对比损失，有效处理模糊负样本，在多个大规模基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型对所有负样本一视同仁，忽略了与正样本仅差微小细节的模糊负样本，这些边界情况蕴含着重要的学习信号。

Method: 提出边界感知课程学习，包含边界感知负采样器和对比局部注意力损失两个模块，前者逐步提升难度，后者突出不匹配位置，两者完全可微分且兼容现有双编码器。

Result: 理论预测误差率为O(1/n)，实践中在四个大规模基准测试上相比CLIP提升高达32%的R@1，达到新的SOTA水平，且无需额外标注。

Conclusion: BACL通过有效利用模糊负样本作为课程信号，显著提升了多模态模型的性能，证明了边界案例在模型训练中的重要性。

Abstract: Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.

</details>


### [107] [ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games](https://arxiv.org/abs/2511.08412)
*Ruochuan Shi,Runyu Lu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: 提出了ARAC算法，结合注意力图神经网络和自适应发散正则化机制，解决图结构多智能体强化学习中的稀疏奖励问题，在追捕和对抗任务中实现更快收敛和更高成功率。


<details>
  <summary>Details</summary>
Motivation: 在图结构多智能体对抗任务中，智能体需要在高度动态的交互中协调，稀疏奖励阻碍了策略学习效率。

Method: 使用注意力图神经网络建模智能体依赖关系，结合自适应发散正则化机制，早期利用参考策略进行高效探索，后期减少依赖以避免继承其局限性。

Result: 在追捕和对抗场景中，ARAC相比基线方法实现了更快收敛、更高最终成功率，并在不同智能体数量下展现出更强的可扩展性。

Conclusion: ARAC在复杂图结构环境中有效解决了稀疏奖励问题，提升了多智能体强化学习的性能。

Abstract: In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.

</details>


### [108] [NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization](https://arxiv.org/abs/2511.08417)
*Xiyuan Wei,Chih-Jen Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: NeuCLIP提出了一种新的对比学习优化框架，通过凸分析和变分分析将对比损失重新表述为最小化问题，使用辅助神经网络预测对数归一化因子，解决了传统方法在大数据集或小批量情况下的优化误差问题。


<details>
  <summary>Details</summary>
Motivation: 传统CLIP训练方法依赖大批次来估计归一化项，计算资源需求大。现有方法使用逐样本归一化估计器，但在大数据集或小批量时会产生与数据集大小/批次大小比例相关的优化误差。

Method: 通过凸分析将每个样本的对比损失重新表述为带辅助变量的最小化问题，然后通过变分分析将其转换为紧凑神经网络预测对数归一化因子的最小化问题。设计了交替优化算法联合训练CLIP模型和辅助网络。

Result: 在从百万到数十亿样本的大规模CLIP训练实验中，NeuCLIP相比先前方法实现了更准确的归一化因子估计，获得了更好的性能表现。

Conclusion: NeuCLIP通过新颖的优化框架解决了CLIP训练中的归一化项估计问题，在大规模数据集上表现出优越性能。

Abstract: Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, which are updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches. To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) $\textbf{reformulating}$ the contrastive loss for each sample $\textbf{via convex analysis}$ into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) $\textbf{transforming}$ the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via $\textbf{variational analysis}$ into the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.

</details>


### [109] [Physics-Informed Neural Operators for Cardiac Electrophysiology](https://arxiv.org/abs/2511.08418)
*Hannah Lydon,Milad Kazemi,Martin Bishop,Nicola Paoletti*

Main category: cs.LG

TL;DR: 提出了物理信息神经算子（PINO）方法来解决心脏电生理学中的PDE问题，相比传统数值求解器和PINNs，PINO能够泛化到多种网格分辨率和初始条件，实现长期稳定预测和零样本评估。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高且对离散化敏感，深度学习方法需要大量数据且难以处理混沌动力学和长期预测，PINNs受限于网格分辨率和长期预测稳定性。

Method: 使用物理信息神经算子（PINO）方法，学习函数空间之间的映射关系，而不是像PINNs那样依赖特定网格分辨率。

Result: PINO模型能够准确再现心脏电生理动力学，支持长期滚动预测，可将预测分辨率扩展到训练分辨率的10倍，且计算时间显著少于数值PDE求解器。

Conclusion: PINO方法为高效、可扩展的心脏电生理模拟提供了有前景的解决方案，具有长期预测稳定性和分辨率可扩展性优势。

Abstract: Accurately simulating systems governed by PDEs, such as voltage fields in cardiac electrophysiology (EP) modelling, remains a significant modelling challenge. Traditional numerical solvers are computationally expensive and sensitive to discretisation, while canonical deep learning methods are data-hungry and struggle with chaotic dynamics and long-term predictions. Physics-Informed Neural Networks (PINNs) mitigate some of these issues by incorporating physical constraints in the learning process, yet they remain limited by mesh resolution and long-term predictive stability. In this work, we propose a Physics-Informed Neural Operator (PINO) approach to solve PDE problems in cardiac EP. Unlike PINNs, PINO models learn mappings between function spaces, allowing them to generalise to multiple mesh resolutions and initial conditions. Our results show that PINO models can accurately reproduce cardiac EP dynamics over extended time horizons and across multiple propagation scenarios, including zero-shot evaluations on scenarios unseen during training. Additionally, our PINO models maintain high predictive quality in long roll-outs (where predictions are recursively fed back as inputs), and can scale their predictive resolution by up to 10x the training resolution. These advantages come with a significant reduction in simulation time compared to numerical PDE solvers, highlighting the potential of PINO-based approaches for efficient and scalable cardiac EP simulations.

</details>


### [110] [HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization](https://arxiv.org/abs/2511.08425)
*Zeyang Li,Kaveh Alim,Navid Azizan*

Main category: cs.LG

TL;DR: 提出了HardFlow框架，将硬约束采样重新表述为轨迹优化问题，利用数值最优控制在终端时间精确满足约束，同时保持样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于投影的方法在整个采样路径上施加约束过于严格，会降低样本质量，而许多下游应用需要在生成样本时强制执行硬约束。

Method: 利用流匹配模型的基础结构，采用模型预测控制技术，将复杂约束优化问题转化为可高效求解的替代问题。

Result: 在机器人规划、偏微分方程边界控制和文本引导图像编辑等多个领域的实验表明，HardFlow在约束满足和样本质量方面显著优于现有方法。

Conclusion: 该轨迹优化框架不仅能够精确满足约束，还能通过包含积分成本来最小化分布偏移，并通过终端目标进一步提高样本质量，提供了统一的解决方案。

Abstract: Diffusion and flow-matching have emerged as powerful methodologies for generative modeling, with remarkable success in capturing complex data distributions and enabling flexible guidance at inference time. Many downstream applications, however, demand enforcing hard constraints on generated samples (for example, robot trajectories must avoid obstacles), a requirement that goes beyond simple guidance. Prevailing projection-based approaches constrain the entire sampling path to the constraint manifold, which is overly restrictive and degrades sample quality. In this paper, we introduce a novel framework that reformulates hard-constrained sampling as a trajectory optimization problem. Our key insight is to leverage numerical optimal control to steer the sampling trajectory so that constraints are satisfied precisely at the terminal time. By exploiting the underlying structure of flow-matching models and adopting techniques from model predictive control, we transform this otherwise complex constrained optimization problem into a tractable surrogate that can be solved efficiently and effectively. Furthermore, this trajectory optimization perspective offers significant flexibility beyond mere constraint satisfaction, allowing for the inclusion of integral costs to minimize distribution shift and terminal objectives to further enhance sample quality, all within a unified framework. We provide a control-theoretic analysis of our method, establishing bounds on the approximation error between our tractable surrogate and the ideal formulation. Extensive experiments across diverse domains, including robotics (planning), partial differential equations (boundary control), and vision (text-guided image editing), demonstrate that our algorithm, which we name $\textit{HardFlow}$, substantially outperforms existing methods in both constraint satisfaction and sample quality.

</details>


### [111] [An update to PYRO-NN: A Python Library for Differentiable CT Operators](https://arxiv.org/abs/2511.08427)
*Linda-Sophie Schneider,Yipeng Sun,Chengze Ye,Markus Michen,Andreas Maier*

Main category: cs.LG

TL;DR: PYRO-NN是一个基于Python的可微分CT重建库的更新版本，支持PyTorch和CUDA内核，提供高效的投影和反投影操作，以及模拟成像伪影和灵活端到端训练管道的工具。


<details>
  <summary>Details</summary>
Motivation: 深度学习在CT重建中取得了显著进展，但需要将经典重建技术与数据驱动方法相结合。可微分算子在端到端优化和物理建模整合中发挥关键作用。

Method: 更新PYRO-NN库，扩展对PyTorch的兼容性，引入原生CUDA内核支持，支持平行、扇形和锥形束几何的高效投影和反投影操作，提供模拟成像伪影和任意采集轨迹建模的工具。

Result: 开发了一个具有高效CUDA支持的灵活端到端可训练CT重建框架，通过高级Python API实现。

Conclusion: PYRO-NN的更新版本为CT重建提供了强大的可微分操作工具，促进了深度学习与经典重建方法的有效整合。

Abstract: Deep learning has brought significant advancements to X-ray Computed Tomography (CT) reconstruction, offering solutions to challenges arising from modern imaging technologies. These developments benefit from methods that combine classical reconstruction techniques with data-driven approaches. Differentiable operators play a key role in this integration by enabling end-to-end optimization and the incorporation of physical modeling within neural networks.
  In this work, we present an updated version of PYRO-NN, a Python-based library for differentiable CT reconstruction. The updated framework extends compatibility to PyTorch and introduces native CUDA kernel support for efficient projection and back-projection operations across parallel, fan, and cone-beam geometries. Additionally, it includes tools for simulating imaging artifacts, modeling arbitrary acquisition trajectories, and creating flexible, end-to-end trainable pipelines through a high-level Python API. Code is available at: https://github.com/csyben/PYRO-NN

</details>


### [112] [Coherence Mechanisms for Provable Self-Improvement](https://arxiv.org/abs/2511.08440)
*Mehryar Mohri,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 提出基于一致性的自改进理论框架，通过投影机制确保模型在任务保持变换下输出一致，并提供单调改进的理论保证


<details>
  <summary>Details</summary>
Motivation: 现有自改进方法主要依赖经验启发式，缺乏形式化保证，需要建立有理论保证的自改进框架

Method: 使用投影机制更新基线模型使其满足一致性约束，同时保持与原始行为接近，包括直接投影和两步投影方法

Result: 理论证明这些机制能实现单调改进（Bregman散度减少），并扩展到非可实现设置、有限样本和松弛一致性约束

Conclusion: 一致性是自改进的基本原理，任何具有类似可证明改进保证的机制都必须符合基于一致性的结构

Abstract: Self-improvement is a critical capability for large language models and other intelligent systems, enabling them to refine their behavior and internal consistency without external supervision. Despite its importance, prior approaches largely rely on empirical heuristics and lack formal guarantees. In this paper, we propose a principled framework for self-improvement based on the concept of \emph{coherence}, which requires that a model's outputs remain consistent under task-preserving transformations of the input.
  We formalize this concept using projection-based mechanisms that update a baseline model to be coherent while remaining as close as possible to its original behavior. We provide rigorous theoretical guarantees that these mechanisms achieve \emph{monotonic improvement}, measured by a reduction in expected Bregman divergence. Our analysis is comprehensive, covering both \emph{direct} and \emph{two-step} projection methods, and robustly extends these guarantees to non-realizable settings, empirical (finite-sample) distributions, and relaxed coherence constraints.
  Furthermore, we establish a general \emph{characterization theorem}, showing that any mechanism with similar provable improvement guarantees must inherently conform to a coherence-based structure. This culminates in rigidity results under the demand for universal improvement, establishing coherence as a fundamental and, in a formal sense, necessary principle for provable self-improvement.

</details>


### [113] [One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms](https://arxiv.org/abs/2511.08444)
*Xiang Li,You Li,Yazhou Zhang*

Main category: cs.LG

TL;DR: 提出'One Model for All'通用预训练框架，通过解耦学习解决EEG数据集异质性，在SEED、DEAP、DREAMER数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: EEG情感识别面临数据集异质性（通道/被试变异性）问题，现有方法难以有效迁移知识，需要通用预训练框架

Method: 两阶段学习：1）基于统一通道模式的单变量自监督对比预训练；2）使用ART和GAT架构的多变量微调，捕捉复杂时空依赖

Result: 在SEED(99.27%)、DEAP(93.69%)、DREAMER(93.93%)上实现SOTA性能，跨数据集迁移达94.08%，GAT模块在DEAP上带来+22.19%提升

Conclusion: 该框架为多样化EEG分析任务提供了更通用、可扩展和有效的预训练模型路径

Abstract: EEG-based emotion recognition is hampered by profound dataset heterogeneity (channel/subject variability), hindering generalizable models. Existing approaches struggle to transfer knowledge effectively. We propose 'One Model for All', a universal pre-training framework for EEG analysis across disparate datasets. Our paradigm decouples learning into two stages: (1) Univariate pre-training via self-supervised contrastive learning on individual channels, enabled by a Unified Channel Schema (UCS) that leverages the channel union (e.g., SEED-62ch, DEAP-32ch); (2) Multivariate fine-tuning with a novel 'ART' (Adaptive Resampling Transformer) and 'GAT' (Graph Attention Network) architecture to capture complex spatio-temporal dependencies. Experiments show universal pre-training is an essential stabilizer, preventing collapse on SEED (vs. scratch) and yielding substantial gains on DEAP (+7.65%) and DREAMER (+3.55%). Our framework achieves new SOTA performance on all within-subject benchmarks: SEED (99.27%), DEAP (93.69%), and DREAMER (93.93%). We also show SOTA cross-dataset transfer, achieving 94.08% (intersection) and 93.05% (UCS) on the unseen DREAMER dataset, with the former surpassing the within-domain pre-training benchmark. Ablation studies validate our architecture: the GAT module is critical, yielding a +22.19% gain over GCN on the high-noise DEAP dataset, and its removal causes a catastrophic -16.44% performance drop. This work paves the way for more universal, scalable, and effective pre-trained models for diverse EEG analysis tasks.

</details>


### [114] [Binary Split Categorical feature with Mean Absolute Error Criteria in CART](https://arxiv.org/abs/2511.08470)
*Peng Yu,Yike Chen,Chao Xu,Albert Bifet,Jesse Read*

Main category: cs.LG

TL;DR: 本文提出了一种针对CART算法中分类特征使用MAE准则的高效分割算法，解决了传统数值编码方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统CART算法在处理分类特征时，使用GINI和熵等标准准则已有成熟方法，但使用MAE准则时依赖数值编码方法，而现有无监督数值编码方法对MAE准则无效。

Method: 提出了一种新颖高效的分割算法，专门针对MAE准则处理分类特征的挑战。

Result: 证明了无监督数值编码方法不适用于MAE准则，并展示了新算法的有效性。

Conclusion: 新算法为CART算法中分类数据的处理提供了有前景的解决方案，克服了现有方法的局限性。

Abstract: In the context of the Classification and Regression Trees (CART) algorithm, the efficient splitting of categorical features using standard criteria like GINI and Entropy is well-established. However, using the Mean Absolute Error (MAE) criterion for categorical features has traditionally relied on various numerical encoding methods. This paper demonstrates that unsupervised numerical encoding methods are not viable for the MAE criteria. Furthermore, we present a novel and efficient splitting algorithm that addresses the challenges of handling categorical features with the MAE criterion. Our findings underscore the limitations of existing approaches and offer a promising solution to enhance the handling of categorical data in CART algorithms.

</details>


### [115] [LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics](https://arxiv.org/abs/2511.08544)
*Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 提出了LeJEPA，一种理论完备、简洁高效的联合嵌入预测架构训练方法，通过SIGReg正则化约束嵌入分布，在多个数据集和架构上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的联合嵌入预测架构(JEPAs)缺乏理论指导和实践规范，导致研发过程随意。需要建立理论基础并提供简洁实用的训练方法。

Method: 1) 识别各向同性高斯分布为JEPAs嵌入的最优分布；2) 提出SIGReg正则化方法约束嵌入分布；3) 结合预测损失和SIGReg构建LeJEPA训练目标。

Result: 在10+数据集、60+架构上验证，LeJEPA具有：单超参数、线性复杂度、稳定性强、无需启发式技巧等优势。在ImageNet-1k上ViT-H/14达到79%准确率。

Conclusion: LeJEPA提供了简单且理论友好的自监督预训练框架，有望重新确立自监督预训练在AI研究中的核心地位。

Abstract: Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).

</details>


### [116] [FMMI: Flow Matching Mutual Information Estimation](https://arxiv.org/abs/2511.08552)
*Ivan Butakov,Alexander Semenenko,Alexey Frolov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出了一种基于归一化流的互信息估计器，通过将联合分布转换为边缘分布来估计互信息，具有计算高效、精度高、可扩展性强等优点


<details>
  <summary>Details</summary>
Motivation: 传统的判别式互信息估计方法需要训练分类器来区分联合分布和边缘分布，存在计算效率低和精度限制的问题

Method: 学习一个归一化流来将联合分布转换为边缘分布，从而直接估计互信息

Result: 该方法计算效率高、估计精度好，能够扩展到高维数据并适应广泛的真实互信息值范围

Conclusion: 这种基于归一化流的互信息估计方法为互信息估计提供了新的有效框架

Abstract: We introduce a novel Mutual Information (MI) estimator that fundamentally reframes the discriminative approach. Instead of training a classifier to discriminate between joint and marginal distributions, we learn a normalizing flow that transforms one into the other. This technique produces a computationally efficient and precise MI estimate that scales well to high dimensions and across a wide range of ground-truth MI values.

</details>


### [117] [The Path Not Taken: RLVR Provably Learns Off the Principals](https://arxiv.org/abs/2511.08567)
*Hanqing Zhu,Zhenyu Zhang,Hanxian Huang,DiJia Su,Zechun Liu,Jiawei Zhao,Igor Fedorov,Hamed Pirsiavash,Zhizhou Sha,Jinwon Lee,David Z. Pan,Zhangyang Wang,Yuandong Tian,Kai Sheng Tai*

Main category: cs.LG

TL;DR: RLVR（带可验证奖励的强化学习）通过非主方向权重更新实现性能提升，其稀疏性是模型条件优化偏差的表面现象，而非真正的参数稀疏。


<details>
  <summary>Details</summary>
Motivation: 解释RLVR看似只修改少量参数却能显著提升推理性能的悖论，揭示其背后的优化动态机制。

Method: 提出三门理论：门I（KL锚点）施加KL约束更新；门II（模型几何）将更新引导到低曲率子空间；门III（精度）隐藏非偏好区域的微更新。通过参数级分析验证理论。

Result: RLVR在权重空间的非主方向学习，通过最小频谱漂移、减少主子空间旋转和非主方向更新对齐实现增益。相比之下，SFT针对主权重，扭曲频谱且性能落后于RLVR。

Conclusion: RLVR与SFT处于不同的优化机制，直接适配SFT时代的参数高效微调方法存在缺陷，需要设计几何感知的RLVR原生学习算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.
  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.

</details>


### [118] [Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms](https://arxiv.org/abs/2511.08570)
*Jamison Moody,James Usevitch*

Main category: cs.LG

TL;DR: AdaptKAN是一种改进的Kolmogorov-Arnold网络，通过自动调整域网格来消除训练过程中的手动调整需求，同时保持KAN的优势如可解释性和高精度。


<details>
  <summary>Details</summary>
Motivation: 传统KAN架构需要手动调整域网格，增加了训练过程的复杂性，缺乏根据数据自动更新域的能力。

Method: 提出AdaptKAN，使用直方图算法自动调整域网格，该算法还能用于检测分布外输入。

Result: 在四个任务上表现优于或匹配现有KAN架构和MLPs：Feynman数据集学习科学方程、图像分类、学习控制Lyapunov函数、OpenOOD v1.5基准上的OOD检测。

Conclusion: AdaptKAN通过自动域网格调整简化了KAN训练，同时保持了性能优势，并扩展了OOD检测能力。

Abstract: Kolmogorov-Arnold Networks (KANs) are a class of neural networks that have received increased attention in recent literature. In contrast to MLPs, KANs leverage parameterized, trainable activation functions and offer several benefits including improved interpretability and higher accuracy on learning symbolic equations. However, the original KAN architecture requires adjustments to the domain discretization of the network (called the "domain grid") during training, creating extra overhead for the user in the training process. Typical KAN layers are not designed with the ability to autonomously update their domains in a data-driven manner informed by the changing output ranges of previous layers. As an added benefit, this histogram algorithm may also be applied towards detecting out-of-distribution (OOD) inputs in a variety of settings. We demonstrate that AdaptKAN exceeds or matches the performance of prior KAN architectures and MLPs on four different tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.

</details>
