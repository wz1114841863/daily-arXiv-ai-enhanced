<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 29]
- [cs.LG](#cs.LG) [Total: 155]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)
*Amur Saqib Pal,Muhammad Mohsin Ghaffar,Faisal Shafait,Christian Weis,Norbert Wehn*

Main category: cs.DC

TL;DR: HLS4PC：一个参数化的HLS框架，用于FPGA加速3D点云处理，通过硬件感知压缩技术优化PointMLP模型，实现比GPU和CPU更高的吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有的基于点的3D点云模型计算和内存密集，在服务器级GPU上运行，但点云数据的稀疏和非结构化特性导致GPU利用率低，难以在安全关键应用中实现实时性能

Method: 提出HLS4PC参数化HLS框架，利用FPGA并行化和算法优化实现高效定点数映射和神经网络函数。对PointMLP-Elite模型应用硬件感知压缩技术：用URS替换FPS、参数量化、层融合和输入点剪枝，得到PointMLP-Lite

Result: PointMLP-Lite复杂度降低4倍，在ModelNet40上仅损失2%准确率。FPGA加速的PointMLP-Lite比先前工作吞吐量高3.56倍，比GPU实现高2.3倍，比CPU实现高22倍

Conclusion: HLS4PC框架通过FPGA加速和硬件感知优化，有效解决了3D点云处理中的计算和内存瓶颈，为安全关键应用提供了高效的实时解决方案

Abstract: Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.

</details>


### [2] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 提出一个端到端的LLM框架，通过构建最小可执行程序来优化GPU热点内核，无需完整应用构建，实现跨平台性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中GPU热点内核是主要瓶颈，专家手动调优成本高且难以移植。现有LLM方法假设内核可以廉价编译执行，但在大型应用中完整构建和运行成本昂贵。

Method: 从独立提取的热点内核自动构建最小可执行程序，进行多轮迭代优化和评估。集成自动错误修复和性能模式继承来修复错误、保持正确性、重用有效的分块/内存/同步策略，降低搜索成本。

Result: 在NVIDIA GPU和国产海光DCU平台上评估：PolyBench平均加速5.05x(NVIDIA)和7.77x(DCU)，AMD APP SDK加速1.77x，三个热点内核加速1.25x，超越直接LLM优化。

Conclusion: 该方法无需完整源码依赖，提供跨平台可移植性，实现实用低成本的GPU内核优化，为大规模超算应用提供有效解决方案。

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [3] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: GPU-Virt-Bench是一个全面的GPU虚拟化基准测试框架，用于评估软件虚拟化系统与硬件MIG技术，包含56个性能指标和10个类别，帮助多租户环境中的GPU资源部署决策。


<details>
  <summary>Details</summary>
Motivation: GPU加速工作负载（特别是AI和LLM推理）的激增导致对云和容器环境中高效GPU资源共享的需求急剧增加。虽然NVIDIA的MIG技术提供硬件级隔离，但仅适用于高端数据中心GPU。软件虚拟化方案缺乏标准化评估方法。

Method: 开发了GPU-Virt-Bench基准测试框架，包含56个性能指标，组织成10个类别：开销、隔离质量、LLM特定性能、内存带宽、缓存行为、PCIe吞吐量、多GPU通信、调度效率、内存碎片化和错误恢复。框架支持软件虚拟化方案与理想MIG行为的系统比较。

Result: 通过评估HAMi-core、BUD-FCSP和模拟MIG基线，展示了框架的实用性，揭示了生产部署决策关键的性能特征。提供了对GPU虚拟化系统性能的全面洞察。

Conclusion: GPU-Virt-Bench为GPU虚拟化系统提供了标准化评估方法，帮助从业者在多租户环境中做出明智的GPU资源部署决策，弥合了软件虚拟化方案与硬件MIG技术之间的评估差距。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [4] [SoDA: An Efficient Interaction Paradigm for the Agentic Web](https://arxiv.org/abs/2512.22135)
*Zicai Cui,Zhouyuan Jian,Weiwen Liu,Weinan Zhang*

Main category: cs.DC

TL;DR: 论文提出面向未来的用户主权交互范式，通过主权数字化身(SoDA)实现数据与应用的解耦，从"杀时间"转向"省时间"，在零信任环境下降低认知负荷和平台垄断。


<details>
  <summary>Details</summary>
Motivation: 随着互联网从移动App主导的注意力经济转向意图互联的智能体网络时代，现有交互模式无法解决日益严重的数据锁定和认知过载问题。需要建立新的用户主权交互范式，实现从"杀时间"到"省时间"的根本转变。

Method: 提出主权数字化身(SoDA)架构，采用存储、计算、交互的正交解耦设计，建立"数据作为持久资产、模型作为临时工具"的架构原则。设计基于A2A协议的意图-权限握手机制，采用双因子（敏感系数和严格参数）自适应路由实现主动风险治理。

Result: 在高保真仿真环境中，该范式在跨平台服务迁移和复杂任务执行中减少约27-35%的token消耗。在多模态复杂任务编排中，相比标准RAG架构减少72%的用户认知负荷，相比手动工作流减少88%，同时显著提升信息信噪比(SNR)。

Conclusion: SoDA是构建高效、低摩擦、去中心化智能体网络的关键交互基础设施，通过解耦记忆与应用逻辑、从显式手动指令转向隐式意图对齐，从根本上打破平台对用户记忆的垄断，解决认知过载问题。

Abstract: As the internet evolves from the mobile App-dominated Attention Economy to the Intent-Interconnection of the Agentic Web era, existing interaction modes fail to address the escalating challenges of data lock-in and cognitive overload. Addressing this, we defines a future-oriented user sovereignty interaction paradigm, aiming to realize a fundamental shift from killing time to saving time. Specifically, we argue that decoupling memory from application logic eliminates the structural basis of data lock-in, while shifting from explicit manual instruction to implicit intent alignment resolves cognitive overload by offloading execution complexity. This paradigm is implemented via the Sovereign Digital Avatar (SoDA), which employs an orthogonal decoupling design of storage, computation, and interaction. This establishes the architectural principle of data as a persistent asset, model as a transient tool, fundamentally breaking the platform monopoly on user memory. To support the operation of this new paradigm in zero-trust environments, we design an Intent-Permission Handshake Mechanism based on A2A protocols, utilizing dual-factor (Sensitivity Coefficient and Strictness Parameter) adaptive routing to achieve active risk governance. Empirical evaluation with a high-fidelity simulation environment indicates that this paradigm reduces token consumption by approximately 27-35\% during cross-platform service migration and complex task execution. Furthermore, in the orchestration of multi-modal complex tasks, it reduces user cognitive load by 72\% compared to standard Retrieval-Augmented Generation (RAG) architectures, by 88\% relative to manual workflows, while significantly boosting the Information Signal-to-Noise Ratio (SNR). These results demonstrate that the SoDA is the essential interaction infrastructure for building an efficient, low-friction, and decentralized Agentic Web.

</details>


### [5] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: 本文提出局部会合哈希(LRH)，在保持令牌环结构的同时，通过限制HRW选择到缓存局部窗口的C个相邻物理节点，实现负载均衡与性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统一致性哈希中，基于环的方案需要大量虚拟节点才能获得良好负载均衡，而多探针方法虽然改善了平衡性，但以分散内存访问为代价。需要一种既能保持环结构优势，又能实现良好负载均衡和高效性能的解决方案。

Method: 提出局部会合哈希(LRH)：保留令牌环结构，但将最高随机权重(HRW)选择限制在缓存局部窗口的C个相邻物理节点。通过一次二分查找定位键，使用预计算的下一个不同偏移量枚举恰好C个不同候选节点，选择HRW胜者（可选加权）。查找成本为O(log|R| + C)。在固定拓扑存活变化下，固定候选过滤仅重新映射原始胜者宕机的键，实现零额外扰动。

Result: 在N=5000、V=256(|R|=1.28M)、K=50M、C=8的基准测试中，LRH将最大/平均负载从1.2785降低到1.0947，达到60.05 Mkeys/s，比8探针多探针一致性哈希(8.80 Mkeys/s)快约6.8倍，同时接近其平衡性(最大/平均1.0697)。微基准测试表明多探针分配主要受重复环搜索和内存流量而非探针生成算术支配。

Conclusion: LRH通过结合环结构的简单性和局部候选选择，在保持低扰动的同时实现了接近多探针方法的负载均衡，显著提升了性能。该方法为分布式系统提供了一种高效的一致性哈希解决方案，特别适合需要高吞吐量和良好负载均衡的场景。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [6] [SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware](https://arxiv.org/abs/2512.22136)
*Mahadev Sunil Kumar,Arnab Raha,Debayan Das,Gopakumar G,Amitava Mukherjee*

Main category: cs.DC

TL;DR: 提出一种针对分布式深度网络的联合优化方法，通过结构化剪枝和多目标优化，在满足硬件约束的同时保持任务性能，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 深度分布式网络在计算机视觉中应用广泛，但其在资源受限的边缘设备上部署受到参数数量和计算需求的限制，需要一种既能满足硬件约束又能保持性能的部署方法。

Method: 结合结构化模型剪枝和多目标优化，根据异构设备约束调整网络容量。以MVCNN为例，量化单个视图对分类准确率的贡献，并相应分配剪枝预算。

Result: 实验结果显示，生成的模型在满足用户指定的准确率和内存占用限制的同时，在不同硬件平台上将推理延迟降低了1.2倍到5.0倍。

Conclusion: 性能感知、视图自适应的压缩方法为在分布式边缘环境中部署复杂视觉模型提供了可行的途径。

Abstract: Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respects hardware limitations and preserves task performance. Our method integrates a structured model pruning with a multi-objective optimization to tailor network capacity to heterogeneous device constraints. We demonstrate this framework using Multi-View Convolutional Neural Network (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets, respectively. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by factors ranging from 1.2x to 5.0x across diverse hardware platforms. These findings suggest that performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments.

</details>


### [7] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 将离线性能优化框架应用于DevOps发布阶段的微服务资源分配优化，通过算法比较和因子筛选来平衡采样成本与最优配置距离


<details>
  <summary>Details</summary>
Motivation: 当前微服务优化研究主要集中在运维阶段的智能调度和自动扩缩容，而发布阶段的CPU和内存资源分配配置优化仍未被充分探索。水平自动扩缩容可能因缺乏前期调优而导致内存分配不当。

Method: 使用TeaStore微服务应用评估性能优化框架，统计比较不同优化算法，分析因子筛选对搜索空间缩减的影响，支持在采样成本和最优配置距离之间做出权衡决策。

Result: 研究表明：1) 当目标是找到最优资源配置时，前期因子筛选有助于在可承受的采样预算内实现；2) 当需要统计比较不同算法时，筛选也使搜索空间中所有数据点的收集变得可行；3) 当目标是找到接近最优的配置时，无筛选的贝叶斯优化效果更好。

Conclusion: 在DevOps发布阶段应用离线性能优化框架可以有效优化微服务资源分配，但需要根据具体目标（最优配置、算法比较或近优配置）选择是否采用因子筛选策略。

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


### [8] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: HybridFlow是一个资源自适应的边缘-云端协作推理框架，通过细粒度任务分解和并行执行来降低LLM推理延迟和token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上部署面临高推理延迟和token消耗问题，现有边缘-云协作方法采用粗粒度任务分配策略，无法充分利用细粒度推理并行性，导致冗余计算和资源利用效率低下。

Method: HybridFlow采用两阶段方法：1) 任务分解与并行执行，将复杂查询动态拆分为相互依赖的子任务；2) 资源感知的子任务路由，通过学习的路由器根据预测的效用增益和实时预算状态自适应地将子任务分配给边缘或云端模型。

Result: 在GPQA、MMLU-Pro、AIME和LiveBench-Reasoning上的综合评估表明，HybridFlow有效减少了端到端推理时间和总体token使用量，同时保持了有竞争力的准确性。

Conclusion: HybridFlow通过细粒度的边缘-云协作推理，实现了更高效的资源利用，为资源受限的边缘设备上的LLM部署提供了有效的解决方案。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [9] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: Cleave是一个用于边缘设备上分散式基础模型训练的新范式，通过选择性混合张量并行和参数服务器框架，解决了现有边缘训练方法的性能、可扩展性、内存限制和通信开销等问题。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型训练生态系统高度集中化，仅限于大型云数据中心运营商，训练成本高昂。利用边缘设备空闲计算资源进行分散式训练是一个民主化的替代方案，但现有边缘训练方法存在性能不足、可扩展性有限、内存超限、通信开销大等问题，且无法有效处理设备异构性和动态性。

Method: Cleave采用选择性混合张量并行方法精细划分训练操作，结合参数服务器中心的训练框架，应对设备内存限制并避免通信瓶颈。通过成本优化模型指导设备选择和训练工作负载分配，有效处理设备异构性和变动。

Result: 评估显示Cleave能够匹配基于云的GPU训练性能，可扩展到更大模型和数千台设备，支持比基线边缘训练方法多8倍的设备。在每批次训练时间上比最先进的边缘训练方法快10倍，并能高效处理设备故障，恢复速度比先前方法快至少100倍。

Conclusion: Cleave通过创新的选择性混合张量并行和参数服务器框架，成功实现了在边缘设备上高效训练大型基础模型，解决了现有边缘训练方法的关键限制，为分散式模型训练提供了可行的解决方案。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [10] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: 提出自适应GPU资源分配框架，在服务器无GPU平台上实现多智能体系统的高效部署，相比轮询调度减少85%延迟，同时保持与静态分配相当的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在服务器无GPU平台部署面临资源分配挑战：智能体工作负载异构、计算需求多变、需要成本效益扩展。

Method: 开发自适应GPU资源分配框架，基于工作负载特征、智能体优先级和最小资源需求动态分配GPU资源，使用O(N)复杂度算法实现实时适应。

Result: 在模拟四类异构智能体的现实工作流中，自适应分配在延迟、成本和GPU利用率指标上优于静态平均和轮询策略，实现85%延迟减少。

Conclusion: 该框架为在服务器无GPU基础设施上部署成本效益的多智能体AI系统提供了实用解决方案。

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [11] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: TL是一个端到端框架，可将基于tile的程序编译到空间数据流架构上，解决了在分布式核心间分配tile实例、利用片上网络和分布式内存的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 空间数据流加速器通过显式的编译器管理数据移动减少内存瓶颈，但端到端性能严重依赖于工作负载到硬件的映射。现有编译器主要优化单个tile内的代码生成，而如何跨空间分布的核心分配tile实例、利用片上网络和分布式内存来增加数据重用和减少通信是核心挑战。

Method: TL提出了一种硬件表示方法，捕捉互连拓扑、内存层次和计算能力，支持特定架构优化和多样化空间数据流目标。基于MLIR生态系统构建，定义了不同前端的通用入口点和不同后端的终点。

Result: 论文没有提供具体的实验结果，但描述了TL框架的设计和实现方法，能够编译基于tile的程序到空间数据流架构，解决分布式核心间的tile分配和通信优化问题。

Conclusion: TL框架通过端到端的编译方法，提高了空间数据流加速器的可编程性，解决了现有编译器在跨核心tile分配和通信优化方面的不足，有助于推动空间数据流架构的广泛应用。

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [12] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab平台已从材料科学扩展到多学科，通过浏览器界面简化复杂计算工作流，自动追踪模拟溯源确保可重复性，并集成电子实验笔记本支持FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力提升，需要自动化工作流来管理大量相互依赖的模拟，但执行这些工作流通常需要技术专业知识来设置输入、解释输出和处理并行代码执行的复杂性。

Method: 开发AiiDAlab平台，通过直观的Web浏览器界面使复杂计算工作流可访问，基于AiiDA引擎自动追踪完整模拟溯源，简化用户入门，优化计算资源访问，处理大数据集，并与电子实验室笔记本集成。

Result: AiiDAlab已成功扩展到量子化学、大气建模、电池研究和大规模设施实验数据分析等多个学科，在教育环境中也被积极使用，支持研究人员专注于研究而非计算细节。

Conclusion: AiiDAlab已成为加速多学科科学发现的强大平台，通过简化计算复杂性、确保可重复性和支持FAIR原则，帮助研究人员轻松生成可重复的开放研究数据。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [13] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: BitFlipScope是一个软件框架，用于在Transformer架构中定位由硬件故障或攻击引起的比特翻转故障，支持有参考模型和无参考模型两种场景，并能实现轻量级性能恢复。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键环境中部署时，容易受到硬件退化、宇宙辐射或Rowhammer等攻击引起的比特翻转故障影响，这些故障会无声地破坏模型参数，导致不可预测的危险行为。定位这些故障对于诊断问题、应用针对性修复措施至关重要。

Method: 提出BitFlipScope框架，包含两种部署场景的方法：1）有干净参考模型时，通过输出、隐藏状态和内部激活的差异分析来检测异常行为；2）无参考模型时，使用残差路径扰动和损失敏感性分析直接从损坏模型中推断故障影响区域。

Result: 该框架不仅能有效进行故障诊断，还支持无需微调的轻量级性能恢复，为在硬件易出错和对抗环境中部署可信赖、容错的LLM提供了实用路径。

Conclusion: BitFlipScope是实现可信赖、容错LLM部署的重要一步，能够在硬件易出错和对抗环境中定位比特翻转故障并恢复模型性能。

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [14] [iOS as Acceleration](https://arxiv.org/abs/2512.22180)
*Alexander K. Chen*

Main category: cs.DC

TL;DR: 利用iOS手机作为分布式计算节点，通过流水线并行技术增强本地机器学习能力，实现零成本提升较弱计算环境


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习需要强大计算资源，但本地环境受限时面临挑战。虽然云计算可解决本地资源不足问题，但在涉及隐私数据、物理环境不可用或成本较高时，仍需本地计算。本研究探索利用普遍存在但未充分利用的移动设备资源（特别是iOS手机）来增强本地计算能力。

Method: 提出概念验证系统，采用分布式流水线并行方法，利用iOS设备作为计算节点。系统克服了iOS设备的内存限制、热节流和操作系统沙盒等限制，实现模型训练、批量推理和智能LRM工具使用的加速。

Result: 在较弱计算环境中实现了显著性能提升，证明了iOS设备作为机器学习计算资源的潜力。系统能够加速适度规模的模型训练、批量推理和智能工具使用。

Conclusion: 研究表明普通移动设备有潜力为机器学习做出更大贡献。讨论了实际用例、限制和未来研究方向，强调了利用现有移动设备资源增强本地计算能力的重要性。

Abstract: Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.

</details>


### [15] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: MatKV：通过预计算和存储RAG文档的KV向量到闪存，在推理时直接复用而非重新计算，显著降低推理时间和能耗


<details>
  <summary>Details</summary>
Motivation: LLM推理成本已超过训练成为主要开销，RAG处理长文本时prefill阶段计算KV向量能耗高且耗时，需要提高RAG推理效率

Method: 预计算RAG文档的KV向量，将其物化存储在廉价高效的闪存中，推理时直接加载复用而非GPU重新计算

Result: 相比GPU完全计算KV，MatKV将RAG推理时间和能耗减半，问答任务准确率影响小；支持GPU解码与KV加载并行，低端GPU也能高效解码

Conclusion: MatKV能显著降低大规模生成式AI应用的成本和能耗，使其在更广泛的任务和硬件环境中更易部署

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [16] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: SPUMA实现了OPENFOAM在NVIDIA和AMD GPU上的完整移植，通过可移植编程模型和内存池管理器，在LUMI和Leonardo集群上展示了优异的性能和能效


<details>
  <summary>Details</summary>
Motivation: 尽管GPU在HPC中广泛应用，但在开源CFD软件中的可编程性仍是挑战，特别是在混合集群环境中需要有效利用现代加速器

Method: 基于可移植编程模型，采用内存池管理器利用现代GPU的统一内存特性，在LUMI（AMD MI250X）和Leonardo（NVIDIA A100）集群上进行测试

Result: 强可扩展性在每GPU800万网格时达到65%效率；弱可扩展性在20个GPU上达到75-85%；使用AmgX求解器时效率不低于90%；A100 GPU相当于200-300个Intel Sapphire Rapids核心；能耗降低达82%

Conclusion: SPUMA成功实现了OPENFOAM在异构GPU集群上的高效移植，显著提升了计算性能和能效，为开源CFD软件在混合HPC环境中的应用提供了可行方案

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [17] [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)
*Xinhao Cheng,Zhihao Zhang,Yu Zhou,Jianan Ji,Jinchen Jiang,Zepeng Zhao,Ziruo Xiao,Zihao Ye,Yingyi Huang,Ruihang Lai,Hongyi Jin,Bohan Hou,Mengdi Wu,Yixin Dong,Anthony Yip,Zihao Ye,Songting Wang,Wenqin Yang,Xupeng Miao,Tianqi Chen,Zhihao Jia*

Main category: cs.DC

TL;DR: MPK是首个将多GPU模型推理自动转换为单一高性能megakernel的编译器和运行时系统，通过SM级图表示实现跨算子软件流水线等优化，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统通常采用算子级内核调度，存在调度开销大、GPU利用率低的问题，无法充分利用现代GPU的并行计算能力，需要更细粒度的优化方法。

Method: 提出SM级图表示捕获数据依赖，编译器将张量程序转换为优化的SM级任务图并生成CUDA实现，运行时在单一megakernel内通过去中心化调度执行任务。

Result: MPK相比现有算子级LLM服务系统将端到端推理延迟降低高达1.7倍，将LLM推理性能推向硬件极限，且保持现有编程模型的灵活性。

Conclusion: MPK通过自动化的端到端内核融合和细粒度优化，显著提升了多GPU模型推理性能，为高效LLM服务提供了新解决方案。

Abstract: We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.

</details>


### [18] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: 提出一个结合AI、边缘计算和云计算的云原生架构，用于处理大规模PMU数据，实现低延迟、可扩展的实时电网监控和分析。


<details>
  <summary>Details</summary>
Motivation: PMU产生的高频时间同步数据对实时电网监控至关重要，但大规模PMU部署带来了延迟、可扩展性和可靠性挑战。传统的集中式处理架构难以处理现代动态电网中的PMU数据量和速度。

Method: 提出一个云原生架构，整合人工智能与边缘和云计算，采用分布式流处理、容器化微服务和弹性资源编排，实现低延迟数据摄取、实时异常检测和高级分析，并嵌入机器学习模型进行时间序列分析。

Result: 开发了分析模型评估系统延迟、吞吐量和可靠性，显示该架构可实现亚秒级响应时间，并能扩展到大型PMU部署。同时嵌入了安全和隐私机制，支持关键基础设施环境部署。

Conclusion: 该架构为下一代智能电网分析提供了强大而灵活的基础，解决了大规模PMU数据处理的关键挑战。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [19] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: Pick and Spin是一个基于Kubernetes的LLM编排框架，通过统一部署、自适应扩缩容和混合路由，实现自托管LLM的高效经济管理，相比静态部署可提升21.6%成功率、降低30%延迟和33%GPU成本。


<details>
  <summary>Details</summary>
Motivation: 组织自托管大型语言模型面临GPU利用率低、工作负载路由困难和可靠性挑战，需要一种既能保证隐私、成本控制和定制化，又能实现高效经济管理的解决方案。

Method: 基于Kubernetes构建，包含：1) 统一的Helm部署系统；2) 自适应扩缩容到零的自动化；3) 混合路由模块，结合关键词启发式和轻量级DistilBERT分类器，平衡成本、延迟和准确性。

Result: 在四个模型(Llama-3 90B、Gemma-3 27B、Qwen-3 235B、DeepSeek-R1 685B)上评估，涵盖8个公开基准数据集、5种推理策略和2种路由变体，共31,019个提示和163,720次推理运行。相比静态部署，成功率最高提升21.6%，延迟降低30%，每个查询的GPU成本降低33%。

Conclusion: Pick and Spin框架使自托管LLM编排具有可扩展性和经济性，通过智能路由和资源管理显著提升性能并降低成本，为组织自托管LLM提供了实用解决方案。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [20] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: Nightjar是一种基于学习的自适应推测解码算法，能根据请求负载动态调整推测长度，在实时服务中显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码使用固定推测长度，无法适应动态请求率，在高负载计算密集型环境中因验证开销导致性能下降，在真实服务场景中形成性能瓶颈。

Method: 提出Nightjar算法，基于学习自适应调整推测推理，根据批量大小动态选择最优推测长度，在无益时甚至完全禁用推测解码。

Result: 实验显示Nightjar相比标准推测解码实现高达14.8%的吞吐量提升和20.2%的延迟降低，在实时服务中表现出鲁棒效率。

Conclusion: Nightjar通过自适应推测解码解决了传统固定长度方法的性能瓶颈，为LLM推理服务提供了更高效、适应性强的解决方案。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [21] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: RobustRL：首个针对LLM RL后训练的系统级容错框架，通过角色隔离、非中断恢复和动态重连，在GPU故障下保持高训练效率


<details>
  <summary>Details</summary>
Motivation: RL后训练混合了训练和推理工作负载，现有容错框架只针对单一场景，无法充分利用异步执行的优化潜力。GPU故障会导致整个RL任务重启，带来巨大的回放和初始化开销

Method: 基于角色隔离的容错机制：1) 角色感知监控区分真实故障和角色特定行为；2) 非中断恢复：训练器通过回滚热备快速恢复，rollout进行隔离机器替换；3) 动态重连：用UCX点对点通信替代静态集合通信，实现权重同步

Result: 在256-GPU集群上，Qwen3-8B-Math工作负载，10%故障注入频率下，RobustRL的ETTR超过80%（相比ByteRobust的60%），端到端训练时间快8.4%-17.4%

Conclusion: RobustRL通过角色隔离和细粒度恢复机制，显著提升了RL后训练在GPU故障下的容错能力和训练效率，为大规模RL系统提供了实用的容错解决方案

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [22] [Object Abstraction To Streamline Edge-Cloud-Native Application Development](https://arxiv.org/abs/2512.22534)
*Pawissanutt Lertpongrujikorn*

Main category: cs.DC

TL;DR: 该论文提出Object-as-a-Service (OaaS) 范式，通过统一资源、状态和工作流管理来解决云原生开发中的碎片化问题，显著降低开发复杂度并提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前云计算的serverless承诺与实际实现之间存在差距，主要由于函数运行时、状态管理和编排的碎片化问题。基础设施复杂性降低了开发效率，实践者更关注自动化和可维护性而非成本优化。

Method: 基于三项实证研究：21名从业者访谈、39名参与者的开发者体验研究、以及101次跨86个组织的客户发现访谈。提出OaaS范式，开发Oparaca原型，并扩展至边缘计算(OaaS-IoT with EdgeWeaver)。

Result: OaaS范式实现可忽略的开销和最优可扩展性；OaaS-IoT相比传统FaaS任务完成速度快31%，代码行数减少44.5%；建立了面向技术型中小企业和初创企业的商业化路径。

Conclusion: OaaS通过整合碎片化抽象和自动化性能优化，为云原生平台奠定基础，隐藏基础设施复杂性，使开发者能专注于创新。同时建立了将技术研究基于已验证实践者需求的实证方法学。

Abstract: Cloud computing has fundamentally transformed application development, yet a gap remains between the serverless promise of simplified deployment and its practical realization due to fragmentation across function runtimes, state management, and orchestration. This dissertation addresses this gap through empirical validation and technical innovation, establishing the Object-as-a-Service (OaaS) paradigm as a unified approach to cloud-native development. Grounded in evidence from three studies - practitioner interviews (21 participants), a human study on developer experience (39 participants), and NSF I-Corps customer discovery (101 interviews across 86 organizations) - this work demonstrates that infrastructure complexity taxes productivity, with practitioners prioritizing automation and maintainability over cost optimization. The dissertation makes five major contributions: (1) the OaaS paradigm unifies resource, state, and workflow management via the Oparaca prototype, demonstrating negligible overhead and state-of-the-art scalability; (2) SLA-driven OaaS enables declarative management of non-functional requirements like availability, consistency, and latency; (3) OaaS-IoT with EdgeWeaver extends the paradigm to the edge-cloud continuum, achieving 31% faster task completion and a 44.5% reduction in lines of code compared to traditional FaaS; (4) commercialization validation establishes a pathway targeting technology SMEs and startups; and (5) an empirical methodology for grounding technical research in validated practitioner needs. By consolidating fragmented abstractions and automating performance optimization, OaaS establishes a foundation for cloud-native platforms that hide infrastructure complexity and empower developers to focus on innovation.

</details>


### [23] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: RollArc是一个分布式系统，用于在解耦基础设施上最大化多任务智能体强化学习的训练吞吐量，通过硬件亲和性工作负载映射、细粒度异步和状态感知计算实现1.35-2.05倍的端到端训练时间减少。


<details>
  <summary>Details</summary>
Motivation: 智能体强化学习工作负载高度异构，包含计算密集型预填充阶段、带宽受限的解码和状态化CPU密集型环境模拟。现有解耦基础设施存在同步开销大和资源利用率低的问题，需要专门设计的高效训练系统。

Method: RollArc基于三个核心原则：1) 硬件亲和性工作负载映射，将计算密集和带宽受限任务路由到最适合的GPU设备；2) 细粒度异步，在轨迹级别管理执行以减少资源气泡；3) 状态感知计算，将无状态组件卸载到无服务器基础设施实现弹性扩展。

Result: RollArc有效提高了训练吞吐量，相比单体和同步基线实现了1.35-2.05倍的端到端训练时间减少。在阿里巴巴集群上使用3000多块GPU训练了数百亿参数的MoE模型，证明了系统的可扩展性和鲁棒性。

Conclusion: RollArc通过解耦基础设施设计，解决了智能体强化学习训练中的异构工作负载挑战，显著提升了训练效率，为大规模智能体RL训练提供了有效的系统解决方案。

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [24] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 本文首次对多模态大语言模型推理的能耗进行详细分析，发现多模态输入导致17%-94%的额外能耗，并提出阶段级DVFS优化方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文本模型基础上增加了视觉等模态，但现有研究主要关注文本模型，对多模态引入的能耗权衡缺乏理解。需要分析模态膨胀带来的效率问题。

Method: 将MLLM推理流水线分解为视觉编码、预填充和解码三个阶段，在NVIDIA A100 GPU上评估四种代表性MLLM，量化多模态推理相比文本基线的额外能耗，分析GPU功率轨迹和利用情况。

Result: 多模态推理能耗比文本基线高17%-94%，能耗瓶颈因架构而异（计算密集的视觉编码器或大量视觉标记序列）。发现GPU在推理期间利用率不足，输入复杂度导致不同模型的能耗扩展行为差异显著。阶段级DVFS优化能有效节省能耗且性能影响有限。

Conclusion: 多模态LLM推理存在显著的能耗开销，需要针对不同架构特点进行优化。阶段级DVFS是有效的节能技术，为设计更节能的多模态LLM服务系统提供了实用见解和具体指导。

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [25] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC是一个专为分布式机器学习设计的RDMA传输协议，通过放弃传统可靠性和顺序交付保证，采用尽力而为的无序传输模型，显著降低了尾延迟并提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式机器学习扩展到数千个GPU，集体通信中的尾延迟已成为主要瓶颈。传统RDMA传输（如RoCE、IRN等）强制严格的可靠性和顺序交付，依赖重传和包排序来确保正确性，但这些方法在ML场景中引入了复杂性和延迟，即使罕见的包延迟也会阻塞整个模型流水线。

Method: OptiNIC重新审视了传统可靠性保证，基于ML对部分或缺失数据的容忍性，从NIC中消除了重传和顺序交付，实现了尽力而为的无序RDMA传输模型。它引入了自适应超时机制来触发前进进度，同时保留标准拥塞控制机制，将丢失恢复转移到ML流水线本身（如通过Hadamard变换和纠删码）。

Result: 评估显示，OptiNIC在训练和推理中分别将准确率时间（TTA）提高了2倍，吞吐量提高了1.6倍。同时将第99百分位延迟降低了3.5倍，BRAM使用减少了2.7倍，并将NIC对故障的恢复能力几乎翻倍，提供了一个为分布式ML工作负载量身定制的弹性、尾部优化的RDMA传输。

Conclusion: OptiNIC通过重新设计RDMA传输协议以适应ML工作负载的特性，成功解决了分布式机器学习中的尾延迟瓶颈问题，为大规模ML系统提供了更高效、更弹性的通信解决方案。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [26] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: Argus：首个基于token感知的分布式边缘-云LLM推理框架，通过预测输出token长度和Lyapunov优化实现高效任务卸载，在动态异构环境中显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: LLM在现实应用中部署时面临推理时间变异性大的问题，特别是在异构边缘-云系统中。现有解决方案忽略了环境的动态性、随机性和异构性，以及可变输出token长度和设备多样性的影响。

Method: 1) 长度感知语义(LAS)模块：使用微调的语言模型预测输出token长度，采用token长度敏感的特征调制；2) Lyapunov引导的卸载优化(LOO)模块：制定长期QoE优化，考虑LLM预填充和解码成本；3) 带阻尼和拥塞控制的迭代卸载算法(IODCC)：解决整数非线性规划问题。

Result: 理论和实证评估表明，Argus在高度动态、异构的环境中实现了稳健的性能和卓越的效率。

Conclusion: Argus是首个token感知的分布式边缘-云LLM推理框架，通过精确的token长度预测和Lyapunov优化，有效解决了动态异构环境中的推理效率问题，为实际部署提供了可靠解决方案。

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [27] [Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware](https://arxiv.org/abs/2512.23029)
*Alex Khalil,Guillaume Heilles,Maria Parraga,Simon Heilles*

Main category: cs.DC

TL;DR: 本地部署的量化30B参数MoE模型在消费级硬件上可实现与云端服务相当的性能，为中小企业提供低成本、高隐私的LLM解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要依赖云端专有系统，存在数据隐私、运营主权和成本上升等问题。中小企业需要经济实惠且保护隐私的本地部署方案。

Method: 使用消费级服务器和下一代NVIDIA GPU，部署基于Qwen3的量化30B参数MoE模型。从模型内在能力和服务器负载性能两个维度进行评估：模型性能对比学术和行业标准，服务器性能测试延迟、每秒token数和首token时间。

Result: 精心配置的本地部署方案能够实现与云端服务相当的性能表现，为中小企业提供了可行的替代方案。

Conclusion: 通过消费级硬件和量化开源模型的组合，中小企业可以部署高性能LLM而无需承担过高成本或隐私风险，为本地LLM部署提供了可行路径。

Abstract: The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.

</details>


### [28] [Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets](https://arxiv.org/abs/2512.23439)
*Marko Vukolić,Orestis Alpos,Jakov Mitrovski,Themis Papameletiou,Nikola Ristić,Dionysis Zindros*

Main category: cs.DC

TL;DR: Bitcoin-IPC是一个软件栈和协议，通过创建以L1 BTC为质押的PoS Layer-2链（子网），扩展比特币作为通用交换媒介的能力，无需修改比特币L1即可将交易吞吐量从7tps提升到160tps以上。


<details>
  <summary>Details</summary>
Motivation: 解决比特币作为通用交换媒介的扩展性问题，提高交易吞吐量，降低交易成本，同时保持与比特币L1的安全性和兼容性。

Method: 设计受SWIFT消息系统启发的协议，嵌入比特币的SegWit机制，创建无需许可的完全可编程PoS Layer-2链（子网），这些子网以L1 BTC为质押，依赖比特币L1进行关键信息通信、结算和安全保障。

Result: 将每笔交易的虚拟字节成本降低高达23倍，交易吞吐量从7tps提升到超过160tps，实现跨L2子网的无缝价值转移，无需修改比特币L1协议。

Conclusion: Bitcoin-IPC通过创新的Layer-2架构显著提升了比特币的交易扩展性，使其更接近成为通用交换媒介的目标，同时保持了与比特币L1的安全集成。

Abstract: We introduce Bitcoin-IPC, a software stack and protocol that scales Bitcoin towards helping it become the universal Medium of Exchange (MoE) by enabling the permissionless creation of fully programmable Proof-of-Stake (PoS) Layer-2 chains, called subnets, whose stake is denominated in L1 BTC. Bitcoin-IPC subnets rely on Bitcoin L1 for the communication of critical information, settlement, and security.
  Our design, inspired by SWIFT messaging and embedded within Bitcoin's SegWit mechanism, enables seamless value transfer across L2 subnets, routed through Bitcoin L1. Uniquely, this mechanism reduces the virtual-byte cost per transaction (vB per tx) by up to 23x, compared to transacting natively on Bitcoin L1, effectively increasing monetary transaction throughput from 7 tps to over 160 tps, without requiring any modifications to Bitcoin L1.

</details>


### [29] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 该论文探讨了如何通过不同技术方法（软件架构、云原生Operator模式、传统编程）实现TeaStore微服务案例的自适应控制逻辑解耦，分析各种方法在细粒度表达与系统级控制之间的权衡，并提出多层级架构方案。


<details>
  <summary>Details</summary>
Motivation: TeaStore规范为通过控制循环实现自适应提供了微服务案例研究，但实现需要考虑自适应系统的关键属性：系统范围一致性、规划和模块化。需要探索如何将自适应控制逻辑与应用程序解耦。

Method: 分析三种不同方法：软件架构方法、云原生Operator模式、传统编程语言技术，探讨它们如何解耦自适应控制逻辑与TeaStore应用，并分析各种方法在细粒度表达与系统级控制之间的权衡。

Result: 分析表明这些方法并非互斥，可以组合成多层级的自适应微服务架构。不同方法在协调跨副本适应、执行适应计划、以及适应逻辑集成方面各有优劣，适应策略的复用效果取决于具体场景。

Conclusion: 自适应微服务的最佳实践是将软件架构方法、Operator模式和传统编程技术相结合，形成多层级架构，以平衡细粒度表达与系统级控制的需求，实现有效的自适应系统。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses](https://arxiv.org/abs/2512.22128)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出基于对抗鲁棒性评估的图剪枝框架，通过识别并移除图中脆弱或有害的边来增强GNN的防御能力


<details>
  <summary>Details</summary>
Motivation: GNN虽然能有效利用节点特征和图拓扑信息，但这种联合建模也使其对结构或特征中的扰动/噪声高度敏感，容易受到对抗攻击和虚假连接的影响

Method: 提出一个剪枝框架，利用对抗鲁棒性评估来识别图中脆弱或有害的组件，以鲁棒性分数为指导，选择性地剪除最可能降低模型可靠性的边

Result: 在三个代表性GNN架构和基准数据集上的实验表明，该方法能显著增强GNN在高扰动情况下的防御能力

Conclusion: 通过基于对抗鲁棒性评估的剪枝方法，可以获得更干净、更具韧性的图表示，有效提升GNN的防御能力

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.

</details>


### [31] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)
*Gang Liao,Hongsen Qin,Ying Wang,Alicia Golden,Michael Kuchnik,Yavuz Yetim,Jia Jiunn Ang,Chunli Fu,Yihan He,Samuel Hsia,Zewei Jiang,Dianshi Li,Uladzimir Pashkevich,Varna Puvvada,Feng Shi,Matt Steiner,Ruichao Xiao,Nathan Yan,Xiayu Yu,Zhou Fang,Abdul Zainul-Abedin,Ketan Singh,Hongtao Yu,Wenyuan Chi,Barney Huang,Sean Zhang,Noah Weller,Zach Marine,Wyatt Cook,Carole-Jean Wu,Gaoxiang Liu*

Main category: cs.LG

TL;DR: KernelEvolve是一个面向DLRM的自动化内核编码框架，通过多抽象层编程和基于图的搜索策略，解决深度学习推荐模型在异构硬件上的内核生成与优化问题，显著提升开发效率和性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型(DLRM)的训练和推理需要高效快速，但面临三个关键系统挑战：模型架构多样性、内核原语多样性、以及硬件代际和架构异构性。传统方法难以应对这些异构性挑战。

Method: 提出KernelEvolve框架，采用多抽象层编程（从Triton/CuTe DSL到底层硬件无关语言），通过基于图的搜索策略（包含选择策略、通用算子、适应度函数和终止规则），并利用检索增强提示合成动态适应运行时执行上下文。

Result: 在KernelBench测试套件上实现100%通过率（250个问题，三个难度级别），在三个异构硬件平台上验证160个PyTorch ATen算子，达到100%正确性。开发时间从数周缩短到数小时，在多样化生产用例中显著超越PyTorch基线性能。

Conclusion: KernelEvolve有效解决了DLRM在异构硬件上的内核优化挑战，不仅大幅提升性能效率，还通过为内部开发的AI硬件提供自动化内核生成，显著降低了新AI硬件的可编程性障碍。

Abstract: Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.

</details>


### [32] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: LANCA提出了一种基于加性噪声模型的无监督因果发现方法，通过确定性WAE架构和可微分ANM层，将残差独立性从被动假设转化为显式优化目标，在合成和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督表示学习通常依赖统计独立性，但难以捕捉因果依赖关系。因果变量从观测数据中解耦需要监督、辅助信号或强归纳偏置。现有方法在复杂场景下存在局限性。

Method: 提出LANCA（潜在加性噪声模型因果自编码器），将加性噪声模型作为强归纳偏置。采用确定性Wasserstein自编码器替代VAE，结合可微分ANM层，显式优化残差独立性目标。

Result: 理论上证明ANM约束将允许变换从任意微分同胚限制到仿射类。实验表明LANCA在合成物理基准（Pendulum、Flow）和真实环境（CANDLE）上优于现有方法，对复杂背景中的伪相关具有更强鲁棒性。

Conclusion: LANCA通过将加性噪声模型作为归纳偏置，实现了无监督因果发现，解决了传统方法在复杂场景下的局限性，为因果表示学习提供了新思路。

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [33] [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)
*Jiesong Lian,Ruizhe Zhong,Zixiang Zhou,Xiaoyue Mi,Yixue Hao,Yuan Zhou,Qinglin Lu,Long Hu,Junchi Yan*

Main category: cs.LG

TL;DR: SoliReward是一个系统性的视频奖励模型训练框架，通过单项目二元标注收集高质量数据，采用分层渐进查询注意力架构，改进BT损失函数来处理平局情况，从而提升视频生成模型的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型的后训练对齐需要有效的奖励模型，但当前方法面临数据标注噪声、架构设计不足和奖励黑客攻击等挑战，需要系统性的解决方案。

Method: 1. 通过单项目二元标注收集高质量低成本数据，采用跨提示配对策略构建偏好对；2. 使用分层渐进查询注意力机制增强特征聚合；3. 引入改进的BT损失函数，显式处理平局情况，正则化奖励分数分布。

Result: 在评估物理合理性、主体变形和语义对齐的基准测试中，该方法在直接RM评估指标和视频生成模型后训练效果方面都表现出改进。

Conclusion: SoliReward框架通过系统化的数据收集、架构设计和损失函数改进，有效解决了视频奖励模型训练中的关键问题，为视频生成模型的对齐提供了更可靠的解决方案。

Abstract: Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.

</details>


### [34] [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)
*Chuanting Zhang,Haixia Zhang,Jingping Qiao,Zongzhang Li,Mohamed-Slim Alouini*

Main category: cs.LG

TL;DR: TIDES是一个基于大语言模型的无线流量预测框架，通过聚类机制识别区域异质性流量模式，并引入DeepSeek模块实现空间对齐，显著提升了城市无线流量预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络对智能自适应资源管理的需求日益增长，需要准确可扩展的无线流量预测。现有深度学习和基础模型虽然展现了预测潜力，但大多忽视了城市尺度流量动态中固有的空间依赖性。

Method: 1) 通过聚类机制识别区域异质性流量模式，为每个区域训练个性化模型；2) 引入提示工程方案，将统计流量特征作为结构化输入嵌入；3) 设计DeepSeek模块，通过跨域注意力实现空间对齐，使LLM能够利用空间相关区域的信息；4) 仅微调轻量级组件，冻结核心LLM层，实现高效领域适应。

Result: 在真实世界蜂窝流量数据集上的大量实验表明，TIDES在预测准确性和鲁棒性方面显著优于最先进的基线方法。

Conclusion: 将空间感知集成到基于LLM的预测器中是解锁未来6G系统中可扩展智能网络管理的关键。

Abstract: The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demonstrated promising forecasting capabilities, they largely overlook the spatial dependencies inherent in city-scale traffic dynamics. In this paper, we propose TIDES (Traffic Intelligence with DeepSeek-Enhanced Spatial-temporal prediction), a novel LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. TIDES first identifies heterogeneous traffic patterns across regions through a clustering mechanism and trains personalized models for each region to balance generalization and specialization. To bridge the domain gap between numerical traffic data and language-based models, we introduce a prompt engineering scheme that embeds statistical traffic features as structured inputs. Furthermore, we design a DeepSeek module that enables spatial alignment via cross-domain attention, allowing the LLM to leverage information from spatially related regions. By fine-tuning only lightweight components while freezing core LLM layers, TIDES achieves efficient adaptation to domain-specific patterns without incurring excessive training overhead. Extensive experiments on real-world cellular traffic datasets demonstrate that TIDES significantly outperforms state-of-the-art baselines in both prediction accuracy and robustness. Our results indicate that integrating spatial awareness into LLM-based predictors is the key to unlocking scalable and intelligent network management in future 6G systems.

</details>


### [35] [Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection](https://arxiv.org/abs/2512.22179)
*Rajeeb Thapa Chhetri,Zhixiong Chen,Saurab Thapa*

Main category: cs.LG

TL;DR: 提出Latent Sculpting框架解决监督深度学习在高维表格数据中的泛化崩溃问题，通过两阶段表示学习实现零样本异常检测


<details>
  <summary>Details</summary>
Motivation: 监督深度学习在高维表格数据中存在"泛化崩溃"问题：模型对已知分布学习精确决策边界，但在面对分布外数据时完全失效。作者认为这源于潜在空间缺乏拓扑约束，导致流形扩散，使得新异常与正常数据在统计上无法区分。

Method: 提出Latent Sculpting框架：第一阶段使用混合1D-CNN和Transformer编码器，结合新颖的双中心紧凑性损失，将正常流量"雕刻"成低熵超球形簇；第二阶段基于此预结构化流形，使用掩码自回归流学习精确密度估计。

Result: 在CIC-IDS-2017基准测试中，监督基线在未见分布偏移上F1约0.30，最强无监督基线仅0.76，而本框架达到0.87。在"渗透"场景中检测率达88.89%，而最先进的监督模型准确率为0.00%。

Conclusion: 显式流形雕刻是实现鲁棒零样本泛化的前提条件，将结构学习与密度估计解耦为广义异常检测提供了可扩展路径。

Abstract: A fundamental limitation of supervised deep learning in high-dimensional tabular domains is "Generalization Collapse": models learn precise decision boundaries for known distributions but fail catastrophically when facing Out-of-Distribution (OOD) data. We hypothesize that this failure stems from the lack of topological constraints in the latent space, resulting in diffuse manifolds where novel anomalies remain statistically indistinguishable from benign data. To address this, we propose Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 utilizes a hybrid 1D-CNN and Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL) to actively "sculpt" benign traffic into a low-entropy, hyperspherical cluster. Unlike standard contrastive losses that rely on triplet mining, DCCL optimizes global cluster centroids to enforce absolute manifold density. Stage 2 conditions a Masked Autoregressive Flow (MAF) on this pre-structured manifold to learn an exact density estimate. We evaluate this methodology on the rigorous CIC-IDS-2017 benchmark, treating it as a proxy for complex, non-stationary data streams. Empirical results demonstrate that explicit manifold sculpting is a prerequisite for robust zero-shot generalization. While supervised baselines suffered catastrophic performance collapse on unseen distribution shifts (F1 approx 0.30) and the strongest unsupervised baseline achieved only 0.76, our framework achieved an F1-Score of 0.87 on strictly zero-shot anomalies. Notably, we report an 88.89% detection rate on "Infiltration" scenarios--a complex distributional shift where state-of-the-art supervised models achieved 0.00% accuracy. These findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection.

</details>


### [36] [Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL](https://arxiv.org/abs/2512.23310)
*Abolfazl Younesi,Abbas Shabrang Maryan,Elyas Oustad,Zahra Najafabadi Samani,Mohsen Ansari,Thomas Fahringer*

Main category: cs.LG

TL;DR: Splitwise：基于Lyapunov辅助深度强化学习的LLM细粒度自适应边缘-云分区框架，通过将Transformer层分解为注意力头和前馈子块，在动态网络条件下联合优化延迟、能耗和精度，相比现有方法显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型面临内存和功耗限制，纯云端推理延迟高且成本大，静态分区方案无法适应带宽波动，需要一种能够动态适应网络条件并同时优化多个指标的细粒度分区方案。

Method: 提出Splitwise框架：1）将Transformer层分解为注意力头和前馈子块，提供比层级分区更细粒度的选择；2）采用Lyapunov辅助的层次化深度强化学习策略，联合优化延迟、能耗和精度退化；3）通过分区检查点和指数退避恢复机制保证通信故障时的鲁棒性。

Result: 在Jetson Orin NX、Galaxy S23和Raspberry Pi 5设备上测试GPT-2 (1.5B)、LLaMA-7B和LLaMA-13B模型，相比现有分区器：端到端延迟降低1.4-2.8倍，能耗降低高达41%；相比纯云端执行：95%分位数延迟降低53-61%，同时保持精度和适中的内存需求。

Conclusion: Splitwise通过细粒度的自适应边缘-云分区，在动态网络条件下有效解决了LLM部署的延迟、能耗和精度平衡问题，为资源受限的边缘设备提供了高效的LLM推理解决方案。

Abstract: Deploying large language models (LLMs) on edge devices is challenging due to their limited memory and power resources. Cloud-only inference reduces device burden but introduces high latency and cost. Static edge-cloud partitions optimize a single metric and struggle when bandwidth fluctuates. We propose Splitwise, a novel Lyapunov-assisted deep reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of LLMs across edge and cloud environments. Splitwise decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing more partition choices than layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Splitwise also guarantees robustness via partition checkpoints with exponential backoff recovery in case of communication failures. Experiments on Jetson Orin NX, Galaxy S23, and Raspberry Pi 5 with GPT-2 (1.5B), LLaMA-7B, and LLaMA-13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x and cuts energy consumption by up to 41% compared with existing partitioners. It lowers the 95th-percentile latency by 53-61% relative to cloud-only execution, while maintaining accuracy and modest memory requirements.

</details>


### [37] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: 使用DDQN和课程学习的强化学习框架优化网球策略，在模拟环境中实现高胜率，但策略偏向防守型


<details>
  <summary>Details</summary>
Motivation: 网球策略优化是一个复杂的序列决策问题，涉及分层计分、随机结果、长视野信用分配、体力疲劳和对手技能适应等挑战，需要开发有效的强化学习方法

Method: 构建自定义网球模拟环境，集成Dueling Double Deep Q-Network(DDQN)并使用课程学习训练，环境包含完整计分系统、10种战术动作、对称疲劳动态和连续对手技能参数

Result: 训练后的智能体对平衡对手胜率达98-100%，发球效率63.0-67.5%，回球效率52.8-57.1%，但战术分析显示策略有明显防守偏向

Conclusion: DDQN和课程学习对稳定收敛至关重要，但胜率驱动的优化在简化体育模拟中存在局限性，强调了奖励设计对现实体育强化学习的重要性

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [38] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification](https://arxiv.org/abs/2512.22189)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文是系列论文的第二部分，重点介绍如何将物理知识和不确定性量化整合到机器学习中，用于电力变压器的健康评估。


<details>
  <summary>Details</summary>
Motivation: 将基于物理的知识与机器学习模型相结合，对于电力变压器的监测、诊断和预测至关重要。第一部分介绍了神经网络及其变体的基础，而第二部分需要解决如何将物理约束和不确定性量化整合到学习过程中，以提高模型的可靠性和鲁棒性。

Method: 1. 介绍物理信息神经网络(PINNs)的基础，应用于时空热建模和固体绝缘老化分析；2. 提出贝叶斯PINNs作为量化认知不确定性的原则性框架，在稀疏数据下提供稳健预测；3. 概述新兴研究方向。

Result: 论文提出了一个完整的框架：从基础的PINNs到贝叶斯PINNs，能够将物理约束整合到机器学习中，并量化模型的不确定性，特别是在数据稀疏的情况下仍能提供可靠预测。

Conclusion: 物理感知和可信赖的机器学习方法对于关键电力资产的健康评估具有巨大潜力，通过整合物理知识和不确定性量化，可以显著提高变压器监测、诊断和预测的可靠性和实用性。

Abstract: The integration of physics-based knowledge with machine learning models is increasingly shaping the monitoring, diagnostics, and prognostics of electrical transformers. In this two-part series, the first paper introduced the foundations of Neural Networks (NNs) and their variants for health assessment tasks. This second paper focuses on integrating physics and uncertainty into the learning process. We begin with the fundamentals of Physics-Informed Neural Networks (PINNs), applied to spatiotemporal thermal modeling and solid insulation ageing. Building on this, we present Bayesian PINNs as a principled framework to quantify epistemic uncertainty and deliver robust predictions under sparse data. Finally, we outline emerging research directions that highlight the potential of physics-aware and trustworthy machine learning for critical power assets.

</details>


### [39] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants](https://arxiv.org/abs/2512.22190)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文是两篇系列论文的第一部分，探讨神经网络及其扩展在电力变压器状态监测和健康管理中的应用，介绍了CNN用于多模态数据监测以及NN与强化学习的集成。


<details>
  <summary>Details</summary>
Motivation: 电力变压器是电网关键资产，传统基于规则或纯物理的方法难以应对不确定性、数据有限性和现代运行条件的复杂性，需要机器学习方法来提升诊断、预测和控制能力。

Method: 本文首先介绍神经网络基本概念，然后探讨卷积神经网络（CNN）用于多模态数据的状态监测，并讨论将神经网络概念集成到强化学习（RL）范式中进行决策和控制。

Result: 论文提供了神经网络在变压器状态监测中的系统框架，展示了CNN处理多样化数据模态的能力，以及NN-RL集成在决策控制方面的潜力。

Conclusion: 神经网络及其扩展为变压器状态监测和健康管理提供了强大的补充工具，能够克服传统方法的局限性，并指出了新兴研究方向。

Abstract: Power transformers are critical assets in power networks, whose reliability directly impacts grid resilience and stability. Traditional condition monitoring approaches, often rule-based or purely physics-based, struggle with uncertainty, limited data availability, and the complexity of modern operating conditions. Recent advances in machine learning (ML) provide powerful tools to complement and extend these methods, enabling more accurate diagnostics, prognostics, and control. In this two-part series, we examine the role of Neural Networks (NNs) and their extensions in transformer condition monitoring and health management tasks. This first paper introduces the basic concepts of NNs, explores Convolutional Neural Networks (CNNs) for condition monitoring using diverse data modalities, and discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control. Finally, perspectives on emerging research directions are also provided.

</details>


### [40] [Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks](https://arxiv.org/abs/2512.22192)
*Jiahao Lu*

Main category: cs.LG

TL;DR: 该研究从信号处理视角分析深度学习正则化的频谱偏置机制，发现L2正则化通过抑制高频能量积累实现低通滤波效果，揭示了准确性与鲁棒性之间的频谱权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管L2正则化和Dropout等正则化技术是训练深度神经网络的基础，但它们在特征频率选择方面的物理机制仍不清楚。研究者希望从频谱偏置角度理解正则化如何影响卷积神经网络的泛化能力。

Method: 提出视觉诊断框架追踪训练过程中权重频率的动态演化；引入频谱抑制比(SSR)量化不同正则器的"低通滤波"强度；通过离散径向剖面解决小卷积核(如3x3)的混叠问题；在ResNet-18和CIFAR-10上进行实证分析。

Result: L2正则化相比无正则化基线将高频能量积累抑制超过3倍；揭示了关键的准确性与鲁棒性权衡：L2模型对宽带高斯噪声敏感（因过度专注于低频），但在高频信息丢失场景（如低分辨率模糊）中表现更优，在模糊场景中优于基线超过6%。

Conclusion: 正则化强制了强烈的频谱归纳偏置，使模型偏向低频结构；从信号处理视角为泛化提供了新理解，确认正则化通过频谱选择机制影响模型性能。

Abstract: Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.

</details>


### [41] [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)
*Dhruv Tiwari*

Main category: cs.LG

TL;DR: 论文提出情感启发学习信号（EILS）框架，用生物情感类比作为内稳态控制机制，替代传统外部奖励函数，以提升AI在开放环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖外部定义的静态奖励函数，在封闭环境中表现出色但在开放环境中脆弱。标准智能体缺乏内部自主性：难以在没有密集反馈时探索、无法适应分布变化、需要大量手动调参。需要一种类似生物情感的内稳态控制机制来解决这些问题。

Method: 提出情感启发学习信号（EILS）框架，将情感建模为连续的内稳态评估信号（如好奇心、压力、信心），这些信号从交互历史中推导为向量值内部状态，实时动态调节智能体的优化景观：好奇心调节熵防止模式崩溃，压力调节可塑性克服不活跃，信心适应信任区域稳定收敛。

Result: 论文假设这种闭环内稳态调节能使EILS智能体在样本效率和非平稳适应方面优于标准基线方法。

Conclusion: EILS框架为AI自主性提供了生物启发的内稳态控制机制，有望解决传统外部奖励函数方法在开放环境中的脆弱性问题，实现更鲁棒的学习和适应能力。

Abstract: The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.

</details>


### [42] [Transformer Reconstructed with Dynamic Value Attention](https://arxiv.org/abs/2512.22212)
*Xiaowei Wang*

Main category: cs.LG

TL;DR: 提出动态值注意力(DVA)方法，用单头注意力替代多头注意力，动态为每个查询生成值，可节省37.6%训练时间并提升学习能力


<details>
  <summary>Details</summary>
Motivation: Transformer的主要内在限制是每个注意力头中使用相同的静态值，多头注意力试图解决但受复杂度限制。需要更高效的方法动态决定每个查询的值

Method: 提出动态值注意力(DVA)，为每个查询动态生成值，从而可以削减所有冗余的注意力头，只保留一个头。后续的前馈网络也可完全削减，因为每个修订后的嵌入已经获取了足够的有用信息

Result: DVA可以节省37.6%的训练时间，同时提高学习能力。实验表明单头动态值注意力足以替代传统Transformer结构

Conclusion: 动态值注意力(DVA)是Transformer中唯一需要的组件，能够解决静态值限制问题，显著提高效率和学习能力

Abstract: Since transformer was firstly published in 2017, several works have been proposed to optimize it. However, the major structure of transformer remains unchanged, ignoring one of its main intrinsic limitations, which is the same static value is used for every query in a head. Transformer itself tries to solve this problem by implementing multi-head attentions, yet the number of heads is limited by complexity. I propose a method to decide a value for each query dynamically, which could cut down all the redundant heads, keeping only one. Consequently, the following feed forward network could be cut down entirely, as each revised embedding has already fetched enough useful values far beyond the context. As a result, a single-head Dynamic Value Attention (DVA) is all you need in a transformer. According to the experiment, DVA may save 37.6% training time than the original transformer meanwhile increasing the learning capability.

</details>


### [43] [On the Existence and Behaviour of Secondary Attention Sinks](https://arxiv.org/abs/2512.22213)
*Jeffrey T. H. Wong,Cheng Zhang,Louis Mahon,Wayne Luk,Anton Isopoussu,Yiren Zhao*

Main category: cs.LG

TL;DR: 该论文发现了注意力机制中的"次级汇点"现象，与传统的"主汇点"不同，次级汇点主要出现在中间层，持续时间可变，吸引较小但仍显著的注意力质量。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注注意力汇点（如BOS标记），但发现这些汇点表现出类似BOS标记的性质。本研究旨在识别和分析一类新型的注意力汇点——次级汇点，它们在性质上与主汇点存在根本差异。

Method: 通过对11个模型家族进行广泛实验，分析次级汇点的出现位置、性质、形成机制及其对注意力机制的影响。特别研究了中间层MLP模块如何生成次级汇点，以及这些汇点的持续时间和影响。

Result: 发现：(1) 次级汇点由特定中间层MLP模块形成，这些MLP将标记表示映射到与该层主汇点方向对齐的向量；(2) 这些向量的ℓ₂范数决定次级汇点的汇点分数及其持续层数；(3) 主汇点在中间层减弱，与次级汇点出现时间重合。在更大规模模型中，汇点层级（位置和持续时间）以更确定和频繁的方式出现。

Conclusion: 本研究识别了一类新型的注意力汇点——次级汇点，揭示了它们在注意力机制中的独特性质和行为模式。这些发现有助于更深入理解大规模语言模型中注意力机制的工作方式，特别是在中间层出现的复杂注意力分配模式。

Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.

</details>


### [44] [Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning](https://arxiv.org/abs/2512.22221)
*Soroush Vahidi*

Main category: cs.LG

TL;DR: 提出一种基于显式组合推理而非深度消息传递的图神经网络框架，在异配性图上表现优异，具有可解释性、可调性和计算效率优势


<details>
  <summary>Details</summary>
Motivation: 传统GNN在同配性图上表现良好，但在异配性图上（相邻节点常属不同类别）表现不佳，需要更适应异配性场景的可解释方法

Method: 使用基于置信度排序的贪婪算法进行节点分类，结合类别先验、邻居统计、特征相似性和训练得到的标签兼容性；引入验证门控混合策略，选择性将组合预测作为先验注入轻量级神经模型

Result: 在异配性和过渡性基准测试中，与现代GNN相比具有竞争力，同时在可解释性、可调性和计算效率方面具有优势

Conclusion: 提出的组合推理框架为异配性图提供了一种有效的替代方案，平衡了性能、可解释性和适应性，特别适用于需要透明决策的场景

Abstract: Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.
  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

</details>


### [45] [Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases](https://arxiv.org/abs/2512.22222)
*Gnankan Landry Regis N'guessan*

Main category: cs.LG

TL;DR: 提出Müntz-Szász Networks (MSN)，用可学习的分数幂基函数替代固定激活函数，专门用于逼近具有奇异或分数幂行为的函数，在物理问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准神经网络使用固定激活函数（ReLU、tanh、sigmoid）不适合逼近具有奇异或分数幂行为的函数，这类函数在物理问题中普遍存在，如边界层、断裂力学和角点奇异性。

Method: 提出Müntz-Szász Networks (MSN)架构，用可学习的分数幂基函数替代固定激活函数：φ(x) = Σa_k|x|^μ_k + Σb_k sign(x)|x|^λ_k，其中指数{μ_k, λ_k}与系数一同学习。

Result: MSN在奇异目标函数的监督回归中，比MLP误差降低5-8倍且参数减少10倍；在PINN基准测试（奇异ODE和刚性边界层问题）中，性能提升3-6倍，并能学习到与已知解结构匹配的可解释指数。

Conclusion: 理论指导的架构设计能为科学驱动的函数类带来显著改进，MSN在逼近奇异函数方面具有理论保证和实际优势。

Abstract: Standard neural network architectures employ fixed activation functions (ReLU, tanh, sigmoid) that are poorly suited for approximating functions with singular or fractional power behavior, a structure that arises ubiquitously in physics, including boundary layers, fracture mechanics, and corner singularities. We introduce Müntz-Szász Networks (MSN), a novel architecture that replaces fixed smooth activations with learnable fractional power bases grounded in classical approximation theory. Each MSN edge computes $φ(x) = \sum_k a_k |x|^{μ_k} + \sum_k b_k \mathrm{sign}(x)|x|^{λ_k}$, where the exponents $\{μ_k, λ_k\}$ are learned alongside the coefficients. We prove that MSN inherits universal approximation from the Müntz-Szász theorem and establish novel approximation rates: for functions of the form $|x|^α$, MSN achieves error $\mathcal{O}(|μ- α|^2)$ with a single learned exponent, whereas standard MLPs require $\mathcal{O}(ε^{-1/α})$ neurons for comparable accuracy. On supervised regression with singular target functions, MSN achieves 5-8x lower error than MLPs with 10x fewer parameters. Physics-informed neural networks (PINNs) represent a particularly demanding application for singular function approximation; on PINN benchmarks including a singular ODE and stiff boundary-layer problems, MSN achieves 3-6x improvement while learning interpretable exponents that match the known solution structure. Our results demonstrate that theory-guided architectural design can yield dramatic improvements for scientifically-motivated function classes.

</details>


### [46] [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)
*Shaghayegh Shajarian,Kennedy Marsh,James Benson,Sajad Khorsandroo,Mahmoud Abdelsalam*

Main category: cs.LG

TL;DR: ReGAIN是一个结合流量摘要、检索增强生成和LLM推理的多阶段网络流量分析框架，在真实流量数据集上达到95.95%-98.82%的准确率，优于传统方法并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统网络流量分析系统（无论是基于规则还是机器学习）存在高误报率和缺乏可解释性的问题，限制了分析师的信任。现代网络产生大量异构流量，需要持续分析以保障安全和性能。

Method: ReGAIN采用多阶段框架：1) 从网络流量创建自然语言摘要；2) 将摘要嵌入多集合向量数据库；3) 使用分层检索管道（包括元数据过滤、MMR采样、两阶段交叉编码器重排序和弃权机制）来为LLM响应提供证据引用；4) 减少幻觉并确保基于证据的推理。

Result: 在真实流量数据集（ICMP ping flood和TCP SYN flood）上评估，ReGAIN在不同攻击类型和评估基准上实现了95.95%到98.82%的准确率。结果通过数据集真实标签和人类专家评估双重验证，优于基于规则、经典机器学习和深度学习的基线方法。

Conclusion: ReGAIN通过结合流量摘要、检索增强生成和LLM推理，实现了透明且准确的网络流量分析，在保持高性能的同时提供了独特的可解释性，通过可信、可验证的响应增强了分析师信任。

Abstract: Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust. In this paper, we present ReGAIN, a multi-stage framework that combines traffic summarization, retrieval-augmented generation (RAG), and Large Language Model (LLM) reasoning for transparent and accurate network traffic analysis. ReGAIN creates natural-language summaries from network traffic, embeds them into a multi-collection vector database, and utilizes a hierarchical retrieval pipeline to ground LLM responses with evidence citations. The pipeline features metadata-based filtering, MMR sampling, a two-stage cross-encoder reranking mechanism, and an abstention mechanism to reduce hallucinations and ensure grounded reasoning. Evaluated on ICMP ping flood and TCP SYN flood traces from the real-world traffic dataset, it demonstrates robust performance, achieving accuracy between 95.95% and 98.82% across different attack types and evaluation benchmarks. These results are validated against two complementary sources: dataset ground truth and human expert assessments. ReGAIN also outperforms rule-based, classical ML, and deep learning baselines while providing unique explainability through trustworthy, verifiable responses.

</details>


### [47] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: DiRL是一个针对扩散语言模型的高效后训练框架，通过FlexAttention加速块训练和LMDeploy优化推理，结合DiPO策略优化，在数学推理任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案，其后训练方法存在计算效率低、训练与推理目标不匹配的问题，限制了在复杂推理任务（如数学）上的性能。

Method: 提出DiRL框架：1）集成FlexAttention加速的块训练和LMDeploy优化的推理；2）支持高效的两阶段后训练（监督微调+强化学习）；3）提出DiPO，首个为扩散语言模型定制的无偏组相对策略优化实现。

Result: 训练DiRL-8B-Instruct模型，在数学任务上达到扩散语言模型中的SOTA性能，并在多个基准测试中超越Qwen2.5系列可比模型。

Conclusion: DiRL框架有效解决了扩散语言模型后训练的效率问题，DiPO优化方法显著提升了模型在复杂推理任务上的性能，为扩散语言模型的实际应用提供了有力支持。

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [48] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: 提出Masters框架，通过掩码渐进强化学习蒸馏，解决大模型向小模型知识迁移时因尺寸差距导致的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型(VLMs)虽然性能优异，但体积过大难以部署到移动或边缘设备。需要紧凑但能力强的VLMs，但大教师模型向小学生模型的知识蒸馏面临挑战：尺寸差距导致学生模型难以复现教师模型的复杂高维表示，造成学习不稳定和性能下降。

Method: 提出Masters框架：1) 掩码教师非主导权重以减少复杂度；2) 渐进式恢复教师容量，让学生平滑稳定地学习丰富表示；3) 离线强化学习阶段结合两种奖励：准确性奖励和蒸馏奖励；4) 利用掩码教师预生成响应提供高效指导，避免计算昂贵的在线思考-回答过程。

Result: Masters框架使学生模型能够从大教师模型高效学习，实现强性能而不需要复杂的思考-回答过程，解决了尺寸差距导致的知识迁移难题。

Conclusion: Masters通过掩码渐进强化学习蒸馏，有效解决了大规模视觉语言模型向紧凑模型知识迁移的挑战，为移动和边缘设备部署提供了可行的解决方案。

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [49] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: EvoXplain框架通过重复训练检测模型解释的稳定性，发现即使高精度模型也可能存在多种不同的解释机制，挑战了单一解释的可信度假设。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习实践中存在一个被忽视的问题：当两个模型都达到高准确率时，它们是否基于相同的内部逻辑？还是通过不同甚至相互竞争的机制达到相同结果？这挑战了"高准确率意味着解释正确可信"的常见假设。

Method: EvoXplain将解释视为从随机优化过程中抽取的样本，而不是分析单个训练模型。它通过重复训练同一数据分割，检查解释样本是否形成单一连贯解释，还是分离成多个不同的解释模式。在乳腺癌和COMPAS数据集上使用逻辑回归和随机森林进行评估。

Result: 尽管所有模型都达到高预测准确率，但它们的解释经常表现出明显的多模态性。即使是通常被认为稳定的逻辑回归模型，在同一数据分割的重复训练下也能产生多个明显分离的解释盆地。这些差异不能由超参数变化或简单的性能权衡来解释。

Conclusion: EvoXplain不试图选择"正确"解释，而是使解释的不稳定性变得可见和可量化，揭示单一实例或平均解释可能掩盖多种底层机制的存在。更广泛地说，它将可解释性重新定义为模型类在重复实例化下的属性，而不是任何单个训练模型的属性。

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [50] [Enhanced geometry prediction in laser directed energy deposition using meta-learning](https://arxiv.org/abs/2512.22241)
*Abdul Malik Al Mardhouf Al Saadi,Amrita Basak*

Main category: cs.LG

TL;DR: 提出基于元学习的跨数据集知识迁移模型，用于激光定向能量沉积（L-DED）中熔道几何形状预测，能在少量数据下快速适应新沉积条件。


<details>
  <summary>Details</summary>
Motivation: 激光定向能量沉积中熔道几何形状预测常受限于实验数据稀缺性和异质性（不同材料、设备配置、工艺参数），需要解决跨数据集知识迁移问题。

Method: 采用两种基于梯度的元学习算法：模型无关元学习（MAML）和Reptile，构建跨数据集知识迁移框架，使用文献和内部实验的多数据集，涵盖粉末送料、线材送料和混合送料L-DED工艺。

Result: MAML和Reptile仅需3-9个训练样本就能在未见目标任务上实现准确熔道高度预测，性能优于传统前馈神经网络；在多个目标任务中R²值达约0.9，平均绝对误差0.03-0.08mm。

Conclusion: 元学习方法能有效实现跨异质L-DED设置的知识迁移，在数据稀缺条件下仍能保持强泛化性能，为L-DED工艺优化提供有效工具。

Abstract: Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.

</details>


### [51] [Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening](https://arxiv.org/abs/2512.22242)
*Shaurya Gaur,Michel Vitale,Alessa Hering,Johan Kwisthout,Colin Jacobs,Lena Philipp,Fennie van der Graaf*

Main category: cs.LG

TL;DR: 该研究使用JustEFAB框架评估了两种肺癌筛查深度学习模型（Sybil和Venkadesh21）在人口亚组中的公平性，发现存在显著的性能差异，这些差异无法用临床混杂因素解释，可能构成不公平偏见。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症相关死亡的主要原因，低剂量CT筛查可早期发现并降低死亡率，但广泛实施可能使放射科医生资源紧张。AI模型在肺癌风险评估中显示出潜力，但高危人群多样化，这些模型在不同人口群体中的性能表现仍不明确，需要评估其公平性和潜在偏见。

Method: 使用JustEFAB框架评估两种深度学习肺癌风险评估模型（Sybil和Venkadesh21）以及PanCan2b逻辑回归模型。模型基于美国国家肺癌筛查试验（NLST）数据训练，在保留的NLST验证集上评估。评估了AUROC、敏感性和特异性在不同人口亚组（性别、种族）中的差异，并探讨了临床风险因素的潜在混杂影响。

Result: Sybil模型在女性（AUROC 0.88）和男性（AUROC 0.81）之间存在统计学显著性能差异（p < .001）。Venkadesh21模型在90%特异性下，黑人参与者敏感性（0.39）显著低于白人参与者（0.69）。这些差异无法用可用的临床混杂因素解释，根据JustEFAB框架可能构成不公平偏见。

Conclusion: 研究发现肺癌筛查AI模型在不同人口亚组中存在显著性能差异，这些差异可能构成不公平偏见。研究强调了改进和监测模型在代表性不足亚组中性能的重要性，以及进一步研究算法公平性在肺癌筛查中的必要性。

Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.

</details>


### [52] [Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning](https://arxiv.org/abs/2512.22243)
*Alan Inglis,Fiona Doohan,Subramani Natarajan,Breige McNulty,Chris Elliott,Anne Nugent,Julie Meneely,Brett Greer,Stephen Kildea,Diana Bucur,Martin Danaher,Melissa Di Rocco,Lisa Black,Adam Gauley,Naoise McKenna,Andrew Parnell*

Main category: cs.LG

TL;DR: 该研究使用神经网络和迁移学习模型预测爱尔兰燕麦作物中的霉菌毒素污染，发现TabPFN迁移学习方法表现最佳，收获前90天的天气模式是最重要的预测因子。


<details>
  <summary>Details</summary>
Motivation: 霉菌毒素污染对谷物质量、食品安全和农业生产构成重大风险。准确预测霉菌毒素水平可以支持早期干预策略并减少经济损失。

Method: 研究评估了五种建模方法：基线多层感知器（MLP）、带预训练的MLP，以及三种迁移学习模型（TabPFN、TabNet、FT-Transformer）。使用爱尔兰燕麦样本数据集，包含环境、农艺和地理预测因子。通过回归和分类指标评估性能，并进行基于排列的变量重要性分析。

Result: 迁移学习方法TabPFN提供了整体最佳性能，其次是基线MLP。变量重要性分析显示，收获前90天的天气历史模式是最重要的预测因子，其次是种子含水量。

Conclusion: 迁移学习模型特别是TabPFN在预测霉菌毒素污染方面表现优异，天气模式在收获前阶段是关键预测因素，这为早期干预和风险管理提供了重要见解。

Abstract: Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression (RMSE, $R^2$) and classification (AUC, F1) metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.

</details>


### [53] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: 使用线性探针从LLM推理法官的隐藏状态获取校准的不确定性估计，无需额外训练，计算效率高且校准性能优于现有方法


<details>
  <summary>Details</summary>
Motivation: LLM法官在工业应用中越来越重要，需要高效获取良好校准的不确定性估计。现有方法（如口头化置信度和多生成方法）要么校准效果差，要么计算成本高

Method: 引入基于Brier分数的损失函数训练的线性探针，从推理法官的隐藏状态提供校准的不确定性估计，无需额外模型训练

Result: 探针在客观任务（推理、数学、事实性、编码）和主观人类偏好判断上均表现优异：校准性能优于现有方法，计算节省约10倍，对未见评估域具有鲁棒泛化能力，高置信度预测准确率更高

Conclusion: 基于可解释性的不确定性估计为生产中的LLM法官提供了实用、可扩展的即插即用解决方案，虽然会产生保守估计（在简单数据集上表现不佳，但可能有利于安全关键部署中优先考虑低误报率）

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [54] [The Affine Divergence: Aligning Activation Updates Beyond Normalisation](https://arxiv.org/abs/2512.22247)
*George Bird*

Main category: cs.LG

TL;DR: 论文提出激活更新在梯度下降中存在系统性不匹配问题，通过数学分析推导出归一化的新理论框架，并提出替代传统归一化的新方法


<details>
  <summary>Details</summary>
Motivation: 激活更新在梯度下降中未沿最速下降方向更新，而激活比参数更直接影响损失函数，这种不匹配需要理论解释和解决方案

Method: 通过数学分析激活更新的缩放问题，从第一性原理推导归一化机制，提出两种新方法：替代仿射映射的解决方案和卷积层的"PatchNorm"

Result: 提出的新方法在多个测试中优于传统归一化方法，为归一化提供了新的理论框架和替代方案

Conclusion: 归一化应被重新理解为具有参数化缩放的类激活函数映射，这挑战了传统的仿射+非线性模型构建方法

Abstract: A systematic mismatch exists between mathematically ideal and effective activation updates during gradient descent. As intended, parameters update in their direction of steepest descent. However, activations are argued to constitute a more directly impactful quantity to prioritise in optimisation, as they are closer to the loss in the computational graph and carry sample-dependent information through the network. Yet their propagated updates do not take the optimal steepest-descent step. These quantities exhibit non-ideal sample-wise scaling across affine, convolutional, and attention layers. Solutions to correct for this are trivial and, entirely incidentally, derive normalisation from first principles despite motivational independence. Consequently, such considerations offer a fresh and conceptual reframe of normalisation's action, with auxiliary experiments bolstering this mechanistically. Moreover, this analysis makes clear a second possibility: a solution that is functionally distinct from modern normalisations, without scale-invariance, yet remains empirically successful, outperforming conventional normalisers across several tests. This is presented as an alternative to the affine map. This generalises to convolution via a new functional form, "PatchNorm", a compositionally inseparable normaliser. Together, these provide an alternative mechanistic framework that adds to, and counters some of, the discussion of normalisation. Further, it is argued that normalisers are better decomposed into activation-function-like maps with parameterised scaling, thereby aiding the prioritisation of representations during optimisation. Overall, this constitutes a theoretical-principled approach that yields several new functions that are empirically validated and raises questions about the affine + nonlinear approach to model creation.

</details>


### [55] [Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation](https://arxiv.org/abs/2512.22248)
*Rohit Pandey,Rohan Pandey*

Main category: cs.LG

TL;DR: 使用基于模拟的摊销推理方法，通过神经网络从合成飞行数据学习，无需真实飞行数据即可预测火箭空气动力学参数，实现零真实训练样本的模拟到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 传统火箭飞行性能预测方法存在局限性：计算流体动力学或经验相关性方法复杂，数据驱动方法需要大量昂贵且耗时的真实飞行数据。需要一种能够利用合成数据学习并直接应用于真实场景的方法。

Method: 提出基于模拟的摊销推理方法：1）使用物理模拟器生成10,000次合成飞行数据；2）训练神经网络学习反转前向物理模型；3）从单个远地点测量值结合发动机和配置特征直接预测阻力系数和推力修正因子；4）无需微调即可应用于真实飞行。

Result: 在8次真实飞行测试中，远地点预测的平均绝对误差为12.3米，相比OpenRocket基线预测减少了误差。分析显示预测存在系统性正偏差，为理想物理模型与现实飞行条件之间的差距提供了量化见解。

Conclusion: 该方法成功实现了零真实训练样本的模拟到现实迁移，展示了在业余火箭社区的应用潜力。公开实现支持可重复性和采用，为难以直接测量的空气动力学参数估计提供了有效的数据驱动解决方案。

Abstract: Accurate prediction of model rocket flight performance requires estimating aerodynamic parameters that are difficult to measure directly. Traditional approaches rely on computational fluid dynamics or empirical correlations, while data-driven methods require extensive real flight data that is expensive and time-consuming to collect. We present a simulation-based amortized inference approach that trains a neural network on synthetic flight data generated from a physics simulator, then applies the learned model to real flights without any fine-tuning. Our method learns to invert the forward physics model, directly predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. In this proof-of-concept study, we train on 10,000 synthetic flights and evaluate on 8 real flights, achieving a mean absolute error of 12.3 m in apogee prediction - demonstrating promising sim-to-real transfer with zero real training examples. Analysis reveals a systematic positive bias in predictions, providing quantitative insight into the gap between idealized physics and real-world flight conditions. We additionally compare against OpenRocket baseline predictions, showing that our learned approach reduces apogee prediction error. Our implementation is publicly available to support reproducibility and adoption in the amateur rocketry community.

</details>


### [56] [Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models](https://arxiv.org/abs/2512.22249)
*Zheng Xing,Weibing Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种利用大语言模型提取时序视觉语义来增强无监督人体运动分割的方法，通过LLM分析连续帧的运动信息并整合到时序正则化的子空间聚类框架中。


<details>
  <summary>Details</summary>
Motivation: 传统无监督人体运动分割方法忽略了时序语义探索的重要性。本文旨在利用大语言模型的图像到文本能力，从人体运动序列中提取时序视觉语义，以提升子空间聚类的性能。

Method: 1. 使用LLM查询连续帧是否描述相同运动，学习时序相邻信息；2. 开发TVS集成的子空间聚类方法，包含带时序正则化的子空间嵌入；3. 基于时序约束进行分割，使每帧与其时序邻居分组；4. 引入反馈框架，根据分割输出持续优化子空间嵌入。

Result: 在四个人体运动基准数据集上的实验结果表明，该方法优于现有的最先进方法。

Conclusion: 通过将大语言模型提取的时序视觉语义整合到子空间聚类框架中，可以有效提升无监督人体运动分割的性能，证明了时序语义信息在该任务中的重要性。

Abstract: Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.

</details>


### [57] [Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs](https://arxiv.org/abs/2512.22251)
*Pascal Passigan,Kevin zhu,Angelina Ning*

Main category: cs.LG

TL;DR: 提出一个基于生物医学知识图谱和图注意力网络的框架，用于预测药物对基因表达的影响，超越传统的二元药物-疾病关联任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架主要关注知识图谱中的链接预测和二元药物-疾病关联，但缺乏对药物如何扰动基因表达这一更精细机制的研究，而这对于理解药物作用机制、预测副作用和药物重定位至关重要。

Method: 构建融合PrimeKG++和LINCS L1000数据的异质生物医学图谱，使用MolFormerXL和BioBERT等基础模型初始化节点嵌入，然后训练图注意力网络（GAT）来预测药物-细胞对中978个标志基因的表达变化。

Result: 该框架在支架分割和随机分割下均优于多层感知机基线模型，能够更准确地预测差异表达基因。消融实验表明生物医学知识图谱的边信息显著提升了扰动水平预测能力。

Conclusion: 该研究为机制性药物建模提供了新路径，从简单的二元关联任务转向更精细的转录效应预测，有助于深入理解药物作用机制。

Abstract: Understanding how small molecules perturb gene expression is essential for uncovering drug mechanisms, predicting off-target effects, and identifying repurposing opportunities. While prior deep learning frameworks have integrated multimodal embeddings into biomedical knowledge graphs (BKGs) and further improved these representations through graph neural network message-passing paradigms, these models have been applied to tasks such as link prediction and binary drug-disease association, rather than the task of gene perturbation, which may unveil more about mechanistic transcriptomic effects. To address this gap, we construct a merged biomedical graph that integrates (i) PrimeKG++, an augmentation of PrimeKG containing semantically rich embeddings for nodes with (ii) LINCS L1000 drug and cell line nodes, initialized with multimodal embeddings from foundation models such as MolFormerXL and BioBERT. Using this heterogeneous graph, we train a graph attention network (GAT) with a downstream prediction head that learns the delta expression profile of over 978 landmark genes for a given drug-cell pair. Our results show that our framework outperforms MLP baselines for differentially expressed genes (DEG) -- which predict the delta expression given a concatenated embedding of drug features, target features, and baseline cell expression -- under the scaffold and random splits. Ablation experiments with edge shuffling and node feature randomization further demonstrate that the edges provided by biomedical KGs enhance perturbation-level prediction. More broadly, our framework provides a path toward mechanistic drug modeling: moving beyond binary drug-disease association tasks to granular transcriptional effects of therapeutic intervention.

</details>


### [58] [Graph Attention-based Adaptive Transfer Learning for Link Prediction](https://arxiv.org/abs/2512.22252)
*Huashen Lu,Wensheng Gan,Guoting Chen,Zhichao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: GAATNet是一种结合预训练和微调的图注意力自适应迁移网络，通过引入远邻嵌入偏置和轻量自适配器模块，解决了大规模稀疏图处理和跨数据集迁移学习中的对齐问题，在链接预测任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在链接预测任务中面临两大挑战：1) 处理大规模稀疏图时的效率问题；2) 迁移学习中不同数据集需要高度对齐的局限性。同时，自监督方法在图任务中虽成功，但先前研究忽视了跨图数据集迁移学习的潜力。

Method: 提出Graph Attention Adaptive Transfer Network (GAATNet)，结合预训练和微调优势，捕捉不同规模数据集的全局节点嵌入信息。采用两个关键策略：1) 在自注意力模块中引入远邻嵌入作为偏置来捕获全局特征；2) 在微调阶段引入轻量自适配器模块以提高训练效率。

Result: 在七个公开数据集上的综合实验表明，GAATNet在链接预测任务中实现了最先进的性能，为有效整合图神经网络与迁移学习提供了通用且可扩展的解决方案。

Conclusion: GAATNet通过创新的迁移学习框架解决了图神经网络在链接预测中的关键挑战，特别是在处理大规模稀疏图和跨数据集泛化方面表现出色，为图学习领域提供了新的研究方向。

Abstract: Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model's generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet

</details>


### [59] [Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data](https://arxiv.org/abs/2512.22259)
*Daniil Burakov,Ivan Petrov,Dmitrii Khelimskii,Ivan Bessonov,Mikhail Lazarev*

Main category: cs.LG

TL;DR: 该研究开发了一个基于真实和合成数据的机器学习模型，用于预测经皮冠状动脉介入治疗（PCI）后3年心脏死亡风险，通过数据增强改善了少数类别的识别，并识别出年龄、射血分数、外周动脉疾病和脑血管疾病为最重要的预测因素。


<details>
  <summary>Details</summary>
Motivation: 患者状态、血管造影和手术特征对预测PCI后长期结果至关重要。研究旨在开发基于真实和合成数据的预测模型，评估PCI患者心脏死亡风险，并识别对死亡率影响最大的因素。

Method: 分析了2044名接受分叉病变PCI的患者，主要结局为3年随访的心脏死亡。应用多种机器学习模型预测3年死亡率。为解决类别不平衡问题，生成并添加了500个合成样本到训练集。使用排列特征重要性评估个体特征贡献，并进行实验评估移除非信息特征对模型预测的影响。

Result: 未使用过采样时，所有模型总体准确率高（0.92-0.93），但几乎完全忽略少数类别。数据增强后，少数类别召回率持续提高，AUROC损失最小，概率质量改善，对构建的严重风险特征产生更临床合理的风险估计。特征重要性分析显示四个最有影响力的特征：年龄、射血分数、外周动脉疾病和脑血管疾病。

Conclusion: 使用现实和极端案例的简单数据增强可以暴露、量化和减少基于表格记录的临床预测中的脆弱性，并激励在报告主要指标的同时常规报告概率质量和压力测试结果。

Abstract: Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.

</details>


### [60] [The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning](https://arxiv.org/abs/2512.22261)
*Rahul D Ray*

Main category: cs.LG

TL;DR: 研究通过系统消融实验发现物理约束的悖论：显式能量守恒约束在物理方程一致时是冗余的，而法布里-珀罗振荡对带宽预测影响最大，移除后可提升机器学习性能31.3%。


<details>
  <summary>Details</summary>
Motivation: 在科学领域机器学习中，真实数据稀缺，物理约束数据生成至关重要。但现有方法往往过度约束模型，未能识别哪些物理组件是必要的，需要系统评估不同物理约束对数据生成和机器学习性能的影响。

Method: 对物理信息光栅耦合器光谱生成器进行系统消融研究，选择性移除：1) 显式能量守恒约束，2) 法布里-珀罗振荡，3) 带宽变化，4) 噪声。分析不同约束组合对数据生成质量和下游机器学习性能的影响。

Result: 发现物理约束悖论：显式能量守恒约束在物理方程一致时是数学冗余的，约束和非约束变体达到相同的守恒精度（平均误差约7×10⁻⁹）。法布里-珀罗振荡主导带宽变异性，移除后最大半高宽分布减少72%（从132.3nm降至37.4nm）。标准噪声添加加重新归一化流程引入0.5%非物理负吸收值。下游机器学习评估显示物理可学习性权衡：移除法布里-珀罗振荡使带宽预测R²提高31.3%，RMSE降低73.8%。

Conclusion: 研究为物理信息数据集设计提供实用指导，强调机器学习性能可作为评估约束相关性的诊断工具。显式能量守恒约束可能冗余，而法布里-珀罗振荡对带宽预测影响显著，移除后可提升机器学习性能。生成器速度达200样本/秒，比全波求解器快几个数量级。

Abstract: Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.

</details>


### [61] [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/abs/2512.22280)
*Varshith Gudur*

Main category: cs.LG

TL;DR: Valori提出确定性AI内存基板，用定点算术替代浮点运算，确保跨平台内存状态和检索结果的比特一致性


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖浮点运算的向量嵌入存储和搜索，导致跨硬件架构的非确定性：相同模型、输入和代码在不同硬件上产生不同内存状态和检索结果，破坏了可重现性、安全部署和审计追踪

Method: 用定点算术（Q16.16格式）替代浮点内存操作，将内存建模为可重现的状态机，在内存边界强制执行确定性

Result: Valori保证跨平台的比特相同内存状态、快照和搜索结果，非确定性出现在索引或检索之前，确定性内存是可信AI系统的必要基础

Conclusion: 确定性内存是构建可信AI系统的关键基础组件，Valori通过定点算术和状态机方法解决了浮点运算带来的跨平台非确定性问题

Abstract: Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).

</details>


### [62] [LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training](https://arxiv.org/abs/2512.22264)
*Tzamn Melendez Carmona,Federico Marchesin,Marco P. Abrate,Peter Bienstman,Stefano Di Carlo,Alessandro Savino Senior*

Main category: cs.LG

TL;DR: LuxIA框架通过Slicing方法显著提升光子神经网络模拟的可扩展性，降低内存和时间消耗，支持大规模PNN训练


<details>
  <summary>Details</summary>
Motivation: 当前光子神经网络模拟工具面临可扩展性挑战，特别是大规模PNN训练时的计算需求高，导致内存和时间消耗大

Method: 提出Slicing方法，一种高效的传输矩阵计算技术，兼容反向传播，并集成到统一的模拟训练框架LuxIA中

Result: 在各种光子架构和标准数据集（MNIST、Digits、Olivetti Faces）上的实验表明，LuxIA在速度和可扩展性上持续超越现有工具

Conclusion: LuxIA解决了关键计算瓶颈，促进了光子神经网络研究的更广泛采用，为更高效可扩展的光子AI硬件创新铺平道路

Abstract: PNNs present promising opportunities for accelerating machine learning by leveraging the unique benefits of photonic circuits. However, current state of the art PNN simulation tools face significant scalability challenges when training large-scale PNNs, due to the computational demands of transfer matrix calculations, resulting in high memory and time consumption. To overcome these limitations, we introduce the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation. We integrate this method into LuxIA, a unified simulation and training framework. The Slicing method substantially reduces memory usage and execution time, enabling scalable simulation and training of large PNNs. Experimental evaluations across various photonic architectures and standard datasets, including MNIST, Digits, and Olivetti Faces, show that LuxIA consistently surpasses existing tools in speed and scalability. Our results advance the state of the art in PNN simulation, making it feasible to explore and optimize larger, more complex architectures. By addressing key computational bottlenecks, LuxIA facilitates broader adoption and accelerates innovation in AI hardware through photonic technologies. This work paves the way for more efficient and scalable photonic neural network research and development.

</details>


### [63] [Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation](https://arxiv.org/abs/2512.23096)
*Mario Colosi,Reza Farahani,Maria Fazio,Radu Prodan,Massimo Villari*

Main category: cs.LG

TL;DR: OSM-L是一种自监督分布式学习范式，通过渗透过程从分布式数据中提取高层次潜在知识，无需原始数据交换，实现本地表示对齐和信息扩散。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的数据在特定上下文中具有更深层意义，相互依赖的数据源能揭示隐藏关系和潜在结构，这对许多应用具有重要价值。传统方法需要原始数据交换，存在隐私和效率问题。

Method: 提出渗透学习（OSM-L），核心是渗透过程：通过提取上下文信息合成密集紧凑的表示，无需原始数据交换。迭代对齐本地数据表示，实现信息扩散和收敛到动态平衡，捕捉上下文模式。训练过程中识别相关数据组，作为去中心化聚类机制。

Result: 在结构化数据集上验证了OSM-L的收敛性和表示能力，本地信息对齐准确率超过0.99，同时保持上下文完整性。

Conclusion: OSM-L是一种有效的自监督分布式学习范式，能够从分布式数据中提取高层次潜在知识，在保护隐私的同时实现高效的信息融合和表示学习。

Abstract: Data within a specific context gains deeper significance beyond its isolated interpretation. In distributed systems, interdependent data sources reveal hidden relationships and latent structures, representing valuable information for many applications. This paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm designed to uncover higher-level latent knowledge from distributed data. The core of OSM-L is osmosis, a process that synthesizes dense and compact representation by extracting contextual information, eliminating the need for raw data exchange between distributed entities. OSM-L iteratively aligns local data representations, enabling information diffusion and convergence into a dynamic equilibrium that captures contextual patterns. During training, it also identifies correlated data groups, functioning as a decentralized clustering mechanism. Experimental results confirm OSM-L's convergence and representation capabilities on structured datasets, achieving over 0.99 accuracy in local information alignment while preserving contextual integrity.

</details>


### [64] [LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs](https://arxiv.org/abs/2512.22266)
*Bing Hao,Minglai Shao,Zengyi Wo,Yunlong Chu,Yuhang Liu,Ruijie Wang*

Main category: cs.LG

TL;DR: 本文系统研究LLM在时序模体分析中的性能，提出LLMTM基准测试，开发工具增强的LLM代理，并提出结构感知调度器以平衡精度与成本。


<details>
  <summary>Details</summary>
Motivation: LLM在动态图处理中的应用日益广泛，但针对动态图中时序模体分析的研究相对缺乏。时序模体作为动态图的基本单元和重要局部属性，能直接反映异常和独特现象，对理解动态图的演化动态和结构特征至关重要。

Method: 1) 提出LLMTM基准测试，包含6个定制任务和9种时序模体类型；2) 通过大量实验分析不同提示技术和9种LLM模型的影响；3) 开发工具增强的LLM代理；4) 提出结构感知调度器，考虑动态图结构属性和LLM认知负载，智能调度标准LLM提示与更强大代理之间的查询。

Result: 实验表明：1) 工具增强的LLM代理能以高精度解决任务；2) 但高精度带来显著成本；3) 结构感知调度器能有效维持高精度同时降低成本。

Conclusion: 本文系统探索了LLM在时序模体分析中的能力，提出的LLMTM基准和结构感知调度器为平衡精度与成本提供了有效解决方案，推动了LLM在动态图分析领域的应用。

Abstract: The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.

</details>


### [65] [FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs](https://arxiv.org/abs/2512.23235)
*Zihao Zhou,Shusen Yang,Fangyuan Zhao,Xuebin Ren*

Main category: cs.LG

TL;DR: FairGFL算法解决图联邦学习中重叠子图不平衡导致的公平性问题，通过隐私保护的重叠率估计和加权聚合来提升跨客户端公平性，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 图联邦学习中不同客户端之间的子图数据存在重叠，先前研究关注重叠数据缓解数据异质性的好处，但未探索重叠不平衡带来的负面影响。本文发现不平衡重叠子图会导致公平性问题，需要解决这一挑战。

Method: 提出FairGFL算法：1) 采用隐私保护的重叠率估计方法；2) 设计可解释的加权聚合机制来增强跨客户端公平性；3) 在联邦复合损失函数中集成精心设计的正则化器，平衡模型效用与公平性。

Result: 在四个基准图数据集上的实验表明，FairGFL在模型效用和公平性方面均优于四种代表性基线算法。

Conclusion: FairGFL成功解决了图联邦学习中因重叠子图不平衡导致的公平性问题，在保护隐私的同时实现了模型效用与公平性的更好权衡。

Abstract: Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.

</details>


### [66] [Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization](https://arxiv.org/abs/2512.22279)
*Amgad Ahmed Ali Ibrahim,Hein Htet,Ryoji Asahi*

Main category: cs.LG

TL;DR: 提出SoDip分层堆叠优化框架，通过集成Transformer、TabNet/XGBoost、高斯过程回归与狄利克雷过程混合模型，解决辐射诱导接枝聚合中因基膜形态差异导致的重复性问题。


<details>
  <summary>Details</summary>
Motivation: 辐射诱导接枝聚合技术虽能精确功能化聚合物薄膜，但由于基膜形态（结晶度、晶粒取向、自由体积）的未报告变异性，导致单体扩散、自由基分布和Trommsdorff效应存在空间梯度，造成性能不一致和重复性差的问题。

Method: 提出SoDip分层数据驱动框架：1) 使用DeepSeek-R1 Transformer编码文本过程描述符；2) TabNet和XGBoost建模多模态特征交互；3) 高斯过程回归与狄利克雷过程混合模型进行不确定性量化和异方差处理；4) 贝叶斯优化高效探索高维合成空间。

Result: 在交叉验证中，SoDip相比传统高斯过程回归提升约33%，提供校准的置信区间以识别低重复性区域，其堆叠架构能有效整合稀疏文本和数值输入，优于先前模型。

Conclusion: SoDip框架为接枝聚合研究建立了可重复、形态感知设计的基础，通过集成多模态数据和不确定性量化，显著提升了辐射诱导接枝聚合的重复性和性能预测能力。

Abstract: Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.

</details>


### [67] [DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations](https://arxiv.org/abs/2512.22283)
*Guokan Chen,Yao Xiao*

Main category: cs.LG

TL;DR: 提出DBAW-PIKAN方法，结合Kolmogorov-Arnold网络架构与自适应权重策略，解决PINNs在多尺度/高频问题中的梯度流刚度和谱偏差问题，显著提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: PINNs在处理多尺度或高频特征问题时面临梯度流刚度和谱偏差的严重挑战，这些瓶颈限制了其预测能力，需要新的方法来克服这些限制。

Method: 提出动态平衡自适应加权物理信息Kolmogorov-Arnold网络（DBAW-PIKAN），结合基于可学习B样条的Kolmogorov-Arnold网络架构，以及包含动态衰减上界的自适应加权策略。

Result: 与基线模型相比，该方法在不增加计算复杂度的前提下，加速了收敛过程，并将解精度提高了至少一个数量级。在Klein-Gordon、Burgers和Helmholtz方程等数值基准测试中表现出显著优势。

Conclusion: DBAW-PIKAN方法有效缓解了PINNs在多尺度/高频问题中的梯度相关失效模式，克服了函数表示的瓶颈，显著提升了精度和泛化性能。

Abstract: Physics-informed neural networks (PINNs) have led to significant advancements in scientific computing by integrating fundamental physical principles with advanced data-driven techniques. However, when dealing with problems characterized by multi-scale or high-frequency features, PINNs encounter persistent and severe challenges related to stiffness in gradient flow and spectral bias, which significantly limit their predictive capabilities. To address these issues, this paper proposes a Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN), designed to mitigate such gradient-related failure modes and overcome the bottlenecks in function representation. The core of DBAW-PIKAN combines the Kolmogorov-Arnold network architecture, based on learnable B-splines, with an adaptive weighting strategy that incorporates a dynamic decay upper bound. Compared to baseline models, the proposed method accelerates the convergence process and improves solution accuracy by at least an order of magnitude without introducing additional computational complexity. A series of numerical benchmarks, including the Klein-Gordon, Burgers, and Helmholtz equations, demonstrate the significant advantages of DBAW-PIKAN in enhancing both accuracy and generalization performance.

</details>


### [68] [Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation](https://arxiv.org/abs/2512.22287)
*Zikun Guoa,Adeyinka. P. Adedigbaa,Rammohan Mallipeddi*

Main category: cs.LG

TL;DR: 提出Cluster Aggregated GAN框架，通过聚类和分支架构分别处理间歇性和连续性电器，提升合成负载数据的真实性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 非侵入式负载监测需要合成电器数据，但现有GAN方法将所有电器统一处理，忽略了间歇性和连续性电器的行为差异，导致训练不稳定和输出保真度有限。

Method: 提出Cluster Aggregated GAN混合生成框架：1) 根据电器行为特征将电器路由到专门分支；2) 间歇性电器使用聚类模块分组相似激活模式，为每个集群分配专用生成器；3) 连续性电器使用LSTM生成器捕捉时间演化，通过序列压缩保持训练稳定性。

Result: 在UVIC智能插座数据集上的实验表明，该框架在真实性、多样性和训练稳定性指标上均优于基线方法，将聚类作为主动生成组件显著提高了可解释性和可扩展性。

Conclusion: 该框架为非侵入式负载监测研究中的合成负载生成提供了有效方法，通过专门处理不同电器类型的行为特征，解决了现有方法的局限性。

Abstract: Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.

</details>


### [69] [Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model](https://arxiv.org/abs/2512.22288)
*Renping Zhou,Zanlin Ni,Tianyi Chen,Zeyu Liu,Yang Yue,Yulin Wang,Yuxuan Wang,Jingshu Liu,Gao Huang*

Main category: cs.LG

TL;DR: Co-GRPO通过将掩码扩散模型生成重新表述为统一的马尔可夫决策过程，联合优化模型参数和推理调度参数，解决了训练与推理之间的不一致问题。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在训练和推理之间存在显著差异：推理是多步迭代过程，受模型和调度策略共同影响，而训练使用简化的单步BERT式目标，导致推理调度从未在训练中优化。

Method: 将MDM生成重新表述为统一的马尔可夫决策过程，应用轨迹级别的组相对策略优化，在共享奖励下协同优化模型参数和调度参数，无需通过多步生成过程进行昂贵的反向传播。

Result: 在ImageReward、HPS、GenEval和DPG-Bench四个基准测试中，该方法显著提高了生成质量，证明了其有效性。

Conclusion: Co-GRPO通过整体优化使训练与推理更加一致，显著提升了掩码扩散模型的生成性能，为MDM训练提供了更有效的范式。

Abstract: Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .

</details>


### [70] [When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing](https://arxiv.org/abs/2512.22290)
*Arunkumar V,Nivethitha S,Sharan Srinivas,Gangadharan G. R*

Main category: cs.LG

TL;DR: 使用双重机器学习框架分析算法管理对工人福祉与绩效的非线性影响，发现算法监督的模糊中间地带会削弱支持性HR实践与绩效的联系，而透明可解释的监督则能加强这种联系。


<details>
  <summary>Details</summary>
Motivation: 探讨算法管理时代下以人为本的管理能否持续存在，传统线性分析工具无法捕捉工人对算法系统的非线性响应模式。

Method: 采用双重机器学习框架估计调节中介模型，避免强加限制性函数形式，使用464名零工工人的调查数据进行分析。

Result: 发现明显的非单调模式：支持性HR实践改善工人福祉，但在算法监督存在却难以解释的"模糊中间地带"，其与绩效的联系减弱；当监督透明可解释时，这种联系再次加强。

Conclusion: 简单线性设定可能遗漏甚至得出相反结论；平台设计应避免部分定义的模糊控制，而采用清晰规则和可信追索机制；双重机器学习方法为组织研究中的条件间接效应估计提供了非线性分析工具。

Abstract: A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.

</details>


### [71] [Multi-Head Spectral-Adaptive Graph Anomaly Detection](https://arxiv.org/abs/2512.22291)
*Qingyue Cao,Bo Jin,Changwei Gong,Xin Tong,Wenzheng Li,Xiaodong Zhou*

Main category: cs.LG

TL;DR: 提出MHSA-GNN，通过轻量级超网络根据图实例的"频谱指纹"动态生成Chebyshev滤波器参数，结合双正则化策略解决图异常检测中异常节点伪装和频谱信号丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法在处理复杂异常模式时面临挑战：异常节点常伪装成正常节点，导致图中同质性和异质性共存。当前频谱图神经网络使用固定全局滤波器，容易导致过度平滑，丢失欺诈检测所需的高频信号，且缺乏对不同图实例的自适应能力。

Method: 提出多头部频谱自适应图神经网络(MHSA-GNN)：1) 设计轻量级超网络，基于包含结构统计和Rayleigh商特征的"频谱指纹"，动态生成针对每个实例的Chebyshev滤波器参数；2) 引入双正则化策略：教师-学生对比学习确保表示准确性，Barlow Twins多样性损失强制头部正交性，防止多头机制中的模式崩溃。

Result: 在四个真实世界数据集上的实验表明，该方法能有效保留高频异常信号，显著优于现有最先进方法，在高度异构数据集上表现出优秀的鲁棒性。

Conclusion: MHSA-GNN通过实例自适应的频谱滤波和双正则化策略，成功解决了图异常检测中异常信号丢失和模式崩溃问题，为金融欺诈等应用提供了更有效的解决方案。

Abstract: Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This 'one-size-fits-all' approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a 'spectral fingerprint' containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.

</details>


### [72] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 语言模型无法从警告性内容中学习避免不良行为，警告框架与直接内容产生相似的激活模式，统计共现主导于语用解释


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何处理警告性内容（如"不要使用-此代码有漏洞"），探索模型是否能够真正理解警告的语用含义并避免不良行为

Method: 通过实验比较模型在警告框架内容和直接内容下的行为表现，使用稀疏自编码器分析潜在特征激活，识别"描述X"和"执行X"的重叠特征

Result: 模型暴露于警告性内容时复制被标记内容的比率（76.7%）与直接给出内容的比率（83.3%）无统计差异；特征#8684在警告和利用情境下激活程度相似；存在"隐形滑移"现象

Conclusion: 当前架构中统计共现主导语用解释，模型学习上下文中的统计模式而非语用意图；训练时特征消融有效，但提示和推理时引导无效

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [73] [Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing](https://arxiv.org/abs/2512.22296)
*Reda Heddad,Lamiae Bouanane*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子-经典混合专家架构（QMoE），通过量子门控网络（路由器）解决传统MoE中的专家不平衡和计算复杂度问题，验证了量子干涉假设，在非线性可分数据上展现出拓扑优势。


<details>
  <summary>Details</summary>
Motivation: 传统混合专家架构存在专家不平衡和经典路由机制计算复杂度高的根本限制，本文旨在探索量子机器学习如何通过量子增强路由范式来解决这些限制。

Method: 提出混合量子-经典混合专家架构（QMoE），使用量子门控网络作为路由器，结合经典专家进行消融研究。利用量子特征映射（角度嵌入）和波干涉，使量子路由器作为高维核方法建模复杂非线性决策边界。

Result: 在非线性可分数据（如Two Moons数据集）上，量子路由器展现出显著的拓扑优势，能有效"解开"线性经典路由器无法高效分离的数据分布。量子路由器在参数效率上优于经典对应物，且在模拟量子噪声下表现出鲁棒性，适合近期中尺度量子硬件。

Conclusion: 量子路由器通过量子干涉实现了高维核方法，为解决传统MoE的限制提供了新途径。该架构在联邦学习、隐私保护机器学习和自适应系统等实际应用中具有潜力，验证了量子增强路由范式的可行性。

Abstract: The Mixture-of-Experts (MoE) architecture has emerged as a powerful paradigm for scaling deep learning models, yet it is fundamentally limited by challenges such as expert imbalance and the computational complexity of classical routing mechanisms. This paper investigates the potential of Quantum Machine Learning (QML) to address these limitations through a novel Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture. Specifically, we conduct an ablation study using a Quantum Gating Network (Router) combined with classical experts to isolate the source of quantum advantage. Our central finding validates the Interference Hypothesis: by leveraging quantum feature maps (Angle Embedding) and wave interference, the Quantum Router acts as a high-dimensional kernel method, enabling the modeling of complex, non-linear decision boundaries with superior parameter efficiency compared to its classical counterparts. Experimental results on non-linearly separable data, such as the Two Moons dataset, demonstrate that the Quantum Router achieves a significant topological advantage, effectively "untangling" data distributions that linear classical routers fail to separate efficiently. Furthermore, we analyze the architecture's robustness against simulated quantum noise, confirming its feasibility for near-term intermediate-scale quantum (NISQ) hardware. We discuss practical applications in federated learning, privacy-preserving machine learning, and adaptive systems that could benefit from this quantum-enhanced routing paradigm.

</details>


### [74] [Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions](https://arxiv.org/abs/2512.22302)
*Jennifer Sawyer,Julian Allagan*

Main category: cs.LG

TL;DR: 该研究扩展了Sawyer的热点分析，通过集成先进统计方法、机器学习和空间建模技术，分析美国158号公路5年事故数据，识别时空事故模式，为交通安全改进提供依据。


<details>
  <summary>Details</summary>
Motivation: 扩展先前的基础统计热点分析方法，通过更先进的技术为美国158号公路这一重要交通走廊提供可操作的交通安全改进见解，同时为乡村公路安全分析方法论做出贡献。

Method: 整合核密度估计(KDE)、负二项回归、随机森林分类和公路安全手册(HSM)安全性能函数比较，分析2019-2023年5年事故数据，使用Moran's I检验空间聚类。

Result: 随机森林分类器预测伤害严重程度准确率达67%，优于HSM SPF；Moran's I检验确认空间聚类(I=0.32, p<0.001)；KDE分析显示主要交叉口附近存在热点区域。

Conclusion: 研究验证并扩展了先前热点识别方法，支持针对性的交通安全干预措施，为改进美国158号公路安全提供可操作见解，同时推进了乡村公路安全分析方法论。

Abstract: This study extends previous hotspot and Chi-Square analysis by Sawyer \cite{sawyer2025hotspot} by integrating advanced statistical analysis, machine learning, and spatial modeling techniques to analyze five years (2019--2023) of traffic accident data from an 8.4-mile stretch of US 158 in Currituck County, NC. Building upon foundational statistical work, we apply Kernel Density Estimation (KDE), Negative Binomial Regression, Random Forest classification, and Highway Safety Manual (HSM) Safety Performance Function (SPF) comparisons to identify comprehensive temporal and spatial crash patterns. A Random Forest classifier predicts injury severity with 67\% accuracy, outperforming HSM SPF. Spatial clustering is confirmed via Moran's I test ($I = 0.32$, $p < 0.001$), and KDE analysis reveals hotspots near major intersections, validating and extending earlier hotspot identification methods. These results support targeted interventions to improve traffic safety on this vital transportation corridor. Our objective is to provide actionable insights for improving safety on US 158 while contributing to the broader understanding of rural highway safety analysis through methodological advancement beyond basic statistical techniques.

</details>


### [75] [PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations](https://arxiv.org/abs/2512.22305)
*Sultan Amed,Chan Yu Hang,Sayantan Banerjee*

Main category: cs.LG

TL;DR: PDx是一个基于MLOps的自适应决策系统，用于数字借贷中的信用风险预测，通过动态模型生命周期管理解决传统PD模型静态化、性能退化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PD模型虽然注重开发阶段的预测准确性，但缺乏对借款人行为变化的持续适应能力，导致在生产环境中性能随时间退化。金融机构也难以将ML模型从开发环境迁移到生产环境并维持其健康状态。

Method: 采用动态端到端模型生命周期管理方法，集成持续模型监控、重新训练和验证的稳健MLOps流水线。引入动态冠军-挑战者框架，定期更新基线模型，通过最新数据重新校准独立参数，并通过超时验证选择最佳性能模型。

Result: 决策树集成模型在违约分类中表现最佳但需要频繁更新以维持性能；线性模型和神经网络性能退化更严重。PDx能有效减轻数字贷款机构的价值侵蚀，特别是在短期小额贷款场景中。

Conclusion: PDx通过MLOps驱动的自适应决策系统，解决了传统PD模型的静态性和性能退化问题，在P2P借贷、商业贷款和汽车贷款等多种场景中验证了其可扩展性和适应性，为现代信用风险预测提供了有效解决方案。

Abstract: This paper presents PDx, an adaptive, machine learning operations (MLOps) driven decision system for forecasting credit risk using probability of default (PD) modeling in digital lending. While conventional PD models prioritize predictive accuracy during model development with complex machine learning algorithms, they often overlook continuous adaptation to changing borrower behaviour, resulting in static models that degrade over time in production and generate inaccurate default predictions. Many financial institutes also find it difficult transitioning ML models from development environment to production and maintaining their health. With PDx we aimed to addresses these limitations using a dynamic, end-to-end model lifecycle management approach that integrates continuous model monitoring, retraining, and validation through a robust MLOps pipeline. We introduced a dynamic champion-challenger framework for PDx to regularly update baseline models to recalibrate independent parameters with the latest data and select the best-performing model through out-of-time validation, ensuring resilience against data drift and changing credit risk patterns. Our empirical analysis shows that decision tree-based ensemble models consistently outperform others in classifying defaulters but require frequent updates to sustain performance. Linear models (e.g., logistic regression) and neural networks exhibit greater performance degradation. The study demonstrate with PDx we can mitigates value erosion for digital lenders, particularly in short-term, small-ticket loans, where borrower behavior shifts rapidly. We have validated the effectiveness of PDx using datasets from peer-to-peer lending, business loans, and auto loans, demonstrating its scalability and adaptability for modern credit risk forecasting.

</details>


### [76] [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)
*Zehao Chen,Tianxiang Ai,Yifei Li,Gongxun Li,Yuyang Wei,Wang Zhou,Guanghui Li,Bin Yu,Zhijun Chen,Hailong Sun,Fuzhen Zhuang,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.LG

TL;DR: LLMBoost：一种新颖的LLM集成微调框架，通过跨模型注意力机制利用中间状态，实现层次化错误校正和高效推理


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法通常将模型视为黑盒，仅组合输入或最终输出，忽略了丰富的内部表示和跨模型交互。需要打破这一障碍，充分利用LLM的中间状态来提升性能。

Method: 提出LLMBoost框架，包含三个关键创新：1）跨模型注意力机制，使后续模型能够访问和融合前驱模型的隐藏状态；2）链式训练范式，以错误抑制为目标逐步微调连接模型；3）近并行推理范式，逐层流水线化隐藏状态传输。

Result: 在常识推理和算术推理任务上的大量实验表明，LLMBoost能持续提升准确性，同时降低推理延迟。理论分析证明在有限校正假设下，顺序集成能保证单调改进。

Conclusion: LLMBoost通过利用LLM中间状态打破了传统集成方法的黑盒限制，实现了层次化错误校正和高效推理，为LLM集成学习提供了新方向。

Abstract: Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.

</details>


### [77] [Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making](https://arxiv.org/abs/2512.22313)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出OFS算法，在公平性和服务率约束下在线学习阈值策略，通过置信区间处理反馈效应，在合成和半合成基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 闭环决策系统（如贷款、筛选、累犯风险评估）在公平性和服务约束下运行时会引发反馈效应：决策改变未来人群构成，导致数据非平稳并可能放大不平等。需要在线学习满足约束的阈值策略。

Method: 提出乐观可行搜索（OFS）算法：基于网格方法维护每个候选阈值的奖励和约束残差置信区间；每轮选择置信区间内看似可行且乐观奖励最大的阈值；若无可行阈值，则选择乐观约束违反最小的阈值。

Result: 在三个基准测试中评估：1)具有稳定收缩动态的合成闭环基准；2)基于德国信用和COMPAS的半合成闭环基准。OFS在所有环境中都比无约束和原始对偶bandit基线获得更高奖励和更小累积约束违反，接近最优可行固定阈值。

Conclusion: OFS算法能有效处理闭环决策系统中的反馈效应，在满足人口统计公平性和服务率约束的同时实现高性能，特别适用于低维可解释策略类。

Abstract: Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.
  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.
  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.

</details>


### [78] [LangPrecip: Language-Aware Multimodal Precipitation Nowcasting](https://arxiv.org/abs/2512.22317)
*Xudong Ling,Tianxi Huang,Qian Dong,Tao He,Chaorong Li,Guiduo Duan*

Main category: cs.LG

TL;DR: LangPrecip：利用语言描述作为降水演化的语义运动约束，通过多模态框架提升短时降水临近预报精度


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要依赖视觉条件，对未来运动约束较弱且模糊，特别是在快速演变的极端天气事件中。需要更强的语义约束来提升预报准确性。

Method: 提出语言感知多模态临近预报框架(LangPrecip)，将气象文本作为降水演化的语义运动约束，在Rectified Flow范式下将临近预报建模为语义约束的轨迹生成问题，在潜在空间高效整合文本和雷达信息。

Result: 在瑞典和MRMS数据集上相比最先进方法持续改进，在80分钟预报时效上，强降水CSI分别提升超过60%和19%。

Conclusion: 语言描述作为语义运动约束能有效提升短时降水临近预报性能，特别是在极端天气事件中，多模态方法为气象预报提供了新方向。

Abstract: Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \% and 19\% gains in heavy-rainfall CSI at an 80-minute lead time.

</details>


### [79] [Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough](https://arxiv.org/abs/2512.22318)
*Chorok Lee*

Main category: cs.LG

TL;DR: 现有概率知识图谱嵌入方法存在关系无关的不确定性估计缺陷，无法区分新兴实体和新型关系上下文。本文提出分解不确定性为语义和结构两部分，证明两者互补且组合优于单一信号，CAGP方法在时序OOD检测上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有概率知识图谱嵌入方法使用实体级方差量化认知不确定性，但这些方差是关系无关的，导致无法区分两种行为相反的OOD现象：新兴实体（罕见、学习不足）和新型关系上下文（熟悉实体在未观测关系中的出现）。

Method: 提出不确定性分解为互补组件：语义不确定性（来自实体嵌入方差，检测新兴实体）和结构不确定性（来自实体-关系共现，检测新型上下文）。理论证明这些信号非冗余，任何凸组合都严格优于单一信号。CAGP方法通过学习的权重组合语义和结构不确定性。

Result: 在三个数据集（FB15k-237、WN18RR、YAGO3-10）上验证了100%的新型上下文三元组都有频率匹配的分布内对应物。CAGP在时序OOD检测上达到0.94-0.99 AUROC，比关系无关基线相对改进60-80%。在选择性预测中，在85%回答率下减少43%错误。

Conclusion: 关系无关的不确定性估计存在根本限制，无法检测新型关系上下文。通过分解不确定性为语义和结构组件并适当组合，可以显著改进OOD检测性能，特别是在时序分布偏移场景下。

Abstract: Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.

</details>


### [80] [Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers](https://arxiv.org/abs/2512.22326)
*Sravan Karthick T*

Main category: cs.LG

TL;DR: 该论文提出了一种结合全球M2流动性作为外生变量的TimeXer-Exog模型，用于比特币价格预测，相比传统单变量模型在长期预测上显著提升稳定性，70天预测误差降低89%。


<details>
  <summary>Details</summary>
Motivation: 比特币价格预测面临极端波动性和非平稳性挑战，传统单变量时间序列模型在长期预测上表现不佳。论文旨在通过引入宏观经济变量来改善长期预测稳定性。

Method: 使用TimeXer架构，整合来自18个主要经济体的全球M2流动性作为外生变量，采用12周滞后结构。构建流动性条件预测模型(TimeXer-Exog)，并与LSTM、N-BEATS、PatchTST和标准单变量TimeXer等基准模型进行比较。

Result: 在2020年1月至2025年8月的比特币日价格数据上实验表明，宏观经济条件显著稳定了长期预测。在70天预测范围内，TimeXer-Exog模型达到均方误差1.08e8，比单变量TimeXer基线提升超过89%。

Conclusion: 在深度学习模型中引入全球流动性条件可以显著改善比特币价格的长期预测性能，为加密货币预测提供了新的宏观经济视角。

Abstract: Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.

</details>


### [81] [The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2512.22337)
*Matthew Riemer,Erik Miehling,Miao Liu,Djallel Bouneffouf,Murray Campbell*

Main category: cs.LG

TL;DR: LoRA微调可能导致模型能力灾难性下降，但通过正则化近似回放方法（惩罚KL散度并混合预训练数据）可以解决此问题，在Qwen模型上验证有效。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA等参数高效微调方法只修改少量参数，但在指令微调实验中可能灾难性地降低模型能力，即使在小数据集上训练少量步数也会发生。需要找到既能保持模型通用知识又不阻碍新任务学习的方法。

Method: 提出正则化近似回放方法：1）惩罚与初始模型的KL散度；2）混合来自类似预训练语料库的下一个词预测数据。该方法计算开销很小，应用于Qwen指令微调模型。

Result: 该方法能有效保持模型中的通用知识，同时不阻碍新任务的学习能力，仅增加适度的计算开销。在Qwen模型上验证了其有效性。

Conclusion: 虽然简单的LoRA微调方法在实践中可能导致灾难性能力下降，但通过正则化近似回放的小调整可以几乎完全消除该问题，实现知识保持与任务适应性的平衡。

Abstract: Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.

</details>


### [82] [Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration](https://arxiv.org/abs/2512.22382)
*Bruno Mlodozeniec,Pierre Ablin,Louis Béthune,Dan Busbridge,Michal Klein,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 该论文提出了Complete(d)参数化方法，统一了宽度、深度、批次大小和训练时长的缩放，并研究了模块级超参数优化与迁移，显著提升了大型语言模型的训练速度。


<details>
  <summary>Details</summary>
Motivation: 超参数调优对大规模模型的训练稳定性和最终性能有巨大影响。现有工作如μP虽然实现了跨模型尺寸的全局超参数迁移，但主要关注宽度缩放，缺乏对深度、批次大小等多维缩放轴的支持，且未探索模块级超参数优化。

Method: 提出Complete(d)参数化方法，基于CompleteP的改进版本，统一处理宽度、深度、批次大小和训练时长的缩放。研究模块级超参数优化与迁移，分析高维超参数空间的导航挑战，并提出实用的优化指导原则。

Result: 实验证明，在正确的参数化下，超参数迁移在模块级超参数机制中仍然有效。研究覆盖了现代模型的各种优化超参数（学习率、AdamW参数、权重衰减、初始化尺度、残差块乘子等）。使用迁移的模块级超参数，大型语言模型的训练速度得到显著提升。

Conclusion: Complete(d)参数化方法能够统一处理多个重要缩放轴，模块级超参数优化与迁移是可行的，并且能带来显著的训练加速效果。这为大规模模型的高效训练提供了新的超参数调优框架。

Abstract: Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $μ$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.

</details>


### [83] [BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks](https://arxiv.org/abs/2512.22388)
*Omar Alsaqa,Linh Thi Hoang,Muhammed Fatih Balin*

Main category: cs.LG

TL;DR: BLISS使用多臂老虎机动态选择每层最信息丰富的节点，解决GNN在大图上计算成本高的问题，在保持或超过全批次训练精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理大图时面临计算成本高的问题，需要处理每个节点的所有邻居导致内存和计算瓶颈。现有静态采样方法无法适应节点重要性的动态变化。

Method: 提出BLISS（Bandit Layer Importance Sampling Strategy），使用多臂老虎机动态选择每层最信息丰富的节点，平衡探索与利用，确保全面覆盖图结构。该方法可集成到GCN和GAT中，根据其特定聚合机制调整选择策略。

Result: 实验表明BLISS能够保持或超过全批次训练的精度，同时显著降低计算开销，展示了其在大图处理中的有效性。

Conclusion: BLISS通过动态采样策略有效解决了GNN在大图上的计算瓶颈问题，提供了一种高效且性能优越的图神经网络训练方法。

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.

</details>


### [84] [Causality-Inspired Safe Residual Correction for Multivariate Time Series](https://arxiv.org/abs/2512.22428)
*Jianxiang Xie,Yuncheng Hua*

Main category: cs.LG

TL;DR: CRC是一个因果启发的安全残差校正框架，通过解耦自变量和跨变量动态来确保预测性能不退化，具有严格的安全机制防止有害更新。


<details>
  <summary>Details</summary>
Motivation: 现代多变量预测器（如Transformers和GNNs）虽然在基准测试中表现良好，但存在特定变量或时间范围的系统性误差，且缺乏部署时性能不退化的保证。现有的残差校正方法虽然可能提高平均准确率，但可能"以错误的方式帮助"，过度校正可靠预测并在未见场景中导致局部失败。

Method: CRC采用分而治之的哲学：1）使用因果启发的编码器通过解耦自变量和跨变量动态来暴露方向感知结构；2）使用混合校正器建模残差误差；3）通过严格的四重安全机制控制校正过程，防止有害更新。

Result: 在多个数据集和预测骨干网络上的实验表明，CRC能持续提高准确率，深入的消融研究证实其核心安全机制确保了极高的非退化率（NDR），使CRC成为适合安全可靠部署的校正框架。

Conclusion: CRC填补了预测系统的"安全空白"，提供了一个即插即用的框架，确保预测性能在部署时不会退化，特别适用于需要安全可靠预测的场景。

Abstract: While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.

</details>


### [85] [AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing](https://arxiv.org/abs/2512.22455)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Feiye Huo,Yerui Sun,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出AFA-LoRA方法，通过退火激活函数为LoRA引入非线性表达能力，同时保持可合并性，缩小了LoRA与全参数微调之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: LoRA作为广泛采用的参数高效微调方法，其线性适应过程限制了表达能力，存在线性训练与非线性训练之间的表达能力差距。

Method: 提出AFA-LoRA训练策略，核心创新是退火激活函数，在训练过程中从非线性变换过渡到线性变换，使适配器先获得更强的表示能力，最终收敛到可合并的线性形式。

Result: 在监督微调、强化学习和推测解码上实现该方法，结果显示AFA-LoRA减少了LoRA与全参数训练之间的性能差距。

Conclusion: 这项工作实现了更强大和实用的参数高效适应范式，为LoRA带来了非线性表达能力同时保持了其可合并性。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.

</details>


### [86] [AMBIT: Augmenting Mobility Baselines with Interpretable Trees](https://arxiv.org/abs/2512.22466)
*Qizhi Wang*

Main category: cs.LG

TL;DR: AMBIT是一个灰盒框架，通过将物理移动模型与可解释的树模型结合，解决OD流预测中准确性与可解释性的冲突问题。


<details>
  <summary>Details</summary>
Motivation: OD流预测在GIS和城市分析中面临两个冲突需求：高准确性和清晰可解释性。现有物理模型在小时级时间分辨率下表现脆弱，而黑盒模型缺乏可解释性。

Method: 首先对经典空间交互模型进行全面审计，确定PPML重力模型为最强物理基线。然后在物理基线上构建残差学习器，使用梯度提升树和SHAP分析，形成物理基础残差和POI锚定残差。

Result: 物理基础残差接近强树基预测器的准确性，同时保持可解释结构；POI锚定残差在空间泛化下最具鲁棒性。提供了可复现的管道、丰富诊断和空间误差分析。

Conclusion: AMBIT框架成功平衡了OD流预测的准确性和可解释性，为城市决策提供了实用的灰盒解决方案。

Abstract: Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.

</details>


### [87] [GLUE: Gradient-free Learning to Unify Experts](https://arxiv.org/abs/2512.22467)
*Jong-Ik Park,Shreyas Chaudhari,Srinivasa Pranav,Carlee Joe-Wong,José M. F. Moura*

Main category: cs.LG

TL;DR: GLUE提出了一种无需梯度的专家模型融合方法，通过梯度自由的两点更新学习混合系数，只需两次前向传播即可有效初始化目标模型。


<details>
  <summary>Details</summary>
Motivation: 在部署系统中，多个预训练专家模型并存，但新目标域需要超越单个专家能力的泛化模型。现有方法要么使用启发式混合系数（基于数据大小或代理指标）导致性能不佳，要么需要计算昂贵的全网络反向传播来学习系数。

Method: GLUE将目标模型初始化为固定专家的凸组合，通过梯度自由的两点（SPSA）更新学习混合系数，每个步骤仅需两次前向传播，无需反向传播。

Result: 在三个数据集和三种网络架构上的实验表明，GLUE产生的先验模型经过微调后优于基线方法：比基于数据大小的加权方法提升8.5%测试准确率，比基于代理指标的选择方法提升9.1%，与基于反向传播的全梯度混合方法相当或优于其性能（差距在1.4%以内）。

Conclusion: GLUE提供了一种高效、无需梯度的专家模型融合方法，能够为新的目标域生成有效的单一先验模型，显著优于启发式方法，且计算成本远低于基于反向传播的方法。

Abstract: In many deployed systems (multilingual ASR, cross-hospital imaging, region-specific perception), multiple pretrained specialist models coexist. Yet, new target domains often require domain expansion: a generalized model that performs well beyond any single specialist's domain. Given such a new target domain, prior works seek a single strong initialization prior for the model parameters by first blending expert models to initialize a target model. However, heuristic blending -- using coefficients based on data size or proxy metrics -- often yields lower target-domain test accuracy, and learning the coefficients on the target loss typically requires computationally-expensive full backpropagation through the network. We propose GLUE, Gradient-free Learning To Unify Experts, which initializes the target model as a convex combination of fixed experts, learning the mixture coefficients of this combination via a gradient-free two-point (SPSA) update that requires only two forward passes per step. Across experiments on three datasets and three network architectures, GLUE produces a single prior that can be fine-tuned effectively to outperform baselines. GLUE improves test accuracy by up to 8.5% over data-size weighting and by up to 9.1% over proxy-metric selection. GLUE either outperforms backpropagation-based full-gradient mixing or matches its performance within 1.4%.

</details>


### [88] [The Bayesian Geometry of Transformer Attention](https://arxiv.org/abs/2512.22471)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: 小Transformer在受控环境中能精确执行贝叶斯推理（10^-3-10^-4比特精度），而容量匹配的MLP则失败，表明注意力机制对贝叶斯推理至关重要


<details>
  <summary>Details</summary>
Motivation: 验证Transformer是否真正执行贝叶斯推理存在挑战：自然数据缺乏解析后验，大模型混淆推理与记忆。需要创建受控环境来严格测试

Method: 构建"贝叶斯风洞"——后验已知且记忆不可能的受控环境，在两个任务（双射消除和HMM状态跟踪）中分析小Transformer的机制

Result: 小Transformer精确再现贝叶斯后验（10^-3-10^-4比特精度），MLP失败；发现Transformer通过几何机制实现贝叶斯推理：残差流作为信念基底，前馈网络执行后验更新，注意力提供内容寻址路由

Conclusion: 分层注意力通过几何设计实现贝叶斯推理，解释了注意力机制的必要性和扁平架构的失败。贝叶斯风洞为连接小可验证系统与大语言模型的推理现象提供了基础

Abstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.
  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \emph{frame-precision dissociation} predicted by recent gradient analyses.
  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.

</details>


### [89] [Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting](https://arxiv.org/abs/2512.22478)
*Chuantao Li,Zhi Li,Jiahao Xu,Jie Li,Sheng Li*

Main category: cs.LG

TL;DR: 提出了一种用于多类不平衡学习的协同优化Boosting模型，通过整合密度因子和置信度因子，设计了抗噪声权重更新机制和动态采样策略，实现了不平衡学习与模型训练的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未探索不平衡学习与模型训练的协同优化，这一限制阻碍了性能的进一步提升。为了弥补这一空白，本研究旨在开发一个能够协同优化这两个方面的模型。

Method: 提出了一种协同优化的Boosting模型，整合了密度因子和置信度因子，设计了抗噪声权重更新机制和动态采样策略。这些模块紧密集成，协调权重更新、样本区域划分和区域引导采样。

Result: 在20个公共不平衡数据集上的广泛实验表明，该模型显著优于8个最先进的基线方法。

Conclusion: 该研究成功实现了不平衡学习与模型训练的协同优化，提出的模型简单而有效，显著提升了多类不平衡分类的性能。

Abstract: Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.

</details>


### [90] [Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment](https://arxiv.org/abs/2512.22488)
*Hassan Wasswa,Timothy Lynar*

Main category: cs.LG

TL;DR: 提出一个无需持续重训练的自适应IoT威胁检测框架，通过潜在空间对齐和GNN分类来应对概念漂移


<details>
  <summary>Details</summary>
Motivation: 现有AI模型依赖静态数据集，无法适应真实IoT网络流量的动态变化（概念漂移），而周期性重训练方案计算开销大且存在灾难性遗忘风险

Method: 1) 在历史流量潜在空间表示上训练一次分类器；2) 使用对齐模型将新流量映射到已学习的潜在空间；3) 将低维潜在表示转换为图结构，用图神经网络分类以捕捉攻击样本间的实例关系

Result: 在真实异构IoT流量数据集上的实验表明，该框架在概念漂移下保持鲁棒的检测性能

Conclusion: 该框架具有在动态大规模IoT环境中实际部署的潜力，解决了概念漂移问题且无需持续重训练

Abstract: Although AI-based models have achieved high accuracy in IoT threat detection, their deployment in enterprise environments is constrained by reliance on stationary datasets that fail to reflect the dynamic nature of real-world IoT NetFlow traffic, which is frequently affected by concept drift. Existing solutions typically rely on periodic classifier retraining, resulting in high computational overhead and the risk of catastrophic forgetting. To address these challenges, this paper proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining. The proposed approach trains a classifier once on latent-space representations of historical traffic, while an alignment model maps incoming traffic to the learned historical latent space prior to classification, thereby preserving knowledge of previously observed attacks. To capture inter-instance relationships among attack samples, the low-dimensional latent representations are further transformed into a graph-structured format and classified using a graph neural network. Experimental evaluations on real-world heterogeneous IoT traffic datasets demonstrate that the proposed framework maintains robust detection performance under concept drift. These results highlight the framework's potential for practical deployment in dynamic and large-scale IoT environments.

</details>


### [91] [The Quest for Winning Tickets in Low-Rank Adapters](https://arxiv.org/abs/2512.22495)
*Hamed Damirchi,Cristian Rodriguez-Opazo,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 论文发现彩票假设在LoRA参数高效微调中成立，提出Partial-LoRA方法，能在减少87%可训练参数的同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着对大预训练模型微调的依赖增加，研究彩票假设是否适用于参数高效微调（特别是LoRA方法），探索是否存在稀疏子网络能达到密集适配器的性能。

Method: 提出Partial-LoRA方法，系统识别稀疏子网络，训练与预训练模型任务相关子空间对齐的稀疏低秩适配器，重点关注各层稀疏度分配而非具体权重选择。

Result: 在8个视觉和12个语言任务的单任务和多任务设置中，Partial-LoRA将可训练参数减少高达87%，同时保持或提高准确率。

Conclusion: 研究证实彩票假设在LoRA微调中成立，加深了对迁移学习及预训练与微调相互作用的理论理解，为开发更高效适配策略开辟了新途径。

Abstract: The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks ("winning tickets") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.

</details>


### [92] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: 该研究探索了使用元数据和幻觉信号来预测LLM在牙科考试中回答正确性的可行性，发现这种方法可以提高准确性，但尚不足以用于高风险部署。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗等高风险领域的应用增加，预测其回答正确性成为关键但尚未充分探索的问题。现有研究主要关注检测和缓解幻觉，但预测正确性本身仍是一个挑战。

Method: 研究分析了GPT-4o和OSS-120B两个模型在牙科多选题考试上的表现，使用三种不同的提示策略，并利用元数据和幻觉信号为每个（模型，提示）组合构建正确性预测器。

Result: 元数据方法可将准确性提高最多+7.14%，达到83.12%的精确度。虽然实际幻觉是错误回答的强指标，但元数据信号本身不能可靠预测幻觉。提示策略虽不影响总体准确性，但显著改变了模型内部行为和元数据的预测效用。

Conclusion: 该研究为开发LLM可靠性信号提供了有前景的方向，但所探索的方法尚不够稳健，不适合关键高风险部署。需要进一步研究来改进预测能力。

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [93] [Decomposing Task Vectors for Refined Model Editing](https://arxiv.org/abs/2512.22511)
*Hamed Damirchi,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 提出一种任务向量分解方法，将每个任务向量分解为共享知识和独特信息两个组件，以解决任务向量算术运算中的概念重叠干扰问题，实现更精确的概念操控。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型难以精确适应特定概念行为。任务向量（微调与预训练参数之差）可用于引导神经网络行为，但多个任务向量在算术运算时存在概念重叠干扰，导致不可预测的结果。

Method: 提出原则性分解方法：将每个任务向量分解为两个组件——一个捕获多个任务向量间的共享知识，另一个隔离每个特定任务的独特信息。通过识别投影中的不变子空间来实现。

Result: 1) 图像分类多任务合并提升5%（使用共享组件作为额外任务向量）；2) 扩散模型中实现干净的风格混合而无生成退化（仅混合独特组件）；3) 语言模型毒性降低47%同时保持通用知识任务性能（通过抵消独特组件中的有毒信息）。

Conclusion: 该方法为理解和控制任务向量算术提供了新框架，解决了模型编辑操作中的基本限制，实现了更精确的概念操控。

Abstract: Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows for the seamless combination of desired behaviors without the need for large datasets. However, these vectors often contain overlapping concepts that can interfere with each other during arithmetic operations, leading to unpredictable outcomes. We propose a principled decomposition method that separates each task vector into two components: one capturing shared knowledge across multiple task vectors, and another isolating information unique to each specific task. By identifying invariant subspaces across projections, our approach enables more precise control over concept manipulation without unintended amplification or diminution of other behaviors. We demonstrate the effectiveness of our decomposition method across three domains: improving multi-task merging in image classification by 5% using shared components as additional task vectors, enabling clean style mixing in diffusion models without generation degradation by mixing only the unique components, and achieving 47% toxicity reduction in language models while preserving performance on general knowledge tasks by negating the toxic information isolated to the unique component. Our approach provides a new framework for understanding and controlling task vector arithmetic, addressing fundamental limitations in model editing operations.

</details>


### [94] [Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks](https://arxiv.org/abs/2512.22522)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 该论文提出了一种更可靠的SNN对抗鲁棒性评估框架，包括自适应锐度替代梯度(ASSG)和稳定自适应投影梯度下降(SA-PGD)攻击方法，显著提高了攻击成功率，揭示了当前SNN鲁棒性被高估的问题。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)的脉冲激活具有二元性和不连续性，导致梯度消失问题，使得基于梯度下降的对抗鲁棒性评估不可靠。虽然已有改进的替代梯度方法，但在强对抗攻击下的有效性仍不清楚。

Method: 1) 理论分析替代梯度中的梯度消失程度；2) 提出自适应锐度替代梯度(ASSG)，在攻击迭代过程中根据输入分布自适应演化替代函数形状；3) 设计L∞约束下的稳定自适应投影梯度下降(SA-PGD)攻击，在梯度不精确情况下实现更快更稳定的收敛。

Result: 实验表明，该方法在各种对抗训练方案、SNN架构和神经元模型中显著提高了攻击成功率，提供了更通用可靠的SNN对抗鲁棒性评估。结果进一步揭示当前SNNs的鲁棒性被显著高估。

Conclusion: 该研究提出了一个更可靠的SNN对抗鲁棒性评估框架，通过ASSG和SA-PGD方法解决了梯度消失问题，提高了攻击效果。实验结果表明当前SNN的鲁棒性被高估，需要更可靠的对抗训练方法。

Abstract: Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.

</details>


### [95] [TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting](https://arxiv.org/abs/2512.22550)
*Jaebin Lee,Hankook Lee*

Main category: cs.LG

TL;DR: TimePerceiver：一个统一的编码器-解码器时间序列预测框架，通过广义预测目标和创新的编码解码设计，在多种基准数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测研究主要关注编码器设计，而将预测和解码视为次要问题。本文旨在提出一个统一的编码器-解码器框架，将预测任务与有效的训练策略紧密结合。

Method: 1. 将预测任务广义化为外推、插值和填补等多种时间预测目标；2. 设计新颖的编码器-解码器架构，灵活感知和适应输入和目标段在时间轴上的任意位置；3. 编码器使用潜在瓶颈表示来联合捕获时间和跨通道依赖；4. 解码器使用可学习查询对应目标时间戳来有效检索相关信息。

Result: 在广泛的基准数据集上，TimePerceiver框架一致且显著地超越了先前的最先进基线方法。

Conclusion: TimePerceiver提供了一个统一的编码器-解码器预测框架，通过将广义预测目标与创新的架构设计相结合，实现了卓越的性能，为时间序列预测提供了更全面的解决方案。

Abstract: In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.

</details>


### [96] [On Admissible Rank-based Input Normalization Operators](https://arxiv.org/abs/2512.22587)
*Taeyun Kim*

Main category: cs.LG

TL;DR: 论文指出当前广泛使用的可微排序和排名操作符在严格单调变换、批次组成变化和微小输入扰动下存在固有稳定性问题，提出了三个公理来形式化基于排名的输入归一化所需的最小不变性和稳定性属性，并构建了一个满足这些标准的最小操作符。


<details>
  <summary>Details</summary>
Motivation: 基于排名的输入归一化在现代机器学习中被广泛使用，因其对尺度、单调变换和批次间变化的鲁棒性而受到重视。然而，尽管在许多实际系统中特征值的排序比原始数值更重要，但基于排名的归一化操作符在这些不变性下保持稳定所需的结构条件从未被正式确定。

Method: 提出了三个公理来形式化基于排名的输入归一化所需的最小不变性和稳定性属性，证明了任何满足这些公理的操作符必须分解为：(i) 特征级别的排名表示和 (ii) 单调且Lipschitz连续的标量化映射。然后构建了一个满足这些标准的最小操作符。

Result: 证明了广泛使用的可微排序和排名操作符由于依赖数值间隙和批次级别的成对交互，在严格单调变换、小批量组成变化和微小输入扰动下具有内在不稳定性。提出的公理框架和最小操作符为有效的基于排名的归一化操作符划定了清晰的设计空间。

Conclusion: 该研究形式化了基于排名的输入归一化的基本稳定性要求，揭示了现有可微排序方法的固有局限性，并为构建在单调变换和批次变化下保持稳定的有效归一化操作符提供了理论基础和具体设计指导。

Abstract: Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.
  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.
  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.

</details>


### [97] [Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining](https://arxiv.org/abs/2512.22589)
*Jewel Rana Palit,Vijayalakshmi K Kumarasamy,Osama A. Osman*

Main category: cs.LG

TL;DR: 本研究分析美国NHTSA超过2500起AV事故记录，开发两阶段数据挖掘框架：先用K-means聚类将事故分为4个行为集群，再用关联规则挖掘揭示各集群中事故模式与贡献因素的多变量关系。


<details>
  <summary>Details</summary>
Motivation: 虽然自动驾驶车辆有潜力减少人为驾驶错误，但近期事故数据显示AV行为可能偏离预期安全结果，引发对混合交通环境中技术安全性和操作可靠性的担忧。现有研究大多依赖小规模加州数据集，且对SAE不同自动化级别的事故趋势理解有限。

Method: 开发两阶段数据挖掘框架：1) 应用K-means聚类基于时间、空间和环境因素将事故记录分为4个行为集群；2) 使用关联规则挖掘提取每个集群内事故模式与贡献因素（照明条件、路面状况、车辆动态、环境条件）之间的可解释多变量关系。

Result: 分析覆盖SAE 2级和4级的2500多起AV事故记录，识别出4个不同的事故行为集群，并揭示了各集群中事故模式与多种贡献因素之间的关联规则。

Conclusion: 研究结果为AV开发者、安全监管机构和政策制定者提供了可操作的指导，有助于制定AV部署策略和最小化事故风险。该方法能够深入理解不同自动化级别下的事故动态。

Abstract: Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.

</details>


### [98] [Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification](https://arxiv.org/abs/2512.22597)
*Guikun Xu,Xiaohan Yi,Peilin Zhao,Yatao Bian*

Main category: cs.LG

TL;DR: EnFlow是一个统一的框架，通过将流匹配与显式学习的能量模型相结合，利用能量引导采样生成低能量构象系综并准确识别基态构象。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的计算方法生成低能量构象系综和识别基态构象计算成本高。现有的学习方法存在碎片化问题：生成模型能捕捉多样性但缺乏可靠的能量校准，而确定性预测器只针对单一结构且无法表示系综变异性。

Method: EnFlow将流匹配(FM)与显式学习的能量模型通过能量引导采样方案耦合，该方案定义在非高斯FM路径上。在采样过程中加入能量梯度引导，将轨迹导向低能量区域，提高构象保真度。学习的能量函数还能对生成的系综进行基于能量的高效排序。

Result: 在GEOM-QM9和GEOM-Drugs数据集上的广泛实验表明，EnFlow仅用1-2个ODE步骤就能同时改进生成指标，并相比最先进方法减少了基态预测误差。

Conclusion: EnFlow提供了一个统一的框架，能够高效生成低能量构象系综并准确识别基态构象，解决了当前方法中生成多样性与能量校准之间的权衡问题。

Abstract: Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.

</details>


### [99] [Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units](https://arxiv.org/abs/2512.22599)
*Milad Asadpour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种名为PGRU（并行门控循环单元）的新深度模型，用于加密货币价格预测，通过并行独立的循环神经网络处理不同价格特征，最终结合输出进行预测。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币和比特币的发展，在线投资和交易日益增多。比特币使用区块链技术确保交易安全、透明、可追溯和不可篡改，但其价格波动剧烈，吸引了金融领域的广泛关注。投资者需要预测加密货币价格的方法，这是经济学中的重要挑战。

Method: 提出PGRU（并行门控循环单元）模型，使用循环神经网络以并行独立的方式预测价格。并行网络处理不同的输入特征（代表不同的价格相关特征），最后通过神经网络结合并行网络的输出，预测加密货币的未来价格。

Result: 实验结果表明，所提模型在窗口长度20和15时，分别达到3.243%和2.641%的平均绝对百分比误差（MAPE）。与现有方法相比，该方法使用更少的输入数据和更低的计算成本，实现了更高的准确性和效率。

Conclusion: PGRU模型在加密货币价格预测方面表现出色，具有更高的准确性和效率，同时减少了输入数据和计算成本，为投资者提供了有效的预测工具。

Abstract: According to the advent of cryptocurrencies and Bitcoin, many investments and businesses are now conducted online through cryptocurrencies. Among them, Bitcoin uses blockchain technology to make transactions secure, transparent, traceable, and immutable. It also exhibits significant price fluctuations and performance, which has attracted substantial attention, especially in financial sectors. Consequently, a wide range of investors and individuals have turned to investing in the cryptocurrency market. One of the most important challenges in economics is price forecasting for future trades. Cryptocurrencies are no exception, and investors are looking for methods to predict prices; various theories and methods have been proposed in this field. This paper presents a new deep model, called \emph{Parallel Gated Recurrent Units} (PGRU), for cryptocurrency price prediction. In this model, recurrent neural networks forecast prices in a parallel and independent way. The parallel networks utilize different inputs, each representing distinct price-related features. Finally, the outputs of the parallel networks are combined by a neural network to forecast the future price of cryptocurrencies. The experimental results indicate that the proposed model achieves mean absolute percentage errors (MAPE) of 3.243% and 2.641% for window lengths 20 and 15, respectively. Our method therefore attains higher accuracy and efficiency with fewer input data and lower computational cost compared to existing methods.

</details>


### [100] [Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer](https://arxiv.org/abs/2512.22606)
*Hesam Taghipour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种基于LSTM-MLP混合模型的金价预测算法，结合灰狼优化算法优化网络结构，在日度和月度时间框架上预测金价，并开发了相应的交易策略。


<details>
  <summary>Details</summary>
Motivation: 黄金市场作为重要的金融市场，涉及众多机构投资者和个人投资者，但由于经济和政治因素的复杂性，准确预测金价具有挑战性。开发能够准确预测金价的模型对市场参与者具有重要意义。

Method: 使用两个LSTM网络分别进行日度和月度金价预测，将预测结果输入MLP网络进行整合，得到最终预测。采用灰狼优化算法优化各网络的神经元数量。数据集涵盖2010-2021年的宏观经济、能源市场、股票和发达国家货币数据。

Result: 模型预测日度收盘价的MAE为0.21美元，月度预测MAE为22.23美元。基于预测结果开发的交易策略在三个月内实现了171%的回报率。

Conclusion: 提出的LSTM-MLP混合模型能够有效预测金价，优化的网络结构和多时间框架预测为黄金市场交易提供了有价值的工具，展示了人工智能在金融预测中的应用潜力。

Abstract: The global gold market, by its fundamentals, has long been home to many financial institutions, banks, governments, funds, and micro-investors. Due to the inherent complexity and relationship between important economic and political components, accurate forecasting of financial markets has always been challenging. Therefore, providing a model that can accurately predict the future of the markets is very important and will be of great benefit to their developers. In this paper, an artificial intelligence-based algorithm for daily and monthly gold forecasting is presented. Two Long short-term memory (LSTM) networks are responsible for daily and monthly forecasting, the results of which are integrated into a Multilayer perceptrons (MLP) network and provide the final forecast of the next day prices. The algorithm forecasts the highest, lowest, and closing prices on the daily and monthly time frame. Based on these forecasts, a trading strategy for live market trading was developed, according to which the proposed model had a return of 171% in three months. Also, the number of internal neurons in each network is optimized by the Gray Wolf optimization (GWO) algorithm based on the least RMSE error. The dataset was collected between 2010 and 2021 and includes data on macroeconomic, energy markets, stocks, and currency status of developed countries. Our proposed LSTM-MLP model predicted the daily closing price of gold with the Mean absolute error (MAE) of $ 0.21 and the next month's price with $ 22.23.

</details>


### [101] [Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback](https://arxiv.org/abs/2512.22623)
*Tomas Ortega,Chun-Yin Huang,Xiaoxiao Li,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 提出两种无需客户端状态的新框架CAFe和CAFe-S，解决联邦学习中偏置压缩的通信瓶颈问题，避免传统误差反馈的隐私泄露和状态依赖问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端到服务器上行传输的通信瓶颈，传统偏置压缩技术需要误差反馈机制，但标准误差反馈依赖客户端特定控制变量，这既违反用户隐私，也不兼容大规模联邦学习中常见的无状态客户端。

Method: 提出两种新框架：1) CAFe使用前一轮全局聚合更新作为所有客户端共享的控制变量；2) CAFe-S扩展此思想，当服务器拥有小型私有数据集时，生成服务器引导的候选更新作为更准确的预测器。以分布式梯度下降为代表性算法进行分析。

Result: 在非凸且有界梯度差异的情况下，理论证明CAFe优于分布式压缩梯度下降；CAFe-S收敛到平稳点，且收敛速度随服务器数据代表性增强而提高。联邦学习场景的实验结果验证了所提方法优于现有压缩方案。

Conclusion: CAFe和CAFe-S框架有效解决了联邦学习中偏置压缩的通信瓶颈，避免了传统误差反馈的隐私和状态依赖问题，在理论和实验上都表现出优越性能。

Abstract: Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.

</details>


### [102] [Scaling Unverifiable Rewards: A Case Study on Visual Insights](https://arxiv.org/abs/2512.22650)
*Shuyu Gan,James Mooney,Pan Hao,Renxiang Wang,Mingyi Hong,Qianwen Wang,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出Selective TTS框架，在多阶段多智能体流水线中分布计算资源，通过过程特定评估器早期剪枝低质量分支，以解决无验证奖励任务中的错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多阶段流水线任务（如数据科学分析）通常缺乏可验证的最终奖励或足够数据训练鲁棒的奖励模型，导致基于评估器的迭代细化容易在多个阶段中累积错误。

Method: 提出Selective TTS框架，将计算资源分布到流水线的不同阶段而非时间上的重复细化。使用过程特定评估器早期剪枝低质量分支，减少评估漂移并稳定细化过程。基于数据科学流水线构建端到端多智能体系统，设计可靠的LLM评估器模型。

Result: 在固定计算预算下，Selective TTS将洞察质量平均得分从61.64提升到65.86，同时降低了方差。设计的LLM评估器与人类专家对齐（Kendall's τ=0.55）。

Conclusion: Selective TTS为扩展具有不可验证奖励的复杂开放任务（如科学发现和故事生成）提供了第一步，通过过程特定评估和多阶段计算分布改善了多智能体流水线的性能。

Abstract: Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's τ=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.

</details>


### [103] [Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations](https://arxiv.org/abs/2512.22672)
*Achraf Hsain,Fouad Mohammed Abbou*

Main category: cs.LG

TL;DR: 首次将量子生成模型应用于计算流体动力学数据的潜在空间表示，比较了量子与经典生成方法在物理模拟压缩表示上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探索量子模型学习流体系统统计特性，但将离散潜在空间压缩与量子生成采样结合用于CFD数据仍是未探索领域，需要建立CFD模拟与量子机器学习的完整桥梁。

Method: 使用GPU加速的Lattice Boltzmann Method生成流体涡度场，通过Vector Quantized Variational Autoencoder压缩到7维离散潜在空间，然后比较Quantum Circuit Born Machine、Quantum Generative Adversarial Network与经典LSTM基准的生成性能。

Result: 在实验条件下，两种量子模型产生的样本与真实分布的平均最小距离均低于LSTM，其中QCBM取得了最优的评估指标。

Conclusion: 该研究建立了完整的开源流程连接CFD模拟与量子机器学习，首次在物理模拟压缩表示上进行了量子生成建模的实证研究，为未来该交叉领域的深入研究奠定了基础。

Abstract: This paper presents the first application of quantum generative models to learned latent space representations of computational fluid dynamics (CFD) data. While recent work has explored quantum models for learning statistical properties of fluid systems, the combination of discrete latent space compression with quantum generative sampling for CFD remains unexplored. We develop a GPU-accelerated Lattice Boltzmann Method (LBM) simulator to generate fluid vorticity fields, which are compressed into a discrete 7-dimensional latent space using a Vector Quantized Variational Autoencoder (VQ-VAE). The central contribution is a comparative analysis of quantum and classical generative approaches for modeling this physics-derived latent distribution: we evaluate a Quantum Circuit Born Machine (QCBM) and Quantum Generative Adversarial Network (QGAN) against a classical Long Short-Term Memory (LSTM) baseline. Under our experimental conditions, both quantum models produced samples with lower average minimum distances to the true distribution compared to the LSTM, with the QCBM achieving the most favorable metrics. This work provides: (1)~a complete open-source pipeline bridging CFD simulation and quantum machine learning, (2)~the first empirical study of quantum generative modeling on compressed latent representations of physics simulations, and (3)~a foundation for future rigorous investigation at this intersection.

</details>


### [104] [Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning](https://arxiv.org/abs/2512.22675)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 本文提出了一种去中心化多任务表示学习方法，通过低秩结构学习共享特征，通信复杂度与目标精度无关，显著降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺环境下，表示学习是提取相关任务共同特征的重要框架。虽然集中式方法已被广泛研究，但去中心化方法仍未被充分探索。本文旨在研究去中心化多任务表示学习，其中特征共享低秩结构，任务数据分布在多个节点上，节点间通信受网络约束。

Method: 提出了一种新的交替投影梯度和最小化算法，具有可证明的精度保证。该方法在去中心化设置中学习低秩特征矩阵，其中任务数据分布在多个节点上，节点间通过通信网络交换信息。

Result: 提供了时间、通信和样本复杂度的全面表征。重要的是，通信复杂度与目标精度无关，这显著降低了与先前方法相比的通信成本。数值模拟验证了理论分析，并展示了去中心化学习优于集中式联邦方法的场景。

Conclusion: 本文提出的去中心化多任务表示学习方法在通信效率方面具有显著优势，通信复杂度与精度无关，为数据分布式环境下的表示学习提供了有效的解决方案，在某些场景下优于集中式联邦方法。

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task representation learning in which the features share a low-rank structure. We consider multiple tasks, each with a finite number of data samples, where the observations follow a linear model with task-specific parameters. In the decentralized setting, task data are distributed across multiple nodes, and information exchange between nodes is constrained by a communication network. The goal is to recover the underlying feature matrix whose rank is much smaller than both the parameter dimension and the number of tasks. We propose a new alternating projected gradient and minimization algorithm with provable accuracy guarantees. We provide comprehensive characterizations of the time, communication, and sample complexities. Importantly, the communication complexity is independent of the target accuracy, which significantly reduces communication cost compared to prior methods. Numerical simulations validate the theoretical analysis across different dimensions and network topologies, and demonstrate regimes in which decentralized learning outperforms centralized federated approaches.

</details>


### [105] [Learning with the $p$-adics](https://arxiv.org/abs/2512.22692)
*André F. T. Martins*

Main category: cs.LG

TL;DR: 该论文探索使用p-adic数（ℚₚ）作为实数（ℝ）的替代场，用于机器学习框架，利用其超度量结构和分层特性进行表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习框架基于实数场（ℝ），但作者质疑这是否是唯一选择。p-adic数具有超度量、非阿基米德特性，其分层结构和无限字符串解释使其在代码理论和分层表示学习中具有吸引力。

Method: 建立p-adic数分类、回归和表示学习的理论基础和构建模块，提供学习模型和算法。展示了如何将简单的Quillian语义网络表示为紧凑的p-adic线性网络，这在实数场中无法实现。

Result: 提出了使用p-adic数进行机器学习的新框架，展示了其在分层表示学习中的潜力，特别是在语义网络表示方面的独特优势。

Conclusion: p-adic数为机器学习提供了有前景的替代数学基础，开启了新的研究方向，特别是在分层表示和代码理论方面，需要进一步探索其实际应用和算法实现。

Abstract: Existing machine learning frameworks operate over the field of real numbers ($\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.
  But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.

</details>


### [106] [Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors](https://arxiv.org/abs/2512.22699)
*Antar Kumar Biswas,Masoud H. Nazari*

Main category: cs.LG

TL;DR: 提出基于学习的极端事件停电预测框架，整合多源数据，评估四种机器学习模型，LSTM表现最佳，发现经济条件和基础设施发展程度与停电发生率负相关。


<details>
  <summary>Details</summary>
Motivation: 针对低概率高后果的停电场景，现有预测方法可能不够准确，需要整合更多维度的数据来理解社区脆弱性和极端条件下的停电风险。

Method: 整合EAGLE-I停电记录（2014-2024）与天气、社会经济、基础设施和季节性事件数据，评估随机森林、支持向量机、自适应提升和LSTM四种机器学习模型。

Result: 在密歇根下半岛县区的大规模数据集上验证，LSTM网络获得最低预测误差，同时发现更强的经济条件和更发达的基础设施与更低的停电发生率相关。

Conclusion: 提出的学习框架能有效预测极端事件导致的停电，LSTM模型表现最佳，社会经济和基础设施因素对停电风险有显著影响，为电力系统韧性规划提供重要见解。

Abstract: This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.

</details>


### [107] [What Matters in Deep Learning for Time Series Forecasting?](https://arxiv.org/abs/2512.22702)
*Valentina Moretti,Andrea Cini,Ivan Marisca,Cesare Alippi*

Main category: cs.LG

TL;DR: 该论文分析了时间序列预测深度学习架构的设计空间，指出简单但设计良好的架构往往能与最先进方法匹敌，强调了基于时间序列组预测原则进行模型设计的重要性，并提出了辅助预测模型卡来规范架构设计。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列应用中日益流行，但大量新提出的架构和相互矛盾的实证结果使得难以评估哪些组件对最终性能有显著贡献。作者旨在通过讨论设计维度和权衡来解释观察到的结果，理清时间序列预测深度学习架构的设计空间。

Method: 论文讨论了基于时间序列组预测原则进行模型设计的必要性，评估了局部性和全局性等概念在近期预测架构中的应用。分析了现有架构中被忽视的实现细节如何改变预测方法的类别并影响实证结果。提出了辅助预测模型卡来表征现有和新预测架构的关键设计选择。

Result: 研究表明，考虑局部性和全局性等设计原则比采用特定的序列建模层对实现准确结果更为重要。简单但设计良好的预测架构往往能与最先进方法匹敌。被忽视的实现细节会从根本上改变预测方法的类别并显著影响实证结果。

Conclusion: 当前有缺陷的基准测试实践需要重新思考，设计架构时需要关注预测问题的基本方面。提出的辅助预测模型卡可作为朝此方向迈出的一步，帮助表征预测架构的关键设计选择。

Abstract: Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.

</details>


### [108] [FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents](https://arxiv.org/abs/2512.22733)
*Jiaqi Shao,Yufeng Miao,Wei Zhang,Bing Luo*

Main category: cs.LG

TL;DR: FoldAct框架解决了长视野RL中上下文折叠方法的三个核心挑战：梯度稀释、自条件化和计算成本，通过分离损失计算、全上下文一致性损失和选择性片段训练实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 长视野强化学习面临上下文无限增长的扩展性挑战，现有上下文折叠方法将摘要动作视为标准动作，忽略了摘要会改变智能体未来观察空间，导致策略依赖的非平稳观察分布，违反了RL核心假设。

Method: FoldAct框架包含三个关键创新：1) 分离损失计算，为摘要和动作token提供独立梯度信号；2) 全上下文一致性损失，减少分布偏移；3) 选择性片段训练，降低计算成本。

Result: 该方法实现了长视野搜索智能体的稳定训练，解决了非平稳观察问题，同时将训练效率提高了5.19倍。

Conclusion: FoldAct通过明确处理上下文折叠中的非平稳观察问题，为长视野RL提供了稳定高效的训练框架，解决了现有方法的核心局限性。

Abstract: Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.

</details>


### [109] [When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction](https://arxiv.org/abs/2512.22740)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 多任务学习在合金材料预测中呈现分化结果：回归任务性能显著下降，但分类任务性能提升，主要原因是数据不平衡和属性独立性


<details>
  <summary>Details</summary>
Motivation: 测试多任务学习（MTL）在材料科学中的有效性，假设相关材料属性共享底层物理机制，可以相互促进预测性能

Method: 使用54,028个合金样本，同时预测电阻率、维氏硬度和非晶形成能力，比较单任务模型与标准和结构化多任务学习模型

Result: MTL显著降低回归性能（电阻率R²: 0.897→0.844；硬度R²: 0.832→0.694），但改善分类性能（非晶形成能力F1: 0.703→0.744，召回率提升17%），分析显示任务间权重接近零，表明属性独立

Conclusion: 对于精确回归任务建议使用独立模型，而对于分类任务（特别是召回率关键时）可考虑使用多任务学习，回归失败主要归因于数据严重不平衡（52k vs. 800样本）导致的负迁移

Abstract: Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\to$ 0.844; hardness $R^2$: 0.832 $\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.

</details>


### [110] [Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL](https://arxiv.org/abs/2512.22744)
*Rihong Qiu,Zhibang Yang,Xinke Jiang,Weibin Liao,Xin Gao,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: HEROSQL提出了一种层次化SQL表示方法，结合逻辑计划(LPs)和抽象语法树(ASTs)，用于Text-to-SQL语义验证，通过NMPNN捕获SQL关系信息，并采用AST驱动的子SQL增强策略生成高质量负样本。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL验证方法大多只关注语法正确性，缺乏语义验证（检测问题与SQL之间的不一致）。语义验证面临两个关键挑战：1) 同时捕获全局用户意图和SQL结构细节；2) 构建高质量细粒度子SQL标注。

Method: 提出HEROSQL层次化SQL表示方法：1) 集成全局意图（逻辑计划LPs）和局部细节（抽象语法树ASTs）；2) 使用嵌套消息传递神经网络(NMPNN)捕获SQL内在关系信息并聚合模式引导的语义；3) 提出AST驱动的子SQL增强策略生成高质量负样本，优化细粒度语义不一致性。

Result: 在Text-to-SQL验证基准测试（域内和域外设置）中，方法优于现有最先进方法，AUPRC平均提升9.40%，AUROC平均提升12.35%。能够有效检测细粒度语义错误，为大型语言模型提供更细粒度反馈，增强数据查询平台的可靠性和可解释性。

Conclusion: HEROSQL通过层次化SQL表示和NMPNN有效解决了Text-to-SQL语义验证的关键挑战，显著提升了语义不一致检测性能，为数据查询平台提供了更可靠和可解释的验证机制。

Abstract: Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.

</details>


### [111] [From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms](https://arxiv.org/abs/2512.22749)
*Rui Ai,David Simchi-Levi,Feng Zhu*

Main category: cs.LG

TL;DR: 研究第三方平台面对战略代理时的定价行为，开发了在混淆条件下学习需求的算法，实现了最优后悔界，并揭示了供应侧噪声对需求可学习性的根本影响。


<details>
  <summary>Details</summary>
Motivation: 第三方平台作为收入最大化者，需要根据市场特征进行定价，但只能观察到均衡价格和数量，这构成了一个在混淆条件下的一般需求学习问题。

Method: 开发了具有最优后悔界的算法，利用非独立同分布行动作为工具变量学习需求，提出新颖的同胚构造方法，无需假设星形结构，首次为深度神经网络学习需求提供了效率保证。

Result: 算法实现了 $\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$ 的最优后悔界，揭示了供应侧噪声导致后悔的相变现象，通过模拟和Zomato、Lyft的真实数据验证了方法的实用性。

Conclusion: 供应侧噪声从根本上影响需求的可学习性，非独立同分布行动可作为学习需求的工具变量，提出的同胚构造方法为深度神经网络学习需求提供了理论保证，方法在实际应用中具有良好效果。

Abstract: We study the pricing behavior of third-party platforms facing strategic agents. Assuming the platform is a revenue maximizer, it observes market features that generally affect demand. Since only the equilibrium price and quantity are observable, this presents a general demand learning problem under confounding. Mathematically, we develop an algorithm with optimal regret of $\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$. Our results reveal that supply-side noise fundamentally affects the learnability of demand, leading to a phase transition in regret. Technically, we show that non-i.i.d. actions can serve as instrumental variables for learning demand. We also propose a novel homeomorphic construction that allows us to establish estimation bounds without assuming star-shapedness, providing the first efficiency guarantee for learning demand with deep neural networks. Finally, we demonstrate the practical applicability of our approach through simulations and real-world data from Zomato and Lyft.

</details>


### [112] [A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants](https://arxiv.org/abs/2512.22758)
*Eswarasanthosh Kumar Mamillapalli,Nishtha Sharma*

Main category: cs.LG

TL;DR: 提出微-宏观机器学习框架，整合个体层面健康数据与宏观环境特征，预测儿童肥胖风险并识别环境驱动的不平等


<details>
  <summary>Details</summary>
Motivation: 传统流行病学研究通常独立分析个体、家庭和环境层面的风险因素，限制了理解结构性环境条件如何与个体特征相互作用影响健康结果。需要整合多尺度数据集来识别环境驱动的肥胖风险差异。

Method: 1) 整合NHANES个体层面人体测量和社会经济数据；2) 从USDA和EPA数据集中提取宏观结构性环境特征（食品获取、空气质量、社会经济脆弱性）；3) 使用四种机器学习模型（逻辑回归、随机森林、XGBoost、LightGBM）预测肥胖；4) 构建综合环境脆弱性指数（EnvScore）；5) 进行多层次比较分析

Result: XGBoost模型表现最佳；构建了州级环境脆弱性指数；多层次比较显示高环境负担州与全国预测的微观肥胖风险分布具有强烈地理相似性

Conclusion: 证明了整合多尺度数据集识别环境驱动肥胖风险差异的可行性，为公共卫生信息学提供了可扩展的数据驱动多层次建模管道，具有扩展到因果建模、干预规划和实时分析的强大潜力

Abstract: Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.

</details>


### [113] [Understanding the Mechanisms of Fast Hyperparameter Transfer](https://arxiv.org/abs/2512.22768)
*Nikhil Ghosh,Denny Wu,Alberto Bietti*

Main category: cs.LG

TL;DR: 论文提出一个理论框架分析超参数跨规模迁移，证明快速迁移在计算最优网格搜索中比直接调优更高效，并通过μP参数化在宽度缩放中的实证研究验证了该框架。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型规模不断增大，使得标准超参数优化变得极其昂贵。需要研究如何通过小规模网格搜索获得的最优超参数直接迁移到大规模模型，同时保持性能损失最小。

Method: 1. 建立跨规模超参数迁移的概念框架，定义"快速迁移"（迁移导致的次优性渐近消失速度快于有限规模性能差距）
2. 形式化证明快速迁移在计算最优网格搜索中等价于有用迁移
3. 分析最大更新参数化(μP)在宽度缩放中的迁移机制
4. 提出分解优化轨迹的假设：损失减少包含宽度稳定分量（决定最优超参数）和宽度敏感分量（随宽度改善但弱扰动超参数最优值）
5. 在大语言模型预训练等多种设置中进行实证验证

Result: 1. 证明了快速迁移在计算最优网格搜索中比直接调优更高效
2. 发现μP参数化在宽度缩放中表现出快速迁移特性
3. 展示了问题结构对迁移效果的关键影响：在某些合成设置中迁移提供可证明的计算优势，而在其他设置中即使使用μP也无法超越直接调优
4. 通过实证证据支持了优化轨迹分解假设，包括在大语言模型预训练中的验证

Conclusion: 论文建立了超参数跨规模迁移的理论框架，证明了快速迁移的计算优势，揭示了μP参数化实现有效迁移的机制，并通过优化轨迹分解假设解释了实际观察到的快速迁移现象。这为大规模深度学习模型的高效超参数优化提供了理论基础和实践指导。

Abstract: The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.

</details>


### [114] [GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks](https://arxiv.org/abs/2512.22772)
*Xuyan Li,Jie Wang,Zheng Yan*

Main category: cs.LG

TL;DR: GRExplainer：首个通用、高效、用户友好的TGNN解释方法，通过节点序列统一表示、BFS搜索优化和RNN生成模型，解决现有TGNN解释方法在通用性、效率和用户友好性方面的不足。


<details>
  <summary>Details</summary>
Motivation: TGNN在动态图处理中表现出色，但缺乏透明度和可解释性限制了实际应用。现有TGNN解释方法存在三个主要问题：1）针对特定TGNN类型设计，通用性差；2）计算成本高，不适合大规模网络；3）忽略解释的结构连通性且需要先验知识，用户友好性不足。

Method: 提出GRExplainer方法：1）提取节点序列作为统一特征表示，使其独立于特定输入格式，适用于快照式和事件式TGNN；2）利用广度优先搜索（BFS）和时间信息构建输入节点序列，减少冗余计算；3）设计基于循环神经网络（RNN）的生成模型，实现自动化、连续的解释生成。

Result: 在6个真实世界数据集和3个目标TGNN上的实验表明，GRExplainer在通用性、效率和用户友好性方面均优于现有基线方法。

Conclusion: GRExplainer是首个通用、高效、用户友好的TGNN解释方法，通过统一的节点序列表示、优化的搜索策略和生成模型设计，有效解决了现有TGNN解释方法的局限性，为TGNN的实际应用提供了更好的可解释性支持。

Abstract: Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.

</details>


### [115] [Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization](https://arxiv.org/abs/2512.22774)
*Truong Son Nguyen*

Main category: cs.LG

TL;DR: Schrödinger AI是一个受量子力学启发的统一机器学习框架，包含波能求解器、动态求解器和低秩算子演算三个组件，提供可解释的语义和鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在创建一个受量子力学启发的机器学习框架，作为传统交叉熵训练和Transformer注意力的物理驱动替代方案，以提供更好的泛化能力、可解释语义和涌现拓扑结构。

Method: 提出Schrödinger AI框架，包含三个紧密耦合的组件：1) 时间无关波能求解器，将感知和分类视为学习哈密顿量下的谱分解；2) 时间相关动态求解器，控制语义波函数随时间演化，实现上下文感知的决策修订和推理；3) 低秩算子演算，通过学习量子类转移算子学习符号变换。

Result: 实验表明：1) 涌现出反映人类概念类关系的语义流形，无需显式监督；2) 动态推理能适应变化环境，包括实时势场扰动的迷宫导航；3) 在模算术任务上实现精确算子泛化，学习群操作并在远超训练长度的序列上组合它们。

Conclusion: 该框架为机器学习提供了新的基础方向，将学习视为发现和导航底层语义能量景观的过程，展示了物理驱动方法的潜力。

Abstract: We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.
  Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.

</details>


### [116] [Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning](https://arxiv.org/abs/2512.22777)
*Kasra Jalaldoust,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出Circuit-TR算法，通过因果可迁移理论实现零样本组合泛化，利用因果图和机制共享信息构建模块化预测电路


<details>
  <summary>Details</summary>
Motivation: 跨领域泛化需要结构约束来连接源域和目标域，但传统方法缺乏对未见目标域的约束机制，需要结合因果理论和领域知识来实现零样本组合泛化

Method: 基于因果可迁移理论设计Circuit-TR算法：1) 从源数据学习局部预测模块集合；2) 利用因果图（域内结构）和差异预言机（域间机制共享）指导模块迁移；3) 在因果结构许可下组合模块构建目标域预测电路；4) 提出无需显式因果结构的监督域适应方案

Result: 理论分析建立了少样本可学习任务类与图论电路可迁移准则的关系，将少样本泛化能力与电路规模复杂度联系起来，仿真实验验证了理论结果

Conclusion: Circuit-TR通过因果可迁移理论实现了有效的零样本组合泛化，将模块化学习与因果结构约束相结合，为跨领域泛化提供了理论框架和实用算法

Abstract: Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.

</details>


### [117] [Discovering Transmission Dynamics of COVID-19 in China](https://arxiv.org/abs/2512.22787)
*Zhou Yang,Edward Dougherty,Chen Zhang,Zhenhe Pan,Fang Jin*

Main category: cs.LG

TL;DR: 基于中国SARS-CoV-2传播数据的回顾性分析，研究公共卫生干预措施效果，发现大城市感染更多与社会活动相关，感染源随时间从湖北旅行相关转向社会活动传播。


<details>
  <summary>Details</summary>
Motivation: 通过对COVID-19公共卫生干预措施（大规模检测、隔离、接触者追踪）的全面回顾性分析，识别最有效的防控机制，为未来疫情应对提供参考。

Method: 收集中国地方卫健委、中国CDC和官方社交媒体发布的病例报告，应用NLP和人工整理构建传播/追踪链，结合武汉人口流动数据量化分析时空传播动态。

Result: 发现显著地区差异：大城市感染更多与社会活动相关；79%有症状者在症状出现5天内住院；有确诊病例接触史者入院时间更短；感染源随时间从湖北旅行相关转向社会活动传播。

Conclusion: 公共卫生干预措施效果存在地区差异，社会活动是后期主要传播途径，早期快速住院和接触者追踪对控制疫情传播至关重要。

Abstract: A comprehensive retrospective analysis of public health interventions, such as large scale testing, quarantining, and contact tracing, can help identify mechanisms most effective in mitigating COVID-19. We investigate China based SARS-CoV-2 transmission patterns (e.g., infection type and likely transmission source) using publicly released tracking data. We collect case reports from local health commissions, the Chinese CDC, and official local government social media, then apply NLP and manual curation to construct transmission/tracking chains. We further analyze tracking data together with Wuhan population mobility data to quantify and visualize temporal and spatial spread dynamics. Results indicate substantial regional differences, with larger cities showing more infections, likely driven by social activities. Most symptomatic individuals (79\%) were hospitalized within 5 days of symptom onset, and those with confirmed-case contact sought admission in under 5 days. Infection sources also shifted over time: early cases were largely linked to travel to (or contact with travelers from) Hubei Province, while later transmission was increasingly associated with social activities.

</details>


### [118] [SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance](https://arxiv.org/abs/2512.22792)
*Shuai Chen,Chen Wang,Ziran Wang*

Main category: cs.LG

TL;DR: SNM-Net是一个用于开放集气体识别的深度学习框架，通过几何解耦机制和Mahalanobis距离评分，有效解决了电子鼻系统中的特征分布漂移和未知干扰问题。


<details>
  <summary>Details</summary>
Motivation: 电子鼻系统在开放集气体识别中面临两个主要挑战：信号漂移导致特征分布偏移，以及未知干扰引起的决策失败。现有方法主要依赖欧氏距离，未能充分考虑各向异性的气体特征分布和动态信号强度变化。

Method: 提出SNM-Net框架，核心创新包括：1）通过级联批归一化和L2归一化实现几何解耦机制，将高维特征投影到单位超球面上以消除信号强度波动；2）引入Mahalanobis距离作为评分机制，利用类统计信息构建自适应椭圆决策边界。该框架与CNN、RNN和Transformer等骨干网络兼容。

Result: 在Vergara数据集上的系统实验表明，Transformer+SNM配置达到接近理论性能：AUROC为0.9977，未知气体检测率为99.57%（5% FPR下的TPR）。相比最先进方法，AUROC提升3.0%，标准差降低91.0%（相比Class Anchor Clustering）。在不同传感器位置下表现出卓越鲁棒性，标准差低于0.0028。

Conclusion: SNM-Net有效解决了准确性与稳定性之间的权衡问题，为工业电子鼻部署提供了坚实的技术基础。该框架通过几何解耦和自适应决策边界，显著提升了开放集气体识别的性能。

Abstract: Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.

</details>


### [119] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出基于强化学习的扩散模型蒸馏框架，将蒸馏过程视为策略优化问题，通过奖励信号动态指导学生模型，实现更少推理步骤和计算资源的高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法依赖固定的重建损失或一致性损失，无法充分利用扩散模型处理大步骤的能力，限制了蒸馏效率和性能。

Method: 将扩散模型蒸馏视为策略优化问题，学生模型通过从教师模型输出对齐度导出的奖励信号进行训练，动态探索多个去噪路径，采取更长的优化步骤。

Result: 实验结果显示，该方法在显著减少推理步骤和计算资源的情况下，性能优于现有蒸馏技术，且框架与模型无关，适用于各种扩散模型。

Conclusion: 基于强化学习的蒸馏框架为扩散模型提供了一种通用的优化范式，能够有效利用扩散模型处理大步骤的能力，实现高效生成。

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [120] [MoR: Mixture Of Representations For Mixed-Precision Training](https://arxiv.org/abs/2512.22804)
*Bor-Yiing Su,Peter Dykas,Mike Chrzanowski,Jatin Chhugani*

Main category: cs.LG

TL;DR: MoR是一种新颖的混合精度量化框架，通过动态分析张量数值特性，在FP8和BF16表示之间进行选择，实现了98.38%张量量化为FP8格式，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 混合精度训练是扩展深度学习模型的关键技术，但成功实施需要找到合适的训练方法组合。现有方法通常需要精细的分区策略，而本文旨在开发一种更通用、动态的量化框架。

Method: 提出Mixture-of-Representations (MoR)框架，这是一种按张量和子张量级别的量化方法。框架动态分析张量的数值特性，在多种表示（如FP8和BF16）之间进行选择。提出了具体的算法实现，支持不同粒度的量化分区策略。

Result: 初步结果显示，该方法能够将98.38%的张量量化为FP8格式，同时保持模型质量。在不需要精细分区的情况下，达到了与现有方法相当的FP8精度，展示了动态量化方法的潜力。

Conclusion: MoR框架展示了动态、属性感知量化的潜力，能够保持模型质量的同时提高低精度训练的鲁棒性。该方法可以单独使用，也可以与其他训练方法结合，为使用更低精度格式（如NVFP4）铺平道路。

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

</details>


### [121] [Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models](https://arxiv.org/abs/2512.22814)
*Scott A. Martin,Noah Brenowitz,Dale Durran,Michael Pritchard*

Main category: cs.LG

TL;DR: 提出长程蒸馏方法，用自回归教师模型生成大量合成气候数据，训练单步长时步学生模型进行长期天气预报，在完美模型实验中接近教师模型性能，在真实世界中达到与ECMWF集合预报相当的次季节-季节预报技能。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气模型采用自回归方式，在长期预报中误差累积且不稳定；而单步长时步概率模型需要大量训练数据，但再分析数据集仅有40年记录，容易过拟合。需要解决长期天气预报中数据不足和误差累积的问题。

Method: 提出长程蒸馏方法：使用DLESyM作为自回归教师模型生成超过10,000年的合成气候数据，用这些数据训练单步长时步概率学生模型，学生模型可以直接进行长期预报而无需多次自回归步骤。

Result: 在完美模型实验中，蒸馏模型优于气候学基准，接近自回归教师模型的技能，同时用单步替代了数百个自回归步骤。在真实世界中，经过ERA5微调后，其S2S预报技能与ECMWF集合预报相当。模型技能随合成训练数据量增加而提升。

Conclusion: 首次证明AI生成的合成训练数据可以扩展长期预报技能，长程蒸馏方法为解决长期天气预报中的数据不足问题提供了有效途径，能够在不依赖大量真实数据的情况下实现高质量的长期预报。

Abstract: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic "student" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive "teacher" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.

</details>


### [122] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出基于时间方差驱动课程的学生-教师学习范式，通过教师模块动态选择策略置信度方差最大的目标，加速目标条件强化学习


<details>
  <summary>Details</summary>
Motivation: 传统多目标强化学习中均匀目标选择导致样本效率低下，受生物系统自适应结构化学习过程启发，需要更高效的目标选择方法

Method: 学生-教师学习范式，教师模块基于状态-动作值函数的时间方差动态优先选择高不确定性目标，提供自适应学习信号，算法无关且可集成到现有RL框架

Result: 在11个机器人操作和迷宫导航任务中评估，相比最先进的课程学习和目标选择方法取得一致且显著的性能提升

Conclusion: 时间方差驱动的自适应课程学习能有效加速目标条件强化学习，理论分析建立了Q值时间方差与策略演化的联系

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [123] [Fundamental Novel Consistency Theory: $H$-Consistency Bounds](https://arxiv.org/abs/2512.22880)
*Yutao Zhong*

Main category: cs.LG

TL;DR: 该论文提出了H-一致性边界理论，为机器学习中代理损失函数与目标损失函数之间的估计误差提供了更强的理论保证，覆盖了二元分类、多分类、对抗性场景等多种情况。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练中优化的损失函数（代理损失）通常与定义任务性能的目标损失不同，这源于计算不可行性或不可微性。现有理论保证如贝叶斯一致性或H-校准不够强，需要更精确的理论框架来分析代理损失与目标损失之间的估计误差关系。

Method: 提出了H-一致性边界理论框架，该框架考虑了假设集H的影响。首先在二元分类中建立了紧致的分布依赖和独立边界，分析了凸代理损失（线性模型和神经网络）以及对抗性场景下的ρ-边界和sigmoid损失。扩展到多分类，提出了max、sum和约束损失的H-一致性边界。研究了comp-sum损失（如交叉熵、MAE）并引入了平滑对抗变体。建立了统一的边界推导框架，分析了边界增长率和最小化间隙。

Result: 建立了首个针对多种损失函数的H-一致性边界理论：1）二元分类的紧致边界；2）多分类中max、sum和约束损失的首个边界；3）comp-sum损失（交叉熵、MAE）的首个边界；4）证明了在某些情况下非平凡的H-一致性边界不可达；5）建立了平滑代理损失在二元和多分类任务中的通用平方根增长率；6）分析了最小化间隙以指导代理损失选择。

Conclusion: H-一致性边界提供了比贝叶斯一致性或H-校准更强的理论保证，比超额误差边界更具信息性。该框架为理解代理损失与目标损失之间的关系提供了系统理论，有助于指导损失函数选择和算法设计，特别是在对抗性场景下。研究还揭示了边界增长的基本规律和损失函数选择的指导原则。

Abstract: In machine learning, the loss functions optimized during training often differ from the target loss that defines task performance due to computational intractability or lack of differentiability. We present an in-depth study of the target loss estimation error relative to the surrogate loss estimation error. Our analysis leads to $H$-consistency bounds, which are guarantees accounting for the hypothesis set $H$. These bounds offer stronger guarantees than Bayes-consistency or $H$-calibration and are more informative than excess error bounds.
  We begin with binary classification, establishing tight distribution-dependent and -independent bounds. We provide explicit bounds for convex surrogates (including linear models and neural networks) and analyze the adversarial setting for surrogates like $ρ$-margin and sigmoid loss. Extending to multi-class classification, we present the first $H$-consistency bounds for max, sum, and constrained losses, covering both non-adversarial and adversarial scenarios. We demonstrate that in some cases, non-trivial $H$-consistency bounds are unattainable. We also investigate comp-sum losses (e.g., cross-entropy, MAE), deriving their first $H$-consistency bounds and introducing smooth adversarial variants that yield robust learning algorithms.
  We develop a comprehensive framework for deriving these bounds across various surrogates, introducing new characterizations for constrained and comp-sum losses. Finally, we examine the growth rates of $H$-consistency bounds, establishing a universal square-root growth rate for smooth surrogates in binary and multi-class tasks, and analyze minimizability gaps to guide surrogate selection.

</details>


### [124] [Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral](https://arxiv.org/abs/2512.22886)
*Anqi Mao*

Main category: cs.LG

TL;DR: 该论文系统研究了多专家延迟学习问题，包括分类中的弃权和回归中的延迟，提出了新的代理损失函数并证明了强一致性保证，实验验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉和高推理成本问题，通过将不确定输入延迟给更强大的专家可以提高可靠性，将简单查询路由到小型蒸馏模型可以提高效率，这激发了多专家延迟学习问题的研究。

Method: 1) 针对分类中的弃权（延迟的特殊情况），分析基于分数和预测器-拒绝器公式，提出新的代理损失函数族并证明强一致性保证；2) 针对一般多专家延迟分类，设计单阶段和两阶段场景的代理损失函数；3) 针对回归延迟，提出支持多专家和多种成本结构的通用框架。

Result: 在CIFAR-10、CIFAR-100和SVHN数据集上的实验表明，提出的算法性能优越；理论分析证明了代理损失函数具有强H-一致性边界，对于两阶段场景，在常数成本函数下实现了可实现的H-一致性。

Conclusion: 该论文为多专家延迟学习问题提供了全面的理论分析和实用算法，解决了现有开放问题，并在分类和回归任务中实现了可靠的延迟决策，为实际应用提供了理论保证。

Abstract: Large language models (LLMs) have achieved remarkable performance but face critical challenges: hallucinations and high inference costs. Leveraging multiple experts offers a solution: deferring uncertain inputs to more capable experts improves reliability, while routing simpler queries to smaller, distilled models enhances efficiency. This motivates the problem of learning with multiple-expert deferral. This thesis presents a comprehensive study of this problem and the related problem of learning with abstention, supported by strong consistency guarantees.
  First, for learning with abstention (a special case of deferral), we analyze score-based and predictor-rejector formulations in multi-class classification. We introduce new families of surrogate losses and prove strong non-asymptotic, hypothesis set-specific consistency guarantees, resolving two existing open questions. We analyze both single-stage and practical two-stage settings, with experiments on CIFAR-10, CIFAR-100, and SVHN demonstrating the superior performance of our algorithms.
  Second, we address general multi-expert deferral in classification. We design new surrogate losses for both single-stage and two-stage scenarios and prove they benefit from strong $H$-consistency bounds. For the two-stage scenario, we show that our surrogate losses are realizable $H$-consistent for constant cost functions, leading to effective new algorithms.
  Finally, we introduce a novel framework for regression with deferral to address continuous label spaces. Our versatile framework accommodates multiple experts and various cost structures, supporting both single-stage and two-stage methods. It subsumes recent work on regression with abstention. We propose new surrogate losses with proven $H$-consistency and demonstrate the empirical effectiveness of the resulting algorithms.

</details>


### [125] [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)
*S. Dai,G. Sun,F. Li,X. Tang,Q. Wang,Y. Cong*

Main category: cs.LG

TL;DR: 提出联邦多任务聚类框架FMTC，解决传统谱聚类无法适应去中心化环境的问题，通过客户端个性化聚类模块和服务端张量相关模块，在保护隐私的同时提升异构客户端的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类算法主要针对集中式设置，无法适应现代去中心化环境。现有的联邦学习方法依赖不可靠的伪标签，泛化性能差，且未能捕捉异构客户端之间的潜在相关性。

Method: 提出FMTC框架，包含两个核心组件：1) 客户端个性化聚类模块，学习参数化映射模型支持鲁棒的样本外推理，避免使用不可靠的伪标签；2) 服务端张量相关模块，将所有客户端模型组织成统一张量，应用低秩正则化发现其共同子空间。采用基于ADMM的高效隐私保护分布式算法进行优化。

Result: 在多个真实世界数据集上的实验表明，FMTC框架显著优于各种基线和最先进的联邦聚类算法。

Conclusion: FMTC框架成功解决了联邦聚类中的异构性和隐私保护问题，通过个性化建模和共享知识发现，实现了优越的聚类性能，为去中心化环境下的聚类任务提供了有效解决方案。

Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.

</details>


### [126] [Debugging Tabular Log as Dynamic Graphs](https://arxiv.org/abs/2512.22903)
*Chumeng Liang,Zhanyang Jin,Zahaib Akhtar,Mona Pereira,Haofei Yu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出GraphLogDebugger框架，基于动态图调试表格日志，用简单动态GNN超越LLMs性能


<details>
  <summary>Details</summary>
Motivation: 现有处理文本丰富的表格日志数据的方法过度依赖大语言模型和其他重负载模型，导致灵活性和可扩展性有限

Method: 构建异构节点表示对象和事件，连接节点边，将表格日志背后的系统恢复为演化动态图，使用简单动态图神经网络

Result: 在计算机系统和学术论文的真实日志数据集上验证，动态图建模使简单动态GNN在调试表格日志方面优于LLMs

Conclusion: GraphLogDebugger框架通过动态图建模有效解决表格日志调试问题，简单动态GNN比复杂LLMs更具优势

Abstract: Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.

</details>


### [127] [MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning](https://arxiv.org/abs/2512.22904)
*Jin Wu,Chanjin Zheng*

Main category: cs.LG

TL;DR: 提出MetaCD框架，结合元学习和持续学习解决认知诊断中的长尾分布和动态变化问题


<details>
  <summary>Details</summary>
Motivation: 现有认知诊断方法受限于数据的长尾分布和动态变化，需要提升模型对新技能和新任务的适应能力

Method: 基于持续学习的元学习框架MetaCD：1）使用元学习学习最优初始化状态以缓解长尾问题；2）采用参数保护机制的持续学习方法适应新技能/任务

Result: 在五个真实数据集上的实验表明，MetaCD在准确率和泛化能力上均优于其他基线方法

Conclusion: MetaCD框架能同时提升模型在单个任务上的可塑性以及在序列任务上的稳定性和泛化能力

Abstract: Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.

</details>


### [128] [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)
*Ünver Çiftçi*

Main category: cs.LG

TL;DR: Sat-EnQ：一种两阶段深度Q学习框架，先通过"满意化"目标训练轻量级Q网络集成，避免早期过估计，再蒸馏到大型网络进行优化，显著提升稳定性和降低方差。


<details>
  <summary>Details</summary>
Motivation: 深度Q学习算法在早期训练时不稳定，最大化操作会放大估计误差，导致灾难性过估计和训练失败。受有限理性理论和发育学习启发，需要一种更稳健的方法。

Method: 1. 第一阶段：使用"满意化"目标训练轻量级Q网络集成，通过动态基线限制早期价值增长，产生多样化、低方差的估计；2. 第二阶段：将集成蒸馏到更大的网络，使用标准Double DQN进行微调。

Result: 理论证明满意化能诱导有界更新且不会增加目标方差；实证显示：方差减少3.8倍，消除灾难性失败（0% vs DQN的50%），在环境噪声下保持79%性能，计算需求比自举集成少2.5倍。

Conclusion: Sat-EnQ通过先满意化后优化的原则性路径，为稳健强化学习提供了有效框架，显著提升了深度Q学习的稳定性和效率。

Abstract: Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.

</details>


### [129] [Multiple Token Divergence: Measuring and Steering In-Context Computation Density](https://arxiv.org/abs/2512.22944)
*Vincent Herrmann,Eric Alcaide,Michael Wand,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 提出MTD（多令牌散度）作为衡量语言模型上下文计算努力的新指标，通过比较完整输出分布与浅层辅助预测头分布的KL散度来评估推理复杂度，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有衡量语言模型计算努力的方法（如下一令牌损失）无法捕捉推理复杂度，而基于潜在状态可压缩性的方法具有侵入性和不稳定性，需要更简单有效的度量指标。

Method: 提出MTD指标，定义为模型完整输出分布与浅层辅助预测头输出分布之间的KL散度。基于此开发Divergence Steering解码方法，控制生成文本的计算特征。方法可直接应用于预训练的多预测头模型，无需额外训练。

Result: MTD比先前方法更能有效区分复杂任务与简单任务；在数学推理基准测试中，MTD与问题难度呈正相关；较低的MTD与更准确的推理相关；MTD为分析和引导语言模型计算动态提供了实用轻量级工具。

Conclusion: MTD是一种简单有效的语言模型计算努力度量指标，无需额外训练即可从预训练模型中计算，为分析和控制语言模型的计算特征提供了新方法。

Abstract: Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.

</details>


### [130] [APO: Alpha-Divergence Preference Optimization](https://arxiv.org/abs/2512.22953)
*Wang Zixian*

Main category: cs.LG

TL;DR: APO提出了一种基于alpha散度的锚定对齐框架，可在前向KL和后向KL之间连续插值，通过奖励和置信度引导的alpha调度实现从覆盖到利用的平稳过渡。


<details>
  <summary>Details</summary>
Motivation: 当前对齐方法存在两种主要分歧：监督微调和蒸馏方法最小化前向KL散度（KL(q||π_θ)），虽然稳定但可能未充分利用高奖励模式；而PPO风格的在线RLHF更接近后向KL散度（KL(π_θ||q)），能进行模式寻求改进但存在模式崩溃风险。现有锚定方法通常只使用单一散度，缺乏灵活性。

Method: 提出了Alpha-Divergence Preference Optimization (APO)，这是一个基于Csiszar alpha散度的锚定框架，可在前向KL和后向KL行为之间连续插值。推导了参数化alpha的统一梯度动态，分析了梯度方差特性，并提出了实用的奖励和置信度引导的alpha调度策略，仅在策略改进且置信度校准良好时才从覆盖过渡到利用。

Result: 在Qwen3-1.7B模型上的math-level3实验中，APO在保持训练稳定性的同时，实现了与GRPO和GSPO基线相当的竞争性能。

Conclusion: APO提供了一个统一的锚定框架，通过alpha散度连续插值实现了前向KL和后向KL行为的灵活平衡，解决了现有方法在稳定性和模式利用之间的权衡问题，同时保持了训练稳定性。

Abstract: Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

</details>


### [131] [FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing](https://arxiv.org/abs/2512.22956)
*Wafaa El Husseini*

Main category: cs.LG

TL;DR: FLOW是一个合成纵向数据集，用于模拟工作负荷、生活方式行为和幸福感之间的日常互动，支持压力建模、行为分析和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、伦理和后勤限制，获取关于工作与生活平衡和幸福感的纵向个体级数据有限，这给可重复研究、方法基准测试和教育带来了挑战。

Method: 使用基于规则、反馈驱动的模拟生成合成数据集，模拟1000名个体在两年内的日常数据，涵盖压力、睡眠、情绪、身体活动和体重等变量。

Result: 创建了FLOW数据集和可配置的数据生成工具，为无法访问真实世界数据的研究提供了受控实验环境。

Conclusion: FLOW作为一个合成数据集，支持探索性分析、方法开发和基准测试，填补了真实数据不可获取时的研究空白。

Abstract: Access to longitudinal, individual-level data on work-life balance and wellbeing is limited by privacy, ethical, and logistical constraints. This poses challenges for reproducible research, methodological benchmarking, and education in domains such as stress modeling, behavioral analysis, and machine learning.
  We introduce FLOW, a synthetic longitudinal dataset designed to model daily interactions between workload, lifestyle behaviors, and wellbeing. FLOW is generated using a rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1{,}000 individuals over a two-year period with daily resolution and is released as a publicly available resource.
  In addition to the static dataset, we describe a configurable data generation tool that enables reproducible experimentation under adjustable behavioral and contextual assumptions. FLOW is intended as a controlled experimental environment rather than a proxy for observed human populations, supporting exploratory analysis, methodological development, and benchmarking where real-world data are inaccessible.

</details>


### [132] [A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging](https://arxiv.org/abs/2512.22976)
*Amirali Vakili,Salar Jahanshiri,Armin Salimi-Badr*

Main category: cs.LG

TL;DR: 提出一个用于单通道EEG睡眠分期的上下文感知可解释框架，特别关注改善N1期检测，在SleepEDF数据集上达到89.72%准确率和85.46%宏平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 自动睡眠分期对睡眠障碍诊疗至关重要，但现有单通道EEG方法面临类别不平衡、感受野有限、可解释性不足等挑战，特别是N1期检测困难。

Method: 结合紧凑多尺度特征提取与时间建模，使用类别加权损失函数和数据增强处理不平衡，将EEG信号分块处理并通过平均softmax概率获得最终预测。

Result: 在SleepEDF数据集上达到89.72%总体准确率和85.46%宏平均F1分数，N1期F1分数达61.7%，相比先前方法有显著提升。

Conclusion: 该框架有效提升了睡眠分期性能，特别是N1期检测，同时保持可解释性和临床适用性，为实际医疗应用提供了实用解决方案。

Abstract: Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.

</details>


### [133] [Fusion or Confusion? Multimodal Complexity Is Not All You Need](https://arxiv.org/abs/2512.22991)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: 复杂多模态学习方法在标准化实验条件下并不比简单的后期融合Transformer基准方法表现更好，研究呼吁从追求架构新颖性转向方法论严谨性。


<details>
  <summary>Details</summary>
Motivation: 挑战当前多模态学习领域的一个基本假设：更复杂的多模态特定方法必然带来性能提升。通过大规模实证研究检验这一假设，揭示现有文献中的方法论缺陷。

Method: 提出SimBaMM（Simple Baseline for Multimodal Learning），一个简单的后期融合Transformer架构。在标准化条件下重新实现19种高影响力方法，在9个多样化数据集（最多23种模态）上进行评估，测试它们在新任务和缺失模态情况下的泛化能力。

Result: 在严格的超参数调优和标准化实验条件下，更复杂的架构并不能可靠地超越SimBaMM。统计分析表明，复杂方法与SimBaMM表现相当，且经常无法可靠地超越调优良好的单模态基线，特别是在许多原始研究考虑的小数据场景中。

Conclusion: 多模态学习领域需要从追求架构新颖性转向方法论严谨性。研究提供了实用性可靠性检查清单，以促进未来可比较、稳健和可信的评估。复杂方法并不总是优于简单基准。

Abstract: Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.

</details>


### [134] [Merge before Forget: A Single LoRA Continual Learning via Continual Merging](https://arxiv.org/abs/2512.23017)
*Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出一种新的持续学习方法，通过正交初始化和顺序合并LoRA更新到单个统一LoRA中，解决现有方法内存增长和任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA持续学习方法存在两个主要问题：1) 内存随任务数量增长且存储空间有限；2) 缺乏有效的LoRA合并机制导致任务干扰。需要一种能保持恒定内存复杂度并最小化任务干扰的方法。

Method: 1) 从已学习LoRA中提取正交基来初始化新任务学习；2) 利用LoRA组件内在不对称性，使用时感知缩放机制在持续合并过程中平衡新旧知识；3) 顺序合并LoRA更新到单个统一LoRA中。

Result: 方法在多个持续学习基准测试中表现出色，保持与任务数量无关的恒定内存复杂度，通过正交基初始化最小化任务干扰，并通过自适应缩放提高性能。

Conclusion: 提出的正交初始化和顺序合并LoRA方法有效解决了持续学习中的内存增长和任务干扰问题，在效率和性能上都优于现有方法。

Abstract: Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.

</details>


### [135] [Mechanistic Analysis of Circuit Preservation in Federated Learning](https://arxiv.org/abs/2512.23043)
*Muhammad Haseeb,Salaar Masood,Muhammad Abdullah Sohail*

Main category: cs.LG

TL;DR: 通过机制可解释性分析发现，联邦学习中非独立同分布数据导致客户端电路崩溃，而非统计漂移


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下性能显著下降，但内部机制原因仍不明确，需要从机制可解释性角度揭示失败模式

Method: 使用机制可解释性分析FedAvg算法，训练可解释的权重稀疏神经网络，通过IoU指标量化电路在不同客户端和通信轮次中的保持情况

Result: 首次提供机制证据表明，非独立同分布数据导致结构不同的本地电路发散，进而在全局模型中退化，电路崩溃是性能下降的直接原因

Conclusion: 将联邦学习中的统计漂移问题重新定义为可观察的机制保持失败，为更有针对性的解决方案铺平道路

Abstract: Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting client updates leads to circuit collapse, the destructive interference of functional, sparse sub-networks responsible for specific class predictions. By training inherently interpretable, weight-sparse neural networks within an FL framework, we identify and track these circuits across clients and communication rounds. Using Intersection-over-Union (IoU) to quantify circuit preservation, we provide the first mechanistic evidence that Non-IID data distributions cause structurally distinct local circuits to diverge, leading to their degradation in the global model. Our findings reframe the problem of statistical drift in FL as a concrete, observable failure of mechanistic preservation, paving the way for more targeted solutions.

</details>


### [136] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: 提出PI-MFM框架，通过物理约束增强多模态基础模型，在训练和适应阶段直接强制执行控制方程，实现数据高效、可迁移的PDE求解器。


<details>
  <summary>Details</summary>
Motivation: 现有多算子学习方法数据需求大且训练时忽略物理约束，需要开发能直接利用物理方程进行训练的数据高效方法。

Method: PI-MFM框架以PDE符号表示为输入，通过向量化导数计算自动组装PDE残差损失，使多模态基础模型能在统一物理约束目标下训练和适应。

Result: 在13个参数化一维时变PDE基准测试中，PI-MFM始终优于纯数据驱动方法，尤其在稀疏标记点、部分观测时间域或少量标记函数对情况下。物理损失提高抗噪性，重采样配置点策略显著提升精度。

Conclusion: PI-MFM为数据高效、可迁移的PDE求解器提供了实用且可扩展的路径，支持零样本物理约束微调到未见PDE族，仅用PDE残差和初始/边界条件即可快速降低测试误差至约1%。

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [137] [Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution](https://arxiv.org/abs/2512.23068)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li,Yinliang Diao*

Main category: cs.LG

TL;DR: 提出Phase Gradient Flow (PGF)框架，解决Selective State Space Models反向传播时的O(L)内存瓶颈，实现O(1)内存复杂度，支持基因组规模建模。


<details>
  <summary>Details</summary>
Motivation: Selective State Space Models (SSMs)虽然实现线性时间推理，但其基于梯度的敏感性分析在反向传播时受到O(L)内存缩放瓶颈，这阻碍了在消费级硬件上进行基因组规模建模(L > 10^5)。

Method: 引入Phase Gradient Flow (PGF)框架，通过在状态空间流形上直接操作计算精确解析导数，避免构建中间计算图。通过将SSM动态重构为Tiled Operator-Space Evolution (TOSE)，实现相对于序列长度的O(1)内存复杂度。

Result: PGF相比标准Autograd减少94%峰值VRAM，吞吐量提高23倍。在128,000步序列的脉冲响应基准测试中，PGF能够处理传统Autograd因内存限制而失败的规模，确保数值稳定性。

Conclusion: PGF使单GPU上的染色体规模敏感性分析成为可能，弥合了理论无限上下文模型与实际硬件限制之间的差距。

Abstract: Selective State Space Models (SSMs) achieve linear-time inference, yet their gradient-based sensitivity analysis remains bottlenecked by O(L) memory scaling during backpropagation. This memory constraint precludes genomic-scale modeling (L > 10^5) on consumer-grade hardware. We introduce Phase Gradient Flow (PGF), a framework that computes exact analytical derivatives by operating directly in the state-space manifold, bypassing the need to materialize the intermediate computational graph. By reframing SSM dynamics as Tiled Operator-Space Evolution (TOSE), our method delivers O(1) memory complexity relative to sequence length, yielding a 94% reduction in peak VRAM and a 23x increase in throughput compared to standard Autograd. Unlike parallel prefix scans that exhibit numerical divergence in stiff ODE regimes, PGF ensures stability through invariant error scaling, maintaining near-machine precision across extreme sequences. We demonstrate the utility of PGF on an impulse-response benchmark with 128,000-step sequences - a scale where conventional Autograd encounters prohibitive memory overhead, often leading to out-of-memory (OOM) failures in multi-layered models. Our work enables chromosome-scale sensitivity analysis on a single GPU, bridging the gap between theoretical infinite-context models and practical hardware limitations.

</details>


### [138] [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)
*Boyang Zhang,Xiaobing Chen,Songyang Zhang,Shuai Zhang,Xiangwei Zhou,Mingxuan Sun*

Main category: cs.LG

TL;DR: FLEX-MoE：一种联邦学习中的MoE框架，通过联合优化专家分配和负载均衡来解决边缘设备容量限制和非IID数据导致的专家负载不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中部署MoE模型面临两个关键挑战：1）资源受限的边缘设备无法存储完整的专家集合；2）非IID数据分布导致严重的专家负载不平衡，从而降低模型性能。

Method: 提出FLEX-MoE框架，引入客户端-专家适应度评分来量化专家对本地数据集的适用性，并采用基于优化的算法在系统范围内最大化客户端-专家专业化同时强制平衡专家利用率。

Result: 在三个不同数据集上的综合实验表明，FLEX-MoE具有优越的性能，并能在各种资源受限场景下保持平衡的专家利用率。

Conclusion: FLEX-MoE能够有效解决联邦学习环境中MoE模型的专家分配和负载平衡问题，相比现有仅关注个性化而忽略负载不平衡的贪婪方法具有明显优势。

Abstract: Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.

</details>


### [139] [Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models](https://arxiv.org/abs/2512.23073)
*Mingyuan Zhang,Yue Bai,Yifan Wang,Yiyang Huang,Yun Fu*

Main category: cs.LG

TL;DR: 该论文提出将掩码微调（MFT）应用于视觉语言模型（VLMs），通过为每个权重分配可学习的门控分数来重组内部子网络，而不是更新权重，从而实现高效的下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型微调方法大多依赖显式的权重更新，忽略了预训练模型中已编码的丰富表示结构。这些方法未能充分利用模型已有的知识，而掩码微调已被证明在语言模型中是一种强大且高效的训练范式。

Method: 从结构重参数化角度重新思考VLM微调，将掩码微调应用于VLM的语言和投影器组件。该方法不更新权重，而是为每个权重分配可学习的门控分数，让模型重组其内部子网络以适应下游任务。

Result: 实验表明，MFT在多种语言骨干网络下均一致超越LoRA变体甚至全微调，在保持冻结骨干网络不变的情况下实现了高性能。该方法在多个下游任务上表现出色。

Conclusion: 有效的模型适应不仅可以通过更新权重实现，还可以通过重新建立模型已有知识之间的连接来实现。掩码微调为视觉语言模型提供了一种高效且强大的微调范式。

Abstract: Explorations in fine-tuning Vision-Language Models (VLMs), such as Low-Rank Adaptation (LoRA) from Parameter Efficient Fine-Tuning (PEFT), have made impressive progress. However, most approaches rely on explicit weight updates, overlooking the extensive representational structures already encoded in pre-trained models that remain underutilized. Recent works have demonstrated that Mask Fine-Tuning (MFT) can be a powerful and efficient post-training paradigm for language models. Instead of updating weights, MFT assigns learnable gating scores to each weight, allowing the model to reorganize its internal subnetworks for downstream task adaptation. In this paper, we rethink fine-tuning for VLMs from a structural reparameterization perspective grounded in MFT. We apply MFT to the language and projector components of VLMs with different language backbones and compare against strong PEFT baselines. Experiments show that MFT consistently surpasses LoRA variants and even full fine-tuning, achieving high performance without altering the frozen backbone. Our findings reveal that effective adaptation can emerge not only from updating weights but also from reestablishing connections among the model's existing knowledge. Code available at: https://github.com/Ming-K9/MFT-VLM

</details>


### [140] [Trust Region Masking for Long-Horizon LLM Reinforcement Learning](https://arxiv.org/abs/2512.23075)
*Yingru Li,Jiacai Liu,Jiawei Xu,Yuxuan Tong,Ziniu Li,Baoxiang Wang*

Main category: cs.LG

TL;DR: 该论文针对大语言模型强化学习中策略梯度方法的离策略误差问题，提出了更紧的误差边界和信任区域掩码方法，为长序列任务提供非空单调改进保证。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型强化学习中，由于实现差异、专家混合路由不连续性和分布式训练延迟等原因，滚动策略与目标策略之间存在不可避免的离策略不匹配。经典信任区域边界随序列长度呈O(T²)增长，对于长序列任务变得无效。

Method: 推导了两个更紧的误差边界：Pinsker-Marginal边界（O(T³/²)）和Mixed边界（O(T)），两者都依赖于序列级最大token级KL散度。提出了信任区域掩码方法，当序列中任何token违反信任区域时排除整个序列的梯度计算。

Result: 提出的边界比经典边界更紧，信任区域掩码方法为长序列大语言模型强化学习提供了首个非空单调改进保证。

Conclusion: 通过序列级信任区域控制，可以有效管理大语言模型强化学习中的离策略误差，为长序列任务提供理论保证和实践方法。

Abstract: Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.

</details>


### [141] [Multimodal Functional Maximum Correlation for Emotion Recognition](https://arxiv.org/abs/2512.23076)
*Deyang Zheng,Tianyi Zhang,Wenming Zheng,Shujian Yu*

Main category: cs.LG

TL;DR: MFMC提出了一种基于双总相关目标的多模态自监督学习框架，通过最大化高阶多模态依赖性来捕捉情感状态下的脑-自主神经系统协调响应，在多个情感计算基准上取得了SOTA或竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 情感状态表现为中枢和自主神经系统之间协调但异质的生理响应，这对情感计算中的多模态表示学习提出了挑战。现有自监督学习方法主要依赖成对对齐目标，无法捕捉两个以上模态间的依赖关系，也难以表征脑-自主神经响应协调产生的高阶交互。

Method: 提出多模态功能最大相关（MFMC）框架，通过双总相关（DTC）目标最大化高阶多模态依赖性。该方法推导了紧致的夹逼界，并使用基于功能最大相关分析（FMCA）的迹代理进行优化，直接捕捉联合多模态交互，无需依赖成对对比损失。

Result: 在三个公开情感计算基准测试中，MFMC在受试者依赖和受试者独立评估协议下均取得SOTA或竞争性性能。在CEAP-360VR数据集上，仅使用EDA信号就将受试者依赖准确率从78.9%提升至86.8%，受试者独立准确率从27.5%提升至33.1%。在MAHNOB-HCI最具挑战性的EEG受试者独立划分上，与最佳方法差距在0.8个百分点内。

Conclusion: MFMC通过最大化高阶多模态依赖性，有效解决了情感计算中多模态表示学习的挑战，特别是在处理脑-自主神经系统协调响应和受试者间变异性方面表现出色，为情感状态识别提供了更鲁棒的自监督学习框架。

Abstract: Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.
  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.
  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.

</details>


### [142] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 针对大语言模型强化学习中训练-推理不匹配问题，提出动态剪枝"安全"词汇表方法，通过排除极端低概率词元来稳定训练


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习面临训练-推理不匹配的根本矛盾：高吞吐量推理引擎和数值精确训练系统从相同参数产生不同的概率分布，导致训练效果不稳定

Method: 提出将RL目标约束到动态剪枝的"安全"词汇表，排除极端低概率词元，用小的有界优化偏差替代大的系统性不匹配

Result: 经验上实现了稳定训练，理论上对词汇表剪枝引入的优化偏差进行了界定

Conclusion: 通过动态剪枝低概率词元，有效解决了训练-推理不匹配问题，实现了稳定的强化学习训练

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [143] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: 提出一个统一框架，将模仿学习与强化学习结合用于LLM微调，通过分析复合目标梯度分解为可计算的稠密梯度（用于token级模仿）和稀疏梯度（用于长期奖励优化）


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调方法通常将模仿学习和强化学习分开处理，缺乏统一的框架来同时优化token级模仿和长期奖励目标

Method: 通过分析结合轨迹级KL散度和任务奖励的复合目标梯度，将其分解为：1）可解析计算的稠密梯度（用于token级模仿），2）蒙特卡洛估计的稀疏梯度（用于长期奖励优化。稠密梯度具有闭式logit级公式，支持高效GPU实现

Result: 提出了一个统一的LLM微调框架，能够同时处理模仿学习和强化学习目标，其中稠密梯度部分可高效计算，稀疏梯度部分通过蒙特卡洛方法估计

Conclusion: 该框架为LLM微调提供了统一的数学基础，将模仿学习和强化学习自然地结合起来，支持高效的GPU实现，有望提升微调效果和效率

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [144] [How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure](https://arxiv.org/abs/2512.23109)
*Paul M. Thompson*

Main category: cs.LG

TL;DR: 研究生成模型和视觉语言模型在生物医学应用中如何实现均匀准确和校准的预测，而非仅平均表现良好，特别关注有限样本下的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在生物医学决策支持中，生成模型和VLM的预测概率需要既准确又校准良好。当前模型在中等数据量下表现良好，但不确定其预测是否能在所有输入、类别或亚群中均匀泛化，而不仅仅是平均表现良好。这在生物医学中至关重要，因为罕见病症和特定群体即使在总体损失较低时也可能出现较大误差。

Method: 从有限样本角度研究问题，关注通过改变提示或语义嵌入在受限表示空间内获得的分类器族。假设模型输出在低维语义表示上平滑依赖（这一假设得到文本和联合图像-文本嵌入中谱结构的支持），应用经典均匀收敛工具获得非渐近保证。

Result: 主要结果为VLM诱导分类器的准确性和校准泛函提供了有限样本均匀收敛边界，这些边界在提示嵌入的Lipschitz稳定性下成立。样本复杂度取决于内在/有效维度而非环境嵌入维度，并进一步推导了谱依赖边界，明确展示特征值衰减如何控制数据需求。

Conclusion: 研究对数据有限的生物医学建模有重要启示：阐明了当前数据集大小何时能支持均匀可靠的预测，以及为什么平均校准指标可能错过最坏情况下的校准错误。为生成模型和VLM在生物医学应用中的可靠性提供了理论框架。

Abstract: Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.
  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.
  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.

</details>


### [145] [SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals](https://arxiv.org/abs/2512.23131)
*Yankang Li,Changsheng Li*

Main category: cs.LG

TL;DR: 提出SE-MLP架构，结合通道注意力机制和残差连接，快速预测侵彻加速度特征值，替代传统耗时仿真计算


<details>
  <summary>Details</summary>
Motivation: 传统侵彻过程识别依赖加速度特征值，但这些特征值通常通过长时间仿真和昂贵计算获得，需要快速预测方法

Method: 提出挤压激励多层感知器(SE-MLP)，集成通道注意力机制和残差连接，建立物理参数与侵彻特征之间的非线性映射

Result: SE-MLP在预测精度、泛化能力和稳定性方面优于传统MLP、XGBoost和Transformer模型；预测与实测加速度峰值和脉冲宽度的差异在工程容差范围内

Conclusion: 验证了所提方法的可行性和工程适用性，为侵彻引信快速生成先验特征值提供了实用基础

Abstract: Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.

</details>


### [146] [Principled Algorithms for Optimizing Generalized Metrics in Binary Classification](https://arxiv.org/abs/2512.23133)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出METRO算法，用于优化类别不平衡或代价不对称场景下的广义度量（如Fβ、AM、Jaccard等），通过将度量优化转化为代价敏感学习问题，设计具有H-一致性保证的代理损失函数。


<details>
  <summary>Details</summary>
Motivation: 在类别不平衡或代价不对称的应用中，传统二元分类损失不适用，而Fβ、AM、Jaccard等广义度量更合适。现有方法依赖贝叶斯最优分类器，使用基于阈值的方法，先估计类别概率再寻找最优阈值，导致算法不适用于受限假设集且缺乏有限样本性能保证。

Method: 将度量优化重新表述为广义代价敏感学习问题，设计具有可证明H-一致性保证的新型代理损失函数。基于此框架开发METRO算法，提供强理论性能保证。

Result: 实验结果表明，与现有基线方法相比，METRO方法具有显著优势。算法支持H-一致性和有限样本泛化边界。

Conclusion: 提出了一个理论严谨的框架和算法（METRO）来优化广义度量，解决了现有方法在受限假设集和有限样本保证方面的不足，为类别不平衡和代价敏感学习提供了有效的解决方案。

Abstract: In applications with significant class imbalance or asymmetric costs, metrics such as the $F_β$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (Metric Optimization), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.

</details>


### [147] [Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use](https://arxiv.org/abs/2512.23137)
*Runzhi Zhou,Xi Luo*

Main category: cs.LG

TL;DR: GNN-TF模型整合非欧几里得脑成像数据和欧几里得表格数据，通过时间感知的图神经网络和transformer融合，在预测未来烟草使用方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 整合非欧几里得脑成像数据（如动态脑连接）与欧几里得表格数据（临床和人口统计信息）是医学影像分析的重要挑战，特别是在预测未来结果方面。现有方法在纵向成像研究中有效预测结果仍面临困难。

Method: 提出时间感知的图神经网络与transformer融合模型（GNN-TF），灵活整合表格数据和动态脑连接数据，在统一框架内利用这些变量的时间顺序。使用NCANDA纵向静息态fMRI数据集，结合非欧几里得和欧几里得信息源。

Result: 与多种成熟的机器学习和深度学习模型比较，GNN-TF在预测未来烟草使用方面表现出优越的预测准确性，优于现有最先进方法。

Conclusion: GNN-TF的端到端、时间感知的transformer融合结构成功整合了多种数据模态并利用了时间动态特性，使其成为功能脑成像研究中临床结果预测的有价值分析工具。

Abstract: Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.

</details>


### [148] [A Weak Signal Learning Dataset and Its Baseline Method](https://arxiv.org/abs/2512.23160)
*Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang*

Main category: cs.LG

TL;DR: 该研究构建了首个弱信号特征学习专用数据集，并提出PDVFN模型，通过双视图表示和并行特征提取处理低信噪比、类别不平衡问题，在弱信号学习任务中取得更高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 弱信号学习在故障诊断、医学影像、自动驾驶等领域面临挑战，关键信息常被噪声和干扰掩盖。缺乏专用数据集长期制约研究，需要解决低信噪比、类别不平衡等问题。

Method: 构建包含13,158个光谱样本的专用数据集，具有低信噪比主导（55%以上样本SNR低于50）和极端类别不平衡（类别比例达29:1）。提出双视图表示（向量+时频图）和PDVFN模型，并行提取局部序列特征和全局频域结构，遵循局部增强、序列建模、噪声抑制、多尺度捕获、频率提取和全局感知原则。

Result: 实验表明该方法在处理弱信号、高噪声和极端类别不平衡时具有更高准确性和鲁棒性，尤其在低信噪比和不平衡场景中表现优异。

Conclusion: 该研究提供了专用数据集、基线模型，为未来弱信号学习研究奠定基础，为天文光谱学等WSL任务提供了新颖解决方案。

Abstract: Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.

</details>


### [149] [Diffusion-based Decentralized Federated Multi-Task Representation Learning](https://arxiv.org/abs/2512.23161)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的去中心化多任务表示学习算法，用于解决数据稀缺环境下的多任务线性回归问题，通过交替投影梯度下降恢复低秩特征矩阵。


<details>
  <summary>Details</summary>
Motivation: 表示学习在数据稀缺环境中被广泛采用，但去中心化方法研究相对不足。本文旨在开发去中心化的多任务表示学习算法，解决多个线性回归模型共享低维线性表示的问题。

Method: 提出基于扩散的去中心化和联邦式交替投影梯度下降最小化算法，用于恢复低秩特征矩阵。算法采用投影梯度下降方法，在去中心化网络中通过扩散机制进行协作学习。

Result: 获得了构造性的理论保证：提供了所需样本复杂度的下界和迭代复杂度的上界。分析显示算法具有快速和通信高效的特点，数值模拟验证了算法性能优于基准算法。

Conclusion: 成功开发了一种高效的去中心化多任务表示学习算法，在理论保证和实际性能方面均表现出色，为数据稀缺环境下的分布式学习提供了有效解决方案。

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.

</details>


### [150] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: 本文系统评估了12种参数高效微调方法在强化学习可验证奖励范式下的表现，发现传统LoRA并非最优选择，结构变体如DoRA、AdaLoRA和MiSS表现更佳，同时揭示了SVD初始化策略的频谱崩溃现象和极端参数缩减的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA等参数高效微调方法在强化学习可验证奖励范式中常用，但最优的PEFT架构仍未确定。RLVR通过可验证反馈激励语言模型提升推理能力，但缺乏对PEFT方法的系统性评估。

Method: 在DeepSeek-R1-Distill模型家族上，对超过12种PEFT方法进行首次全面评估，包括LoRA、DoRA、AdaLoRA、MiSS、PiSSA、MiLoRA、VeRA、Rank-1等，并在数学推理基准上进行测试。通过消融研究和缩放实验验证发现。

Result: 1. 结构变体如DoRA、AdaLoRA和MiSS持续优于标准LoRA；2. 发现SVD初始化策略存在频谱崩溃现象，主成分更新与RL优化存在根本性不匹配；3. 极端参数缩减严重限制推理能力；4. 挑战了默认采用标准LoRA的做法。

Conclusion: 本研究为参数高效强化学习方法提供了明确的指导，表明需要更多探索而非默认使用标准LoRA，结构变体和适当的参数配置对RLVR性能至关重要。

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [151] [HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction](https://arxiv.org/abs/2512.23175)
*Seungeon Lee,Takuto Koyama,Itsuki Maeda,Shigeyuki Matsumoto,Yasushi Okuno*

Main category: cs.LG

TL;DR: HELM-BERT：首个基于HELM表示法的肽语言模型，显著优于SMILES模型，在肽性质预测任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有分子语言模型（SMILES或氨基酸级表示）无法充分捕捉治疗性肽的化学复杂性和拓扑结构，需要更合适的表示方法

Method: 提出HELM-BERT模型，基于DeBERTa架构，使用HELM（大分子层次编辑语言）表示法，在39,079个化学多样性肽序列上进行预训练

Result: HELM-BERT在环肽膜通透性预测和肽-蛋白质相互作用预测等下游任务中显著优于最先进的SMILES语言模型

Conclusion: HELM的明确单体感知和拓扑感知表示为治疗性肽建模提供了显著的数据效率优势，填补了小分子和蛋白质语言模型之间的空白

Abstract: Therapeutic peptides have emerged as a pivotal modality in modern drug discovery, occupying a chemically and topologically rich space. While accurate prediction of their physicochemical properties is essential for accelerating peptide development, existing molecular language models rely on representations that fail to capture this complexity. Atom-level SMILES notation generates long token sequences and obscures cyclic topology, whereas amino-acid-level representations cannot encode the diverse chemical modifications central to modern peptide design. To bridge this representational gap, the Hierarchical Editing Language for Macromolecules (HELM) offers a unified framework enabling precise description of both monomer composition and connectivity, making it a promising foundation for peptide language modeling. Here, we propose HELM-BERT, the first encoder-based peptide language model trained on HELM notation. Based on DeBERTa, HELM-BERT is specifically designed to capture hierarchical dependencies within HELM sequences. The model is pre-trained on a curated corpus of 39,079 chemically diverse peptides spanning linear and cyclic structures. HELM-BERT significantly outperforms state-of-the-art SMILES-based language models in downstream tasks, including cyclic peptide membrane permeability prediction and peptide-protein interaction prediction. These results demonstrate that HELM's explicit monomer- and topology-aware representations offer substantial data-efficiency advantages for modeling therapeutic peptides, bridging a long-standing gap between small-molecule and protein language models.

</details>


### [152] [Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR](https://arxiv.org/abs/2512.23177)
*Will Sebelik-Lassiter,Evan Schubert,Muhammad Alliyu,Quentin Robbins,Excel Olatunji,Mustafa Barry*

Main category: cs.LG

TL;DR: 使用机器学习算法自动识别声带并区分正常声带与声带麻痹的超声图像，提高诊断准确性


<details>
  <summary>Details</summary>
Motivation: 声带超声检查虽然创伤小、耐受性好，但其准确性高度依赖操作者经验，需要开发自动分析工具来减少人为误差

Method: 收集30名志愿者的声带超声视频，分割为静态帧并统一裁剪尺寸；使用健康和模拟声带麻痹图像训练声带分割模型和声带麻痹分类模型

Result: 声带分割模型验证准确率达96%，最佳分类模型(VIPRnet)验证准确率达99%

Conclusion: 机器学习辅助的声带超声分析在提高诊断准确性方面展现出巨大潜力，有望超越依赖操作者经验的人工判读

Abstract: Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.

</details>


### [153] [A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization](https://arxiv.org/abs/2512.23190)
*Yi-Han Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: LightONS：一种改进的在线牛顿步算法，通过延迟昂贵的Mahalanobis投影，将总运行时间从O(d^ωT)降低到O(d²T + d^ω√T)，同时保持最优O(d log T)遗憾，解决了COLT'13的开放问题。


<details>
  <summary>Details</summary>
Motivation: 在线指数凹优化(OXO)中，标准算法ONS存在计算瓶颈，其Mahalanobis投影步骤成本为Ω(d^ω)，导致总运行时间高达O(d^ωT)。对于随机指数凹优化(SXO)，使用ONS需要O(d^{ω+1}/ε)运行时间，COLT'13的开放问题要求找到运行时间更低的SXO算法。

Method: 提出LightONS算法，这是ONS的简单变体。利用参数免费在线学习中的域转换技术，引入滞后机制，延迟昂贵的Mahalanobis投影直到必要时才执行，从而减少计算成本。

Result: LightONS将总运行时间降低到O(d²T + d^ω√T log T)，同时保持最优O(d log T)遗憾。对于SXO问题，运行时间降低到O(d³/ε)，解决了COLT'13的开放问题。

Conclusion: LightONS通过延迟Mahalanobis投影有效解决了ONS的计算瓶颈，在保持最优遗憾的同时显著降低运行时间。该算法可作为ONS的高效替代品，适用于更广泛的场景，包括梯度范数自适应遗憾、参数随机赌博机和内存高效在线学习。

Abstract: Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $Ω(d^ω)$ arithmetic operations for bounded domains, even for the unit ball, where $ω\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\tilde{O}(d^ωT)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \tilde{O}(d/ε)$ rounds to achieve an excess risk of $ε$, and thereby necessitates an $\tilde{O}(d^{ω+1}/ε)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\tilde{O}(d^{ω+1}/ε)$.
  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^ω\sqrt{T \log T})$ while preserving the optimal $O(d \log T)$ regret. LightONS implies an SXO method with runtime $\tilde{O}(d^3/ε)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.

</details>


### [154] [PGOT: A Physics-Geometry Operator Transformer for Complex PDEs](https://arxiv.org/abs/2512.23192)
*Zhuo Zhang,Xi Yang,Yuan Zhao,Canqun Yang*

Main category: cs.LG

TL;DR: PGOT提出了一种物理-几何算子Transformer，通过谱保持几何注意力机制解决大规模非结构化网格建模中的几何混叠问题，实现线性计算复杂度的多尺度几何特征保持。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在建模PDE时，对大规模非结构化网格和复杂几何形状处理能力有限。常用的特征降维策略会导致几何混叠，丢失关键的物理边界信息。

Method: 提出PGOT框架，核心是谱保持几何注意力机制，采用"物理切片-几何注入"机制融入多尺度几何编码。同时根据空间坐标动态路由计算：平滑区域使用低阶线性路径，激波和不连续区域使用高阶非线性路径。

Result: 在四个标准基准测试中取得一致的最先进性能，并在翼型和汽车设计等大规模工业任务中表现出色。

Conclusion: PGOT通过显式几何感知重建物理特征学习，有效解决了几何混叠问题，实现了空间自适应的高精度物理场建模，计算复杂度保持线性。

Abstract: While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.

</details>


### [155] [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)
*Ziru Niu,Hai Dong,A. K. Qin,Tao Gu,Pengcheng Zhang*

Main category: cs.LG

TL;DR: FedOLF：一种通过有序层冻结和张量操作近似来降低联邦学习计算、内存和通信开销的高效框架


<details>
  <summary>Details</summary>
Motivation: 物联网边缘设备计算能力、内存和带宽有限，现有联邦学习方法通过dropout或层冻结降低开销，但往往牺牲准确性或忽视内存约束

Method: 提出FedOLF框架：1）在训练前按预定义顺序一致冻结层，显著降低计算和内存需求；2）引入张量操作近似（TOA）作为传统量化的轻量级替代，更好保持模型准确性

Result: 在非独立同分布数据上，FedOLF在多个数据集和模型上相比现有方法获得显著更高的准确率（0.3%至6.4%），同时具有更高的能效和更低的内存占用

Conclusion: FedOLF通过有序层冻结和TOA技术有效解决了物联网边缘设备在联邦学习中的资源限制问题，在保持高准确性的同时显著降低了计算、内存和通信开销

Abstract: Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when training deep neural networks. Various FL frameworks have been proposed to reduce computation and communication overheads through dropout or layer freezing. However, these approaches often sacrifice accuracy or neglect memory constraints. To this end, in this work, we introduce Federated Learning with Ordered Layer Freezing (FedOLF). FedOLF consistently freezes layers in a predefined order before training, significantly mitigating computation and memory requirements. To further reduce communication and energy costs, we incorporate Tensor Operation Approximation (TOA), a lightweight alternative to conventional quantization that better preserves model accuracy. Experimental results demonstrate that over non-iid data, FedOLF achieves at least 0.3%, 6.4%, 5.81%, 4.4%, 6.27% and 1.29% higher accuracy than existing works respectively on EMNIST (with CNN), CIFAR-10 (with AlexNet), CIFAR-100 (with ResNet20 and ResNet44), and CINIC-10 (with ResNet20 and ResNet44), along with higher energy efficiency and lower memory footprint.

</details>


### [156] [PFed-Signal: An ADR Prediction Model based on Federated Learning](https://arxiv.org/abs/2512.23262)
*Tao Li,Peilin Li,Kui Lu,Yilei Wang,Junliang Shang,Guangshun Li,Huiyu Zhou*

Main category: cs.LG

TL;DR: PFed-signal：基于联邦学习的ADR信号预测模型，通过欧氏距离消除FAERS数据偏差，提高预测准确性


<details>
  <summary>Details</summary>
Motivation: FAERS系统中存在偏差记录，基于这些数据预测的药物不良反应可能误导在线诊断。传统方法（如ROR、PRR）依赖统计方法无法消除数据偏差，导致信号预测不准确。

Method: 提出PFed-signal联邦学习模型：1) Pfed-Split方法按ADR分割原始数据集；2) ADR-signal模型包含基于联邦学习的偏差数据识别（使用欧氏距离）和基于Transformer的ADR预测模型。先识别并删除偏差数据生成干净数据集，再在干净数据上训练Transformer预测模型。

Result: 干净数据集上的ROR和PRR优于传统方法。PFed-Signal的准确率0.887、F1分数0.890、召回率0.913、AUC 0.957，均高于基线模型。

Conclusion: PFed-signal通过联邦学习和欧氏距离有效消除FAERS数据偏差，显著提高了ADR信号预测的准确性，优于传统统计方法。

Abstract: The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.

</details>


### [157] [On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases](https://arxiv.org/abs/2512.23265)
*Alexander Korotin,Gudmund Pammer*

Main category: cs.LG

TL;DR: 研究流匹配逆问题的唯一性，在一维和高斯分布情况下证明解的唯一性，多维情况仍为开放问题


<details>
  <summary>Details</summary>
Motivation: 研究流匹配逆问题的动机源于现代生成式AI应用，特别是流匹配模型的蒸馏问题

Method: 研究具有有限指数矩分布之间的流匹配逆问题，建立解的唯一性理论

Result: 在一维设置和高斯分布情况下证明了流匹配逆问题解的唯一性

Conclusion: 一般多维流匹配逆问题的唯一性仍然是一个开放问题，需要未来进一步研究

Abstract: This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.

</details>


### [158] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: HC-PINNs通过硬约束严格满足边界条件，但边界函数B作为空间调制器改变了学习景观。本文建立了HC-PINNs的NTK框架，发现B作为谱滤波器重塑网络核的特征谱，有效秩能预测训练收敛性，常用边界函数可能导致谱塌陷和优化停滞。


<details>
  <summary>Details</summary>
Motivation: HC-PINNs通过试函数严格强制执行边界条件，但其训练动态的理论机制尚未被探索。与软约束不同，边界函数B引入了乘法空间调制，这从根本上改变了学习景观。

Method: 建立了HC-PINNs的严格神经正切核(NTK)框架，推导了显式核组合定律。通过谱分析，识别残差核的有效秩作为训练收敛的确定性预测因子。验证了边界函数作为谱滤波器的作用。

Result: 边界函数B作为谱滤波器重塑神经网络原生核的特征谱。有效秩比经典条件数更能预测训练收敛性。常用的边界函数可能无意中导致谱塌陷，尽管精确满足边界条件，但会导致优化停滞。

Conclusion: 该框架将边界函数设计从启发式选择转变为原则性的谱优化问题，为科学机器学习中的几何硬约束提供了坚实的理论基础。边界函数的选择应基于谱特性而非仅几何考虑。

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


### [159] [Deep learning for pedestrians: backpropagation in Transformers](https://arxiv.org/abs/2512.23329)
*Laurent Boué*

Main category: cs.LG

TL;DR: 论文延续之前CNN反向传播向量化推导的工作，将轻量级无索引方法应用于Transformer架构，推导了嵌入层、多头自注意力、层归一化等层的梯度表达式，并提供了LoRA层的梯度以说明参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 尽管有自动微分工具，但手动推导反向传播能帮助深入理解每个操作如何影响最终输出，填补前向传播理解中的空白，从而获得对模型工作原理的更深层直觉。

Method: 采用轻量级无索引方法学，将之前为CNN开发的向量化反向传播推导技术扩展到Transformer架构，特别针对嵌入层、多头自注意力机制和层归一化层进行梯度推导，并包含LoRA层的梯度表达式。

Result: 提供了完整的最小化GPT-like网络的PyTorch实现，以及所有梯度更新的解析表达式，为Transformer架构的反向传播提供了系统的向量化推导框架。

Conclusion: 通过手动推导Transformer架构的反向传播，不仅获得了对模型内部工作机制的深入理解，还建立了一个系统的方法论框架，有助于更有效地理解和优化基于Transformer的模型。

Abstract: This document is a follow-up to our previous paper dedicated to a vectorized derivation of backpropagation in CNNs. Following the same principles and notations already put in place there, we now focus on transformer-based next-token-prediction architectures. To this end, we apply our lightweight index-free methodology to new types of layers such as embedding, multi-headed self-attention and layer normalization. In addition, we also provide gradient expressions for LoRA layers to illustrate parameter-efficient fine-tuning. Why bother doing manual backpropagation when there are so many tools that do this automatically? Any gap in understanding of how values propagate forward will become evident when attempting to differentiate the loss function. By working through the backward pass manually, we gain a deeper intuition for how each operation influences the final output. A complete PyTorch implementation of a minimalistic GPT-like network is also provided along with analytical expressions for of all of its gradient updates.

</details>


### [160] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 该论文提出了多模型协作定律，表明多个LLM协作的性能上限遵循总参数量幂律缩放，且比单模型缩放具有更显著的改进趋势和更低的理论损失下限。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型的能力存在固有上限，而多模型协作（如模型路由和后处理集成）虽然能超越单个模型性能，但缺乏统一的理论框架来预测多模型协作的性能缩放规律。

Method: 提出多模型协作定律，采用方法无关的公式化方法，假设存在理想化的集成预言机，其中每个样本的总交叉熵损失由模型池中任何模型的最小损失决定，以此来量化多模型协作的内在性能上限。

Result: 实验结果显示：1）多模型系统遵循总参数量幂律缩放；2）相比单模型缩放，多模型协作具有更显著的改进趋势和更低的理论损失下限；3）异构模型家族的集成比同构模型家族获得更好的性能缩放，表明模型多样性是协作增益的主要驱动力。

Conclusion: 模型协作是扩展LLM智能前沿的关键维度，多模型协作定律为理解和预测LLM集成系统的性能极限提供了理论基础。

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [161] [ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling](https://arxiv.org/abs/2512.23347)
*Hai Duong Nguyen,Xuan-The Tran*

Main category: cs.LG

TL;DR: ECG-RAMBA：通过分离心电图的形态学和节律特征，再通过上下文感知融合重新整合，提升心电图分类的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心电图分类中表现出色，但跨异构采集设置的泛化能力不足，限制了临床部署和长期监测。现有模型常将形态波形模式和节律动态隐式纠缠，导致捷径学习和分布偏移敏感性增强。

Method: 提出ECG-RAMBA框架：1) 使用MiniRocket提取确定性形态特征；2) 从心率变异性计算全局节律描述符；3) 通过双向Mamba骨干进行长距离上下文建模；4) 引入数值稳定的Power Mean池化算子(Q=3)增强对瞬态异常的敏感性。

Result: 在Chapman-Shaoxing数据集上达到宏观ROC-AUC≈0.85；在零样本迁移中，在CPSC-2021数据集上获得PR-AUC=0.708（房颤检测），显著优于原始信号Mamba基线；在PTB-XL上表现一致。

Conclusion: 确定性形态特征提供坚实基础，显式节律建模和长距离上下文是跨域鲁棒性的关键驱动因素。ECG-RAMBA通过分离和重新整合形态与节律特征，有效提升心电图分类的泛化能力。

Abstract: Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.

</details>


### [162] [ISOPO: Proximal policy gradients without pi-old](https://arxiv.org/abs/2512.23353)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: ISOPO是一种高效的单步梯度方法，用于近似自然策略梯度，相比现有方法减少了计算开销


<details>
  <summary>Details</summary>
Motivation: 现有近端策略方法如GRPO或CISPO需要多步梯度计算和重要性采样裁剪来近似自然梯度步长，计算效率较低

Method: ISOPO在Fisher度量下对每个序列的对数概率梯度进行归一化，然后与优势函数收缩；另一种变体基于神经正切核在每层转换微批次优势

Result: ISOPO可以在单次反向传播中实现层间转换，相比标准REINFORCE方法计算开销可忽略不计

Conclusion: ISOPO提供了一种高效的单步自然策略梯度近似方法，显著提升了计算效率

Abstract: This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.

</details>


### [163] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: 华为openPangu-Embedded模型集成三种CoT推理模式，但推理轨迹带来内存和延迟开销。本文通过低比特量化（INT8和W4A8）优化推理效率，在Ascend NPU上保持高精度同时提升速度。


<details>
  <summary>Details</summary>
Motivation: 华为openPangu-Embedded模型集成了三种不同的思维链推理模式（slow_think、auto_think、no_think），这些模式增强了推理能力，但生成了冗长的推理轨迹，导致显著的内存和延迟开销，在昇腾NPU上的实际部署面临挑战。

Method: 采用低比特量化技术，将FP16计算转换为更高效的整数运算。提出统一低比特推理框架，支持INT8（W8A8）和W4A8量化，专门针对Atlas A2上的openPangu-Embedded模型进行优化。

Result: 在代码生成基准测试（HumanEval和MBPP）上对所有三种CoT模式进行全面评估：INT8量化保持超过90%的FP16基线精度，在Atlas A2上实现1.5倍预填充速度提升；W4A8量化显著减少内存消耗，但精度有适度折衷。

Conclusion: 低比特量化能有效促进昇腾NPU上的高效思维链推理，同时保持高模型保真度，为实际部署提供了可行的解决方案。

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [164] [Diffusion priors enhanced velocity model building from time-lag images using a neural operator](https://arxiv.org/abs/2512.23375)
*Xiao Ma,Mohammad Hasyim Taufik,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 提出结合生成模型与神经算子的新框架，用于高效构建高分辨率速度模型，通过神经算子作为前向映射快速生成RTM扩展图像，并利用生成模型作为正则化器提升分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统速度模型构建方法计算成本高、耗时，而深度学习特别是生成模型和神经算子的发展为解决这些限制提供了新途径，需要高效高精度的速度模型构建方法。

Method: 提出生成神经算子框架：1) 神经算子作为前向映射算子，从真实和偏移速度模型快速生成时间滞后RTM扩展图像；2) 通过自动微分逐步更新偏移速度；3) 嵌入在真实速度模型分布上训练的生成模型作为正则化器，提升分辨率。

Result: 合成和实际数据实验表明，所提出的生成神经算子速度模型构建方法有效，能够获得更清晰、包含更高分辨率信息的预测结果。

Conclusion: 结合生成模型与神经算子的框架能够高效构建高分辨率速度模型，解决了传统方法计算成本高的问题，为地下成像提供了有效解决方案。

Abstract: Velocity model building serves as a crucial component for achieving high precision subsurface imaging. However, conventional velocity model building methods are often computationally expensive and time consuming. In recent years, with the rapid advancement of deep learning, particularly the success of generative models and neural operators, deep learning based approaches that integrate data and their statistics have attracted increasing attention in addressing the limitations of traditional methods. In this study, we propose a novel framework that combines generative models with neural operators to obtain high resolution velocity models efficiently. Within this workflow, the neural operator functions as a forward mapping operator to rapidly generate time lag reverse time migration (RTM) extended images from the true and migration velocity models. In this framework, the neural operator is acting as a surrogate for modeling followed by migration, which uses the true and migration velocities, respectively. The trained neural operator is then employed, through automatic differentiation, to gradually update the migration velocity placed in the true velocity input channel with high resolution components so that the output of the network matches the time lag images of observed data obtained using the migration velocity. By embedding a generative model, trained on a high-resolution velocity model distribution, which corresponds to the true velocity model distribution used to train the neural operator, as a regularizer, the resulting predictions are cleaner with higher resolution information. Both synthetic and field data experiments demonstrate the effectiveness of the proposed generative neural operator based velocity model building approach.

</details>


### [165] [A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers](https://arxiv.org/abs/2512.23380)
*Mohammad Nasirzadeh,Jafar Tahmoresnezhad,Parviz Rashidi-Khazaee*

Main category: cs.LG

TL;DR: CoLog是一个用于日志异常检测的多模态框架，通过协作式Transformer和多头注意力机制处理日志的不同模态，在多个基准数据集上实现了超过99.5%的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态方法忽略了日志数据的不同模态特性，而多模态方法又未能有效处理模态间的交互作用。日志数据根据收集来源包含多种信息模态，需要一种能够协同处理这些模态并学习其交互关系的框架。

Method: 提出CoLog框架：1）使用协作式Transformer和多头注意力机制学习不同模态间的交互；2）引入模态适配层处理由交互引起的异质性；3）能够检测点异常和集体异常。

Result: 在7个日志异常检测基准数据集上，CoLog实现了平均精度99.63%、召回率99.59%、F1分数99.61%的优异性能，超越了现有最先进方法。

Conclusion: CoLog代表了日志异常检测的重要进展，通过统一框架解决了点异常和集体异常检测的复杂挑战，为网络安全、系统监控和运维效率提供了有效的解决方案。

Abstract: Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.

</details>


### [166] [On the Sample Complexity of Learning for Blind Inverse Problems](https://arxiv.org/abs/2512.23405)
*Nathan Buskulic,Luca Calatroni,Lorenzo Rosasco,Silvia Villa*

Main category: cs.LG

TL;DR: 该论文研究了盲逆问题中的学习框架，在线性最小均方误差估计器(LMMSE)框架下，提供了闭式最优估计器表达式，建立了与Tikhonov正则化的等价关系，并推导了有限样本误差界和收敛率。


<details>
  <summary>Details</summary>
Motivation: 盲逆问题在许多实验设置中出现，其中前向算子部分或完全未知。现有的数据驱动方法虽然表现出良好的经验性能，但缺乏可解释性和严格的理论保证，限制了其在成像等应用领域的可靠性。

Method: 在线性最小均方误差估计器(LMMSE)的简化框架下研究盲逆问题学习。推导闭式最优估计器表达式，建立与Tikhonov正则化公式的等价关系，其中正则化项明确依赖于未知信号、噪声和随机前向算子的分布。

Result: 在适当的源条件假设下证明了收敛结果，推导了严格的有限样本误差界，这些界明确量化了算子随机性的影响，并揭示了当随机性消失时的收敛速率。通过数值实验验证了理论发现。

Conclusion: 该工作为盲逆问题中的学习提供了深入的理论分析框架，建立了数据驱动方法与经典正则化理论之间的联系，为实际应用提供了可靠的理论基础和性能保证。

Abstract: Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.

</details>


### [167] [Task-driven Heterophilic Graph Structure Learning](https://arxiv.org/abs/2512.23406)
*Ayushman Raghuvanshi,Gonzalo Mateos,Sundeep Prabhakar Chepuri*

Main category: cs.LG

TL;DR: FgGSL是一种频率引导的图结构学习框架，通过联合学习同质性和异质性图结构以及谱编码器，解决GNN在异质图上的表示学习问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质图上表现不佳，因为异质图中相连节点往往标签不同，特征相似性提供的结构线索较弱。需要一种能够同时捕捉同质和异质关系的图结构学习方法。

Method: 提出端到端的频率引导图结构学习框架，使用可学习的对称特征驱动掩码函数推断互补图结构，通过预设计的低通和高通图滤波器组处理，并引入基于标签的结构损失来显式促进同质和异质边的恢复。

Result: 在六个异质图基准测试中，FgGSL一致优于最先进的GNN和图重连方法，证明了结合频率信息和监督拓扑推断的优势。

Conclusion: FgGSL通过联合学习同质和异质图结构，结合频率引导的图滤波器，为异质图上的节点表示学习提供了有效的解决方案，具有理论稳定性和实际性能优势。

Abstract: Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.

</details>


### [168] [Theoretical Foundations of Scaling Law in Familial Models](https://arxiv.org/abs/2512.23407)
*Huan Song,Qingfei Zhao,Ting Long,Shuyu Tian,Hongjun An,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 论文将神经缩放定律扩展到家族模型范式，引入粒度(G)作为新的缩放变量，证明部署灵活性不会损害计算最优性


<details>
  <summary>Details</summary>
Motivation: 传统神经缩放定律假设单一密集模型输出，忽视了家族模型这一在异构设备-边缘-云层次结构中实现普适智能的关键范式。需要理论扩展来支持"一次训练，多次部署"的实践

Method: 提出统一函数形式L(N, D, G)，采用IsoFLOP实验设计隔离架构影响，系统扫描模型大小(N)和粒度(G)，动态调整训练令牌(D)，解耦粒度边际成本与规模收益

Result: 粒度惩罚遵循乘性幂律且指数极小，理论上连接了固定计算训练与动态架构，实践上验证了"一次训练，多次部署"范式，部署灵活性不会损害密集基线的计算最优性

Conclusion: 成功将缩放定律扩展到家族模型，引入粒度作为基本缩放变量，为异构部署环境中的高效模型训练和部署提供了理论依据和实践指导

Abstract: Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks "Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this "one-run, many-models" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the "train once, deploy many" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.

</details>


### [169] [Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks](https://arxiv.org/abs/2512.23410)
*Yusuf Kalyoncuoglu*

Main category: cs.LG

TL;DR: 提出解耦解几何与搜索空间的方法，通过构造子空间实现模型压缩，在ResNet-50、ViT、BERT上可将分类头压缩16倍而性能几乎不变，并提出子空间原生蒸馏新范式。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络虽然具有低内在维度，但仍需巨大高维宽度来解决非凸优化搜索问题。这种冗余对表示不必要，但对寻找全局最小值是必需的。紧凑网络难以解决此优化瓶颈。

Method: 通过解耦解几何与搜索空间，构造性地绕过优化瓶颈。提出子空间原生蒸馏范式：在构造的子空间中直接定义目标，为学生模型提供稳定的几何坐标系。

Result: 在ResNet-50、ViT和BERT上实验表明，分类头可压缩高达16倍而性能下降可忽略。这验证了构造子空间方法的有效性。

Conclusion: 该方法为实现"训练大模型，部署小模型"愿景提供了新途径，让学生模型可能完全绕过高维搜索问题，在构造的子空间中直接学习。

Abstract: While it is well-established that the weight matrices and feature manifolds of deep neural networks exhibit a low Intrinsic Dimension (ID), current state-of-the-art models still rely on massive high-dimensional widths. This redundancy is not required for representation, but is strictly necessary to solve the non-convex optimization search problem-finding a global minimum, which remains intractable for compact networks. In this work, we propose a constructive approach to bypass this optimization bottleneck. By decoupling the solution geometry from the ambient search space, we empirically demonstrate across ResNet-50, ViT, and BERT that the classification head can be compressed by even huge factors of 16 with negligible performance degradation. This motivates Subspace-Native Distillation as a novel paradigm: by defining the target directly in this constructed subspace, we provide a stable geometric coordinate system for student models, potentially allowing them to circumvent the high-dimensional search problem entirely and realize the vision of Train Big, Deploy Small.

</details>


### [170] [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](https://arxiv.org/abs/2512.23441)
*Taha Emre,Arunava Chakravarty,Thomas Pinetz,Dmitrii Lachinov,Martin J. Menten,Hendrik Scholl,Sobha Sivaprasad,Daniel Rueckert,Andrew Lotery,Stefan Sacu,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.LG

TL;DR: STAMP是一个基于Siamese MAE框架的自监督学习方法，通过随机过程编码时间信息，用于学习医学影像中的疾病进展动态。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法（如MAE）缺乏时间感知能力，无法捕捉纵向医学数据中的疾病进展动态，而确定性方法无法处理疾病演变中的固有不确定性。

Method: 提出STAMP框架，将MAE重构损失重新构建为条件变分推断目标，通过两个输入体积之间的时间差作为条件，以随机过程编码时间信息。

Result: 在OCT和MRI数据集上评估，STAMP预训练的ViT模型在年龄相关性黄斑变性和阿尔茨海默病进展预测任务中优于现有时间MAE方法和基础模型。

Conclusion: STAMP能够有效学习疾病进展的非确定性时间动态，为纵向医学影像分析提供了更好的时间感知表示学习方法。

Abstract: Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.

</details>


### [171] [Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion](https://arxiv.org/abs/2512.23448)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出Dynamic Subspace Composition (DSC)框架，通过状态依赖的稀疏基向量组合近似上下文相关权重，解决MoE模型中的表示坍塌和梯度不稳定问题，显著降低参数复杂度和内存开销。


<details>
  <summary>Details</summary>
Motivation: 混合专家(MoE)模型虽然能扩展容量，但常面临表示坍塌和梯度不稳定的问题。现有方法如Mixture-of-LoRAs需要O(M rd)参数复杂度，效率较低。

Method: DSC框架将权重更新建模为星形域内的残差轨迹，采用幅度门控单纯形插值确保在恒等变换处的连续性。通过解耦的单位范数基向量构建组合式秩K近似，而非检索独立的秩r矩阵。

Result: 将参数复杂度从O(M rd)降低到O(M d)，内存流量减少到O(Kd)。通过框架理论正则化和谱约束提供了动态更新的严格最坏情况边界。

Conclusion: DSC为MoE模型提供了一种高效、稳定的动态权重更新机制，在降低计算和内存开销的同时保证了理论上的稳定性边界。

Abstract: Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC

</details>


### [172] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出DIR方法，通过信息优化来消除奖励模型中的归纳偏差，提升RLHF性能


<details>
  <summary>Details</summary>
Motivation: 奖励模型训练数据质量低，包含各种归纳偏差（如响应长度、迎合性、格式等），现有去偏方法要么针对单一偏差，要么仅建模简单线性相关，无法处理复杂非线性偏差

Method: 基于信息瓶颈理论，最大化奖励模型分数与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入中偏差属性之间的互信息

Result: DIR能有效缓解三种归纳偏差（响应长度、迎合性、格式），提升RLHF在各种基准测试上的性能，增强泛化能力

Conclusion: DIR通过信息理论优化，能处理更复杂的非线性偏差，扩展了奖励模型去偏方法的实际应用场景

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [173] [FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence](https://arxiv.org/abs/2512.23485)
*Guoan Wan,Tianyu Chen,Fangzheng Feng,Haoyi Zhou,Runhua Xu*

Main category: cs.LG

TL;DR: FRoD是一种参数高效微调方法，通过分层联合分解和旋转自由度，在仅使用1.72%可训练参数的情况下，达到全模型微调的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）在效率和表达能力之间存在权衡，由于低秩约束导致收敛慢、适应能力有限，难以捕捉复杂模式。

Method: FRoD结合分层联合分解和旋转自由度，提取跨层全局共享基，向缩放因子注入稀疏可学习扰动，实现灵活的全秩更新。

Result: 在20个涵盖视觉、推理和语言理解的基准测试中，FRoD在相同训练预算下，仅使用1.72%可训练参数就达到了全模型微调的准确率。

Conclusion: FRoD通过增强表达能力和效率，实现了更快、更稳健的收敛，解决了现有PEFT方法在收敛速度和适应能力方面的限制。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.

</details>


### [174] [ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment](https://arxiv.org/abs/2512.23487)
*Vassilis Digalakis,Ramayya Krishnan,Gonzalo Martin Fernandez,Agni Orfanoudaki*

Main category: cs.LG

TL;DR: 提出ML Compass框架，将AI模型选择视为能力-成本前沿上的约束优化问题，解决能力排行榜与实际部署决策之间的差距


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的能力排行榜不能直接转化为部署决策，存在能力-部署差距。组织在选择AI模型时需要综合考虑用户效用、部署成本和合规要求

Method: 开发ML Compass框架：1)从异构模型描述符中提取低维内部度量；2)从能力和成本数据估计经验前沿；3)从交互结果数据学习用户或任务特定的效用函数；4)使用这些组件定位能力-成本配置并推荐模型

Result: 在对话和医疗两个案例研究中验证，框架产生的推荐和部署感知排行榜与仅基于能力的排名有显著差异，阐明了能力、成本和安全性之间的权衡如何影响最优模型选择

Conclusion: ML Compass框架能够有效弥合能力排行榜与实际部署决策之间的差距，通过系统级视角将模型选择与应用结果、操作约束和能力-成本前沿联系起来，为组织提供更实用的AI模型选择方法

Abstract: We study how organizations should select among competing AI models when user utility, deployment costs, and compliance requirements jointly matter. Widely used capability leaderboards do not translate directly into deployment decisions, creating a capability -- deployment gap; to bridge it, we take a systems-level view in which model choice is tied to application outcomes, operating constraints, and a capability-cost frontier. We develop ML Compass, a framework that treats model selection as constrained optimization over this frontier. On the theory side, we characterize optimal model configurations under a parametric frontier and show a three-regime structure in optimal internal measures: some dimensions are pinned at compliance minima, some saturate at maximum levels, and the remainder take interior values governed by frontier curvature. We derive comparative statics that quantify how budget changes, regulatory tightening, and technological progress propagate across capability dimensions and costs. On the implementation side, we propose a pipeline that (i) extracts low-dimensional internal measures from heterogeneous model descriptors, (ii) estimates an empirical frontier from capability and cost data, (iii) learns a user- or task-specific utility function from interaction outcome data, and (iv) uses these components to target capability-cost profiles and recommend models. We validate ML Compass with two case studies: a general-purpose conversational setting using the PRISM Alignment dataset and a healthcare setting using a custom dataset we build using HealthBench. In both environments, our framework produces recommendations -- and deployment-aware leaderboards based on predicted deployment value under constraints -- that can differ materially from capability-only rankings, and clarifies how trade-offs between capability, cost, and safety shape optimal model choice.

</details>


### [175] [Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization](https://arxiv.org/abs/2512.23493)
*Wei Gao,Paul Zheng,Peng Wu,Yulin Hu,Anke Schmeink*

Main category: cs.LG

TL;DR: 提出基于贝叶斯优化的TD3方法，用于IIoT网络中多设备动态URLLC场景下的联合链路自适应和设备调度，解决CSI不完美、样本不平衡等问题，实现更快的收敛速度和更高的总传输速率。


<details>
  <summary>Details</summary>
Motivation: 在工业物联网(IIoT)网络中，支持多设备动态超可靠低延迟通信(URLLC)时面临信道状态信息(CSI)不完美的问题。需要设计联合链路自适应和设备调度方案，在严格的块错误率约束下最大化总传输速率。

Method: 提出贝叶斯优化驱动的Twin Delayed Deep Deterministic Policy Gradient (TD3)方法，基于不完美的CSI自适应确定设备服务顺序和相应的调制编码方案。针对CSI不完美、URLLC网络中的错误样本不平衡以及TD3算法参数敏感性问题，提出了基于贝叶斯优化的训练机制，提供更可靠的学习方向和样本选择方法。

Result: 通过大量仿真验证，所提算法相比现有解决方案实现了更快的收敛速度和更高的总速率性能。

Conclusion: 提出的BO驱动的TD3方法有效解决了IIoT网络中多设备动态URLLC场景下的联合优化问题，通过贝叶斯优化改进训练机制，显著提升了算法收敛速度和系统性能。

Abstract: In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm's convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.

</details>


### [176] [Trustworthy Machine Learning under Distribution Shifts](https://arxiv.org/abs/2512.23524)
*Zhuo Huang*

Main category: cs.LG

TL;DR: 该论文研究在分布偏移下的可信机器学习，针对扰动偏移、域偏移和模态偏移三种常见分布偏移，从鲁棒性、可解释性和适应性三个维度提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在人工智能领域取得了显著进展，但分布偏移问题仍然是限制ML系统可靠性和通用性的关键瓶颈。分布偏移下的泛化问题还会引发AI的信任问题，因此需要研究可信机器学习来解决这些挑战。

Method: 将常见的分布偏移分为三类：扰动偏移、域偏移和模态偏移。从三个维度研究可信性：鲁棒性、可解释性和适应性。基于这些维度提出有效的解决方案和基础见解。

Result: 通过系统研究分布偏移下的可信机器学习问题，提出了增强机器学习关键问题（如效率、适应性和安全性）的解决方案和基础见解。

Conclusion: 分布偏移是限制机器学习系统可靠性和通用性的关键挑战，通过系统研究扰动偏移、域偏移和模态偏移下的可信机器学习，可以提升AI的鲁棒性、可解释性和适应性，从而增强AI的责任感和可靠性。

Abstract: Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.

</details>


### [177] [EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition](https://arxiv.org/abs/2512.23526)
*Maryam Mirzaei,Farzaneh Shayegh,Hamed Narimani*

Main category: cs.LG

TL;DR: EGDA框架通过联合对齐全局和类别特定分布来减少跨会话差异，同时通过图正则化保持EEG数据的固有结构，在SEED-IV数据集上实现了鲁棒的跨会话情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）因其高时间分辨率和直接反映神经活动而成为情感识别的可靠来源，但跨会话的变异对模型泛化构成了主要挑战。需要解决跨会话差异问题以实现更准确的情感识别。

Method: 提出EGDA框架，通过联合对齐全局（边缘）分布和类别特定（条件）分布来减少跨会话差异，同时使用图正则化技术来保持EEG数据的固有结构。

Result: 在SEED-IV数据集上，EGDA在三个迁移任务中分别获得了81.22%、80.15%和83.27%的准确率，超越了多个基线方法。分析显示Gamma频段最具判别性，中央-顶叶和前额叶脑区对情感识别最为关键。

Conclusion: EGDA框架通过联合分布对齐和图正则化有效减少了跨会话差异，实现了鲁棒的跨会话情感识别，为基于EEG的情感识别系统提供了实用的解决方案。

Abstract: Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.

</details>


### [178] [VL-RouterBench: A Benchmark for Vision-Language Model Routing](https://arxiv.org/abs/2512.23562)
*Zhehao Huang,Baijiong Lin,Jingyuan Zhang,Jingying Wang,Yuhang Liu,Ning Lu,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: VL-RouterBench：首个系统性、可复现的视觉语言模型路由基准，涵盖14个数据集、17个模型、51.9万样本对，评估10种路由方法，发现现有路由器与理想Oracle仍有明显差距。


<details>
  <summary>Details</summary>
Motivation: 多模型路由已从工程技术发展为关键基础设施，但现有工作缺乏系统性、可复现的基准来评估视觉语言模型路由系统。

Method: 基于VLMs原始推理和评分日志构建质量-成本矩阵，覆盖14个数据集、3个任务组、30,540个样本，包含15个开源模型和2个API模型，建立联合评估协议测量平均准确率、平均成本和吞吐量。

Result: 评估了10种路由方法和基线，观察到显著的路由能力增益，但当前最佳路由器与理想Oracle仍有明显差距，表明在视觉线索细化和文本结构建模方面有较大改进空间。

Conclusion: VL-RouterBench为多模态路由研究提供了系统性评估框架，将开源完整数据构建和评估工具链，以促进可比性、可复现性和实际部署。

Abstract: Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.

</details>


### [179] [Distribution-Free Process Monitoring with Conformal Prediction](https://arxiv.org/abs/2512.23602)
*Christopher Burger*

Main category: cs.LG

TL;DR: 传统SPC受统计假设限制，本文提出结合Conformal Prediction的混合框架，增强质量控制鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统统计过程控制(SPC)在现代复杂制造环境中存在局限性，其依赖的统计假设经常被违反，导致监控不可靠

Method: 提出混合框架，将Conformal Prediction的分布自由、模型无关保证集成到SPC中，包括两种新应用：Conformal-Enhanced Control Charts（可视化过程不确定性）和Conformal-Enhanced Process Monitoring（将多变量控制重构为异常检测问题）

Result: 框架提供了更鲁棒和统计严谨的质量控制方法，同时保持了经典方法的可解释性和易用性

Conclusion: 通过集成Conformal Prediction，增强了SPC在现代复杂制造环境中的可靠性和实用性

Abstract: Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.

</details>


### [180] [Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning](https://arxiv.org/abs/2512.23617)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: 本文提出Le Cam Distortion框架，通过方向性可模拟性替代对称不变性，为风险可控的迁移学习提供理论保证，在医疗影像、自动驾驶等安全关键领域防止负迁移。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应方法强制特征不变性存在根本缺陷：当域信息量不等时（如高质量vs降质传感器），严格不变性会导致信息破坏，引发"负迁移"，在安全关键应用中可能造成灾难性后果。

Method: 基于Le Cam统计实验理论，提出决策理论框架，用方向性可模拟性替代对称不变性。引入Le Cam Distortion（通过缺陷距离δ(E₁,E₂)量化），作为可模拟性条件下迁移风险的严格上界。通过学习从源到目标的模拟核实现迁移而不降低源域性能。

Result: 在五个实验中：1）HLA基因组学中实现近乎完美的频率估计（相关性r=0.999）；2）CIFAR-10图像分类中保持81.2%准确率（CycleGAN下降34.7%）；3）RL控制中实现安全策略迁移，而不变方法出现灾难性崩溃。

Conclusion: Le Cam Distortion为医疗影像、自动驾驶系统和精准医学等负迁移不可接受的领域，提供了首个风险可控迁移学习的理论框架，解决了传统方法在域信息量不等时的根本缺陷。

Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.

</details>


### [181] [BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization](https://arxiv.org/abs/2512.23631)
*Iris Xu,Guangtao Zeng,Zexue He,Charles Jin,Aldo Pareja,Dan Gutfreund,Chuang Gan,Zhang-Wei Hong*

Main category: cs.LG

TL;DR: 提出BOAD框架，通过多臂老虎机优化自动发现分层多智能体系统，显著提升大语言模型在复杂软件工程任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在解决真实世界软件工程问题时表现不佳，这些问题通常具有长视野和分布外特性。传统单智能体设计需要处理整个工作流程，导致保留无关上下文、产生虚假关联和泛化能力差。

Method: 提出BOAD框架，将软件工程智能体构建为协调专业子智能体（如定位、编辑、验证）的编排器。将层次发现建模为多臂老虎机问题，每个臂代表候选子智能体，奖励衡量其与其他智能体协作时的帮助程度。

Result: 在SWE-bench-Verified上，BOAD优于单智能体和手动设计的多智能体系统。在SWE-bench-Live上，36B系统在评估时排名第二，超越了GPT-4和Claude等更大模型。

Conclusion: 自动发现的分层多智能体系统能显著提升大语言模型在具有挑战性的长视野软件工程任务上的泛化能力，为解决复杂现实问题提供了有效框架。

Abstract: Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.

</details>


### [182] [Random Controlled Differential Equations](https://arxiv.org/abs/2512.23670)
*Francesco Piatti,Thomas Cass,William F. Turner*

Main category: cs.LG

TL;DR: 提出基于随机特征与控制微分方程的训练高效时间序列学习框架，仅训练线性读出层，实现快速可扩展模型。提出RF-CDEs和R-RDEs两种变体，分别近似RBF增强序列模型和粗糙路径输入，在无限宽度极限下分别诱导RBF提升签名核和粗糙签名核。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列学习方法训练成本高，需要大量参数优化。签名计算虽然具有理论优势但计算复杂度高。需要一种既能保留签名理论归纳偏置，又能实现高效训练的时间序列学习框架。

Method: 使用大型随机参数化控制微分方程作为连续时间储层，仅训练线性读出层。提出两种变体：1) RF-CDEs：使用随机傅里叶特征提升输入信号，近似RBF增强序列模型；2) R-RDEs：通过log-ODE离散化直接处理粗糙路径输入，使用对数签名捕获高阶时间交互。

Result: 在多个时间序列基准测试中表现出竞争性或最先进的性能。证明在无限宽度极限下，RF-CDEs诱导RBF提升签名核，R-RDEs诱导粗糙签名核，为随机特征储层、连续时间深度架构和路径签名理论提供统一视角。

Conclusion: 该框架提供了显式签名计算的实用替代方案，保留了签名理论的归纳偏置，同时受益于随机特征的高效性。实现了训练高效、可扩展的时间序列学习模型，在理论和实践上都有重要意义。

Abstract: We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.

</details>


### [183] [End-to-End Test-Time Training for Long Context](https://arxiv.org/abs/2512.23675)
*Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun*

Main category: cs.LG

TL;DR: 将长上下文语言建模重新定义为持续学习问题，提出TTT-E2E方法：在测试时通过下一个token预测持续学习，训练时通过元学习优化初始化，实现恒定推理延迟和良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统长上下文建模主要关注架构设计（如注意力机制改进），本文提出将其重新定义为持续学习问题，旨在通过测试时学习压缩上下文到模型权重中，同时保持恒定推理延迟。

Method: 使用标准滑动窗口注意力Transformer架构，在测试时通过下一个token预测进行持续学习（压缩上下文到权重），训练时通过元学习优化模型初始化。该方法称为端到端测试时训练（TTT-E2E）。

Result: 对于3B模型（训练164B tokens），TTT-E2E在上下文长度扩展性上与全注意力Transformer相当，优于Mamba 2和Gated DeltaNet。同时保持RNN式的恒定推理延迟，在128K上下文上比全注意力快2.7倍。

Conclusion: 将长上下文建模重新定义为持续学习问题是有效的，TTT-E2E方法结合了标准架构、测试时学习和元学习训练，实现了良好的扩展性和恒定推理延迟，为长上下文建模提供了新思路。

Abstract: We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

</details>


### [184] [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/abs/2512.23707)
*Shashwat Goel,Rishi Hazra,Dulhan Jayalath,Timon Willi,Parag Jain,William F. Shen,Ilias Leontiadis,Francesco Barbieri,Yoram Bachrach,Jonas Geiping,Chenxi Whitehouse*

Main category: cs.LG

TL;DR: 利用现有研究论文训练语言模型生成更好的研究计划，通过强化学习和自评分机制，在机器学习和医学领域均取得显著改进


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在生成符合约束和隐含要求的研究计划方面存在困难，需要开发能够更好协助人类研究者的AI共同科学家

Method: 从多领域论文中自动提取研究目标和目标特定评分标准，构建训练语料库，通过强化学习和自评分机制训练研究计划生成模型

Result: 微调后的Qwen3-30B-A3B模型在70%的研究目标上优于初始模型，84%的自动提取评分标准获得专家认可，在医学领域也取得12-22%的相对改进

Conclusion: 该方法展示了可扩展的自动化训练方法在改进通用AI共同科学家方面的潜力，即使在医学研究等执行反馈不可行的问题设置中也有效

Abstract: AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [185] [PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System](https://arxiv.org/abs/2512.22381)
*Mohammad Zakaria Haider,Amit Kumar Podder,Prabin Mali,Aranya Chakrabortty,Sumit Paudyal,Mohammad Ashiqur Rahman*

Main category: cs.ET

TL;DR: PHANTOM：基于物理感知对抗网络和多智能体强化学习的电动汽车充电站攻击策略研究，揭示电网级联安全风险


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车充电站在配电网中的快速部署，需要智能自适应控制来维持电网的韧性和可靠性。传统检测机制可能无法应对复杂的攻击策略，因此需要研究物理感知的网络安全方法。

Method: 1. 提出PHANTOM框架，结合物理信息神经网络和联邦学习构建EVCS集成系统的数字孪生
2. 基于数字孪生构建多智能体强化学习环境，使用DQN和SAC方法推导对抗性虚假数据注入攻击策略
3. 开发输配电网双仿真平台，捕捉EVCS扰动在配电网与输电系统之间的级联交互

Result: 学习到的攻击策略能够破坏负载平衡并引发电压不稳定，这些扰动会跨越输配电网边界传播。结果表明传统检测机制可能被绕过，导致电网级联故障。

Conclusion: 研究强调了物理感知网络安全对于确保大规模车网集成韧性的关键需求，需要开发更智能的防御机制来应对复杂的对抗性攻击。

Abstract: The rapid deployment of electric vehicle charging stations (EVCS) within distribution networks necessitates intelligent and adaptive control to maintain the grid's resilience and reliability. In this work, we propose PHANTOM, a physics-aware adversarial network that is trained and optimized through a multi-agent reinforcement learning model. PHANTOM integrates a physics-informed neural network (PINN) enabled by federated learning (FL) that functions as a digital twin of EVCS-integrated systems, ensuring physically consistent modeling of operational dynamics and constraints. Building on this digital twin, we construct a multi-agent RL environment that utilizes deep Q-networks (DQN) and soft actor-critic (SAC) methods to derive adversarial false data injection (FDI) strategies capable of bypassing conventional detection mechanisms. To examine the broader grid-level consequences, a transmission and distribution (T and D) dual simulation platform is developed, allowing us to capture cascading interactions between EVCS disturbances at the distribution level and the operations of the bulk transmission system. Results demonstrate how learned attack policies disrupt load balancing and induce voltage instabilities that propagate across T and D boundaries. These findings highlight the critical need for physics-aware cybersecurity to ensure the resilience of large-scale vehicle-grid integration.

</details>


### [186] [Protonic Nickelate Device Networks for Spatiotemporal Neuromorphic Computing](https://arxiv.org/abs/2512.22722)
*Yue Zhou,Shaan Shah,Tamal Dey,Yucheng Zhou,Ashwani Kumar,Sashank Sriram,Siyou Guo,Siddharth Kumar,Ranjan Kumar Patel,Eva Y. Andrei,Ertugrul Cubukcu,Shriram Ramanathan,Duygu Kuzum*

Main category: cs.ET

TL;DR: 研究人员开发了一种基于钙钛矿镍酸盐的集成神经形态计算平台，在同一材料系统中实现了非线性时空处理和可编程存储器，用于实时模式识别。


<details>
  <summary>Details</summary>
Motivation: 生物神经电路的计算源于非线性时间响应和空间分布动态网络相互作用的结合，但在硬件中复制这种丰富性一直具有挑战性，因为大多数神经形态设备仅模拟孤立的神经元或突触功能。

Method: 通过在同一晶片上设计对称和不对称氢化NdNiO3结器件，结合超快质子介导的瞬态动力学与稳定的多级电阻状态。对称NdNiO3结网络通过质子再分布实现空间相互作用，同时每个节点提供短期时间记忆。

Result: 该平台实现了纳秒级操作，每个输入能耗为0.2 nJ，在语音数字分类和早期癫痫发作检测中表现出高准确率，优于仅时间处理或非耦合架构。

Conclusion: 质子镍酸盐作为一个紧凑、节能、CMOS兼容的平台，集成了处理和存储器功能，为可扩展的智能硬件提供了新途径。

Abstract: Computation in biological neural circuits arises from the interplay of nonlinear temporal responses and spatially distributed dynamic network interactions. Replicating this richness in hardware has remained challenging, as most neuromorphic devices emulate only isolated neuron- or synapse-like functions. In this work, we introduce an integrated neuromorphic computing platform in which both nonlinear spatiotemporal processing and programmable memory are realized within a single perovskite nickelate material system. By engineering symmetric and asymmetric hydrogenated NdNiO3 junction devices on the same wafer, we combine ultrafast, proton-mediated transient dynamics with stable multilevel resistance states. Networks of symmetric NdNiO3 junctions exhibit emergent spatial interactions mediated by proton redistribution, while each node simultaneously provides short-term temporal memory, enabling nanoseconds scale operation with an energy cost of 0.2 nJ per input. When interfaced with asymmetric output units serving as reconfigurable long-term weights, these networks allow both feature transformation and linear classification in the same material system. Leveraging these emergent interactions, the platform enables real-time pattern recognition and achieves high accuracy in spoken-digit classification and early seizure detection, outperforming temporal-only or uncoupled architectures. These results position protonic nickelates as a compact, energy-efficient, CMOS-compatible platform that integrates processing and memory for scalable intelligent hardware.

</details>


### [187] [Relaxation-based dynamical Ising machines for discrete tomography](https://arxiv.org/abs/2512.22784)
*Mikhail Erementchouk,Aditya Shukla,Pinaki Mazumder*

Main category: cs.ET

TL;DR: V₂动力学伊辛机可精确求解离散层析问题，从随机初始状态高概率收敛到满足层析数据的精确解，而非近似优化解


<details>
  <summary>Details</summary>
Motivation: 传统伊辛机主要解决优化问题的近似解，本文探索动力学伊辛机能否精确求解非平凡数据处理任务，特别是离散层析问题

Method: 使用V₂动力学模型作为伊辛机，将二进制图像重建问题映射到伊辛模型，利用V₂模型的运动方程实现超越汉明邻域约束的非局部跃迁

Result: 对于每个像素最多被两条射线相交的问题，V₂模型能以高概率(P_succ≈1)精确收敛到满足层析数据的图像，收敛时间仅弱依赖于图像尺寸

Conclusion: V₂动力学模型的内在动力学特性使其能精确求解高度非平凡的层析问题，展示了特定动力系统在精确数据处理任务中的潜力

Abstract: Dynamical Ising machines are continuous dynamical systems that evolve from a generic initial state to a state strongly related to the ground state of the classical Ising model. We show that such a machine driven by the V${}_2$ dynamical model can solve exactly discrete tomography problems about reconstructing a binary image from the pixel sums along a discrete set of rays. In contrast to usual applications of Ising machines, targeting approximate solutions to optimization problems, the randomly initialized V${}_2$ model converges with high probability ($P_{\mathrm{succ}} \approx 1$) to an image precisely satisfying the tomographic data. For the problems with at most two rays intersecting at each pixel, the V${}_2$ model converges in internal machine time that depends only weakly on the image size. Our consideration is an example of how specific dynamical systems can produce exact solutions to highly non-trivial data processing tasks. Crucially, this solving capability arises from the dynamical features of the V${}_2$ model itself, in particular its equations of motion that enable non-local transitions of the discrete component of the relaxed spin beyond Hamming-neighborhood constraints, rather than from merely recasting the tomography problem in spin form.

</details>


### [188] [LIMO: Low-Power In-Memory-Annealer and Matrix-Multiplication Primitive for Edge Computing](https://arxiv.org/abs/2512.23212)
*Amod Holla,Sumedh Chatterjee,Sutanu Sen,Anushka Mukherjee,Fernando Garcia-Redondo,Dwaipayan Biswas,Francesca Iacopi,Kaushik Roy*

Main category: cs.ET

TL;DR: LIMO：基于混合信号计算宏的存内退火算法，通过STT-MTJ随机切换逃逸局部最优，结合分治算法处理大规模TSP问题，同时支持神经网络推理。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构在解决大规模组合优化问题（如TSP）时面临内存墙瓶颈，现有存内计算退火方法在问题规模增大时解质量下降，需要更有效的状态空间探索方法。

Method: 提出LIMO混合信号计算宏，实现存内退火算法，利用STT-MTJ的随机切换特性逃逸局部最优；针对大规模问题，采用基于细化的分治算法，支持并行优化；架构模块化设计可复用于向量矩阵乘法等应用。

Result: 在多达85,900个城市的TSP实例上，LIMO系统相比现有硬件退火器获得更优解质量和更快求解时间；同时支持神经网络推理，在图像分类和人脸检测中达到软件可比精度，且延迟和能耗低于基线存内计算架构。

Conclusion: LIMO通过混合信号存内退火宏与分治算法协同设计，有效解决了大规模组合优化的内存墙问题，同时保持架构灵活性以支持多种计算应用，为存内计算系统提供了高效解决方案。

Abstract: Combinatorial optimization (CO) underpins applications in science and engineering, ranging from logistics to electronic design automation. A classic example is the NP-complete Traveling Salesman Problem (TSP). Finding exact solutions for large-scale TSP instances remains computationally intractable; on von Neumann architectures, such solvers are constrained by the memory wall, incurring compute-memory traffic that grows with instance size. Metaheuristics, such as simulated annealing implemented on compute-in-memory (CiM) architectures, offer a way to mitigate the von Neumann bottleneck. This is accomplished by performing in-memory optimization cycles to rapidly find approximate solutions for TSP instances. Yet this approach suffers from degrading solution quality as instance size increases, owing to inefficient state-space exploration. To address this, we present LIMO, a mixed-signal computational macro that implements an in-memory annealing algorithm with reduced search-space complexity. The annealing process is aided by the stochastic switching of spin-transfer-torque magnetic-tunnel-junctions (STT-MTJs) to escape local minima. For large instances, our macro co-design is complemented by a refinement-based divide-and-conquer algorithm amenable to parallel optimization in a spatial architecture. Consequently, our system comprising several LIMO macros achieves superior solution quality and faster time-to-solution on instances up to 85,900 cities compared to prior hardware annealers. The modularity of our annealing peripherals allows the LIMO macro to be reused for other applications, such as vector-matrix multiplications (VMMs). This enables our architecture to support neural network inference. As an illustration, we show image classification and face detection with software-comparable accuracy, while achieving lower latency and energy consumption than baseline CiM architectures.

</details>


### [189] [interID -- An Ecosystem-agnostic Verifier Application for Self-sovereign Identity](https://arxiv.org/abs/2512.23383)
*Hakan Yildiz,Axel Küpper*

Main category: cs.ET

TL;DR: interID是一个模块化凭证验证应用，通过编排生态系统特定的验证器服务来解决不同SSI生态系统之间的互操作性问题


<details>
  <summary>Details</summary>
Motivation: 不同SSI生态系统（如欧洲数字身份和欧洲区块链服务基础设施）由于技术和信任框架差异，在跨生态系统互操作性方面存在重大挑战

Method: 开发interID系统，包含：(1) 生态系统无关的编排层，与多个SSI验证服务接口；(2) 统一API，抽象底层协议复杂性；(3) 实际实现桥接Hyperledger Indy/Aries、EBSI和EUDI三大SSI生态系统

Result: interID成功验证了所有测试钱包的凭证，性能开销最小，同时保持了可扩展的灵活架构，能够接受来自其他SSI生态系统的凭证

Conclusion: 该工作为SSI验证器实现提供了技术解决方案和架构模式，实现了跨SSI生态系统的互操作性

Abstract: Self-Sovereign Identity is a transformative paradigm in digital identity management, empowering individuals with full control over their credentials. However, the coexistence of diverse SSI ecosystems, such as the European Digital Identity and the European Blockchain Services Infrastructure, poses significant challenges for cross-ecosystem interoperability due to technological and trust framework differences. This paper introduces \textit{interID}, a modular credential verification application that addresses this fragmentation by orchestrating ecosystem-specific verifier services. Our key contributions include: (1) an ecosystem-agnostic orchestration layer that interfaces with multiple SSI verification services, (2) a unified API that abstracts underlying protocol complexities for service providers, and (3) a practical implementation that bridges three major SSI ecosystems: Hyperledger Indy/Aries, EBSI, and EUDI. Evaluation results demonstrate that interID successfully verifies credentials across all tested wallets with minimal performance overhead, while maintaining a flexible architecture that can be extended to accept credentials from additional SSI ecosystems. This work offers both a technical solution and architectural pattern for achieving interoperability in SSI verifier implementations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [190] [An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator](https://arxiv.org/abs/2512.22131)
*Sheng Lu,Qianhou Qu,Sungyong Jung,Qilian Liang,Chenyun Pan*

Main category: cs.AR

TL;DR: 提出基于可重构场效应晶体管(RFET)的随机计算神经网络架构，显著降低硬件资源消耗


<details>
  <summary>Details</summary>
Motivation: 传统随机计算神经网络(SCNN)虽然能降低卷积神经网络硬件复杂度，但仍受限于随机数生成器和累加并行计数器等高资源消耗组件，限制了性能提升

Method: 利用RFET器件级可重构特性，设计高效紧凑的随机数生成器、累加并行计数器等核心模块，并开发专用SCNN加速器架构进行系统级仿真

Result: 实验结果表明，在相同技术节点下，基于RFET的SCNN加速器相比FinFET设计在面积、延迟和能耗方面均有显著降低

Conclusion: RFET技术为随机计算神经网络提供了有效的硬件优化方案，能够显著提升能效和性能

Abstract: Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.

</details>


### [191] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: AnalogSAGE：一个开源的自进化多智能体框架，通过分层记忆和仿真反馈实现模拟电路设计的自动化，相比现有方法显著提升了通过率和效率。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计高度依赖人类经验和直觉，现有基于LLM的方法通常局限于提示驱动的网表生成或预定义拓扑模板，难以满足复杂规格要求。

Method: 提出AnalogSAGE框架，协调三阶段智能体探索，通过四个分层记忆层实现迭代优化，利用仿真反馈进行验证，支持开源SKY130 PDK和ngspice仿真。

Result: 在10个不同难度的运算放大器设计任务中，相比现有框架实现了10倍总体通过率、48倍Pass@1，并将参数搜索空间减少4倍。

Conclusion: 分层记忆和基于仿真的推理显著提升了模拟设计自动化的可靠性和自主性，为模拟电路设计自动化提供了有效的解决方案。

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [192] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: TYTAN：基于泰勒级数的非线性激活引擎，通过可重构硬件和动态近似算法加速AI推理，在边缘设备上实现2倍性能提升、56%功耗降低和35倍面积减少。


<details>
  <summary>Details</summary>
Motivation: 随着AI架构快速发展和AI系统普及，边缘设备需要特定领域架构来提升AI推理的加速和能效。当前AI算法部署面临计算成本和能耗等资源限制，特别是GEMM和激活函数等高功耗操作需要优化。

Method: 提出TYTAN：基于泰勒级数的非线性激活引擎，开发广义非线性近似引擎（G-NAE）。结合可重构硬件设计和专用算法，动态估计每个激活函数所需的近似，以最小化与基准精度的偏差。

Result: 在Silvaco FreePDK45工艺节点上的系统级仿真显示，TYTAN能以>950 MHz时钟频率运行。相比基准开源NVIDIA深度学习加速器（NVDLA），实现约2倍性能提升、约56%功耗降低和约35倍面积减少。

Conclusion: TYTAN能有效支持边缘设备上加速、高能效的AI推理，通过优化非线性激活函数在保持精度的同时显著提升性能和降低功耗。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>
